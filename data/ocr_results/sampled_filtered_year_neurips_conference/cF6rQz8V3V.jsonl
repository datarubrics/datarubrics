{"id": "cF6rQz8V3V", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bitstream-Corrupted Video Recovery: A Novel Benchmark Dataset and Method\\n\\nTianyi Liu\\nKejun Wu\\nYi Wang\\nWenyang Liu\\nKim-Hui Yap\\nLap-Pui Chau\\n\\nSchool of EEE, Nanyang Technological University, Singapore\\nDepartment of EEE, The Hong Kong Polytechnic University, Hong Kong\\n\\n{liut0038, wang1241, wenyang001}@e.ntu.edu.sg, {kejun.wu, ekhyap}@ntu.edu.sg, lap-pui.chau@polyu.edu.hk\\n\\nAbstract\\n\\nThe past decade has witnessed great strides in video recovery by specialist technologies, like video inpainting, completion, and error concealment. However, they typically simulate the missing content by manual-designed error masks, thus failing to fill in the realistic video loss in video communication (e.g., telepresence, live streaming, and internet video) and multimedia forensics. To address this, we introduce the bitstream-corrupted video (BSCV) benchmark, the first benchmark dataset with more than 28,000 video clips, which can be used for bitstream-corrupted video recovery in the real world. The BSCV is a collection of 1) a proposed three-parameter corruption model for video bitstream, 2) a large-scale dataset containing rich error patterns, multiple corruption levels, and flexible dataset branches, and 3) a new video recovery framework that serves as a benchmark. We evaluate state-of-the-art video inpainting methods on the BSCV dataset, demonstrating existing approaches' limitations and our framework's advantages in solving the bitstream-corrupted video recovery problem. The benchmark and dataset are released at https://github.com/LIUTIGHE/BSCV-Dataset.\\n\\n1 Introduction\\n\\nAs Cisco's report [1] shows, video traffic is expected to account for 82% of all internet traffic by 2022, making it the commonest multimedia type on the internet. However, due to unreliable channels and physical damage of the storage medium, videos are vulnerable to generated errors in the case of packet loss during transmission and context corruption during compression and storage [2]. Meanwhile, malicious attacks on the video decoder ecosystem may cause the risk of severe damage to video bitstreams [3]. Therefore, bitstream damage during compression, storage, and transmission chains is a common and crucial problem. The various types of damage factors yield different corruption degrees and error patterns in decoded frames, which are irreversible and unpredictable. Recovering the video content in corrupted bitstreams is of vital importance but beset with difficulties.\\n\\nResearchers have been dedicated to video recovery at the encoding, transmission, and decoding stages [4]. Reed-Solomon codes [5] add redundant information during the encoding stage to enable error correction for the receiver. Checksum [6] is used in the transmission process to detect errors and...\"}"}
{"id": "cF6rQz8V3V", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Summary of the corruption pattern in video recovery problem. Compared with the simulated video corruption in existing inpainting or error concealment research, our dataset contains various realistic corruption patterns including (1) blocking artifacts (artfs.), (2) color artifacts, (3) duplication artifacts, (4) misalignment, (5) texture loss, (6) trailing artifacts, which is closer to the corrupted videos 1, 2, 3 in real world.\\n\\nInitiate re-transmission when errors are detected. These methods introduce additional requirements and inflexibility in hardware design and system reliability, and they cannot deal with long sequence loss in bitstreams. More research has focused on visual-based solutions in the decoding stage due to its intuitive and easy access to images, such as error concealment, completion, frame interpolation, and video inpainting.\\n\\nTypically, the error concealment is to mitigate the effect of errors on video quality [7]. However, the error patterns are generally simulated by error masks of the slice or block shapes directly on the decoded video content. The fixed and simple error simulation limits the application scenarios, as the error patterns in realistic scenarios are neither fixed nor simple. Frame interpolation [8, 9, 10, 11, 12] is another visual-based solution. It synthesizes intermediate frames from a given set of correctly decoded frames to replace damaged or lost frames. Nonetheless, interpolation methods are barely satisfactory when there exists a large scene motion between frames [13], and when they encounter errors spread across a sequence of frames. Video inpainting is similar to video completion, which aims to complete missing regions in a given video [14]. Generally, video inpainting takes the surrounding temporal and spatial content as a reference to fill corrupted regions by learning underlying patterns and structural features of videos [15, 16]. However, the corrupted regions are commonly simulated by user-predefined binary error masks instead of natural errors generated from the real bitstream. For the application scenarios of video storage, communication, and internet video, the manually created masks have difficulty in reflecting the shapes and patterns of real corruption. The requirements of video content coherence in temporal and spatial dimensions are hard to meet when large motion and details are missing across frames [15]. Therefore, the real bitstream and video datasets, as well as video content recovery methods, are highly necessary and urgently needed. So far, there is no large-scale dataset specialized for bitstream-corrupted video recovery. Existing inpainting datasets are limited to simulated error masks, and error concealment datasets are small-scale and may require extracting motion information from bitstream, which is not always available [17].\\n\\n1 https://blog.quindorian.org/2013/03/fixing-rtsp-stream-corruption.html/\\n2 https://www.youtube.com/watch?app=desktop&v=M7wZxk7yPeQ\\n3 https://www.youtube.com/watch?v=l66kIS_-UmI\"}"}
{"id": "cF6rQz8V3V", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this paper, we construct the first large-scale benchmark to facilitate the research of bitstream-corrupted video (BSCV) recovery. Our BSCV dataset includes more than 28,000 bitstream-corrupted video clips (over 3,500,000 frames), which are extracted and elaborated from the most popular video inpainting datasets, YouTube-VOS [18] and DA VIS [19]. Specifically, we compress these video clips into bitstreams using the most popular H.264 video codec [20]. Segments in bitstreams are randomly removed to simulate the effect of packet loss error and storage damage error on decoded videos, and these error types are common in real-world multimedia communications [21]. The simulated error patterns used in typical video recovery tasks and the real error patterns of our dataset are shown in Fig. 1. It can be observed that the video error types in our dataset are sophisticated and unpredictable, while others are simple and fixed. Therefore, our dataset enables us to reveal the problem of real-world video corruption in multimedia communications completely. Furthermore, we also provide a specialized recovery method for bitstream-corrupted video. The remaining semantic information in corrupted regions is incorporated with the spatially and temporally adjacent information to recover the corrupted regions.\\n\\nThe main contributions are as follows: (i) We construct BSCV, the first large-scale dataset used for bitstream-corrupted video recovery in the real world. The provided videos are decoded from real corrupted bitstreams, which are generated by our three-parameter bitstream corruption model. The dataset contains over 28,000 challenging corrupted video clips with realistic and unpredictable error patterns, multiple corruption levels, and flexible dataset branches. (ii) We propose a new video recovery framework inspired by video inpainting pipeline. It completes and enhances the feature representation capability by extracting residual visual information from the corrupted region, achieving higher recovery quality. (iii) We perform a comprehensive evaluation of our dataset to reveal the limitation of existing video inpainting algorithms and point out the future direction.\\n\\n2 Related Works\\n\\nBenchmark and dataset. To the best of our knowledge, currently, there is no bitstream-corrupted video benchmark for the research of bitstream-corrupted video recovery. Table 1 shows the comparisons among various video datasets.\\n\\nAs shown in Table 1, for conventional error concealment and corruption recovery research, using a small set of YUV sequence [22] to test the algorithm performance is a common practice. In that case, researchers usually simulate different kinds of stripe or packet loss-caused masks on those video sequences [25, 26, 27, 7]. However, the scale issue limits the application of deep learning methods. Along with the development of video datasets for different computer vision applications, some datasets such as Vimeo90K [23], REDS [24] are proposed for video restoration tasks including super-resolution, deblurring, and so on. Recently, deep learning-based video completion assumes a very similar task setting with error concealment which is a sub-task of video inpainting. By accepting arbitrary masks, learning-based video inpainting can be trained by a large number of samples. The setting of the mask is usually a fixed mask [14] or an object-like mask with limited size, random shape, and motion [16, 28, 15, 29]. Most datasets involve the content of the videos in the DA VIS [19] and YouTube-VOS [18]. However, DA VIS is still a relatively small scale with only 150 video clips, and therefore it is usually used for qualitative evaluation in video inpainting research. Recently, YouTube-VOS has been a widely-used large-scale dataset for various video inpainting algorithm training because of its content diversity. Nevertheless, along with recent research of bitstream-corrupted image restoration [30], the large-scale video dataset never considers such video corruption. With the simulated mask setting, video inpainting and different kinds of video recovery research are difficult to perform well in complex and unpredictable video corruptions because of the gap between the human-predefined binary mask and unpredictable mask supervision. Besides, modern video datasets...\"}"}
{"id": "cF6rQz8V3V", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Header Data \u2026\\n\\nStart code + Header + SPS + PPS are generally < 0.01% of a bitstream\\nFrame data is generally > 99.99% of a bitstream \u2026\\n\\nFigure 2: Left: H.264 bitstream statistics and the proposed corruption model. Right: Inter-frame correlations and error propagation in the frame domain.\\n\\nYou are usually packed in frame sequences, bitstream-related research still hungers for data in the current deep learning era.\\n\\nVideo restoration.\\n\\nRestoring image and video data in various visual environments [31, 32, 33] is currently a key application of artificial intelligence. As videos can be treated as multiple consecutive images/frames, earlier works [34, 35] simply reuse the ideas from image restoration with the temporal redundancy of neighboring frames fails to be explored. To fully utilize temporal information, Xue et al. [23] proposed a task-oriented flow to achieve feature alignment explicitly. Other studies utilized dynamic upsampling filter [36] or deformable convolution [37] to achieve implicit motion compensation. As for feature fusion, either a one-stage direct fusion structure [37, 38] or multi-stage progressive fusion structure [39] have been used in existing methods.\\n\\nVideo error concealment.\\n\\nVideo error concealment, a commonly-used post-processing technique at the decoder side, aims to recover the error regions in decoded videos [26]. It can be divided into various categories in the bitstream and pixel levels, including spatial, temporal, and hybrid spatial-temporal methods [17, 7, 40]. Traditionally, at the bitstream level, missing motion information can be estimated by surrounding motion vector and block partitioning in the previous frame [41, 42]. At the pixel level, pixel-wise processing is capable but relies on deficient spatial information [17]. Recently, deep learning-based methods still assume a traditional corruption pattern and use experimental mask settings to simulate stripe or patch loss [43, 44, 26, 25]. It makes these methods not suitable for recovering bitstream-corrupted videos because the corruption caused by realistic packet loss is generally unpredictable and irregular.\\n\\nVideo inpainting/completion.\\n\\nVideo inpainting is to generate content in unfilled regions of a video, which accepts arbitrarily defined masks to indicate corrupted regions. Traditionally, video inpainting is considered as a patch matching or pixel diffusion problem [45, 45, 46, 47, 48]. In the era of deep learning, the patch-based method also makes significant success [16, 49]. Flow-guided generative methods are currently mainstream in video inpainting, leveraging motion information for spatial and temporal relationships between frames [14, 50, 28, 51, 52, 15, 53]. DFVI [14] was a pioneering work that formulates the generative video inpainting problem as a pixel propagation task rather than simply filling RGB values in corrupted regions. Li et al. [15] built the traditional 3-stage video inpainting pipeline optimized jointly and achieved an efficient end-to-end framework for video inpainting. In the context of bitstream-corrupted video recovery, video inpainting is closely related. However, existing research often overlooks the performance of inpainting algorithms when dealing with dynamic and large masks. Consequently, they fail to address complex recovery scenarios with significant corrupted areas and partially remained content caused by bitstream corruption.\\n\\n3 Bitstream-corrupted Video Dataset\\n\\nH.264 bitstream and bitstream corruption.\\n\\nThe most popular video codec, H.264, was used by 85% of video developers in 2022 [54]. The compatibility of H.264 with a variety of devices and platforms empowers the delivery of video content bitstream over the internet. The H.264 bitstream\"}"}
{"id": "cF6rQz8V3V", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The typical format of H.264 bitstream consists of successive NALUs (network abstraction layer units). A bitstream contains several bytes of start code prefix and Header. The SPS (sequence parameter sets) and PPS (picture parameter sets) also occupy a small number of bytes. By investigating the bitstream component, we find that the bytes of SPS, PPS, header, and start code only take up a negligible proportion, e.g., 0.01% on example bitstream. In contrast, the frame data occupies a dominant proportion of a H.264 bitstream.\\n\\nThe bitstream segments and packets are possibly corrupted or lost in the chains of video storage, encoding, transmission, and decoding. Therefore, video recovery from corrupted bitstreams is in surging demand. Due to the significant proportion of frame data in a bitstream file, corruption is most likely to occur in the frame data parts, which is the basic assumption in this paper. A frame can generally refer to other previously coded frames for high coding efficiency [55, 56]. As shown in the frame domain of Fig. 2, there exist inter-frame correlations among frames of a video when encoding a video into the bitstream. The inter-NALU dependencies are accordingly created in a bitstream. Thus, error propagation among frames tends to be irregular and unpredictable.\\n\\n### Bitstream-corrupted video generation.\\n\\nDue to the popularity of H.264, video clips are encoded by H.264 codec to generate bitstream files of these videos. The coding configuration selects widely used close-GOP (group of pictures), and the GOP sizes adopt 16 frames for a long-range reference. We simulate the corruption pattern by removing specific segments of some NALUs in a bitstream, as shown in the bottom part of Fig. 2. The extremely low proportions of the start code, NALU headers, SPS, and PPS yield a extremely low corruption probability on these parts. Furthermore, corruption on these parts may cause severe errors, e.g., even decoding failures, which is out of vision research. Therefore, we mainly focus on the remaining bitstream of frame data with dominant proportion.\\n\\nBased on the analysis, we can randomly corrupt frames in visual level. Therefore, we parameterize a three-parameter corruption model \\\\((P, L, S)\\\\), where the corrupted fragments are defined by frame corruption probability \\\\(P\\\\), corruption location \\\\(L\\\\), and fragment size \\\\(S\\\\).\\n\\n![Figure 3: Taking the BSCV dataset branch in the parameters of \\\\((1/16, 0.4, 4096)\\\\) as an example for illustration. (a) The statistics of corruption distribution. (b) The corruption level changes among frames for some sampled videos.](image-url)\"}"}
{"id": "cF6rQz8V3V", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"By setting the parameter combinations of the proposed corruption model, multiple branches of the BSCV dataset can be generated. We also provide error region masks in the dataset. Specifically, grayscale difference maps are calculated by subtracting the corrupted videos from the corresponding original videos decoded from the corruption-free bitstream. The slight changes below the default threshold are suppressed, and the small outliers inside or outside masks are removed by morphological filtering.\\n\\nFor the BSCV dataset branch in the parameters of \\\\(\\\\left(\\\\frac{1}{16}, 0.4, 4096\\\\right)\\\\), Fig. 3 illustrates the corruption statistics of the branch and the corruption degree of randomly selected video samples. The area ratio of corrupted regions to their corresponding frame is referred to as the \u201ccorrupted area ratio\u201d. The ratios range from 0-10%, 10-30%, and above 30% are defined as minor (min.), moderate (mod.), and severe (sev.) corruption levels. The ratio of 0 is defined as corruption-free (unc.). We observe that nearly 30% of frames are corrupted for this example dataset branch. Compared with the existing video inpainting tasks with fixed mask area settings (e.g., 1/16), the frame corruption in our dataset is complex, variable, and unpredictable, making it closer to realistic scenarios and more challenging.\\n\\nWe further analyze the rich error patterns of our dataset shown in Fig. 1. Color artifacts occur when chrominance information is corrupted, which is more severe than edge color bleeding in typical compression artifacts [59]. The trailing artifacts come from the corruption of motion information, which causes a floating trailing effect in subsequent frames. The texture information corruption and error propagation may cause blocking artifacts. Duplicate artifacts are common in intra-coding regions, which duplicate the error pixels in the adjacent regions. More details on dataset construction and analysis of error patterns can refer to the Supplementary Material.\\n\\nFlexibility and extensibility.\\n\\nThe constructed dataset and proposed three-parameter corruption model can provide flexibility in dataset customization and extensibility in application scenarios. By setting different parameter combinations, it is flexible to construct custom datasets to meet specific application scenarios, which is demonstrated in the experiment section. We also developed a video recovery framework without relying on the motion, partition, and residual information in case they are not available in a corrupted bitstream. Thus, the provided dataset and recovery framework can extend to broad bitstream corruption scenarios, such as packet loss during transmission, segment corruption in compression, and deletion of partial data in storage. The application scenarios are not limited to bitstream-related video recovery. It is also suitable for local and cloud video processing tasks, like video inpainting, completion, and manipulation.\\n\\n4 Bitstream-corrupted Video Recovery Framework\\n\\nIn this section, inspired by video inpainting framework, we propose a specialized video recovery method achieves feature completion through segmented feature extraction and fusion, thereby better coordinating with optical flow information to guide the generation of high-quality recovery content.\\n\\nThe overview of the proposed bitstream-corrupted video recovery (BSCVR) framework is illustrated in Fig. 4. We propose to enable an additional perception channel to video inpainting frameworks. It extracts and fuses local features from corrupted and corruption-free regions. By encoding the residual information inside the corrupted regions into the local features, it can greatly enhance the feature completion and representation capability compared to existing video inpainting frameworks. Consequently, the enhanced feature can provide a solid reference for the subsequent recovery process. Then a flow-guided feature propagation module is used in [15] to propagate the content. Combining with reference content from non-local frames, the content generation module is implemented by stacking several temporal focal transformers [15].\\n\\nTo be specific, given a corrupted video frame sequence \\\\(X_t \\\\in \\\\mathbb{R}^{3 \\\\times h \\\\times w} | t = 1, 2, \\\\ldots, T\\\\), and its corresponding mask sequence indicating the corrupted regions \\\\(M_t \\\\in \\\\mathbb{R}^{1 \\\\times h \\\\times w} | t = 1, 2, \\\\ldots, T\\\\). The video recovery framework is expected to recover the corrupted region with spatially and temporally plausible content. According to Fig. 4, for the input corrupted frame sequence, we use a context encoder (\\\\(E\\\\)) [60] to perform region-based encoding.\"}"}
{"id": "cF6rQz8V3V", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Overview of our bitstream-corrupted video recovery (BSCVR) framework. Compared with existing methods, we follow the common practice by inputting the corruption-free content as the basic information source when constructing local features for recovery. We additionally enable a new input channel for the corrupted region and extract the feature of its partial contents which is completely ignored by existing methods. With transformer-based architecture, the local feature can be completed and enhanced by encoding the feature of partial contents into it.\\n\\nBy masks will be separately input into the recovery framework. Then, we propose to use several transformer encoder layers [61] to fuse and re-encode these two features to achieve feature completion. By attention-based decoding and channel fusion, an intermediate feature is generated. Consequently, with skip connection and output projection, the representative capability of the resulting feature can be further enhanced by fusing multi-scale and multi-level information. We then follow the approach of flow-guided video inpainting to extract and complete optical flows from neighboring frames to serve as guidance for feature alignment and propagation. Afterward, a content generation module based on temporal focal transformer and soft spliting will combine the enhanced, aligned, and propagated features of local neighboring frames with the reference features of non-local frames\u2019 corruption-free regions to generate content and finally reconstruct a result frame sequence through a decoder (D) module. More detailed descriptions of the methodology can be found in the Supplementary Material.\\n\\n5 Experiment\\n\\nThe proposed BSCVR and state-of-the-art (SOTA) video inpainting methods are performed on the constructed BSCV dataset. We conduct comprehensive quantitative and qualitative evaluations to demonstrate the effectiveness of our dataset and method. The flexibility of our corruption model and the robustness of our BSCVR framework are validated on multiple branches of BSCV.\\n\\nExperimental setting. We adopt the corruption parameter of \\\\((1/16, 0.4, 4096)\\\\), and its corresponding statistics have been illustrated in Fig. 3. This setting has moderate difficulties with adequate corruption types, which is suitable for video inpainting methods. The corrupted region is usually recoverable yet challenging. SOTA video inpainting methods are compared with our method to recover corruption-free videos. STTN [28] and FuseFormer [16] downsample videos to 240P due to the limitation of computational complexity. E2FGVI-HQ is an upgraded version of E2FGVI [15], stating that it could take arbitrary resolution and generate results with the original input resolution. They could be viewed as the main competitor of our method. Noted that previous works set the mask with \u201crandom shape and location\u201d to augment inpainting data, and those pre-trained models are trained with 500K iterations. In contrast, training those methods on our dataset requires only 250K iterations to converge. The detailed implementation of our method can refer to the Supplementary Material.\"}"}
{"id": "cF6rQz8V3V", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method               | Accuracy | Efficiency |\\n|---------------------|----------|------------|\\n| YouTube-VOS (720P) Subset | DA VIS (480P) Subset | Runtime |\\n| PSNR                | SSIM     | LPIPS      | VFID       |\\n| PSNR                | SSIM     | LPIPS      | VFID       |\\n\\n**Parameter (P, L, S):**\\n\\n- **Input:** 18.4384 0.7979 0.1490 0.1999\\n- **E2FGVI-HQ [15]** 26.3734 0.8415 0.0466 0.0444\\n- **BSCVR-S (Ours)** 27.2770 0.8809 0.0427 0.0500\\n- **BSCVR-P (Ours)** 26.3564 0.8511 0.0416 0.0406\\n\\n**Evaluation Metrics:**\\n\\n- **Peak Signal-to-Noise Ratio (PSNR)**\\n- **Structural Similarity Index (SSIM)**\\n- **Learned Perceptual Image Patch Similarity (LPIPS)**\\n- **Video Fr\u00e9chet Inception Distance (VFID)**\\n\\n**Table 2:** Quantitative results of SOTA pre-trained video inpainting methods, their corresponding models trained on our dataset (denoted by *), and our method. In our method, BSCVR-S means that the feature completion module considers the input feature as a sequence like traditional Transformer [61], and BSCVR-P indicates that the module considers the input as patches referring to SwinIR [31]. The comparison is conducted under the 240P setting due to the model capability of previous works. For the methods which are able to handle arbitrary-resolution video, we calculate metrics based on the original frame sequence, and we measured and demonstrate the runtime of the model under 720P (former) / 480P (latter) input, respectively.\\n\\n**Table 3:** Performance comparison with E2FGVI-HQ on different branches with varied corruption parameter combinations.\"}"}
{"id": "cF6rQz8V3V", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Qualitative comparison of our method and SOTA video inpainting methods on low (a) and high (b) resolutions. The involved corruption types include (1) blocking artifacts, (2) color artifacts, (3) duplication artifacts, (4) misalignment, (5) texture loss, (6) trailing artifacts and their combinations.\\n\\nFor the metrics of PSNR, SSIM, LPIPS, and VFID, our method can achieve significant improvement, which makes the bitstream-corrupted video recoverable, e.g., >30dB PSNR for YouTube-VOS. Our method could comprehensively refine the content in corrupted regions to guide plausible content generation and keep low computational complexity by applying efficient model architecture referring to E2FGVI-HQ [15]. For the results on different corruption parameters and more comparison with latest non-end-to-end methods, we provided more experiments results and discussion in the Supplementary Material.\"}"}
{"id": "cF6rQz8V3V", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Corruption ratio distribution of the YouTube-VOS&DAVIS dataset branches for experiments under different corruption parameter combinations.\\n\\nThe limitation of current methods on the proposed dataset and the advantage of our high-quality data and method. More visualized results can be found in the Supplementary Material.\\n\\nFlexibility in Dataset Construction.\\n\\nWe use the three-parameter corruption model \\\\((P, L, S)\\\\) in our dataset construction. We validate the model by generating more branches of dataset with seven additional parameter settings:\\n\\n- \\\\((1/16, 0.4, 2048)\\\\)\\n- \\\\((1/16, 0.4, 8192)\\\\)\\n- \\\\((1/16, 0.4, 4096)\\\\)\\n- \\\\((1/16, 0.2, 4096)\\\\)\\n- \\\\((1/16, 0.8, 4096)\\\\)\\n- \\\\((2/16, 0.4, 4096)\\\\)\\n- \\\\((4/16, 0.4, 4096)\\\\)\\n\\nThese settings represent varying corruption probabilities, where \\\\(P = \\\\frac{m}{l}\\\\) implies that the corruption happens in random \\\\(m\\\\) frames out of \\\\(l\\\\) frames of a GOP.\\n\\nWe analyze the corruption distribution by calculating the ratio of the corrupted region to the frame resolution, as shown in Fig. 6. It indicates that parameter \\\\(P\\\\) has the most significant impact on corruption, leading to a higher number of corrupted frames and more severe damage. For these dataset branches, we compare our BSCVR and E2FGVI-HQ under the original resolution on the DAVIS 480P subset. The results are listed in Tab. 3. It shows that the proposed BSCVR consistently outperforms E2FGVI-HQ, validating the robustness of our BSCVR on multiple dataset branches.\\n\\n6 Conclusion\\n\\nAiming at the challenging problem of bitstream-corrupted video recovery in the real world, we construct the first large-scale benchmark, BSCV. The BSCV provides a bitstream corruption model, a realistic decoded video dataset, and a video recovery framework, BSCVR. The bitstream corruption model enables to flexibly generate dataset branches by specifying parameter combinations. The dataset contains 28,000 realistic video clips decoded from corrupted bitstreams with unpredictable error patterns and corruption levels. The BSCVR offers an effective framework for high-quality video recovery. Extensive experiments demonstrate that the proposed BSCVR outperforms SOTA video inpainting methods quantitatively and qualitatively. The flexibility of dataset construction and the robustness of our BSCVR framework are also validated on various dataset branches. The benchmark dataset is expected to benefit video recovery in multimedia forensics and video communication applications, including live streaming and online conferences, etc. The future work will concentrate on designing more reasonable bitstream corruption models, engaging more dataset sources, and creating more effective recovery frameworks.\"}"}
{"id": "cF6rQz8V3V", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgement\\n\\nThis research is supported by the National Research Foundation, Singapore, and Cyber Security Agency of Singapore under its National Cybersecurity Research & Development Programme (Cyber-Hardware Forensic & Assurance Evaluation R&D Programme <NRF2018NCRNCR009-0001>). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the view of National Research Foundation, Singapore and Cyber Security Agency of Singapore.\\n\\nReferences\\n\\n[1] Cisco. Cisco visual networking index: Forecast and trends, 2017\u20132022. https://twiki.cern.ch/twiki/pub/HEPIX/TechwatchNetwork/HtwNetworkDocuments/white-paper-c11-741490.pdf.\\n[2] V. Boussard, F. Golaghazadeh, S. Coulombe, F. X. Coudoux, and P. Corlay. Robust h.264 video decoding using crc-based single error correction and non-desynchronizing bits validation. In 2020 IEEE International Conference on Image Processing (ICIP), pages 1098\u20131102. IEEE, 2020.\\n[3] Willy R. Vasquez, Stephen Checkoway, and Hovav Shacham. The most dangerous codec in the world: Finding and exploiting vulnerabilities in h.264 decoders. In USENIX Security Symposium, 2023.\\n[4] Kejun Wu, You Yang, Qiong Liu, and Xiao-Ping Zhang. Focal stack image compression based on basis-quadtree representation. IEEE Transactions on Multimedia, 25:3975\u20133988, 2023.\\n[5] Stephen B Wicker and Vijay K Bhargava. Reed-Solomon codes and their applications. John Wiley & Sons, 1999.\\n[6] Firouzeh Golaghazadeh, St\u00e9phane Coulombe, Fran\u00e7ois-Xavier Coudoux, and Patrick Corlay. Checksum-filtered list decoding applied to h. 264 and h. 265 video error correction. IEEE Transactions on Circuits and Systems for Video Technology, 28(8):1993\u20132006, 2017.\\n[7] J. Koloda, J. \u00d8stergaard, S. H. Jensen, V. S\u00e1nchez, and A. M. Peinado. Sequential error concealment for video/images by sparse linear prediction. IEEE Transactions on Multimedia, 15(4):957\u2013969, 2013.\\n[8] Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive separable convolution. In Proceedings of the IEEE international conference on computer vision, pages 261\u2013270, 2017.\\n[9] Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive convolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 670\u2013679, 2017.\\n[10] Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan Yang. Depth-aware video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3703\u20133712, 2019.\\n[11] Ai-Mei Huang and Truong Q Nguyen. A multistage motion vector processing method for motion-compensated frame interpolation. IEEE transactions on image processing, 17(5):694\u2013708, 2008.\\n[12] Jiefu Zhai, Keman Yu, Jiang Li, and Shipeng Li. A low complexity motion compensated frame interpolation method. In 2005 IEEE International Symposium on Circuits and Systems (ISCAS), pages 4927\u20134930. IEEE, 2005.\\n[13] Fitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun, Caroline Pantofaru, and Brian Curless. Film: Frame interpolation for large motion. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part VII, pages 250\u2013266. Springer, 2022.\\n[14] Rui Xu, Xiaoxiao Li, Bolei Zhou, and Chen Change Loy. Deep flow-guided video inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3723\u20133732, 2019.\\n[15] Zhen Li, Cheng-Ze Lu, Jianhua Qin, Chun-Le Guo, and Ming-Ming Cheng. Towards an end-to-end framework for flow-guided video inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17562\u201317571, 2022.\\n[16] Rui Liu, Hanming Deng, Yangyi Huang, Xiaoyu Shi, Lewei Lu, Wenxiu Sun, Xiaogang Wang, Jifeng Dai, and Hongsheng Li. Fuseformer: Fusing fine-grained information in transformers for video inpainting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14040\u201314049, 2021.\\n[17] Mohammad Kazemi, Mohammad Ghanbari, and Shervin Shirmohammadi. A review of temporal video error concealment techniques and their suitability for hevc and vvc. Multimedia Tools and Applications, 80:12685\u201312730, 2021.\\n[18] Linjie Yang, Yuchen Fan, and Ning Xu. The 2nd large-scale video object segmentation challenge - video object segmentation track, October 2019.\"}"}
{"id": "cF6rQz8V3V", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[108x710][19] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel\u00e1ez, Alexander Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv:1704.00675, 2017.\\n\\n[20] Video Team JVT Joint et al. Draft itu-t recommendation and final draft international standard of joint video specification. ITU-T Rec. H. 264/ISO/IEC 14496-10 AVC, 2003.\\n\\n[21] Anthony O Adeyemi-Ejeye, Mohammed Alreshoodi, Laith Al-Jobouri, and Martin Fleury. Impact of packet loss on 4k uhd video for portable devices. Multimedia tools and applications, 78:31733\u201331755, 2019.\\n\\n[22] Yuv sequence. http://trace.eas.asu.edu/yuv/. Accessed: 2023-06-06.\\n\\n[23] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William T Freeman. Video enhancement with task-oriented flow. International Journal of Computer Vision, 127:1106\u20131125, 2019.\\n\\n[24] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu Timofte, and Kyoung Mu Lee. Ntire 2019 challenge on video deblurring and super-resolution: Dataset and study. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 0\u20130, 2019.\\n\\n[25] Byungjin Chung and Changhoon Yim. Bi-sequential video error concealment method using adaptive homography-based registration. IEEE Transactions on Circuits and Systems for Video Technology, 30(6):1535\u20131549, 2020.\\n\\n[26] Kejun Wu, Yi Wang, Wenyang Liu, Kim-Hui Yap, and Lap-Pui Chau. A spatial-focal error concealment scheme for corrupted focal stack video. In 2023 Data Compression Conference (DCC), pages 91\u2013100, 2023.\\n\\n[27] J. Koloda, A. M. Peinado, and V. S\u00e1nchez. Kernel-based mmse multimedia signal reconstruction and its application to spatial error concealment. IEEE Transactions on Multimedia, 16(6):1729\u20131738, 2014.\\n\\n[28] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning joint spatial-temporal transformations for video inpainting. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XVI, pages 528\u2013543. Springer, 2020.\\n\\n[29] Kaidong Zhang, Jingjing Fu, and Dong Liu. Flow-guided transformer for video inpainting. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVIII, pages 74\u201390. Springer, 2022.\\n\\n[30] Wenyang Liu, Yi Wang, Kim-Hui Yap, and Lap-Pui Chau. Bitstream-corrupted jpeg images are restorable: Two-stage compensation and alignment framework for image restoration. arXiv preprint arXiv:2304.06976, 2023.\\n\\n[31] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1833\u20131844, 2021.\\n\\n[32] Yi Wang, Hui Liu, and Lap-Pui Chau. Single underwater image restoration using adaptive attenuation-curve prior. IEEE Transactions on Circuits and Systems I: Regular Papers, 65(3):992\u20131002, 2018.\\n\\n[33] Xintao Zhao, Wenrui Ding, Chunhui Liu, and Hongguang Li. Haze removal for unmanned aerial vehicle aerial video based on spatial-temporal coherence optimisation. IET Image Processing, 12(1):88\u201397, 2018.\\n\\n[34] Hiroyuki Takeda, Peyman Milanfar, Matan Protter, and Michael Elad. Super-resolution without explicit subpixel motion estimation. IEEE Transactions on Image Processing, 18(9):1958\u20131975, 2009.\\n\\n[35] Qiqin Dai, Seunghwan Yoo, Armin Kappeler, and Aggelos K Katsaggelos. Dictionary-based multiple frame video super-resolution. In 2015 IEEE International Conference on Image Processing (ICIP), pages 83\u201387. IEEE, 2015.\\n\\n[36] Younghyun Jo, Seoung Wug Oh, Jaeyeon Kang, and Seon Joo Kim. Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3224\u20133232, 2018.\\n\\n[37] Yapeng Tian, Yulun Zhang, Yun Fu, and Chenliang Xu. Tdan: Temporally-deformable alignment network for video super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3360\u20133369, 2020.\\n\\n[38] Xintao Wang, Kelvin CK Chan, Ke Yu, Chao Dong, and Chen Change Loy. Edvr: Video restoration with enhanced deformable convolutional networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 0\u20130, 2019.\\n\\n[39] Peng Yi, Zhongyuan Wang, Kui Jiang, Junjun Jiang, and Jiayi Ma. Progressive fusion video super-resolution network via exploiting non-local spatio-temporal correlations. In Proceedings of the IEEE/CVF international conference on computer vision, pages 3106\u20133115, 2019.\\n\\n[40] S. Ye, M. Ouaret, F. Dufaux, and T. Ebrahimi. Hybrid spatial and temporal error concealment for distributed video coding. In 2008 IEEE International Conference on Multimedia and Expo, pages 633\u2013636. IEEE, 2008.\"}"}
{"id": "cF6rQz8V3V", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[41] Yueh-Lun Chang, Yuriy A Reznik, Zhifeng Chen, and Pamela C Cosman. Motion compensated error concealment for hevc based on block-merging and residual energy. In 2013 20th International Packet Video Workshop, pages 1\u20136. IEEE, 2013.\\n\\n[42] Ting-Lan Lin, Neng-Chieh Yang, Ray-Hong Syu, Chin-Chie Liao, and Wei-Lin Tsai. Error concealment algorithm for hevc coded video using block partition decisions. In 2013 IEEE International Conference on Signal Processing, Communication and Computing (ICSPCC 2013), pages 1\u20135. IEEE, 2013.\\n\\n[43] Arun Sankisa, Arjun Punjabi, and Aggelos K Katsaggelos. Video error concealment using deep neural networks. In 2018 25th IEEE International Conference on Image Processing (ICIP), pages 380\u2013384. IEEE, 2018.\\n\\n[44] Chongyang Xiang, Jiajun Xu, Chuan Yan, Qiang Peng, and Xiao Wu. Generative adversarial networks based error concealment for low resolution video. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1827\u20131831. IEEE, 2019.\\n\\n[45] Jia-Bin Huang, Sing Bing Kang, Narendra Ahuja, and Johannes Kopf. Temporally coherent completion of dynamic video. ACM Transactions on Graphics (TOG), 35(6):1\u201311, 2016.\\n\\n[46] Mounira Ebdelli, Olivier Le Meur, and Christine Guillemot. Video inpainting with short-term windows: application to object removal and error concealment. IEEE Transactions on Image Processing, 24(10):3034\u20133047, 2015.\\n\\n[47] Miguel Granados, James Tompkin, K Kim, Oliver Grau, Jan Kautz, and Christian Theobalt. How not to be seen\u2014object removal from videos of crowded scenes. In Computer Graphics Forum, volume 31, pages 219\u2013228. Wiley Online Library, 2012.\\n\\n[48] Alasdair Newson, Andr\u00e9s Almansa, Matthieu Fradet, Yann Gousseau, and Patrick P\u00e9rez. Video inpainting of complex scenes. Siam journal on imaging sciences, 7(4):1993\u20132019, 2014.\\n\\n[49] Jingjing Ren, Qingqing Zheng, Yuanyuan Zhao, Xuemiao Xu, and Chen Li. Dlformer: Discrete latent transformer for video inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3511\u20133520, 2022.\\n\\n[50] Chen Gao, Ayush Saraf, Jia-Bin Huang, and Johannes Kopf. Flow-edge guided video completion. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XII 16, pages 713\u2013729. Springer, 2020.\\n\\n[51] Dong Lao, Peihao Zhu, Peter Wonka, and Ganesh Sundaramoorthi. Flow-guided video inpainting with scene templates. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14599\u201314608, 2021.\\n\\n[52] Jaeyeon Kang, Seoung Wug Oh, and Seon Joo Kim. Error compensation framework for flow-guided video inpainting. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XV, pages 375\u2013390. Springer, 2022.\\n\\n[53] Kaidong Zhang, Jingjing Fu, and Dong Liu. Inertia-guided flow completion and style fusion for video inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5982\u20135991, 2022.\\n\\n[54] The 6th annual bitmovin video developer report. https://bitmovin.com/video-developer-report, 2022.\\n\\n[55] Gary J. Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. Overview of the high efficiency video coding (hevc) standard. IEEE Transactions on Circuits and Systems for Video Technology, 22(12):1649\u20131668, 2012.\\n\\n[56] Kejun Wu, You Yang, Qiong Liu, Gangyi Jiang, and Xiao-Ping Zhang. Hierarchical independent coding scheme for varifocal multiview images based on angular-focal joint prediction. IEEE Transactions on Multimedia, pages 1\u201313, 2023.\\n\\n[57] Yilin Wang, Sasi Inguva, and Balu Adsumilli. Youtube ugc dataset for video compression research. In 2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP), pages 1\u20135. IEEE, 2019.\\n\\n[58] Shi Guo, Xi Yang, Jianqi Ma, Gaofeng Ren, and Lei Zhang. A differentiable two-stage alignment scheme for burst image reconstruction with large shift. 2022.\\n\\n[59] Liqun Lin, Shiqi Yu, Liping Zhou, Weiling Chen, Tiesong Zhao, and Zhou Wang. Pea265: Perceptual assessment of video compression artifacts. IEEE Transactions on Circuits and Systems for Video Technology, 30(11):3898\u20133910, 2020.\\n\\n[60] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2536\u20132544, 2016.\"}"}
{"id": "cF6rQz8V3V", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600\u2013612, 2004.\\n\\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.\\n\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84\u201390, 2017.\\n\\nDahun Kim, Sanghyun Woo, Joon-Young Lee, and In So Kweon. Deep video inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5792\u20135801, 2019.\\n\\nJoao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299\u20136308, 2017.\"}"}
