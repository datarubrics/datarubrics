{"id": "DJVyRhT8nP", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Select 3 human activities !!!\\n\\n(a) Criterion 1: High relevance between human activities and their respective regions\\n\\n(b) Criterion 2: Human activities contain verb-rich interactions\\n\\nNotably, 49.2% of humans engage in stationary activities (less than 1 meter), 30.5% move short trajectory lengths. The total trajectory length spans 1066.81m, with an average of 2.85m per human.\\n\\nHuman Activity Trajectory Lengths.\\n\\nThe 145 human activity descriptions in the HAPS Dataset, categorized by their respective indoor spaces. [Zoom in to view]\\n\\nCriteria for filtering suitable human activity descriptions through human surveys. The three key criteria ensure the relevance, interactivity, and realism of the selected activities, resulting in a curated set of 145 human activity descriptions for the HAPS Dataset. [Zoom in to view]\"}"}
{"id": "DJVyRhT8nP", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Human skeletons in the HAPS Dataset, showcasing the diversity of human activities across 6 common indoor regions. Each row represents five different activity descriptions within the same indoor region, with the corresponding activity description displayed above each human skeleton diagram. The HAPS Dataset captures a wide range of realistic and interactive human behaviors. [Zoom in to view]\\n\\nDistances (1-5 meters), 18.4% move long distances (5-15 meters), and 1.9% move very long distances (more than 15 meters). This diverse range of trajectory lengths captures the varied nature of human activities within indoor environments.\\n\\nHuman Impact on the Environment. The presence of humans significantly influences the navigation environment, as depicted in Fig. 10(d). Among the 10,567 viewpoints in the environment, 8.16% are directly affected by human activities, i.e., viewpoints through which humans pass. Furthermore, 46.47% of the viewpoints are indirectly affected, meaning that humans are visible from these locations. This substantial impact highlights the importance of considering human presence and movement when developing navigation agents for real-world applications.\"}"}
{"id": "DJVyRhT8nP", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure a: Room Popularity\\n\\nFigure b: Human Count by Interval\\n\\nFigure c: Distribution of Distances Traveled\\n\\nFigure d: Impact Distribution\\n\\nB.3 Realistic Human Rendering\\n\\nThe rendering process has been meticulously optimized to ensure spatial and visual coherence between human motion models and the scene. Fig. 11 showcases the realistic rendering of humans in various indoor environments, demonstrating the simulator's ability to generate lifelike and visually diverse scenarios. The following key optimizations contribute to high-quality rendering:\\n\\nCamera Alignment with Agent\u2019s Perspective. The rendering process aligns the camera settings with the agent\u2019s perspective, incorporating a 60-degree field of view (FOV), 120 frames per second (fps), and a resolution of 640x480 pixels. This alignment ensures that the rendered visuals accurately mirror the agent\u2019s visual acuity and motion fluidity, providing a realistic and immersive experience.\\n\\nIntegration of Human Motion Models. To generate continuous and lifelike movements, the simulator leverages 120-frame sequences of SMPL mesh data when placing human motion models in the scene. This approach allows for the sequential output of both RGB and depth frames, effectively capturing the dynamics of human motion and enhancing the realism of the rendered environment.\\n\\nUtilization of Depth Maps. The rendering process employs depth maps to distinctly segregate the foreground (human models) from the background (scene). By doing so, the simulator ensures that the rendered humans accurately integrate with the environmental context without visual discrepancies, resulting in a seamless and visually coherent experience. Fig. 13 presents continuous video frames captured from the HA3D simulator. These optimizations ensure that the HA3D simulator provides a high level of realism and detail in rendering human activities within indoor environments. By accurately replicating human movements and interactions, the simulator creates a rich and dynamic setting for training and evaluating human-aware navigation agents.\\n\\nThese optimizations ensure that the HA3D simulator provides a high level of realism and detail in rendering human activities within indoor environments. By accurately replicating human movements and interactions, the simulator creates a rich and dynamic setting for training and evaluating human-aware navigation agents. By incorporating adjustable video observations, navigable viewpoints, and collision feedback signals, the HA3D simulator offers a comprehensive and flexible environment for advancing research in human-aware vision-and-language navigation. These features ensure that...\"}"}
{"id": "DJVyRhT8nP", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the agents developed and tested within this simulator are well-prepared for the complexities and challenges of real-world navigation tasks.\\n\\nB.4 Agent-Environment Interaction\\nTo ensure the versatility and applicability of the HA3D simulator across a wide range of navigation tasks, we have designed the agent's posture and basic actions to align with the configurations of the well-established Matterport3D simulator. This design choice facilitates a seamless transition for researchers and practitioners, allowing them to leverage their existing knowledge and methodologies when utilizing the HA3D simulator. At each time step, agents within the HA3D simulator can receive several critical environmental feedback signals that enhance their understanding of the dynamic navigation environment.\\n\\nFirst-person RGB-D Video Observation. The simulator provides agents with first-person video observations that include dynamic human images corresponding to the agent's perspective. The frame rates and field of view (FOV) of these video observations are adjustable, enabling researchers to tailor the visual input to their specific requirements and computational constraints. This flexibility ensures that the simulator can accommodate a variety of research objectives and hardware configurations.\\n\\nFigure 11: Single-frame in the HA3D simulator showcase viewpoints with human presence in each scene (120-degree FOV), demonstrating the diversity of human activities and environments. Common indoor regions such as bedrooms, hallways, kitchens, balconies, and bathrooms are displayed. Multiple humans can appear in the same region, as seen in the third row, sixth column, and the fifth row, fifth and sixth columns. [Zoom in to view]\\n\\nSet of Navigable Viewpoints. The HA3D simulator provides agents with reachable viewpoints around them, referred to as navigable viewpoints. This feature enhances the navigation flexibility and practicality of the simulator, allowing agents to make informed decisions based on their current position and the available paths. By providing agents with a set of navigable viewpoints, the simulator empowers them to explore the environment efficiently and effectively, mimicking the decision-making process of real-world navigational agents.\\n\\nHuman \u201cCollision\u201d Feedback Signal. To promote safe and socially-aware navigation, the HA3D simulator incorporates a human \u201ccollision\u201d feedback signal. Specifically, when the distance between an agent and a human falls below a predefined threshold (default: 1 meter), the simulator triggers a feedback signal, indicating that the human has been \u201ccrushed\u201d by the agent. This feedback mechanism serves as a critical safety measure, encouraging agents to maintain a safe distance from humans and avoid potential collisions. By integrating this feedback signal, the simulator reinforces the importance of socially-aware navigation and facilitates the development of algorithms that prioritize human safety in dynamic environments.\\n\\nB.5 Implementation and Performance\\nThe HA3D Simulator is a powerful and efficient platform designed specifically for simulating human-aware navigation scenarios. Built using a combination of C++, Python, OpenGL, and Pyrender,\"}"}
{"id": "DJVyRhT8nP", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"the simulator seamlessly integrates with popular deep learning frameworks, enabling researchers to efficiently train and evaluate navigation agents in dynamic, human-populated environments. One of the key strengths of the HA3D Simulator is its customizable settings, which allow researchers to tailor the environment to their specific requirements. Users can easily adjust parameters such as image resolution, field of view, and frame rate, ensuring that the simulator can accommodate a wide range of research objectives and computational constraints. In terms of performance, the HA3D Simulator achieves impressive results, even on modest hardware. When running on an NVIDIA RTX 3050 GPU, the simulator can maintain a frame rate of up to 300 fps at a resolution of 640x480. This level of performance is comparable to state-of-the-art simulation platforms [49, 39, 42], demonstrating the simulator's efficiency and optimization. Resource efficiency is another notable aspect of the HA3D Simulator. On a Linux operating system, the simulator boasts a memory footprint of only 40MB, making it accessible to a wide range of computing environments. Additionally, the simulator supports multi-processing operations, enabling researchers to leverage parallel computing capabilities and significantly enhance training efficiency.\\n\\nTo further facilitate the annotation process and improve accessibility, we have developed a user-friendly annotation toolset based on PyQt5 (Fig. 12). These tools feature an intuitive graphical user interface (GUI) that allows users to efficiently annotate human viewpoint pairs, motion models, and navigation data. The annotation toolset streamlines the process of creating rich, annotated datasets for human-aware navigation research.\"}"}
{"id": "DJVyRhT8nP", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13: Video frames in the HA3D simulator showcasing viewpoints with human presence in each scene (120-degree FOV), reflecting visual diversity. Common indoor regions such as hallways, offices, dining rooms, closets, TV rooms, living rooms, and bedrooms are displayed. The simulator is capable of rendering multiple humans within the same region and field of view, as exemplified in the 9th and 11th rows of the grid, where two people appear simultaneously. [Zoom in to view]\"}"}
{"id": "DJVyRhT8nP", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.1 HA-R2R Dataset\\n\\nInstruction Generation.\\n\\nTo generate new instructions for the HA-R2R dataset, we utilize LangChain and sqlang to interface with GPT-4, leveraging its powerful language generation capabilities to create contextually relevant and coherent instructions. Note that we use GPT-4 Turbo in our code; it refers to the model ID gpt-4-1106-preview in the OpenAI API. Our approach to instruction generation involves the use of a carefully designed few-shot template prompt. This prompt serves as a guiding framework for the language model, providing it with the necessary context and structure to generate instructions that align with the objectives of the HA-R2R dataset.\\n\\nThe few-shot template prompt consists of two key components: a system prompt and a set of few-shot examples. The system prompt is designed to prime the language model with the overall context and requirements for generating navigation instructions in the presence of human activities. It outlines the desired characteristics of the generated instructions, such as their relevance to the navigation task, incorporation of human activity descriptions, and adherence to a specific format. The few-shot examples, on the other hand, serve as a sequence of representative instructions that demonstrate the desired output format and content. These examples are carefully curated to showcase the inclusion of human activity descriptions, the use of relative position information, and the integration of these elements with the original navigation instructions from the R2R dataset.\\n\\nBy providing both the system prompt and the few-shot examples, we effectively guide the generation process towards producing instructions that are consistent with the objectives of the HA-R2R dataset. List. 1 and List. 2 provide a detailed illustration of our prompt engineering approach, showcasing the system prompt and the few-shot examples used for sequential instruction generation. Through this prompt engineering technique, we are able to harness the power of GPT-4 to generate a diverse set of new instructions that effectively incorporate human activity descriptions and relative position information, enhancing the realism and complexity of the navigation scenarios in the HA-R2R dataset.\\n\\nYour role is to function as an instruction generator. You will receive Route-to-Route (R2R) navigation instructions and information about human activities. Your task is to integrate these R2R instructions with the details of the human activities to create new, clear instructions that a robot can understand for navigation. Please adhere to the following guidelines when generating instructions:\\n\\n- Deliver the instructions in a single paragraph...\\n- Conclude with a relative position description, which should be limited to one or two sentences.\\n\\nNow I give you 5 examples...\\n\\n| Original instruction | Human Activity Description | Relative Position Description | Question | Answer |\\n|----------------------|---------------------------|-------------------------------|----------|--------|\\n| Walk between the columns and make a sharp turn right. Walk down the steps and stop on the landing. | The 1th Human Description: Relative Position: Beginning Location and Activity: balcony: A couple having a quiet, intimate conversation. | | What is the new instruction? | Navigate between the columns and execute a sharp right turn, taking special care at the beginning of your path where a couple might be engaged in a quiet, intimate conversation on the balcony. Proceed to walk down the steps and come to a halt on the landing, ensuring you do not disturb the couple\u2019s privacy or space during your movement. |\\n\\nWord Frequency Analysis.\\n\\nTo assess the quality and practicality of the instructions in the HA-R2R dataset, we conducted a comprehensive word frequency analysis. Fig. 14 shows the dataset's potential...\"}"}
{"id": "DJVyRhT8nP", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to support the development and evaluation of robust navigation agents that can effectively interpret\\nand follow human-like instructions in complex, dynamic environments.\\n\\nThe left chart in Fig. 14 illustrates the frequency of various nouns used in the instructions. The top 5\\nmost frequent nouns are turn, stair, room, hallway, and door. Among these, the noun turn exhibits\\nthe highest frequency, appearing more than 5000 times throughout the dataset. Other nouns in the list\\ninclude exit, left, bedroom, right, bathroom, walk, doorway, towards, table, kitchen, area, way, step,\\nproceed, chair, hall, bed, side, path, and living. The presence of these nouns indicates the rich spatial\\nand contextual information conveyed in the navigation instructions.\\n\\nSimilarly, the right chart in Fig. 14 presents the frequency distribution of various verbs used in the\\ninstructions. The top 5 most frequent verbs are proceed, make, walk, turn, and leave. Among these,\\nthe verb proceed exhibits the highest frequency, also appearing over 5000 times throughout the\\ndataset. Other verbs in the list include reach, take, continue, go, enter, begin, exit, stop, pass, keep,\\nnavigate, move, ascend, approach, descend, straight, ensure, be, follow, and locate. The diversity of\\nthese verbs highlights the range of actions and directions provided in the navigation instructions.\\n\\nThe word frequency analysis provides valuable insights into the composition and quality of the\\nHA-R2R dataset. The prevalence of common navigation instruction words, such as spatial nouns and\\naction verbs, demonstrates the dataset's adherence to established conventions in navigation instruction\\nformulation. This consistency ensures that the instructions are practical, easily understandable, and\\naligned with real-world navigation scenarios. Moreover, the balanced distribution of nouns and verbs\\nacross the dataset indicates the presence of rich spatial and temporal information in the instructions.\\nThe nouns provide crucial details about the environment, landmarks, and objects, while the verbs\\nconvey the necessary actions and movements required for successful navigation.\\n\\nFigure 14: Word frequency distribution of all instructions in the HA-R2R dataset, showcasing the prevalence\\nof common navigation instruction words. The x-axis of both charts represents the frequency range, while the\\ny-axis lists the words. The bars are colored light blue for nouns (left chart) and light red for verbs (right chart),\\nproviding a clear visual distinction between the two word categories. The balanced distribution of nouns and\\nverbs highlights the rich spatial and temporal information conveyed in the navigation instructions, ensuring their\\nquality and practicality. [Zoom in to view]\"}"}
{"id": "DJVyRhT8nP", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.2 Algorithm to Construct Oracle (Expert) Agent\\n\\nThe Expert agent, also known as the Oracle agent, is handcrafted using a sophisticated path planning and collision avoidance strategy. The algorithm employed to construct the expert agent is summarized in Algorithm 1.\\n\\nThe Oracle agent operates by parsing the provided language instructions \\\\( I = \\\\langle w_1, w_2, \\\\ldots, w_L \\\\rangle \\\\) and identifying the current state \\\\( s_t = p_t, \\\\phi_t, \\\\lambda_t, \\\\Theta_60_t \\\\). It then updates the navigation graph \\\\( G = (N, E) \\\\) by excluding the subset of nodes \\\\( N_h \\\\) that are affected by human activity, resulting in a modified graph \\\\( G' = (N \\\\setminus N_h, E') \\\\). This step ensures that the agent avoids navigating through areas where human activities are present.\\n\\nUsing the updated graph \\\\( G' \\\\), the Oracle agent computes the shortest path to the goal using the A* search algorithm. This algorithm efficiently explores the navigation graph, considering the cost of each node and the estimated distance to the goal, to determine the optimal path.\\n\\nIf human activity is detected along the planned path, the Oracle agent employs a two-step approach for collision avoidance. First, it attempts to make a dynamic adjustment to its trajectory. If a safe alternative path is available, the agent selects the next state \\\\( s'_{t+1} \\\\) that minimizes the cost function \\\\( c(s_t, s_{t+1}) \\\\) while avoiding the human-occupied state \\\\( h_t \\\\). This dynamic adjustment allows the agent to smoothly navigate around human activities without significantly deviating from its original path. In cases where dynamic adjustment is not possible, the Oracle agent resorts to rerouting. If the distance between the current state \\\\( s_t \\\\) and the human-occupied state \\\\( h_t \\\\) is less than the avoidance threshold distance \\\\( \\\\delta \\\\), the agent reroutes to an alternative state \\\\( s'_{t+1} \\\\). This rerouting strategy ensures that the agent maintains a safe distance from human activities and prevents potential collisions.\\n\\nThroughout the navigation process, the Oracle agent continuously monitors the distance between its current state \\\\( s_t \\\\) and any human-occupied states \\\\( h_t \\\\). If the distance falls below the minimum safe distance \\\\( \\\\epsilon \\\\), the collision indicator \\\\( C(s_t, h_t) \\\\) is set to 1, signifying a potential collision. This information is used to guide the agent's decision-making and ensure safe navigation.\\n\\nFinally, the Oracle agent executes the determined action \\\\( a_t \\\\) and continues to navigate towards the goal until it is reached. By iteratively parsing instructions, updating the navigation graph, computing optimal paths, and employing dynamic adjustments and rerouting strategies, the Oracle agent effectively navigates through the environment while avoiding human activities and maintaining a safe distance.\\n\\n**Algorithm 1**\\n\\n**Oracle Agent Path Planning and Collision Avoidance Strategies**\\n\\nRequire:\\n- Language instructions \\\\( I = \\\\langle w_1, w_2, \\\\ldots, w_L \\\\rangle \\\\), current state \\\\( s_t = p_t, \\\\phi_t, \\\\lambda_t, \\\\Theta_60_t \\\\), navigation graph \\\\( G = (N, E) \\\\), subset of nodes affected by human activity \\\\( N_h \\\\), minimum safe distance \\\\( \\\\epsilon \\\\), avoidance threshold distance \\\\( \\\\delta \\\\).\\n\\nEnsure:\\n- Next action \\\\( a_t \\\\) while goal not reached\\n\\n1. Parse \\\\( I \\\\), identify \\\\( s_t \\\\)\\n2. Update \\\\( G' = (N \\\\setminus N_h, E') \\\\) \\\\{Exclude nodes \\\\( N_h \\\\}\\\\)\\n3. Compute shortest path using A* on \\\\( G' \\\\)\\n4. if human activity detected then\\n5. if dynamic adjustment possible then\\n6. \\\\( s'_{t+1} = \\\\text{arg min}_{s \\\\neq h_t} c(s_t, s) \\\\) \\\\{Dynamic interaction strategy: find new state avoiding human activity\\\\}\\n7. else\\n8. Reroute to \\\\( s'_{t+1} \\\\) if \\\\( d(s_t, h_t) < \\\\delta \\\\) \\\\{Conservative avoidance strategy: reroute if within avoidance threshold\\\\}\\n9. end if\\n10. end if\\n11. \\\\( C(s_t, h_t) = 1 \\\\) if \\\\( d(s_t, h_t) < \\\\epsilon \\\\) \\\\{Collision avoidance strategy: mark collision if too close\\\\}\\n12. Execute \\\\( a_t \\\\)\\n13. end while\"}"}
{"id": "DJVyRhT8nP", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.3 Algorithm to Construct VLN-DT\\n\\nThe pseudocode for the structure and training of VLN-DT, presented in a Python-style format, is summarized in algorithm 2. Note that we use the pseudocode template from [8]. The VLN-DT model takes as input the returns-to-go ($R$), instructions ($I$), current observations ($\\\\Theta$), actions ($a$), and timesteps ($t$). The key components of the model include the transformer with causal masking, embedding layers for state, action, and returns-to-go, a learned episode positional embedding, a cross-modality fusion module, BERT layers for language embedding, a ResNet-152 feature extractor for visual embedding, and a linear action prediction layer.\\n\\nThe main VLNDecisionTransformer function computes the BERT embedding for instructions and the CNN feature map for visual observations. These embeddings are then fused using the cross-modality fusion module to obtain a unified representation. Positional embeddings are computed for each timestep and added to the token embeddings for state, action, and returns-to-go. The resulting interleaved tokens are passed through the transformer to obtain hidden states, from which the hidden states corresponding to action prediction tokens are selected. Finally, the next action is predicted using the linear action prediction layer.\\n\\nDuring the training loop, the VLN-DT model is trained using a cross-entropy loss for discrete actions. The optimizer is used to update the model parameters based on the computed gradients. In the evaluation loop, the target return is set (e.g., expert-level return), and the model generates actions autoregressively. At each timestep, the next action is sampled using the VLN-DT model, and the environment is stepped forward to obtain a new observation and reward. The returns-to-go are updated, and new tokens are appended to the sequence while maintaining a context length of $K$.\\n\\nC.4 Different Reward Types for VLN-DT\\n\\nTo train the VLN-DT agent effectively, we define three distinct reward types that capture different aspects of the navigation task and encourage desirable behaviors.\\n\\nTarget Reward. The target reward is defined as follows:\\n\\n$$r_{target}^t = 5, \\\\text{if } d(s^t, target) \\\\leq \\\\text{threshold} - 5,$$\\n$$\\\\text{otherwise} \\\\quad -5.$$  \\n\\nThis reward type incentivizes the agent to reach the target location within a specified distance threshold. If the agent stops within a distance $\\\\text{threshold}$ from the target, it receives a positive reward of 5. Otherwise, if the agent fails to reach the target or stops far from it, a negative reward of -5 is given. This reward encourages the agent to navigate accurately and reach the desired destination.\\n\\nDistance Reward. The distance reward is defined as follows:\\n\\n$$r_{distance}^t = 1, \\\\text{if } d(s^t, target) < d(s^{t-1}, target) - 0.1,$$\\n$$\\\\text{otherwise} \\\\quad -0.1.$$  \\n\\nThe distance reward aims to encourage the agent to move closer to the target location with each step. If the agent\u2019s current state $s^t$ is closer to the target than its previous state $s^{t-1}$, it receives a positive reward of 1. On the other hand, if the agent moves away from the target, a small penalty of -0.1 is applied. This reward type helps guide the agent towards the target and promotes efficient navigation.\\n\\nHuman Reward. The human reward is defined as follows:\\n\\n$$r_{human}^t = 0, \\\\text{if no collision with human}$$\\n$$-2, \\\\text{if collision occurs}.$$  \\n\\nThe human reward is designed to penalize the agent for colliding with humans. If the agent navigates without colliding with any humans, it receives a neutral reward of 0. However, if a collision with a human occurs, the agent incurs a significant penalty of -2. This reward type encourages the agent to navigate safely and avoid collisions, promoting socially-aware navigation behaviors.\\n\\nBy incorporating these three reward types, the VLN-DT agent is trained to balance multiple objectives: reaching the target location accurately, moving closer to the target with each step, and avoiding collisions with humans. The target reward provides a strong incentive for the agent to reach the desired destination, while the distance reward encourages efficient navigation by rewarding the agent.\"}"}
{"id": "DJVyRhT8nP", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for making progress towards the target. The human reward ensures that the agent learns to navigate in a socially-aware manner, prioritizing the safety of humans in the environment. During training, these rewards are combined to form the overall reward signal that guides the learning process of the VLN-DT agent. By optimizing its behavior based on these rewards, the agent learns to navigate in the presence of human activities, aligning with the goals of the HA-VLN task.\\n\\nAlgorithm 2\\n\\nVLN-DT Structure and Training Pseudocode (for discrete actions)\\n\\n```python\\n# R, a, t: returns, actions, or timesteps\\n# I, Theta, instructions, current observations\\n# transformer: transformer with causal masking (GPT)\\n# embed_s, embed_a, embed_R: linear embedding layers\\n# embed_t: learned episode positional embedding\\n# modality_fuse: cross modality fusion module\\n# bert_embed: bert layers\\n# cnn: Resnet152 feature extractor\\n# pred_a: linear action prediction layer\\n# main model\\ndef VLNDecisitionTransformer(R, I, Theta, a, t):\\n    # compute bert embedding and image feature map\\n    e = bert_embed(I)\\nc = cnn(Theta)\\n    # compute unified representation\\n    s = modality_fuse(e, c)\\n    # compute embeddings for tokens in transformer\\n    pos_embedding = embed_t(t) # per-timestep, not per-token\\n    s_embedding = embed_s(s) + pos_embedding\\n    a_embedding = embed_a(a) + pos_embedding\\n    R_embedding = embed_R(R) + pos_embedding\\n    # interleave tokens as (R_1, s_1, a_1, ..., R_K, s_K)\\n    input_embeds = stack(R_embedding, s_embedding, a_embedding)\\n    # use transformer to get hidden states\\n    hidden_states = transformer(input_embeds=input_embeds)\\n    # select hidden states for action prediction tokens\\n    a_hidden = unstack(hidden_states).actions\\n    # predict action\\n    return pred_a(a_hidden)\\n```\\n\\n# training loop\\n\\n```python\\nfor (R, I, Theta, t) in dataloader: # dims: (batch_size, K, dim)\\na_preds = VLNDecisionTransformer(I, Theta, a, t)\\nloss = ce(a_preds, a) # cross entropy loss for continuous actions\\noptimizer.zero_grad(); loss.backward(); optimizer.step()\\n```\\n\\n# evaluation loop\\n\\n```python\\ntarget_return = 1 # for instance, expert-level return\\nR, I, Theta, a, t, done = [target_return], [env.reset()], [], [1], False\\nwhile not done: # autoregressive generation/sampling\\n    # sample next action\\n    action = VLNDecisionTransformer(R, I, Theta, a, t)[-1]\\n    I, new_Theta, r, done, _ = env.step(action)\\n    # append new tokens to sequence\\n    R = R + [R[-1] - r] # decrement returns-to-go with reward\\n    Theta, a, t = Theta + [new_Theta], a + [action], t + [len(R)]\\n    R, Theta, a, t = R[-K:], ..., # only keep context length of K\\n```\\n\\n27\"}"}
{"id": "DJVyRhT8nP", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D Experiment Details\\n\\nD.1 Evaluation Protocol\\n\\nIn HA-VLN, we construct a fair and comprehensive assessment of the agent's performance by incorporating critical nodes in the evaluation metrics. To help better understand the new evaluation metrics defined in the main text, the original metrics before such an update are as follows:\\n\\n**Total Collision Rate (TCR).**\\n\\nThe Total Collision Rate measures the overall frequency of the agent colliding with any obstacles or areas within a specified radius. It is calculated as the average number of collisions per navigation instruction, taking into account the presence of critical nodes. The formula for TCR is given by:\\n\\n$$\\\\text{TCR} = \\\\frac{\\\\sum_{i=1}^{L} c_i}{L}$$\\n\\nwhere $c_i$ represents the number of collisions within a 1-meter radius in navigation instance $i$, and $L$ denotes the total number of navigation instances. By considering collisions in the vicinity of critical nodes, TCR provides a comprehensive assessment of the agent's ability to navigate safely in the presence of obstacles and important areas.\\n\\n**Collision Rate (CR).**\\n\\nThe Collision Rate assesses the proportion of navigation instances that experience at least one collision, taking into account the impact of critical nodes. It is calculated using the following formula:\\n\\n$$\\\\text{CR} = \\\\frac{\\\\sum_{i=1}^{L} \\\\min(c_i, 1)}{L}$$\\n\\nwhere $\\\\min(c_i, 1)$ ensures that any instance with one or more collisions is counted only once. By focusing on the occurrence of collisions rather than their frequency, CR provides a complementary perspective on the agent's navigation performance, highlighting the proportion of instructions that encounter collisions in the presence of critical nodes.\\n\\n**Navigation Error (NE).**\\n\\nThe Navigation Error measures the average distance between the agent's final position and the target location across all navigation instances, considering the influence of critical nodes. It is calculated using the following formula:\\n\\n$$\\\\text{NE} = \\\\frac{\\\\sum_{i=1}^{L} d_i}{L}$$\\n\\nwhere $d_i$ represents the distance error in navigation instance $i$. By taking into account the proximity to critical nodes when calculating the distance error, NE provides a more nuanced evaluation of the agent's navigation accuracy, penalizing deviations that occur near important areas.\\n\\n**Success Rate (SR).**\\n\\nThe Success Rate calculates the proportion of navigation instructions completed successfully without any collisions, considering the presence of critical nodes. It is determined using the following formula:\\n\\n$$\\\\text{SR} = \\\\frac{\\\\sum_{i=1}^{L} I(c_i = 0)}{L}$$\\n\\nwhere $I(c_i = 0)$ is an indicator function equal to 1 if there are no collisions in the navigation instance $i$ and 0 otherwise. By requiring the absence of collisions for a successful navigation, SR provides a stringent evaluation of the agent's ability to complete instructions safely.\\n\\nThe Total Collision Rate (TCR) and Collision Rate (CR) capture different aspects of collision avoidance, with TCR measuring the overall frequency of collisions and CR focusing on the proportion of instructions affected by collisions. The Navigation Error (NE) evaluates the agent's accuracy in reaching the target location, while the Success Rate (SR) assesses the agent's ability to complete instructions without any collisions.\\n\\nBy leveraging these metrics, researchers can gain a holistic understanding of the agent's performance in the HA-VLN task, identifying strengths and weaknesses in navigation safety, accuracy, and success. Compared to the original metrics, our updated comprehensive evaluation framework enables the development and comparison of agents that can effectively navigate in the presence of critical nodes, paving the way for more robust and reliable human-aware navigation systems. This approach also ensures that the evaluation of agents is rigorous and reflects real-world scenarios where navigating in human-populated environments presents significant challenges.\"}"}
{"id": "DJVyRhT8nP", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: Impact of Critical Nodes on Agent Navigation Performance on HA-VLN. The table compares the performance of the Airbert agent excluding the impact of critical nodes (w/o critical nodes) and including the impact of critical nodes (w/ critical nodes). The results show that ignoring critical nodes can overestimate the human perception ability of agents.\\n\\n| Env Name       | w/ critical nodes | w/o critical nodes | Difference  |\\n|----------------|-------------------|--------------------|-------------|\\n|                | TCR CR TCR CR     | TCR CR CR CR CR    |             |\\n| Validation Seen | 0.191 0.644 0.146 | 0.515              | +30.8% +25.0% |\\n| Validation Unseen | 0.281 0.764 0.257 | 0.689              | +9.3% +10.9% |\\n\\nD.2 Evaluating the Impact of Critical Nodes\\n\\nTo assess the impact of critical nodes on agent performance in the HA-VLN task, we trained the Airbert agent using a panoramic action space and sub-optimal expert supervision. Tab. 10 presents the human-aware performance difference between including the impact of critical nodes (w/ critical nodes) and excluding their impact (w/o critical nodes).\\n\\nThe results reveal that including the impact of critical nodes in the HA-VLN task leads to an underestimation of the agent's ability to navigate in realistic environments (Sim2Real ability). Specifically, when critical nodes are excluded from the evaluation, both the Total Collision Rate (TCR) and Collision Rate (CR) show considerable improvements of 30.8% and 25.0%, respectively, in the validation seen environment. This suggests that ignoring the impact of critical nodes can lead to an overestimation of the agent's human perception and navigation capabilities.\\n\\nThe observed differences in performance highlight the importance of considering critical nodes when assessing an agent's navigational efficacy in the HA-VLN task. Critical nodes represent crucial points in the navigation environment where the agent's behavior and decision-making are particularly important, such as narrow passages, doorways, or areas with high human activity. By including the impact of critical nodes, we obtain a more realistic and accurate evaluation of the agent's ability to navigate safely and efficiently in the presence of human activities.\\n\\nFurthermore, the results underscore the significance of critical nodes in bridging the gap between simulated and real-world environments (Sim2Real gap). By incorporating the impact of critical nodes during training and evaluation, we can develop agents that are better equipped to handle the challenges and complexities encountered in real-world navigation scenarios.\\n\\nIn light of these findings, we argue that excluding the impact of critical nodes leads to a fairer and more comprehensive assessment of an agent's navigational performance on the HA-VLN task. By focusing on the agent's behavior and decision-making at critical nodes, we can obtain insights into its ability to perceive and respond to human activities effectively.\\n\\nTherefore, in the experiments presented in this work, we exclude the impact of critical navigation nodes to ensure a rigorous and unbiased evaluation of the agents' performance on the HA-VLN task. This approach allows us to accurately assess the agents' capabilities in navigating dynamic, human-aware environments and provides a solid foundation for developing robust and reliable navigation systems that can operate effectively in real-world settings.\\n\\nD.3 Evaluating the Oracle Performance\\n\\nTo evaluate the performance of the oracle agents in the HA-VLN task, we conducted a comparative analysis between the sub-optimal expert and the optimal expert. Tab. 11 presents the results of this evaluation, providing insights into the strengths and limitations of each expert agent.\\n\\nThe optimal expert achieves the highest Success Rate (SR) of 100% in both seen and unseen environments, demonstrating its ability to navigate effectively and reach the target destination. However, this high performance comes at the cost of increased Total Collision Rate (TCR) and Collision Rate (CR). In the validation unseen environment, the optimal expert exhibits a staggering 800% increase in TCR and a 1700% increase in CR compared to the sub-optimal expert. These substantial increases in collision-related metrics indicate that the optimal expert prioritizes reaching the goal over avoiding collisions with humans and obstacles.\"}"}
{"id": "DJVyRhT8nP", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11: Impact of Expert Quality on Ground Truth (oracle) in HA-VLN. The table compares the performance of expert agents. The results indicate that the sub-optimal expert provides weak supervision signals for navigation by balancing NE, TCR, CR, and SR.\\n\\n| Expert Agent       | Validation Seen | Validation Unseen |\\n|--------------------|-----------------|-------------------|\\n|                    | NE  | TCR  | CR   | SR   | NE  | TCR  | CR   | SR   |\\n| Sub-optimal oracle | 0.67| 0.04 | 0.04 | 0.89 | 0.62| 0.01 | 0.01 | 0.91 |\\n| Optimal oracle     | 0.00| 0.14 | 0.22 | 1.00 | 0.00| 0.09 | 0.18 | 1.00 |\\n| Difference         | -0.67| +0.10| +0.18| +0.11| -0.62| +0.08| +0.17| +0.09 |\\n| Percentage Change  | -100.0% | +250.0% | +450.0% | +12.4% | -100.0% | +800.0% | +1700.0% | +9.9% |\\n\\nOn the other hand, the sub-optimal expert provides a more balanced approach to navigation. Although its SR is slightly lower than the optimal expert by 11.0% in seen environments and 9.9% in unseen environments, the sub-optimal expert achieves significantly lower TCR and CR. This suggests that the sub-optimal expert strikes a better balance between navigation efficiency and human-aware metrics, making it more suitable for real-world applications.\\n\\nThe sub-optimal expert's performance can be attributed to its ability to navigate while considering the presence of humans and obstacles in the environment. By prioritizing collision avoidance and maintaining a safe distance from humans, the sub-optimal expert provides a more practical approach to navigation in dynamic, human-populated environments. This is particularly important in real-world scenarios where the safety and comfort of humans are paramount.\\n\\nMoreover, the sub-optimal expert's balanced performance across navigation-related and human-aware metrics makes it an ideal candidate for providing weak supervision signals during the training of navigation agents. By learning from the sub-optimal expert's demonstrations, navigation agents can acquire the necessary skills to navigate efficiently while being mindful of human presence and potential collisions.\\n\\nThe oracle performance analysis highlights the importance of considering both navigation efficiency and human-aware metrics when evaluating expert agents and training navigation agents. While the optimal expert excels in reaching the target destination, its high collision rates limit its practicality in real-world scenarios. The sub-optimal expert, on the other hand, provides a more balanced approach, achieving reasonable success rates while minimizing collisions with humans and obstacles. By incorporating the sub-optimal expert's demonstrations during training, navigation agents can learn to navigate effectively and safely in complex, human-populated environments, bridging the gap between simulation and real-world applications (i.e., Sim2Real Challenges).\\n\\nD.4 Evaluating on Real-World Robots\\n\\nRobot Setup. To validate the performance of our navigation agents in real-world scenarios, we conducted experiments using a Unitree GO1-EDU quadruped robot. Fig. 15 provides a detailed visual representation of the robot and its key components. The robot is equipped with a stereo fisheye camera mounted on its head, which captures RGB images with a 180-degree field of view. To align with the agent's Ergonomic Action Space setup, we cropped the central 60 degrees of the camera's field of view and used it as the agent's visual input. It is important to note that our approach only utilizes monocular images from the fisheye camera.\\n\\nIn addition to the camera, the robot is equipped with an ultrasonic distance sensor located beneath the fisheye camera. This sensor measures the distance between the robot and humans, enabling the calculation of potential collisions. An Inertial Measurement Unit (IMU) is also integrated into the robot to capture its position and orientation during navigation.\\n\\nTo deploy our navigation agents, the robot is equipped with an NVIDIA Jetson TX2 AI computing device. This high-performance computing module handles the computational tasks required by the agent, such as receiving images and inferring the next action command. The agent's action commands are then executed by the Motion Control Unit, which is implemented using a Raspberry Pi 4B. This unit sets the robot in a high-level motion mode, allowing it to directly execute movement commands such as \u201cturn left\u201d or \u201cmove forward.\u201d The minimum movement distance is set to 0.5m, and the turn angle is set to 45 degrees. Throughout the robot's movements, the IMU continuously tracks the motion to ensure that the rotations and forward movements align with the issued commands.\"}"}
{"id": "DJVyRhT8nP", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Stereo Fisheye Camera\\nUltrasonic Distance Sensor\\nUnitree GO1-EDU\\nNVIDIA Jetson TX2\\nInertial Measurement Unit\\n\\nFigure 15: Real-world robot used in our experiments. The robot is Unitree GO1-EDU, a quadruped robot equipped with an NVIDIA Jetson TX2 high-performance computing module for handling computational tasks. The robot features an Inertial Measurement Unit (IMU) for measuring acceleration and rotational speed, a Stereo Fisheye Camera for wide-angle perception of its surroundings, and an Ultrasonic Distance Sensor for measuring the distance between the robot and obstacles.\\n\\nVisual Results of Demonstration. To showcase the real-world performance of our navigation agents, we provide visual results of the robot navigating in various office environments. Fig. 16 demonstrates the robot successfully navigating an office environment without human presence. The figure presents the instruction given to the robot, the robot's view captured by the fisheye camera, and a third-person view of the robot's navigation.\\n\\nIn Fig. 17, we present an example of the robot navigating in an office environment with human activity. The robot observes humans in its surroundings, adjusts its path accordingly, circumvents the humans, and ultimately reaches its designated destination. This showcases the robot's ability to perceive and respond to human presence while navigating.\\n\\nHowever, it is important to acknowledge that the robot's performance is not infallible. Fig. 18 illustrates a scenario where the robot collides with a human, even in the same environment. This collision occurs when the human's status changes unexpectedly, leading to a mission failure. This example highlights the challenges and limitations of real-world navigation in dynamic human environments. To provide a more view of the robot's navigation capabilities, we have made the complete robot navigation video available on our project website. This video showcases various scenarios and provides a deeper understanding of the robot's performance in real-world settings.\"}"}
{"id": "DJVyRhT8nP", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Turn around and walk forward to the hallway. Be aware that a man is looking at his phone in the hallway.\\n\\nTurn left and continue straight to the water dispenser at the end of the hallway, then turn left and stop in front of the first workstation.\\n\\nFigure 18: Example of robot navigation failures in real environments with human presence. The robot collides with a human when their status changes unexpectedly, leading to a mission failure. [Zoom in to view]\"}"}
{"id": "DJVyRhT8nP", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions\\n\\nHeng Li, Minghan Li, Zhi-Qi Cheng, Yifei Dong, Yuxuan Zhou, Jun-Yan He, Qi Dai, Teruko Mitamura, Alexander G. Hauptmann\\n\\n1 Carnegie Mellon University 2 Columbia University 3 University of Mannheim 4 Alibaba Group 5 Microsoft Research\\n\\nProject Page: https://lpercc.github.io/HA3D_simulator/\\n\\nAbstract\\nVision-and-Language Navigation (VLN) aims to develop embodied agents that navigate based on human instructions. However, current VLN frameworks often rely on static environments and optimal expert supervision, limiting their real-world applicability. To address this, we introduce Human-Aware Vision-and-Language Navigation (HA-VLN), extending traditional VLN by incorporating dynamic human activities and relaxing key assumptions. We propose the Human-Aware 3D (HA3D) simulator, which combines dynamic human activities with the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R) dataset, extending R2R with human activity descriptions. To tackle HA-VLN challenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and Non-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing cross-modal fusion and diverse training strategies for effective navigation in dynamic human environments. A comprehensive evaluation, including metrics considering human activities, and systematic analysis of HA-VLN's unique challenges, underscores the need for further research to enhance HA-VLN agents' real-world robustness and adaptability. Ultimately, this work provides benchmarks and insights for future research on embodied AI and Sim2Real transfer, paving the way for more realistic and applicable VLN systems in human-populated environments.\\n\\n1 Introduction\\n\\nThe dream of autonomous robots carrying out assistive tasks, long portrayed in \\\"The Simpsons,\\\" is becoming a reality through embodied AI, which enables agents to learn by interacting with their environment [43]. However, effective Sim2Real transfer remains a critical challenge [3, 53]. Vision-and-Language Navigation (VLN) [2, 7, 9, 40] has emerged as a key benchmark for evaluating Sim2Real transfer [23], showing impressive performance in simulation [9, 21, 38]. Nevertheless, many VLN frameworks [2, 12, 21, 44, 46, 52] rely on simplifying assumptions, such as static environments [25, 39, 50], panoramic action spaces, and optimal expert supervision, limiting their real-world applicability and often leading to an overestimation of Sim2Real capabilities [51].\\n\\nTo bridge this gap, we propose Human-Aware Vision-and-Language Navigation (HA-VLN), extending traditional VLN by incorporating dynamic human activities and relaxing key assumptions. HA-VLN advances previous frameworks by (1) adopting a limited 60\u00b0 field-of-view egocentric action space, (2) integrating dynamic environments with 3D human motion models encoded using the SMPL model [31], and (3) learning to navigate considering dynamic environments from suboptimal expert supervision.\"}"}
{"id": "DJVyRhT8nP", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: HA-VLN Scenario: The agent navigates through environments populated with dynamic human activities. The task involves optimizing routes while maintaining safe distances from humans to address the Sim2Real gap. In this scenario, the agent encounters various human activities, such as someone talking on the phone while pacing in the hallway, someone taking off their shoes in the entryway/foyer, and someone carrying groceries upstairs. The HA-VLN agent must adapt its path by waiting for humans to move, adjusting its path, or proceeding through when clear, thereby enhancing real-world applicability.\\n\\ndemonstrations through an adaptive policy (Fig. 6). This setup creates a more realistic and challenging scenario, enabling agents to navigate in human-populated environments while maintaining safe distances, narrowing the gap between simulation and real-world scenes.\\n\\nTo support HA-VLN research, we introduce the Human-Aware 3D (HA3D) simulator, a realistic environment combining dynamic human activities with the Matterport3D dataset [6]. HA3D utilizes the self-collected Human Activity and Pose Simulation (HAPS) dataset, which includes 145 human activity descriptions converted into 435 detailed 3D human motion models using the SMPL model [31] (Sec. 2.1). The simulator provides an interactive annotation tool for placing human models in 29 different indoor areas across 90 building scenes (Fig. 12). Moreover, we introduce the Human-Aware Room-to-Room (HA-R2R) dataset, an extension of the Room-to-Room (R2R) dataset [2] incorporating human activity descriptions. HA-R2R includes 21,567 instructions with an expanded vocabulary and activity coverage compared to R2R (Fig. 3 and Sec. 2.2).\\n\\nBuilding upon the HA-VLN task and the HA3D simulator, we propose two multimodal agents to address the challenges posed by dynamic human environments: the Expert-Supervised Cross-Modal (VLN-CM) agent and the Non-Expert-Supervised Decision Transformer (VLN-DT) agent. The innovation of these agents lies in their cross-modal fusion module, which dynamically weights language and visual information, enhancing their understanding and utilization of different modalities. VLN-CM learns by imitating expert demonstrations (Sec. 2.2), while VLN-DT demonstrates the potential to learn solely from random trajectories without expert supervision (Fig. 4, right). We also design a rich reward function to incentivize agents to navigate effectively (Fig. 5).\\n\\nTo comprehensively evaluate the performance of the HA-VLN task, we design new metrics considering human activities, and highlight the unique challenges faced by HA-VLN (Sec. 3.2). Evaluating state-of-the-art VLN agents on the HA-VLN task reveals a significant performance gap compared to the Oracle, even after retraining, thereby underscoring the complexity of navigating in dynamic human environments (Sec. 3.3). Moreover, experiments show that VLN-DT, trained solely on random data, achieves performance comparable to VLN-CM under expert supervision, thus demonstrating its superior generalization ability (Sec. 3.4). Finally, we validate the agents in the real world using a quadruped robot, exhibiting perception and avoidance capabilities, while also emphasizing the necessity of further improving real-world robustness and adaptability (Sec. 3.5).\\n\\nOur main contributions are as follows: (1) Introducing HA-VLN, a new task that extends VLN by incorporating dynamic human activities and relaxing assumptions; (2) Offering HA3D, a realistic simulator, and HA-R2R, an extension of the R2R dataset, to support HA-VLN research and enable the development of robust navigation agents; (3) Proposing VLN-CM and VLN-DT agents that utilize expert and non-expert supervised learning to address the challenges of HA-VLN, showcasing the effectiveness of cross-modal fusion and diverse training strategies; and (4) Designing comprehensive evaluations for HA-VLN, providing benchmarks and insights for future research.\"}"}
{"id": "DJVyRhT8nP", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We introduce Human-Aware Vision-and-Language Navigation (HA-VLN), an extension of traditional Vision-and-Language Navigation (VLN) that bridges the Sim2Real gap [3, 23, 53] between simulated and real-world navigation scenarios. As shown in Fig. 1, HA-VLN involves an embodied agent navigating from an initial position to a target location within a dynamic environment, guided by natural language instructions $I = \\\\langle w_1, w_2, \\\\ldots, w_L \\\\rangle$, where $L$ denotes the total number of words and $w_i$ represents an individual word. At the beginning of each episode, the agent assesses its initial state $s_0 = \\\\langle p_0, \\\\phi_0, \\\\lambda_0, \\\\Theta_0 \\\\rangle$ within a $\\\\Delta t = 2$ seconds observation window, where $p_0 = \\\\langle x_0, y_0, z_0 \\\\rangle$ represents the initial 3D position, $\\\\phi_0$ the heading, $\\\\lambda_0$ the elevation, and $\\\\Theta_0$ the egocentric view within a 60-degree field of view. The agent executes a sequence of actions $A_T = \\\\langle a_0, a_1, \\\\ldots, a_T \\\\rangle$, resulting in states and observations $S_T = \\\\langle s_0, s_1, \\\\ldots, s_T \\\\rangle$, where each action $a_t \\\\in A = \\\\langle a_{\\\\text{forward}}, a_{\\\\text{left}}, a_{\\\\text{right}}, a_{\\\\text{up}}, a_{\\\\text{down}}, a_{\\\\text{stop}} \\\\rangle$ leads to a new state $s_{t+1} = \\\\langle p_{t+1}, \\\\phi_{t+1}, \\\\lambda_{t+1}, \\\\Theta_{t+1} \\\\rangle$. The episode concludes with the stop action $a_{\\\\text{stop}}$.\\n\\nIn contrast to traditional VLN tasks [2, 13, 26, 40, 45], HA-VLN addresses the Sim2Real gap [3, 23, 53] by relaxing three key assumptions, as depicted in Fig. 1:\\n\\n1. **Egocentric Action Space:** HA-VLN employs an egocentric action space $A$ with a limited 60\u00b0 field of view $\\\\Theta_{60}$, requiring the agent to make decisions based on human-like visual perception. The state $s_t = \\\\langle p_t, \\\\phi_t, \\\\lambda_t, \\\\Theta_{60} \\\\rangle$ captures the agent's egocentric perspective at time $t$, enabling effective navigation in real-world scenarios.\\n\\n2. **Dynamic Environments:** HA-VLN introduces dynamic environments based on 3D human motion models $H = \\\\langle h_1, h_2, \\\\ldots, h_N \\\\rangle$, where each frame $h_i \\\\in \\\\mathbb{R}^{6890 \\\\times 3}$ encodes human positions and shapes using the Skinned Multi-Person Linear (SMPL) model [31]. The agent must perceive and respond to these activities in real-time while maintaining a safe distance $d_{\\\\text{safe}}$, reflecting real-world navigation challenges.\\n\\n3. **Sub-optimal Expert Supervision:** In HA-VLN, agents learn from sub-optimal expert demonstrations that provide navigation guidance accounting for the dynamic environment. The agent\u2019s policy $\\\\pi_{\\\\text{adaptive}}(a_t | s_t, I, H)$ aims to maximize the expected reward $E[r(s_{t+1})]$, considering human interactions and safe navigation. The reward function $r: S \\\\rightarrow \\\\mathbb{R}$ assesses the quality of navigation at each state, allowing better handling of imperfect instructions in real-world tasks.\\n\\nBuilding upon these relaxed assumptions, a key feature of HA-VLN is the inclusion of human activities captured at 16 FPS. When human activities fall within the agent\u2019s field of view $\\\\Theta_{60}$, the agent is considered to be interacting with humans. HA-VLN introduces the Adaptive Response Strategy, where the agent detects and responds to human movements, anticipating trajectories and making real-time path adjustments. Formally, this strategy is defined as:\\n\\n$$\\\\pi_{\\\\text{adaptive}}(a_t | s_t, I, H) = \\\\arg \\\\max_{a_t \\\\in A} P(a_t | s_t, I) \\\\cdot E[r(s_{t+1})],$$\\n\\nwhere $E[r(s_{t+1})]$ represents the expected reward considering human interactions and safe navigation.\\n\\nTo support the agent in learning, the HA3D simulator (Sec. 2.1) provides interfaces to access human posture, position, and trajectories, while HA-VLN employs sub-optimal expert supervision (Sec. 2.2) to provide weak signals, reflecting real-world scenarios with imperfect demonstration.\\n\\n### 2.1 HA3D Simulator: Integrating Dynamic Human Activities\\n\\nThe Human-Aware 3D (HA3D) Simulator generates dynamic environments by integrating natural human activities from the custom-collected Human Activity and Pose Simulation (HAPS) dataset with the photorealistic environments of the Matterport3D dataset [6] (see Fig. 2 and Fig. 12).\\n\\n**HAPS Dataset.**\\n\\nHAPS addresses the limitations of existing human motion datasets by identifying 29 distinct indoor regions across 90 architectural scenes and generating 145 human activity descriptions. These descriptions, validated through human surveys and quality control using GPT-4 [5], encompass realistic actions such as walking, sitting, and using a laptop. The Motion Diffusion Model (MDM) [17] converts these descriptions into 435 detailed 3D human motion models $H$ using the SMPL model, with each description transformed into three distinct 120-frame motion sequences. The dataset also represents 435 models, 120 frames each, with shape, pose, and mesh vertex parameters. See Realistic Human Rendering for more details.\"}"}
{"id": "DJVyRhT8nP", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Human-Aware 3D (HA3D) Simulator Annotation Process: HA3D integrates dynamic human activities from the Human Activity and Pose Simulation (HAPS) dataset into the photorealistic environments of Matterport3D. The annotation process involves: (1) integrating the HAPS dataset, which includes 145 human activity descriptions converted into 435 detailed 3D human motion models in 52,200 frames; (2) annotating human activities within various indoor regions across 90 building scenes using an interactive annotation tool; (3) rendering realistic human models; and (4) enabling interactive agent-environment interactions.\\n\\nIncludes annotations of human-object interactions and the relationship between human activities and architectural layouts. After manual selection, approximately 422 models were retained. Further details on the dataset are provided in App. B.1.\\n\\nHuman Activity Annotation. An interactive annotation tool accurately locates humans in different building regions (see Fig. 12). Users explore buildings, select viewpoints $p_i = (x_i, y_i, z_i)$, set initial human positions, and choose 3D human motion models $H_i$ based on the environment of $p_i$. To follow real-world scenarios, multiple initial human viewpoints $P_{random} = \\\\{p_1, p_2, \\\\ldots, p_k\\\\}$ are randomly selected from a subset of all viewpoints in the building. The number of people in each building is estimated by dividing the building area by the average area per capita in the U.S. (2021, 67 m$^2$) [35] and rounding up. In the Matterport3D dataset, these viewpoints are manually annotated to facilitate the transfer from other VLN tasks to HA-VLN. This setup ensures agents can navigate environments with dynamic human activities updated at 16 FPS, allowing real-time perception and response. Detailed statistics of activity annotation are in App. B.2.\\n\\nRealistic Human Rendering. HA3D employs Pyrender to render dynamic human bodies with high visual realism. The rendering process aligns camera settings with the agent's perspective and integrates dynamic human motion using a 120-frame SMPL mesh sequence $H = \\\\langle h_1, h_2, \\\\ldots, h_{120} \\\\rangle$. Each frame $h_t = (\\\\beta_t, \\\\theta_t, \\\\gamma_t)$ consists of shape parameters $\\\\beta_t \\\\in \\\\mathbb{R}^{10}$, pose parameters $\\\\theta_t \\\\in \\\\mathbb{R}^{72}$, and mesh vertices $\\\\gamma_t \\\\in \\\\mathbb{R}^{6890 \\\\times 3}$ calculated based on $\\\\beta_t$ and $\\\\theta_t$ through the SMPL model. At each time step, the 3D mesh $h_t$ is dynamically generated, with vertices $\\\\gamma_t$ algorithmically determined to form the human model accurately. These vertices are then used to generate depth maps $D_t$, distinguishing human models from other scene elements. HA3D allows real-time adjustments of human body parameters, enabling the representation of diverse appearances and enhancing interactivity. More details on the rendering pipeline and examples of rendered human models are in App. B.3.\\n\\nAgent-Environment Interaction. Compatible with the Matterport3D simulator's configurations [2], HA3D provides agents with environmental feedback signals at each time step $t$, including first-person RGB-D video observation $\\\\Theta_{60}^t$, navigable viewpoints, and a human \\\"collision\\\" feedback signal $c_t$. The agent receives its state $s_t = p_t, \\\\phi_t, \\\\lambda_t, \\\\Theta_{60}^t$, where $p_t = (x_t, y_t, z_t)$, $\\\\phi_t$, and $\\\\lambda_t$ denote position, heading, and elevation, respectively. The agent's policy $\\\\pi_{adaptive}(a_t | s_t, I, H)$ maximizes expected reward $E[r(s_{t+1})]$ by considering human interactions for safe navigation. The collision feedback signal $c_t$ is triggered when the agent-human distance $d_{a,h}(t)$ falls below a threshold $d_{threshold}$. Customizable collision detection and feedback parameters enhance agent-environment interaction. Details on visual feedback, optimization, and extended interaction capabilities are in App. B.4.\"}"}
{"id": "DJVyRhT8nP", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implementation and Performance.\\n\\nDeveloped using C++/Python, OpenGL, and Pyrender, HA3D integrates with deep learning frameworks like PyTorch and TensorFlow. It offers flexible configuration options, achieving up to 300 fps on an NVIDIA RTX 3050 GPU with 640x480 resolution. Running on Linux, the simulator has a low memory usage of 40MB and supports multi-processing for parallel execution of simulation tasks. Its modular architecture enables easy extension and customization. The simulator supports various rendering techniques, enhancing visual realism. It provides high-level APIs for real-time data streaming and interaction with external controllers. PyQt5-based annotation tools with an intuitive interface will be made available to researchers. Additional details on the simulator's implementation, performance, and extensibility are provided in App. B.5.\\n\\n2.2 Human-Aware Navigation Agents\\n\\nWe introduce the Human-Aware Room-to-Room (HA-R2R) dataset, extending the Room-to-Room (R2R) dataset [2] by incorporating human activity descriptions to create a more realistic and dynamic navigation environment. To address HA-VLN challenges, we propose two agents: the expert-supervised Cross Modal (VLN-CM) agent and the non-expert-supervised Decision Transformer (VLN-DT) agent. An Oracle agent provides ground truth supervision for training and benchmarking.\\n\\n**HA-R2R Dataset.** HA-R2R extends R2R dataset by incorporating human activity annotations while preserving its fine-grained navigation properties. The dataset was constructed in two steps: 1) mapping R2R paths to the HA3D simulator, manually annotating human activities at key locations; and 2) using GPT-4 [1] to generate new instructions by combining original instructions, human activity descriptions, and relative position information, followed by human validation. The resulting dataset contains 21,567 human-like instructions with 145 activity types, categorized as start (1,047), obstacle (3,666), surrounding (14,469), and end (1,041) based on their positions relative to the agent's starting point (see App. C.1 for details). Compared to R2R, HA-R2R's average instruction length increased from 29 to 69 words, with the vocabulary expanding from 990 to 4,500. Fig. 3A shows the instruction length distribution by activity count, while Fig. 3B compares HA-R2R and R2R distributions. Fig. 3C summarizes viewpoints affected by human activities, and Fig. 14 illustrates the instruction quality by analyzing common word frequencies. More details are provided in App. C.1.\\n\\n**Oracle Agent: Ground Truth Supervision.** The Oracle agent serves as the ground truth supervision source to guide and benchmark the training of expert-supervised and non-expert-supervised agents in the HA-VLN system. Designed as a teacher, the Oracle provides realistic supervision derived from the HA-R2R dataset, strictly following language instructions while dynamically avoiding human activities along navigation paths to ensure maximal expected rewards. Let $G = (N, E)$ be the global navigation graph, with nodes $N$ (locations) and edges $E$ (paths). When human activities affect nodes $n \\\\in N$ within radius $r$, those nodes form subset $N_h$. The Oracle's policy $\\\\pi^*$ adaptive re-routes on the modified graph $G' = (N \\\\setminus N_h, E')$, where $E'$ only includes edges avoiding $N_h$, ensuring the Oracle avoids human-induced disturbances while following navigation instructions optimally. Algorithm 1 details the Oracle's path planning and collision avoidance strategies. During training, at step $t$, a cross-entropy loss maximizes the likelihood of true target action $a^*_t$ given the previous state-action sequence $\\\\langle s_0, a_0, s_1, a_1, \\\\ldots, s_t \\\\rangle$. The target output $a^*_t$ is defined as the Oracle's next action from the current location to the goal. Please refer to App. C.2 for more details.\"}"}
{"id": "DJVyRhT8nP", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Model Architectures of Navigation Agents: The architectures of the Vision-Language Navigation Cross-Modal (VLN-CM) agent (left) and the Vision-Language Navigation Decision Transformer (VLN-DT) agent (right). Both agents employ a cross-modality fusion module to effectively integrate visual and linguistic information for predicting navigation actions. VLN-CM utilizes an LSTM-based sequence-to-sequence model for expert-supervised learning, while VLN-DT leverages an autoregressive transformer model to learn from random trajectories without expert supervision.\\n\\nVLN-CM: Multimodal Integration for Supervised Learning. We propose the Vision-Language Navigation Cross-Modal (VLN-CM) agent, an LSTM-based sequence-to-sequence model augmented with a cross modality fusion module for effective multimodal integration (Fig. 4, left). The language instruction \\\\( I = [w_1, w_2, \\\\cdots, w_L] \\\\), where \\\\( w_i \\\\) denotes the \\\\( i \\\\)-th word, is encoded into BERT embeddings \\\\( \\\\{e_1, e_2, \\\\cdots, e_L\\\\} \\\\), which are processed by an LSTM to yield context-aware representations \\\\( \\\\{u_1, u_2, \\\\cdots, u_L\\\\} \\\\). Simultaneously, visual observations \\\\( \\\\Theta_t \\\\) at each timestep \\\\( t \\\\) are encoded using ResNet-152 \\\\( [19] \\\\), producing an image feature map \\\\( \\\\{c_1, c_2, \\\\cdots, c_N\\\\} \\\\), where \\\\( N \\\\) is the number of visual features. The fusion module integrates the context encoder outputs and image features via cross-attention, generating a unified representation \\\\( m_t \\\\) at each timestep \\\\( t \\\\). An LSTM-based action decoder predicts the next action \\\\( a_{t+1} \\\\) from the action space \\\\( A = \\\\{\\\\text{forward}, \\\\text{left}, \\\\text{right}, \\\\text{up}, \\\\text{down}, \\\\text{stop}\\\\} \\\\) conditioned on \\\\( m_t \\\\) and the previous action \\\\( a_t \\\\). The agent is trained via supervised learning from an expert Oracle agent using cross-entropy loss:\\n\\n\\\\[\\nL_{CE} = \\\\sum_{a \\\\in A} y_t(a) \\\\log p(a_t|I, \\\\Theta_t),\\n\\\\]\\n\\nwhere \\\\( L_{CE} \\\\) is the cross-entropy loss, \\\\( y_t(a) \\\\) is the ground truth action distribution from the expert trajectory at timestep \\\\( t \\\\), and \\\\( p(a_t|I, \\\\Theta_t) \\\\) is the predicted action distribution given instruction \\\\( I \\\\) and observation \\\\( \\\\Theta_t \\\\) at timestep \\\\( t \\\\).\\n\\nVLN-DT: Reinforcement Learning with Decision Transformers. We present the Vision-Language Navigation Decision Transformer (VLN-DT), an autoregressive transformer \\\\( [8, 41] \\\\) with a cross-modality fusion module for navigation without expert supervision (Fig. 4, right). VLN-DT learns from sequence representations \\\\( \\\\tau = (\\\\hat{G}_1, s_1, a_1, \\\\cdots, \\\\hat{G}_t, s_t) \\\\) to predict the next action \\\\( a_t \\\\in A \\\\), where \\\\( s_t \\\\) is the state at timestep \\\\( t \\\\), and \\\\( \\\\hat{G}_t=P_{t'=t}r_{t'} \\\\) is the Return to Go. The cross-modality fusion module computes \\\\( s_t \\\\) by processing the average pooling vector of the BERT embedding \\\\( [11] \\\\) for a language instruction \\\\( I \\\\) (excluding the \\\\([CLS]\\\\) token) and the image feature map of the current observation \\\\( \\\\Theta_t \\\\), extracted using a pre-trained ResNet-152 \\\\( [19] \\\\). The fusion module dynamically weights the language and visual modalities using an attention mechanism, enhancing \\\\( s_t \\\\). The fused representations are then fed into the causal transformer, which models \\\\( \\\\tau \\\\) autoregressively to determine \\\\( a_t \\\\). We train VLN-DT using 10^4 random walk trajectories, each with a maximum length of 30 steps, a context window size of 15 steps, and an initial Return To Go of 5 to guide the agent's exploration-exploitation balance \\\\( [8] \\\\). Three reward types are designed to incentivize effective navigation: target reward (based on distance to the target), distance reward (based on movement towards the target), and human reward (based on collisions with humans) \\\\( [2, 26, 40] \\\\). Fig. 5 shows the impact of different reward strategies on navigation performance. The loss function \\\\( L_{CE} \\\\) for training VLN-DT is a supervised learning objective with cross-entropy loss:\\n\\n\\\\[\\nL_{CE} = \\\\sum_{a \\\\in A} y^*_t(a) \\\\log p(a_t|s_t),\\n\\\\]\\n\\nWithout Expert Supervision\u201d means training with random trajectories instead of expert ones.\"}"}
{"id": "DJVyRhT8nP", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Egocentric vs. Panoramic Action Space Comparison\\n\\n| Action Space | Validation Seen | Validation Unseen |\\n|--------------|----------------|------------------|\\n| NE \u2193 TCR \u2193 CR \u2193 SR \u2191 NE \u2193 TCR \u2193 CR \u2193 SR \u2191 | | |\\n| Egocentric | 7.21 0.69 1.00 0.20 | 8.09 0.54 0.58 0.16 |\\n| Panoramic | 5.58 0.24 0.80 0.34 | 7.16 0.25 0.57 0.23 |\\n| Difference | -2.63 -0.45 -0.20 +0.14 | -0.93 -0.29 -0.01 +0.07 |\\n| Percentage | -22.6% -9.2% -20.0% +70.0% | -11.5% -53.7% -1.7% +43.8% |\\n\\nTable 2: Optimal vs. Sub-Optimal Expert Comparison\\n\\n| Expert Type | Validation Seen | Validation Unseen |\\n|-------------|----------------|------------------|\\n| NE \u2193 TCR \u2193 CR \u2193 SR \u2191 NE \u2193 TCR \u2193 CR \u2193 SR \u2191 | | |\\n| Optimal | 3.61 0.15 0.52 0.53 | 5.43 0.26 0.69 0.41 |\\n| Sub-optimal | 3.98 0.18 0.63 0.50 | 5.24 0.24 0.67 0.40 |\\n| Difference | +0.37 +0.03 +0.11 -0.03 | -0.19 -0.02 -0.02 -0.01 |\\n| Percentage | +10.2% +20.0% +21.2% -5.7% | -3.5% -7.7% -2.9% -2.4% |\\n\\nwhere \\\\( y^* t(a) \\\\) is the ground truth action distribution from the random trajectory at timestep \\\\( t \\\\), and \\\\( p(a t|s t) \\\\) is the predicted action distribution given instruction \\\\( I \\\\) and observation \\\\( \\\\Theta t \\\\) at timestep \\\\( t \\\\). The implementation of VLN-DT is summarized in App. C.3.\\n\\n3 Experiments\\n\\nWe evaluated our Human-Aware Vision-and-Language Navigation (HA-VLN) task, focusing on human perception and navigation. Experiments included assessing different assumptions (Sec. 3.2), comparing with state-of-the-art (SOTA) VLN agents (Sec. 3.3), analyzing our agents' performance (Sec. 3.4), and validating with real-world quadruped robot tests (Sec. 3.5).\\n\\n3.1 Evaluation Protocol for HA-VLN Task\\n\\nWe propose a two-fold evaluation protocol for the HA-VLN task, focusing on both human perception and navigation aspects. The human perception metrics evaluate the agent's ability to perceive and respond to human activities, while the navigation-related metrics assess navigation performance. As human activities near critical nodes greatly influence navigation, we introduce a strategy to handle dynamic human activities for more accurate evaluation.\\n\\nLet \\\\( A_{ci} \\\\) be the set of human activities at critical nodes in navigation instance \\\\( i \\\\). The updated human perception metrics are:\\n\\n\\\\[\\nTCR = P_{L \\\\cdot i} (c_{i} - |A_{ci}|) L ,\\n\\\\]\\n\\n\\\\[\\nCR = P_{L \\\\cdot i} \\\\min (c_{i} - |A_{ci}|, 1) \\\\beta L ,\\n\\\\]\\n\\nwhere \\\\( TCR \\\\) reflects the overall frequency of the agent colliding with human-occupied areas within a 1-meter radius, \\\\( CR \\\\) is the ratio of navigation instances with at least one collision, and \\\\( \\\\beta \\\\) denotes the ratio of instructions affected by human activities. The updated navigation metrics are:\\n\\n\\\\[\\nNE = P_{L \\\\cdot i} d_i L ,\\n\\\\]\\n\\n\\\\[\\nSR = P_{L \\\\cdot i} (c_{i} - |A_{ci}| = 0) L ,\\n\\\\]\\n\\nwhere \\\\( NE \\\\) is the distance between the agent's final position and the target location, and \\\\( SR \\\\) is the proportion of navigation instructions successfully completed without collisions and within a predefined navigation range. Please refer to App. D.1 for more details.\\n\\n3.2 Evaluating HA-VLN Assumptions\\n\\nWe assessed the impact of relaxing traditional assumptions on navigation performance by comparing HA-VLN and VLN task, relaxing each assumption individually. Panoramic vs. Egocentric Action Space (Tab. 1): Shifting from a panoramic to an egocentric action space significantly degrades overall performance, with Success Rate (SR) dropping by 70.0% in seen environments and by 43.8% in unseen environments. Additionally, there is a marked increase in\\n\\n3 We report performance of SOTA agents in traditional VLN \u2013 Room-to-Room (R2R) [2] for comparison.\\n\\n4 Node \\\\( n_i \\\\) is critical if \\\\( U(n_i) \\\\cap U(n_i+1) = \\\\emptyset \\\\), where \\\\( U(n_i) \\\\) is the set of nodes reachable from \\\\( n_i \\\\).\\n\\n5 Ignoring activities at critical nodes decreases \\\\( CR \\\\) and \\\\( TCR \\\\), while increasing \\\\( SR \\\\).\\n\\n7\"}"}
{"id": "DJVyRhT8nP", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Performance of SOTA VLN Agents on HA-VLN (Retrained)\\n\\n| Method               | Validation Seen | Validation Unseen | Difference |\\n|----------------------|-----------------|-------------------|------------|\\n|                      | w/o human       | w/ human          |            |\\n| NE                   | \u2193 SR            | \u2193 SR              |            |\\n| Speaker-Follower     | 6.62 0.35       | 7.12 0.24         | +7.6%      |\\n|                      |                 |                   | -31.4%     |\\n| Rec (PREV ALENT)     | 3.93 0.63       | 6.93 0.26         | +76.3%     |\\n|                      |                 |                   | -58.7%     |\\n| Rec (OSCAR)          | 4.29 0.59       | 7.45 0.23         | +73.4%     |\\n|                      |                 |                   | -61.0%     |\\n| Airbert              | 4.01 0.62       | 6.27 0.30         | +56.4%     |\\n|                      |                 |                   | -51.6%     |\\n\\nTable 6: Comparison of SOTA VLN Agents and Oracle (Ground-truth) on HA-VLN\\n\\n| Method     | Validation Seen | Validation Unseen | Difference |\\n|------------|-----------------|-------------------|------------|\\n|            | w/o human       | w/ human          |            |\\n| NE         | \u2193 TCR           | \u2193 CR              |            |\\n| Speaker-Follower | 0.24 0.87     | 0.25 0.63         |            |\\n| Rec (Prevalent) | 0.21 0.75      | 0.24 0.70         |            |\\n| Rec (OSCAR) | 0.18 0.75       | 0.23 0.69         |            |\\n| Airbert    | 0.18 0.68       | 0.24 0.74         |            |\\n\\nBoth Navigation Error (NE) and Target Collision Rate (TCR), underscoring the critical importance of panoramic action spaces for effective and reliable navigation in complex, dynamically human-populated environments.\\n\\nTable 3: Static vs. Dynamic Environment Comparison\\n\\n| Environment Type | Validation Seen | Validation Unseen | Difference |\\n|------------------|-----------------|-------------------|------------|\\n| Static           | 2.68 0.75       | 4.01 0.62         |            |\\n| Dynamic          | 5.24 0.40       | 3.98 0.50         |            |\\n| Difference       | +2.56           | -0.35             | -0.03      |\\n| Percentage       | +95.5%           | -46.7%             | -0.7%      |\\n\\nStatic vs. Dynamic Environment (Tab. 3): Introducing dynamic human motion into the environment reduces SR by 46.7% in seen environments and by 19.4% in unseen settings, presenting a substantial obstacle to reliable and effective navigation while highlighting the challenges inherent in human-aware task performance.\\n\\nOptimal vs. Sub-optimal Expert (Tab. 2): Training with a sub-optimal expert marginally increases NE by 10.2% and reduces SR by 5.7% in seen environments. Although slightly lower in accuracy, sub-optimal expert guidance introduces greater realism to the agent's training, offering navigation experiences more aligned with real-world variability and thus contributing to improved robustness in human-aware metrics.\\n\\nTable 7: Comparison of SOTA Agents on Traditional VLN vs. HA-VLN (Zero-shot)\\n\\n| Method               | Validation Seen | Validation Unseen | Difference |\\n|----------------------|-----------------|-------------------|------------|\\n|                      | w/o human       | w/ human          |            |\\n| NE                   | \u2193 SR            | \u2193 SR              |            |\\n| Speaker-Follower     | 6.62 0.35       | 7.12 0.24         | +7.6%      |\\n|                      |                 |                   | -31.4%     |\\n| Rec (PREV ALENT)     | 3.93 0.63       | 6.93 0.26         | +76.3%     |\\n|                      |                 |                   | -58.7%     |\\n| Rec (OSCAR)          | 4.29 0.59       | 7.45 0.23         | +73.4%     |\\n|                      |                 |                   | -61.0%     |\\n| Airbert              | 4.01 0.62       | 6.27 0.30         | +56.4%     |\\n|                      |                 |                   | -51.6%     |\\n\\n3. Evaluation of SOTA VLN Agents on the HA-VLN Task\\n\\nWe evaluated state-of-the-art (SOTA) Vision-and-Language Navigation (VLN) agents on the Human-Aware Vision-and-Language Navigation (HA-VLN) task. Each agent was adapted for HA-VLN by incorporating panoramic action spaces and sub-optimal expert guidance to navigate dynamic, human-occupied environments. Our evaluations included both retrained and zero-shot performance assessments, revealing substantial performance degradations in HA-VLN scenarios compared to traditional VLN tasks and significant gaps from the oracle, underscoring the increased complexity introduced by human-aware navigation.\\n\\nRetrained Performance. In retrained HA-VLN settings, even the best-performing agent achieved a maximum success rate (SR) of only 40% in unseen environments, which is 49% lower than the oracle's SR (Tab. 4, Tab. 6). The impact of human occupancy is marked, with SR reductions of up to 65% in unseen settings. Despite retraining, agents remain limited in their human-aware capabilities, exhibiting high Target Collision Rates (TCR) and Collision Rates (CR). For instance, the Speaker-Follower model records TCR and CR values of 0.24 and 0.87 in seen environments, which contrast...\"}"}
{"id": "DJVyRhT8nP", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"sharply with the oracle\u2019s significantly lower TCR of 0.04 and CR of 0.175 (Tab. 5, Tab. 6). These disparities highlight the challenges agents face in adapting to human-centered dynamics.\\n\\nZero-shot Performance. The zero-shot performance of SOTA VLN agents in HA-VLN environments reveals even more pronounced challenges. While leading agents achieve up to 72% SR in traditional VLN tasks for unseen environments, this drops significantly under HA-VLN constraints (Tab. 7). Even Airbert, designed to manage complex environmental contexts, struggles in human-occupied settings, with navigation errors rising by over 167% and SR falling by nearly 67%. These results highlight the considerable difficulty agents encounter in dynamic, human-centric settings, emphasizing the necessity for further advancements in training strategies and navigation models to improve robustness and adaptability in real-world, human-aware navigation tasks.\\n\\nTable 8: Performance Comparison of Our Proposed Agents on HA-VLN Tasks.\\n\\n| Method       | Proportion Validation Seen | Validation Unseen |\\n|--------------|---------------------------|-------------------|\\n| NE           | \u2193                          | \u2193                 |\\n| TCR          | \u2193                          | \u2193                 |\\n| CR           | \u2193                          | \u2193                 |\\n| SR           | \u2191                          | \u2191                 |\\n\\nVLN-DT (Ours)\\n\\n| Proportion Validation Seen | Validation Unseen |\\n|---------------------------|-------------------|\\n| 100%                      | 8.22 0.37 0.58 0.11 |\\n| 7.31 0.38 0.73 0.19       |\\n| 8.23 0.82 0.61 0.12       |\\n\\nVLN-CM (Ours)\\n\\n| Proportion Validation Seen | Validation Unseen |\\n|---------------------------|-------------------|\\n| 25%                       | 8.42 0.99 0.52 0.12 |\\n| 8.67 0.98 0.52 0.11       |\\n| 10.61 1.01 0.62 0.03      |\\n\\nEvaluation of Agents on HA-VLN Task\\n\\nIn this work, we introduce two agent models: the Vision-Language Navigation Decision Transformer (VLN-DT), trained on a dataset generated via random walk, and the Vision-Language Navigation Cross-Modal (VLN-CM), trained under expert supervision. This section compares their performance and examines the impact of various reward strategies on task execution.\\n\\nPerformance Comparison.\\n\\nTable 8 presents a comparative analysis of our agents on HA-VLN tasks. VLN-DT, trained with 100% random walk data, demonstrates comparable performance to the expert-supervised VLN-CM, exhibiting strong generalization capabilities. Notably, VLN-CM's performance degrades significantly as the proportion of random walk data increases; with 100% random data, Success Rate (SR) declines by 83.6% in seen and 81.5% in unseen environments. This outcome underscores VLN-DT's robustness and reduced dependency on expert guidance, making it well-suited for diverse and unpredictable scenarios.\\n\\nReward Strategy Analysis.\\n\\nFigure 5 illustrates the effect of different reward strategies on VLN-DT's performance. A straightforward reward for decreasing target distance resulted in inefficient trajectories with an elevated collision rate. Introducing a penalty-based distance reward achieved modest improvements in Success Rate (SR) and Collision Rate (CR). However, applying additional penalties for human collisions did not significantly enhance performance, underscoring the need for more advanced, human-aware reward strategies to effectively navigate agents through dynamic, human-populated environments.\\n\\nThis analysis highlights the advantages of VLN-DT's design in balancing adaptability and efficiency across various conditions while identifying key areas for future development in reward strategies tailored for human-aware navigation. Detailed performance metrics can be found in Appendix D.4.\\n\\nEvaluation on Real-World Robots\\n\\nTo assess real-world applicability, we deployed our trained agent on a Unitree quadruped robot equipped with a stereo fisheye camera, ultrasonic distance sensors, and an inertial measurement unit.\"}"}
{"id": "DJVyRhT8nP", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The agent operates on an NVIDIA Jetson TX2, processing RGB images to make action inferences, which are subsequently executed via a Raspberry Pi 4B. Continuous IMU feedback enables the robot to monitor and adjust its movement for precision.\\n\\nExperiments were conducted in office environments to evaluate the agent's navigation performance both in the absence and presence of humans. In human-free scenarios (Fig. 16), the agent successfully demonstrated accurate navigation by reliably following prescribed instructions. In human-populated settings, the agent exhibited human-aware navigation, detecting and actively avoiding individuals in its path (Fig. 17). However, we also observed cases where the robot's performance degraded, resulting in collisions due to sudden, unpredictable changes in human behavior (Fig. 18), which highlights the inherent challenges of navigating dynamic, human-centric environments.\\n\\nThese experiments underscore the effectiveness of transferring learned policies from simulated settings to physical robots, while also revealing areas for improvement. Specifically, the findings highlight the necessity for enhanced robustness and adaptability to better manage real-world complexity.\\n\\nAdditional experimental details and results are provided in App. D.4.\\n\\n4 Discussion\\n\\nApplications & Extensions.\\n\\nThe HA3D simulator advances the field of human-centered simulation by accommodating widely-adopted 3D formats, including .obj and .glb, thus streamlining integration and promoting broader research utility. This adaptability enables researchers to expand character diversity and customize agents within simulated scenes, fostering the creation of complex, multi-agent interactive environments. Moreover, the framework's architecture readily supports the incorporation of additional dynamic entities, such as animals and autonomous robots, thereby further enhancing the simulation's capacity to represent realistic, richly populated scenarios. For implementation details, please refer to our GitHub repository.\\n\\nLimitations.\\n\\nWhile the Human-Aware Vision and Language Navigation (HA-VLN) framework constitutes a significant step forward in embodied AI navigation, certain limitations persist. The framework's current scope captures human presence and basic movement but does not yet model the breadth of human behavioral patterns and social nuances, which may affect the robustness of trained agents in real-world applications where human interactions are more complex and varied. Additionally, the HA3D and HA-R2R datasets are confined to indoor environments, which may limit the generalizability of trained agents across diverse real-world settings, particularly in outdoor contexts where navigation dynamics differ substantially.\\n\\nFuture Work.\\n\\nTo further enhance the HA-VLN framework, future research should prioritize refining human behavior modeling to encompass more sophisticated social interactions, nuanced group dynamics, and contextualized interpersonal behaviors. The inclusion of avatars with heightened behavioral fidelity would enrich the simulation's realism, enabling more effective modeling of human-agent interactions. Extending the simulator to support outdoor environments is also paramount, as this expansion would allow for the development of agents capable of navigating across a wider range of real-world scenarios. These improvements, coupled with advanced domain adaptation techniques and robust strategies for managing environmental uncertainty, are essential to foster the development of highly adaptable and resilient VLN systems capable of seamless operation within diverse, human-populated environments.\\n\\n5 Conclusion\\n\\nThis work presents the Human-Aware Vision and Language Navigation (HA-VLN) framework, which integrates dynamic human activities while relaxing restrictive assumptions inherent to conventional VLN systems. Through the development of the Human-Aware 3D (HA3D) simulator and the Human-Aware Room-to-Room (HA-R2R) dataset, we provide a comprehensive environment for the training and evaluation of HA-VLN agents. We introduce two agent architectures\u2014the Expert-Supervised Cross-Modal (VLN-CM) and the Non-Expert-Supervised Decision Transformer (VLN-DT)\u2014each leveraging cross-modal fusion and diverse training paradigms to support effective navigation in dynamically populated settings. Extensive evaluation highlights the contributions of this framework while underscoring the need for continued research to strengthen HA-VLN agents' robustness and adaptability for deployment in complex, real-world environments.\"}"}
{"id": "DJVyRhT8nP", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Author Contributions\\n\\nHeng Li was responsible for agent development and evaluations, drafted the initial agent and evaluation sections, and revised the final manuscript based on review feedback. Minghan Li was responsible for simulator development and evaluations, prepared the initial draft of the simulator and evaluation sections, conducted real-world testing, and created the project website. Zhi-Qi Cheng supervised the design and development of both the agent and simulator, managed the entire project execution, designed the evaluation plan, drafted the initial manuscript, revised the final version, and provided guidance to the entire team. Yifei Dong designed the initial simulation prototyping, drafted the related work section, and provided revision suggestions. Yuxuan Zhou offered collaborative feedback and contributed revision suggestions. Jun-Yan He participated in project discussions. Qi Dai provided invaluable strategic guidance and contributed to manuscript revisions. Teruko Mitamura offered constructive feedback, and Alexander G. Hauptmann provided critical insights and contributed to manuscript refinement. We also thank the anonymous reviewers for their valuable suggestions.\\n\\nAcknowledgments\\n\\nThis work was partially supported by the Air Force Research Laboratory under agreement number FA8750-19-2-0200; the financial assistance award 60NANB17D156 from the U.S. Department of Commerce, National Institute of Standards and Technology (NIST); the Intelligence Advanced Research Projects Activity (IARPA) via the Department of Interior/Interior Business Center (DOI/IBC) contract number D17PC00340; and the Defense Advanced Research Projects Agency (DARPA) grant under the GAILA program (award HR00111990063) and the AIDA program (award FA8750-18-20018). Additional support was provided by the Carnegie Mellon Manufacturing Futures Institute and the Manufacturing PA Innovation Program.\\n\\nThe authors also acknowledge the Intel Ph.D. Fellowship and the IBM Outstanding Students Scholarship awarded to Zhi-Qi Cheng, as well as the computing resources provided by Microsoft Research. We further extend our gratitude to the School of Computer Science (SCS) at Carnegie Mellon University, particularly the High Performance Computing (HPC) facility, for providing essential computational resources.\\n\\nThe U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation therein. The views and conclusions presented in this work are those of the authors and do not necessarily represent the official policies or endorsements, either expressed or implied, of the Air Force Research Laboratory, NIST, IARPA, DARPA, Microsoft Research, or other funding agencies.\\n\\nReferences\\n\\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\\n\\n[2] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3674\u20133683, 2018.\\n\\n[3] Peter Anderson, Ayush Shrivastava, Joanne Truong, Arjun Majumdar, Devi Parikh, Dhruv Batra, and Stefan Lee. Sim-to-real transfer for vision-and-language navigation. In Conference on Robot Learning, pages 671\u2013681. PMLR, 2021.\\n\\n[4] Valts Blukis, Dipendra Misra, Ross A Knepper, and Yoav Artzi. Mapping navigation instructions to continuous control actions with position-visitation prediction. In Conference on Robot Learning, pages 505\u2013518. PMLR, 2018.\\n\\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\"}"}
{"id": "DJVyRhT8nP", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017.\\n\\n[2] Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12538\u201312547, 2019.\\n\\n[3] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling, 2021.\\n\\n[4] Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, and Ivan Laptev. History aware multimodal transformer for vision-and-language navigation. Advances in neural information processing systems, 34:5834\u20135847, 2021.\\n\\n[5] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1\u201310, 2018.\\n\\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n\\n[7] Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-follower models for vision-and-language navigation. Advances in Neural Information Processing Systems, 31, 2018.\\n\\n[8] Xiaofeng Gao, Qiaozi Gao, Ran Gong, Kaixiang Lin, Govind Thattai, and Gaurav S Sukhatme. Dialfred: Dialogue-enabled agents for embodied instruction following. IEEE Robotics and Automation Letters, 7(4):10049\u201310056, 2022.\\n\\n[9] Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, and Ali Farhadi. Iqa: Visual question answering in interactive environments. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4089\u20134098, 2018.\\n\\n[10] Jing Gu, Eliana Stefani, Qi Wu, Jesse Thomason, and Xin Wang. Vision-and-language navigation: A survey of tasks, methods, and future directions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7606\u20137623, 2022.\\n\\n[11] Pierre-Louis Guhur, Makarand Tapaswi, Shizhe Chen, Ivan Laptev, and Cordelia Schmid. Airbert: In-domain pretraining for vision-and-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1634\u20131643, 2021.\\n\\n[12] Tevet Guy, Raab Sigal, Gordon Brian, Shafir Yonatan, Cohen-Or Daniel, and H. Bermano Amit. Mdm: Human motion diffusion model. arXiv preprint arXiv:2209.14916, 2022.\\n\\n[13] Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao. Towards learning a generic agent for vision-and-language navigation via pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13137\u201313146, 2020.\\n\\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\\n\\n[15] Yicong Hong, Cristian Rodriguez, Yuankai Qi, Qi Wu, and Stephen Gould. Language and visual entity relationship graph for agent navigation. Advances in Neural Information Processing Systems, 33:7685\u20137696, 2020.\\n\\n[16] Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, and Stephen Gould. A recurrent vision-and-language bert for navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1643\u20131653, June 2021.\\n\\n[17] Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie, and Jason Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. arXiv preprint arXiv:1905.12255, 2019.\\n\\n[18] Abhishek Kadian, Joanne Truong, Aaron Gokaslan, Alexander Clegg, Erik Wijmans, Stefan Lee, Manolis Savva, Sonia Chernova, and Dhruv Batra. Sim2real predictivity: Does evaluation in simulation predict real-world performance? IEEE Robotics and Automation Letters, 5(4):6670\u20136677, 2020.\"}"}
{"id": "DJVyRhT8nP", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[24] Micha\u0142 Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja\u00b4skowski. Vizdoom: A doom-based ai research platform for visual reinforcement learning. In 2016 IEEE conference on computational intelligence and games (CIG), pages 1\u20138. IEEE, 2016.\\n\\n[25] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474, 2017.\\n\\n[26] Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, and Stefan Lee. Beyond the nav-graph: Vision-and-language navigation in continuous environments. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXVIII 16, pages 104\u2013120. Springer, 2020.\\n\\n[27] Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge. Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. arXiv preprint arXiv:2010.07954, 2020.\\n\\n[28] Jialu Li, Hao Tan, and Mohit Bansal. Envedit: Environment editing for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15407\u201315417, 2022.\\n\\n[29] Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pre-training for vision-language tasks. ECCV 2020, 2020.\\n\\n[30] Kunyang Lin, Peihao Chen, Diwei Huang, Thomas H Li, Mingkui Tan, and Chuang Gan. Learning vision-and-language navigation from youtube videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8317\u20138326, 2023.\\n\\n[31] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. Smpl: A skinned multi-person linear model. ACM transactions on graphics (TOG), 34(6):1\u201316, 2015.\\n\\n[32] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems 32, 2019.\\n\\n[33] Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, and Caiming Xiong. Self-monitoring navigation agent via auxiliary progress estimation. arXiv preprint arXiv:1901.03035, 2019.\\n\\n[34] Matt MacMahon, Brian Stankiewicz, and Benjamin Kuipers. Walk the talk: Connecting language, knowledge, and action in route instructions. Def, 2(6):4, 2006.\\n\\n[35] Sameer Maithel, Smita Chandiwala, Prashant Bhanware, Rajan Rawal, Sonal Kumar, Vrinda Gupta, and Mohit Jain. Developing cost-effective and low carbon options to meet india\u2019s cooling demand in urban residential building through 2050, 05 2020.\\n\\n[36] Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, and Dhruv Batra. Improving vision-and-language navigation with image-text pairs from the web, 2020.\\n\\n[37] Khanh Nguyen, Debadeepta Dey, Chris Brockett, and Bill Dolan. Vision-based navigation with language-based assistance via imitation learning with indirect intervention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12527\u201312537, 2019.\\n\\n[38] Alexander Pashevich, Cordelia Schmid, and Chen Sun. Episodic transformer for vision-and-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15942\u201315952, 2021.\\n\\n[39] Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William Clegg, Michal Hlavac, So Yeon Min, et al. Habitat 3.0: A co-habitat for humans, avatars and robots. arXiv preprint arXiv:2310.13724, 2023.\\n\\n[40] Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, and Anton van den Hengel. Reverie: Remote embodied visual referring expression in real indoor environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9982\u20139991, 2020.\\n\\n[41] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training, 2018.\"}"}
{"id": "DJVyRhT8nP", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied AI research. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9339\u20139347, 2019.\\n\\nLinda Smith and Michael Gasser. The development of embodied cognition: Six lessons from babies. Artificial life, 11(1-2):13\u201329, 2005.\\n\\nHao Tan, Licheng Yu, and Mohit Bansal. Learning to navigate unseen environments: Back translation with environmental dropout. arXiv preprint arXiv:1904.04195, 2019.\\n\\nJesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer. Vision-and-dialog navigation. In Conference on Robot Learning, pages 394\u2013406. PMLR, 2020.\\n\\nXin Wang, Wenhan Xiong, Hongmin Wang, and William Yang Wang. Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation. In Proceedings of the European Conference on Computer Vision, pages 37\u201353, 2018.\\n\\nXin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6629\u20136638, 2019.\\n\\nZun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, and Yu Qiao. Scaling data generation in vision-and-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12009\u201312020, 2023.\\n\\nYi Wu, Yuxin Wu, Georgia Gkioxari, and Yuandong Tian. Building generalizable agents with a realistic and rich 3D environment. arXiv preprint arXiv:1801.02209, 2018.\\n\\nFei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9068\u20139079, 2018.\\n\\nChengguang Xu, Hieu T Nguyen, Christopher Amato, and Lawson LS Wong. Vision and language navigation in the real world via online visual language mapping. arXiv preprint arXiv:2310.10822, 2023.\\n\\nAn Yan, Xin Eric Wang, Jiangtao Feng, Lei Li, and William Yang Wang. Cross-lingual vision-language navigation. arXiv preprint arXiv:1910.11301, 2019.\\n\\nAlbert Yu, Adeline Foote, Raymond Mooney, and Roberto Mart\u00edn-Mart\u00edn. Natural language can help bridge the sim2real gap, 2024.\\n\\nJiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, and Wang He. Navid: Video-based VLM plans the next step for vision-and-language navigation. arXiv preprint arXiv:2402.15852, 2024.\\n\\nGengze Zhou, Yicong Hong, and Qi Wu. Navgpt: Explicit reasoning in vision-and-language navigation with large language models, 2023.\\n\\nFengda Zhu, Yi Zhu, Xiaojun Chang, and Xiaodan Liang. Vision-language navigation with self-supervised auxiliary reasoning tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10012\u201310022, 2020.\"}"}
{"id": "DJVyRhT8nP", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Related Work\\n\\nWe trace the evolution of the Visual-and-Language Navigation (VLN) task and highlight the key differences between our proposed Human-Aware VLN (HA-VLN) task and prior work, focusing on three critical aspects: Egocentric Action Space, Human Interactivity, and Sub-optimal Expert.\\n\\nTable 9 provides a detailed comparison of tasks, simulators, and agents based on these aspects.\\n\\nEvolution of VLN Tasks\\n\\nVLN originated with tasks like Room-to-Room (R2R) [2, 15, 27] for indoor navigation, while TOUCHDOWN and MARCO [7, 34] focused on outdoor navigation. Goal-driven navigation with simple instructions was explored in REVERIE [40] and VNLA [37], and DialFRED [13] and CVDN [45] introduced navigation through human dialogue. However, since the Speaker-Follower [12], panoramic action spaces have been predominantly used, deviating from our first assumption of an Egocentric Action Space, which provides a more realistic and challenging navigation scenario. More recent tasks, such as Room-for-Room (R4R), RoomXRoom, VNLA, CVDN, and VLN-CE [22, 26, 27, 37, 45], have started to address dynamic navigation scenarios in Egocentric Action Space. Nevertheless, they still lack the complexity of real-world human interactions that HA-VLN specifically targets, which is crucial for developing agents that can navigate effectively in the presence of humans.\\n\\nSimulator for VLN Tasks\\n\\nVLN simulators can be categorized into photorealistic and non-photorealistic. Non-photorealistic simulators like AI2-THOR [25] and Gibson GANI [50] do not include human activities, while photorealistic simulators such as House3D [49], Matterport3D [2], and Habitat [42] offer high visual fidelity but typically lack dynamic human elements. The absence of human interactivity in these simulators limits their ability to represent real-world navigation scenarios, which is crucial for our second assumption of Human Interactivity. Some simulators, like Habitat3.0 [39], AI2-THOR [25], and ViZDoom [24], consider human interaction but provide non-photorealistic scenes, while Google Street View offers a photorealistic outdoor environment with static humans. In contrast, our HA3D simulator bridges the gap between simulated tasks and real-world applicability by integrating photorealistic indoor environments enriched with human activities, enabling the development of agents that can navigate effectively in the presence of dynamic human elements.\\n\\nAgent for VLN Tasks\\n\\nEarly VLN models, enhanced by attention mechanisms and reinforcement learning algorithms [13, 33, 40, 47], paved the way for recent works based on pre-trained visual-language models like ViLBert [32]. These models, such as VLN-BERT [36], PREVALENT [18], Oscar [29], Lily [30], and ScaleVLN [48], have significantly improved navigation success rates by expanding the scale of pre-training data. However, most of these agents navigate using a panoramic action space, unlike [2, 54, 55], which operate in an Egocentric action space. Notably, NaVid [54] demonstrated the transfer of the agent to real robots. Despite these advancements, most of these agents are guided by an optimal expert, which conflicts with our third assumption of using a sub-optimal expert. In real-world scenarios, expert guidance may not always be perfect, and agents need to be robust to handle such situations. Our agents are specifically designed to operate effectively under less stringent and more realistic expert supervision, enhancing their ability to perform in true Sim2Real scenarios and setting them apart from previous approaches.\\n\\nTable 9: Comparison of Tasks, Simulators, and Agents based on the three key aspects: Egocentric Action Space, Human Interactivity, and Sub-optimal Expert.\"}"}
{"id": "DJVyRhT8nP", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The HA3D simulator's code structure is inspired by the Matterport3D (MP3D) simulator, which can be found at https://github.com/peteanderson80/Matterport3DSimulator. To obtain access to the Matterport Dataset, we sent an email request to matterport3d@googlegroups.com. The source code for the HA3D simulator is available in our GitHub repository at https://github.com/lpercc/HA3D_simulator.\\n\\nAs illustrated in Fig. 6, the HA3D simulator provides agents with three key features that distinguish it from traditional VLN frameworks: an Ergonomic Action Space, Dynamic Environments, and a Sub-Optimal Expert.\\n\\nFigure 6: Overview of the VLN framework assumptions in the HA3D simulator. The simulator introduces an Ergonomic Action Space, Dynamic Environments, and a Sub-Optimal Expert to bridge the gap between simulated and real-world navigation scenarios. The Ergonomic Action Space limits the agent's field of view to 60 degrees, requiring a more realistic navigation strategy compared to the panoramic view used in traditional VLN tasks. Dynamic Environments incorporate time-varying elements, such as human activities, challenging the agent to adapt its navigation strategy to handle video streams that include people. The Sub-Optimal Expert provides navigation guidance that accounts for human factors and dynamic elements, resulting in a more realistic and human-like navigation strategy compared to the optimal expert model that always finds the shortest path without considering these factors. [Best viewed in color]\\n\\nB.1 HAPS Dataset\\n\\nThe HAPS Dataset encompasses a diverse range of 29 indoor regions, including bathroom, bedroom, closet, dining room, entryway/foyer/lobby, family room, garage, hallway, library, laundry room/mudroom, kitchen, living room, meeting room/conference room, lounge, office, porch/terrace/deck/driveway, recreation/game room, stairs, toilet, utility room/tool room, TV room, workout/gym/exercise room, outdoor areas containing grass, plants, bushes, trees, etc., balcony, other room, bar, classroom, dining booth, and spa/sauna. The dataset features skinned human motion models devoid of identifiable biometric features or offensive content. Fig. 9 illustrates the skeletons of the dataset's human activities, accompanied by their corresponding descriptions, which exhibit diverse forms and interactions with the environment.\\n\\nTo ensure the quality and relevance of the human activity descriptions, we employed GPT-4 to generate an extensive set of descriptions for each of the 29 indoor regions. Subsequently, we conducted a rigorous human survey involving 50 participants from diverse demographics to evaluate and select the most appropriate descriptions. As depicted in Fig. 7, each participant assessed the descriptions for a specific indoor region based on three key criteria: 1) High Relevance to the specified region, 2) Verb-Rich Interaction with the environment, and 3) Conformity to Daily Life patterns. The survey was conducted in five rounds, with the highest-rated descriptions from previous rounds being excluded from subsequent evaluations to ensure a comprehensive review process. Upon analyzing the survey responses, we identified the activity descriptions with the highest selection frequency for each region, ultimately curating a set of 145 human activity descriptions (Fig. 8).\\n\\nThe resulting HAPS Dataset, available for download at https://drive.google.com/drive/folders/1aswHATnKNViqw6QenAwdQRTwXQQE5jd3?usp=sharing, represents a meticulously crafted resource for studying and simulating human activities in indoor environments.\"}"}
