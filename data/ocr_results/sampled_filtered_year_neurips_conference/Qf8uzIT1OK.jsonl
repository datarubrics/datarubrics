{"id": "Qf8uzIT1OK", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023.\\n\\nAndrew C Gallagher and Tsuhan Chen. Understanding images of groups of people. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 256\u2013263, 2009.\\n\\nTzvi Ganel. Smiling makes you look older. Psychonomic bulletin & review, 22(6):1671\u20131677, 2015.\\n\\nDenia Garcia and Maria Abascal. Colored perceptions: Racially distinctive names and assessments of skin color. American Behavioral Scientist, 60(4):420\u2013441, 2016.\\n\\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 III, and Kate Crawford. Datasheets for datasets. 2018.\\n\\nJohn G Geer. Do open-ended questions measure \u201csalient\u201d issues? Public Opinion Quarterly, 55(3):360\u2013370, 1991.\\n\\nAndreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3354\u20133361, 2012.\\n\\nR Stuart Geiger, Kevin Yu, Yanlai Yang, Mindy Dai, Jie Qiu, Rebekah Tang, and Jenny Huang. Garbage in, garbage out? do machine learning application papers in social computing report where human-labeled training data comes from? In ACM Conference on Fairness, Accountability, and Transparency (FAccT), pages 325\u2013336, 2020.\\n\\nRobert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665\u2013673, 2020.\\n\\nPatricia A George and Graham J Hole. Factors influencing the accuracy of age estimates of unfamiliar faces. Perception, 24(9):1059\u20131073, 1995.\\n\\nAthinodoros S. Georghiades, Peter N. Belhumeur, and David J. Kriegman. From few to many: Illumination cone models for face recognition under variable lighting and pose. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(6):643\u2013660, 2001.\\n\\nMarkos Georgopoulos, Yannis Panagakis, and Maja Pantic. Investigating bias in deep face analysis: The kanface dataset and empirical study. Image and vision computing, 102:103954, 2020.\\n\\nGoogle PAIR. Google pair. people + ai guidebook. https://pair.withgoogle.com/guidebook, 2019. [Accessed February 1, 2023].\\n\\nBruce G Gordon. Vulnerability in research: basic ethical concepts and general approach to review. Ochsner Journal, 20(1):34\u201338, 2020.\\n\\nMitchell L Gordon, Michelle S Lam, Joon Sung Park, Kayur Patel, Jeff Hancock, Tatsunori Hashimoto, and Michael S Bernstein. Jury learning: Integrating dissenting voices into machine learning models. In Conference on Human Factors in Computing Systems (CHI), pages 1\u201319, 2022.\\n\\nPriya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Ishan Misra, Levent Sagun, Armand Joulin, and Piotr Bojanowski. Vision models are more robust and fair when pretrained on uncurated images without supervision. arXiv preprint arXiv:2202.08360, 2022.\\n\\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18995\u201319012, 2022.\\n\\nMislav Grgic, Kresimir Delac, and Sonja Grgic. Scface\u2013surveillance cameras face database. Multimedia tools and applications, 51(3):863\u2013879, 2011.\\n\\nPatrick J Grother, Mei L Ngan, Kayee K Hanaoka, et al. Face recognition vendor test part 3: demographic effects. 2019.\\n\\nManuel G\u00fcnther, Peiyun Hu, Christian Herrmann, Chi-Ho Chan, Min Jiang, Shufan Yang, Akshay Raj Dhamija, Deva Ramanan, J\u00fcrgen Beyerer, Josef Kittler, et al. Unconstrained face detection and open-set face recognition challenge. In IEEE International Joint Conference on Biometrics (IJCB), pages 697\u2013706. IEEE, 2017.\\n\\nGuodong Guo, Yun Fu, Charles R Dyer, and Thomas S Huang. Image-based human age estimation by manifold learning and locally adjusted robust regression. IEEE Transactions on Image Processing, 17(7):1178\u20131188, 2008.\\n\\nYandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In European Conference on Computer Vision (ECCV), pages 87\u2013102. Springer, 2016.\\n\\nFoad Hamidi, Morgan Klaus Scheuerman, and Stacy M Branham. Gender recognition or gender reductionism? the social implications of embedded gender recognition systems. In Conference on Human Factors in Computing Systems (CHI), pages 1\u201313, 2018.\\n\\nHu Han, Anil K Jain, Fang Wang, Shiguang Shan, and Xilin Chen. Heterogeneous face attribute estimation: A deep multi-task learning approach. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(11):2597\u20132609, 2017.\\n\\nMargot Hanley, Apoorv Khandelwal, Hadar Averbuch-Elor, Noah Snavely, and Helen Nissenbaum. An ethical highlighter for people-centric dataset creation. 2020.\\n\\nAlex Hanna and Tina M Park. Against scale: Provocations and resistances to scale thinking. arXiv preprint arXiv:2010.08850, 2020.\\n\\nAlex Hanna, Emily Denton, Razvan Amironesei, Andrew Smart, and Hilary Nicole. Lines of sight. https://logicmag.io/commons/lines-of-sight/, 2020. [Accessed February 7, 2023].\\n\\nAlex Hanna, Emily Denton, Andrew Smart, and Jamila Smith-Loud. Towards a critical race methodology in algorithmic fairness. In ACM Conference on Fairness, Accountability, and Transparency (FAccT), pages 501\u2013512, 2020.\\n\\nAdam Harvey and Jules LaPlace. Exposing. ai, 2021.\\n\\nTatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without demographics in repeated loss minimization. In International Conference on Machine Learning, pages 1929\u20131938. PMLR, 2018.\\n\\nKenji Hata, Ranjay Krishna, Li Fei-Fei, and Michael S Bernstein. A glimpse far into the future: Understanding long-term crowd worker quality. In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing, pages 889\u2013901, 2017.\\n\\nCaner Hazirbas, Joanna Bitton, Brian Dolhansky, Jacqueline Pan, Albert Gordo, and Christian Canton Ferrer. Towards measuring fairness in ai: the casual conversations dataset. IEEE Transactions on Biometrics, Behavior, and Identity Science, 2021.\\n\\nCaner Hazirbas, Yejin Bang, Tiezheng Yu, Parisa Assar, Bilal Porgali, V\u00edtor Albiero, Stefan Hermanek, Jacqueline Pan, Emily McReynolds, Miranda Bogen, Pascale Fung, and Christian Canton Ferrer. Casual conversations v2: Designing a large consent-driven dataset to measure algorithmic bias and robustness, 2022.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Steven Y He, Charles E McCulloch, W John Boscardin, Mary-Margaret Chren, Eleni Linos, and Sarah T Arron. Self-reported pigmentary phenotypes and race are significant but incomplete predictors of Fitzpatrick skin phototype in an ethnically diverse population. *Journal of the American Academy of Dermatology*, 71(4):731\u2013737, 2014.\\n\\nLisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, and Anna Rohrbach. Women also snowboard: Overcoming bias in captioning models. In *European Conference on Computer Vision (ECCV)*, pages 771\u2013787, 2018.\\n\\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In *International Conference on Learning Representations (ICLR)*, 2019.\\n\\nJulie J Henkelman and Robin D Everall. Informed consent with children: Ethical and practical implications. *Canadian Journal of Counselling and Psychotherapy*, 35(2), 2001.\\n\\nAlex Hern. Flickr faces complaints over \u2018offensive\u2019 auto-tagging for photos. https://www.theguardian.com/technology/2015/may/20/flickr-complaints-offensive-auto-tagging-photos, May 2015.\\n\\nAlex Hern. Google\u2019s solution to accidental algorithmic racism: Ban gorillas. https://www.theguardian.com/technology/2018/jan/12/google-racism-ban-gorilla-black-people, January 2018.\\n\\nAlex Hern. Twitter apologises for \u2018racist\u2019 image-cropping algorithm. https://www.theguardian.com/technology/2020/sep/21/twitter-apologises-for-racist-image-cropping-algorithm, September 2020.\\n\\nEvan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, and Jacob Andreas. Natural language descriptions of deep visual features. In *International Conference on Learning Representations*, 2021.\\n\\nKashmir Hill. Wrongfully accused by an algorithm. *The New York Times*, 2020.\\n\\nMark E Hill. Race of the interviewer and perception of skin color: Evidence from the multi-city study of urban inequality. *American Sociological Review*, pages 99\u2013108, 2002.\\n\\nYusuke Hirota, Yuta Nakashima, and Noa Garcia. Quantifying societal bias amplification in image captioning. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2022.\\n\\nSarah Holland, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia Chmielinski. The dataset nutrition label: A framework to drive higher data quality standards. 2018.\\n\\nKenneth Holstein, Jennifer Wortman Vaughan, Hal Daum\u00e9 III, Miro Dud\u00edk, and Hanna Wallach. Improving fairness in machine learning systems: What do industry practitioners need? In *Conference on Human Factors in Computing Systems (CHI)*, pages 1\u201316, 2019.\\n\\nSara Hooker. Moving beyond \u201calgorithmic bias is a data problem\u201d. *Patterns*, 2(4):100241, 2021.\\n\\nAyanna Howard, Cha Zhang, and Eric Horvitz. Addressing bias in machine learning algorithms: A pilot study on emotion recognition for intelligent systems. In *2017 IEEE Workshop on Advanced Robotics and its Social Impacts (ARSO)*, pages 1\u20137. IEEE, 2017.\\n\\nGary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. In *Workshop on faces in \u2018Real-Life\u2019 Images: detection, alignment, and recognition*, 2008.\\n\\nHan-Yin Huang and Cynthia CS Liem. Social inclusion in curated contexts: Insights from museum practices. In *ACM Conference on Fairness, Accountability, and Transparency (FAccT)*, pages 300\u2013309, 2022.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zhanyuan Huang, Yang Liu, Yajun Fang, and Berthold KP Horn. Video-based fall detection for seniors with human pose estimation. In 2018 4th international conference on Universal Village (UV), pages 1\u20134. IEEE, 2018.\\n\\nJennifer L Hughes, Abigail A Camden, Tenzin Yangchen, et al. Rethinking and updating demographic questions: Guidance to improve descriptions of research samples. Psi Chi Journal of Psychological Research, 21(3):138\u2013151, 2016.\\n\\nHuman Rights Campaign Foundation. Talking about pronouns in the workplace. https://www.thehrcfoundation.org/professional-resources/talking-about-pronouns-in-the-workplace, n.d.\\n\\nAndrew Hundt, William Agnew, Vicky Zeng, Severin Kacianka, and Matthew Gombolay. Robots enact malignant stereotypes. In ACM Conference on Fairness, Accountability, and Transparency (FAccT), pages 743\u2013756, 2022.\\n\\nBen Hutchinson, Andrew Smart, Alex Hanna, Emily Denton, Christina Greer, Oddur Kjar\u00f0tansson, Parker Barnes, and Margaret Mitchell. Towards accountability for machine learning datasets: Practices from software engineering and infrastructure. In ACM Conference on Fairness, Accountability, and Transparency (FAccT), pages 560\u2013575, 2021.\\n\\nIBM. Design for AI. https://www.ibm.com/design/ai, 2019. [Accessed February 1, 2023].\\n\\nIllinois Legislature. Biometric information privacy act. https://www.ilga.gov/legislation/ilcs/ilcs3.asp?ActID=3004&ChapterID=57, 2008. [Accessed November 12, 2022].\\n\\nLilly Irani. The cultural work of microwork. New media & society, 17(5):720\u2013739, 2015.\\n\\nJoel Janai, Fatma G\u00fcney, Aseem Behl, Andreas Geiger, et al. Computer vision for autonomous vehicles: Problems, datasets and state of the art. Foundations and Trends\u00ae in Computer Graphics and Vision, 12(1\u20133):1\u2013308, 2020.\\n\\nOliver Jesorsky, Klaus J Kirchberg, and Robert W Frischholz. Robust face detection using the Hausdorff distance. In Audio-and Video-Based Biometric Person Authentication: Third International Conference, AVBPA 2001 Halmstad, Sweden, June 6\u20138, 2001 Proceedings 3, pages 90\u201395. Springer, 2001.\\n\\nJulie Jiang, Emily Chen, Luca Luceri, Goran Muri\u00b4c, Francesco Pierri, Ho-Chun Herbert Chang, and Emilio Ferrara. What are your pronouns? examining gender pronoun usage on twitter. arXiv preprint arXiv:2207.10894, 2022.\\n\\nEun Seo Jo and Timnit Gebru. Lessons from archives: Strategies for collecting sociocultural data in machine learning. In ACM Conference on Fairness, Accountability and Transparency (FAccT), 2020.\\n\\nSonam Joshi. Why Indians are sharing their pronouns on social media. https://timesofindia.indiatimes.com/india/why-indians-are-sharing-their-pronouns-on-social-media/articleshow/71669703.cms, October 2019.\\n\\nRie Kamikubo, Utkarsh Dwivedi, and Hernisa Kacorri. Sharing practices for datasets related to accessibility and aging. In Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility, pages 1\u201316, 2021.\\n\\nShivani Kapania, Alex S Taylor, and Ding Wang. A hunt for the snark: Annotator diversity in data practices. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 1\u201315, 2023.\\n\\nKimmo Karkkainen and Jungseock Joo. Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation. In IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1548\u20131558, 2021.\\n\\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4401\u20134410, 2019.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Matthew Kay, Cynthia Matuszek, and Sean A Munson. Unequal representation and gender stereotypes in image search results for occupations. In Conference on Human Factors in Computing Systems (CHI), pages 3819\u20133828, 2015.\\n\\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.\\n\\nJane Kaye, Edgar A Whitley, David Lund, Michael Morrison, Harriet Teare, and Karen Melham. Dynamic consent: a patient interface for twenty-first century research networks. European journal of human genetics, 23(2):141\u2013146, 2015.\\n\\nIra Kemelmacher-Shlizerman, Steven M Seitz, Daniel Miller, and Evan Brossard. The megaface benchmark: 1 million faces for recognition at scale. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4873\u20134882, 2016.\\n\\nNorbert L Kerr. Harking: Hypothesizing after the results are known. Personality and social psychology review, 2(3):196\u2013217, 1998.\\n\\nSuzanne J Kessler and Wendy McKenna. Gender: An ethnomethodological approach. University of Chicago Press, 1985.\\n\\nFlorian Keusch. The influence of answer box format on response behavior on list-style open-ended questions. Journal of Survey Statistics and Methodology, 2(3):305\u2013322, 2014.\\n\\nOs Keyes. The misgendering machines: Trans/hci implications of automatic gender recognition. Proceedings of the ACM on human-computer interaction, 2(CSCW):1\u201322, 2018.\\n\\nZaid Khan and Yun Fu. One label, one billion faces: Usage and consistency of racial categories in computer vision. In ACM Conference on Fairness, Accountability, and Transparency (FAccT), pages 587\u2013597, 2021.\\n\\nWon Kim, Byoung-Ju Choi, Eui-Kyeong Hong, Soo-Kyung Kim, and Doheon Lee. A taxonomy of dirty data. Data mining and knowledge discovery, 7:81\u201399, 2003.\\n\\nJennifer Klima, Sara M Fitzgerald-Butt, Kelly J Kelleher, Deena J Chisolm, R Dawn Comstock, Amy K Ferketich, and Kim L McBride. Understanding of informed consent by parents of children enrolled in a genetic biobank. Genetics in Medicine, 16(2):141\u2013148, 2014.\\n\\nBernard Koch, Emily Denton, Alex Hanna, and Jacob Gates Foster. Reduced, reused and recycled: The life of a dataset in machine learning research. In Advances in Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS D&B), 2021.\\n\\nAdam Kortylewski, Bernhard Egger, Andreas Schneider, Thomas Gerig, Andreas Morel-Forster, and Thomas Vetter. Empirically analyzing the effect of dataset biases on deep face recognition systems. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 2093\u20132102, 2018.\\n\\nAdam Kortylewski, Bernhard Egger, Andreas Schneider, Thomas Gerig, Andreas Morel-Forster, and Thomas Vetter. Analyzing and reducing the damage of dataset bias to face recognition with synthetic data. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2019.\\n\\nMatthew B Kugler. From identification to identity theft: Public perceptions of biometric privacy harms. UC Irvine L. Rev., 10:107, 2019.\\n\\nNeeraj Kumar, Alexander C Berg, Peter N Belhumeur, and Shree K Nayar. Attribute and simile classifiers for face verification. In IEEE/CVF International Conference on Computer Vision (ICCV), pages 365\u2013372, 2009.\\n\\nAlina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International Journal of Computer Vision (IJCV), 128(7):1956\u20131981, 2020.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and Ed Chi. Fairness without demographics through adversarially reweighted learning. Advances in Neural Information Processing Systems (NeurIPS), 33:728\u2013740, 2020.\\n\\nAnjana Lakshmi, Bernd Wittenbrink, Joshua Correll, and Debbie S Ma. The india face set: International and cultural boundaries impact face impressions and perceptions of category membership. Frontiers in psychology, 12:161, 2021.\\n\\nMin Kyung Lee and Katherine Rich. Who is included in human perceptions of ai?: Trust and perceived fairness around healthcare ai and cultural mistrust. In Conference on Human Factors in Computing Systems (CHI), pages 1\u201314, 2021.\\n\\nJohn Leuner. A replication study: Machine learning models are capable of predicting sexual orientation from facial images. arXiv preprint arXiv:1902.10739, 2019.\\n\\nGil Levi and Tal Hassner. Age and gender classification using convolutional neural networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 34\u201342, 2015.\\n\\nJizhizi Li, Sihan Ma, Jing Zhang, and Dacheng Tao. Privacy-preserving portrait matting. In ACM International Conference on Multimedia, pages 3501\u20133509, 2021.\\n\\nTao Li and Lei Lin. Anonymousnet: Natural face de-identification with measurable privacy. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2019.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision (ECCV), pages 740\u2013755. Springer, 2014.\\n\\nZhenyi Liu, Trisha Lian, Joyce Farrell, and Brian A. Wandell. Neural network generalization: The impact of camera parameters. IEEE Access, 8:10443\u201310454, 2020.\\n\\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In IEEE International Conference on Computer Vision (ICCV), pages 3730\u20133738, 2015.\\n\\nDuri Long and Brian Magerko. What is ai literacy? competencies and design considerations. In Proceedings of the 2020 CHI conference on human factors in computing systems, pages 1\u201316, 2020.\\n\\nZhongyu Lou, Fares Alnajar, Jose M Alvarez, Ninghang Hu, and Theo Gevers. Expression-invariant age estimation using structured learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(2):365\u2013375, 2017.\\n\\nMike Loukides, Hilary Mason, and D Patil. Of oaths and checklists. https://www.oreilly.com/radar/of-oaths-and-checklists/, 2018. [Accessed August 7, 2023].\\n\\nAlexandra Sasha Luccioni, Frances Corry, Hamsini Sridharan, Mike Ananny, Jason Schultz, and Kate Crawford. A framework for deprecating datasets: Standardizing documentation, identification, and communication. In ACM Conference on Fairness, Accountability, and Transparency (FAccT), pages 199\u2013212, 2022.\\n\\nDebbie S Ma, Joshua Correll, and Bernd Wittenbrink. The chicago face database: A free stimulus set of faces and norming data. Behavior research methods, 47(4):1122\u20131135, 2015.\\n\\nDebbie S Ma, Justin Kantner, and Bernd Wittenbrink. Chicago face database: Multiracial expansion. Behavior Research Methods, 53(3):1289\u20131300, 2021.\\n\\nMichael A Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. Co-designing checklists to understand organizational challenges and opportunities around fairness in ai. In Proceedings of the 2020 CHI conference on human factors in computing systems, pages 1\u201314, 2020.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Nicolas Malev\u00e9. On the data set's ruins. AI & SOCIETY, pages 1\u201315, 2020.\\n\\nGianclaudio Malgieri and J\u0119drzej Niklas. Vulnerable data subjects. Computer Law & Security Review, 37:105415, 2020.\\n\\nVarun Manjunatha, Nirat Saini, and Larry S Davis. Explicit bias discovery in visual question answering models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9562\u20139571, 2019.\\n\\nDavid Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In IEEE/CVF International Conference on Computer Vision (ICCV), pages 416\u2013423, 2001.\\n\\nNatalia L Martinez, Martin A Bertran, Afroditi Papadaki, Miguel Rodrigues, and Guillermo Sapiro. Blind pareto fairness and subgroup robustness. In International Conference on Machine Learning, pages 7492\u20137501. PMLR, 2021.\\n\\nDeborah Mascalzoni, Roberto Melotti, Cristian Pattaro, Peter Paul Pramstaller, Martin G\u00f6gele, Alessandro De Grandi, and Roberta Biasiotto. Ten years of dynamic consent in the chris study: informed consent as a dynamic process. European Journal of Human Genetics, 30(12):1391\u20131397, 2022.\\n\\nAnna McKie. South african university drops gender titles in student correspondence. https://www.timeshighereducation.com/news/south-african-university-drops-gender-titles-student-correspondence, July 2018.\\n\\nRichard McPherson, Reza Shokri, and Vitaly Shmatikov. Defeating image obfuscation with deep learning. arXiv preprint arXiv:1609.00408, 2016.\\n\\nHelen Meng, PC Ching, Tan Lee, Man Wai Mak, Brian Mak, Y Moon, Man-Hung Siu, Xiaoou Tang, H Hui, Andrew Lee, et al. The multi-biometric, multi-device and multilingual (m3) corpus. In Proc. Workshop Multimodal User Authentication, 2006.\\n\\nSachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. Pulse: Self-supervised photo upsampling via latent space exploration of generative models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2437\u20132445, 2020.\\n\\nMichele Merler, Nalini Ratha, Rogerio S Feris, and John R Smith. Diversity in faces. arXiv preprint arXiv:1901.10436, 2019.\\n\\nJacob Metcalf. \u201cthe study has been approved by the irb\u201d': Gayface AI, research hype and the pervasive data ethics gap. Pervade Team, Nov, 2017.\\n\\nJacob Metcalf and Kate Crawford. Where are human subjects in big data research? the emerging ethics divide. Big Data & Society, 3(1):2053951716650211, 2016.\\n\\nMilagros Miceli, Martin Schuessler, and Tianling Yang. Between subjectivity and imposition: Power dynamics in data annotation for computer vision. Proceedings of the ACM on Human-Computer Interaction, 4(CSCW2):1\u201325, 2020.\\n\\nAlex Mihailidis, Brent Carmichael, and Jennifer Boger. The use of computer vision in an intelligent environment to support aging-in-place, safety, and independence in the home. IEEE Transactions on Information Technology in Biomedicine, 8(3):238\u2013247, 2004.\\n\\nEric Mintun, Alexander Kirillov, and Saining Xie. On interaction between augmentations and corruptions in natural corruption robustness. In Advances in Neural Information Processing Systems (NeurIPS), pages 3571\u20133583, 2021.\\n\\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In ACM Conference on Fairness, Accountability, and Transparency (FAccT), pages 220\u2013229, 2019.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Qf8uzIT1OK", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Seong Joon Oh, Rodrigo Benenson, Mario Fritz, and Bernt Schiele. Faceless person recognition: Privacy implications in social media. In European Conference on Computer Vision (ECCV), pages 19\u201335. Springer, 2016.\\n\\nOpenReview. Fairface: A novel face attribute dataset for bias measurement and mitigation. https://openreview.net/forum?id=S1xSSTNKDB, 2019. [Accessed August 1, 2022].\\n\\nRoy Or-El, Soumyadip Sengupta, Ohad Fried, Eli Shechtman, and Ira Kemelmacher-Shlizerman. Lifespan age transformation synthesis. In European Conference on Computer Vision (ECCV), pages 739\u2013755. Springer, 2020.\\n\\nTribhuvanesh Orekondy, Mario Fritz, and Bernt Schiele. Connecting pixels to privacy and utility: Automatic redaction of private information in images. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8466\u20138475, 2018.\\n\\nOrestis Papakyriakopoulos, Anna Seo Gyeong Choi, William Thong, Dora Zhao, Jerone Andrews, Rebecca Bourke, Alice Xiang, and Allison Koenecke. Augmented datasheets for speech datasets and ethical decision-making. In ACM Conference on Fairness, Accountability, and Transparency (FAccT), pages 881\u2013904, 2023.\\n\\nOmkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman. Deep face recognition. 2015.\\n\\nAmandalynne Paullada, Inioluwa Deborah Raji, Emily M Bender, Emily Denton, and Alex Hanna. Data and its (dis) contents: A survey of dataset development and use in machine learning research. Patterns, 2(11):100336, 2021.\\n\\nNikita Pavlichenko, Ivan Stelmakh, and Dmitry Ustalov. Crowdspeech and voxdiy: Benchmark datasets for crowdsourced audio transcription. In Advances in Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS D&B), 2021.\\n\\nKenny Peng, Arunesh Mathur, and Arvind Narayanan. Mitigating dataset harms requires stewardship: Lessons from 1000 papers. In Advances in Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS D&B), 2021.\\n\\nBilly Perrigo. Inside Facebook\u2019s African sweatshop. https://time.com/6147458/facebook-africa-content-moderation-employee-treatment/, February 2022.\\n\\nMark Phillips. International data-sharing norms: From the OECD to the General Data Protection Regulation (GDPR). Human genetics, 137:575\u2013582, 2018.\\n\\nP Jonathon Phillips, Fang Jiang, Abhijit Narvekar, Julianne Ayyad, and Alice J O\u2019Toole. An other-race effect for face recognition algorithms. ACM Transactions on Applied Perception (TAP), 8(2):1\u201311, 2011.\\n\\nTrisha Phillips. Exploitation in payments to research subjects. Bioethics, 25(4):209\u2013219, 2011.\\n\\nAJ Piergiovanni and Michael Ryoo. Avid dataset: Anonymized videos from diverse countries. Advances in Neural Information Processing Systems (NeurIPS), pages 16711\u201316721, 2020.\\n\\nEugenia Politou, Efthimios Alepis, and Constantinos Patsakis. Forgetting personal data and revoking consent under the GDPR: Challenges and proposed solutions. Journal of cybersecurity, 4(1):tyy001, 2018.\\n\\nBilal Porgali, V\u00edtor Albiero, Jordan Ryda, Cristian Canton Ferrer, and Caner Hazirbas. The casual conversations v2 dataset. arXiv preprint arXiv:2303.04838, 2023.\\n\\nW Nicholson Price and I Glenn Cohen. Privacy in the age of medical big data. Nature medicine, 25(1):37\u201343, 2019.\\n\\nMattia Prosperi and Jiang Bian. Is it time to rethink institutional review boards for the era of big data? Nature Machine Intelligence, 1(6):260\u2013260, 2019.\\n\\nCarina EA Prunkl, Carolyn Ashurst, Markus Anderljung, Helena Webb, Jan Leike, and Allan Dafoe. Institutionalizing ethics in AI through broader impact requirements. Nature Machine Intelligence, 3(2):104\u2013110, 2021.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mahima Pushkarna, Andrew Zaldivar, and Oddur Kjartansson. Data cards: Purposeful and transparent dataset documentation for responsible AI. In ACM Conference on Fairness, Accountability, and Transparency (FAccT), pages 1776\u20131826, 2022.\\n\\nDeborah Raji, Emily Denton, Emily M. Bender, Alex Hanna, and Amandalynne Paullada. AI and the everything in the whole wide world benchmark. In Advances in Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS D&B), 2021.\\n\\nInioluwa Deborah Raji and Genevieve Fried. About face: A survey of facial recognition evaluation. 2021.\\n\\nInioluwa Deborah Raji, Morgan Klaus Scheuerman, and Razvan Amironesei. You can\u2019t sit with us: Exclusionary pedagogy in AI ethics education. In ACM conference on Fairness, Accountability, and Transparency (FAccT), pages 515\u2013525, 2021.\\n\\nVikram V Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron B Adcock, Laurens van der Maaten, Deepti Ghadiyaram, and Olga Russakovsky. Geode: a geographically diverse evaluation dataset for object recognition. arXiv preprint arXiv:2301.02560, 2023.\\n\\nDaniel A Reid and Mark S Nixon. Using comparative human descriptions for soft biometrics. In International Joint Conference on Biometrics (IJCB), pages 1\u20136. IEEE, 2011.\\n\\nKarl Ricanek and Tamirat Tesafaye. Morph: A longitudinal image database of normal adult age-progression. In 7th international conference on automatic face and gesture recognition (FGR06), pages 341\u2013345. IEEE, 2006.\\n\\nRashida Richardson, Jason M Schultz, and Kate Crawford. Dirty data, bad predictions: How civil rights violations impact police data, predictive policing systems, and justice. NYUL Rev. Online, 94:15, 2019.\\n\\nErgys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. Performance measures and a data set for multi-target, multi-camera tracking. In European Conference on Computer Vision (ECCV), pages 17\u201335. Springer, 2016.\\n\\nJoseph P Robinson, Gennady Livitz, Yann Henon, Can Qin, Yun Fu, and Samson Timoner. Face recognition: too bias, or not too bias? In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 0\u20131, 2020.\\n\\nWilliam A Gaviria Rojas, Sudnya Diamos, Keertan Ranjan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. The dollar street dataset: Images representing the geographic and socioeconomic diversity of the world. In Advances in Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS D&B), 2022.\\n\\nRata Rokhshad, Maxime Ducret, Akhilanand Chaurasia, Teodora Karteva, Miroslav Radenkovic, Jelena Roganovic, Manal Hamdan, Hossein Mohammad-Rahimi, Joachim Krois, Pierre Lahoud, et al. Ethical considerations on artificial intelligence in dentistry: A framework and checklist. Journal of Dentistry, page 104593, 2023.\\n\\nNorma RA Romm. Interdisciplinary practice as reflexivity. Systemic Practice and Action Research, 11:63\u201377, 1998.\\n\\nAdam Rose. Are face-detection cameras racist? http://content.time.com/time/business/article/0,8599,1954643-1,00.html, January 2010.\\n\\nAmir Rosenfeld, Richard Zemel, and John K Tsotsos. The elephant in the room. arXiv preprint arXiv:1808.03305, 2018.\\n\\nNegar Rostamzadeh, Diana Mincu, Subhrajit Roy, Andrew Smart, Lauren Wilcox, Mahima Pushkarna, Jessica Schrouff, Razvan Amironesei, Nyalleng Moorosi, and Katherine Heller. Healthsheet: development of a transparency artifact for health datasets. In ACM Conference on Fairness, Accountability, and Transparency (FAccT), pages 1943\u20131961, 2022.\\n\\nWendy Roth. Race migrations: Latinos and the cultural transformation of race. Stanford University Press, 2012.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Wendy D Roth. The multiple dimensions of race. Ethnic and Racial Studies, 39(8):1310\u20131338, 2016.\\n\\nMyron Rothbart and Marjorie Taylor. Category labels and social reality: Do we view social categories as natural kinds? 1992.\\n\\nRasmus Rothe, Radu Timofte, and Luc Van Gool. Dex: Deep expectation of apparent age from a single image. In IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), pages 10\u201315, 2015.\\n\\nRasmus Rothe, Radu Timofte, and Luc Van Gool. Deep expectation of real and apparent age from a single image without facial landmarks. International Journal of Computer Vision, 126(2):144\u2013157, 2018.\\n\\nGavin Rowe, Paul Willner. Alcohol servers\u2019 estimates of young people\u2019s ages. Drugs: education, prevention and policy, 8(4):375\u2013383, 2001.\\n\\nJoanna R\u00f3\u017cy\u0144ska. The ethical anatomy of payment for research participants. Medicine, Health Care and Philosophy, 25(3):449\u2013464, 2022.\\n\\nShiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks. In International Conference on Learning Representations (ICLR), 2020.\\n\\nNithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. \u201ceveryone wants to do the model work, not the data work\u201d: Data cascades in high-stakes AI. In Conference on Human Factors in Computing Systems (CHI). ACM, may 2021. doi: 10.1145/3411764.3445518.\\n\\nSudeep Sarkar, P Jonathon Phillips, Zongyi Liu, Isidro Robledo Vega, Patrick Grother, and Kevin W Bowyer. The humanid gait challenge problem: Data sets, performance, and analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(2):162\u2013177, 2005.\\n\\nMorgan Klaus Scheuerman, Jacob M Paul, and Jed R Brubaker. How computers see gender: An evaluation of gender classification in commercial facial analysis services. Proceedings of the ACM on Human-Computer Interaction, 3(CSCW):1\u201333, 2019.\\n\\nMorgan Klaus Scheuerman, Katta Spiel, Oliver L Haimson, Foad Hamidi, and Stacy M Brannham. Hci guidelines for gender equity and inclusivity. https://www.morgan-klaus.com/gender-guidelines, May 2020. [Accessed August 1, 2022].\\n\\nMorgan Klaus Scheuerman, Kandrea Wade, Caitlin Lustig, and Jed R Brubaker. How we\u2019ve taught algorithms to see identity: Constructing race and gender in image databases for facial analysis. Proceedings of the ACM on Human-computer Interaction, 4(CSCW1):1\u201335, 2020.\\n\\nMorgan Klaus Scheuerman, Alex Hanna, and Emily Denton. Do datasets have politics? disciplinary values in computer vision dataset development. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW2):1\u201337, 2021.\\n\\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\\n\\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems (NeurIPS), 35:25278\u201325294, 2022.\\n\\nCandice Schumann, Susanna Ricco, Utsav Prabhu, Vittorio Ferrari, and Caroline Rebecca Pantofaru. A step toward more inclusive people annotations for fairness. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES), 2021.\\n\\nMarshall H Segall, Donald Thomas Campbell, and Melville Jean Herskovits. The influence of culture on visual perception. Bobbs-Merrill Indianapolis, 1966.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson, and D Sculley. No classification without representation: Assessing geodiversity issues in open data sets for the developing world. arXiv preprint arXiv:1711.08536, 2017.\\n\\nBoaz Shmueli, Jan Fell, Soumya Ray, and Lun-Wei Ku. Beyond fair pay: Ethical implications of nlp crowdsourcing. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 3758\u20133769, 2021.\\n\\nHera Siddiqui, Ajita Rattani, Karl Ricanek, and Twyla Hill. An examination of bias of facial analysis based bmi prediction models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2926\u20132935, 2022.\\n\\nLaura Silver. Smartphone ownership is growing rapidly around the world, but not always equally. https://www.pewresearch.org/global/2019/02/05/smartphone-ownership-is-growing-rapidly-around-the-world-but-not-always-equally/, August 2020.\\n\\nEleanor Singer and Mick P Couper. Some methodological uses of responses to open questions and other verbatim comments in quantitative surveys. Methods, data, analyses: a journal for quantitative methods and survey methodology (mda), 11(2):115\u2013134, 2017.\\n\\nRicha Singh, Mayank Vatsa, Himanshu S Bhatt, Samarth Bharadwaj, Afzel Noore, and Shahin S Nooreyezdan. Plastic surgery: A new dimension to face recognition. IEEE Transactions on Information Forensics and Security, 5(3):441\u2013448, 2010.\\n\\nJolene D Smyth, Don A Dillman, Leah Melani Christian, and Mallory McBride. Open-ended questions in web surveys: Can increasing the size of answer boxes and providing extra verbal instructions improve response quality? Public Opinion Quarterly, 73(2):325\u2013337, 2009.\\n\\nJacob Snow. Amazon's face recognition falsely matched 28 members of congress with mugshots. https://www.aclu.org/news/privacy-technology/amazons-face-recognition-falsely-matched-28, July 2018.\\n\\nBenjamin Sobel. A taxonomy of training data: Disentangling the mismatched rights, remedies, and rationales for restricting machine learning. Artificial Intelligence and Intellectual Property (Reto Hilty, Jyh-An Lee, Kung-Chung Liu, eds.), Oxford University Press, Forthcoming, 2020.\\n\\nOlivia Solon. Facial recognition's 'dirty little secret': Millions of online photos scraped without consent. NBC News, 2019.\\n\\nGowri Somanath, MV Rohith, and Chandra Kambhamettu. Vadana: A dense dataset for facial image analysis. In IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), pages 2175\u20132182, 2011.\\n\\nPatrik S\u00f6rqvist, Linda Langeborg, and M\u00e5rten Eriksson. Women assimilate across gender, men don't: The role of gender to the own-anchor effect in age, height, and weight estimates 1. Journal of Applied Social Psychology, 41(7):1733\u20131748, 2011.\\n\\nKatta Spiel, Oliver L Haimson, and Danielle Lottridge. How to do better with gender on surveys: a guide for hci researchers. Interactions, 26(4):62\u201365, 2019.\\n\\nAaron Springer, Jean Garcia-Gathright, and Henriette Cramer. Assessing and addressing algorithmic bias\u2014but before we get there... In AAAI Spring Symposia, 2018.\\n\\nMadhulika Srikumar, Rebecca Finlay, Grace Abuhamad, Carolyn Ashurst, Rosie Campbell, Emily Campbell-Ratcliffe, Hudson Hongo, Sara R Jordan, Joseph Lindley, Aviv Ovadya, et al. Advancing ethics review practices in ai research. Nature Machine Intelligence, 4(12):1061\u20131064, 2022.\\n\\nRamya Srinivasan, Emily Denton, Jordan Famularo, Negar Rostamzadeh, Fernando Diaz, and Beth Coleman. Artsheets for art datasets. In Advances in Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS D&B), 2021.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ryan Steed and Aylin Caliskan. Image representations learned with unsupervised pre-training contain human-like biases. In ACM Conference on Fairness, Accountability, and Transparency (FAccT), pages 701\u2013713, 2021.\\n\\nNikki Stevens. Open demographics documentation. https://nikkistevens.com/open-demographics/index.htm, n.d. [Accessed November 22, 2021].\\n\\nRussell Stewart, Mykhaylo Andriluka, and Andrew Y Ng. End-to-end people detection in crowded scenes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2325\u20132333, 2016.\\n\\nQianru Sun, Liqian Ma, Seong Joon Oh, Luc Van Gool, Bernt Schiele, and Mario Fritz. Natural and effective obfuscation by head inpainting. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5050\u20135059, 2018.\\n\\nHarini Suresh and John Guttag. A framework for understanding sources of harm throughout the machine learning life cycle. In Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO). 2021.\\n\\nKeiko Tagai, Hitomi Ohtaka, and Hiroshi Nittono. Faces with light makeup are better recognized than faces with heavy makeup. Frontiers in psychology, 7:226, 2016.\\n\\nHarriet JA Teare, Megan Prictor, and Jane Kaye. Reflections on dynamic consent in biomedical research: the story so far. European journal of human genetics, 29(4):649\u2013656, 2021.\\n\\nTech Inquiry. Official response from wiley. https://techinquiry.org/WileyResponse.html, 2019. [Accessed June 30, 2022].\\n\\nEdward E Telles. Racial ambiguity among the Brazilian population. Ethnic and racial studies, 25(3):415\u2013441, 2002.\\n\\nGraham Thomas, Rikke Gade, Thomas B Moeslund, Peter Carr, and Adrian Hilton. Computer vision for sports: Current applications and research topics. Computer Vision and Image Understanding, 159:3\u201318, 2017.\\n\\nWilliam Thong, Przemyslaw Joniak, and Alice Xiang. Beyond skin tone: A multidimensional measure of apparent skin color. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4903\u20134913, 2023.\\n\\nSchrasing Tong and Lalana Kagal. Investigating bias in image classification using model explanations. arXiv preprint arXiv:2012.05463, 2020.\\n\\nAntonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large dataset for nonparametric object and scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(11):1958\u20131970, 2008.\\n\\nCarlos Toxtli, Siddharth Suri, and Saiph Savage. Quantifying the invisible labor in crowd work. Proceedings of the ACM on human-computer interaction, 5(CSCW2):1\u201326, 2021.\\n\\nJohn Twigg. The Right to Safety: some conceptual and practical issues. Benfield Hazard Research Centre, 2003.\\n\\nRies Uittenbogaard, Clint Sebastian, Julien Vijverberg, Bas Boom, Dariu M Gavrila, et al. Privacy protection in street-view panoramas using depth and multi-view imagery. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10581\u201310590, 2019.\\n\\nUK Information Commissioner\u2019s Office. What do we need to do to ensure lawfulness, fairness, and transparency in AI systems? https://ico.org.uk/for-organisations/guide-to-data-protection/key-dataprotection-themes/guidance-on-ai-and-data-protection/what-do-we-need-to-do-to-ensure-lawfulness-fairness-and-transparency-in-ai-systems/, 2020. [Accessed June 30, 2022].\"}"}
{"id": "Qf8uzIT1OK", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Qf8uzIT1OK", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Angelina Wang, Alexander Liu, Ryan Zhang, Anat Kleiman, Leslie Kim, Dora Zhao, Iroha Shirai, Arvind Narayanan, and Olga Russakovsky. Revise: A tool for measuring and mitigating bias in visual datasets. volume 130, pages 1790\u20131810. Springer, 2022.\\n\\nJialu Wang, Yang Liu, and Caleb Levy. Fair classification with group-dependent label noise. In ACM Conference on Fairness, Accountability, and Transparency (FAccT), pages 526\u2013536, 2021.\\n\\nMei Wang and Weihong Deng. Mitigating bias in face recognition using skewness-aware reinforcement learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9322\u20139331, 2020.\\n\\nMei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, and Yaohai Huang. Racial faces in the wild: Reducing racial bias by information maximization adaptation network. In IEEE/CVF International Conference on Computer Vision (ICCV), 2019.\\n\\nYilun Wang and Michal Kosinski. Deep neural networks are more accurate than humans at detecting sexual orientation from facial images. Journal of personality and social psychology, 114(2):246, 2018.\\n\\nZe Wang, Xin He, and Fan Liu. Examining the effect of smile intensity on age perceptions. Psychological reports, 117(1):188\u2013205, 2015.\\n\\nZeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, and Olga Russakovsky. Towards fairness in visual recognition: Effective strategies for bias mitigation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8919\u20138928, 2020.\\n\\nOlivia R Ware, Jessica E Dawson, Michi M Shinohara, and Susan C Taylor. Racial limitations of fitzpatrick skin type. Cutis, 105(2):77\u201380, 2020.\\n\\nGriffin M Weber, Kenneth D Mandl, and Isaac S Kohane. Finding the missing link for big biomedical data. Jama, 311(24):2479\u20132480, 2014.\\n\\nDavid Wen, Saad M Khan, Antonio Ji Xu, Hussein Ibrahim, Luke Smith, Jose Caballero, Luis Zepeda, Carlos de Blas Perez, Alastair K Denniston, Xiaoxuan Liu, et al. Characteristics of publicly available skin cancer image datasets: a systematic review. The Lancet Digital Health, 4(1):e64\u2013e74, 2022.\\n\\nEdgar A Whitley. Informational privacy, consent and the \u201ccontrol\u201d of personal data. Information security technical report, 14(3):154\u2013159, 2009.\\n\\nMeredith Whittaker, Meryl Alper, Cynthia L Bennett, Sara Hendren, Liz Kaziunas, Mara Mills, Meredith Ringel Morris, Joy Rankin, Emily Rogers, Marcel Salas, et al. Disability, bias, and ai. AI Now Institute, page 8, 2019.\\n\\nBenjamin Wilson, Judy Hoffman, and Jamie Morgenstern. Predictive inequity in object detection. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2019.\\n\\nLloyd Windrim, Arman Melkumyan, Richard Murphy, Anna Chlingaryan, and Juan Nieto. Unsupervised feature learning for illumination robustness. In IEEE International Conference on Image Processing (ICIP), pages 4453\u20134457, 2016.\\n\\nWorld Economic Forum. The digital revolution is leaving poorer kids behind. https://www.weforum.org/agenda/2022/04/the-digital-revolution-is-leaving-poorer-kids-behind/, 2022. [Accessed November 24, 2022].\\n\\nWorld Health Organization. Ageism in artificial intelligence for health. https://www.who.int/publications/i/item/9789240040793, 2022. [Accessed November 24, 2022].\\n\\nWorld Health Organization and others. Ethics and governance of artificial intelligence for health: Who guidance. 2021.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"David Wright. A framework for the ethical impact assessment of information technology. *Ethics and Information Technology*, 13:199\u2013226, 2011.\\n\\nXiaolin Wu and Xi Zhang. Automated inference on criminality using face images. *arXiv* preprint arXiv:1611.04135, pages 4038\u20134052, 2016.\\n\\nAlice Xiang. Being 'seen' vs. 'mis-seen': Tensions between privacy and fairness in computer vision. *Harvard Journal of Law & Technology*, Forthcoming, 2022.\\n\\nRongchang Xie, Fei Yu, Jiachao Wang, Yizhou Wang, and Li Zhang. Multi-level domain adaptive learning for cross-domain detection. In *IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)*, 2019.\\n\\nYuanjun Xiong, Kai Zhu, Dahua Lin, and Xiaoou Tang. Recognize complex events from static images by fusing deep channels. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 1600\u20131609, 2015.\\n\\nRunhua Xu, Nathalie Baracaldo, and James Joshi. Privacy-preserving machine learning: Methods, challenges and directions. *arXiv preprint arXiv:2108.04417*, 2021.\\n\\nTian Xu, Jennifer White, Sinan Kalkan, and Hatice Gunes. Investigating bias and fairness in facial expression recognition. In *European Conference on Computer Vision Workshops (ECCVW)*, pages 506\u2013523. Springer, 2020.\\n\\nXin Xu and Jie Wang. Extended non-local feature for visual saliency detection in low contrast images. In *European Conference on Computer Vision Workshops (ECCVW)*, 2019.\\n\\nBlaise Ag\u00fcera y Arcas, Margaret Mitchell, and Alexander Todorov. Physiognomy's new clothes. https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a, 2017. [Accessed October 22, 2022].\\n\\nKaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga Russakovsky. Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy. In *ACM Conference on Fairness, Accountability, and Transparency (FAccT)*, pages 547\u2013558, 2020.\\n\\nKaiyu Yang, Jacqueline H Yau, Li Fei-Fei, Jia Deng, and Olga Russakovsky. A study of face obfuscation in imagenet. In *International Conference on Machine Learning (ICML)*, pages 25313\u201325330. PMLR, 2022.\\n\\nShuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang. Wider face: A face detection benchmark. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 5525\u20135533, 2016.\\n\\nYu Yang, Aayush Gupta, Jianwei Feng, Prateek Singhal, Vivek Yadav, Yue Wu, Pradeep Natarajan, Varsha Hedau, and Jungseock Joo. Enhancing fairness in face detection in computer vision systems by demographic bias mitigation. In *Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES)*, pages 813\u2013822, 2022.\\n\\nRui-Jie Yew and Alice Xiang. Regulating facial processing technologies: Tensions between legal and technical considerations in the application of Illinois BIPA. In *ACM Conference on Fairness, Accountability, and Transparency (FAccT)*, page 1017\u20131027, 2022.\\n\\nDong Yin, Raphael Gontijo Lopes, Jon Shlens, Ekin Dogus Cubuk, and Justin Gilmer. A fourier perspective on model robustness in computer vision. In *Advances in Neural Information Processing Systems (NeurIPS)*, 2019.\\n\\nSeyma Yucer, Furkan Tektas, Noura Al Moubayed, and Toby P Breckon. Measuring hidden bias within face recognition via racial phenotypes. In *IEEE Winter Conference on Applications of Computer Vision (WACV)*, pages 995\u20131004, 2022.\\n\\nZhanpeng Zhang, Ping Luo, Chen-Change Loy, and Xiaoou Tang. Facial landmark detection by deep multi-task learning. In *European Conference on Computer Vision (ECCV)*, pages 94\u2013108. Springer, 2014.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial autoencoder. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5810\u20135818, 2017.\\n\\nDora Zhao, Angelina Wang, and Olga Russakovsky. Understanding and evaluating racial biases in image captioning. In IEEE/CVF International Conference on Computer Vision (ICCV), 2021.\\n\\nDora Zhao, Jerone T. A. Andrews, and Alice Xiang. Men also do laundry: Multi-attribute bias amplification. In International Conference on Machine Learning (ICML), 2023.\\n\\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Men also like shopping: Reducing gender bias amplification using corpus-level constraints. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2017.\\n\\nMingyuan Zhou, Haiting Lin, S. Susan Young, and Jingyi Yu. Hybrid sensing face detection and registration for low-light and unconstrained conditions. Applied Optics, 57(1):69\u201378, January 2018.\\n\\nMichael Zimmer. \u201cbut the data is already public\u201d: On the ethics of research in facebook. Ethics and Information Technology, 12(4):313\u2013325, 2010.\\n\\nMatthew Zook, Solon Barocas, Danah Boyd, Kate Crawford, Emily Keller, Seeta Pe\u00f1a Gangadharan, Alyssa Goodman, Rachelle Hollander, Barbara A Koenig, Jacob Metcalf, Arvind Narayanan, Alondra Nelson, and Frank Pasquale. Ten simple rules for responsible big data research, 2017.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Responsible Data Curation Checklist for Fairness and Robustness Evaluations\\n\\nThis checklist translates our HCCV data curation considerations and recommendations into action items for researchers and practitioners. Presented as a series of questions, these items are designed to stimulate discussions among data collection teams. The questions are purposefully worded to avoid binary responses, encouraging open-ended dialogues. The primary focus of the checklist is to underscore the ethical dimensions and ensure that teams address concerns encompassing purpose, consent and privacy, as well as diversity.\\n\\nIt is important to engage with the checklist as a preliminary exercise before beginning data collection. This approach promotes informed decision-making and minimizes risks, leading to more responsible and reliable outcomes.\\n\\nContextual diversity is acknowledged to avoid a one-size-fits-all approach. Moreover, customization is encouraged, as not all items apply universally; teams should modify or expand the checklist to align with their context and use case. As with existing AI ethics checklists [90, 201, 205, 225, 269, 357], it is important to recognize that the checklist is not a guarantee for ethical compliance; rather, it functions as a catalyst for discussion and reflection.\\n\\nWe understand that answering these questions is time-consuming, increasing the burden on data collection teams whose work is already undervalued [247, 282]. Therefore, when navigating through these lists, priority should be put on items related to the specific domain and task of interest. The level of engagement needed for each question will invariably differ. Keep in mind that the questions aim to spur ethical thinking during dataset development: \u201cEthics is often about finding a good or better, but not perfect, answer\u201d [380].\\n\\nA.1 Purpose\\n\\nThe questions in this section focus on eliciting strategies for curating HCCV evaluation datasets specifically for fairness and robustness assessments. They seek alignment with objectives and inquire about factors known to influence these assessments to ensure comprehensive evaluations. Moreover, the questions aim to assist in formulating clear dataset purpose statements, preventing ambiguity and misuse of data, as well as exploring external validation to enhance transparency and accountability.\\n\\nDataset Development Strategy\\n\\n\u2022 Can you provide details about your strategy for developing a new dataset tailored specifically for conducting fairness and robustness assessments in the context of HCCV? How do you plan to ensure that this dataset is aligned with the objectives of evaluating fairness and robustness?\\n\\n\u2022 Can you elaborate on the factors your dataset will encompass to comprehensively enable fairness and robustness evaluations for HCCV models? How do you intend to capture the primary factors, including data subjects, instruments, and environments, that influence these evaluations?\\n\\nDataset Purpose Statement\\n\\n\u2022 Can you provide details about your plan to formulate a comprehensive dataset purpose statement? How will this statement effectively communicate the core motivations driving, e.g., data collection, outline the intended dataset composition, specify permissible uses of the data, and identify the specific audience you aim to serve with the dataset?\\n\\n\u2022 Can you elaborate on your strategy for ensuring the accuracy and ethical alignment of your dataset\u2019s purpose statement? How do you plan to externally validate the content and ethical considerations of the statement?\\n\\n\u2022 Can you provide insights into the benefits and implications of submitting your dataset\u2019s purpose statement as part of a research study proposal in the format of a registered report for your project?\"}"}
{"id": "Qf8uzIT1OK", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.2 Consent and Privacy\\n\\nThe questions in this section explore informed consent, legal compliance, and privacy protection measures within anonymization strategies. The questions emphasize clarity and voluntariness in consent processes to prevent coercion or misuse of data. Moreover, they attempt to elicit strategies for explaining data collection purposes, consent revocation, and accommodating diverse participation circumstances. Furthermore, the questions seek insights into addressing anonymization challenges, aiming to prevent re-identification risks, unauthorized exposure, and legal noncompliance, while preserving data utility and protecting data subjects' rights.\\n\\nInformed and Voluntary Consent\\n\\n\u2022 Can you elaborate on your approach to ensuring that you secure explicit, voluntary, and informed consent from all individuals who either appear in the dataset or can be discerned from it? How do you plan to handle consent for data annotators who may have disclosed personal information for the purposes of quantifying and addressing annotator perspectives and bias?\\n\\n\u2022 Can you provide a comprehensive explanation of your strategy for conveying the purpose of data collection to the subjects? How do you intend to emphasize the utilization of their data, which includes various types of information such as facial, body, biometric images, as well as information about themselves and their environment, all in the context of assessing the fairness and robustness of HCCV systems?\\n\\n\u2022 In what ways will you incorporate consent forms that are composed in plain language to enhance the understanding of AI technologies? How do you plan to make sure these forms effectively convey the intricacies of data usage?\\n\\n\u2022 How do you plan to inform data subjects about their ability to withdraw consent at any given point during, or after, the data collection process? Can you provide details about the mechanisms you will have in place for facilitating this?\\n\\n\u2022 Please provide insight into your strategy for collecting data from individuals below the age of majority or vulnerable individuals. How will you seek both guardian consent and voluntary informed assent in such cases?\\n\\n\u2022 How do you plan to evaluate vulnerability along a continuous spectrum, taking into account contextual factors and recognizing that vulnerability is not solely binary or based solely on group affiliations, but can also be influenced by specific situations or circumstances?\\n\\n\u2022 Can you also provide details about how you will consider the circumstances of participation, which might include the potential need for participatory design, assurances of compensation, provision of educational materials, and safeguards against authoritative structures? How will you address these various aspects in your approach?\\n\\n\u2022 How do you intend to ensure that vulnerable individuals have a comprehensive understanding of the data usage and willingly provide informed assent? Can you outline the specific measures you intend to implement for this purpose?\\n\\n\u2022 Can you elaborate on how you will respect the decision of a vulnerable individual who expresses dissent, regardless of the preferences of their guardian?\\n\\nConsent Revocation Mechanisms\\n\\n\u2022 How do you plan to integrate mechanisms that allow data subjects to easily withdraw their consent? Can you provide specifics on how this process will be designed and executed?\\n\\n\u2022 Can you provide insights into the benefits and implications of implementing dynamic consent mechanisms that utilize personalized communication interfaces? How do you intend to ensure that these mechanisms adapt to the preferences and needs of individual data subjects?\\n\\n\u2022 How do you intend to enable data subjects to actively participate in research activities and manage their consent preferences? Can you provide more details about the tools or processes you plan to put in place to achieve this?\"}"}
{"id": "Qf8uzIT1OK", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In what ways will you explore the feasibility of online platforms for consent management that are user-friendly and minimize complexity for data subjects? What steps will you take to ensure easy accessibility?\\n\\nCan you provide insights into the options you will provide to data subjects for granting consent? How will you offer choices between blanket consent, case-by-case selection, or opt-in based on specific data usage?\\n\\nCan you elaborate on your considerations regarding the formation of a steering board or charitable trust composed of representative subjects from the dataset? How do you envision this entity contributing to decision-making processes?\\n\\nHow do you plan to empower data subjects to actively participate in decisions concerning the usage of their data? What mechanisms or channels will you establish to facilitate this involvement?\\n\\nCan you provide information about the method you will offer data subjects to easily and promptly revoke their consent? How will you ensure that this process is straightforward and accessible?\\n\\nHow do you intend to address varying levels of technological know-how and internet access among data subjects? Can you detail the measures you will take to accommodate these variations?\\n\\nWhat alternatives do you plan to offer for revoking consent that do not rely solely on online-based processes? How will you ensure that individuals with different needs and preferences can effectively revoke their consent?\\n\\nHow do you plan to assess the practicality and suitability of the chosen mechanisms for consent revocation, taking into account the expected dataset size and the resources available to you? What criteria will you use to evaluate their effectiveness?\\n\\nHow do you plan to address the fact that anonymization measures might not universally meet legal requirements in specific regions, necessitating additional considerations? Can you provide insights into your strategy for ensuring legal compliance while implementing anonymization?\\n\\nCan you elaborate on your approach to collecting information about the country of residence for each individual in your dataset? How do you intend to use this information to ensure legal compliance and address potential privacy concerns?\\n\\nHow do you plan to familiarize yourself with the data protection laws that are applicable in the countries of residence of your data subjects? Can you provide details about your process for gaining this understanding and how you will apply it to your data curation project?\\n\\nHow do you intend to prioritize safeguarding data subjects' rights as stipulated by the data protection laws in their respective countries? What steps will you take to ensure that the creation and utilization of the dataset strictly adhere to the relevant data protection regulations? Can you provide specifics about the measures you will put in place to achieve this?\\n\\nWhat mechanisms do you intend to implement to ensure the adaptability of your dataset management strategy to changing legislative requirements? Can you provide details about how you will monitor and accommodate legislative changes in your dataset management approach? Can you provide insights into how you will strike a balance between maintaining compliance and effective dataset management in dynamic legal environments?\\n\\nHow do you plan to implement measures that effectively safeguard against re-identification risks, encompassing singling out, linkability, and inference, within your anonymization approach?\\n\\nCan you elaborate on your strategy for redacting all image regions that could inadvertently disclose privacy-related information? How do you intend to comprehensively identify and address these regions?\"}"}
{"id": "Qf8uzIT1OK", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Can you elaborate on your strategy for the removal of elements such as body parts, clothing, and accessories for nonconsenting subjects to enhance privacy protection? Can you provide more details about the considerations and methods involved in this process?\\n\\nCan you elaborate on your strategy for the removal of text (possibly excluding copyright owner information) from the dataset's images to enhance privacy protection? Can you provide more details about the considerations and methods involved in this process?\\n\\nCan you explain your plan for empirically validating the chosen anonymization methods? How will you assess the methods' effectiveness in mitigating re-identification risks while preserving the utility of the data?\\n\\nCan you provide details about how human annotators will be engaged in the creation and verification of privacy leaking image region proposals for anonymization purposes? How will you ensure accuracy and consistency in this process?\\n\\nCan you provide details about how you intend to align region proposals predicted by algorithms with human judgment, addressing any potential failures or biases? Can you describe your strategy for maintaining a sensitive approach to these factors?\\n\\nWhat steps will you take to address jurisdiction-specific requirements that might necessitate human-generated proposals for biometric identifiers in order to comply with legal and regulatory standards?\\n\\nCan you elaborate on the measures you will take to prevent image metadata from inadvertently revealing unauthorized identifying information? How will you ensure that metadata remains privacy-conscious?\\n\\nHow will you identify specific metadata elements that you intend to retain to ensure a comprehensive understanding during the evaluation process? Can you provide examples of the types of metadata you plan to retain for this purpose?\\n\\nHow do you plan to replace or remove sensitive information within metadata while retaining its usefulness for fairness and robustness analyses? Can you provide insights into your approach for striking a balance in this regard?\\n\\nA.3 Diversity\\nThe questions in this section revolve around obtaining accurate image annotations related to identity, phenotype, environmental factors, and instruments, while upholding inclusivity, sensitivity, and privacy. Additionally, the questions attempt to elicit strategies for documenting identity, ensuring fair compensation, and effective (anonymous) communication.\\n\\nSelf-Reported Annotations\\nHow do you plan to acquire annotations for images directly from the data subjects, leveraging their self-awareness and contextual knowledge to enhance the accuracy and quality of annotations? Can you elaborate on the methods and strategies you intend to use for this purpose?\\n\\nCan you elaborate on your strategy for addressing biases and ensuring careful handling when inferring labels about individuals? Can you provide reasoning as to why labels about individuals will be inferred as opposed to being self-identified? How will you actively mitigate potential biases that may arise during the labeling process?\\n\\nHow do you intend to consider the implications of inferred labels, for example, in relation to data access request rights?\\n\\nVersatile and Inclusive Response Options\\nHow do you plan to enhance the accuracy and nuance of identity information collection by providing respondents with both closed-ended and open-ended response choices? Can you elaborate on your strategy for using open-ended responses to gather more detailed and comprehensive data?\\n\\nHow do you intend to ensure inclusivity and prevent any potential implications of exclusion in the response choices you offer?\"}"}
{"id": "Qf8uzIT1OK", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Can you elaborate on your preparedness to manage the coding and analysis effort required for processing open-ended responses? What effective strategies do you plan to implement for managing and analyzing the data collected from open-ended questions? How will you handle the potential complexities and variations that can arise from these responses, ensuring that the insights and information derived can be accurately captured and utilized?\\n\\nDynamic Nature of Identity\\n\\nHow do you plan to collect self-identified information on a per-image basis, accounting for the fact that identity is intrinsically contextual and temporal? Can you elaborate on your strategy for capturing nonstatic aspects of identity?\\n\\nCan you elaborate on your strategy for enabling data subjects to freely choose multiple identity categories without imposing any limitations? How will you ensure that subjects have the flexibility to express their identity in a comprehensive and unrestrictive manner?\\n\\nHow do you intend to address potential requests for per-image updates to self-identified information provided by subjects over time, respecting their autonomy? What factors have you considered in relation to the potential effects of permitting updates?\\n\\nDemographic Information\\n\\nHow do you plan to collect precise biological age in years from data subjects to ensure an accurate representation of their age?\\n\\nCan you elaborate on your approach to gathering pronoun information from data subjects to enhance gender inclusivity and mitigate the risk of misgendering? How will you ensure that respondents feel comfortable providing this information?\\n\\nCan you explain your strategy for gathering consistent ancestry information from data subjects? How will you approach the collection of this information in a sensitive and inclusive manner?\\n\\nHow do you intend to offer the option for data subjects not to disclose their sensitive attributes if they choose not to? Can you provide more details about how you will handle the sensitivity and privacy of these attributes?\\n\\nSensitive Attributes in Aggregate\\n\\nHow do you plan to collect voluntarily disclosed sensitive attributes such as disability and pregnancy status? Can you elaborate on your approach to respecting the willingness of data subjects to provide these details?\\n\\nCan you provide insight into your strategy for reporting sensitive attributes, such as disability and pregnancy status, in aggregate data while safeguarding subjects' safety and privacy? How do you intend to ensure that individual identities are protected?\\n\\nCan you elaborate on your approach to relying on credible and appropriate sources for the categorization and definitions of sensitive attributes like disability or difficulty? How will you account for the potential variations in these definitions based on cultural, identity, and historical contexts?\\n\\nPhenotypic and Neutral Performative Features\\n\\nHow do you plan to collect phenotypic attributes, encompassing characteristics such as skin color, eye color, hair type, hair color, height, and weight? Can you provide insights into your strategy for obtaining these attributes in a sensitive and comprehensive manner?\\n\\nCan you elaborate on your approach to collecting a diverse range of neutral performative features, including aspects such as facial hair, hairstyle, cosmetics, clothing, and accessories? How do you intend to ensure inclusivity and accuracy in capturing these features?\"}"}
{"id": "Qf8uzIT1OK", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How do you plan to gather data on environment-related factors, which encompass details such as image capture time, season, weather, ambient lighting, scene, geography, camera position, and camera distance? Can you provide insights into your strategy for capturing these factors accurately and comprehensively?\\n\\nCan you elaborate on your approach to collecting instrument-related factors concerning the imaging devices used, including aspects such as lens, sensor, stabilization, flash usage, and post-processing software? How do you intend to ensure accuracy in capturing these details?\\n\\nHow do you plan to obtain environment- and instrument-related information? Can you provide more details about the methods you will use, such as self-reporting, annotator estimation, and sourcing information from Exif metadata? How will you leverage contextual knowledge from image subjects to enhance data quality?\\n\\nCan you explain your approach to handling information such as precise geolocation and user-added details in Exif metadata that might contain personally identifying information? How will you ensure compliance with copyright regulations (if applicable) while maintaining privacy and adhering to ethical considerations?\\n\\nAnnotators as Contributors\\n\\nHow do you plan to document the identities of data annotators, including capturing demographic details such as pronouns, age, and ancestry? Can you provide insights into your strategy for gathering and preserving this information while respecting privacy and ensuring transparency?\\n\\nCan you elaborate on your approach to highlighting the contributions of annotators beyond data labeling in the dataset documentation after the curation process? How do you intend to accurately represent the multifaceted roles and contributions of annotators?\\n\\nHow do you plan to report the demographic information of annotators to analyze potential sources of bias in dataset annotations? Can you provide more details about your proposed approach for conducting this analysis while ensuring privacy and ethical considerations?\\n\\nFair Treatment and Compensation\\n\\nHow do you plan to ensure that all contributors receive compensation that exceeds the minimum hourly wage of their respective country or jurisdiction of residence? Can you provide insights into your compensation strategy to ensure fair and ethical remuneration?\\n\\nCan you elaborate on your approach to exploring alternative payment models, such as compensation based on the average hourly wage? How do you intend to determine a compensation structure that is both fair and reflective of contributors' efforts?\\n\\nHow will you establish direct communication channels between dataset creators and contributors? Can you provide more details about the methods you intend to implement for effective and transparent communication?\\n\\nWhat communication methods do you plan to explore that maintain the anonymity of contributors? Can you provide insights into your approach to balancing communication and privacy needs, such as using anonymous feedback forms?\\n\\nCan you provide information about your strategy for developing clear and accessible plain language guides to facilitate various tasks, such as image submission and data annotation? How do you plan to ensure that these guides effectively assist contributors?\\n\\nHow do you intend to ensure that contributors from diverse backgrounds can easily understand and follow any instructions provided? Can you elaborate on your approach to promoting inclusivity and accessibility in your communication and guidelines?\\n\\nCan you provide details about how you plan to subject your recruitment and compensation procedures to ethics review? What steps will you take to ensure that your procedures align with ethical considerations and best practices?\"}"}
{"id": "Qf8uzIT1OK", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Through a thematic search strategy, we identified relevant research studies and datasets, revealing deficiencies in current image data curation practices or proposing potential solutions. By utilizing Semantic Scholar and Google Scholar, we curated relevant papers covering a wide spectrum of themes, including:\\n\\n- HCAI\\n- Human-subjects research\\n- HCCV datasets\\n- Dataset curation\\n- Ethical frameworks and considerations\\n- Data and model documentation\\n- Legal and regulatory considerations\\n- Privacy and data protection\\n- Consent\\n- Fairness\\n- Auditing and verification\\n- Guidelines and best practices\\n- Values in design\\n- Diversity and inclusion\\n- Representation\\n- Robustness and reliability\\n- Benchmarking and evaluation\\n- Bias detection and mitigation\\n- Critical AI\\n- Social implications\\n- Responsible AI\\n\\nThe themes were chosen based on our expertise and experience in designing CV datasets, training models, and developing ethical guidelines. To ensure a focused approach, we manually selected papers aligned with the scope of our study based on the relevance of a paper's title and abstract. This informed our initial categorization scheme, shown in Table 1, detailing key ethical considerations related to HCCV.\\n\\nInitially broad, we further refined the categories to address the most prominent ethical issues pertaining to HCCV dataset curation, particularly for fairness and robustness evaluations. Consent and privacy categories were combined due to their interrelated nature and the influence of shared legal frameworks.\\n\\n| Category        | Explanation                                                                 |\\n|-----------------|-----------------------------------------------------------------------------|\\n| Purpose         | The study discusses the underlying objectives and motivations for HCCV datasets. |\\n| Acquisition     | The study discusses ethical considerations related to the acquisition, collection, and labeling of image data, including recruitment and compensation for contributors. |\\n| Consent         | The study discusses consent and the responsible use of personal information.   |\\n| Privacy         | The study discusses privacy issues related to HCCV datasets or public data.   |\\n| Ownership       | The study discusses legal and ethical aspects of intellectual property rights in the context of HCCV datasets or public data. |\\n| Diversity       | The study discusses factors concerning diversity, inclusion, and fair representation within HCCV datasets. This encompasses matters such as identifying and addressing biases, ensuring fairness, and mitigating discrimination. |\\n| Maintenance     | The study discusses maintenance strategies for ensuring the integrity of HCCV datasets, including security measures. |\"}"}
{"id": "Qf8uzIT1OK", "page_num": 41, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Additionally, we integrated acquisition-related considerations into the categories of diversity, consent and privacy, as well as purpose, recognizing their interconnectedness in ethical image data collection, labeling, and usage. Maintenance-related matters were intentionally excluded from our scope, as these primarily pertain to post-dataset creation activities, while technical and organizational security measures are typically covered through consent forms. Ownership concerns, often intertwined with privacy issues, were incorporated into the consent and privacy category.\\n\\nTo establish a comprehensive view, we expanded our corpus as necessary. This encompassed examining cited works within our initial corpus, studies referencing our primary sources, and additional contributions by authors from our initial corpus. Our review was supplemented by incorporating publicly available resources from reputable sources, such as government bodies, private institutions, and reliable news organizations. In total, our analysis covered 500 research studies and online resources.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nHuman-centric computer vision (HCCV) data curation practices often neglect privacy and bias concerns, leading to dataset retractions and unfair models. HCCV datasets constructed through nonconsensual web scraping lack crucial metadata for comprehensive fairness and robustness evaluations. Current remedies are post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. Our research focuses on proactive, domain-specific recommendations, covering purpose, privacy and consent, and diversity, for curating HCCV evaluation datasets, addressing privacy and bias concerns. We adopt an ante hoc reflective perspective, drawing from current practices, guidelines, dataset withdrawals, and audits, to inform our considerations and recommendations.\\n\\n1 Introduction\\n\\nContemporary human-centric computer vision (HCCV) data curation practices, which prioritize dataset size and utility, have pushed issues related to privacy and bias to the periphery, resulting in dataset retractions and modifications [78, 126, 175, 216, 244, 320], as well as models that are unfair or rely on spurious correlations [22, 26, 112, 139, 146, 215, 272, 281]. HCCV datasets primarily rely on nonconsensual web scraping [99, 122, 124, 228, 260, 266, 310]. These datasets not only regard image subjects as free raw material [32], but also lack the ground-truth metadata required for fairness and robustness evaluations [91, 171, 196, 216]. This makes it challenging to obtain a comprehensive understanding of model blindspots and cascading harms [30, 85] across dimensions, such as data subjects, instruments, and environments, which are known to influence performance [222]. While, for example, image subject attributes can be inferred [7, 43, 170, 188, 198, 241, 267, 290, 343, 375], this is controversial for social constructs, notably race and gender [28, 132, 179, 180]. Inference introduces further biases [19, 107, 147, 263, 291] and can induce psychological harm when incorrect [47, 275].\\n\\nRecent efforts in machine learning (ML) to address these issues often rely on post hoc reflective processes. Dataset documentation focuses on interrogating and describing datasets after data collection [5, 27, 44, 94, 108, 149, 243, 258, 273, 307]. Similarly, initiatives by NeurIPS and ICML ask authors to consider the ethical and societal implications of their research after completion [257]. Further, dataset audits [247, 292] and bias detection tools [29, 340] expose dataset management issues and representational biases without offering guidance on responsible data collection. Although there are existing proposals for artificial intelligence (AI) and data design guidelines [35, 79, 116, 160, 202, 247], as well as calls to adopt methodologies from more established fields [154, 159, 166], general-purpose guidelines lack domain specificity and task-oriented guidance [307]. For example, remedies may prioritize privacy and governance [35] but overlook data composition and image content. Other recommended practices lack persuasive justification for adoption [116, 160] or fail to provide proper contextualization for appropriate application.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"contextualization for appropriate application. For instance, the People + AI Guidebook suggests creating dataset specifications without explaining the rationale, and privacy methodologies are advocated without cognizant of privacy and data protection laws. These efforts, which hold significance in promoting responsible practices, would benefit from being supplemented by proactive, domain-specific recommendations aimed at tackling privacy and bias concerns starting from the inception of a dataset.\\n\\nOur research directly addresses these critical concerns by examining purpose (Section 3), consent and privacy (Section 4), and diversity (Section 5). Compared to recent scholarship, we adopt an ante hoc reflective perspective, offering considerations and recommendations for curating HCCV datasets for fairness and robustness evaluations. Our work, therefore, resonates with the call for domain-specific resources to operationalize fairness. We draw insights from current practices, guidelines, dataset withdrawals, and audits, to motivate each recommendation, focusing on HCCV evaluation datasets that present unique challenges (e.g., visual leakage of personally identifiable information) and opportunities (e.g., leveraging image metadata for analysis). To guide curators towards more ethical yet resource-intensive curation, we provide a checklist in Appendix A. This translates our considerations and recommendations into pre-curation questions, functioning as a catalyst for discussion and reflection.\\n\\nWhile several of our recommendations can also be applied retroactively such measures cannot undo incurred harm, e.g., resulting from inappropriate uses, privacy violations, and unfair representation. It is important to make clear that our proposals are not intended for the evaluation of HCCV systems that detect, predict, or label sensitive or objectionable attributes such as race, gender, sexual orientation, or disability.\\n\\n2 Development Process\\n\\nHCCV should adhere to the most stringent ethical standards to address privacy and bias concerns. As stated in the NeurIPS Code of Ethics, it is essential to abide by established institutional research protocols, ensuring the safeguarding of human subjects. These protocols, initially designed for biomedical research, have, however, been met with confusion, resulting in inconsistencies when applied in the context of data-centric research. For example, HCCV research often amasses millions of \u201cpublic\u201d images without obtaining informed consent or participation, disregarding serious privacy and ethical concerns. This exemption from research ethics regulation is grounded in the limited definition of human-subjects research, which categorizes extant, publicly available data as minimal risk. Thus, numerous ethically-dubious HCCV datasets would not fall under Institutional Review Board (IRB) oversight. What's more, the NeurIPS Code of Ethics only mandates following existing protocols when research involves \u201cdirect\u201d interaction between human participants and researchers or technical systems. Even when research is subjected to supervision, IRBs are restricted from considering broader societal consequences beyond the immediate study context. Compounding matters, CV-centric conferences are still to adopt ethics review practices.\\n\\nThese limitations are concerning, especially considering the potential for predictive privacy harms when seemingly non-identifiable data is combined or when data is used for harmful downstream applications such as predicting sexual orientation, crime propensity, or emotion. Acknowledging this, our research study employed the same principles underpinning established guidelines for protecting human subjects in research to identify ethical issues in HCCV dataset design, namely autonomy, justice, beneficence, and non-maleficence.\\n\\nAutonomy respects individuals' self-determination\u2014e.g., through informed consent and assent for HCCV datasets. Justice promotes the fair distribution of risks, costs, and benefits, guiding decisions on compensation, data accessibility, and diversity. Beneficence entails the proactive promotion of positive outcomes and well-being, e.g., by soliciting individuals' to self-identify, while non-maleficence centers on minimizing harm and risks during dataset design, e.g., by redacting privacy-leaking image regions and metadata. To ensure comprehensive consideration, we harnessed diverse expertise, following contemporary, interdisciplinary practices. Our team comprises researchers, practitioners, and...\"}"}
{"id": "Qf8uzIT1OK", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"lawyers with backgrounds in ML, CV, algorithmic fairness, philosophy, and social science. With a range of ethnic, cultural, and gender backgrounds, we bring extensive experience in designing CV datasets, training models, and developing ethical guidelines. To align our expertise with the principles, we collectively discussed them, considering each author's background. After identifying key ethical issues in HCCV data curation practices, we iteratively refined them into an initial draft of ethical considerations. We extensively collected, analyzed, and discussed papers spanning a range of themes such as HCAI, HCCV datasets, data and model documentation, bias detection and mitigation, AI and data design, fairness, and critical AI. Our comprehensive literature review incorporated pertinent studies and datasets, resulting in refined considerations with detailed explanations and recommendations for responsible data curation. Additional details are provided in Appendix B.\\n\\n3 Purpose\\nIn ML, significant emphasis has been placed on the acquisition and utilization of \\\"general-purpose\\\" datasets [259]. Nevertheless, without a clearly defined task pre-data collection, it becomes challenging to effectively handle issues related to data composition, labeling, data collection methodologies, informed consent, and assessments related to data protection. This section addresses conflicting dataset motivations and provides recommendations.\\n\\n3.1 Ethical Considerations\\nFairness-unaware datasets are inadequate for measuring fairness. Datasets lacking explicit fairness considerations are inadequate for mitigating or studying bias, as they often lack the necessary labels for assessing fairness. For instance, the COCO dataset [196], focused on scene understanding, lacks subject information, making fairness assessments challenging. Researchers, consequently, resort to human annotators to infer, e.g., subject characteristics, limiting bias measurement to visually \\\"inferable\\\" attributes. This introduces annotation bias [56] and the potential for harmful inferences [47, 275].\\n\\nFairness-aware datasets are incompatible with common HCCV tasks. Industry practitioners stress the importance of carefully designed and collected \\\"fairness-aware\\\" datasets to detect bias issues [150]. Fabris et al. [93] found that out of 28 CV datasets used in fairness research between 2014 and 2021, only eight were specifically created with fairness in mind. Among these, seven were HCCV datasets (scraped from the web) [43, 170, 216, 308, 319, 342, 343], including five focused on facial analysis. Due to the limited availability and delimited task focus of fairness-aware datasets, researchers repurpose \\\"fairness-unaware\\\" datasets [120, 139, 196, 198, 208, 346, 373].\\n\\nFairness-aware datasets fall short in addressing the original tasks associated with well-known HCCV datasets, which encompass a range of tasks, such as segmentation [64, 209], pose estimation [13, 196], localization and detection [73, 91, 110], identity verification [153], action recognition [173], as well as reconstruction, synthesis and manipulation [114, 171]. The absence of fairness-aware datasets with task-specific labels hampers the practical evaluation of HCCV systems, despite their importance in domains such as healthcare [155, 220], autonomous vehicles [163], and sports [317]. Additionally, fairness-aware datasets lack self-identified annotations from image subjects, relying on inferred attributes, e.g., from online resources [43, 308, 319].\\n\\n3.2 Practical Recommendations\\nRefrain from repurposing datasets. Existing datasets, repurposable but optimized for specific functions, can inadvertently perpetuate biases and undermine fairness [183]. Repurposing fairness-unaware data for fairness evaluations can result in dirty data, characterized by missing or incorrect information and distorted by individual and societal biases [181, 265]. Dirty data, including inferred data, can have significant downstream consequences, compromising the validity of research, policy, and decision-making [14, 63, 265, 341]. ML practitioners widely agree that a proactive approach to fairness is preferable, involving the direct collection of demographic information from the outset [150]. To mitigate epistemic risk, curated datasets should capture key dimensions influencing fairness and robustness evaluation of HCCV models, i.e., data subjects, instruments, and environments. Model Cards explicitly highlight the significance of these dimensions in fairness and robustness assessments [222].\\n\\n3\"}"}
{"id": "Qf8uzIT1OK", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Create purpose statements. Pre-data collection, dataset creators should establish purpose statements, focusing on motivation rather than cause. Purpose statements address, e.g., data collection motivation, desired composition, permissible uses, and intended consumers. While dataset documentation covers similar questions, it is a reflective process and can be manipulated to fit the narrative of the collected data, as opposed to directing the narrative of the data to be collected. Purpose statements can play a crucial role in preventing both hindsight bias and purpose creep, ensuring alignment with stakeholders' consent and intentions. To enhance transparency and accountability, as recommended by Peng et al., purpose statements can undergo peer review, similar to registered reports. Registered reports, recognized by the UK 2021 Research Excellence Framework, incentivize rigorous research practices and can lead to increased institutional funding.\\n\\n4 Consent and Privacy\\n\\nInformed consent is crucial in research ethics involving humans, ensuring participant safety, protection, and research integrity. Shaping data collection practices in various fields, informed consent consists of three elements: information (i.e., the participant should have sufficient knowledge about the study to make their decision), comprehension (i.e., the information about the study should be conveyed in an understandable manner), and voluntariness (i.e., consent must be given free of coercion or undue influence). While consent is not the only legal basis for data processing, it is globally preferred for its legitimacy and ability to foster trust. We address concerns related to consent and privacy, and provide recommendations.\\n\\n4.1 Ethical Considerations\\n\\nHuman-subjects research. As aforementioned in Section 2, HCCV datasets are frequently collected without informed consent or participation, primarily due to the classification of publicly available data as \\\"minimal risk\\\" within human-subjects research. However, beyond possible predictive privacy harms and unethical downstream uses, collecting data without informed consent hinders researchers and practitioners from fully understanding and addressing potential harms to data subjects. Some argue that consent is pivotal as it provides individuals with a last line of defense against the misuse of their personal information, particularly when it contradicts their interests or well-being.\\n\\nCreative Commons loophole. Some datasets have been created based on the misconception that the \\\"unlocking [of] restrictive copyright\\\" through Creative Commons licenses implies data subject consent. However, the Illinois Biometric Information Privacy Act (BIPA) mandates data subject consent, even for publicly available images. In the UK and EU General Data Protection Regulation (GDPR) Article 4(11), images containing faces are considered biometric data, requiring \\\"freely given, specific, informed, and unambiguous\\\" consent from data subjects for data processing. Similarly, in China, the Personal Information Protection Law (PIPL) Article 29 mandates obtaining individual consent for processing sensitive personal information, including biometric data. While a Creative Commons license may release copyright restrictions on specific artistic expressions within images, it does not apply to image regions containing biometric data such as faces, which are protected by privacy and data protection laws.\\n\\nVulnerable persons. Nonconsensual data collection methods can result in the inclusion of vulnerable individuals unable to consent or oppose data processing due to power imbalances, limited capacity, or increased risks of harm. While scraping vulnerable individuals' biometric data may be incidental, some researchers actively target them, jeopardizing their sensitive information without guardian consent. Paradoxically, attempts to address racial bias in data have involved soliciting homeless persons of color, further compromising their vulnerability. When participation is due to economic or situational vulnerability, as opposed to one's best interests, monetary offerings may be perceived as inducement. Further ethical concerns manifest when it is unclear whether participants were adequately informed about a research study. For instance, in ethnicity recognition research, despite obtaining informed consent, criticism arose due to training a model that discriminates between Chinese Uyghur, Korean, and Tibetan faces. Although the study's focus is on the technology...\"}"}
{"id": "Qf8uzIT1OK", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"itself, its potential use in enhancing surveillance on Chinese Uyghurs raises ethical questions due to the human rights violations against them.\\n\\nConsent revocation. Dataset creators sometimes view autonomy as a challenge to collecting biometric data for HCCV, especially when data subjects prioritize privacy. Nonetheless, informed consent emphasizes voluntariness, encompassing both the ability to give consent and the right to withdraw it at any time. GDPR grants explicit revocation rights (Article 7) and the right to request erasure of personal data (Article 17). However, image subjects whose data is collected without consent are denied these rights. The nonconsensual FFHQ face dataset offers an opt-out mechanism, but since inclusion was involuntary, subjects may be unaware of their inclusion, rendering the revocation option hollow. Moreover, this burdens data subjects with tracking the usage of their data in datasets, primarily accessible by approved researchers.\\n\\nImage- and metadata-level privacy attributes. Researchers have focused on obfuscation techniques, e.g., blurring, inpainting, and overlaying, to reduce private information leakage of nonconsensual individuals. Nonetheless, face detection algorithms used in obfuscation may raise legal concerns, particularly if they involve predicting facial landmarks, potentially violating BIPA. BIPA focuses on collecting and using face geometry scans regardless of identification capability, while GDPR protects any identifiable person, requiring data holders to safeguard the privacy of nonconsenting individuals. Moreover, reliance on automated face detection methods raises ethical concerns, as demonstrated by the higher precision of pedestrian detection models on lighter skin types compared to darker skin types. This predictive inequity leads to allocative harm, denying certain groups opportunities and resources, including the rights to safety and privacy.\\n\\nIt is important to note that face obfuscation may not guarantee privacy. The Visual Redactions dataset includes 68 image-level privacy attributes, covering biometrics, sensitive attributes, tattoos, national identifiers, signatures, and contact information. Training faceless person recognition systems using full-body cues reveals higher than chance re-identification rates for face blurring and overlaying, indicating that solely obfuscating face regions might be insufficient under GDPR. Furthermore, image metadata can also disclose sensitive details, e.g., date, time, and location, as well as copyright information that may include names. This is worrisome for users of commonly targeted platforms like Flickr, which retain metadata by default.\\n\\n4.2 Practical Recommendations\\n\\nObtain voluntary informed consent. Similar to recent consent-driven HCCV datasets, explicit informed consent should be obtained from each person depicted in, or otherwise identifiable, in a dataset, allowing the sharing of their facial, body, and biometric information for evaluating the fairness and robustness of HCCV technologies. Datasets collected with consent reduce the risk of being fractured, however, data subjects may later revoke their consent over, e.g., privacy concerns they may not have been aware of at the time of providing consent or language nuances. Following GDPR (Article 7), plain language consent and notice forms are recommended to address the lack of public understanding of AI technologies. When collecting images of individuals under the age of majority or those whose ability to protect themselves is significantly impaired on account of disability, illness, or otherwise, guardian consent is necessary. However, relying solely on guardian consent overlooks the views and dignity of the vulnerable person. To address this, in addition to guardian consent, voluntary informed assent can be sought from a vulnerable person, in accordance with UNICEF's principlism-guided data collection procedures. When employing appropriate language and tools, assent establishes the vulnerable person understands the use of their data and willingly participates. If a vulnerable person expresses dissent or unwillingness to participate, their data should not be collected, regardless of guardian wishes.\\n\\nInformed by the U.S. National Bioethics Advisory Commission's contextual vulnerability framework, dataset creators should assess vulnerability on a continuous scale. That is, the circumstances of participation should be considered, which may require, e.g., a participatory design approach, assurances over compensation, supplementary educational materials, and insulation from hierarchical or authoritative systems.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Adopt techniques for consent revocation. To permit consent revocation, dataset creators should implement an appropriate mechanism. One option is dynamic consent, where personalized communication interfaces enable participants to engage more actively in research activities [174, 348]. This approach has been implemented successfully through online platforms, offering options for blanket consent, case-by-case selection, or opt-in depending on the data's use [174, 211, 314]. Alternatively, another recommended approach is to establish a steering board or charitable trust composed of representative dataset participants to make decisions regarding data use [255]. The feasibility of these proposals may vary based on a dataset's scale. Nonetheless, at a minimum, data subjects should be provided a simple and easily accessible method to revoke consent [136, 254, 268]. This aligns with guidance provided by the UK Information Commission's Office (ICO), emphasizing the need to provide alternatives to online-based revocation processes to accommodate varying levels of technology competency and internet access among data subjects [325].\\n\\nCollect country of residence information. Anonymizing nonconsensual human subjects through face obfuscation, as done in datasets such as ImageNet [367], may not respect the privacy laws specific to the subjects' country of residence. To comply with relevant data protection laws, dataset curators should collect the country of residence from each data subject to determine their legal obligations, helping to ensure that data subjects' rights are protected and future legislative changes are addressed [249, 268]. For instance, GDPR Article 7(3) grants data subjects the right to withdraw consent at any time, which was not explicitly addressed in its predecessor [253].\\n\\nRedact privacy leaking image regions and metadata. The European Data Protection Board emphasizes that anonymization of personal data must guard against re-identification risks such as singling out, linkability, and inference [76]. Re-identification remains possible even when nonconsensual subjects' faces are obfuscated, through other body parts or contextual information [242]. One solution is to redact all privacy-leaking regions related to nonconsensual subjects (including their entire bodies, clothing, and accessories) and text (excluding copyright owner information). However, anonymization approaches should be validated empirically, especially when using methods without formal privacy guarantees. Moreover, to mitigate algorithmic failures or biases, human annotators should be involved in creating region proposals, as well as verifying automatically generated proposals, for image regions with identifying or private information [367]. For nonconsensual individuals residing in certain jurisdictions (e.g., Illinois, California, Washington, Texas), automated region proposals requiring biometric identifiers should be avoided. Instead, human annotators should take the responsibility of generating these proposals.\\n\\nNotwithstanding, to further protect privacy, dataset creators should take steps to ensure that image metadata does not reveal identifying information that data subjects did not consent to sharing. This may involve replacing exact geolocation data with a more general representation, such as city and country, and excluding user-contributed details from metatags containing personally identifiable information, except when this action would violate copyright. However, we do not advise blanket redaction of all metadata, as it contains valuable image capture information that can be useful for assessing model bias and robustness related to instrument factors.\\n\\nDiversity\\nHCCV dataset creators widely acknowledge the significance of dataset diversity [13, 64, 78, 170, 171, 173, 196, 283, 361, 368], realism [110, 153, 164, 173, 196, 368], and difficulty [13, 16, 64, 73, 78, 91, 110, 173, 196, 198, 361, 368] to enhance fairness and robustness in real-world applications. Previous research has emphasized diversity across image subjects, environments, and instruments [43, 139, 222, 287], but there are many ethical complexities involved in specifying diversity criteria [14, 15]. This section examines taxonomy challenges and offers recommendations.\\n\\n5.1 Ethical Considerations\\nRepresentational and historical biases. The Council of Europe have expressed concerns about the threat posed by AI systems to equality and non-discrimination principles [67]. Many dataset creators often prioritize protected attributes, i.e., gender, race, and age, as key factors of dataset diversity [287]. Nevertheless, most HCCV datasets exhibit historical and representational biases [35, 166, 172, 312, 366]. These biases can be pernicious, particularly when models learn and amplify them. For instance, image captioning models may rely on contextual cues related to activities like shopping [377].\"}"}
{"id": "Qf8uzIT1OK", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and laundry to generate gendered captions. Spurious correlations are detrimental, as they are not causally related and perpetuate harmful associations. In addition, prominent examples in HCCV research demonstrate disparate algorithmic performance based on race and skin color. Most recently, autonomous robots have displayed racist, sexist, and physiognomic stereotypes. Furthermore, face detection models have shown lower accuracy when processing images of older individuals compared to younger individuals. While not endorsing these applications, discrepancies have also been observed in facial emotion recognition services for children in both commercial and research systems, as well as age estimation.\\n\\nDespite concerns regarding privacy, liability, and public relations, the collection of special and sensitive category data is crucial for bias assessments. GDPR guidance from the UK ICO confirms that sensitive attributes can be collected for fairness purposes. However, obtaining this information presents challenges, such as historical mistrust in clinical research among African-Americans or the social stigma of being photographed that some women face. Nonetheless, marginalized communities may require explicit explanations and assurances about data usage to address concerns related to service provision, security, allocation, and representation. This is particularly important as remaining unseen does not protect against being mis-seen.\\n\\nThe digital divide and accessibility. Healthcare datasets often lack representation of minority populations, compromising the reliability of automated decisions. The World Health Organization (WHO) emphasizes the need for data accuracy, completeness, and diversity, particularly regarding age, in order to address ageism in AI. ML systems may prioritize younger populations for resource allocation, assuming they would benefit the most in terms of life expectancy. The digital divide further exacerbates the underrepresentation of vulnerable groups, including older generations, low-income school-aged children, and children in East Asia and the South Pacific who lack access to digital technology. Insufficient access to digital technology hampers the representation of vulnerable persons in datasets, leading to outcome homogenization\u2014i.e., the systematic failing of the same individuals or groups.\\n\\nConfused taxonomies. Sex and gender are often used interchangeably, treating gender as a consequence of one's assigned sex at birth. However, this approach erases intersex individuals who possess non-binary physiological sex characteristics. Treating sex and gender as interchangeable perpetuates normative views by casting gender as binary, immutable, and solely based on biological sex. This perspective disregards transgender and gender nonconforming individuals. Moreover, sex, like gender, is a social construct, as sexed bodies do not exist outside of their social context.\\n\\nSimilar to sex and gender, race and ethnicity are often used synonymously. Nations employ diverse census questions to ascertain ethnic group composition, encompassing factors such as nationality, race, color, language, religion, customs, and tribe. However, these categories and their definitions lack consistency over time and geography, often influenced by political agendas and socio-cultural shifts. This variability makes it challenging to collect globally representative and meaningful data on ethnic groups. Consequently, several HCCV datasets have incorporated inconsistent and arbitrary racial categorization systems. For instance, the FairFace dataset creators reference the US Census Bureau's racial categories without considering the social definition of race they represent. The US Census Bureau explicitly states that their categories reflect a social definition rather than a biological, anthropological, or genetic one. Consequently, labeling the \u201cphysical race\u201d of image subjects based on nonphysiological categories is contradictory. Furthermore, the FairFace creators do not disclose the demographics or cultural compatibility of their annotators.\\n\\nOwn-anchor bias. HCCV approaches for encoding age in datasets vary, using either integer labels or group labels. Age groupings are often preferred when collecting unconstrained images from the web, as human annotators must infer subjects' ages, which is challenging. This is evident in crowdsourced annotations, where 40.2% of individuals in the OpenImages MIAP dataset could not be categorized into an age group. Factors unrelated to age, such as facial expression and makeup, influence age perception. Furthermore, annotators have exhibited lower accuracy when labeling people outside of their own demographic group.\\n\\nPost hoc rationalization of the use of physiological markers. Gender information about data subjects is obtained through inference.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Inference raises concerns as it assumes that gender can be determined solely from imagery without consent or consultation with the subject, which is noninclusive and harmful. Even when combined with non-image-based information, inferred gender fails to account for the fluidity of identity, potentially mislabeling subjects at the time of image capture. Moreover, physical traits are just one of many dimensions, including posture, clothing, and vocal cues, used to infer not only gender but also race.\\n\\nErasure of nonstereotypical individuals. HCCV datasets frequently adopt a US-based racial schema, which may oversimplify and essentialize groups. This approach may not align with other more nuanced models, e.g., the continuum-based color system used in Brazil, which considers a wide range of physical characteristics. Nonconsensual image datasets rely on annotators to assign semantic categories, perpetuating stereotypes and disseminating them beyond their cultural context. Notably, images without label consensus are often discarded, potentially excluding individuals who defy stereotypes, such as multi-ethnic individuals.\\n\\nPhenotypic attributes. Protected attributes may not be the most appropriate criteria for evaluating HCCV models. Social constructs like race and gender lack clear delineations for subgroup membership based on visible or invisible characteristics. These labels capture invisible aspects of identity that are not solely determined by visible appearance. Moreover, the phenotypic characteristics within and across subgroups exhibit significant variability.\\n\\nEnvironment and instrument. The image capture device and environmental conditions significantly influence model performance, and their impact should be considered. Factors such as camera software, hardware, and environmental conditions affect HCCV model robustness in various settings. Understanding performance differences is crucial from ethical and scientific perspectives. For example, sensitivity to illumination or white balance may be linked to sensitive attributes, e.g., skin tone, while available instruments or environmental co-occurrences may correlate with demographic attributes.\\n\\nAnnotator positionality. Psychological research highlights the influence of annotators' sociocultural background on their visual perception. However, recent empirical studies have evidenced a lack of regard for the impact an annotator's social identity has on data. Only a handful of HCCV datasets provide annotator demographic details.\\n\\nRecruitment and compensation. Data collected without consent patently lacks compensation. Balancing between excessive and deficient payment is crucial to avoid coercion and exploitation. An additional concern is the employment of remote workers from disadvantaged regions, often with low wages and fast-paced work conditions. This can lead to arbitrary denial of payment based on opaque quality criteria and prevents union formation, creating a sense of invisibility and uncertainty for workers.\\n\\n5.2 Practical Recommendations\\n\\nObtain self-reported annotations. Practitioners are cautious about inferring labels about people to avoid biases. Moreover, data access request rights, e.g., as offered by GDPR, California Consumer Privacy Act, and PIPL, may require data holders to disclose inferred information. To avoid stereotypical annotations and minimize harm from misclassification, labels should be collected directly from image subjects, who inherently possess contextual knowledge of their environment and awareness of their own attributes.\\n\\nProvide open-ended response options. Closed-ended questions, such as those on census forms, may lead to incongruous responses and inadequate options for self-identification. Open-ended questions provide more accurate answers but can be taxing, require extensive coding, and are harder to analyze. To balance this, closed-ended questions should be augmented with an open-ended response option, avoiding the term \u201cother,\u201d which implies othering norms. This gives subjects a voice and allows for future question design improvement.\\n\\nAcknowledge the mutability and multiplicity of identity. Identity shift\u2014the intentional self-transformation in mediated contexts\u2014is often overlooked. To address this, we propose collecting self-identified information on a per-image basis, acknowledging that identity is temporal and nonstatic. Specifically, for sensitive attributes, allowing the selection of multiple identity categories without limitations is preferable. This prevents oversimplification and marginalization. While we...\"}"}
{"id": "Qf8uzIT1OK", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"acknowledge the potential burden of self-identification on fluid and dynamic identities, an image captures a single moment. Thus, evolving identity may not require metadata updates; however, we recommend providing subjects with mechanisms for updates when needed.\\n\\nCollect age, pronouns, and ancestry. First, to capture accurate age information, dataset curators should collect the exact biological age in years from image subjects, corresponding to their age at the time of image capture. This approach offers flexibility, insofar as permitting the appropriate aggregation of the collected data. This is particularly important given the lack of consistent age groupings in the literature.\\n\\nSecond, dataset curators should consider opting to collect self-identified pronouns. This promotes mutual respect and common courtesy, reducing the likelihood of causing harm through misgendering [157]. Self-identified pronouns are particularly important for sexual and gender minority communities as they \\\"convey and affirm gender identity\\\" [232]. Significantly, pronoun use is increasingly prevalent in social media platforms [86, 165, 167], fostering gender inclusivity [21]. However, subjects should always have the option of not disclosing this information.\\n\\nFinally, to address issues with ethnic and racial classification systems [180, 286], dataset creators should consider collecting ancestry information instead. Ancestry is defined by historically shaped borders and has been shown to offer a more stable and less confusing concept [17]. The United Nations' M49 geoscheme can be used to operationalize ancestry [329], where subjects select regions that best describe their ancestry. To situate responses, subjects could be asked, e.g., \\\"Where do your ancestors (e.g., great-grandparents) come from?\\\". This avoids reliance on proxies, e.g., skin tone, that risk normalizing their inadequacies without reflecting their limitations [15].\\n\\nCollect aggregate data for commonly ignored groups. Additional sensitive attributes should also be collected, such as disability and pregnancy status, when voluntarily disclosed by subjects. These attributes should be reported in aggregate data to reduce the safety concerns of subjects [309, 351]. Given that definitions of these attributes may be inconsistent and tied to culture, identity, and histories of oppression [37, 41], navigating tensions between benefits and risks is necessary. Despite potential reluctance, sourcing data from underrepresented communities contributes to dataset inclusivity [37, 168]. Regarding disability, the American Community Survey [330] covers categories related to hearing, vision, cognitive, ambulatory, self-care, and independent living difficulties.\\n\\nCollect phenotypic and neutral performative features. Collecting phenotypic characteristics can serve as objective measures of diversity, i.e., attributes which, in evolutionary terms, contribute to individual-level recognition [57], e.g., skin color, eye color, hair type, hair color, height, and weight [19]. These attributes have enabled finer-grained analysis of model performance and biases [43, 75, 294, 318, 349, 372]. Additionally, considering a multiplicity of neutral performative features, e.g., facial hair, hairstyle, cosmetics, clothing, and accessories, is important to surface the perpetuation of social stereotypes and spurious relationships in trained models [6, 18, 166, 284, 340].\\n\\nRecord environment and instrument information. Data should capture variations in environmental conditions and imaging devices, including factors such as image capture time, season, weather, ambient lighting, scene, geography, camera position, distance, lens, sensor, stabilization, use of flash, and post-processing software. Instrument-related factors may be easily captured, by restricting data collection to images with exchangeable image file format (Exif) metadata. The remaining factors, e.g., season and weather can be self-reported or coarsely estimated utilizing information such as image capture time and location.\\n\\nRecontextualize annotators as contributors. Dataset creators should document the identities of annotators and their contributions to the dataset [12, 79], rather than treating them as anonymous entities responsible for data labeling alone [52, 206]. While many datasets [78, 137, 196] neglect to report annotator demographics, assuming objectivity in annotation for visual categories is flawed [23, 169, 219]. Furthermore, using majority voting to reach the assumed ground truth, disregards minority opinions, treating them as noise [169]. Annotator characteristics, including pronouns, age, and ancestry, should be recorded and reported to quantify and address annotator perspectives and bias in datasets [12, 118]. Additionally, allowing annotators freedom in labeling helps to avoid replicating socially dominant viewpoints [219].\\n\\nFair treatment and compensation for contributors. In accordance with Australia's National Health and Medical Research Council [231] and the WHO [66], dataset contributors should not only be\"}"}
{"id": "Qf8uzIT1OK", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"guaranteed compensation above the minimum hourly wage of their country of residence, but also according to the complexity of tasks to be performed. However, alternative payment models, for example, based on the average hourly wage, may offer benefits in terms of promoting diversity by increasing the likelihood of higher socio-economic status contributors.\\n\\nBesides payment, the implementation of direct communication channels and feedback mechanisms, such as anonymized feedback forms, can help to address issues faced by annotators while providing a level of protection from retribution. Furthermore, the creation of plain language guides can ease task completion and reduce quality control overheads. Ideally, recruitment and compensation processes should be well-documented and undergo ethics review, which can help to further reduce the number of \\\"glaring ethical lapses.\\\"\"}"}
{"id": "Qf8uzIT1OK", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgments and Disclosure of Funding\\n\\nThis work was funded by Sony Research.\\n\\nReferences\\n\\n[1] Announcing the NeurIPS Code of Ethics \u2014 NeurIPS Blog \u2014 blog.neurips.cc. https://blog.neurips.cc/2023/04/20/announcing-the-neurips-code-of-ethics/ [Accessed August 14, 2023].\\n\\n[2] OpenAI and Microsoft Sued for $3 Billion Over Alleged ChatGPT 'Privacy Violations' \u2014 vice.com. https://www.vice.com/en/article/wxjxgx/openai-and-microsoft-sued-for-dollar3-billion-over-alleged-chatgpt-privacy-violations [Accessed August 14, 2023].\\n\\n[3] Shared, but not up for grabs. Nature Machine Intelligence, 1(4):163\u2013163, April 2019. doi:10.1038/s42256-019-0047-y.\\n\\n[4] Mahmoud Afifi and Michael S. Brown. What else can fool deep learning? addressing color constancy errors on deep neural network performance. In IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.\\n\\n[5] Shazia Afzal, C Rajmohan, Manish Kesarwani, Sameep Mehta, and Hima Patel. Data readiness report. In 2021 IEEE International Conference on Smart Data Services (SMDS), pages 42\u201351. IEEE, 2021.\\n\\n[6] V\u00edtor Albiero, Kai Zhang, Michael C King, and Kevin W Bowyer. Gendered differences in face recognition accuracy explained by hairstyles, makeup, and facial morphology. IEEE Transactions on Information Forensics and Security, 17:127\u2013137, 2021.\\n\\n[7] Mohsan Alvi, Andrew Zisserman, and Christoffer Nell\u00e5ker. Turning a blind eye: Explicit removal of biases and variation from deep neural network embeddings. In European Conference on Computer Vision Workshops (ECCVW), pages 0\u20130, 2018.\\n\\n[8] Jeffrey S Anastasi and Matthew G Rhodes. An own-age bias in face recognition for children and older adults. Psychonomic bulletin & review, 12(6):1043\u20131047, 2005.\\n\\n[9] Jeffrey S Anastasi and Matthew G Rhodes. Evidence for an own-age bias in face recognition. North American Journal of Psychology, 8(2), 2006.\\n\\n[10] Nazanin Andalibi and Justin Buss. The human in emotion recognition on social media: Attitudes, outcomes, risks. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, pages 1\u201316, 2020.\\n\\n[11] Jerone T. A. Andrews. The hidden fingerprint inside your photos. https://www.bbc.com/future/article/20210324-the-hidden-fingerprint-inside-your-photos, 2021. [Accessed June 30, 2022].\\n\\n[12] Jerone T A Andrews, Przemyslaw Joniak, and Alice Xiang. A view from somewhere: Human-centric face representations. In International Conference on Learning Representations (ICLR), 2023.\\n\\n[13] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele. 2d human pose estimation: New benchmark and state of the art analysis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3686\u20133693, 2014.\\n\\n[14] McKane Andrus, Elena Spitzer, and Alice Xiang. Working to address algorithmic bias? don't overlook the role of demographic data. Partnership on AI, 2020.\\n\\n[15] McKane Andrus, Elena Spitzer, Jeffrey Brown, and Alice Xiang. What we can't measure, we can't understand: Challenges to demographic data procurement in the pursuit of fairness. In ACM Conference on Fairness, Accountability, and Transparency (FAccT), pages 249\u2013260, 2021.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Anelia Angelova, Yaser Abu-Mostafa, and Pietro Perona. Pruning training sets for learning of object categories. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 494\u2013501, 2005.\\n\\nPeter J. Aspinall. Operationalising the collection of ethnicity data in studies of the sociology of health and illness. Sociology of health & illness, 23(6):829\u2013862, 2001.\\n\\nGuha Balakrishnan, Yuanjun Xiong, Wei Xia, and Pietro Perona. Towards causal benchmarking of bias in face analysis algorithms. In Deep Learning-Based Face Analytics, pages 327\u2013359. Springer, 2021.\\n\\nP. Balaresque and T. E. King. Human phenotypic diversity. In Genes and Evolution, pages 349\u2013390. Elsevier, 2016. doi: 10.1016/bs.ctdb.2016.02.001.\\n\\nRich Barlow and Cydney Scott. Students can adjust their pronouns and gender identity in bu\u2019s updated data system. https://www.bu.edu/articles/2022/pronouns-and-gender-identities-in-updated-data-system/, November 2022.\\n\\nDennis Baron. What\u2019s Your Pronoun?: Beyond He and She. Liveright Publishing, 2020.\\n\\nAlistair Barr. Google mistakenly tags Black people as \u2018gorillas,\u2019 showing limits of algorithms. The Wall Street Journal, 2015.\\n\\nTeanna Barrett, Quan Ze Chen, and Amy X Zhang. Skin deep: Investigating subjectivity in skin tone annotations for computer vision benchmark datasets. arXiv preprint arXiv:2305.09072, 2023.\\n\\nTom Beauchamp and James Childress. Principles of biomedical ethics: marking its fortieth anniversary, 2019.\\n\\nFabiola Becerra-Riera, Annette Morales-Gonz\u00e1lez, and Heydi M\u00e9ndez V\u00e1zquez. A survey on facial soft biometrics for video surveillance and forensic applications. Artificial Intelligence Review, 52(2):1155\u20131187, 2019.\\n\\nSara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In European Conference on Computer Vision (ECCV), pages 456\u2013473, 2018.\\n\\nEmily M. Bender and Batya Friedman. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587\u2013604, December 2018. doi: 10.1162/tacl_a_00041.\\n\\nSebastian Benthall and Bruce D. Haynes. Racial categories in machine learning. In ACM Conference on Fairness, Accountability, and Transparency (FAccT), pages 289\u2013298, 2019.\\n\\nElena Beretta, Antonio Vetr\u00f2, Bruno Lepri, and Juan Carlos De Martin. Detecting discriminatory risk through data annotation based on Bayesian inferences. In ACM Conference on Fairness, Accountability, and Transparency (FAccT), pages 794\u2013804, 2021.\\n\\nA Stevie Bergman, Lisa Anne Hendricks, Maribeth Rauh, Boxi Wu, William Agnew, Markus Kunesch, Isabella Duan, Iason Gabriel, and William Isaac. Representation in AI evaluations. In ACM Conference on Fairness, Accountability, and Transparency (FAccT), pages 519\u2013533, 2023.\\n\\nGabrielle Berman and Kerry Albright. Children and the data cycle: Rights and ethics in a big data world. arXiv preprint arXiv:1710.06881, 2017.\\n\\nAbeba Birhane. Algorithmic colonization of Africa. SCRIPTed, 17:389, 2020.\\n\\nAbeba Birhane. Automating ambiguity: Challenges and pitfalls of artificial intelligence. arXiv preprint arXiv:2206.04179, 2022.\\n\\nAbeba Birhane and Vinay Uday Prabhu. Large image datasets: A pyrrhic win for computer vision? In IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1536\u20131546, 2021.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Qf8uzIT1OK", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Qf8uzIT1OK", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kate Crawford and Trevor Paglen. Excavating AI: The politics of images in machine learning training sets. *Ai & Society*, 36(4):1105\u20131116, 2021.\\n\\nKate Crawford and Jason Schultz. Big data and due process: Toward a framework to redress predictive privacy harms. *BCL Rev.*, 55:93, 2014.\\n\\nNicolas Croce and Moh Musa. The new assembly lines: Why AI needs low-skilled workers too. [https://www.weforum.org/agenda/2019/08/ai-low-skilled-workers/](https://www.weforum.org/agenda/2019/08/ai-low-skilled-workers/), August 2019.\\n\\nWang Cunrui, Q Zhang, W Liu, Y Liu, and L Miao. Facial feature discovery for ethnicity recognition. *Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery*, 9(2):e1278, 2019.\\n\\nNavneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, volume 1, pages 886\u2013893. IEEE, 2005.\\n\\nFida K Dankar, Marton Gergely, and Samar K Dankar. Informed consent in biomedical research. *Computational and structural biotechnology journal*, 17:463\u2013474, 2019.\\n\\nSaloni Dash, Vineeth N Balasubramanian, and Amit Sharma. Evaluating and mitigating bias in image classifiers: A causal perspective using counterfactuals. In *IEEE Winter Conference on Applications of Computer Vision (WACV)*, pages 915\u2013924, 2022.\\n\\nData Protection Commission. [https://www.dataprotection.ie/en/dpc-guidance/anonymisation-and-pseudonymisation](https://www.dataprotection.ie/en/dpc-guidance/anonymisation-and-pseudonymisation), June 2019. [Accessed August 1, 2022].\\n\\nPaul De Hert and Vagelis Papakonstantinou. The new general data protection regulation: Still a sound system for the protection of individuals? *Computer law & security review*, 32(2):179\u2013194, 2016.\\n\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 248\u2013255, 2009.\\n\\nEmily Denton, Mark D\u00edaz, Ian Kivlichan, Vinodkumar Prabhakaran, and Rachel Rosen. Whose ground truth? accounting for individual and collective identities underlying dataset annotation. *arXiv preprint arXiv:2112.04554*, 2021.\\n\\nOliver Diggelmann and Maria Nicole Cleis. How the right to privacy became a human right. *Human Rights Law Review*, 14(3):441\u2013458, 2014.\\n\\nChris Dulhanty. Issues in computer vision data collection: Bias, consent, and label taxonomy. Master's thesis, University of Waterloo, 2020.\\n\\nLilian Edwards. Privacy, security and data protection in smart cities: A critical EU law perspective. *Eur. Data Prot. L. Rev.*, 2:28, 2016.\\n\\nVincent Egan and Giray Cordan. Barely legal: Is attraction and estimated age of young female faces disrupted by alcohol use, make up, and the sex of the observer? *British Journal of Psychology*, 100(2):415\u2013427, 2009.\\n\\nEran Eidinger, Roee Enbar, and Tal Hassner. Age and gender estimation of unfiltered faces. *IEEE Transactions on information forensics and security*, 9(12):2170\u20132179, 2014.\\n\\nMadeleine Clare Elish. Moral crumple zones: Cautionary tales in human-robot interaction (pre-print). *Engaging Science, Technology, and Society (pre-print)*, 2019.\\n\\nSonia Elks. Why Twitter and Instagram are inviting people to share their pronouns. [https://www.contextnews/big-tech/why-twitter-and-instagram-are-inviting-people-to-share-pronouns](https://www.contextnews/big-tech/why-twitter-and-instagram-are-inviting-people-to-share-pronouns), October 2021.\"}"}
{"id": "Qf8uzIT1OK", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Severin Engelmann, Chiara Ullstein, Orestis Papakyriakopoulos, and Jens Grossklags. What people think AI should infer from faces. In ACM Conference on Fairness, Accountability, and Transparency (FAccT), pages 128\u2013141, 2022.\\n\\nEuropean Commission. General data protection regulation. https://gdpr-info.eu/, 2016. [Accessed August 1, 2022].\\n\\nEuropean Data Protection Board (Article 29 Working Party). The working party on the protection of individuals with regard to the processing of personal data. https://ec.europa.eu/newsroom/document.cfm?doc_id=44137, 2017. [Accessed August 1, 2022].\\n\\nEuropean Union High-level Expert Group. Ethics guidelines for trustworthy AI: Building trust in human-centric AI. https://ec.europa.eu/futurium/en/ai-alliance-consultation/guidelines.1.html, 2019. [Accessed August 7, 2023].\\n\\nMark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The Pascal Visual Object Classes (VOC) Challenge. International journal of computer vision, 88(2):303\u2013338, 2010.\\n\\nNir Eyal. Using informed consent to save trust. Journal of medical ethics, 40(7):437\u2013444, 2014.\\n\\nAlessandro Fabris, Stefano Messina, Gianmaria Silvello, and Gian Antonio Susto. Algorithmic fairness datasets: the story so far. Data Mining and Knowledge Discovery, 36(6):2074\u20132152, 2022.\\n\\nAlessandro Fabris, Stefano Messina, Gianmaria Silvello, and Gian Antonio Susto. Tackling documentation debt: a survey on algorithmic fairness datasets. In Equity and Access in Algorithms, Mechanisms, and Optimization, pages 1\u201313. 2022.\\n\\nAnne Fausto-Sterling. Sexing the Body: Gender Politics and the Construction of Sexuality. Basic books, 2000.\\n\\nCynthia Feliciano. Shades of Race: How Phenotype and Observer Characteristics Shape Racial Classification. American Behavioral Scientist, 60(4):390\u2013419, 2016.\\n\\nKlaus Fiedler and Norbert Schwarz. Questionable research practices revisited. Social Psychological and Personality Science, 7(1):45\u201352, 2016.\\n\\nChristian Fieseler, Eliane Bucher, and Christian Pieter Hoffmann. Unfairness by design? the perceived fairness of digital labor on crowdworking platforms. Journal of Business Ethics, 156:987\u20131005, 2019.\\n\\nAndrew P. Founds, Nick Orlans, Whiddon Genevieve, and Craig I. Watson. NIST Special Database 32-Multiple Encounter Dataset II (Meds-II). 2011.\\n\\nJonathan B. Freeman, Andrew M. Penner, Aliya Saperstein, Matthias Scheutz, and Nalini Ambady. Looking the part: Social status cues shape race perception. PLoS One, 6(9):e25107, 2011.\\n\\nAndrea Frome, German Cheung, Ahmad Abdulkader, Marco Zennaro, Bo Wu, Alessandro Bissacco, Hartwig Adam, Hartmut Neven, and Luc Vincent. Large-scale privacy protection in Google Street View. In IEEE/CVF International Conference on Computer Vision (ICCV), pages 2373\u20132380, 2009.\\n\\nYun Fu, Ye Xu, and Thomas S. Huang. Estimating human age by manifold analysis of face pictures and regression on aging features. In 2007 IEEE International Conference on Multimedia and Expo, pages 1383\u20131386. IEEE, 2007.\\n\\nSidney Fussell. How an attempt at correcting bias in tech goes wrong. https://www.theatlantic.com/technology/archive/2019/10/google-allegedly-used-homeless-train-pixel-phone/599668/, 2019. [Accessed June 30, 2022].\\n\\nYun Fu, Ye Xu, and Thomas S. Huang. Estimating human age by manifold analysis of face pictures and regression on aging features. In 2007 IEEE International Conference on Multimedia and Expo, pages 1383\u20131386. IEEE, 2007.\"}"}
