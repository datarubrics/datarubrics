{"id": "C0zw2ERKiQ", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure A3: The class distribution of randomly generated images (left) and histogram matched images (right), predicted by the fully-supervised Inception-V3 [56].\\n\\nFID\\n\\nCKA\\n\\n5K 10K 50K 100K 250K 500K\\n\\nCLIP-ViT\\n\\nConvNeXt\\n\\nInception\\n\\n40.0\\n\\n45.0\\n\\n50.0\\n\\n55.0\\n\\n98.0\\n\\n98.5\\n\\n99.0\\n\\n99.5\\n\\n100\\n\\n35.0\\n\\n\u219317.11%\\n\\n\u219314.49%\\n\\n\u21937.30%\\n\\nFigure A4: Fr\u00e9chet Distance (FD) and Centered Kernel Alignment (CKA) scores evaluated under various data regimes on ImageNet dataset. FID scores are scaled for better visualization. \u2193 denotes the results fluctuate downward. The percentages represent the magnitude of the numerical variation.\\n\\nstudy to compare two paired generative models. Concretely, we prepare groups of paired images of different generative models and ask 100 individuals to assess which model could produce high-quality images. The same groups are repeated several times by changing the order of images, ensuring the human evaluation is reliable and consistent.\\n\\nFig. A8 provides the interface of comparing two paired generative models, users are asked to choose which set of images looks more plausible. Additionally, Fig. A9 shows the pipeline of analyzing the paired comparison results. Specifically, the same groups of images are repeated for 4 times in random order and users are shown 16 images from two models to determine the more photorealistic one. In this way, the results of choosing both Projected-GAN and StyleGAN2 two times are identified as indistinguishable for enduring the consistency. Namely, the users choose different rankings between the two sets when the order of images is changed, which does not meet the consistency. Consequently, the final scores for paired comparison are obtained by quantifying the percentage of the human preferences that correlate the consistency.\"}"}
{"id": "C0zw2ERKiQ", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Here's an image produced by a generative model. Please select whether you believe it to be photorealistic by clicking the button below.\\n\\nPlease check the image carefully and keep your consistency of your requirements for the photo realism. Thank you so much!\\n\\n1. No\\n2. Yes\\n\\nFigure A5: User interface for benchmarking the synthesis quality.\\n\\n| StyleGAN2 | Projected-GAN | InsGen | EqGAN | StyleGAN-XL |\\n|-----------|---------------|--------|-------|-------------|\\n| 66%       | 62%           | 58%    | 39%   | 45%         |\\n\\nHuman evaluation results\\n\\n100 individuals\\n2000 randomly synthesized images for visual perception comparison\\n\\n\u221a \u221a \u221a X \u221a\\nX \u221a \u221a X \u221a \u221a X \u221a \u221a X\\n\u221a \u221a \u221a \u221a X \u221a \u221a \u221a \u221a \u221a\\n\u221a \u221a \u221a \u221a \u221a \u221a \u221a \u221a X \u221a\\n\u221a \u221a \u221a \u221a \u221a \u221a \u221a \u221a X \u221a\\n\\nFigure A6: Human judgment results of various generative models on FFHQ.\\n\\n2K images randomly generated by different models are selected for comparison.\\n\\nD More Quantitative and Qualitative Results\\n\\nIn this section, we provide more quantitative and qualitative results to further demonstrate the efficacy of our newly developed measurement system.\\n\\nComparing with more metrics.\\n\\nIn order to further investigate the efficacy of our proposed metric, we further involve Kernel Inception Distance (KID) [4], precision, and recall [49] into our comparison. Note that the original KID employs Inception-V3 as the feature extractor, and there is a large \u201cperceptual null space\u201d in Inception-V3. Therefore, we first investigate whether KID scores can be altered by attacking the feature extractor with the histogram matching mechanism. The experimental details are consistent with computing Fr\u00e9chet Distance (FD) in Tab.2 of the main paper.\\n\\nTab. A4 presents the quantitative results. Still, some extractors, such as Inception, Swin-Transformer, and ResMLP, are susceptible to the histogram matching attack. For instance, the KID score of Swin-Transformer is improved by 5.31% when the chosen set is used. These observations agree with our findings in our main paper, suggesting that certain extractors can be hacked when KID is employed as the distributional distance. Then, we investigate the sample efficiency of KID, Precision, and Recall to probe the impacts of the amount of generated samples. Fig. A10 presents the curves of KID, Precision, and Recall scores computed under different data regimes. Similarly, we could observe that the KID scores can be improved by synthesizing more images. Interestingly, the recall scores decrease as the generated sample size increases whereas the precision is stable. This is caused by the definition of recall: recall measures the proportion of the real distribution that is covered by\"}"}
{"id": "C0zw2ERKiQ", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure A7: Human judgment results of various generative models on ImageNet.\\n\\nTable A4: Quantitative comparison results of Kernel Inception Distance (KID\u2193, \u00d7e^\u22123) on FFHQ dataset. \u201cRandom, Chosen\u201d respectively represent the synthesized distribution of randomly generated and matching the class prediction of Inception-V3. Moreover, \u201cR\u201d and \u201cV\u201d respectively denote the architecture of ResNet and ViT. (\u2193) indicates the results are hacked by the histogram matching mechanism. Notably, the values across different rows are not comparable and we report the stds of three testings to better illustrate the numerical fluctuation of various extractors towards the histogram attack.\\n\\n| Model          | Inception | ConvNeXt | SW | A | V | MoCo-R | RepVGG | CLIP-R |\\n|----------------|-----------|----------|----|---|---|--------|--------|--------|\\n| Random         | 1.88 \u00b1 0.02 | 34.81 \u00b1 0.11 | 9.61 \u00b1 0.06 | 5.31 \u00b1 0.06 | 33.88 \u00b1 0.29 | 2.85 \u00b1 0.05 |\\n| Chosen         | 1.71 \u00b1 0.02 | 34.82 \u00b1 0.10 | 9.61 \u00b1 0.06 | 5.31 \u00b1 0.05 | 33.89 \u00b1 0.27 | 2.85 \u00b1 0.05 |\\n\\n| Model          | Swin      | ViT      | DeiT | CLIP-V | MoCo-V | ResMLP |\\n|----------------|-----------|----------|------|--------|--------|--------|\\n| Random         | 21.64 \u00b1 0.10 | 16.74 \u00b1 0.10 | 18.01 \u00b1 0.19 | 38.06 \u00b1 0.20 | 15.41 \u00b1 0.09 | 4.86 \u00b1 0.02 |\\n| Chosen         | 20.49 \u00b1 0.09 | 16.74 \u00b1 0.12 | 19.39 \u00b1 0.22 | 38.09 \u00b1 0.19 | 15.40 \u00b1 0.07 | 4.70 \u00b1 0.02 |\\n\\nIn addition to evaluating the robustness of these extractors on the FFHQ dataset, we further perform the same experiment on the ImageNet dataset. Tab. A5 presents the quantitative results. We can tell from these results that the chosen feature extractors are robust to the attack, further demonstrating their reliability.\\n\\nMore results of MLP-based extractors. We further incorporate two MLP-based models as the feature extractor for synthesis evaluation, namely gMLP [37] and MLP-mixer [59]. Following...\"}"}
{"id": "C0zw2ERKiQ", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Here are two sets of images produced by two different generative models. Please click the button below to indicate which one you find to be more plausible.\\n\\n1. Please check both sets of images carefully and choose the better one, as they may contain subtle differences which are not immediately obvious. Thank you so much!\\n\\nLeft is better  Right is better\\n\\nFigure A8: User interface for comparing the synthesis quality of two paired generative models. People are asked to determine which set of images look more photorealistic.\\n\\n| StyleGAN2 | Projected-GAN |\\n|-----------|---------------|\\n| 100 individuals | 40% | 48% | 12% |\\n\\nv.s.\\n\\nStyleGAN2 is better than Projected-GAN: 48% / (40% + 48%) = 55%\\nProjected-GAN is better than StyleGAN2: 40% / (40% + 48%) = 45%\\n\\nFigure A9: The pipeline of analyzing the paired comparison results.\\n\\nIn the experimental settings in our main paper, we identify the reliability and robustness of these MLP-based models via 1) visualizing the highlighted regions that contribute most significantly to the measurement results, and 2) attacking the feature extractor with histogram matching attack. Tab. A6 presents the qualitative (left) and quantitative (right) results. On one hand, the heatmap visualization results indicate that both gMLP and mixer-MLP capture limited semantics. Considering that more visual semantics should be considered for a more comprehensive evaluation, gMLP and MLP-mixer might not be adequate for synthesis comparison. On the other hand, the quantitative results demonstrate that their FD scores could be altered by the histogram matching attack, without actually improving the synthesis quality. That is, gMLP and MLP-mixer are susceptible to the histogram attack. Together with the finding that the FD scores of ResMLP could be manipulated without any improvement to the generative models in Tab. 2 of our main paper, we do not integrate MLP-based feature extractors into our measurement system.\"}"}
{"id": "C0zw2ERKiQ", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table A5: Quantitative comparison results of Fr\u00e9chet Distance (FD) on ImageNet dataset.\\n\\n| Model       | Random | Chosen |\\n|-------------|--------|--------|\\n| **Inception** |        |        |\\n| ConvNeXt SW | 34.29 \u00b1 0.09 | 33.05 \u00b1 0.08 |\\n| CLIP-R      | 78.02 \u00b1 0.16 | 78.10 \u00b1 0.14 |\\n| MoCo-R      | 0.13 \u00b1 0.002 | 0.13 \u00b1 0.002 |\\n| RepVGG      | 0.32 \u00b1 0.002 | 0.32 \u00b1 0.002 |\\n| **ConvNet** |        |        |\\n| Random      | 54.98 \u00b1 0.22 | 54.30 \u00b1 0.24 |\\n| Chosen I    | 27.64 \u00b1 0.15 | 27.66 \u00b1 0.17 |\\n| **ResNet**  |        |        |\\n| Random      | 323.12 \u00b1 0.88 | 301.91 \u00b1 0.92 |\\n| Chosen I    | 50.97 \u00b1 0.20 | 50.96 \u00b1 0.18 |\\n| **ViT**     |        |        |\\n| Random      | 621.98 \u00b1 1.02 | 597.32 \u00b1 1.11 |\\n| Chosen I    | 5.46 \u00b1 0.09 | 5.46 \u00b1 0.07 |\\n| **DeiT**    |        |        |\\n| Random      | 50.01 \u00b1 0.21 | 50.00 \u00b1 0.19 |\\n| Chosen I    | 145.32 \u00b1 1.02 | 133.06 \u00b1 1.09 |\\n\\n*Note: Results are hacked by the histogram matching mechanism.*\\n\\n### Table A6: Heatmaps from MLP-based extractors, namely Mixer-MLP [59] and gMLP [37].\\n\\n**Mixer-MLP vs gMLP**\\n\\n| Extractor | Original | mixer-MLP | gMLP |\\n|-----------|----------|----------|------|\\n| Mixer-MLP | 2.93 \u00b1 0.004 | 5.51 \u00b1 0.010 | 2.89 \u00b1 0.004 |\\n| gMLP      | 2.89 \u00b1 0.004 | 5.35 \u00b1 0.010 | 2.93 \u00b1 0.004 |\\n\\n*Note: The table compares the Fr\u00e9chet Distance between the original extractors and their respective Mixer-MLP and gMLP versions. The results show a significant increase in dissimilarity when compared to the original extractors.*\"}"}
{"id": "C0zw2ERKiQ", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure A11: Representation similarity of various extractors. \\n\\nDarker Yellow denotes higher similarity.\\n\\nRepresentations of high dimension in different feature extractors are calculated following [32]. In particular, a considerable number of images (i.e., 10K images from ImageNet) are fed into these extractors for computing their correspondence. Fig. A11 shows the similarity of their representations. Obviously, the representations of CLIP-ResNet and MoCo-ResNet have higher similarity with other extractors. Considering these two extractors are both CNN-based and they capture similar semantics with other CNN-based extractors, we remove the CLIP-ResNet and MoCo-ResNet to avoid redundancy. Accordingly, we obtain a set of feature extractors that 1) capture rich semantics in a complementary way, 2) are robust toward the histogram matching attack, and 3) define meaningful and distinctive representation spaces for synthesis comparison. The following table presents these feature extractors. These extractors, including both CNN-based and ViT-based architectures, have demonstrated strong performance in pre-defined and downstream tasks, facilitating more comprehensive and reliable evaluation. Notably, the inclusion of self-supervised extractors SWAV, CLIP-V, and MoCo-V aligns with previous findings [41, 35, 3]. This selection of feature extractors provides a diverse and complementary set of representations, enabling a more comprehensive and reliable evaluation of synthesis quality in generative models.\\n\\nMore results of hierarchical levels from various extractors. Tab. A7, Tab. A8, Tab. A9, Tab. A10, and Tab. A11 respectively present the heatmaps and quantitative results of various semantic levels. We could tell that despite the Fr\u00e9chet Distance (FD) scores consistently reflect synthesis quality, their numerical values fluctuate dramatically. On the contrary, CKA provides normalized distances w.r.t. the numerical scale across various levels. Also, the heatmaps from various semantic levels reveal that hierarchical features encode different semantics. Such observation provides interesting insights that feature hierarchy should be also considered for synthesis comparison. Notably, benefiting from the bounded quantitative results, CKA demonstrates great potentials for comparison across hierarchical layers.\"}"}
{"id": "C0zw2ERKiQ", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table A7: Heatmaps from various semantic levels on FFHQ dataset (left) and quantitative results of Fr\u00e9chet Distance (FD $\\\\downarrow$) and Centered Kernel Alignment (CKA $\\\\uparrow$) on ImageNet dataset (right). ConvNext [40] serves as the feature extractor for hierarchical evaluation here.\\n\\n| Shallow Model | BigGAN | BigGAN-deep | StyleGAN-XL |\\n|---------------|--------|-------------|-------------|\\n| Layer 1       | 2.64   | 2.56        | 0.58        |\\n| Layer 2       | 40.20  | 32.32       | 11.84       |\\n| Layer 3       | 687.40 | 364.95      | 264.87      |\\n| Layer 4       | 140.04 | 102.26      | 19.22       |\\n| Overall       | N/A    | N/A         | N/A         |\\n\\nTable A8: Heatmaps from various semantic levels on FFHQ dataset (left) and quantitative results of Fr\u00e9chet Distance (FD $\\\\downarrow$) and Centered Kernel Alignment (CKA $\\\\uparrow$) on ImageNet dataset (right). RepVGG [15] serves as the feature extractor for hierarchical evaluation here.\\n\\n| Shallow Model | BigGAN | BigGAN-deep | StyleGAN-XL |\\n|---------------|--------|-------------|-------------|\\n| Layer 1       | 0.35   | 0.32        | 0.04        |\\n| Layer 2       | 0.35   | 0.33        | 0.03        |\\n| Layer 3       | 0.23   | 0.18        | 0.04        |\\n| Layer 4       | 67.53  | 58.85       | 15.93       |\\n| Overall       | N/A    | N/A         | N/A         |\\n\\nTable A9: Heatmaps from various semantic levels on FFHQ dataset (left) and quantitative results of Fr\u00e9chet Distance (FD $\\\\downarrow$) and Centered Kernel Alignment (CKA $\\\\uparrow$) on ImageNet dataset (right). SW A V [6] serves as the feature extractor for hierarchical evaluation here.\\n\\n| Shallow Model | BigGAN | BigGAN-deep | StyleGAN-XL |\\n|---------------|--------|-------------|-------------|\\n| Layer 1       | 0.67   | 0.46        | 0.07        |\\n| Layer 2       | 0.87   | 0.60        | 0.31        |\\n| Layer 3       | 16.15  | 12.02       | 1.90        |\\n| Layer 4       | 11.18  | 8.69        | 1.85        |\\n| Overall       | N/A    | N/A         | N/A         |\\n\\nTable A10: Heatmaps from various semantic levels on FFHQ dataset (left) and quantitative results of Fr\u00e9chet Distance (FD $\\\\downarrow$) and Centered Kernel Alignment (CKA $\\\\uparrow$) on ImageNet dataset (right). ViT [16] serves as the feature extractor for hierarchical evaluation here.\\n\\n| Shallow Model | BigGAN | BigGAN-deep | StyleGAN-XL |\\n|---------------|--------|-------------|-------------|\\n| Layer 1       | 0.20   | 0.19        | 0.01        |\\n| Layer 2       | 1.31   | 1.19        | 0.18        |\\n| Layer 3       | 6.93   | 6.06        | 1.22        |\\n| Layer 4       | 29.95  | 23.98       | 8.51        |\\n| Overall       | N/A    | N/A         | N/A         |\"}"}
{"id": "C0zw2ERKiQ", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table A11: Heatmaps from various semantic levels on FFHQ dataset (left) and quantitative results of Fr\u00e9chet Distance (FD \u2193) and Centered Kernel Alignment (CKA \u2191) on ImageNet dataset (right). MoCo-ViT [9] serves as the feature extractor for hierarchical evaluation here.\\n\\n| Layer | FD (\u2193) | CKA (\u2191) | FD (\u2193) | CKA (\u2191) |\\n|-------|--------|---------|--------|---------|\\n| Layer 1 | 0.10 | 98.62 | 0.05 | 99.04 |\\n| Layer 2 | 1.01 | 97.15 | 0.68 | 97.30 |\\n| Layer 3 | 9.18 | 96.07 | 9.01 | 96.77 |\\n| Layer 4 | 3.35 | 97.25 | 3.22 | 97.82 |\\n| Overall | N/A | 97.27 | N/A | 97.73 |\\n\\nE Visualized quantitative results\\n\\nTo make our results easier to parse, we visualize the correlation between different metrics and the human evaluation results. Specifically, we plot the correlation of the averaged ranks of various models given by human judgment, CKA, and FID. Fig. A12 and Fig. A13 respectively present the visualization results of the ImageNet, FFHQ, and LSUN-Church datasets. Obviously, the averaged ranks given by CKA are more consistent with that of the human evaluation, demonstrating the accuracy of CKA. Moreover, we plot the comparison between the stds and the improvements obtained by the histogram attack for better illustration. Fig. A14 presents the results. Similarly, we could observe that the improvement is actually caused by the histogram attack rather than the variance of attempts.\\n\\nFigure A12: The correlation of the averaged ranks of various models on ImageNet given by human judgment, CKA, and FID.\"}"}
{"id": "C0zw2ERKiQ", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure A13: The correlation of the averaged ranks of various models on FFHQ and LSUN-Church given by human judgment, CKA, and FID.\\n\\nFigure A14: The quantitative comparison between the stds and the improvements obtained by the histogram attack.\"}"}
{"id": "C0zw2ERKiQ", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Revisiting the Evaluation of Image Synthesis with GANs\\n\\nMengping Yang\\n\\nCeyuan Yang\\n\\nYichi Zhang\\n\\nQingyan Bai\\n\\nYujun Shen\\n\\nBo Dai\\n\\nShanghai AI Laboratory\\n\\nAnt Group\\n\\nTsinghua University\\n\\nAbstract\\n\\nA good metric, which promises a reliable comparison between solutions, is essential for any well-defined task. Unlike most vision tasks that have per-sample ground-truth, image synthesis tasks target generating unseen data and hence are usually evaluated through a distributional distance between one set of real samples and another set of generated samples. This study presents an empirical investigation into the evaluation of synthesis performance, with generative adversarial networks (GANs) as a representative of generative models. In particular, we make in-depth analyses of various factors, including how to represent a data point in the representation space, how to calculate a fair distance using selected samples, and how many instances to use from each set. Extensive experiments conducted on multiple datasets and settings reveal several important findings. Firstly, a group of models that include both CNN-based and ViT-based architectures serve as reliable and robust feature extractors for measurement evaluation. Secondly, Centered Kernel Alignment (CKA) provides a better comparison across various extractors and hierarchical layers in one model. Finally, CKA is more sample-efficient and enjoys better agreement with human judgment in characterizing the similarity between two internal data correlations. These findings contribute to the development of a new measurement system, which enables a consistent and reliable re-evaluation of current state-of-the-art generative models.\\n\\nIntroduction\\n\\nThrough reproducing realistic data distribution, generative models [67, 2, 19, 38, 28, 52] have enabled thrilling opportunities to go beyond existing observations via content recreation. Their technical breakthroughs in recent years also directly lead to the blooming of metaverse, AI Generated Content (AIGC), and various other downstream applications. However, accurately measuring the progress and performance of generative models poses significant challenges, as it requires a consistent and comprehensive evaluation of the divergence between real and synthesized data distributions. Among existing evaluation metrics [41, 4, 34, 44], Fr\u00e9chet Inception Distance (FID) [22] is the most popular evaluation paradigm for synthesis comparison. Despite its popularity, recent studies [35, 44, 36, 1, 26] have identified several flaws in the FID metric that may miscalculate the actual improvements of generative models. Consequently, there is a pressing need for a more systematic and thorough investigation in order to provide a more accurate assessment of synthesis performance.\\n\\nTherefore, this paper presents an empirical study that rigorously revisits the consistency and comprehensiveness of evaluation paradigms for generative models. Commonly used paradigms including FID typically contain two key components: a feature extractor $\\\\phi(\\\\cdot)$ and a distributional distance $d(\\\\cdot)$. Through $\\\\phi(\\\\cdot)$, i.e., Inception-V3 [56], a number of real samples ($x \\\\in X$) and synthesized samples are compared to evaluate the performance of generative models.\"}"}
{"id": "C0zw2ERKiQ", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ones ($y \\\\in Y$) are projected into a pre-defined representation space to approximate the respective data distributions. $d(\\\\cdot)$, i.e., Fr\u00e9chet Distance [17] is then calculated in the space to deliver the similarity index, indicating the synthesis quality. Following the philosophy, our study revolves around the representation space defined by the feature extractor $\\\\phi(\\\\cdot)$, and the distributional distance $d(\\\\cdot)$.\\n\\nThe most commonly used feature extractor, i.e., Inception-V3, has been found to encode limited semantics and possesses a large perceptual null representation space [35], making it hardly reflect the actual improvement of synthesis. Accordingly, we gather multiple models that vary in supervision signals, architectures, and representation similarities to investigate the impact of the feature extractor $\\\\phi(\\\\cdot)$, which are respectively motivated by 1) representation spaces defined by the extractor $\\\\phi(\\\\cdot)$ usually encode different levels of semantics, understanding which extractor or set of extractors can capture rich semantics is crucial yet less explored; 2) and it remains uncertain how the correlations between various representation spaces affect the evaluation results. In addition to studying the feature extractor $\\\\phi(\\\\cdot)$, we further delve into the consistency across spaces, the choice of distances, and the number of evaluated samples for the distributional distance $d(\\\\cdot)$. It is imperative that the distributional distance consistently provides a reliable similarity index, even when measured in various representation spaces. Similarly, selecting an appropriate number of samples to represent the synthesis distribution is a critical consideration. Regarding the choice of $d(\\\\cdot)$, besides Fr\u00e9chet Distance, we incorporate Centered Kernel Alignment (CKA) [11, 10] to explore alternatives for a more accurate evaluation.\\n\\nIn order to qualitatively and quantitatively compare different choices of the aforementioned aspects of $\\\\phi(\\\\cdot)$ and $d(\\\\cdot)$, we re-implement the visualization pipeline and the histogram matching technique as in [35]. These techniques allow us to respectively highlight the most relevant semantics of an image w.r.t the similarity indexes and to attack the measurement system through a selected subset. Moreover, we conduct an extensive user study involving 100 participants to investigate the correlation between the synthesis measurement and human judgment. Through these analysis tools, we make several significant findings: 1) One specific extractor (e.g., Inception-V3) tends to capture limited semantics and provide unreliable measurement results. 2) Various extractors naturally focus on different aspects of semantics thus demonstrating the potential generalization across different domains, motivating us to incorporate multiple extractors to deliver a comprehensive and reliable measurement. 3) With respect to features obtained from multiple representation spaces defined by different extractors, CKA proves to be effective in measuring the discrepancy and produces bounded values that facilitate comparison across different spaces. 4) In conjunction with the extensive user study, CKA consistently agrees with human judgment whereas FID failed in some circumstances, further underscoring the advantages of using CKA as the evaluation metric.\\n\\nAfter revisiting various factors, we leverage the newly developed measurement system to re-evaluate extensive generative models under various settings. Current state-of-the-art generative models on several domains are first benchmarked through our system. Moreover, the performances and intrinsic properties of diffusion models and GANs are comprehensively compared with the new measurement system. Furthermore, the measurement system is employed to evaluate the performance of image-to-image translation models. It turns out that our system not only delivers a similar assessment with FID and human evaluation in most cases, but also demonstrates a more reliable and consistent correlation with human judgment than FID, demonstrating the robustness and superiority of the proposed metric.\\n\\n2 Preliminary\\nThis section briefly introduces the feature extractor $\\\\phi(\\\\cdot)$, distributional distance $d(\\\\cdot)$, evaluated datasets and generative models, as well as auxiliary analysis approaches used in our study.\\n\\n2.1 Representation Spaces\\nTo investigate the effect of feature extractors $\\\\phi(\\\\cdot)$, we gather multiple models that are pre-trained on different objectives in fully-supervised/self-supervised manners, and with various architectures (e.g., ViT, CNN, and MLP).\\n\\nSupervision. Models that are trained in fully-supervised and self-supervised manners are collected due to their potential for generalization. In particular, we include the backbone networks which are\"}"}
{"id": "C0zw2ERKiQ", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"well-trained on supervised ImageNet classification tasks \\\\cite{13, 21}. Further, we gather the weights derived from single-modal/multi-modal self-supervised learning approaches \\\\cite{46, 8, 9, 6}.\\n\\n**Architecture.** In addition to considering models trained with different supervisions, we also include models with various architectures. Concretely, models with CNNs \\\\cite{21, 56, 46, 8}, ViTs \\\\cite{9, 39, 46}, as well as MLPs \\\\cite{61} architectures are gathered together for our investigation.\\n\\n| Properties  | Feature Extractors | Supervision | Architecture |\\n|-------------|--------------------|-------------|--------------|\\n|             |                    | Fully-supervised \\\\cite{56, 21} | CNNs \\\\cite{21, 46, 8}, ViTs \\\\cite{39, 46, 9}, MLPs \\\\cite{61, 59, 37} |\\n\\n2.2 Distributional Distances\\n\\n**Fr\u00e9chet Inception Distance (FID)**\\n\\ncomputes the Fr\u00e9chet Distance \\\\cite{17} between two estimated Gaussian distributions, \\\\( N(\\\\mu_s, \\\\Sigma_s) \\\\) and \\\\( N(\\\\mu_r, \\\\Sigma_r) \\\\), which represent the feature distributions of synthesized and real images extracted by the pre-trained Inception-V3. Formally, Fr\u00e9chet Distance (FD) is calculated by\\n\\n\\\\[\\nFD(X, Y) = \\\\|\\\\mu_s - \\\\mu_r\\\\|^2_2 + \\\\text{Tr}(\\\\Sigma_s + \\\\Sigma_r - 2(\\\\Sigma_s \\\\Sigma_r)^{1/2}),\\n\\\\]\\n\\nwhere \\\\( X \\\\) and \\\\( Y \\\\) represent the real distribution and synthesized distribution, respectively. \\\\( \\\\mu \\\\) and \\\\( \\\\Sigma \\\\) correspond to the mean and variance of Gaussian distribution, and \\\\( \\\\text{Tr}(\\\\cdot) \\\\) is the trace operation.\\n\\n**Centered Kernel Alignment (CKA)**\\n\\nas a widely used similarity index for quantifying neural network representations \\\\cite{11, 32, 12}, could also serve as a metric of similarity between two given data distributions. CKA has been identified to have several advantages: 1) CKA is invariant to orthogonal transformation and isotropic scaling, making it stable under various image transformations; 2) CKA can capture the non-linear correspondence between representations benefit from its kernel mapping; and 3) CKA can determine the correspondence across different features and with different widths, whereas previous metrics fail \\\\cite{32}.\\n\\nFormally, CKA is normalized from Hilbert-Schmidt Independence Criterion (HSIC) \\\\cite{20} to ensure invariant to isotropic scaling and is calculated by\\n\\n\\\\[\\n\\\\text{CKA}(X, Y) = \\\\frac{\\\\text{HSIC}(x, y)}{\\\\sqrt{\\\\text{HSIC}(x, x) \\\\text{HSIC}(y, y)}}.\\n\\\\]\\n\\nHere, HSIC determines whether two distributions are independent. Formally, let \\\\( K_{ij} = k(x_i, x_j) \\\\) and \\\\( L_{ij} = l(y_i, y_j) \\\\), where \\\\( k \\\\) and \\\\( l \\\\) are two kernels. HSIC is defined as\\n\\n\\\\[\\n\\\\text{HSIC}(K, L) = \\\\frac{1}{n^2 - 1} \\\\text{Tr}(KHLLH),\\n\\\\]\\n\\nwhere \\\\( H \\\\) denotes the centering matrix (i.e., \\\\( H_n = I_n - \\\\frac{1}{n} 1_n 1_n^T \\\\)). For kernel selections of \\\\( k \\\\) and \\\\( l \\\\), we find that different kernels (RBF, polynomial, and linear) give similar results and rankings, and the RBF kernel contributes to the distinguishability of quantitative results. Therefore, RBF kernel is used for all experiments, and the bandwidth is set as a fraction of the median distance between examples \\\\cite{32}.\\n\\nThese metrics are compared in a consistent setting for fair comparison, more implementation details and comparisons on CKA are given in Supplementary Material.\\n\\n2.3 Benchmarks and Analysis Approaches\\n\\n**Benchmarks.** In order to analyze various factors of measurement, we also collect multiple generators to produce synthesized images. To be more specific, we employ state-of-the-art generative models trained on various datasets for comparison in Tab. 1. We download corresponding publicly available models for comparison. Unless otherwise specified, all of these models are compared in a consistent setting.\\n\\n**Visualization tool.** To qualitatively compare where these feature extractors \u201cfocus on\u201d, we employ the visualization technique of \\\\cite{35} to localize the regions that contribute the most to the similarity index. The highlighted regions reveal the most relevant parts of an image regarding the measurement results. Accordingly, larger highlighted regions indicate that more visual semantics are involved.\"}"}
{"id": "C0zw2ERKiQ", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Generative models used in our study. Publicly available models are gathered for evaluation.\\n\\n| Method                  | Year | Training Datasets         |\\n|-------------------------|------|---------------------------|\\n| StyleGAN2 [29]          | 2020 | FFHQ, LSUN Church         |\\n| BigGAN, BigGAN-deep [5] | 2019 | ImageNet                  |\\n| ADM, ADM-G [14]         | 2021 | ImageNet                  |\\n| Projected-GAN [50]      | 2021 | FFHQ, LSUN Church         |\\n| InsGen [66]             | 2021 | FFHQ                      |\\n| StyleGAN-XL [66]        | 2022 | FFHQ, ImageNet            |\\n| Aided-GAN [33]          | 2022 | LSUN Church               |\\n| EqGAN [62]              | 2022 | FFHQ, LSUN Church         |\\n| Diffusion-GAN [64]      | 2022 | LSUN Church               |\\n| DiT [45], BigRoC [18]   | 2022 | ImageNet                  |\\n| GigaGAN [27], DG-Diffusion [31], MDT [19] | 2023 | ImageNet                  |\\n\\nin the evaluation. Note that generating a realistic image requires all parts, even each pixel, to be well-synthesized. Thus, a metric that focuses on more visual regions appears to be more dependable.\\n\\nHistogram matching attack. We employ the histogram matching [35] to attack the system to investigate the robustness of the measurement results. Concretely, a subset is selected from a superset by matching the class distribution of the synthesized set with that of the real set. As pointed out by [35], the synthesis performance could be substantially improved using the chosen subset. We thus prepare two distinct sets of synthesized images. One reference set (\u201cRandom\u201d) is produced by generating images randomly, and the other set (\u201cChosen\u201d) is carefully curated by matching the class distribution histogram of the supervised Inception-V3 [56]. The matched histograms are available in Supplementary Material. Since the generator and real data remain unaltered, the evaluation should keep consistent, and any performance gains directly indicate the unreliability of a given extractor.\\n\\nHuman judgment. In order to examine the correlation between the evaluation system and human perceptual judgment, we conduct extensive user studies employing two strategies. Firstly, to benchmark the synthesis quality of various generative models, we prepare a substantial number of randomly generated images (i.e., 5K), and ask 100 individuals to assess the photorealism of these images. The final scores are averaged across all 100 participants. Secondly, to qualitatively compare two paired generative models with similar quantitative performances (e.g., Projected-GAN [50] and Aided-GAN [33] on LSUN Church dataset in Sec. 4.1), we prepare groups of paired images generated by different models and ask 100 individuals to assess the perceptual quality of paired images. More details of our user studies can be found in Supplementary Material. In this way, we could obtain reliable and consistent human judgments, facilitating better investigation with the evaluation system.\\n\\n3 Analysis on Representation Spaces and Distributional Distances\\nIn this section, we investigate the potential impacts of the representation space and distributional distances with respect to the final similarity index. Concretely, Sec. 3.1 presents the study of extractors that define the representation spaces, followed by the analysis of distributional distances in Sec. 3.2.\\n\\n3.1 Representation Spaces\\n\\nPrior works [35, 36, 41, 26] have demonstrated that the most commonly used feature extractor i.e., Inception-V3 [56], could hardly reflect the exact improvement of synthesis due to its limited consideration of semantics. We thus conduct a comprehensive study by incorporating various models that differ in supervision signals and architectures, to identify which set of extractors serve as reliable feature extractors for synthesis comparison. Distinct feature extractors with different architectures yield varying semantic areas of focus. Fig. 1 shows the highlighted regions that contribute most significantly to the measurement results. Obviously, CNN-based extractors consistently emphasize concentrated regions with or without manual labels for pre-training, including limited semantics. Specifically, CNN-based extractors remain to highlight objects (e.g., microphone, hat, and sunglasses), rather than the main focus of the evaluation domains (i.e., Human Faces here). In contrast, ViT-based extractors capture larger regions that encompass more synthesis details and semantics. This observation aligns with the finding that ViTs possess a global and expansive receptive field compared to the local receptive field of...\"}"}
{"id": "C0zw2ERKiQ", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CNN-based extractors (i.e., Inception [56], ConvNeXt [40], SWAV [6], MoCo-R [9], CLIP-R [46], and RepVGG [15]) focus on objects whereas ViT-based (i.e., Swin-Transformer [39], ViT [16], DeiT [60], CLIP-V [46], and MoCo-V [9]) and MLP-based (i.e., ResMLP [61]) ones pour attention on wider areas.\\n\\nTable 2: Quantitative comparison results of Fr\u00e9chet Distance (FD) \u2193 on FFHQ dataset. \u201cRandom, Chosen\u201d respectively represent the synthesized distribution of randomly generated and matching the class prediction of Inception-V3. Moreover, \u201c-R\u201d and \u201c-V\u201d respectively denote the architecture of ResNet and ViT. (\u2193) indicates the results are hacked by the histogram matching mechanism. Notably, the values across different rows are not comparable and we report the stds of three testings to better illustrate the numerical fluctuation of various extractors towards the histogram attack. The results of Fr\u00e9chet Distance (FD) on ImageNet can be found in Supplementary Material.\\n\\n| Model       | Random       | Chosen       |\\n|-------------|--------------|--------------|\\n| Inception   | 2.81\u00b10.01    | 2.65\u00b10.01    |\\n| ConvNeXt    | 78.03\u00b10.10   | 78.19\u00b10.11   |\\n| SWAV        | 0.13\u00b10.002   | 0.13\u00b10.002   |\\n| MoCo-R      | 0.24\u00b10.003   | 0.24\u00b10.003   |\\n| RepVGG      | 129.61\u00b10.41  | 129.67\u00b10.39  |\\n| CLIP-R      | 10.34\u00b10.06   | 10.36\u00b10.08   |\\n| Swin        | 142.87\u00b10.12  | 140.01\u00b10.12  |\\n| ViT         | 15.11\u00b10.09   | 15.11\u00b10.10   |\\n| DeiT        | 437.80\u00b10.14  | 430.81\u00b10.16  |\\n| CLIP-V      | 1.06\u00b10.01    | 1.06\u00b10.01    |\\n| MoCo-V      | 7.32\u00b10.03    | 7.40\u00b10.03    |\\n| ResMLP      | 99.11\u00b10.06   | 95.36\u00b10.06   |\\n\\nMultiple extractors incorporate more visual semantics in a complementary manner. When reproducing the whole data distribution, generative models are required to synthesize not only the main objects but also the background, texture, and intricate details. Similarly, the extractors should strive to capture more regions of given images to approximate the data distribution, enabling better visual perception. However, the above observation regarding the heatmaps of different extractors reveals that each individual extractor could only capture partial semantics of the entire image for measurement, inadequately reflecting the overall synthesis performance. Consequently, various extractors with different architectures should be considered since they could involve more semantics, enhancing the reliability of the evaluation.\\n\\nExtractors that are vulnerable to the histogram matching attack are not reliable for evaluation. Prior study [35] has highlighted that extractors focusing on limited semantics may be susceptible to the histogram matching attack, undermining the trustworthiness of the evaluation. Motivated by this, we investigate the robustness of the above extractors toward the attack as they attach different importance to the visual concepts. Concretely, we obtain the publicly available generator of StyleGAN2 [30] and calculate the Fr\u00e9chet Distance (FD) results on the FFHQ dataset. We compare two evaluated sets: one is randomly generated using the model, while the other is chosen by matching the predicted class distribution of Inception-V3 (the matched histograms are provided in Supplementary Material).\\n\\nTab. 2 shows the quantitative results. Comparing the performances of the random set and chosen set, we could tell that certain extractors, regardless of their architectures (e.g., CNN-based Inception-V3 [56], ViT-based Swin-Transformer [39], and MLP-based ResMLP [61]), are susceptible to the histogram matching attack. For instance, the FD score of Inception shows an improvement of 5.7% when the chosen set is used, and ResMLP exhibits a 3.8% improvement (More quantitative}\"}"}
{"id": "C0zw2ERKiQ", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and qualitative results of MLP-based extractors gMLP [37] and MLP-mixer [59] are shown in Supplementary Material). Namely, the FD score could be improved without making any changes to the generators, aligning with the observations of [35]. We thus filter out the extractors that are vulnerable to the attack since their rankings can be manipulated without actual improvement to the generative models.\\n\\nExtractors that define similar representation spaces are redundant. So far, we have demonstrated the importance of considering multiple extractors for a more comprehensive evaluation and filtered out the extractors that are susceptible to the histogram attack. However, it is also crucial to avoid redundancy among the remaining extractors, as they may define similar representation spaces. To address this, we examine the correlation between representation spaces across different feature extractors, following the approach outlined in [32]. Specifically, a significant number of images (10K images from ImageNet) are fed into these extractors to compute their correspondence. After calculating the similarity matrix, we can further filter out extractors that define homogeneous representation spaces (the similarity analysis is presented in Supplementary Material). The remaining extractors are presented in the table below. These extractors 1) capture rich semantics in a CNN-based ConvNeXt [40], SW A V [6], RepVGG [15] ViT-based CLIP-ViT [46], MoCo-ViT [9], ViT [16] complementary way, 2) are robust toward the histogram matching attack, and 3) define meaningful and distinctive representation spaces. Besides, both CNN-based and ViT-based extractors are considered and all of them have demonstrated strong performance for the pre-defined and downstream tasks, facilitating more comprehensive and reliable evaluation. Notably, the selection of self-supervised extractors SW A V, CLIP-ViT, and MoCo-ViT agrees with previous studies [41, 35, 3]. The selected extractors can serve as reliable tools for synthesis evaluation.\\n\\nIn order to investigate the reliability of the selected extractors, we employ them to test the synthesis quality of representative generative models on the ImageNet dataset. Tab. 3 presents the quantitative FD scores of various extractors. Although their results differ in numerical scales, they consistently indicate that StyleGAN-XL outperforms both BigGAN and BigGAN-deep by a significant margin, and BigGAN-deep performs slightly better than BigGAN. Such observation also agrees with the human judgment in Tab. 5. Overall, the consistent trends observed across different extractors and the alignment with human judgment confirm that the selected extractors are reliable for comparing the synthesis quality.\\n\\n3.2 Distributional Distances\\n\\nAfter the study of feature extractors, we shift our focus to another essential component of measurement, i.e., the distributional distance $d(\\\\cdot)$. Besides Fr\u00e9chet Distance (FD), we also incorporate Centered Kernel Alignment (CKA) for our investigation. CKA provides normalized distances w.r.t numerical scales in variable representation spaces. Tab. 5 demonstrates the quantitative results of Centered Kernel Alignment (CKA). Unlike the Fr\u00e9chet Distance (FD) scores that exhibit significant fluctuations across various extractors, the CKA scores demonstrate remarkable stability when evaluated in different representation spaces. For instance, the FD score of BigGAN on MoCo-ViT is 238.78 whereas 3.35 on CLIP-ViT, making it challenging to combine the results of different extractors. By contrast, the stability of CKA scores allows the ability to combine results from multiple extractors (i.e., average) for better comparison. CKA demonstrates great potential for quantitative comparison and combination across hierarchical layers. Here we investigate the distributional distances across various layers of the neural network, as these layers typically extract multi-level features that span from high-level semantics to low-level details. Accordingly, considering hierarchical features can provide a more comprehensive measurement. The left part of Tab. 4 presents the qualitative results of different layers' heatmaps. We can observe that different layers indeed extract different semantics, highlighting the importance of considering hierarchical features in evaluation. Additionally, we provide the quantitative results of FD and CKA in the right part of Tab. 4. Still, the FD scores of various layers fluctuate dramatically, e.g., the FD results of 0.60 in Layer 1 while 104.10 in Layer 4. Differently, the CKA results from hierarchical layers are comparable and the overall score could be derived by averaging multi-level scores. Importantly, the overall score still reflects synthesis quality consistently and reliably. CKA shows satisfactory sample-efficiency and stability under different number of samples. Typically, the synthesis quality are measured between real and synthesized distributions, where the\"}"}
{"id": "C0zw2ERKiQ", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Quantitative comparison results of Fr\u00e9chet Distance (FD \u2193) on ImageNet dataset. \u2020 scores are quoted from the original paper and other results are tested three times. Notably, the values across different columns are not comparable.\\n\\n| Model                  | FID     | ConvNeXt | RepVGG  | SW A    | V ViT   | MoCo-ViT | CLIP-ViT |\\n|------------------------|---------|----------|---------|---------|---------|----------|----------|\\n| **Overall User study** |         |          |         |         |         |          |          |\\n| BigGAN [5]             | 8.70    | 140.04   | 67.53   | 1.12    | 29.95   | 238.78   | 3.35     |\\n| N/A                    |         |          |         |         |         |          |          |\\n| BigGAN-deep [5]        | 6.02    | 102.26   | 58.85   | 0.87    | 23.98   | 85.83    | 3.22     |\\n| N/A                    |         |          |         |         |         |          |          |\\n| StyleGAN-XL [51]       | 1.81    | 19.22    | 15.93   | 0.18    | 8.51    | 29.38    | 1.85     |\\n| N/A                    |         |          |         |         |         |          |          |\\n\\nTable 4: Heatmaps from various semantic levels on FFHQ dataset (left) and Fr\u00e9chet Distance (FD \u2193) and Centered Kernel Alignment (CKA \u2191) scores on ImageNet dataset (right). CLIP-ViT serves as the feature extractor here, more results can be found in Supplementary Material.\\n\\n| Layer | FD \u2193 | CKA \u2191 | FD \u2193 | CKA \u2191 | FD \u2193 | CKA \u2191 |\\n|-------|------|-------|------|-------|------|-------|\\n| Layer 1 | 0.60 | 99.06 | 0.54 | 98.95 | 0.05 | 99.84 |\\n| Layer 2 | 7.45 | 86.89 | 5.58 | 90.09 | 0.77 | 91.06 |\\n| Layer 3 | 30.24 | 82.80 | 23.55 | 83.63 | 6.11 | 85.75 |\\n| Layer 4 | 104.10 | 80.13 | 81.02 | 81.05 | 35.77 | 83.55 |\\n| Overall | N/A | 87.22 | N/A | 88.43 | N/A | 90.05 |\\n\\nWhole training data is used as the real distribution and 50K generated images as the synthesized, regardless of how many samples contained in the training data. However, when evaluating on the large-scale datasets (e.g., 1.28 million images for ImageNet), 50K images may be insufficient to represent the entire distribution. Therefore, we study the impacts of the amount of generated samples.\\n\\nConcretely, we prepare several synthesized sets with different numbers of samples and calculate their distances to the real distribution (more details are presented in Supplementary Material). Fig. 2 demonstrates the curves of FD and CKA scores evaluated under different data regimes on the FFHQ dataset. Obviously, the FD scores can be drastically improved by synthesizing more data regardless of different extractors until sufficient samples (\u223c100K) are used, whereas the CKA scores are stable under different data regimes. Moreover, CKA could measure the distributional distances precisely with only 5K synthesized samples, suggesting significant sample efficiency. Such observations demonstrate CKA's impressive adaptability toward the amount of synthesized data.\\n\\nDeveloping a reliable and comprehensive measurement system for synthesis evaluation. Overall, a set of feature extractors that 1) are robust to the histogram matching attack, 2) capture sufficient semantics, and 3) define distinctive representation spaces could serve as reliable extractors for synthesis comparison. Together with a bounded distance (i.e., CKA) that is comparable across various representation spaces and hierarchical layers, as well as enjoys satisfactory sample efficiency. These two essential components constitute a reliable system to deliver the distributional discrepancy.\\n\\nFigure 2: Fr\u00e9chet Distance (FD) and Centered Kernel Alignment (CKA) scores evaluated under various data regimes on FFHQ dataset. FID scores are scaled for better visualization. \u2193 denotes the results fluctuate downward. The percentages represent the magnitude of the numerical variation. The curve of KID [4], Precision and Recall [49] can be found in Supplementary Material.\"}"}
{"id": "C0zw2ERKiQ", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Quantitative comparison results of Centered Kernel Alignment (CKA $\\\\uparrow$) on ImageNet dataset.\u2020 scores are quoted from the original paper and others are tested three times. To make our results more trivial to parse, we visualize the correlation between different metrics the human evaluation results in the Supplementary Material.\\n\\n| Model                | FID   | Overall | User study |\\n|----------------------|-------|---------|------------|\\n| ICGAN [7]            | 15.60 | 77.99   | 32%        |\\n| ADM [14]             | 10.94 | 78.69   | 45%        |\\n| BigGAN [5]           | 8.70  | 79.60   | 53%        |\\n| C-ICGAN [7]          | 7.50  | 78.01   | 31%        |\\n| BigGAN-deep [5]      | 6.95  | 80.27   | 55%        |\\n| Guided-ADM [14]      | 4.59  | 80.46   | 57%        |\\n| BigRoc [18]          | 3.69  | 82.25   | 65%        |\\n| GigaGAN [27]         | 3.45  | 82.40   | 65%        |\\n| DG-Diffusion [31]    | 3.18  | 82.51   | 66%        |\\n| StyleGAN-XL [51]     | 2.30  | 82.64   | 67%        |\\n| DiT [45]             | 2.27  | 82.90   | 67%        |\\n| MDT [19]             | 1.79  | 83.43   | 69%        |\\n\\nTable 6: Quantitative comparison results of Centered Kernel Alignment (CKA $\\\\uparrow$) on FFHQ dataset.\u2020 scores are quoted from the original paper and others are tested three times.\\n\\n| Model                | FID   | Overall | User study |\\n|----------------------|-------|---------|------------|\\n| StyleGAN2 [29]       | 3.66  | 97.46   | 45%        |\\n| Projected-GAN [50]   | 3.39  | 97.31   | 39%        |\\n| InsGen [66]          | 3.31  | 97.75   | 58%        |\\n| EqGAN [62]           | 2.89  | 98.61   | 62%        |\\n| StyleGAN-XL [51]     | 2.19  | 97.33   | 66%        |\\n\\n4 Benchmark Existing Generative Models\\n\\nBased on the findings regarding the feature extractors and distributional distances, we construct a new synthesis valuation system. Concretely, our system leverages a set of models with both CNN and ViT architectures as feature extractors, namely, CNN-based ConvNeXt [40], RepVGG [15], SWAV [6] and ViT-based ViT [16], MoCo-ViT [9], CLIP-ViT [46], with which more comprehensive evaluation could be accomplished. Further, Centered Kernel Alignment (CKA) serves as the similarity indicator. Accordingly, in this part we re-evaluate and compare the progress of existing generative models with our measurement system. Concretely, the latest generative models are re-evaluated with our system in Sec. 4.1, followed by our discussion about the diffusion models and GANs in Sec. 4.2. Notably, user studies are conducted for investigating the correlation between our system and human judgment.\\n\\n4.1 Comparison on Existing Generative models\\n\\nIn order to investigate the actual improvement of existing generative models, we collect multiple publicly available generators trained on several popular benchmarks (i.e., FFHQ, LSUN Church, and ImageNet) for comparison. Benefiting from the impressive sample efficiency of CKA, we generate $50K$ images as the synthesized distribution and use the whole datasets as the real distribution. Results from various selected extractors and their averaged scores are reported for a thorough comparison. Our system can measure the synthesis quality in a consistent and reliable way. Tab. 5, Tab. 7, and Tab. 6 respectively demonstrate the quantitative results of different generative models on the ImageNet, LSUN Church, and FFHQ datasets. In most cases, CKA scores from various extractors and the overall scores provide a consistent ranking with FID scores, as well as agree with human perceptual judgment. These results suggest that our new metric could precisely measure the synthesis quality. However, for the Projected-GAN [50] and StyleGAN2 [29] (resp., Aided-GAN [33]) evaluated on FFHQ (resp., LSUN Church) dataset in Tab. 6 (resp., Tab. 7), our evaluation system gives the opposite ranking to the FID. Namely, the quantitative results of StyleGAN2 (resp., Aided-GAN) are determined better than that of Projected-GAN under our evaluation, whereas the FID scores vote Projected-GAN for the better one. Additionally, the performances of ICGAN [7] and class-conditional ICGAN on ImageNet in Tab. 5 are identified basically the same by our metric, while FID scores indicate that the class-conditional one significantly surpasses the unconditional one.\"}"}
{"id": "C0zw2ERKiQ", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Quantitative comparison results of Centered Kernel Alignment (CKA) on LSUN Church dataset. \u2020 scores are quoted from the original paper and others are tested three times.\\n\\n| Model           | FID  | ConvNeXt RepVGG SW A V ViT MoCo-ViT CLIP-ViT |\\n|-----------------|------|---------------------------------------------|\\n| Overall         |      | 3.86                                        |\\n| User study      |      | 96.99                                       |\\n| StyleGAN2 [29]  | 3.17 | 97.15                                       |\\n| Diffusion-GAN   | 3.02 | 97.34                                       |\\n| EqGAN [66]      |      | 97.91                                       |\\n| Aided-GAN [33]  | 1.72 | 97.63                                       |\\n| Projected-GAN   | 1.59 | 97.95                                       |\\n\\nTable 8: Fine-grained investigation of human judgment. Percentages indicate the ratio of generated images that are considered to be more plausible.\\n\\n| Dataset     | FFHQ | LSUN Church | ImageNet |\\n|-------------|------|-------------|----------|\\n| Model       |      |             |          |\\n| Projected-GAN| 45%  | 55%         | 43%      |\\n| StyleGAN2   | 57%  |             | 50%      |\\n| Projected-GAN| 50%  | 50%         | 50%      |\\n| Aided-GAN   |      | 50%         | 50%      |\\n\\nIn order to compare the performance of these models in a fine-grained way, we further perform paired-wise human evaluation. Specifically, groups of paired images synthesized by different models (e.g., StyleGAN2 and Projected-GAN on FFHQ dataset) are randomly picked for visual comparison. Then, presented with two sets of images produced by different models, users are asked to determine which set of images is more plausible.\\n\\nOur measurement system provides the right rankings and better correlations with human visual judgment. Tab. 8 presents the quantitative results of human visual comparison. Observably, the synthesis quality of StyleGAN (resp. Aided-GAN) is more preferred by human visual judgment. That is, our measurement system produces the same rankings as the human perceptual evaluation, demonstrating the reliability of our metric. Moreover, our metric's indication that there's no significant gap between ICGAN and class-conditional ICGAN is also verified by the human evaluation.\\n\\nConsidering the perceptual null space of Inception-V3, one possible reason for the FID performance gains of Projected-GAN might be the usage of pre-trained models, which is also identified by [35]. By contrast, our measurement produces the right rankings and agrees well with human evaluation, reflecting the actual synthesis quality in a comprehensive and reliable way.\\n\\n4.2 Comparison between GANs and Diffusion Models\\n\\nDiffusion models [23, 54, 55, 14, 45, 48, 19, 70, 42, 24] have demonstrated significant advancements in visual synthesis and became the new trend of generative models recently. Benefiting from the reliability of our new measurement system, here we perform a comprehensive comparison between GANs and diffusion models. Specifically, we report the FID and the overall CKA scores, as well as human judgment for quantitative comparison. Additionally, the model parameters and the synthesis speed (tested on an A100 GPU) are also included.\\n\\nTab. 9 presents the quantitative results. Obviously, diffusion model (i.e., DiT) obtains comparable results with GAN (i.e., StyleGAN-XL), yet with much more parameters (i.e., 675 M vs. 166.3 M).\\n\\nMoreover, diffusion models usually require extra inference time to obtain realistic images. Such comparisons reveal that GANs achieve better trade-offs between efficiency and synthesis quality, and designing computation-efficient diffusion models is essential for the community.\\n\\n4.3 Comparing the performance of image-to-image translation\\n\\nIn order to testify the compatibility of our metric, here we employ our measurement system to evaluate the performance of image-to-image translation. We collect publicly available image-to-image translation models that are officially released to translate images from one domain to another domain for evaluation. Specifically, three translation benchmarks are involved here, namely Horse-to-Zebra [57, 71, 43], Cat-to-Dog [68, 43], and Dog-to-Cat [68, 25]. For each benchmark, we translate the tested images to the target domain following the original experimental settings. Then we compute the distributional discrepancies between the translated images and the real target images. Tab. 10 presents the quantitative results of the evaluated three image-to-image translation benchmarks. It can\"}"}
{"id": "C0zw2ERKiQ", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Quantitative comparison results of diffusion models and GANs on ImageNet dataset.\\n\\n| Model             | FID  | CKA  | User | #Params | Sec/Img |\\n|-------------------|------|------|------|---------|---------|\\n| BigGAN            | 8.70 | 82.82| 53%  | 158.3M  | 33.6    |\\n| BigGAN-deep       | 6.95 | 83.65| 55%  | 85M     | 27.6    |\\n| StyleGAN-XL       | 2.30 | 86.52| 67%  | 166.3M  | 64.8    |\\n| ADM               | 10.94| 82.12| 45%  | 500M    | 17.274  |\\n| Guided-ADM        | 4.59 | 84.66| 57%  | 554M    | 17.671  |\\n| DiT               | 2.27 | 86.61| 67%  | 675M    | 37.36   |\\n\\nIt can be seen from these results that CKA provides consistent ranks with FID among various extractors, and the averaged score can reflect the performance of different image translation models. For instance, the performance of CUT [43] on Horse-to-Zebra is identified better than that of CycleGAN [71] by both FID and our proposed metric. And the qualitative results in the original paper of CUT [43] also suggest that the performance of CUT surpasses CycleGAN. That is, our measurement system can provide a reliable evaluation under such settings. This indicates that our measurement system can also be used for evaluating the performance of image translation tasks.\\n\\nTable 10: Quantitative comparison results of Centered Kernel Alignment (CKA) on Image-to-Image translation tasks.\\n\\n| Horse-to-Zebra dataset | Model          | FID  | ConvNeXt | RepVGG | SW | A | V | ViT | MoCo-ViT | CLIP-ViT | Overall |\\n|------------------------|----------------|------|----------|--------|----|---|---|-----|----------|----------|---------|\\n|                        | CycleGAN [71]  | 83.32| 73.55    | 88.67  | 85.82 | 83.96 | 74.72 | 73.74 | 80.08    |\\n|                        | AttentionGAN [57] | 76.05| 75.59    | 91.73  | 86.37 | 85.16 | 76.65 | 75.49 | 81.83    |\\n|                        | CUT [43]       | 51.29| 78.48    | 93.22  | 88.83 | 87.84 | 78.75 | 77.36 | 84.08    |\\n\\n| Cat-to-Dog dataset     | Model          | FID  | ConvNeXt | RepVGG | SW | A | V | ViT | MoCo-ViT | CLIP-ViT | Overall |\\n|------------------------|----------------|------|----------|--------|----|---|---|-----|----------|----------|---------|\\n|                        | CUT [43]       | 74.95| 84.93    | 78.75  | 88.83 | 84.31 | 93.56 | 70.91 | 83.55    |\\n|                        | GP-UNIT [68]   | 60.96| 90.45    | 87.79  | 94.05 | 90.12 | 95.91 | 75.32 | 88.94    |\\n|                        | MUNIT [25]     | 31.66| 79.58    | 78.18  | 96.79 | 86.93 | 93.92 | 77.42 | 85.47    |\\n\\n5 Conclusion\\n\\nThis work revisits the evaluation of generative models from the perspectives of the feature extractor and the distributional distance. Through extensive investigation regarding the potential contribution of various feature extractors and distributional distances, we identify the impacts of several potential factors that contribute to the final similarity index. With these findings, we construct a new measurement system that provides a more comprehensive and holistic comparison for synthesis evaluation. Importantly, our system could present more consistent measurements with human judgment, enabling more reliable evaluation.\\n\\nDiscussion\\n\\nDespite a comprehensive investigation, our study could still be extended in several aspects. For instance, the impacts of different low-level image processing techniques (e.g., resizing) could be identified since they also play an important role in synthesis evaluation [44]. Besides, comparing datasets with various resolutions could be further studied. Nonetheless, our study could be considered an empirical revisiting towards the paradigm of evaluating generative models. We hope this work could inspire more fascinating works of synthesis evaluation and provide potential insight to develop more comprehensive evaluation protocols. We will also conduct more investigation on the unexplored factors and compare more generative models with our system.\\n\\nReferences\\n\\n[1] M. Alfarra, J. C. P\u00e9rez, A. Fr\u00fchst\u00fcck, P. H. Torr, P. Wonka, and B. Ghanem. On the robustness of quality measures for GANs. Eur. Conf. Comput. Vis., pages 18\u201333, 2022.\"}"}
{"id": "C0zw2ERKiQ", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Q. Bai, C. Yang, Y. Xu, X. Liu, Y. Yang, and Y. Shen. Glead: Improving gans with a generator-leading task. IEEE Conf. Comput. Vis. Pattern Recog., 2023.\\n\\n[2] E. Betzalel, C. Penso, A. Navon, and E. Fetaya. A study on the evaluation of generative models. arXiv preprint arXiv:2206.10935, 2022.\\n\\n[3] M. Binkowski, D. J. Sutherland, M. Arbel, and A. Gretton. Demystifying MMD GANs. In Int. Conf. Learn. Represent., 2018.\\n\\n[4] A. Brock, J. Donahue, and K. Simonyan. Large scale gan training for high fidelity natural image synthesis. In Int. Conf. Learn. Represent., 2019.\\n\\n[5] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Adv. Neural Inform. Process. Syst., pages 9912\u20139924, 2020.\\n\\n[6] A. Casanova, M. Careil, J. Verbeek, M. Drozdzal, and A. Romero-Soriano. Instance-conditioned gan. In Adv. Neural Inform. Process. Syst., 2021.\\n\\n[7] X. Chen, H. Fan, R. Girshick, and K. He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.\\n\\n[8] X. Chen, S. Xie, and K. He. An empirical study of training self-supervised vision transformers. In Int. Conf. Comput. Vis., pages 9640\u20139649, 2021.\\n\\n[9] C. Cortes, M. Mohri, and A. Rostamizadeh. Algorithms for learning kernels based on centered alignment. The Journal of Machine Learning Research, 13:795\u2013828, 2012.\\n\\n[10] N. Cristianini, J. Shawe-Taylor, A. Elisseeff, and J. Kandola. On kernel-target alignment. Adv. Neural Inform. Process. Syst., 2001.\\n\\n[11] M. Davari, S. Horoi, A. Natik, G. Lajoie, G. Wolf, and E. Belilovsky. Reliability of cka as a similarity measure in deep learning. arXiv preprint arXiv:2210.16156, 2022.\\n\\n[12] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conf. Comput. Vis. Pattern Recog., pages 248\u2013255, 2009.\\n\\n[13] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. In Adv. Neural Inform. Process. Syst., pages 8780\u20138794, 2021.\\n\\n[14] X. Ding, X. Zhang, N. Ma, J. Han, G. Ding, and J. Sun. Repvgg: Making vgg-style convnets great again. In IEEE Conf. Comput. Vis. Pattern Recog., pages 13733\u201313742, 2021.\\n\\n[15] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Int. Conf. Learn. Represent., 2020.\\n\\n[16] D. Dowson and B. Landau. The fr\u00e9chet distance between multivariate normal distributions. Journal of multivariate analysis, 12(3):450\u2013455, 1982.\\n\\n[17] R. Ganz and M. Elad. BIGRoc: Boosting image generation via a robust classifier. Transactions on Machine Learning Research, 2022. ISSN 2835-8856.\\n\\n[18] S. Gao, P. Zhou, M.-M. Cheng, and S. Yan. Masked diffusion transformer is a strong image synthesizer. Int. Conf. Comput. Vis., 2023.\\n\\n[19] A. Gretton, O. Bousquet, A. Smola, and B. Sch\u00f6lkopf. Measuring statistical dependence with hilbert-schmidt norms. In International conference on algorithmic learning theory, pages 63\u201377, 2005.\\n\\n[20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conf. Comput. Vis. Pattern Recog., pages 770\u2013778, 2016.\\n\\n[21] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Adv. Neural Inform. Process. Syst., 2017.\\n\\n[22] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Adv. Neural Inform. Process. Syst., pages 6840\u20136851, 2020.\\n\\n[23] L. Huang, D. Chen, Y. Liu, Y. Shen, D. Zhao, and J. Zhou. Composer: Creative and controllable image synthesis with composable conditions. arXiv preprint arXiv:2302.09778, 2023.\"}"}
{"id": "C0zw2ERKiQ", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[26] M. Jiralerspong, A. J. Bose, and G. Gidel. Feature likelihood score: Evaluating generalization of generative models using samples. arXiv preprint arXiv:2302.04440, 2023.\\n\\n[27] M. Kang, J.-Y. Zhu, R. Zhang, J. Park, E. Shechtman, S. Paris, and T. Park. Scaling up GANs for text-to-image synthesis. In IEEE Conf. Comput. Vis. Pattern Recog., pages 10124\u201310134, 2023.\\n\\n[28] M. Kang, J.-Y. Zhu, R. Zhang, J. Park, E. Shechtman, S. Paris, and T. Park. Scaling up GANs for text-to-image synthesis. IEEE Conf. Comput. Vis. Pattern Recog., 2023.\\n\\n[29] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila. Analyzing and improving the image quality of StyleGAN. In IEEE Conf. Comput. Vis. Pattern Recog., pages 8110\u20138119, 2020.\\n\\n[30] T. Karras, M. Aittala, S. Laine, E. H\u00e4rk\u00f6nen, J. Hellsten, J. Lehtinen, and T. Aila. Alias-free generative adversarial networks. Adv. Neural Inform. Process. Syst., pages 852\u2013863, 2021.\\n\\n[31] D. Kim, Y. Kim, W. Kang, and I.-C. Moon. Refining generative process with discriminator guidance in score-based diffusion models. Int. Conf. Mach. Learn., 2023.\\n\\n[32] S. Kornblith, M. Norouzi, H. Lee, and G. Hinton. Similarity of neural network representations revisited. In Int. Conf. Mach. Learn., pages 3519\u20133529, 2019.\\n\\n[33] N. Kumari, R. Zhang, E. Shechtman, and J.-Y. Zhu. Ensembling off-the-shelf models for GAN training. In IEEE Conf. Comput. Vis. Pattern Recog., pages 10651\u201310662, 2022.\\n\\n[34] T. Kynk\u00e4\u00e4nniemi, T. Karras, S. Laine, J. Lehtinen, and T. Aila. Improved precision and recall metric for assessing generative models. Adv. Neural Inform. Process. Syst., 32, 2019.\\n\\n[35] T. Kynk\u00e4\u00e4nniemi, T. Karras, M. Aittala, T. Aila, and J. Lehtinen. The role of ImageNet classes in Fr\u00e9chet Inception Distance. In arXiv preprint arXiv:2203.06026, 2022.\\n\\n[36] J. Lee and J.-S. Lee. Trend: Truncated generalized normal density estimation of Inception embeddings for GAN evaluation. In Eur. Conf. Comput. Vis., pages 87\u2013103, 2022.\\n\\n[37] H. Liu, Z. Dai, D. So, and Q. V. Le. Pay attention to MLPs. Adv. Neural Inform. Process. Syst., 34:9204\u20139215, 2021.\\n\\n[38] H. Liu, W. Zhang, B. Li, H. Wu, N. He, Y. Huang, Y. Li, B. Ghanem, and Y. Zheng. Improving GAN training via feature space shrinkage. arXiv preprint arXiv:2303.01559, 2023.\\n\\n[39] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Int. Conf. Comput. Vis., pages 10012\u201310022, 2021.\\n\\n[40] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s. In IEEE Conf. Comput. Vis. Pattern Recog., pages 11976\u201311986, 2022.\\n\\n[41] S. Morozov, A. Voynov, and A. Babenko. On self-supervised image representations for GAN evaluation. In Int. Conf. Learn. Represent., 2021.\\n\\n[42] C. Mou, X. Wang, L. Xie, J. Zhang, Z. Qi, Y. Shan, and X. Qie. T2I-Adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023.\\n\\n[43] T. Park, A. A. Efros, R. Zhang, and J.-Y. Zhu. Contrastive learning for unpaired image-to-image translation. In Eur. Conf. Comput. Vis., pages 319\u2013345, 2020.\\n\\n[44] G. Parmar, R. Zhang, and J.-Y. Zhu. On aliased resizing and surprising subtleties in GAN evaluation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 11410\u201311420, 2022.\\n\\n[45] W. Peebles and S. Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022.\\n\\n[46] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In Int. Conf. Mach. Learn., pages 8748\u20138763, 2021.\\n\\n[47] M. Raghu, T. Unterthiner, S. Kornblith, C. Zhang, and A. Dosovitskiy. Do vision transformers see like convolutional neural networks? In Adv. Neural Inform. Process. Syst., 2021.\"}"}
{"id": "C0zw2ERKiQ", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., pages 10684\u201310695, 2022.\\n\\nM. S. Sajjadi, O. Bachem, M. Lucic, O. Bousquet, and S. Gelly. Assessing generative models via precision and recall. Adv. Neural Inform. Process. Syst., 31, 2018.\\n\\nA. Sauer, K. Chitta, J. M\u00fcller, and A. Geiger. Projected gans converge faster. Adv. Neural Inform. Process. Syst., pages 17480\u201317492, 2021.\\n\\nA. Sauer, K. Schwarz, and A. Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH, pages 1\u201310, 2022.\\n\\nA. Sauer, T. Karras, S. Laine, A. Geiger, and T. Aila. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis. arXiv preprint arXiv:2301.09515, 2023.\\n\\nK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. Int. Conf. Learn. Represent., 2015.\\n\\nJ. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. In Int. Conf. Learn. Represent., 2021.\\n\\nY. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. In Int. Conf. Learn. Represent., 2021.\\n\\nC. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In IEEE Conf. Comput. Vis. Pattern Recog., pages 2818\u20132826, 2016.\\n\\nH. Tang, H. Liu, D. Xu, P. H. Torr, and N. Sebe. Attentiongan: Unpaired image-to-image translation using attention-guided generative adversarial networks. IEEE Trans. Neural Networks Learn. Syst., 2021.\\n\\nT. A. Tero Karras, Samuli Laine. Flickr-faces-hq dataset (ffhq). URL https://github.com/NVlabs/ffhq-dataset.\\n\\nI. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Adv. Neural Inform. Process. Syst., 34:24261\u201324272, 2021.\\n\\nH. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jegou. Training data-efficient image transformers & distillation through attention. In Int. Conf. Mach. Learn., pages 10347\u201310357, 2021.\\n\\nH. Touvron, P. Bojanowski, M. Caron, M. Cord, A. El-Nouby, E. Grave, G. Izacard, A. Joulin, G. Synnaeve, J. Verbeek, et al. Resmlp: Feedforward networks for image classification with data-efficient training. IEEE Trans. Pattern Anal. Mach. Intell., 2022.\\n\\nJ. Wang, C. Yang, Y. Xu, Y. Shen, H. Li, and B. Zhou. Improving gan equilibrium by raising spatial awareness. In IEEE Conf. Comput. Vis. Pattern Recog., pages 11285\u201311293, 2022.\\n\\nP. Wang, Y. Li, and N. Vasconcelos. Rethinking and improving the robustness of image style transfer. In IEEE Conf. Comput. Vis. Pattern Recog., pages 124\u2013133, 2021.\\n\\nZ. Wang, H. Zheng, P. He, W. Chen, and M. Zhou. Diffusion-gan: Training gans with diffusion. arXiv preprint arXiv:2206.02262, 2022.\\n\\nZ. Xia, X. Pan, S. Song, L. E. Li, and G. Huang. Vision transformer with deformable attention. In IEEE Conf. Comput. Vis. Pattern Recog., pages 4794\u20134803, 2022.\\n\\nC. Yang, Y. Shen, Y. Xu, and B. Zhou. Data-efficient instance generation from instance discrimination. Adv. Neural Inform. Process. Syst., pages 9378\u20139390, 2021.\\n\\nC. Yang, Y. Shen, Y. Xu, D. Zhao, B. Dai, and B. Zhou. Improving gans with a dynamic discriminator. In Adv. Neural Inform. Process. Syst., 2022.\\n\\nS. Yang, L. Jiang, Z. Liu, and C. C. Loy. Unsupervised image-to-image translation with generative prior. In IEEE Conf. Comput. Vis. Pattern Recog., pages 18332\u201318341, 2022.\\n\\nF. Yu, A. Seff, Y. Zhang, S. Song, T. Funkhouser, and J. Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.\\n\\nL. Zhang and M. Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023.\\n\\nJ.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Int. Conf. Comput. Vis., pages 2223\u20132232, 2017.\"}"}
{"id": "C0zw2ERKiQ", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This Supplementary Material is organized as follows: appendix B provides the implementation details of our experiments, appendix C demonstrates how human visual judgment is performed and presents the interface of our user study. Appendix D presents additional quantitative and qualitative results, including 1) comparison results with more metrics (KID, Precision and Recall), 2) more results of MLP-based extractors, 3) the similarities between various representation spaces, as well as 4) more hierarchical results from different semantic levels of various extractors. Finally, in appendix E, we provide 2D plots of the correlation between different metrics with human visual judgment to make our results easier to parse.\\n\\n**B Implementation Details**\\n\\n**B.1 Datasets**\\n\\nFFHQ [58] contains unique 70,000 human-face images with large variations in terms of age, ethnicity, and facial expressions. We employ the resolution of 256\u00d7256\u00d73 for our experiments.\\n\\nImageNet [13] includes 1,280,000 images with 1,000 classes of different objects such as goldfish, bow tie, etc. All experiments on ImageNet are performed with the resolution of 256\u00d7256\u00d73 unless otherwise specified.\\n\\nLSUN Church [69] consists of 126,227 images of the church, varies in the background, perspectives, etc. We employ the resolution of 256\u00d7256\u00d73 for our experiments.\\n\\n**B.2 Experimental Settings and Hyperparameters**\\n\\n**Kernel selection.** We consistently employ the RBF kernel $K(x_i, x_j) = \\\\exp(-\\\\|x_i - x_j\\\\|^2/\\\\sigma^2)$ for calculating the CKA. The bandwidth $\\\\sigma$ is set as a fraction of the median distance between examples. In practice, three commonly used kernels could be employed for calculation, namely linear, polynomial, and RBF kernels. In order to investigate their difference, three publicly available models with clear performance margins are collected for evaluation. Concretely, we gather models of InsGen [66] trained on FFHQ with different data regimes (i.e., 2K, 10K, 140K), the ranking of their synthesis quality is clear and reasonable. Tab. A1 demonstrates the quantitative results of CKA with different kernels. Obviously, these kernels give similar results and rankings. However, the RBF kernel contributes to the distinguishability of quantitative results, making the results more comparable. Consequently, the RBF kernel is employed in our experiments.\\n\\n| Kernel | InsGen-2k FID | InsGen-10k FID | InsGen-140k FID |\\n|--------|---------------|----------------|-----------------|\\n| Linear | 99.58         | 99.87          | 99.93           |\\n| Poly   | 99.83         | 99.93          | 99.98           |\\n| RBF    | 95.72         | 98.65          | 99.10           |\\n\\n\u2020 results are quoted from the original paper.\\n\\n**Table A1:** CKA results with different kernels. The publicly available models are gathered for comparison.\\n\\n**ViT features calculation.** The feature maps of ViT-based extractors are three-dimensional tensors (N, W, C), where W contains the global token and local features. The global token captures the same semantic information as the local features. Thus the global taken features are used for computation in implementation. Tab. A2 shows the comparison results of using local features and the global token. Consistently, they give similar results and rankings, so we use the global token for calculation in our experiments.\\n\\n**Feature normalization.** In practice, the activations of features play an essential role in computing the similarity index. Namely, the quantitative results would be dominated by a few activations with large\"}"}
{"id": "C0zw2ERKiQ", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table A2: CKA results with different features for calculation.\\n\\n| Metrics    | InsGen-2k | InsGen-10k | InsGen-140k |\\n|------------|-----------|------------|-------------|\\n| Local Features | 96.62     | 97.42      | 97.38       |\\n| Global Token    | 97.46     | 97.88      | 97.93       |\\n\\nTable A3: CKA scores with different normalization techniques.\\n\\n| Metrics    | InsGen-2k | InsGen-10k | InsGen-140k |\\n|------------|-----------|------------|-------------|\\n| CKA        | 97.46     | 97.88      | 97.93       |\\n| CKA L\u2081     | 96.62     | 98.91      | 99.33       |\\n| CKA L\u2082     | 96.62     | 98.91      | 99.32       |\\n| CKA Softmax| 95.72     | 98.65      | 99.10       |\\n\\npeaks, neglecting other correlation patterns [63]. To investigate the activations of our self-supervised extractors, we visualize the activations of different samples and their statistics.\\n\\nFig. A1 and Fig. A2 respectively illustrate the activation of different samples and their statistics. Obviously, there are several peaks in the activations. And these peaks may dominate the similarity index as they are substantially larger than other activations. To mitigate the peaks and create a more uniform distribution, we employ the softmax transformation [63] to normalize the features. Such operation smooths the activations while maintaining the original distributional information of features. Thus the similarity index remains consistent to deliver the distribution discrepancy. Besides the softmax transformation, we also compare the behavior of different normalization techniques (i.e., L\u2081 and L\u2082 normalization).\\n\\nTab. A3 demonstrate the quantitative results with different normalization techniques. They consistently provide similar results and rankings, and the softmax transformation ameliorates the peaks more significantly, providing more comparable results. Consequently, we adopt Softmax normalization in our experiments.\\n\\nHistogram matching. In order to investigate the robustness of the measurement system, we employ the histogram matching [35] to attack the system. To be specific, a subset with a considerable number (e.g., 50K) of images is chosen as the referenced distribution, and the corresponding class distribution is predicted by a given classifier (i.e., Inception-V3 [56]. With the guidance of the classifier, the generator is encouraged to produce a synthesis distribution that matches the predicted class distribution of real images. Recall that the generator used to produce these synthesized distributions stays unchanged, thus a robust measurement system should give consistent similarities between the randomly generated and the matched distribution.\\n\\nFig. A3 provides the class distribution of real and synthesized FFHQ images predicted by Inception-V3. Obviously, the class distribution of the matched distribution is well-aligned with the predicted real distribution.\\n\\nSample-efficiency. In order to investigate the impacts of the number of synthesized samples, we compute the distributional distances between the real distribution with synthesized distributions with various numbers of generated images. Concretely, FFHQ (with 70K images) and ImageNet (with 1.28 million images) are investigated for universal conclusions. For both datasets, we synthesis 500K images as candidate, and randomly choose 5K, 10K, 50K, 100K, 250K, and 500K images as the synthesized distribution for computing the metrics. The entire training data is utilized as the real distribution, and the publicly accessible models on FFHQ [3] and ImageNet [4] are employed.\\n\\nThe curve of FD and CKA under various data regimes on ImageNet dataset is shown in Fig. A4. Consistent with the aforementioned results in the main paper, CKA could measure the distributional distances precisely with only 5K samples, whereas FID fails to deliver the actual measurement until sufficient samples are used. That is, CKA could give reliable results even when limited data is given, suggesting impressive sample efficiency. Equipped with the bounded quantitative results and consistency under different data regimes, as well as the robustness to the histogram matching attack, CKA outperforms FID as a reliable distance for delivering the distributional discrepancy.\\n\\n3 https://github.com/NVlabs/stylegan3\\n4 https://github.com/autonomousvision/stylegan-xl\"}"}
{"id": "C0zw2ERKiQ", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure A1: Visualization of different samples' activations. The large peaks may dominate the similarity index as their numerical values substantially surpass smaller values.\\n\\nFigure A2: Statistics of different samples' activations. There are clear margins between different statistics (e.g., Max and Min) of each sample, suggesting that the activation distribution is very peaky.\\n\\nC User Preference Study\\n\\nHere we present more details about our human perceptual judgment. Recall that two strategies are designed for different investigations, namely benchmarking the synthesis quality of one specific generative model and comparing two paired generative models. Fig. A5 shows the user interface for benchmarking the synthesis quality of one specific generative model (i.e., BigGAN on ImageNet here). To be more specific, considerable randomly generated images are shown to the user, and the user is required to determine the fidelity of synthesized images. We then obtain the final scores by averaging the judgments of the participants (i.e., 100 individuals).\\n\\nFig. A6 and Fig. A7 show the human evaluation results on FFHQ and ImageNet dataset, respectively. The percentages denote how many samples of the selected images are considered photo-realistic. Together with the quantitative results in our main paper, we could tell that the proposed metric shows a better correlation with human visual comparison.\\n\\nRecall that in our main paper, we find that our evaluation system gives the opposite ranking to the existing metric (i.e., FID) in some circumstances. For instance, the synthesis quality of ICGAN is determined basically the same as that of the class-conditional ICGAN (C-ICGAN) under our evaluation, whereas the FID votes C-ICGAN for the much better one. We thus conduct the other user study...\"}"}
