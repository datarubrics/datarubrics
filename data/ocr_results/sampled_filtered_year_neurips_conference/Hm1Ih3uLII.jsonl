{"id": "Hm1Ih3uLII", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Salient object detection (SOD) aims to identify standout elements in a scene, with recent advancements primarily focused on integrating depth data (RGB-D) or temporal data from videos to enhance SOD in complex scenes. However, the unison of two types of crucial information remains largely underexplored due to data constraints. To bridge this gap, we in this work introduce the DViSal dataset, fueling further research in the emerging field of RGB-D video salient object detection (DVSOD). Our dataset features 237 diverse RGB-D videos alongside comprehensive annotations, including object and instance-level markings, as well as bounding boxes and scribbles. These resources enable a broad scope for potential research directions. We also conduct benchmarking experiments using various SOD models, affirming the efficacy of multimodal video input for salient object detection. Lastly, we highlight some intriguing findings and promising future research avenues. To foster growth in this field, our dataset and benchmark results are publicly accessible at: https://dvsod.github.io/.\\n\\n1 Introduction\\n\\nSalient object detection (SOD) is a fascinating field of study that aims at identifying and distinguishing the most eye-catching components in static images [59, 91] or dynamic videos [66, 89]. This intriguing concept hails from cognitive science research into human visual attention behavior - our incredible ability to swiftly direct attention to the most valuable portions of visual scenes [4]. As such, SOD assumes an indispensable role in a myriad of real-world applications [52, 53, 30, 50, 69, 96, 24, 93, 81], such as image retrieval, video editing/analysis, medical diagnosis, object detection, and target tracking.\\n\\nThis field has witnessed significant advances in recent years, particularly in RGB-image based SOD [65, 90, 82, 72]. However, it has been observed that performance often degrades when objects and their surroundings share similar appearances or when backgrounds are heavily cluttered. With the growing ubiquity of 3D imaging sensors in depth cameras, such as Kinect and Intel RealSense, as well as in mobile devices like iPhone 13 Pro, Huawei Mate 50, and Samsung Galaxy S21, the value of fully leveraging RGB-D information for SOD has gained considerable research interest [14, 58, 94, 95].\\n\\nAs visualized in Fig. 1, paired depth maps contribute valuable 3D geometric information that helps the discernment of object silhouettes against backgrounds. Consequently, augmenting RGB images with depth maps as additional input dramatically enhances the accuracy in localizing salient objects in complex scenes, i.e., $w_{RGB}$ vs. $w_{RGBD}$ in Fig. 1.\\n\\nOn another front, considering the dynamic nature of real-world scenes, video salient object detection (VSOD) has been a prominent focus of research [19, 56, 71]. In contrast to static images, dynamic video scenario presents considerable difficulties due to the diversity of motion patterns, occlusions, blurring, large object deformations, and the inherent complexity of human visual attention behavior.\"}"}
{"id": "Hm1Ih3uLII", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Visual illustration of the advantages of employing RGBD videos for salient object detection. The last three columns exhibit the segmentation results achieved using different input modalities. By leveraging temporal contexts within a video sequence, VSOD methods [19, 63] have demonstrated improved capture of moving foreground objects and modeling of visual attention shifts in dynamic environments.\\n\\nWhile there have been significant strides in both RGB-D SOD and VSOD individually, there remains a paucity of research into combining multimodal and temporal contexts, both of which are vital for accurately detecting the salient objects in a scene. This lack is primarily due to the absence of suitable benchmark datasets. As an alternative, Lu et al. [48] have made the initial attempt to amalgamate existing VSOD datasets with estimated depth maps. However, these estimated depth maps often fall short of accurately reflecting the real world, and are prone to errors introduced by depth estimation methods, compared to data captured by actual depth sensors.\\n\\nTo bridge the existing gap and prosper the progress in RGB-D video SOD (i.e., DVSOD), we introduce the first real-world RGB-D Video Salience detection dataset, abbreviated as DViSal, in this study. Specifically, the DViSal dataset comprises 237 RGB-D videos at a frame rate of 25 fps, including 175,442 RGB-D pairs in total and 7,117 annotated frames. Besides, the videos were carefully selected to encompass a broad diversity of real scenarios and motion patterns. Another distinguishing characteristic of the DViSal dataset is the variety of annotation formats provided. In addition to the conventional object-level annotations used in SOD tasks, the dataset offers more comprehensive instance-level annotations, as well as weak annotations consisting of bounding boxes and scribbles. These diverse annotation sets may serve as valuable resources for a wider range of potential research directions, such as instance-level DVSOD, weakly-supervised DVSOD, and more.\\n\\nWe further benchmark the DVSOD by implementing 11 cutting-edge SOD models on the newly-proposed DViSal dataset, including RGB-image-based SOD models, RGB-D SOD models, and VSOD models. Additionally, we have assembled a straightforward DVSOD model to dissect the specific contributions offered by depth maps and video contexts. The empirical results underscore the utility of both the multimodal RGB-D cues and temporal video contexts in accurately identifying salient objects. In particular, the DVSOD model is shown to significantly enhance the performance of the base RGB network [73], as evidenced by a noteworthy increase in the F-measure (i.e., 0.548 to 0.610).\\n\\nLastly, we contemplate and highlight promising avenues for future research. We observe that the field of DVSOD is far from fully explored, with substantial room for improvement and innovation. This research presents the community with an exciting opportunity to delve deeper into this burgeoning field.\\n\\n2 Related Works\\n\\nIn the era of deep learning, benchmark datasets have become the foundational infrastructure for advancing the state-of-the-art in computer vision research. Prominent publicly available benchmarks, such as DUTS [70], SOC [17], and PASCAL-S [42], have greatly contributed to the remarkable progress in RGB-image-based salient object detection (SOD) [73, 65, 72] over the past decade. Despite their significant contributions, these benchmarks primarily center their attention on RGB images, overlooking the valuable depth data that can enhance the discrimination of object silhouettes from backgrounds.\\n\\nWith the rising popularity of 3D imaging sensors, acquiring RGB-D data has become easier with modern equipments (e.g., Kinect, Samsung Galaxy S21 and iPhone 13 Pro). This development has\"}"}
{"id": "Hm1Ih3uLII", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Overview of existing RGB SOD, RGB-D SOD, VSOD datasets, and our proposed DViSal dataset. The table shows whether each dataset provides depth maps/video sequences (Depth/Video), object-level ground truths (Obj-GT), instance-level annotations (Ins-GT), and weak supervision signals (Weak-GT) such as bounding boxes or scribbles. The table also lists the number of videos (#Video), annotations (#GT), and instances (#Ins) each dataset includes.\\n\\n| Dataset   | Year | Depth/Video | Obj-GT | Ins-GT | Weak-GT | #Video | #GT | #Ins |\\n|-----------|------|-------------|--------|--------|---------|--------|-----|------|\\n| MSRA-B    | 2007 | \u2717           | \u2717      | \u2713      | \u2717       | -      | 5,000| -    |\\n| SED1&2    | 2007 | \u2717           | \u2717      | \u2713      | \u2717       | \u2717      | 200 | -    |\\n| ASD       | 2009 | \u2717           | \u2717      | \u2713      | \u2717       | \u2717      | 1,000| -    |\\n| SOD       | 2010 | \u2717           | \u2717      | \u2713      | \u2717       | \u2717      | 300 | -    |\\n| M10K      | 2011 | \u2717           | \u2717      | \u2713      | \u2717       | \u2717      | 10,000| -    |\\n| DUT-O     | 2013 | \u2717           | \u2717      | \u2713      | \u2717       | \u2713      | 5,168| -    |\\n| ECSSD     | 2013 | \u2717           | \u2717      | \u2713      | \u2717       | \u2717      | 1,000| -    |\\n| PASCAL-S  | 2014 | \u2717           | \u2717      | \u2713      | \u2717       | \u2717      | 850 | -    |\\n| HKU-IS    | 2015 | \u2717           | \u2717      | \u2713      | \u2717       | \u2717      | 4,447| -    |\\n| SOS       | 2015 | \u2717           | \u2717      | \u2713      | \u2717       | \u2717      | 6,900| -    |\\n| MSO       | 2015 | \u2717           | \u2717      | \u2713      | \u2717       | \u2717      | 1,224| -    |\\n| DUTS      | 2017 | \u2717           | \u2717      | \u2713      | \u2717       | \u2713      | 15,572| -    |\\n| SOC       | 2022 | \u2717           | \u2717      | \u2713      | \u2713       | \u2713      | 6,000| 5,776|\\n| STERE     | 2012 | \u2713           | \u2717      | \u2713      | \u2717       | \u2717      | 1,000| -    |\\n| NJU2K     | 2014 | \u2713           | \u2717      | \u2713      | \u2717       | \u2717      | 1,985| -    |\\n| NLPR      | 2014 | \u2713           | \u2717      | \u2713      | \u2717       | \u2717      | 1,000| -    |\\n| DES       | 2014 | \u2713           | \u2717      | \u2713      | \u2717       | \u2717      | 135 | -    |\\n| LFSD      | 2014 | \u2713           | \u2717      | \u2713      | \u2717       | \u2717      | 100 | -    |\\n| SSD       | 2017 | \u2713           | \u2717      | \u2713      | \u2717       | \u2717      | 80  | -    |\\n| DUT-D     | 2019 | \u2713           | \u2717      | \u2713      | \u2717       | \u2717      | 1,200| -    |\\n| SIP       | 2020 | \u2713           | \u2717      | \u2713      | \u2713       | \u2717      | 929 | 1,471|\\n| ReDWeb-S  | 2021 | \u2713           | \u2717      | \u2713      | \u2717       | \u2717      | 3,179| -    |\\n| COME      | 2021 | \u2713           | \u2717      | \u2713      | \u2713       | \u2713      | 15,625| 26,862|\\n| SegV2     | 2013 | \u2717           | \u2713      | \u2713      | \u2717       | \u2717      | 14  | 1,065|\\n| FBMS      | 2014 | \u2717           | \u2713      | \u2713      | \u2717       | \u2717      | 59  | 720  |\\n| MCL       | 2015 | \u2717           | \u2713      | \u2713      | \u2717       | \u2717      | 9   | 463  |\\n| ViSal     | 2015 | \u2717           | \u2713      | \u2713      | \u2717       | \u2717      | 17  | 193  |\\n| DAVIS     | 2016 | \u2717           | \u2713      | \u2713      | \u2717       | \u2717      | 50  | 3,455|\\n| UVSD      | 2017 | \u2717           | \u2713      | \u2713      | \u2717       | \u2717      | 18  | 3,262|\\n| VOS       | 2018 | \u2717           | \u2713      | \u2713      | \u2717       | \u2717      | 200 | 7,467|\\n| DA VSOD   | 2019 | \u2717           | \u2713      | \u2713      | \u2713       | \u2717      | 226 | 23,938| 39,498|\\n| Our DViSal|      | \u2713           | \u2713      | \u2713      | \u2713       | \u2713      | 237 | 7,117| 20,226|\\n\\nSpurred the creation of several RGB-D datasets aimed at addressing the segmentation of salient objects in complex scenarios. For instance, the STERE dataset [55], which serves as the first collection of stereoscopic photos in this domain, boasts of 797 samples. The NJU2K dataset [31], on the other hand, houses 1,985 paired images in its most recent version, sourced from various platforms such as the internet, 3D movies, and images captured by a Fuji W3 stereo camera. Large-scale benchmarks such as ReDWeb-S [44] and COME [79] have been curated to validate the scalability of diverse models, thereby fostering advancement in this domain. Many novel RGB-D models [28, 34, 23, 16, 85, 86, 45, 38, 39] have been developed, showing that depth data, which includes spatial structure and 3D layout cues within a scene, can assist segmentation in challenging scenarios. For example, [34] proposed a prototype sampling network to adaptively weight RGB and depth superpixels according to a reliance selection module. [45] explored visual transformer to capture global contexts, achieving promising results.\\n\\nIt is worth noting that the existing RGB-D SOD datasets rely on single static images. The lack of a mechanism to account for the temporal contexts might impede their effectiveness when dealing with video inputs that depict dynamic scenes, which are omnipresent in our daily lives [87, 83, 40, 26]. As depicted in Table 1, the current SOD datasets provide as input either single pairs of RGB and depth images (e.g., STERE [55] and NJU2K [31]), or RGB only video sequences (e.g., VOS [37] and DA VSOD [15]). There unfortunately lacks a suitable dataset to provide both types of critical information - spatial layout cues derived from depth data and temporal context derived from dynamic video sequences, in addition to the RGB images.\\n\\nIn this work, we focus on a systematic exploration of RGB-D video salient object detection to bridge this existing gap. We introduce a carefully curated benchmark dataset named DViSal.\"}"}
{"id": "Hm1Ih3uLII", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Examples of the DViSal dataset. We provide diverse annotations, including fully-supervised object-/instance-level markings, as well as weakly-supervised scribbles and bounding boxes.\\n\\n3 The DVSOD Dataset\\n\\nIn this section, we introduce the new RGB-D Video Saliency detection dataset, i.e., DViSal. We first describe the construction process of the DViSal dataset, and then analyze its statistical results.\\n\\n3.1 Dataset Construction\\n\\nDataset Collection. The main principle of data acquisition is to provide a comprehensive collection of calibrated RGB and depth video sequences, covering a broad spectrum of scenarios. Toward this objective, we initially gathered over 703 RGB-D videos, sourced from multiple repositories, including CDTB [49], People [68], PTB [67], Scene [33], DET [75], Tracklam [5], and Track3D [77]. The collected video samples exhibit diverse locations (e.g., parks, campuses, streets and indoor scenes), and are recorded under a wide range of challenging conditions (e.g., cluttered backgrounds, low-light conditions, and reflective environments). We are committed to ensuring the quality of our dataset. To this end, we meticulously remove unqualified videos or frames - ones that are blurry, misaligned, or feature ambiguous salient objects. After this selection process, we consolidate the DViSal dataset, which consists of 237 high-quality RGB-D videos, with 175,442 paired frames in total. Some visual examples from our dataset are illustrated in Fig. 2.\\n\\nDataset Annotation. Diverging from most established SOD datasets that primarily offer object-level saliency masks, our proposed dataset aims to deliver more comprehensive annotations. We employ the Labelme toolkit to annotate the collated RGB-D videos. The acquisition of the ground-truth saliency adheres to the annotation principle utilized in widely-recognized SOD datasets [62, 15, 41]. Initially, five annotators select candidate salient objects based on their initial instinct. Subsequently, a majority voting strategy is deployed to finalize the salient objects. Nonetheless, annotating a large-scale DViSal dataset presents additional challenges compared to labeling an RGB image dataset. First, our DViSal dataset includes many challenging scenes recorded under diverse conditions, e.g., multiple objects, transparent objects, similar foreground and background, and cluttered environment. These complexities make it more difficult to identify whole objects and distinguish their silhouettes. Second, for video sequences, it becomes necessary to account for the attention shift [15] of salient objects within a single video. For example, an object labeled as \u201cbackground\u201d in one frame may be categorized as \u201csalient\u201d in another if the camera view changes. To overcome these challenges, we put into action several proactive measures. We carry out a visualization assistance process that overlays depth heatmaps onto corresponding RGB images, facilitating easier verification of salient objects.\"}"}
{"id": "Hm1Ih3uLII", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"objects in complex scenarios by the annotators. Moreover, for videos showcasing saliency shifting, two additional inspectors are required to review the initial annotations on an item-by-item basis, identifying any mislabeled samples and sending them back to the annotators for corrections and re-verification. Through these efforts, we successfully obtain a large-scale DViSal dataset comprising 7,117 high-quality annotations. These include object-level saliency ground truths, instance-level IDs, and weak supervision signals, such as bounding boxes and scribbles.\\n\\nDataset Splits.\\n\\nThe entire dataset is partitioned into training, validation and test sets, which consist of 103, 26, and 108 videos, respectively, with 3,560, 200, and 3,357 annotated frames each. To be specific, the training set includes the entire CDTB [49] dataset, which contains 71 video sequences, and an additional 32 randomly selected videos from the PTB [67], Tracklam [5] and Track3D [77] datasets. The validation set is made up of 26 videos sourced from PTB, serving the purpose of model performance assessment during training. The remaining 108 videos, extracted from Tracklam, Track3D, DET [75], People [68], and Scene [33] constitute the test set, designed to evaluate the performance of the models. The videos derived from the DET, People, and Scene datasets are fully preserved within the test set, facilitating a comprehensive evaluation of various SOD models and corroborating their ability to generalize across different scenarios. As presented in Table 2, we report the benchmarking results for each subset and provide an overall measurement.\\n\\nFigure 3: Statistics of the proposed DViSal dataset in terms of (a) salient object size/ratio (b) salient object numbers (c) the number of annotated frames in each video (d) annotated salient object instances in each video.\\n\\n3.2 Dataset Statistics\\n\\nComparison with Existing Datasets.\\n\\nTable 1 presents a comparative overview of popular SOD datasets and our proposed DViSal dataset. As observed, our DViSal dataset offers several notable advantages over existing datasets. First, DViSal integrates more rich data types in both RGB and depth views and video sequences. This stands in contrast to current datasets such as COME [79] and DA VSOD [15], which either provide single pairs of RGB and depth images or solely RGB video sequences as input. Second, the DViSal furnishes a more comprehensive set of annotations. Compared to typical SOD datasets that often involve only one type of object-level annotation [31, 36, 60, 62, 74], the diverse annotation sets in our DViSal dataset enable the exploration of additional domains such as instance-level or weakly-supervised DVSOD. This expanded capacity for research could foster a wider spectrum of research opportunities and catalyze the development of more sophisticated algorithms. Additionally, our dataset covers a wide range of scenarios and contains many challenging cases, such as cluttered background in Fig. 1 and multi-object scenario in Fig. 2. The diversity will be a valuable asset in assessing the scalability of saliency models.\\n\\nStatistical Analysis.\\n\\nStatistical Analysis.\\n\\nFig. 3 provides the statistical results of our DViSal dataset from several dimensions. (1) Size of salient objects: The sizes of salient objects in the dataset significantly impact the performance of saliency detection, as smaller objects are generally more challenging to detect. In our analysis, we define the object size (OS) as the proportion of salient pixels to the total number of pixels in the annotated frames. The results in Fig. 3 (a) reveal that the DViSal dataset exhibits a relatively small proportion of images with large objects, and the size of salient objects varies widely.\"}"}
{"id": "Hm1Ih3uLII", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Within a broad range of 0.02% to 54.55%. Moreover, the mean OS score is 14.65%, which is notably appealing compared to the score of 23.16% in current popular RGB-D datasets as reported in [29].\\n\\nNumber of salient objects: Fig. 3 (b) analyzes the distribution of the number of salient objects within the images. It is observed that images with a single salient object constitute only 8% of our dataset, whereas these with multiple salient objects (i.e., the number of salient objects \u2265 3) constitute a significant 45.26%, surpassing the majority of existing RGB-D SOD and VSOD datasets as reported in [29, 15]. These findings highlight not only the diversity and challenge presented in our dataset but also underscore its potential for advancing saliency detection research.\\n\\nVideo-level statistics: we give video-level analysis on the distribution of labeled frames and the number of salient instances in the videos, with results depicted in Figs. 3 (c) and (d), respectively. These results further showcase the versatility of the proposed DViSal dataset.\\n\\n4 The DVSOD Benchmark\\n\\n4.1 DVSOD Baseline\\n\\nTo date, various network architectures [28, 83] have been developed for the tasks of RGB-D SOD and VSOD. In the former task, advanced feature fusion techniques are adopted to fuse features extracted from RGB-D images based on two-stream encoders. The latter task focuses more on exploiting temporal associations in video sequence. Based on existing techniques in RGB-D SOD [27] and VSOD [57], we assemble a straightforward DVSOD baseline, aiming at examining the advantages of utilizing depth and temporal information in improving detection accuracy & robustness.\\n\\nFigure 4: An overview of the DVSOD baseline. Fig. 4 shows an overview of the proposed DVSOD baseline, which consists of four steps: 1) feature extraction; 2) fusion of RGB-D multimodal information; 3) video temporal aggregation; and 4) prediction of salient objects. Specifically, the input is a RGB-D video clip that contains a Query pair of RGB and depth images at current frame \\\\( t \\\\), and \\\\( L \\\\) Memory pairs at past frames. We first feed these image pairs into two-stream encoders to extract RGB and depth features, respectively. Then, a multimodal fusion module, i.e., cross reference modules (CRM) [27], is adopted to unify the information from two modalities at each network layer. The features across multiple layers are then decoded by the segmentation decoder [73], resulting in a series of fused multimodal features. We represent them as \\\\( \\\\{ f_{d} \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times D} \\\\}_{d \\\\in U} \\\\), where \\\\( H \\\\times W \\\\) represents the spatial size, \\\\( D \\\\) is the channel dimension, and \\\\( d \\\\) represents the time subscript of a certain frame in the set of \\\\( U = \\\\{ t - L, \\\\ldots, t - 1, t \\\\} \\\\). To incorporate the temporal cues, we exploit a basic temporal fusion module, i.e., the space-time memory read block [57], to furnish the Query features by engaging the rich features of Memory frames. Finally, we apply a \\\\( 1 \\\\times 1 \\\\) convolutional layer on the memory-augmented feature to predict the saliency map, which is supervised by the ground-truth saliency map using the conventional binary cross entropy (BCE) loss. More details to DVSOD baseline are described in the supplementary materials.\\n\\nImplementation Details. The framework is implemented with PyTorch and trained using a NVIDIA RTX A6000 GPU. We utilize the CPD decoder [73] in conjunction with the ResNet-50 encoder [21] as the base network. For the depth stream, we generate 3-channel depth maps by repeating the 1-channel depth maps. Each image is uniformly resized to \\\\( 320 \\\\times 320 \\\\), and we perform random horizontal flipping and cropping to avoid potential over-fitting. During model training, the learning rate is set to 1e-4, and Adam optimizer is adopted with mini-batch of 2. In the inference stage, the proposed baseline predicts saliency maps in an end-to-end manner and no post-processing procedure (e.g., CRF [22]) is applied in this work.\"}"}
{"id": "Hm1Ih3uLII", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2 Evaluation Metrics\\n\\nWe adopt four widely-used metrics to evaluate the performance of saliency models, including mean absolute error (MAE or $M$) [3], F-measure ($F_\\\\beta$) [1], S-measure ($S_\\\\alpha$) [12] and E-measure ($E_\\\\xi$) [13]. The lower the MAE, the better. For other metrics, the higher score is better.\\n\\nConcretely, F-measure is an overall performance measurement and is computed by the weighted harmonic mean of the precision and recall:\\n\\n$$F_\\\\beta = \\\\frac{1 + \\\\beta^2}{\\\\beta^2 \\\\times \\\\text{Precision} + \\\\text{Recall}},$$\\n\\n(1)\\n\\nwhere $\\\\beta^2$ is set to 0.3 as suggested in [1]. $M$ represents the average absolute difference between the saliency map and ground truth. It is used to calculate how similar a normalized saliency maps $S \\\\in [0, 1]^{W \\\\times H}$ is compared to the ground truth $G \\\\in \\\\{0, 1\\\\}^{W \\\\times H}$:\\n\\n$$M = \\\\frac{1}{W \\\\times H} \\\\sum_{x=1}^{W} \\\\sum_{y=1}^{H} |S(x, y) - G(x, y)|,$$\\n\\n(2)\\n\\nwhere $W$ and $H$ denote the width and height of $S$, respectively. Structural measure (S-measure) evaluates the structural similarity between the predicted saliency maps and the binary ground truths. S-measure (denoted as $S_\\\\alpha$) contains two terms, $S_o$ and $S_r$, referring to object-aware and region-aware structural similarities, respectively:\\n\\n$$S_\\\\alpha = \\\\lambda \\\\times S_o + (1 - \\\\lambda) \\\\times S_r,$$\\n\\n(3)\\n\\nwhere $\\\\lambda$ is the balance parameter and is set to 0.5 as in [12]. Enhanced-alignment measure ($E_\\\\xi$) considers the global means of the image and local pixel matching simultaneously.\\n\\n$$E_\\\\xi = \\\\frac{1}{W \\\\times H} \\\\sum_{i=1}^{W} \\\\sum_{j=1}^{H} \\\\phi_{s}(i, j),$$\\n\\n(4)\\n\\nwhere $\\\\phi_{s}($\u00b7$)$ is the enhanced alignment matrix, which reflects the correlation between $S$ and $G$ after subtracting their global means, respectively.\\n\\n4.3 Benchmark Results\\n\\nIn this section, we benchmark the DVSOD task by conducting a range of experiments using the DViSal dataset with 11 popular state-of-the-art SOD methods. These methods include four RGB-image-based SOD models (PoolNet [43], BASNet [65], CPD [73], and F3Net [72]), five RGB-D SOD models (DMRA [62], CoNet [28], BBSNet [16], RD3D [7] and SPNet [95]), two VSOD models (PCSA [19] and UGPL [63]), and our DVSOD baseline. We obtained results from these methods on the new dataset by reproducing them with their publicly available codes and default setups.\\n\\nQuantitative Results. In DVSOD, one important expectation compared to the RGB-based model is whether properly utilizing RGB-D and temporal features improves the per-frame segmentation accuracy. To verify it, we first conduct three comparative experiments (last three columns of Table 2) to evaluate the influence of each data type. The basic model, CPD [73], which is trained with just RGB images, achieves an overall Mean Absolute Error (MAE) score of 13.2%. When we incorporate a depth component (CPD + Dep), the error significantly decreases by 1.1%. As we employ multimodal RGB-D data and temporal video data jointly (CPD + DepVid), the performance further improves to 11.3%. These results consistently highlight the advantages of using 3D layout data and temporal contexts to locate salient objects. Considering that DVSOD is relatively new and its development is still at an early stage, we additionally present the segmentation results of related SOD/RGB-D SOD/VSOD models in Table 2, to provide observational reference for our readers.\\n\\nQualitative Results. Fig. 5 visualizes the saliency predictions from different saliency models on the DViSal dataset. We find that the model trained with both RGB-D data and video data generates saliency maps that closely match the ground truths. For example, the DVSOD baseline effectively identifies entire objects even in challenging low-light situations as displayed in Fig. 5 where other methods struggle. We attribute this success to the combined benefits of multimodal and temporal contexts. In addition, our DViSal dataset's diverse scenarios in Figs. 1-2&5 demonstrate its versatility to provide a sufficiently realistic benchmark in this field.\"}"}
{"id": "Hm1Ih3uLII", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Benchmarking results of related SOD approaches tested on the newly-proposed DViSal dataset. We report the numerical evaluation for each test subset and provide an overall measurement. \u2191 & \u2193 denote larger and smaller is better, respectively. The notations \u2020 and \u2021 refer to RGB-D SOD and VSOD models, respectively. The best performing model is highlighted in bold.\\n\\n| Model   | PoolNet | BASNet | F3Net | DMRA | CoNet | BBSNet | RD3D | SPNet | PCSA | UGPL | CPD |\\n|---------|---------|--------|-------|------|-------|--------|------|-------|------|------|-----|\\n| DET E\u03be  | 0.762   | 0.761  | 0.766 | 0.768| 0.775 | 0.784  | 0.786| 0.786 | 0.771| 0.781| 0.761|\\n| DET S\u03b1  | 0.666   | 0.672  | 0.688 | 0.678| 0.696 | 0.692  | 0.695| 0.686 | 0.680| 0.688| 0.677|\\n| DET F\u03b2  | 520     | 528    | 543   | 532  | 549   | 563    | 562  | 565   | 535  | 552  | 532  |\\n| DET M   | 0.143   | 0.142  | 0.146 | 0.135| 0.139 | 0.134  | 0.127| 0.126 | 0.134| 0.136| 0.142|\\n| Scene E\u03be| 0.769   | 0.826  | 0.905 | 0.838| 0.830 | 0.906  | 0.852| 0.888 | 0.837| 0.868| 0.804|\\n| Scene S\u03b1| 0.646   | 0.682  | 0.813 | 0.738| 0.772 | 0.802  | 0.722| 0.728 | 0.738| 0.768| 0.719|\\n| Scene F\u03b2| 541     | 514    | 751   | 571  | 652   | 759    | 575  | 749   | 569  | 721  | 535  |\\n| Scene M  | 0.093   | 0.075  | 0.043 | 0.065| 0.062 | 0.043  | 0.065| 0.049 | 0.067| 0.050| 0.078|\\n| People E\u03be| 0.472   | 0.467  | 0.517 | 0.474| 0.488 | 0.528  | 0.515| 0.574 | 0.478| 0.581| 0.409|\\n| People S\u03b1| 0.492   | 0.476  | 0.510 | 0.529| 0.502 | 0.553  | 0.551| 0.532 | 0.541| 0.550| 0.463|\\n| People F\u03b2| 1.162   | 1.114  | 1.160 | 1.192| 1.138 | 1.178  | 1.198| 1.192 | 1.192| 2.24  | 1.06  |\\n| People M  | 0.153   | 0.146  | 0.119 | 0.139| 0.128 | 0.086  | 0.103| 0.092 | 0.128| 0.078 | 0.098 |\\n| Track3D E\u03be| 0.657   | 0.600  | 0.780 | 0.634| 0.658 | 0.839  | 0.655| 0.860 | 0.638| 0.838| 0.612|\\n| Track3D S\u03b1| 0.590   | 0.589  | 0.779 | 0.629| 0.655 | 0.724  | 0.633| 0.667 | 0.629| 0.693| 0.600|\\n| Track3D F\u03b2| 3.01    | 2.90   | 2.567 | 3.43 | 4.23  | 6.64   | 3.69 | 6.82  | 3.42 | 6.57  | 3.22  |\\n| Track3D M  | 0.127   | 0.145  | 0.087 | 0.118| 0.101 | 0.080  | 0.117| 0.083 | 0.116| 0.078 | 0.085 |\\n| Tracklam E\u03be| 0.815   | 0.798  | 0.913 | 0.828| 0.896 | 0.923  | 0.809| 0.889 | 0.833| 0.924| 0.787|\\n| Tracklam S\u03b1| 0.701   | 0.682  | 0.769 | 0.717| 0.761 | 0.793  | 0.697| 0.739 | 0.717| 0.785| 0.702|\\n| Tracklam F\u03b2| 6.22    | 5.95   | 2.799 | 3.43 | 4.23  | 6.64   | 3.69 | 6.82  | 3.42 | 6.57  | 3.22  |\\n| Tracklam M  | 0.122   | 0.133  | 0.076 | 0.114| 0.082 | 0.068  | 0.127| 0.084 | 0.112| 0.063 | 0.069 |\\n| PTB E\u03be    | 0.908   | 0.880  | 0.909 | 0.893| 0.913 | 0.924  | 0.925| 0.911 | 0.894| 0.919| 0.888|\\n| PTB S\u03b1    | 0.793   | 0.791  | 0.826 | 0.810| 0.824 | 0.834  | 0.811| 0.793 | 0.808| 0.832| 0.816|\\n| PTB F\u03b2    | 0.826   | 0.710  | 0.795 | 0.753| 0.808 | 0.821  | 0.843| 0.835 | 0.754| 0.837| 0.745|\\n| PTB M      | 0.062   | 0.067  | 0.057 | 0.060| 0.056 | 0.054  | 0.053| 0.060 | 0.060 | 0.052 | 0.058 |\\n\\nDiagnostic Analysis. Here we discuss the impact of memory size M and sample rate S for memory frame selection. As shown in Table 3, adding memory frames consistently improves F\u03b2 scores compared to the single-frame model (i.e., M = 0). When using more memory frames (i.e., M = 3), we see a clear performance increase (i.e., 59.9% \u2192 61.0%). Increasing the M further beyond 3 gives marginal returns in performance. Hence, we set M = 3 as a balance between accuracy and memory usage. We then tested different sample rates while fixing memory size M = 3. It is discovered in Table 4 that the best performance is achieved with a moderate sample rate S = 3. Thus, we set both M and S to 3, effectively using past video frames without retaining too much outdated information.\\n\\nTable 3: Ablation on the impact of memory size using F-measure and MAE.\\n\\n| M  | F\u03b2\u2191  | M\u2193  |\\n|----|------|-----|\\n| 0  | 0.599| 0.121|\\n| 1  | 0.603| 0.118|\\n| 2  | 0.607| 0.114|\\n| 3  | 0.610| 0.113|\\n| 4  | 0.608| 0.113|\\n\\nTable 4: Ablation on the impact of sample rate using F-measure and MAE.\\n\\n| S  | F\u03b2\u2191  | M\u2193  |\\n|----|------|-----|\\n| 1  | 0.605| 0.117|\\n| 2  | 0.608| 0.114|\\n| 3  | 0.610| 0.113|\\n| 4  | 0.605| 0.114|\\n\\n5 Discussion and Outlook\\n\\nIn this section, we discuss several potential research problems on the emerging DVSOD task, and provide some feasible solutions for reference. Hopefully this could encourage more inspirations and contributions to this community. They are summarized as follows:\"}"}
{"id": "Hm1Ih3uLII", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Visual results of different SOD models. The 'CPD + DepVid' stands for the DVSOD baseline in Sec. 4.1 that integrates both multimodal RGB-D cues and temporal contexts.\\n\\n(1) Accuracy: The research of DVSOD is still in its initial stage and DVSOD research has room for improvement, particularly regarding the accuracy of the DVSOD models. There\u2019s potential to enhance the accuracy of DVSOD models by incorporating ideas from the established fields of RGB, RGB-D, and Video SODs. For instance, we could integrate the multi-scale learning techniques [6, 43, 92, 88] into cross-modal and cross-frame fusion to improve the model\u2019s contextual representation. Moreover, we could introduce extra edge signals [65, 28] to aid the model in capturing object boundary details. In addition, it could be beneficial to explore more sophisticated fusion techniques [16, 95, 25] to promote effective interaction between multimodal and temporal information.\\n\\n(2) Efficiency: Although the engagement of RGB-D videos brings significant improvement, it introduces additional model parameters. More lightweight strategies can be explored to improve efficiency. For instance, we can develop lightweight operations such as depthwise separable convolution [10] or neural architecture search techniques [84], to advance feature extractors. On the other hand, we could explore knowledge distillation schemes [64, 54] to transfer depth structure knowledge to the RGB stream, thereby reducing the heavy overhead of the depth encoder. To evaluate the impact of lightweight strategy, we conduct an exploratory experiment by replacing the conventional convolution in our network encoder with the more efficient depth-wise separable convolution. Our experiments show a reduction in network parameters from 97.3M to 58.1M and memory usage from 17.15G to 16.21G, along with an increase in inference speed from 16.7 FPS to 21.5 FPS. However, this change leads to a drop in performance, with the F-measure falling from 0.610 to 0.575. These findings underscore the potential of efficiency-enhancing strategies, and also highlight the need to balance accuracy with efficiency in saliency model design.\\n\\n(3) Temporal Modeling: In this work, we conduct a preliminary investigation on the benefits of RGB-D videos for saliency detection by incorporating an well-designed temporal aggregation scheme [57] that focuses on modeling long-term dependencies. As a potential extension, we could consider introducing optical flow to capture short-term temporal cues. However, it\u2019s important to note that directly integrating optical flow could increase training complexity and potentially introduce noise, especially in complex scenes. Meanwhile, it is wroth exploring the saliency change problem (e.g. the salient object changing from a person to a table) when referring to temporal information of previous frames. In the DVSOD baseline, we attempt to address it through engaging the attention mechanism within [57], which selectively discounts less relevant information and fetches the most relevant features. We encourage further investigation on this matter, and believe there is a significant opportunity for the development of more effective temporal modeling schemes that can address these challenges.\\n\\n(4) Dataset: As the first dataset proposed for DVSOD, there is still considerable room for improving the various aspects of the dataset such as quality, quantity, and diversity by involving a broader range of scenarios of extreme lighting conditions, as well as various types of occlusions. Furthermore, annotating such large-scale RGB-D video dataset at pixel-level is very costly in terms of both time and effort. We may explore the recent advances of photorealistic rendering techniques [11, 20] to simulate data or supplement annotations to help address challenges within the dataset.\\n\\n(5) Evaluation Metrics: Evaluation metrics are crucial for model training, testing, and benchmarking. However, the saliency detection community largely relies on classic metrics such as MAE and F-measure, which are not designed to assess sequential DVSOD tasks specifically. Meanwhile, the Temporal Coherence (TC) metric, frequently used in evaluating RGB video, may not accurately\"}"}
{"id": "Hm1Ih3uLII", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"depict DVSOD model performance due to the complex scenes within the DViSal benchmark, such as low light or cluttered backgrounds. Thus, how to design appropriate metrics specifically tailored for DVSOD remains an open challenge.\\n\\n(6) **Instance-level Extension**: As revealed in [18], instance-level extension is capable of recognizing individual instances of salient objects. It is more challenging as it requires detailed parsing within the detected salient regions, and holds significant importance for practical applications. Hence exploring the research on RGB-D video salient instance detection emerges as a promising future direction.\\n\\n(7) **Weakly-supervised Learning**: The acquirement of precise per-pixel labels is laborious and time-consuming. As an alternative, training DVSOD models with weak supervisions ([80](#)), e.g., bounding box, scribble, is an appealing research topic, which can avoid heavy annotation costs.\\n\\n6 **Conclusion**\\n\\nIn this work, we present the DViSal dataset to spur research in RGB-D video salient object detection (DVSOD), a field yet to be well explored. This dataset, composed of 237 diverse RGB-D videos and comprehensive annotations, will significantly expand research avenues. Empirical benchmarking experiments demonstrate the effectiveness of multimodal video input in enhancing salient object detection. We highlight some promising future research directions and make the dataset and results publicly available to accelerate progress in this field.\\n\\n**Acknowledgements.**\\n\\nThis work was done jointly by UA and SRA. This research was supported by the CFI-JELF, Mitacs, University of Alberta Start-up grant, NSERC Discovery (RGPIN-2019-04575) grants.\\n\\n**References**\\n\\n[1] Radhakrishna Achanta, Sheila Hemami, Francisco Estrada, and Sabine Susstrunk. Frequency-tuned salient region detection. In *CVPR*, pages 1597\u20131604. IEEE, 2009.\\n\\n[2] Sharon Alpert, Meirav Galun, Ronen Basri, and Achi Brandt. Image segmentation by probabilistic bottom-up aggregation and cue integration. In *CVPR*, pages 1\u20138, 2007.\\n\\n[3] Ali Borji, Ming-Ming Cheng, Huaizu Jiang, and Jia Li. Salient object detection: A benchmark. *IEEE Transactions on Image Processing*, 24(12):5706\u20135722, 2015.\\n\\n[4] Ali Borji, Dicky N Sihite, and Laurent Itti. What stands out in a scene? a study of human explicit saliency judgment. *Vision Research*, 91:62\u201377, 2013.\\n\\n[5] Tim Caselitz, Michael Krawez, Jugesh Sundram, Mark Van Loock, and Wolfram Burgard. Camera tracking in lighting adaptable maps of indoor environments. In *ICRA*, pages 3334\u20133340, 2020.\\n\\n[6] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In *ECCV*, pages 801\u2013818, 2018.\\n\\n[7] Qian Chen, Ze Liu, Yi Zhang, Keren Fu, Qijun Zhao, and Hongwei Du. Rgb-d salient object detection via 3d convolutional neural networks. In *AAAI*, pages 1063\u20131071, 2021.\\n\\n[8] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS Torr, and Shi-Min Hu. Global contrast based salient region detection. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 37(3):569\u2013582, 2014.\\n\\n[9] Yupeng Cheng, Huazhu Fu, Xingxing Wei, Jiangjian Xiao, and Xiaochun Cao. Depth enhanced saliency detection method. In *Proceedings of International Conference on Internet Multimedia Computing and Service*, pages 23\u201327, 2014.\\n\\n[10] Fran\u00e7ois Chollet. Xception: Deep learning with depthwise separable convolutions. In *CVPR*, pages 1251\u20131258, 2017.\\n\\n[11] Maximilian Denninger, Dominik Winkelbauer, Martin Sundermeyer, Wout Boerdijk, Markus Wendelin Knauer, Klaus H Strobl, Matthias Humt, and Rudolph Triebel. Blenderproc2: A procedural pipeline for photorealistic rendering. *Journal of Open Source Software*, 8(82):4901, 2023.\\n\\n[12] Deng-Ping Fan, Ming-Ming Cheng, Yun Liu, Tao Li, and Ali Borji. Structure-measure: A new way to evaluate foreground maps. In *ICCV*, pages 4558\u20134567, 2017.\\n\\n[13] Deng-Ping Fan, Cheng Gong, Yang Cao, Bo Ren, Ming-Ming Cheng, and Ali Borji. Enhanced-alignment measure for binary foreground map evaluation. In *IJCAI*, pages 698\u2013704, 2018.\"}"}
{"id": "Hm1Ih3uLII", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deng-Ping Fan, Zheng Lin, Zhao Zhang, Menglong Zhu, and Ming-Ming Cheng. Rethinking RGB-D salient object detection: Models, data sets, and large-scale benchmarks. IEEE Transactions on Neural Networks and Learning Systems, 32(5):2075\u20132089, 2020.\\n\\nDeng-Ping Fan, Wenguan Wang, Ming-Ming Cheng, and Jianbing Shen. Shifting more attention to video salient object detection. In CVPR, pages 8554\u20138564, 2019.\\n\\nDeng-Ping Fan, Yingjie Zhai, Ali Borji, Jufeng Yang, and Ling Shao. BBS-Net: RGB-D salient object detection with a bifurcated backbone strategy network. In ECCV, pages 275\u2013292, 2020.\\n\\nDeng-Ping Fan, Jing Zhang, Gang Xu, Ming-Ming Cheng, and Ling Shao. Salient objects in clutter. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(2):2344\u20132366, 2022.\\n\\nRuochen Fan, Ming-Ming Cheng, Qibin Hou, Tai-Jiang Mu, Jingdong Wang, and Shi-Min Hu. S4Net: Single stage salient-instance segmentation. In CVPR, pages 6103\u20136112, 2019.\\n\\nYuchao Gu, Lijuan Wang, Ziqin Wang, Yun Liu, Ming-Ming Cheng, and Shao-Ping Lu. Pyramid constrained self-attention network for fast video salient object detection. In AAAI, pages 10869\u201310876, 2020.\\n\\nAnkur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, and Roberto Cipolla. Understanding real-world indoor scenes with synthetic data. In CVPR, pages 4077\u20134085, 2016.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778, 2016.\\n\\nQibin Hou, Ming-Ming Cheng, Xiaowei Hu, Ali Borji, Zhuowen Tu, and Philip HS Torr. Deeply supervised salient object detection with short connections. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(4):815\u2013828, 2019.\\n\\nWei Ji, Jingjing Li, Qi Bi, Chuan Guo, Jie Liu, and Li Cheng. Promoting saliency from depth: Deep unsupervised RGB-D saliency detection. ICLR, 2022.\\n\\nWei Ji, Jingjing Li, Qi Bi, Wenbo Li, and Li Cheng. Segment Anything is not always perfect: An investigation of SAM on different real-world applications. arXiv preprint arXiv:2304.05750, 2023.\\n\\nWei Ji, Jingjing Li, Cheng Bian, Zhicheng Zhang, and Li Cheng. SemanticRT: A large-scale dataset and method for robust semantic segmentation in multispectral images. In ACM MM, 2023.\\n\\nWei Ji, Jingjing Li, Shuang Yu, Miao Zhang, Yongri Piao, Shunyu Yao, Qi Bi, Kai Ma, Yefeng Zheng, Huchuan Lu, et al. Calibrated RGB-D salient object detection. In CVPR, pages 9471\u20139481, 2021.\\n\\nWei Ji, Jingjing Li, Miao Zhang, Yongri Piao, and Huchuan Lu. Accurate RGB-D salient object detection via collaborative learning. In ECCV, pages 52\u201369, 2020.\\n\\nWei Ji, Ge Yan, Jingjing Li, Yongri Piao, Shunyu Yao, Miao Zhang, Li Cheng, and Huchuan Lu. DMRa: Depth-induced multi-scale recurrent attention network for RGB-D saliency detection. IEEE Transactions on Image Processing, 31:2321\u20132336, 2022.\\n\\nWei Ji, Shuang Yu, Junde Wu, Kai Ma, Cheng Bian, Qi Bi, Jingjing Li, Hanruo Liu, Li Cheng, and Yefeng Zheng. Learning calibrated medical image segmentation via multi-rater agreement modeling. In CVPR, pages 12341\u201312351, 2021.\\n\\nRan Ju, Ling Ge, Wenjing Geng, Tongwei Ren, and Gangshan Wu. Depth saliency based on anisotropic center-surround difference. In ICIP, pages 1115\u20131119, 2014.\\n\\nHansang Kim, Youngbae Kim, Jae-Young Sim, and Chang-Su Kim. Spatiotemporal saliency detection for video sequences based on random walk with restart. IEEE Transactions on Image Processing, 24(8):2552\u20132564, 2015.\\n\\nKevin Lai, Liefeng Bo, and Dieter Fox. Unsupervised feature learning for 3D scene labeling. In ICRA, pages 3050\u20133057, 2014.\\n\\nMinhyeok Lee, Chaewon Park, Suhwan Cho, and Sangyoun Lee. SPSN: Superpixel prototype sampling network for RGB-D salient object detection. In ECCV, pages 630\u2013647, 2022.\"}"}
{"id": "Hm1Ih3uLII", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, and James M Rehg. Video segmentation by tracking many figure-ground segments. In ICCV, pages 2192\u20132199, 2013.\\n\\nGuanbin Li and Yizhou Yu. Visual saliency based on multiscale deep features. In CVPR, pages 5455\u20135463, 2015.\\n\\nJia Li, Changqun Xia, and Xiaowu Chen. A benchmark dataset and saliency-guided stacked autoencoders for video-based salient object detection. IEEE Transactions on Image Processing, 27(1):349\u2013364, 2017.\\n\\nJingjing Li, Wei Ji, Qi Bi, Cheng Yan, Miao Zhang, Yongri Piao, Huchuan Lu, et al. Joint semantic mining for weakly supervised rgb-d salient object detection. NeurIPS, 34:11945\u201311959, 2021.\\n\\nJingjing Li, Wei Ji, Miao Zhang, Yongri Piao, Huchuan Lu, and Li Cheng. Delving into calibrated depth for accurate rgb-d salient object detection. International Journal of Computer Vision, 131(4):855\u2013876, 2023.\\n\\nJingjing Li, Tianyu Yang, Wei Ji, Jue Wang, and Li Cheng. Exploring denoised cross-video contrast for weakly-supervised temporal action localization. In CVPR, pages 19914\u201319924, 2022.\\n\\nNianyi Li, Jinwei Ye, Yu Ji, Haibin Ling, and Jingyi Yu. Saliency detection on light field. In CVPR, pages 2806\u20132813, 2014.\\n\\nYin Li, Xiaodi Hou, Christof Koch, James M Rehg, and Alan L Yuille. The secrets of salient object segmentation. In CVPR, pages 280\u2013287, 2014.\\n\\nJiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng, and Jianmin Jiang. A simple pooling-based design for real-time salient object detection. In CVPR, pages 3917\u20133926, 2019.\\n\\nNian Liu, Ni Zhang, Ling Shao, and Junwei Han. Learning selective mutual attention and contrast for rgb-d saliency detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(12):9026\u20139042, 2021.\\n\\nNian Liu, Ni Zhang, Kaiyuan Wan, Ling Shao, and Junwei Han. Visual saliency transformer. In ICCV, pages 4722\u20134732, 2021.\\n\\nTie Liu, Jian Sun, Nan-Ning Zheng, Xiaoou Tang, and Heung-Yeung Shum. Learning to detect a salient object. In CVPR, pages 1\u20138, 2007.\\n\\nZhi Liu, Junhao Li, Linwei Ye, Guangling Sun, and Liquan Shen. Saliency detection for unconstrained videos using superpixel-level graph and spatiotemporal propagation. IEEE Transactions on Circuits and Systems for Video Technology, 27(12):2527\u20132542, 2017.\\n\\nYukang Lu, Dingyao Min, Keren Fu, and Qijun Zhao. Depth-cooperated trimodal network for video salient object detection. In ICIP, pages 116\u2013120, 2022.\\n\\nAlan Lukezic, Ugur Kart, Jani Kapyla, Ahmed Durmush, Joni-Kristian Kamarainen, Jiri Matas, and Matej Kristan. Cdtb: A color and depth visual object tracking dataset and benchmark. In ICCV, pages 10013\u201310022, 2019.\\n\\nStefan Mathe and Cristian Sminchisescu. Dynamic eye movement datasets and learnt saliency models for visual action recognition. In ECCV, pages 842\u2013856, 2012.\\n\\nVida Movahedi and James H Elder. Design and perceptual validation of performance measures for salient object segmentation. In CVPRW, pages 49\u201356, 2010.\\n\\nMunan Ning, Cheng Bian, Dong Wei, Shuang Yu, Chenglang Yuan, Yaohua Wang, Yang Guo, Kai Ma, and Yefeng Zheng. A new bidirectional unsupervised domain adaptation segmentation framework. In IPMI, pages 492\u2013503, 2021.\\n\\nMunan Ning, Donghuan Lu, Dong Wei, Cheng Bian, Chenglang Yuan, Shuang Yu, Kai Ma, and Yefeng Zheng. Multi-anchor active domain adaptation for semantic segmentation. In ICCV, pages 9112\u20139122, 2021.\\n\\nMunan Ning, Donghuan Lu, Yujia Xie, Dongdong Chen, Dong Wei, Yefeng Zheng, Yonghong Tian, Shuicheng Yan, and Li Yuan. Madav2: Advanced multi-anchor based active domain adaptation segmentation. arXiv preprint arXiv:2301.07354, 2023.\\n\\nYuzhen Niu, Yujie Geng, Xueqing Li, and Feng Liu. Leveraging stereopsis for saliency analysis. In CVPR, pages 454\u2013461, 2012.\"}"}
{"id": "Hm1Ih3uLII", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[57] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation using space-time memory networks. In ICCV, pages 9226\u20139235, 2019.\\n\\n[58] Youwei Pang, Lihe Zhang, Xiaoqi Zhao, and Huchuan Lu. Hierarchical dynamic filtering network for rgb-d salient object detection. In ECCV, pages 235\u2013252, 2020.\\n\\n[59] Youwei Pang, Xiaoqi Zhao, Lihe Zhang, and Huchuan Lu. Multi-scale interactive network for salient object detection. In CVPR, pages 9413\u20139422, 2020.\\n\\n[60] Houwen Peng, Bing Li, Weihua Xiong, Weiming Hu, and Rongrong Ji. RGBD salient object detection: a benchmark and algorithms. In ECCV, pages 92\u2013109, 2014.\\n\\n[61] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In CVPR, pages 724\u2013732, 2016.\\n\\n[62] Yongri Piao, Wei Ji, Jingjing Li, Miao Zhang, and Huchuan Lu. Depth-induced multi-scale recurrent attention network for saliency detection. In Proceedings of the IEEE International Conference on Computer Vision, pages 7254\u20137263, 2019.\\n\\n[63] Yongri Piao, Chenyang Lu, Miao Zhang, and Huchuan Lu. Semi-supervised video salient object detection based on uncertainty-guided pseudo labels. NeurIPS, 35:5614\u20135627, 2022.\\n\\n[64] Yongri Piao, Zhengkun Rong, Miao Zhang, Weisong Ren, and Huchuan Lu. A2dele: Adaptive and attentive depth distiller for efficient rgb-d salient object detection. In CVPR, pages 9060\u20139069, 2020.\\n\\n[65] Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao, Masood Dehghan, and Martin Jagersand. Basnet: Boundary-aware salient object detection. In CVPR, pages 7479\u20137489, 2019.\\n\\n[66] Vivek Kumar Singh, Parma Nand, Pankaj Sharma, and Preeti Kaushik. Video-based salient object detection: A survey. In International Conference on Computing, Communication, and Intelligent Systems, pages 617\u2013622, 2022.\\n\\n[67] Shuran Song and Jianxiong Xiao. Tracking revisited using rgbd camera: Unified benchmark and baselines. In ICCV, pages 233\u2013240, 2013.\\n\\n[68] Luciano Spinello and Kai O Arras. People detection in rgb-d data. In IROS, pages 3838\u20133843, 2011.\\n\\n[69] Haoxiang Wang, Zhihui Li, Yang Li, Brij B Gupta, and Chang Choi. Visual saliency guided complex image retrieval. Pattern Recognition Letters, 130:64\u201372, 2020.\\n\\n[70] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng, Dong Wang, Baocai Yin, and Xiang Ruan. Learning to detect salient objects with image-level supervision. In CVPR, pages 136\u2013145, 2017.\\n\\n[71] Wenguan Wang, Jianbing Shen, and Ling Shao. Consistent video saliency using local gradient flow optimization and global refinement. IEEE Transactions on Image Processing, 24(11):4185\u20134196, 2015.\\n\\n[72] Jun Wei, Shuhui Wang, and Qingming Huang. F3net: fusion, feedback and focus for salient object detection. In AAAI, pages 12321\u201312328, 2020.\\n\\n[73] Zhe Wu, Li Su, and Qingming Huang. Cascaded partial decoder for fast and accurate salient object detection. In CVPR, pages 3907\u20133916, 2019.\\n\\n[74] Qiong Yan, Li Xu, Jianping Shi, and Jiaya Jia. Hierarchical saliency detection. In CVPR, pages 1155\u20131162, 2013.\\n\\n[75] Song Yan, Jinyu Yang, Jani K\u00e4pyl\u00e4, Feng Zheng, Ale\u0161 Leonardis, and Joni-Kristian K\u00e4m\u00e4r\u00e4inen. Depth-track: Unveiling the power of rgbd tracking. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10725\u201310733, 2021.\\n\\n[76] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and Ming-Hsuan Yang. Saliency detection via graph-based manifold ranking. In CVPR, pages 3166\u20133173, 2013.\\n\\n[77] Jinyu Yang, Zhongqun Zhang, Zhe Li, Hyung Jin Chang, Ale\u0161 Leonardis, and Feng Zheng. Towards generic 3d tracking in rgbd videos: Benchmark and baseline. In ECCV, pages 112\u2013128, 2022.\"}"}
{"id": "Hm1Ih3uLII", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jianming Zhang, Shugao Ma, Mehrnoosh Sameki, Stan Sclaroff, Margrit Betke, Zhe Lin, Xiaohui Shen, Brian Price, and Radomir Mech. Salient object subitizing. In CVPR, pages 4045\u20134054, 2015.\\n\\nJing Zhang, Deng-Ping Fan, Yuchao Dai, Xin Yu, Yiran Zhong, Nick Barnes, and Ling Shao. Rgb-d saliency detection via cascaded mutual information minimization. In ICCV, pages 4338\u20134347, 2021.\\n\\nJing Zhang, Xin Yu, Aixuan Li, Peipei Song, Bowen Liu, and Yuchao Dai. Weakly-supervised salient object detection via scribble annotations. In CVPR, pages 12546\u201312555, 2020.\\n\\nMiao Zhang, Wei Ji, Yongri Piao, Jingjing Li, Yu Zhang, Shuang Xu, and Huchuan Lu. Lfnet: Light field fusion network for salient object detection. IEEE Transactions on Image Processing, 29:6276\u20136287, 2020.\\n\\nMiao Zhang, Jingjing Li, Ji Wei, Yongri Piao, and Huchuan Lu. Memory-oriented decoder for light field salient object detection. Advances in neural information processing systems, 32, 2019.\\n\\nMiao Zhang, Jie Liu, Yifei Wang, Yongri Piao, Shunyu Yao, Wei Ji, Jingjing Li, Huchuan Lu, and Zhongxuan Luo. Dynamic context-sensitive filtering network for video salient object detection. In ICCV, pages 1553\u20131563, 2021.\\n\\nMiao Zhang, Tingwei Liu, Yongri Piao, Shunyu Yao, and Huchuan Lu. Auto-msfnet: Search multi-scale fusion network for salient object detection. In ACM MM, pages 667\u2013676, 2021.\\n\\nMiao Zhang, Shunyu Yao, Beiqi Hu, Yongri Piao, and Wei Ji. $C^2$dfnet: Criss-cross dynamic filter network for rgb-d salient object detection. IEEE Transactions on Multimedia, 2022.\\n\\nWenbo Zhang, Yao Jiang, Keren Fu, and Qijun Zhao. Bts-net: Bi-directional transfer-and-selection network for rgb-d salient object detection. In ICME, pages 1\u20136, 2021.\\n\\nXiaoqi Zhao, Shijie Chang, Youwei Pang, Jiaxing Yang, Lihe Zhang, and Huchuan Lu. Adaptive multi-source predictor for zero-shot video object segmentation. arXiv preprint arXiv:2303.10383, 2023.\\n\\nXiaoqi Zhao, Hongpeng Jia, Youwei Pang, Long Lv, Feng Tian, Lihe Zhang, Weibing Sun, and Huchuan Lu. $M^2$sn: Multi-scale in multi-scale subtraction network for medical image segmentation. arXiv preprint arXiv:2303.10894, 2023.\\n\\nXiaoqi Zhao, Youwei Pang, Jiaxing Yang, Lihe Zhang, and Huchuan Lu. Multi-source fusion and automatic predictor selection for zero-shot video object segmentation. In ACM MM, pages 2645\u20132653, 2021.\\n\\nXiaoqi Zhao, Youwei Pang, Lihe Zhang, and Huchuan Lu. Joint learning of salient object detection, depth estimation and contour extraction. IEEE Transactions on Image Processing, 31:7350\u20137362, 2022.\\n\\nXiaoqi Zhao, Youwei Pang, Lihe Zhang, Huchuan Lu, and Lei Zhang. Suppress and balance: A simple gated network for salient object detection. In ECCV, pages 35\u201351, 2020.\\n\\nXiaoqi Zhao, Youwei Pang, Lihe Zhang, Huchuan Lu, and Lei Zhang. Towards diverse binary segmentation via a simple yet general gated network. arXiv preprint arXiv:2303.10396, 2023.\\n\\nXiaoqi Zhao, Lihe Zhang, and Huchuan Lu. Automatic polyp segmentation via multi-scale subtraction network. In MICCAI, pages 120\u2013130, 2021.\\n\\nXiaoqi Zhao, Lihe Zhang, Youwei Pang, Huchuan Lu, and Lei Zhang. A single stream network for robust and real-time rgb-d salient object detection. In ECCV, pages 646\u2013662, 2020.\\n\\nTao Zhou, Huazhu Fu, Geng Chen, Yi Zhou, Deng-Ping Fan, and Ling Shao. Specificity-preserving rgb-d saliency detection. In ICCV, pages 4681\u20134691, 2021.\\n\\nZikun Zhou, Wenjie Pei, Xin Li, Hongpeng Wang, Feng Zheng, and Zhenyu He. Saliency-associated object tracking. In ICCV, pages 9866\u20139875, 2021.\\n\\nChunbiao Zhu and Ge Li. A three-pathway psychobiological framework of salient object detection using stereoscopic technology. In ICCVW, pages 3008\u20133014, 2017.\"}"}
