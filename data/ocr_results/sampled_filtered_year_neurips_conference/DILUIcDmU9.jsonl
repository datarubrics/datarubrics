{"id": "DILUIcDmU9", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding\\n\\nHao Zheng\\nhzhe951@aucklanduni.ac.nz\\n\\nRegina Lee\\nklee702@aucklanduni.ac.nz\\n\\nYuqian Lu\\nyuqian.lu@auckland.ac.nz\\n\\nDepartment of Mechanical and Mechatronics Engineering\\nThe University of Auckland\\n\\nAbstract\\nUnderstanding comprehensive assembly knowledge from videos is critical for futuristic ultra-intelligent industry. To enable technological breakthrough, we present HA-ViD \u2013 an assembly video dataset that features representative industrial assembly scenarios, natural procedural knowledge acquisition process, and consistent human-robot shared annotations. Specifically, HA-ViD captures diverse collaboration patterns of real-world assembly, natural human behaviors and learning progression during assembly, and granulate action annotations to subject, action verb, manipulated object, target object, and tool. We provide 3222 multi-view and multi-modality videos, 1.5M frames, 96K temporal labels and 2M spatial labels. We benchmark four foundational video understanding tasks: action recognition, action segmentation, object detection and multi-object tracking. Importantly, we analyze their performance and the further reasoning steps for comprehending knowledge in assembly progress, process efficiency, task collaboration, skill parameters and human intention. Details of HA-ViD is available at: https://iai-hrc.github.io/ha-vid.\\n\\n1 Introduction\\nAssembly knowledge understanding from videos is crucial for futuristic ultra-intelligent industrial applications, such as robot skill learning [1], human-robot collaborative assembly [2] and quality assurance [3]. To enable assembly video understanding, a video dataset is required. Such a video dataset should (1) represent real-world assembly scenarios, (2) capture the comprehensive assembly knowledge, (3) follow a consistent annotation protocol that aligns both human and robot assembly comprehension. However, existing datasets fall short in meeting all these requirements.\\n\\nFirst, the assembled products in existing datasets are either highly application-specific [4\u20139] or lack representative assembly parts and tools [5\u20137, 9]. Second, many datasets did not design assembly tasks to foster the emergence of natural behaviors (e.g., varying efficiency, alternative routes, pauses and errors) during procedural knowledge acquisition. Third, thorough understanding of nuanced assembly knowledge is challenging with existing datasets as they often do not annotate subjects, objects, tools and their interactions in a systematic approach.\\n\\nTherefore, we introduce HA-ViD: a human assembly video dataset recording people assembling the Generic Assembly Box (GAB, see Figure 1). We benchmark on four foundational video understanding tasks:\\n\\n* These authors contributed equally to this work.\\n\u2020 Corresponding author.\"}"}
{"id": "DILUIcDmU9", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tasks: action recognition, action segmentation, object detection and multi-object tracking (MOT), and analyze their performance and the further reasoning steps for comprehending application-oriented knowledge. HA-ViD features three novel aspects:\\n\\n\u2022 Representative industrial assembly scenarios: GAB includes 35 standard and non-standard parts frequently used in real-world industrial assembly scenarios and requires 4 standard tools to assemble it. The assembly tasks are arranged onto 3 plates featuring different task precedence and collaboration requirements to promote the emergence of two-handed collaboration and parallel tasks. Compared to existing assembly video datasets, GAB is more representative of generic industrial assembly scenarios (see Table 1).\\n\\n\u2022 Natural procedural knowledge acquisition process: Progressive observation, thought and practice process (shown as varying efficiency, alternative assembly routes, pauses, and errors) in acquiring and applying complex procedural assembly knowledge is captured via the designed three-stage progressive assembly setup (see Figure 1). This design allows in-depth understanding of the human cognition process, where existing datasets are limited (see Table 1).\\n\\n\u2022 Consistent human-robot shared annotations: We designed a consistent fine-grained hierarchical task/action annotation protocol following a Human-Robot Shared Assembly Taxonomy (HR-SAT, to be introduced in Section 2.3). Using this protocol, we, (1) granulate action annotations to subject, action verb, manipulated object, target object, and tool; (2) provide two-handed collaboration status annotations; and (3) annotate human pauses and errors. Such detailed annotation embeds more knowledge sources for diverse understanding of application-oriented knowledge (see Table 1).\\n\\nFigure 1: HA-ViD, a dataset designed for industrial applications, represents real-world assembly scenarios, and captures the process of acquiring procedural knowledge. The consistent annotation follows a human-robot shared taxonomy. The dataset features 3222 multi-view and multi-modalities videos, 1.5M frames, 96K temporal labels and 2M spatial labels.\\n\\n2 Dataset\\nIn this section, we present the process of building HA-ViD and provide essential statistics.\\n\\n1 HR-SAT, developed by the same authors, is a hierarchical assembly task representation schema that both humans and robots can comprehend. See details via: https://iai-hrc.github.io/hr-sat\"}"}
{"id": "DILUIcDmU9", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset       | Assembled product | Natural procedural knowledge | Acquisition process | Consistent human-robot shared assembly taxonomy | Two-handed collaboration status | Varying assembly efficiency | Alternative route | Pause | Error | Subject | Action verb | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two-handed | Manipulated object | Target object | Tool | Two"}
{"id": "DILUIcDmU9", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To capture the progression of human procedural knowledge acquisition and behaviors (e.g., varying efficiency, alternative routes, pause, and errors) during learning, a three-stage progressive assembly setup is designed. Inspired by discovery learning, we design the three stages as follows:\\n\\n1. **Discovery** \u2013 participants are given minimal exploded view instructions of each plate.\\n2. **Instruction** \u2013 participants are given detailed step-by-step instructions of each plate.\\n3. **Practice** \u2013 participants are asked to complete the task without instruction.\\n\\nThe first stage encourages participants to explore assembly knowledge to reach a goal, the second stage provides targeted instruction to deepen participants' understanding, and the last stage encourages participants to reinforce their learning via practicing. During Instruction and Practice stages, the participants were asked to perform the assembly with the plate both facing upwards and sideways.\\n\\n### 2.1.2 Dataset Annotations\\n\\nWe provide temporal and spatial annotations to capture rich assembly knowledge shown in Figure 1. To enable human-robot assembly knowledge transfer, our annotations follow the HR-SAT (shown in Figure 3). According to the HR-SAT, an assembly task can be decomposed into primitive tasks and further into atomic actions. Each primitive task and atomic action contain five description elements: subject, action verb, manipulated object, target object, and tool.\\n\\n**Primitive tasks annotations** describe a functional change of the manipulated object, such as inserting a gear on a shaft or screwing a nut onto a bolt. **Atomic actions** describe an interaction change between the subject and manipulated object such as a hand grasping the screw or moving the screw. HR-SAT ensures the transferability, adaptability, and consistency of annotations. It can be used to transform annotations from other datasets into our designated description, or annotate new data to extend HA-ViD.\\n\\n![Figure 3: Human-robot shared assembly taxonomy (HR-SAT) schema.](https://iai-hrc.github.io/hr-sat)\\n\\nWe tailored the original taxonomy by removing information that cannot be annotated from videos and incorporating a Disassemble action verb to describe human error-and-correction process. We provide textual annotations (see Figure 2) following the typical input formats of current video understanding algorithms. We also offer SA-TPGs as knowledge graphs in RDF/XML format following the HR-SAT schema to enable advanced assembly knowledge reasoning with enhanced relationship information.\\n\\nWe annotate human pause and error as null and wrong respectively to enable research on understanding assembly efficiency and learning progression. Our annotations treat each hand as a separate subject. Primitive tasks and atomic actions are labeled for each hand to support multi-subject collaboration related research. Alongside the primitive task annotations, we annotate the two-handed collaboration status as: collaboration, when both hand work together on the same task; parallel, when each hand is working on a different task; single-handed, when only one hand is performing the task while the other hand pauses; and pause, when neither hand is performing any task.\\n\\nFor spatial annotations, we use CVAT, a video annotation tool, to manually label bounding boxes for subjects, objects, and tools frame-by-frame. Furthermore, we treat important assemblable features as...\"}"}
{"id": "DILUIcDmU9", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.2 Statistics\\n\\nIn total, we collected 3222 videos with side, front and top camera views. Each video contains one task \u2013 the process of assembling one plate. Our dataset contains 86.9 hours of footage, totaling over 1.5 million frames with an average of 1 min 37 sec per video (1456 frames). To ensure annotation quality, we manually labeled temporal annotations for 609 plate assembly videos and spatial annotations for over 144K frames. The selected videos for labeling collectively capture the dataset diversity by including videos of different participants, lightings, instructions and camera views.\\n\\nOverall, our dataset (in Fig. 5) contains 18831 primitive tasks across 75 classes, 63864 atomic actions across 219 classes, and close to 2M instances of subjects, objects and tools across 42 classes. Our dataset shows potential for facilitating small object detection research as 46.6% of the annotations are of small objects.\\n\\nOur temporal annotations can be used to understand the learning progression and efficiency of participants over the designed three-stage progressive assembly setup. The combined annotation of wrong primitive task, pause collaboration status and total frames can indicate features such as errors, observation patterns and task completion time for each participant. Our dataset captures the natural progress of procedural knowledge acquisition, as indicated by the overall reduction in task completion time and pause time from stage 1 to 3, as well as the significant reduction in errors (see Figure 6). The wrong and pause annotations enable research on understanding varying efficiency between participants.\\n\\nBy annotating the collaboration status and designing three assembly plates with different task precedence and collaboration requirements, HA-ViD captures the two-handed collaborative and parallel tasks commonly featured in real-world assembly, shown in Figure 7. Overall, 49.6% of the annotated frames consist of collaborative or parallel tasks. The high percentage of two-handed tasks enables research in understanding the collaboration patterns of complex assembly tasks.\"}"}
{"id": "DILUIcDmU9", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3 Benchmark Experiments\\n\\nWe benchmark SOTA methods for four foundational techniques for assembly knowledge understanding, i.e., action recognition, action segmentation, object detection, and MOT. Due to the page limit, we highlight key results and findings in this section, and present implementation details, more results and discussions in the Supplementary document.\\n\\n3.1 Action Recognition, Action Segmentation, Object Detection and MOT\\n\\nAction recognition is to classify a sequence of video frames into an action category. We split 123 out of 609 temporally labeled videos to be the test set, and the rest as train set. We benchmark seven action recognition methods from three categories: 2D models (TSM [12], TimeSFormer [13]), 3D models (I3D [14], MVITv2 [15], UniFormerV2 [16]), and skeleton-based methods (ST-GCN [17], ST-GCN++ [18]) and report the Top-1 accuracy and Top-5 accuracy in Table 2.\\n\\nAction segmentation is to temporally locate and recognize human action segments in untrimmed videos [19]. Under the same train/test split as action recognition, we benchmark four action segmentation methods, MS-TCN [20], DTGRM [19], BCN [21], and C2F-TCN [22], and report the frame-wise accuracy (Acc), segmental edit distance (Edit) and segmental F1 score at overlapping thresholds of 10% in Table 3.\\n\\nObject detection is to detect all instances of objects from known classes [23]. We split 18.4K out of 144K spatially labeled frames to be the test set, and the rest as train set. We benchmark classical two-stage method FasterRCNN [24], one-stage method Yolov5 [25], and the SOTA end-to-end Transformer-based method DINO [26] with different backbone networks, and report the parameter size (Params), average precision (AP), AP under different IoU thresholds (50% and 75%), and AP under different object scales (small, medium and large) in Table 4.\\n\\nMOT aims at locating multiple objects, maintaining their identities, and yielding their individual trajectories given an input video [27]. We benchmark SORT [28] and ByteTrack [29] on the detection results of DINO and ground truth annotations (test split of object detection), respectively. We report the average multi-object tracking accuracy (MOTA), ID F1 score (IDF1), false positive (FP), false negative (FN), and ID switch (IDS) over the videos in our testing dataset in Table 5.\"}"}
{"id": "DILUIcDmU9", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Baselines of action recognition. Average results over three views are reported here and more detailed results can be found in the Supplementary document.\\n\\n| Method          | View | Top-1 | Top-5 | Top-1 | Top-5 | Top-1 | Top-5 |\\n|-----------------|------|-------|-------|-------|-------|-------|-------|\\n| TSM [12]        |      | 61.0  | 88.5  | 58.6  | 87.9  | 39.6  | 69.4  |\\n| TimeSFormer [13]|      | 52.1  | 85.4  | 51.8  | 84.4  | 37.6  | 68.8  |\\n| I3D(rgb+flow) [14]|      | 47.7  | 71.5  | 52.9  | 85.1  | 43.0  | 75.0  |\\n| MVITv2 [15]     |      | 61.5  | 86.3  | 58.7  | 84.1  | 48.4  | 76.5  |\\n| UniFormerV2 [16]|      | 62.4  | 89.7  | 61.4  | 89.9  | 48.9  | 80.9  |\\n| ST-GCN [17]     |      | 39.5  | 60.2  | 38.7  | 55.2  | 20.3  | 44.4  |\\n| ST-GCN++ [18]   |      | 38.8  | 58.0  | 37.5  | 56.7  | 19.0  | 41.3  |\\n\\nTable 3: Baselines of action segmentation. Average results over three views are reported here and detailed results can be found in the Supplementary document.\\n\\n| Method  | View | F1    | Edit | Acc  |\\n|---------|------|-------|------|------|\\n| MS-TCN  |      | 36.6  | 37.5 | 40.2 |\\n| DTGRM   |      | 39.1  | 37.5 | 40.2 |\\n| BCN     |      | 43.7  | 41.4 | 44.1 |\\n| C2F-TCN |      | 22.6  | 22.0 | 39.5 |\\n\\nTable 4: Baselines of object detection.\\n\\n| Method         | Backbone | Params | AP   | AP50  | AP75  | AP-s | AP-m | AP-l |\\n|----------------|----------|--------|------|-------|-------|------|------|------|\\n| Faster-RCNN    | ResNet50 | 41.6M  | 21.7 | 32.6  | 24.4  | 13.0 | 37.4 | 40.6 |\\n|                 | ResNet101| 60.6M  | 20.9 | 31.1  | 23.9  | 12.3 | 37.9 | 43.1 |\\n|                 | ResNext101| 99.5M  | 22.2 | 31.6  | 25.7  | 15.0 | 36.2 | 46.2 |\\n| YOLOv5-s       | DarkNet  | 7.1M   | 10.2 | 14.1  | 10.9  | 0.7  | 18.8 | 46.8 |\\n| YOLOv5-l       | DarkNet  | 46.4M  | 12.9 | 17.3  | 14.0  | 1.0  | 28.8 | 59.8 |\\n| DINO           | Swin-L   | 218M   | 35.5 | 54.5  | 37.7  | 27.4 | 36.4 | 59.2 |\\n\\nTable 5: MOT results on object detection results and ground truth object bounding boxes.\\n\\n| Method       | bboxes | MOTA | IDF1 | FP   | FN   | IDS  |\\n|--------------|--------|------|------|------|------|------|\\n| SORT         | dets   | 20.4%| 27.1%| 737.8| 9212.3| 29   |\\n|              | gt     | 94.5%| 69.1%| 223.9| 408.1| 54.8 |\\n| ByteTrack    | dets   | 20.0%| 41.1%| 5175.3| 4678.3| 87.2 |\\n|              | gt     | 98.5%| 67.5%| 32.4  | 32.5  | 121.6|\\n\\nThe baseline results show that our dataset presents great challenges on the four foundational video understanding tasks compared with other datasets. For example, BCN has 70.4% accuracy on Breakfast [30], MViTv2 has 86.1% Top-1 accuracy on Kinetics-400 [31], DINO has 63.3% AP on COCO test-dev [32], and ByteTrack has 77.8% MOTA on MOT20 [33].\\n\\nCompared to the above baseline results, we are more concerned with whether existing video understanding methods can effectively comprehend the application-oriented knowledge (shown in Figure 1). Therefore, in Sections 3.2 to 3.5, we further analyze the performance and limitation of the foundational tasks on comprehending application-oriented knowledge, discuss the required assembly video reasoning tasks, and highlight the potential research directions.\\n\\n3.2 Assembly progress\\n\\nInsight #1: Assembly progress understanding could focus on compositional action understanding and leveraging prior domain knowledge. Basic assembly progress understanding requires real-time action (action verb + interacted objects and tools) recognition, and compare the action history with the predefined assembly plan (represented in a task graph). After further analysis of the sub-optimal action recognition performance in Table 2, we found recognizing the interacting objects and tools are more challenging than recognizing the action verbs, (as shown in Table 6).\"}"}
{"id": "DILUIcDmU9", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Recall of action verb, manipulated object, target object, and tool recognition, via MVITv2.\\n\\n| Action verb | Manipulated Object | Target Object | Tool |\\n|-------------|-------------------|---------------|------|\\n| Primitive Task | 71.1% | 60.4% | 57.1% | 60.8% |\\n| Atomic Action | 67.6% | 50.9% | 53.5% | 55.0% |\\n\\nDeeper assembly progress understanding requires real-time reasoning on human-object interaction dynamics, future operations, and their operation times. Taking a step to address this need, we benchmark FUTR [34] \u2013 a SOTA long-term action anticipation method, and report the mean over classes (MoC) accuracy as per [35] in Table 7. Similar to Table 6, a higher accuracy of action verb anticipation can also be observed in Table 7.\\n\\nTable 7: Mean over classes accuracy of action (action verb + interacted objects and tools) anticipation, action verb anticipation and manipulated object anticipation. Following the problem setting in [34]: for a video with $T$ frames, the first $\\\\alpha T$ frames are observed and anticipate the next $\\\\beta T$ frames.\\n\\n| $\\\\beta$ | $\\\\alpha$ = 0.2 | $\\\\alpha$ = 0.3 | $\\\\alpha$ = 0.5 |\\n|--------|----------------|----------------|----------------|\\n| 0.1    | 7.4 | 6.1 | 4.7 | 4.4 | 12.3 | 9.2 | 6.5 | 5.6 |\\n| 2.2    | 1.8 | 1.6 | 1.4 | 1.4 | 4.7 | 2.7 | 2.2 | 1.8 |\\n| 24.2   | 23.5 | 18.2 | 17.6 | 32.8 | 18.6 | 16.6 | 16.0 |\\n| 14.6   | 12.0 | 10.7 | 10.4 | 15.4 | 13.0 | 11.7 | 10.2 |\\n| 13.5   | 10.0 | 9.1 | 9.6 | 17.0 | 12.9 | 10.7 | 9.1 |\\n| 9.0    | 9.1 | 8.7 | 8.1 | 13.5 | 10.7 | 9.8 | 8.3 |\\n\\nBased on the observation from Table 6 and 7, a promising research direction could be recognizing and anticipating action verbs, objects, and tools compositionally and leveraging prior domain knowledge (such as task precedence and probabilistic correlation between action verbs, objects, and tools) to precisely track and predict the assembly progress. With defined task precedence graphs and rich list of annotated action verb/object/tool pairs, HA-ViD enables research on this aspect.\\n\\nInsight #2: Assembly action segmentation should focus on addressing under-segmentation issues and improving segment-wise sequence accuracy.\\n\\nAssembly progress tracking requires obtaining the accurate number of action segments and their sequence. For obtaining the accurate number of action segments from a given video, previous action segmentation algorithms [19\u201321] focused on addressing over-segmentation issues, but lack metrics for quantifying under/over-segmentation. Therefore, we propose segmentation adequacy (SA) to fill this gap. Considering the predicted segments as $s_{\\\\text{pred}} = \\\\{s'_{1}, s'_{2}, \\\\ldots, s'_{F}\\\\}$ and ground truth segments as $s_{\\\\text{gt}} = \\\\{s_{1}, s_{2}, \\\\ldots, s_{N}\\\\}$, where $F$ and $N$ are the number of segments for a given video, $SA = \\\\tanh\\\\left(\\\\frac{2(F - N)}{F + N}\\\\right)$.\\n\\nTable 8 reveals the significant under-segmentation issues observed from our dataset, which is a potentially important issue to be addressed for assembly action understanding. Our proposed SA metric can offer evaluation support, and even assist in designing the loss function given its use of the hyperbolic tangent function.\\n\\nTable 8: Comparison between our dataset and others on segmentation adequacy. We calculated the average number of ground truth segments ($N$), predicted segments ($F$), and segment adequacy ($SA$) of the videos in the testing datasets of ours and others. The predicted results are from BCN.\\n\\n| Dataset          | $N$  | $F$  | $SA$  |\\n|------------------|------|------|-------|\\n| HA-ViD (ours)    | 14.9 | 8.3  | -0.47 |\\n| Atomic action    | 51.2 | 11.5 | -0.82 |\\n| Breakfast        | 6    | 6.8  | -0.12 |\\n| GTEA             | 32.5 | 32.9 | -0.03 |\\n\\nFor segment-wise sequence accuracy, the low Edit value in Table 3 suggests further research efforts are required. Compared with Breakfast [30] (66.2% Edit score with BCN), our dataset presents greater challenges.\\n\\n3.3 Process Efficiency\\n\\nUnderstanding process efficiency is essential for real-world industry. It requires video understanding methods to be capable of identifying human pause and error via reasoning of the contextual scene and human-object interaction. HA-ViD supports this research by providing null and wrong labels.\"}"}
{"id": "DILUIcDmU9", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Insight #3: Null action understanding requires efforts on addressing imbalanced class distribution. Table 9 shows the recall and precision of action recognition and action segmentation for null actions. We suspect the high recall and low precision is caused by the imbalanced class distribution, as null is the largest head class (see Figure 5). To address the class imbalance problem, we randomly over-sample the minority classes (classes with sample size below a threshold) to reach the threshold, and report the action recognition accuracy of UniFormerV2 on the over-sampled dataset in Table 10. Here, the threshold is set to 300 and more details can be found in the Supplementary.\\n\\nTable 9: Recall and precision of null recognition and segmentation. Action recognition results are from MVITv2 and action segmentation results are from BCN.\\n\\n|                      | Recall | Precision |\\n|----------------------|--------|-----------|\\n| Recognition Primitive Task | 90.8%  | 65.1%     |\\n| Atomic Action        | 81.5%  | 39.1%     |\\n| Segmentation Primitive Task | 80.9%  | 45.1%     |\\n| Atomic Action        | 84.6%  | 37.5%     |\\n\\nTable 10: The action recognition accuracy of UniFormerV2 on the over-sampled dataset.\\n\\n| Method View | Primitive Task | Atomic Action |\\n|-------------|----------------|---------------|\\n|             | Top-1          | Top-5         | Top-1          | Top-5         | Top-1          | Top-5         |\\n| UniFormerV2 | 62.4%          | 89.7%         | 61.4%          | 89.9%         | 48.9%          | 80.9%         |\\n| UniFormerV2 (over-sampling) | 67.3% | 89.2% | 66.2% | 89.1% | 52.0% | 81.2% |\\n\\nInsight #4: New research from wrong action annotations. Wrong actions refer to assembly actions (primitive task level) occurred at the wrong position or order. Our annotated wrong actions can initiate three avenues for research. First, the pattern of wrong actions in different participants across the three-stage progressive assembly can provide insights into human learning progression and performance. Second, investigating the wrong action and the actions leading up to it can help identify assembly errors, contributing to quality assurance. Third, analyzing the actions undertaken after the wrong action can provide insights into how humans resolve errors and re-plan the assembly sequence.\\n\\n3.4 Task Collaboration\\n\\nUnderstanding the states, patterns, and dynamics of two-handed collaboration during the assembly process is essential for applications, such as ergonomics analysis, collaborative task planning, and human-robot collaboration design. HA-ViD can support research in this aspect via providing spatial annotations, two-hand separated temporal annotations and collaboration status annotations.\\n\\nInsight #5: New research on understanding parallel tasks from both hands. Table 11 shows that both action recognition and segmentation have lowest performance on parallel tasks during assembly. One possible reason is that the foundational video understanding methods rely on global features of each image, and do not explicitly detect and track the action of each hand. This calls for new methods to independently track both hands and recognize their actions through local features. Recent research on human-object interaction detection in videos [36, 37] could offer valuable insights.\\n\\nTable 11: Recall of two-handed primitive task recognition and segmentation in four collaboration status. Action recognition results are from MVITv2 and action segmentation results are from BCN.\\n\\n| Collaboration Status | Parallel | Single-handed | Pause |\\n|----------------------|----------|---------------|-------|\\n| Left hand            | 52.5%    | 54.2%         | 18.5% |\\n| Right hand           | 46.1%    | 50.7%         | 17.2% |\\n\\n3.5 Skill Parameters and Human Intention\\n\\nUnderstanding skill parameters and human intentions from videos is essential for robot skill learning and human-robot collaboration (HRC) [38, 39]. Typically, skill parameters vary depending on the specific application. However, there are certain skill parameters that are commonly used, including trajectory, object pose, force, and torque [40, 41].\"}"}
{"id": "DILUIcDmU9", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"While videos cannot capture force and torque directly, our dataset offers spatial annotations that enable tracking the trajectory of each object. Additionally, the object pose can be inferred from our dataset via pose estimation methods. Therefore, HA-ViD can support basic research in this direction.\\n\\nTo acquire more detailed and accurate skill parameters, assembly process comprehension must extend to the 3D space, utilizing 3D reasoning and 3D hand-object interaction estimation techniques to precisely track the temporal changes in hand poses and objects. For human-robot skill transfer, the learned assembly skill parameters can be sourced as input to robot learning environments.\\n\\nUnderstanding human intention in HRC refers to a combination of trajectory prediction, action anticipation and task goal understanding [42]. Our spatial annotations provide trajectory information, SA-TPGs present action sequence constraints, and GAB CAD files offer the final task goals. Therefore, HA-ViD can enhance the research in this aspect.\\n\\n4 Discussion\\n\\nAs identified in Section 3, HA-ViD provides a basis for developing video understanding and reasoning techniques to derive application-oriented knowledge. Model design, results analysis, and knowledge reasoning pipeline development that are based on HA-ViD can accelerate the development of application-specific models. In addition, if a new dataset is required for a specific application, our HR-SAT-based annotation protocol can be employed to ensure the resulting target dataset is compatible with HA-ViD. The annotation alignment eases the process of adapting and deploying pre-trained models to new scenarios.\\n\\nWe acknowledge the limitation of HA-ViD on fully capturing the complexities and diversities of industrial assembly scenarios. Therefore, HA-ViD can be extended to include more products, assembly environments, and even different agents. Following our data collection and annotation protocol could ensure similar high-quality assembly video datasets that are compatible with HA-ViD. Furthermore, we identify that HA-ViD could benefit from 3D and pixel-wise geometric annotations. This can facilitate the research into 3D hand-object interaction and 3D scene understanding, which is essential for assembly quality checking and robot skill learning. Therefore, future work could involve providing more refined annotations, such as hand poses, object poses and 3D key points, via additional sensors. We therefore created a dataset roadmap on our Github repository to outline improvement focuses and encourage collective efforts from the community to extend HA-ViD.\\n\\n5 Conclusion\\n\\nWe present HA-ViD, a human assembly video dataset, to advance comprehensive assembly knowledge understanding toward real-world industrial applications. We designed the Generic Assembly Box to represent industrial assembly scenarios and a three-stage progressive learning setup to capture the natural process of human procedural knowledge acquisition. The dataset annotation follows the Human-Robot Shared Assembly Taxonomy. HA-ViD includes (1) multi-view, multi-modality data, fine-grained action annotations (subject, action verb, manipulated object, target object, and tool), (2) human pause and error annotations, and (3) collaboration status annotations to enable technological breakthroughs industrial application-oriented knowledge comprehension from videos.\\n\\nWe benchmarked strong baseline methods of action recognition, action segmentation, object detection and multi-object tracking, and analyzed their performance and the further reasoning steps for comprehending application-oriented knowledge in assembly progress, process efficiency, task collaboration, skill parameter and human intention. The results show that our dataset captures essential challenges for foundational video understanding tasks, and new methods need to be explored for application-oriented knowledge comprehension. We envision HA-ViD will open opportunities for advancing video understanding techniques to enable the futuristic ultra-intelligent industry.\\n\\n6 Acknowledgements\\n\\nThis work was supported by The University of Auckland FRDF New Staff Research Fund (No. 3720540). Our gratitude extends to the participants and annotators of our dataset for their essential contributions. The Industrial AI Research group has provided invaluable feedback and advice, with\"}"}
{"id": "DILUIcDmU9", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"particular thanks to Benedict Liang for benchmarking assistance, and Saahil Chand for his annotations and insights.\\n\\nReferences\\n\\n[1] D. A. Duque, F. A. Prieto, and J. G. Hoyos, \u201cTrajectory generation for robotic assembly operations using learning by demonstration,\u201d Robotics and Computer Integrated Manufacturing, vol. 57, no. December 2018, pp. 292\u2013302, 2019.\\n\\n[2] E. Lamon, A. De Franco, L. Peternel, and A. Ajoudani, \u201cA Capability-Aware Role Allocation Approach to Industrial Assembly Tasks,\u201d IEEE Robotics and Automation Letters, vol. 4, no. 4, pp. 3378\u20133385, 2019.\\n\\n[3] F. Frustaci, S. Perri, G. Cocorullo, and P. Corsonello, \u201cAn embedded machine vision system for an in-line quality check of assembly processes,\u201d Procedia Manufacturing, vol. 42, pp. 211\u2013218, 2020.\\n\\n[4] G. Cicirelli, R. Marani, L. Romeo, M. G. Dom\u00ednguez, J. Heras, A. G. Perri, and T. D'Orazio, \u201cThe HA4M dataset: Multi-Modal Monitoring of an assembly task for Human Action recognition in Manufacturing,\u201d Scientific Data, vol. 9, p. 745, Dec 2022.\\n\\n[5] Y. Ben-Shabat, X. Yu, F. Saleh, D. Campbell, C. Rodriguez-Opazo, H. Li, and S. Gould, \u201cThe IKEA ASM Dataset: Understanding people assembling furniture through actions, objects and pose,\u201d Proceedings - 2021 IEEE Winter Conference on Applications of Computer Vision, WACV 2021, pp. 846\u2013858, 2021.\\n\\n[6] F. Sener, R. Wang, and A. Yao, \u201cAssembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities,\u201d Cvpr, 2022.\\n\\n[7] S. Toyer, A. Cherian, T. Han, and S. Gould, \u201cHuman Pose Forecasting via Deep Markov Models,\u201d DICTA 2017 - 2017 International Conference on Digital Image Computing: Techniques and Applications, vol. 2017-Decem, pp. 1\u20138, 2017.\\n\\n[8] J. Zhang, P. Byvshev, and Y. Xiao, \u201cA video dataset of a wooden box assembly process: Dataset,\u201d DATA 2020 - Proceedings of the 3rd Workshop on Data Acquisition To Analysis, Part of SenSys 2020, BuildSys 2020, pp. 35\u201339, 2020.\\n\\n[9] F. Ragusa, A. Furnari, S. Livatino, and G. M. Farinella, \u201cThe MECCANO Dataset: Understanding Human-Object Interactions from Egocentric Videos in an Industrial-like Domain,\u201d in 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1568\u20131577, IEEE, Jan 2021.\\n\\n[10] M. Georgeff and A. Lansky, \u201cProcedural knowledge,\u201d Proceedings of the IEEE, vol. 74, no. 10, pp. 1383\u20131398, 1986.\\n\\n[11] R. E. Mayer, \u201cShould There Be a Three-Strikes Rule Against Pure Discovery Learning?,\u201d American Psychologist, vol. 59, no. 1, pp. 14\u201319, 2004.\\n\\n[12] J. Lin, C. Gan, and S. Han, \u201cTSM: Temporal Shift Module for Efficient Video Understanding,\u201d in 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 7082\u20137092, IEEE, Oct 2019.\\n\\n[13] G. Bertasius, H. Wang, and L. Torresani, \u201cIs Space-Time Attention All You Need for Video Understanding?,\u201d in Proceedings of the 38th International Conference on Machine Learning, pp. 813\u2013824, Feb 2021.\\n\\n[14] J. Carreira and A. Zisserman, \u201cQuo Vadis, Action Recognition? A New Model and the Kinetics Dataset,\u201d in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4724\u20134733, IEEE, Jul 2017.\\n\\n[15] Y. Li, C.-Y. Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik, and C. Feichtenhofer, \u201cMViTv2: Improved Multiscale Vision Transformers for Classification and Detection,\u201d in 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4794\u20134804, IEEE, Jun 2022.\"}"}
{"id": "DILUIcDmU9", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"K. Li, Y. Wang, Y. He, Y. Li, Y. Wang, L. Wang, and Y. Qiao, \u201cUniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer,\u201d nov 2022.\\n\\nS. Yan, Y. Xiong, and D. Lin, \u201cSpatial temporal graph convolutional networks for skeleton-based action recognition,\u201d in 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, pp. 7444\u20137452, jan 2018.\\n\\nH. Duan, J. Wang, K. Chen, and D. Lin, \u201cPYSKL: Towards Good Practices for Skeleton Action Recognition,\u201d may 2022.\\n\\nD. Wang, D. Hu, X. Li, and D. Dou, \u201cTemporal Relational Modeling with Self-Supervision for Action Segmentation,\u201d Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, pp. 2729\u20132737, dec 2021.\\n\\nY. A. Farha and J. Gall, \u201cMS-TCN: Multi-Stage Temporal Convolutional Network for Action Segmentation,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), vol. 2019-June, pp. 3570\u20133579, IEEE, jun 2019.\\n\\nZ. Wang, Z. Gao, L. Wang, Z. Li, and G. Wu, \u201cBoundary-Aware Cascade Networks for Temporal Action Segmentation,\u201d in ECCV, vol. Part XXV 1, pp. 34\u201351, 2020.\\n\\nD. Singhania, R. Rahaman, and A. Yao, \u201cC2F-TCN: A Framework for Semi- and Fully-Supervised Temporal Action Segmentation,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1\u201318, 2023.\\n\\nY. Amit and P. Felzenszwalb, \u201cObject Detection,\u201d in Computer Vision, pp. 537\u2013542, Boston, MA: Springer US, 2014.\\n\\nS. Ren, K. He, R. Girshick, and J. Sun, \u201cFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, pp. 1137\u20131149, jun 2017.\\n\\nH. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M. Ni, and H.-Y. Shum, \u201cDINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection,\u201d mar 2022.\\n\\nW. Luo, J. Xing, A. Milan, X. Zhang, W. Liu, and T. K. Kim, \u201cMultiple object tracking: A literature review,\u201d Artificial Intelligence, vol. 293, p. 103448, apr 2021.\\n\\nA. Bewley, Z. Ge, L. Ott, F. Ramos, and B. Upcroft, \u201cSimple online and realtime tracking,\u201d in 2016 IEEE International Conference on Image Processing (ICIP), pp. 3464\u20133468, IEEE, sep 2016.\\n\\nY. Zhang, P. Sun, Y. Jiang, D. Yu, F. Weng, Z. Yuan, P. Luo, W. Liu, and X. Wang, \u201cByteTrack: Multi-Object Tracking by Associating Every Detection Box,\u201d in Proceedings of the European Conference on Computer Vision (ECCV), vol. 2, oct 2022.\\n\\nH. Kuehne, A. Arslan, and T. Serre, \u201cThe Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities,\u201d in 2014 IEEE Conference on Computer Vision and Pattern Recognition, pp. 780\u2013787, IEEE, jun 2014.\\n\\nW. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev, M. Suleyman, and A. Zisserman, \u201cThe Kinetics Human Action Video Dataset,\u201d may 2017.\\n\\nT.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, and P. Doll\u00e1r, \u201cMicrosoft COCO: Common Objects in Context,\u201d may 2014.\\n\\nP. Dendorfer, H. Rezatofighi, A. Milan, J. Shi, D. Cremers, I. Reid, S. Roth, K. Schindler, and L. Leal-Taix\u00e9, \u201cMOT20: A benchmark for multi object tracking in crowded scenes,\u201d mar 2020.\"}"}
{"id": "DILUIcDmU9", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[34] D. Gong, J. Lee, M. Kim, S. J. Ha, and M. Cho, \u201cFuture Transformer for Long-term Action Anticipation,\u201d in 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3042\u20133051, IEEE, Jun 2022.\\n\\n[35] Y. A. Farha, A. Richard, and J. Gall, \u201cWhen will you do what? - Anticipating Temporal Occurrences of Activities,\u201d in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5343\u20135352, IEEE, Jun 2018.\\n\\n[36] D. Tu, W. Sun, X. Min, G. Zhai, and W. Shen, \u201cVideo-based Human-Object Interaction Detection from Tubelet Tokens,\u201d in Advances in Neural Information Processing Systems 35, pp. 23345\u201423357, 2022.\\n\\n[37] M.-J. Chiou, C.-Y. Liao, L.-W. Wang, R. Zimmermann, and J. Feng, \u201cST-HOI: A Spatial-Temporal Baseline for Human-Object Interaction Detection in Videos,\u201d in Proceedings of the 2021 Workshop on Intelligent Cross-Data Analysis and Retrieval, (New York, NY, USA), pp. 9\u201317, ACM, Aug 2021.\\n\\n[38] O. Mees, M. Merklinger, G. Kalweit, and W. Burgard, \u201cAdversarial Skill Networks: Unsupervised Robot Skill Learning from Video,\u201d in 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 4188\u20134194, IEEE, May 2020.\\n\\n[39] P. Zheng, S. Li, L. Xia, L. Wang, and A. Nassehi, \u201cA visual reasoning-based approach for mutual-cognitive human-robot collaboration,\u201d CIRP Annals, vol. 71, no. 1, pp. 377\u2013380, 2022.\\n\\n[40] J. Jeon, H.-r. Jung, F. Yumbla, T. A. Luong, and H. Moon, \u201cPrimitive Action Based Combined Task and Motion Planning for the Service Robot,\u201d Frontiers in Robotics and AI, vol. 9, Feb 2022.\\n\\n[41] E. Berger, S. Grehl, D. Voigt, B. Jung, and H. B. Amor, \u201cExperience-based torque estimation for an industrial robot,\u201d in 2016 IEEE International Conference on Robotics and Automation (ICRA), pp. 144\u2013149, IEEE, May 2016.\\n\\n[42] Y. Lu, H. Zheng, S. Chand, W. Xia, Z. Liu, X. Xu, L. Wang, Z. Qin, and J. Bao, \u201cOutlook on human-centric manufacturing towards Industry 5.0,\u201d Journal of Manufacturing Systems, vol. 62, pp. 612\u2013627, Jan 2022.\"}"}
