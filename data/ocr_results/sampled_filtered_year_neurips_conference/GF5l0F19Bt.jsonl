{"id": "GF5l0F19Bt", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An NLP Benchmark Dataset for Assessing Corporate Climate Policy Engagement\\n\\nGaku Morio\u2217 and Christopher D. Manning\\nStanford University\\n{gaku,manning}@stanford.edu\\n\\nAbstract\\nAs societal awareness of climate change grows, corporate climate policy engagements are attracting attention. We propose a dataset to estimate corporate climate policy engagement from various PDF-formatted documents. Our dataset comes from LobbyMap (a platform operated by global think tank InfluenceMap) that provides engagement categories and stances on the documents. To convert the LobbyMap data into the structured dataset, we developed a pipeline using text extraction and OCR. Our contributions are: (i) Building an NLP dataset including 10K documents on corporate climate policy engagement. (ii) Analyzing the properties and challenges of the dataset. (iii) Providing experiments for the dataset using pre-trained language models. The results show that while Longformer outperforms baselines and other pre-trained models, there is still room for significant improvement. We hope our work begins to bridge research on NLP and climate change.\\n\\n1 Introduction\\nClimate change is one of the most critical challenges confronting our society today [24]. As societal awareness of climate change heightens, how corporations minimize their environmental impact has come under tight scrutiny by the public [12]. Consumers are increasingly warming up to \u201ceco-friendly\u201d products [3] and investors are placing a premium on investments yielding environmental benefits, as evidenced by the popularity of Environment, Social, and Governance (ESG) funds [1].\\n\\nNonetheless, skepticism about whether companies that claim environmental benefits are truly effective in mitigating the impact of climate change persists. Controversies have been sparked by dubious practices such as using vague terms to guide consumers to certain irrelevant conclusions (e.g., using \u2018all-natural\u2019 to imply beneficial to the environment) [12] or engaging in lobbying activities to mislead the public and policy-makers [4]. Such practices are often known as greenwashing [28, 12, 30, 13] \u2013 more formally defined as \u201cbehavior or activities that make people believe that a company is doing more to protect the environment than it really is\u201d [8]. Greenwashing can help a company to enhance their corporate legitimacy, if it is not exposed [36]. To prevent that from happening, instances of corporate greenwashing have been flagged by environmental, non-profit, and non-governmental organizations. The ongoing public scrutiny deters corporate attempts to mislead stakeholders with deceptive messages [36] and encourages truthful engagement with the public.\\n\\nMonitoring and identifying greenwashing at scale is difficult as it requires experts with domain knowledge to analyze corporate documents. Natural language processing (NLP) can help to alleviate the issue by automating the process of extracting relevant data quickly. As Stammbach et al. [38] mentioned, the initial step in identifying greenwashing would involve recognizing corporate environmental claims \u2013 something that NLP would be well-suited to handle. Previous NLP efforts\\n\\n\u2217Also a researcher of Hitachi America, Ltd., Santa Clara, California.\"}"}
{"id": "GF5l0F19Bt", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our paper proposes a dataset representing corporations\u2019 climate policy engagement collected from a plethora of corporate-related documents that can potentially be used to automatically detect greenwashing. We construct the dataset from LobbyMap, a platform operated by global think tank InfluenceMap. As illustrated in Figure 1, our dataset poses a task that takes the text extracted from a PDF file, such as a corporate sustainability report, as input and outputs a set of (query, stance, evidence page indices) triplets. The query represents high-level climate policy issues, such as \u2018Renewable energy\u2019. The stance represents a scale with five levels ranging from \u2018strongly supporting\u2019 to \u2018opposing\u2019. The evidence page indices give the supporting pages for the query and stance.\\n\\nWe invested effort in collecting data, and aligning evidence to transform the raw data of LobbyMap into the dataset. Consequently, our dataset differentiates itself from others in terms of its size, label richness, and diversity. Our contributions can be summarized as follows:\\n\\n1. **NLP Dataset on Climate Change.** We have assembled a high-quality, large-scale (i.e., including over 10K documents), and diverse dataset, designed for the task of evidence-based assessment of corporate climate policy engagement. We anticipate that our dataset will stimulate research on NLP and climate change, steering it towards more accurate detection of greenwashing.\\n\\n2. **Dataset Analysis.** We conducted analyses of the dataset properties, demonstrating that the task of our dataset is challenging as an NLP task.\\n\\n3. **Benchmark Experiments.** We evaluated the performance of pre-trained language models such as BERT, ClimateBERT, and Longformer on the task. While we obtained promising results, e.g., about 70% F-score for evidence-page detection, there remains ample room for improvement in prediction performance. Furthermore, to establish a benchmark for interpretability and explainability in this task, we introduce and evaluate a supplementary task where the model provides a scrutiny comment for the given query, stance, and evidence page indices.\\n\\nOverall, we hope that our dataset will stimulate research on NLP and climate change and possibly serves as a foundation for the detection of corporate greenwashing. The code and dataset are available at https://climate-nlp.github.io.\"}"}
{"id": "GF5l0F19Bt", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"products that tout positive environmental advantages, allegations of greenwashing have concurrently risen [28]. A survey study by de Freitas Netto et al. [12] suggests that certain research highlights selective disclosure wherein greenwashing manifests when corporations selectively highlight positive environmental impact. The renowned seven sins of greenwashing [39] have been widely discussed and are identified as product-level greenwashing in various studies [12, 13]. For instance, the sin of the lesser of two evils pertains to a claim that might be accurate within a specific product category but could potentially divert consumers from the broader environmental implications inherent to that entire category [12].\\n\\nNemes et al. [30] contributed a more exhaustive survey, underscoring the gaps in establishing a definition for greenwashing and setting definitive behavioral standards to identify it. A potential connection between our research and their definition of greenwashing might be found in the contexts of corporate responsibility in action (where a claim is categorized as greenwashing if it is not mirrored by consistent organizational practices) and political spin (deemed as greenwashing when corporations express green undertakings while concurrently lobbying against environmental legislation) [30]. This is because our dataset covers claims from a variety of sources, from official reports to political documents, which can spot potential contradictions in claims.\\n\\nGreenwashing-related NLP Research.\\nIn a few recent years, a burgeoning interest has emerged in leveraging NLP or fact-checking methodologies for the analysis of climate change-centric documents [43, 15]. While not directly addressing greenwashing, there exist Question Answering (QA) systems [10] and bots [34] tailored to facilitate the acquisition of credible climate-related knowledge. These systems typically utilize document retrieval techniques sourcing content from news articles by media and publications disseminated by global institutions to generate responses to user queries. Within the greenwashing context, researchers have implemented keyword analyses in annual reports to analyze mismatches in discourse, actions, and investments [25]. Detecting environmental claims is perceived as a first step towards a greenwashing evaluation [38, 37]. Stammbach et al. [37] introduced a sentence-level classification task for environmental claims in sources such as sustainability reports, earnings calls, and annual reports. Notably, ClimateBERT, a specialized model tailored for the climate domain, has been employed for detecting climate-related paragraphs [43] and for classifying TCFD categories [6]. Bingler et al. [6] provided an analysis that firms are primarily selective in reporting immaterial climate risk information. We believe our research aligns most closely with the detection of environmental claims, while concurrently performing the role of potential fact-checking.\\n\\n3 Understanding LobbyMap\\nLobbyMap (or its organization InfluenceMap) has been referenced in media outlets [40, 42] as well as in academic research [30], typically in the context of corporate greenwashing. LobbyMap analyzes a wide range of diverse information about companies and industry associations, categorizing each corporation's stance on specific topics. (See also their methodology [21] reproduced in the supplementary material.) LobbyMap features various platform categories, such as regional categories (Japan, United States, South Korea, Australia, and European Union) and a specific corporate group known as the Climate Action 100+ (CA100+) Investor Hub [19], which we focus on in our dataset.\\n\\nThe LobbyMap system consists of several interconnected components, as illustrated in Figure 2. These components work together to offer a comprehensive understanding of a corporation's stance on climate-related issues. Any key terms will be defined and explained in detail later.\\n\\nCompany Profile.\\nEach company on LobbyMap has a profile summary page that contains relevant company information, ratings, and a summary review by an expert. These assessments are substantiated by a scoring matrix described below.\\n\\nScoring Matrix.\\nAs illustrated in Figure 2b, the scoring matrix contains 13 query rows and 7 data source columns. Each cell in the matrix links to a page that contains evidence items pertinent to the selected query and data source.\\n\\nEvidence Item.\\nEach evidence item, as depicted in Figure 2c, records the corporate stance for the query along with an excerpt (we call this evidence snippet) quoted from an attached data source file in PDF format. This page also includes an analyst's comment summarizing the reasons for the assigned stance. One may also obtain other metadata such as the year, creation date, and region associated with the evidence.\"}"}
{"id": "GF5l0F19Bt", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Example screenshots of LobbyMap pages. The profile page contains a summary of the company, as well as a scoring matrix consists of 13 query labels and 7 data sources. Each matrix cell contains evidence items for the selected query and data source.\\n\\n3.1 Key Terms in an Evidence Item\\n\\nQuery. The query gives the subcategory of the climate policy engagement agenda. For instance, 'Energy transition & zero carbon technologies' relates to the economy's transition away from carbon-emitting technologies in line with the IPCC's guidelines. A comprehensive list of query definitions can be found in Appendix A.7.\\n\\nData Source. The data source is characterized by seven distinct document types: Main Website, Social Media, CDP Response, Direct Consultation with Governments, Media Reports, CEO Messaging, and Financial Disclosures. PDF files from this data source are attached to the evidence item. Depending on the analyst's approach, a PDF file attached may contain excerpts of only relevant pages, or the entire document. Occasionally, PDF files are screenshots of a website or social media.\\n\\nStance. A stance is a position expressed on a five-level discrete scale between +2 and \u22122. This scale represents whether the company is 'strongly supporting' (+2), 'supporting' (+1), expressing 'no position or mixed position' (0), 'not supporting' (\u22121), or 'opposing' (\u22122) the query.\\n\\nEvidence Snippet. An evidence snippet is an excerpt extracted by a human analyst from the attached PDF file in the evidence item. Snippets can be sentences, clauses, or even paragraphs, and may span multiple pages. We use these excerpts to identify the page indices of the PDF file that contain the evidence.\\n\\nComment. A comment is a brief, human-generated content pertaining to the query, stance, and evidence snippet. It provides an insight into the rationale behind the assigned stance. An example of a comment for a query 'Land use' is \\\"Supports forestry sequestration for carbon offsetting but is unclear if supports regulations.\\\" This comment is used in our supplementary task in Section 6.4.\\n\\n4 Dataset\\n\\nOur dataset is meticulously curated to offer value to various research purposes. For NLP, this dataset poses a real-world challenge concerning corporate documents in the sustainability domain. For sustainability and finance, models trained on this dataset provide a systematic way to predict a corporate stance on environmental categories.\\n\\n4.1 Dataset Design\\n\\nOur dataset adheres to two key design principles: First, it should contain all the necessary information to evaluate a corporation's climate policy engagements. Secondly, the data should be stored in a standard format so that it can quickly and easily be used by downstream tasks.\"}"}
{"id": "GF5l0F19Bt", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Information Included in the Dataset.\\nThe dataset meets the first design principle by recording corporate climate policy engagements, which will be used in our task as in Figure 1. Specifically, our dataset will contain text extracted from a document and its associated triplets \\\\((P, Q, S)\\\\), where \\\\(Q\\\\), \\\\(S\\\\), and \\\\(P\\\\) denote a query label, the stance label, and evidence page indices supporting the query and stance, respectively.\\n\\nOur dataset also includes the comment, which is used in the supplementary task described later. The above information provides sufficient information to assess a corporate position on a particular climate issue category.\\n\\nData Representation.\\nIn order to make the dataset easy to use, we store the data in the JSON Lines format since this is widely used in tasks such as fact-verification [41]. Each line contains an instance of our task. An example instance is shown in Figure 3. The \\\\textit{sentences} field contains the text data extracted and sentence-tokenized from the PDF files, including relevant details such as sentence ID and page numbers. Therefore, researchers can directly use the field as task input. The \\\\textit{evidences} field contains information obtained from evidence items (i.e., \\\\(P\\\\), \\\\(Q\\\\), and \\\\(S\\\\)). The \\\\textit{meta} field contains any other metadata, providing origin information for the evidences items.\\n\\n4.2 Dataset Construction\\nWe invested effort in PDF parsing and evidence alignment to transform the raw data of LobbyMap into the dataset. As depicted in Figure 4, the dataset construction primarily consisted of three stages: (i) data collection, (ii) establishing alignments (i.e., correspondences) between the evidence snippet and text in the PDF to identify the evidence page indices, and (iii) splitting data into training, validation, and test sets. Note that we omit pragmatic technique here, but that can be found in Appendix A.5.\\n\\nData Collection.\\nFrom February to March 2023, we collected evidence items from companies listed under CA100+ that includes firms key for climate-change. A comprehensive list of these companies can be found in Appendix A.8.\\n\\nText Extraction from PDF Files.\\nPDF files have varying layouts. In particular, some files contain only embedded images without embedded text. We employed three different PDF text extraction parsers to obtain textual data from the PDF files robustly: Fitz in PyMuPDF [33], docTR [29], and Tesseract [22]. The first is a tool that extracts embedded text from PDF file whereas the last two are Optical Character Recognition (OCR) based software. Our approach of using three parsers to extract textual data minimizes the chances of not obtaining any usable information. We also preserved the order of the text based on the layout produced by each parser.\\n\\nAlignment between Evidence Snippet and PDF File.\\nWe need to construct \\\\(P\\\\), the indices of the pages containing evidence snippets, because the evidence snippet does not explicitly tell us which part of the PDF file was extracted for the snippet. We used NLTK [7] to split the evidence snippet and text extracted from the PDF file into sentences to align the evidence snippet and PDF at the sentence level. After obtaining alignments from each of the three aforementioned parsers, we selected the one with the highest number of alignment. Finally, we obtained the page indices containing the alignments and designated them as \\\\(P\\\\).\\n\\nData Splitting.\\nWe split the data obtained by the previous steps into training, validation and test sets based on the year metadata included in the evidence items. Documents before 2022 were assigned into the validation set and train set, while those in or after 2022 were designated as test data. This decision ensures that our model evaluation hinges on more recent data, mirroring a realistic scenario where data from the future is employed for evaluation.\"}"}
{"id": "GF5l0F19Bt", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Basic statistics of the dataset (avg. num. per document in parentheses.)\\n\\n|        | Train  | Validation | Test  |\\n|--------|--------|------------|-------|\\n| # Doc  | 7,425  | 825        | 2,354 |\\n| # Output triplets | 11,159 (1.50) | 1,229 (1.49) | 3,336 (1.42) |\\n| # Word | 28,434,661 (3829.58) | 3,067,156 (3717.76) | 6,244,125 (2652.56) |\\n| Page   | 67,091 (9.04) | 7,289 (8.84) | 15,755 (6.69) |\\n\\nDataset Analysis\\n\\nThis section provides data analyses and demonstrates the values of the dataset for benchmarking.\\n\\nBasic Statistics.\\nTable 1 shows statistics on the dataset. The training set contains over 7K documents, totalling 28M words distributed across over 60K pages. Also, the number of pages per document is approximately 6-9 and the word counts per document exceed 2,500. This property suggests that our task can be positioned as long document understanding, which is a challenging aspect of NLP.\\n\\nLabel Distribution.\\nWe analyzed the label distribution of query and stance. (The full table is in Appendix A.9.1.) We found that the labels are imbalanced. There are about 1K training samples of 'Energy transition & zero carbon technologies' for 'supporting', while the query 'Land use' shows a sparse presence of labels. The imbalances introduce another level of challenge, where models must predict less frequent labels accurately. We also found a general trend of companies leaning towards a positive stance. This trend may be explained by the fact that companies often seek to enhance their public reputation.\\n\\nDiversity.\\nWe found that our dataset is diverse in terms of:\\n\\n(i) Time. The dataset includes evidence items spanning over years. (See Appendix Figure 8.) The increasing number of samples collected each year suggests that the reporting and/or scrutiny have increased over time.\\n\\n(ii) Data Source. The most frequent data source is Main Web Site and the least is Financial Disclosures (See Appendix Figure 9.) Interestingly, Social Media and CEO Messaging are also frequent, showing our dataset contains various sources.\\n\\n(iii) Geographic Diversity Regarding the Company. Figure 5 shows the geographical distribution of the corporate headquarters in our dataset. We can see that our dataset contains companies not only from major economic powers such as United States, but also from resource-rich countries such as Australia, which has been accused of state-sponsored greenwashing [17]. The dataset covers all the continents except Antarctica, allowing researchers to perform their analysis across various type of corporations.\\n\\n(iv) Geographic Diversity Regarding the Evidence Item Region. In LobbyMap, each evidence item is associated with a specific regional target. For example, a document from an Australian media article sometimes describe regional events and can be associated within Australia. In the case of a multinational company, it may be associated within 'Global' that is not bound to a specific region. We examined such geographic diversity of evidence items (See Appendix Figure 11 for more details.) We found that 'Global' appears frequently, suggesting most companies in our dataset are multinational. The number of evidence items from Europe and United States is enormous.\\n\\n(v) Sector. There are 14 different types of company sectors in our dataset: automobiles, chemicals, construction materials, consumer staples, energy, food products, healthcare, industrials, information...\"}"}
{"id": "GF5l0F19Bt", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"technology, metals & mining, paper & forest products, retailing, transportation, and utilities. The wide variety of sectors represented in our dataset suggests that it may be of interest to researchers studying diverse industrial fields.\\n\\n6 Benchmark Experiments\\n\\nHere, we benchmark the proposed task on the dataset. Conceptually, our task projects an input text extracted from a document into a set of output triplets \\\\( (P, Q, S) \\\\) \u2208 \\\\( Y \\\\).\\n\\n6.1 Models\\n\\nFirst, we provide a most-frequent baseline that always outputs majority labels: \\\\( (P = \\\\{0\\\\}, Q = \\\\text{'Energy transition & zero carbon technologies'}, S = \\\\text{'supporting'}) \\\\) for each document. This is similar to the majority class baseline and is useful as a simple consideration of the task's lower bounds.\\n\\nNext, given that our benchmark will be used as baselines for future work, we decided to use pre-trained language models that are widely used in current state-of-the-art studies. The challenge of this is that it is difficult to handle dozens of pages at the same time because most pre-trained language models have a limited context length. To this end, we employ a page-wise classification approach, where each document is split into pages, and we feed text of each page into the model and obtain output labels, gathering all the outputs in the document.\\n\\nWe have the following three page-wise classifiers as a pipeline: (i) The evidence page detector is fine-tuned to predict whether each page contains evidence or not. (ii) The query classifier predicts query labels (i.e., multi-label classification) given a page detected by (i). Gold page indices are used for fine-tuning. (iii) The stance classifier predicts one of the five stance labels given the detected page and the predicted query label. Gold page indices and query labels are used for fine-tuning.\\n\\nAs basis of the classifiers, BERT [14] and its variant models, RoBERTa [27], ClimateBERT [43] and Longformer [5], with a classification head, are used. BERT and RoBERTa are known for the strong baseline in text classification tasks. ClimateBERT is pre-trained on sustainability and climate domain so we can verify the effectiveness of domain adaptation. We also provide Longformer, which is specialized for long-document understanding. These pre-trained language models are used as initial weights for the parameters of each of the three classifiers. The optimizer is Adam [23].\\n\\nFor comparison, we introduce a simple linear model using logistic regression [11] and tf-idf [35]. While this model also employs the page-wise approach, it provides a distinct logistic regression binary classifier for each query and stance label.\\n\\nImplementation and Hyperparameters. The input text for each page is created by concatenating all the sentences in the page. During inference for evidence page detection, if none of the pages are identified as containing evidence, the page with the highest probability is considered as the evidence page. For the stance classification, the input text is created by inserting query label in front of the page sentences. If the query classification generates multiple query labels for a single page, we create separate input text for each query label. Finally, we post-process the outputs by gathering evidence page indices which share the same query and stance labels. The implementation detail and hyperparameters are shown in Appendix A.10.\\n\\n6.2 Evaluation Metrics\\n\\nWe evaluate the model outputs in the aspects of evidence page detection, query classification, and stance classification. We provide three types of F-score metrics as follows:\\n\\n**Strict.** This is a standard F-score, based on the set of tuples produced by a model and gold. The F-score for evidence page indices is evaluated using the set of output tuples and the set of gold tuples. The F-score is calculated for the page indices by singletons \\\\( (P) \\\\), for the query by tuples \\\\( (P, Q) \\\\) and for the stance by tuples \\\\( (P, S) \\\\).\\n\\n**Page overlap.** The above metric is \\\"too strict\\\" and can not capture how close are the model predictions to gold. To this end, we provide an evidence page overlap-based metric. This is based on the work of Barnes et al. [2], which calculates graph-based structured sentiment F-scores using the ratio of word token overlap between predicted and gold outputs. In this study, the F-scores are calculated by overlap ratio of gold and predicted evidence page indices. Thus, the more overlap, the better.\"}"}
{"id": "GF5l0F19Bt", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Evaluation results, measuring test F-score (%).\\n\\nQ, S, and P represent query, stance and evidence page indices, respectively.\\n\\n| Document Page overlap | Strict P | Q | S | P | Q | S | P | Q | S |\\n|-----------------------|----------|---|---|---|---|---|---|---|---|\\n| Most-frequent         | 46.7     | 52.6 | 36.8 | 51.8 | 25.6 | 19.8 | 41.2 | 19.6 | 17.5 |\\n| Linear                | 66.0     | 61.9 | 50.3 | 71.4 | 44.5 | 36.1 | 52.0 | 31.2 | 27.0 |\\n| BERT-base             | 71.0     | 63.5 | 51.6 | 73.6 | 48.1 | 37.2 | 50.2 | 31.9 | 25.8 |\\n| ClimateBERT           | 71.8     | 64.0 | 52.8 | 74.4 | 48.9 | 39.0 | 50.2 | 32.2 | 26.8 |\\n| RoBERTa-base          | 71.6     | 64.5 | 53.1 | 73.8 | 49.6 | 38.3 | 50.4 | 33.4 | 26.6 |\\n| Longformer-base       | 73.7     | 66.9 | 54.6 | 75.9 | 53.0 | 40.8 | 52.5 | 36.1 | 28.6 |\\n| Longformer-large      | 73.9     | 68.8 | 57.3 | 76.5 | 55.0 | 44.1 | 53.6 | 38.7 | 31.5 |\\n\\nFigure 6: For two companies in the energy sector, the distributions of gold and predicted stances for 'Energy transition & zero carbon technologies' are shown. The model used is Longformer-large.\\n\\nF-score is. For query and stance labels scores, the overlap is considered only for cases which share the same query or stance label. For more detail, refer to Appendix A.10.\\n\\nThis is the most rough metric where F-scores for query, stance, and evidence page indices are evaluated independently. The motivation of this metric is that there can be cases where the user wants to know just overall trends of query or stance of a company, not fine-grained evidence-based outputs, e.g., one can examine whether energy sector firms are increasingly mentioning renewable energy. For the F-score of evidence page indices, a set of output tuples $s$ and a set of gold tuples $g$, where each tuple represents $(document id, i)$ and $i$ denotes an evidence page index, are used to calculate F-score. Similarly, F-score is calculated for query by output tuples $(document id, Q)$ and for stance by $(document id, S)$. In other words, an output of query/stance is considered correct if the query/stance label is correct, even if the evidence page indices for the output are incorrect.\\n\\n6.3 Result and Discussion\\n\\nTable 2 provides an overview of the F-score for the test set. For the results of validation set, refer to Appendix Table 8. The results illustrate that all models can detect evidence page indices, with an F-score of about 70% in the document or page overlap $P$ metrics. Given that most PDF files contain only one page (See Appendix Figure 10), this result may be generous. Nevertheless, given the F-score in the document $P$ metric of the most-frequent baseline, which always outputs a page index of 0, shows a low F-score, we can see that training the model is highly effective. On the other hand, F-scores of the strict metric suggests the difficulty of our task to exactly identify evidences. This insight will be further explained later. Query ($Q$) and stance ($S$) classification proves to be challenging. All models demonstrate lower F-scores in these aspects across all the metrics, which might reflect the intricate nature of these tasks.\\n\\nIn terms of pre-trained language models, ClimateBERT outperformed BERT. This indicates that pre-training on the sustainability domain is effective in our task. However, the better trained but not climate-specific RoBERTa partially outperformed ClimateBERT. In turn, Longformer outperformed other models like RoBERTa, showing its robustness in handling long documents. Interestingly, the linear model outperformed BERT in the strict metric for $P$ and $S$. The linear model had higher precision but lower recall than BERT. The strict metric deems predicted page indices incorrect if they deviate at all from the gold indices, potentially disadvantaging high-recall models like BERT. These findings highlight the importance of using multiple metrics, including document and page overlap.\\n\\nError Analysis \u2013 Classification.\\n\\nWe investigate representative error patterns of the Longformer-large model in the document metric. For the query, one of the most frequent errors is that the model\"}"}
{"id": "GF5l0F19Bt", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"'Energy transition & zero carbon technologies' and 'Renewable energy', while the gold is 'Energy transition & zero carbon technologies'. This might be because the label 'Energy transition & zero carbon technologies' sometimes includes the topic of 'Renewable energy', and the model could not distinguish that. For the stance, the frequent error is that the model predicted 'supporting', while the gold is 'no position or mixed position'. This suggests that the model finds it difficult to distinguish different nuances regarding the stance.\\n\\nThe number of pages in a document 0\\n\\nFigure 7: Strict F-score based on the number of pages in the document. Error Analysis \u2013 Longer Document. The lower strict F-score, as noted earlier, seems to be influenced by the length of the documents. Figure 7 illustrates a decline in the prediction performance for $P$ in the Longformer-large model with an increase in the number of pages per document. Inherently, due to the metric's nature, $Q$ and $S$ scores are as low as $P$, as they are constrained by $P$. This indicates that our model struggles with longer documents in terms of the fine-grained (i.e., strict) metric. This issue might be because our model, which operates training and inference on the page-by-page manner, does not consider the context of the entire document.\\n\\nThe above results and error analyses suggest the necessity of a domain-adapted model that is capable of comprehending lengthy contexts. Specifically, improving the model's focus on the overall document context and the consistency of multi-label output could potentially enhance its performance.\\n\\nPilot Study. We present a pilot study aimed at greenwashing detection. As depicted in Figure 6, we select two major companies (A and B) from the energy sector and aggregate the predicted stances towards the query 'Energy transition & zero carbon technologies'. The figure shows that our predictions are reasonably capable of replicating the trends present in the gold. The important finding here is that the trend of companies A and B is different even in the same sector. The company A tends to present negative stances, whereas the company B exhibits a more ambiguous stance. Company B's presentation of conflicting stances raises the possibility of greenwashing. Even if this were not the case, such comparisons allow researchers to uncover hypotheses worth exploring further. Thus, our dataset can also be used to model and test hypotheses about the corporate climate policy engagement.\\n\\n6.4 Comment Generation: A Supplementary Task for More Explanation\\n\\nTable 3: Comment generation eval-\\n\\n|              | R-1  | R-2  | R-L  |\\n|--------------|------|------|------|\\n| Test         |      |      |      |\\n| FlanT5-large | 38.4 | 22.1 | 34.8 |\\n| FlanT5-XL    | 39.5 | 22.7 | 35.6 |\\n| Validation   |      |      |      |\\n| FlanT5-large | 43.4 | 28.1 | 40.5 |\\n| FlanT5-XL    | 42.7 | 27.6 | 39.7 |\\n\\nOne can recognize that the output triplet $(P, Q, S)$ may not provide sufficient context in some cases, specifically when there is a need to understand how the evidence page is interpreted and how the query and stance are derived from the evidence. Therefore, we introduce a supplementary task: generating comments (as described in Section 3) for a given query, stance, and evidence page indices. We fine-tune FlanT5 [9], where the model is trained by the comments as reference. The generated comments are evaluated by ROUGE [26]. The input text for the model is formatted as 'Generate a reason why the corporate climate policy engagement for \\\"QUERY\\\" is \\\"STANCE\\\". <PAGE TEXTS>', where QUERY andSTANCE represent the query and stance label respectively. <PAGE TEXTS> represents the concatenated sentences from all reference evidence pages ($P$).\\n\\nTable 3 represents evaluation results, suggesting the potential of using generative models to generate rationale comments for given triplets $(P, Q, S)$. The FlanT5-XL model outperforms the FlanT5-large model on test data. Performance on the test set is lower than on the validation set. Since the validation and train sets are sampled from the same temporal span, this result suggests that generation performance can be undermined due to changes in the distribution of comments over time.\\n\\nNote that our objective here is not to definitively characterize these companies, but to illustrate the concept of our use case. Thus, we have anonymized the company name.\"}"}
{"id": "GF5l0F19Bt", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Conclusion and Future Work\\n\\nThe field of research aimed at effectively utilizing NLP to contribute to solving climate change issues is still in its infancy. However, endeavors undertaken in recent years have demonstrated the considerable scope for advancement in this realm [38, 43]. We introduced an NLP dataset to predict corporate climate policy engagement. This study is one of the attempts to provide fundamental knowledge in this budding research area. We hope that the proposed dataset will stimulate research on NLP and climate change by laying the foundation for the detection of corporate greenwashing.\\n\\nWe recognize that there is future work to be done. The benchmark experiments revealed room for improvement in prediction performance. We describe several promising directions for future research:\\n\\n(i) Multi-modal. The development of multi-modal models capable of processing not only text from PDF files but also embedded images could enhance the prediction performance.\\n\\n(ii) Multi-lingual. While we only considered English text in this work, the capability to work with other languages will allow us to create a more diverse dataset.\\n\\n(iii) Few-shot learning. By leveraging the capabilities of Large Language Models (LLMs), we can explore strategies for few-shot learning, potentially enabling more efficient training with smaller amounts of data.\\n\\nAcknowledgments and Disclosure of Funding\\n\\nDylan Tanner, Edward Collins, Chris Hurst, who founded InfuenceMap in 2015, and Harri Rowlands provided valuable data and allowed us to make our datasets publicly available. We thank them for their assistance and interest in the work. Computational resources of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial Science and Technology (AIST) were used for this work. This research was done in the Stanford Data Science (SDS) Affiliates Program. The first author (GM) conducted this study as a Visiting Scholar at Stanford University. GM is also an employee of Hitachi America, Ltd. and received financial support for this study. We thank Chi Heem Wong, Yasushi Miyata, Terufumi Morishita, Arnab Chakrabarti, and the anonymous referees for their helpful comments on this paper.\\n\\nAbout Dataset Use.\\n\\nThe dataset used in this analysis is part of InfluenceMap\u2019s content and was used and is released with the latter\u2019s approval. InfluenceMap maintains (since 2015) an ongoing database containing millions of data points each consisting of evidence pieces around corporate climate/nature claims and performance. These are scored against globally accepted science based benchmarks such as the IPCC and the IPBES. Subsets of this data for ML/AI and other analysis are available by request from InfluenceMap and use of InfluenceMap\u2019s content is subject to Terms and Conditions. Please contact us at info@influencemap.org for more information (kindly use an organizational email).\\n\\nReferences\\n\\n[1] Amir Amel-Zadeh and George Serafeim. 2018. Why and how investors use ESG information: Evidence from a global survey. *Financial Analysts Journal*, 74(3):87\u2013103.\\n\\n[2] Jeremy Barnes, Robin Kurtz, Stephan Oepen, Lilja \u00d8vrelid, and Erik Velldal. 2021. Structured sentiment analysis as dependency graph parsing. In *Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, pages 3387\u20133402, Online. Association for Computational Linguistics.\\n\\n[3] Lauren M. Baum. 2012. It's not easy being green . . . or is it? A content analysis of environmental claims in magazine advertisements from the united states and united kingdom. *Environmental Communication*, 6(4):423\u2013440.\\n\\n[4] Sharon Beder. 2014. Lobbying, greenwash and deliberate confusion: how vested interests undermine climate change. M. C-T. Huang and R. R-C. Huang (Eds.), *Green Thoughts and Environmental Politics: Green Trends and Environmental Politics*.\\n\\n[5] Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. *CoRR*, abs/2004.05150.\\n\\n[6] Julia Anna Bingler, Mathias Kraus, Markus Leippold, and Nicolas Webersinke. 2022. Cheap talk and cherry-picking: What ClimateBert has to say on corporate climate risk disclosures. *Finance Research Letters*, 47:102776.\"}"}
{"id": "GF5l0F19Bt", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "GF5l0F19Bt", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"No\u00e9mi Nemes, Stephen J. Scanlan, Pete Smith, Tone Smith, Melissa Aronczyk, Stephanie Hill, Simon L. Lewis, A. Wren Montgomery, Francesco N. Tubiello, and Doreen Stabinsky. 2022. An integrated framework to assess greenwashing. *Sustainability*, 14(8).\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Pytorch: An imperative style, high-performance deep learning library. In *Advances in Neural Information Processing Systems 32*, pages 8024\u20138035. Curran Associates, Inc.\\n\\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. *Journal of Machine Learning Research*, 12:2825\u20132830.\\n\\nMd Rashad Al Hasan Rony, Ying Zuo, Liubov Kovriguina, Roman Teucher, and Jens Lehmann. 2022. Climate bot: A machine reading comprehension system for climate change question answering. In *Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22*, pages 5249\u20135252. International Joint Conferences on Artificial Intelligence Organization. AI for Good - Demos.\\n\\nClaude Sammut and Geoffrey I. Webb, editors. 2010. *TF\u2013IDF*, pages 986\u2013987. Springer US, Boston, MA.\\n\\nPeter Seele and Lucia Gatti. 2017. Greenwashing revisited: In search of a typology and accusation-based definition incorporating legitimacy strategies. *Business Strategy and the Environment*, 26(2):239\u2013252.\\n\\nDominik Stammbach, Nicolas Webersinke, Julia Bingler, Mathias Kraus, and Markus Leippold. 2023. Environmental claim detection. In *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)*, pages 1051\u20131066, Toronto, Canada. Association for Computational Linguistics.\\n\\nDominik Stammbach, Nicolas Webersinke, Julia Anna Bingler, Mathias Kraus, and Markus Leippold. 2022. A dataset for detecting real-world environmental claims. *ArXiv preprint arXiv:2209.00507*.\\n\\nTerraChoice. 2020. The sins of greenwashing: home and family edition. [http://sinsofgreenwashing.org/findings/the-seven-sins/](http://sinsofgreenwashing.org/findings/the-seven-sins/).\\n\\nThe Guardian. 2022. Oil and gas firms' green investments fail to match promise of publicity \u2013 study. [https://www.theguardian.com/environment/2022/sep/08/oil-and-gas-firms-green-investments-fail-to-match-promise-of-adverts-study](https://www.theguardian.com/environment/2022/sep/08/oil-and-gas-firms-green-investments-fail-to-match-promise-of-adverts-study). Accessed: 2023-05-11.\\n\\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pages 809\u2013819, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nTIME. 2022. Thinking of investing in a green fund? many don't live up to their promises, a new report claims. [https://time.com/6095472/green-esg-investment-funds-greenwashing/](https://time.com/6095472/green-esg-investment-funds-greenwashing/). Accessed: 2023-05-11.\\n\\nNicolas Webersinke, Mathias Kraus, Julia Bingler, and Markus Leippold. 2022. ClimateBERT: A Pretrained Language Model for Climate-Related Text. In *Proceedings of AAAI 2022 Fall Symposium: The Role of AI in Responding to Climate Challenges*.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, pages 38\u201345, Online. Association for Computational Linguistics.\"}"}
{"id": "GF5l0F19Bt", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In alignment with the Code of Ethics as outlined by NeurIPS, we offer the following discussion on the ethical considerations and potential societal impact of our research.\\n\\nPrivacy. Our dataset comprises text gathered from corporate reports, news media, social media posts and so on. While corporate reports and news media may contain company names, product names, and names of executives or employees, these are typically public-facing pieces of information.\\n\\nConsent. Our dataset includes information such as corporate disclosures and news articles, and we recognize that the information are intended to be publicly available.\\n\\nDepreciated Datasets. Not applicable.\\n\\nCopyright and Fair Use. Given that we are using data from LobbyMap, the license for the data itself is copyrighted to the organization.\\n\\nRepresentative Evaluation Practice. We discuss the geographic diversity of our dataset in our paper, and the sectors of the companies included and their headquarters' locations are listed in Appendix A.8. As a result, we have managed to cover companies from regions, including major continents. However, we must acknowledge that our dataset is predominantly composed of data from companies belonging to economic and resource-rich nations. Users of our dataset should be mindful of this point.\\n\\nSafety. Our research does not employ technologies that directly inflict harm on individuals.\\n\\nSecurity. Our models can potentially be applied to automate investment decisions and corporate auditing. If the model produces erroneous outputs, or if a user inputs malicious data, it could distort the system's judgment and possibly cause accidents resulting from investment or auditing errors. The labels in our dataset are limited, and the model's recall is not flawless. This implies that models might fail to detect specific environmental claims and stances. Such shortcomings could incentivize firms to continue unjustified activities. Also, although not our intended use, firms can use the model to analyze their own documents and adopt unfair strategies to increase their reputation.\\n\\nDiscrimination. Our dataset and models can potentially be used for evaluating corporate climate policy engagement. The model outputs may negatively appraise specific companies, potentially affecting their reputation or that of their employees. In the worst-case scenario, it could contribute to discrimination against specific companies or employees. To minimize such risks, researchers should exercise particular caution when evaluating specific companies and publicizing the results.\\n\\nSurveillance. Our dataset is not targeted at specific individuals but rather at corporations.\\n\\nDeception & Harassment. We believe there is a low likelihood that this dataset could contribute to hate speech or harassment.\\n\\nEnvironment. Similar to many other machine learning studies, the environmental impact due to energy consumption during model development is recognized. Moreover, while we take the utmost care in creating our dataset, errors in the original data or the dataset creation process could cause the model to make erroneous judgments. Such incorrect decisions could lead to misguided investment decisions or environmental measures, which could consequently impose an environmental burden. Researchers should not apply the dataset or model to decision-making or policy-making.\\n\\nHuman Rights. Not applicable.\\n\\nBias and fairness. The likelihood of our dataset containing biases towards specific races, genders, etc., is low. Biases related to specific countries, regions, or industrial sectors may be considered. Researchers should be aware of the potential existence of bias mainly towards corporations.\\n\\nA.2 Limitations\\n\\nData Quality. We proposed a dataset based on LobbyMap data; although LobbyMap data is generated by expert analysts based on an established methodology, the specific process is unknown and may\"}"}
{"id": "GF5l0F19Bt", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"contain biases that we do not perceive. In addition, since an agreement study is not conducted on the data, we do not know the extent to which the task will achieve consensus among the experts. In addition, although we have collected data from a wide variety of firms based on CA100+ to construct the dataset, its comprehensiveness remains insufficient given the huge number of firms that exist worldwide. We have not been able to validate its performance for companies other than those included in the dataset.\\n\\nBias. We recognize that the environmental policy framework within our data might be biased. For instance, a reviewer highlighted that the science of climate change is dynamic; solutions deemed pivotal and popular one year may prove ineffective later. To stay aligned with current information and mitigate this bias, one could either incorporate newer data or diversify data sources. On the other hand, when the model assesses a firm's stance, it's inevitable that the evaluation will reflect the inherent bias of the policy, although the validity of this bias varies depending on the use case. Bias can also arise from the training data of the pre-trained models. For instance, the policy might favor a specific region or industry.\\n\\nDataset Construction. We implemented a number of programs based on pragmatic assumptions to construct the dataset. Although we paid utmost attention to the quality of the dataset, these processes may cause the dataset to contain incorrect information. The output results of PDF parser contain a lot of noise, so the input text will contain many errors caused by layout analysis or OCR. In addition, the dataset do not cover all the evidences in LobbyMap, since evidences that either failed to parse a PDF file or failed to make the alignment are dropped from the dataset. For example, evidences where the text is written in Japanese or Chinese would be discarded because the alignment algorithm only considers English text. Also in LobbyMap, depending on the PDF file, it may represent the entire report or only some pages cut out. Consistency in the composition of PDF files is not guaranteed. Therefore, researchers should take into account the possibility that our dataset may contain errors, noises and biases on the task. However, we believe that technological advances in PDF parsing and alignment can address this problem by continually updating the dataset.\\n\\nLobbyMap only provides documents that contain an evidence item, not documents that do not contain evidences. This means our dataset does not have 'negative samples' in terms of documents. We are based on the strong assumption that at least one page of each document contains evidence item, limiting the benchmark capability for the negative samples.\\n\\nModel and Evaluation. Our proposed pipeline approach performs classification on a page-by-page manner. This does not account for contextual interactions between pages, which may lead to degradation of predictive performance. In addition, our approach cannot be applied to documents that do not contain evidence items because of the above mentioned strong assumption (that at least one page of each document contains an evidence.)\\n\\nOur models use the text extracted from PDF files as input, while they do not use other modality such as layout, image and table presented in each page. This limits the expressive power of the input, leading to possible predictive performance degradation.\\n\\nIn the supplementary task, we evaluated generative models that output comments given the gold triplets \\\\((P, Q, S)\\\\) as input, but we do not perform end-to-end evaluation. Therefore, using the prediction results of the pipeline approach to generate comments would lead to predictive performance degradation.\\n\\nA.3 Computational Resource\\n\\nFor all experiments, we used NVIDIA A100 GPUs provided by ABCI. As of 2022, ABCI ranks 32nd in the Green500 benchmark for power consumption, making it an energy efficient system. We did not conduct hyperparameter search, however, we consumed the computation resource for preliminary experiments. In the experiments, no pre-training was conducted, only fine-tuning, so we believe the environmental impact is considered low.\\n\\n5 https://docs.abci.ai/en/system-overview/\\n6 https://www.top500.org/lists/green500/list/2022/11/\"}"}
{"id": "GF5l0F19Bt", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.4 Code and Data Availability\\nThe code and data are available at https://climate-nlp.github.io.\\n\\nA.5 Dataset Construction Detail\\nHere, we detail the dataset construction procedure.\\n\\nA.5.1 PDF Parsers\\nIn instances where a PDF file reading results in an error, such as I/O errors, we assume that the parser extracts no text. For each PDF parser, we set a maximum limit of 100 pages per document. Any document containing more than 100 pages omits the excess pages. We tokenize the text of each page into sentences using NLTK and follow the text order as per the 'block' outputs of each parser from the PDF file.\\n\\nA.5.2 Evidence Cleaning\\nTo filter out invalid or ambiguous evidence items from the collected LobbyMap data, we applied pragmatic filters. First, we exclude evidence items whose associated companies do not correspond with the company list from CA100+. Second, we remove duplicated evidence items that arose during the data collection process. Third, we discard evidence items that have no attached PDF file or are associated with multiple PDF files, retaining those with a single attached PDF file. Lastly, we remove evidence items that lack evidence excerpts, which were typically indicated as '\u2013no extract\u2013' in the original data. In the final two steps, we eliminate all evidence items referring to PDF files that were referenced by the evidence item to be deleted.\\n\\nA.5.3 Evidence Alignment\\nDuring evidence alignment, we apply a partial string match between sentences in the evidence snippet and sentences on each page of a document. We consider pairs with at least a 95% partial string match as aligned, but only for sentences that contain at least five words. Pages with aligned sentences are included in the evidence page indices. However, to filter out alignment noise, we exclude pages containing 14 words or fewer. Furthermore, when we encounter failure in reading PDF data, the associated evidence items are discarded.\\n\\nA.5.4 Resolving Document Duplication\\nOccasionally, different PDF files contained identical contents even though they were saved under different names. As such, we merged the evidence items for documents with identical hash values of the PDF data that was parsed by each parser.\\n\\nAlso, by calculating the hash value of all sentences contained in a document, we verified that the input documents for the training data does not leak into that for the test data.\\n\\nA.5.5 Best Parser Selection\\nIf all parsers failed to extract text from a document, we discarded that document. We selected the best parser based on the number of alignments. If the same number of alignments were produced by multiple parsers, we followed a preferential order of Fitz, Tesseract, and docTR.\\n\\nA.5.6 Comment Generation Data\\nIn some cases, multiple evidence items are associated with the same triplet \\\\((P, Q, S)\\\\). The differences in comments between such evidence items often arise from paraphrasing or differing target company names. To create a practical gold standard for comments, we selected the longest comment from among the candidates.\\n\\nA.6 Dataset Analysis Detail\\nFigure 8 shows the year distribution of our dataset.\"}"}
{"id": "GF5l0F19Bt", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: Year distribution of our dataset\\n\\nFigure 9 shows the data source distribution.\\n\\nFigure 10: Histogram of page numbers in a document\\n\\nFigure 11 shows the evidence item region distribution of selected regions or countries.\\n\\nA.7 Full Query List\\n\\nThe following 13 query definitions are cited as is from the scoring matrix page of LobbyMap.\\n\\n- **Alignment with IPCC on climate action.**\\n  Is the organization supporting the science-based response to climate change as set out by the IPCC?\\n\\n- **Carbon tax.**\\n  Is the organisation supporting policy and legislative measures to address climate change: carbon tax.\\n\\n- **Communication of climate science**\\n  Is the organization transparent and clear about its position on climate change science?\\n\\n- **Disclosure on relationships.**\\n  Is the organization transparent about its involvement with industry associations that are influencing climate policy, including the extent to which it is aligned with these groups on climate?\\n\\n- **Emissions trading.**\\n  Is the organisation supporting policy and legislative measures to address climate change: emissions trading.\\n\\n- **Energy and resource efficiency**\\n  Is the organization supporting policy and legislative measures to address climate change: energy efficiency policy, standards, and targets.\"}"}
{"id": "GF5l0F19Bt", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11: Distribution of selected evidence item regions\\n\\n- Energy transition & zero carbon technologies\\n  Is the organization supporting an IPCC-aligned transition of the economy away from carbon-emitting technologies, including supporting relevant policy and legislative measures to enable this transition?\\n\\n- GHG emission regulation.\\n  Is the organization supporting policy and legislative measures to address climate change: GHG emission standards and targets. Is the organization supporting policy and legislative measures to address climate change: Standards, targets, and other regulatory measures directly targeting Greenhouse Gas emissions.\\n\\n- Land use.\\n  Is the organization supporting policy and legislative measures to enhance and protect ecosystems and land where carbon is being stored?\\n\\n- Renewable energy.\\n  Is the organization supporting policy and legislative measures to address climate change: Renewable energy legislation, targets, subsidies, and other policy.\\n\\n- Support of UN climate process.\\n  Is the organization supporting the UN FCCC process on climate change?\\n\\n- Supporting the need for regulations.\\n  To what extent does the organization express the need for regulatory intervention to resolve the climate crisis?\\n\\n- Transparency on legislation.\\n  Is the organisation transparent about its positions on climate change legislation/policy and its activities to influence it?\\n\\nA.8 Full Company List\\nWe show the full company list in our dataset below:\\n\\n| Company | Sector          | Headquarters         |\\n|---------|----------------|----------------------|\\n| Adbri (Adelaide Brighton) | Construction Materials | Adelaide, Australia |\\n| AGL Australia Utilities | Utilities          | Sydney, Australia    |\\n| Air France-KLM Transportation | Transportation     | Paris, France        |\\n| Air Liquide Chemicals | Chemicals          | Paris, France        |\\n| Airbus Group Industrials | Industrials        | Leiden, Netherlands  |\\n| American Airlines Group Transportation | Transportation     | Fort Worth, United States |\\n| American Electric Power Utilities | Utilities          | Columbus, United States |\\n| Anglo American Metals & Mining | Metals & Mining    | London, United Kingdom |\\n| Antam (Aneka Tambang) | Metals & Mining    | Jakarta, Indonesia   |\\n| ArcelorMittal Metals & Mining | Metals & Mining    | Luxembourg, Luxembourg |\\n| BASF Chemicals | Chemicals          | Ludwigshafen, Germany |\\n| BHP Metals & Mining | Metals & Mining    | Melbourne, Australia |\\n| Bluescope Steel Metals & Mining | Metals & Mining    | Melbourne, Australia |\\n| BMW Group Automobiles | Automobiles        | Munich, Germany      |\\n| BP Energy | Energy            | London, United Kingdom |\\n| Bayer Healthcare | Healthcare         | Leverkusen, Germany  |\\n| Berkshire Hathaway Industrials | Industrials        | Omaha, United States |\\n| Boeing Industrials | Industrials        | Chicago, United States |\\n| Boral Limited Metals & Mining | Metals & Mining    | Sydney, Australia    |\\n| Bumi Resources Metals & Mining | Metals & Mining    | Jakarta, Indonesia   |\\n| Bunge Limited Consumer Staples | Consumer Staples    | St. Louis, United States |\\n| CEZ Utilities | Utilities          | Czech Republic, Czech Republic |\\n| CNOOC Energy | Energy            | Hong Kong, China     |\\n| CRH Construction Materials | Construction Materials | Dublin, Ireland |\\n| Canadian Natural Resources Ltd (CNRL) | Energy          | Calgary, Canada      |\\n| Caterpillar Industrials | Industrials        | Peoria, United States |\\n| Grupo Argos Construction Materials | Construction Materials | Medell\u00edn, Colombia |\\n| Cemex Construction Materials | Construction Materials | Monterrey, Mexico    |\\n| Centrica Utilities | Utilities          | Berkshire, United Kingdom |\\n| Chevron Energy | Energy            | San Ramon, United States |\\n| China Petroleum & Chemical Corporation (Sinopec) | Energy          | Beijing, China      |\"}"}
{"id": "GF5l0F19Bt", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "GF5l0F19Bt", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Company                              | Industry          | Location                        |\\n|-------------------------------------|-------------------|---------------------------------|\\n| Qantas Airways                     | Transportation    | Brisbane, Australia             |\\n| RWE Utilities                       | Utilities         | Essen, Germany                  |\\n| Reliance Industries Limited         | Energy            | Mumbai, India                   |\\n| Renault Automobiles                 | Automobiles       | Boulogne-Billancourt, France    |\\n| Repsol Energy                       | Energy            | Madrid, Spain                   |\\n| Rio Tinto Group Metals & Mining    | Metals & Mining   | London, United Kingdom          |\\n| Rolls-Royce Industrials             | Industrials       | London, United Kingdom          |\\n| Rosneft Energy                      | Energy            | Moscow, Russia                  |\\n| Shell Energy                        | Energy            | London, United Kingdom          |\\n| SK Innovation Co                   | Energy            | Seoul, South Korea              |\\n| SSAB Metals & Mining                | Metals & Mining   | Stockholm, Sweden               |\\n| SSE Utilities                       | Utilities         | Perth, United Kingdom           |\\n| Saint-Gobain Construction Materials| Construction Materials | Paris, France      |\\n| Santos Energy                       | Energy            | Adelaide, Australia             |\\n| Sasol Chemicals                     | Chemicals         | Johannesburg, South Africa     |\\n| Saudi Aramco Energy                 | Energy            | Dhahran, Saudi Arabia           |\\n| Severstal Metals & Mining           | Metals & Mining   | Cherepovets, Russia             |\\n| Siemens Energy                      | Energy            | Munich, Germany                 |\\n| South32 Metals & Mining             | Metals & Mining   | Perth, Australia                 |\\n| Southern Company Utilities          | Utilities         | Atlanta, United States          |\\n| Equinor (formerly Statoil) Energy   | Energy            | Stavanger, Norway               |\\n| Stellantis Automobiles              | Automobiles       | Amsterdam, Netherlands          |\\n| Suncor Energy                       | Energy            | Calgary, Canada                 |\\n| Suzano (formerly Fibria Celulose)   | Paper & Forest Products | Salvador, Brazil     |\\n| Suzuki Automobiles                  | Automobiles       | Hamamatsu, Japan                |\\n| Teck Resources Limited              | Metals & Mining   | Vancouver, Canada               |\\n| AES Corporation                     | Utilities         | Arlington, United States        |\\n| thyssenkrupp Metals & Mining        | Metals & Mining   | Buisburg and Essen, Germany     |\\n| Toray Industries Inc.               | Chemicals         | Tokyo, Japan                    |\\n| TotalEnergies                        | Energy            | Paris, France                   |\\n| Toyota Motor Automobiles            | Automobiles       | Toyota City, Japan              |\\n| TC Energy                           | Energy            | Calgary, Canada                 |\\n| UltraTech Cement                    | Construction Materials | Mumbai, India       |\\n| Unilever Consumer Staples          | Consumer Staples   | London, United Kingdom          |\\n| Uniper Energy                       | Energy            | D\u00fcsseldorf, Germany              |\\n| United Airlines                     | Transportation    | Chicago, United States          |\\n| Raytheon Technologies Corporation   | Industrials       | Hartford, United States         |\\n| United Tractors                     | Industrials       | Jakarta, Indonesia              |\\n| Vale Metals & Mining                | Metals & Mining   | Rio de Janeiro, Brazil          |\\n| Valero Energy                       | Energy            | San Antonio, United States      |\\n| Vedanta Resources Metals & Mining   | Metals & Mining   | London, United Kingdom          |\\n| Vistra Corp                         | Utilities         | Irving, United States           |\\n| Volkswagen Group                    | Automobiles       | Wolfsburg, Germany              |\\n| Volvo Group                         | Automobiles       | Gothenburg, Sweden              |\\n| WEC Energy Group                    | Utilities         | Milwaukee, United States        |\\n| Woolworths Ltd                      | Consumer Staples   | Bella Vista, Australia          |\\n| Walmart Stores                      | Retailing         | Bentonville, United States      |\\n| Weyerhaeuser Company                | Paper & Forest Products | Seattle, United States     |\\n| Woodside Energy                     | Energy            | Perth, Australia                 |\\n| XCEL Energy                         | Utilities         | Minneapolis, United States      |\"}"}
{"id": "GF5l0F19Bt", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Query and stance distribution\\n\\n| Alignment with IPCC on climate action | train | valid | test |\\n|--------------------------------------|-------|-------|------|\\n|                                      |       |       |      |\\n| Strongly supp.                        | 409   | 54    | 161  |\\n| Supp.                                 | 387   | 38    | 85   |\\n| No or mixed pos.                      | 221   | 29    | 62   |\\n| Not supp.                             | 171   | 21    | 11   |\\n| Opposing                              | 12    | 0     | 0    |\\n\\n| Carbon tax                            |       |       |      |\\n|--------------------------------------|-------|-------|------|\\n|                                      |       |       |      |\\n|                                      | 53    | 9     | 5    |\\n|                                      | 159   | 21    | 28   |\\n|                                      | 84    | 16    | 19   |\\n|                                      | 115   | 7     | 17   |\\n|                                      | 86    | 6     | 1    |\\n\\n| Communication of climate science      |       |       |      |\\n|--------------------------------------|-------|-------|------|\\n|                                      |       |       |      |\\n|                                      | 180   | 29    | 57   |\\n|                                      | 206   | 21    | 71   |\\n|                                      | 19   | 1     | 2    |\\n|                                      | 24   | 5     | 1    |\\n|                                      | 17   | 1     | 0    |\\n\\n| Disclosure on relationships           |       |       |      |\\n|--------------------------------------|-------|-------|------|\\n|                                      |       |       |      |\\n|                                      | 28    | 0     | 2    |\\n|                                      | 74    | 7     | 9    |\\n|                                      | 63    | 7     | 13   |\\n|                                      | 136   | 16    | 23   |\\n|                                      | 23   | 1     | 2    |\\n\\n| Emissions trading                     |       |       |      |\\n|--------------------------------------|-------|-------|------|\\n|                                      |       |       |      |\\n|                                      | 169   | 22    | 10   |\\n|                                      | 210   | 26    | 27   |\\n|                                      | 130   | 10    | 25   |\\n|                                      | 182   | 18    | 35   |\\n|                                      | 47    | 6     | 9    |\\n\\n| Energy and resource efficiency        |       |       |      |\\n|--------------------------------------|-------|-------|------|\\n|                                      |       |       |      |\\n|                                      | 102   | 17    | 29   |\\n|                                      | 145   | 17    | 46   |\\n|                                      | 125   | 13    | 29   |\\n|                                      | 87    | 15    | 15   |\\n|                                      | 85    | 4     | 2    |\\n\\n| Energy transition & zero carbon technologies |       |       |      |\\n|-------------------------------|-------|-------|------|\\n|                                | 286   | 24    | 129  |\\n|                                | 1205  | 137   | 497  |\\n|                                | 622   | 78    | 353  |\\n|                                | 1035  | 91    | 343  |\\n|                                | 526   | 61    | 222  |\\n\\n| GHG emission regulation           |       |       |      |\\n|-----------------------------------|-------|-------|------|\\n|                                  | 267   | 29    | 40   |\\n|                                  | 220   | 22    | 67   |\\n|                                  | 221   | 25    | 82   |\\n|                                  | 217   | 25    | 36   |\\n|                                  | 157   | 9     | 20   |\\n\\n| Land use                          |       |       |      |\\n|-----------------------------------|-------|-------|------|\\n|                                  | 8     | 0     | 1    |\\n|                                  | 18    | 1     | 16   |\\n|                                  | 39    | 2     | 20   |\\n|                                  | 2     | 1     | 2    |\\n|                                  | 0     | 0     | 0    |\\n\\n| Renewable energy                  |       |       |      |\\n|-----------------------------------|-------|-------|------|\\n|                                  | 150   | 16    | 49   |\\n|                                  | 172   | 23    | 76   |\\n|                                  | 123   | 16    | 57   |\\n|                                  | 166   | 14    | 30   |\\n|                                  | 203   | 22    | 13   |\\n\\n| Support of UN climate process     |       |       |      |\\n|-----------------------------------|-------|-------|------|\\n|                                  | 165   | 16    | 9    |\\n|                                  | 433   | 60    | 143  |\\n|                                  | 55    | 4     | 10   |\\n|                                  | 18    | 6     | 0    |\\n|                                  | 6     | 0     | 0    |\\n\\n| Supporting the need for regulations |       |       |      |\\n|-------------------------------------|-------|-------|------|\\n|                                     | 85    | 7     | 23   |\\n|                                     | 317   | 33    | 114  |\\n|                                     | 229   | 24    | 94   |\\n|                                     | 197   | 22    | 45   |\\n|                                     | 6     | 0     | 2    |\\n\\n| Transparency on legislation        |       |       |      |\\n|------------------------------------|-------|-------|------|\\n|                                    | 13    | 3     | 3    |\\n|                                    | 65    | 4     | 12   |\\n|                                    | 58    | 4     | 8    |\\n|                                    | 99    | 12    | 17   |\\n|                                    | 27    | 1     | 7    |\\n\\nTable 6: Hyperparameters\\n\\n|                     | BERT-base | ClimateBERT | Longformer-base | Longformer-large | RoBERTa-base |\\n|---------------------|-----------|-------------|-----------------|------------------|--------------|\\n| Learning rate       | 1e-5      | 1e-5        | 1e-5            | 1e-5             | 1e-5         |\\n| Batch size          | 8         | 8           | 8               | 8                | 8            |\\n| Training steps      | 20000     | 20000       | 20000           | 20000            | 20000        |\\n| Warmup ratio        | 0.1       | 0.1         | 0.1             | 0.1              | 0.1          |\\n| Max sequence len. at training | 512    | 512         | 1532            | 1532             | 512          |\\n| Max sequence len. at inference    | 512     | 512         | 2048            | 2048             | 512          |\\n\\nA.9 Page Consideration\\n\\nAlthough \u201cevidence snippet\u201d is extracted by a human, it is sometimes not consistent in terms of granularity; it can be consists of phrases, sentences, or paragraphs. Thus, we thought training and evaluating models on the origin granularity is not always suitable. Moreover, extracting sentence or paragraph from the PDF file depends on PDF parsers. This means, if one uses different a parser, the output labels of the dataset will need to be reconstructed. Our dataset is considered to be used for benchmarking, so the compatibility is important. Therefore, we believe page-level evidence detection is a pragmatic approach.\\n\\nHowever, we have included information of sentence-level evidence alignments in the metadata so that researchers can refer to that.\\n\\nA.10 Implementation and Hyperparameters\\n\\nWe used PyTorch [31] and HuggingFace [44] libraries for the model implementation.\\n\\nFor the pipeline approach, we applied post-processing technique where evidence page indices are merged for output evidences that have the same query and stance labels.\\n\\nFor the linear model, we used scikit-learn [32] and its default hyperparameters.\\n\\nFor the overlap metric, we calculate the weighted true positive based on page overlap. Precision is defined as $\\\\frac{TP_{\\\\text{weighted}}}{TP_{\\\\text{weighted}} + FP}$. For example, given a gold tuple $((0,1), \\\\text{\\\"renewable_energy\\\"})$ and a predicted tuple $((1,2), \\\\text{\\\"renewable_energy\\\"})$, $TP_{\\\\text{weighted}} = 1$ since at least one page index overlaps between the gold and predicted tuples. $TP_{\\\\text{weighted}}$ becomes $\\\\frac{1}{2}$ because, among the gold page indices $(0,1)$, only one page index (i.e., 1) overlaps with the predicted indices.\"}"}
{"id": "GF5l0F19Bt", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Hyperparameters used in the supplementary task\\n\\n| Model          | Learning rate | Batch size | Training epochs | Warmup ratio | Max sequence len. at training | Max sequence len. at inference | Min sequence len. at inference | Beam size |\\n|----------------|---------------|------------|-----------------|--------------|-------------------------------|-------------------------------|--------------------------------|-----------|\\n| FlanT5-large   | 5e-4          | 32         | 10              | 0.1          | 1532                          | 2048                          | 150                            | 3         |\\n| FlanT5-XL      | 5e-4          | 32         | 10              | 0.1          | 1532                          | 2048                          | 150                            | 3         |\\n\\nTable 8: Evaluation results in validation F-score (%).\\n\\n| Model          | P Q S | P Q S | P Q S |\\n|----------------|-------|-------|-------|\\n| Most-frequent  | 44.8  | 37.5  | 35.4  |\\n| Linear         | 68.1  | 55.8  | 50.7  |\\n| BERT-base      | 70.6  | 58.7  | 51.7  |\\n| ClimateBERT    | 71.4  | 60.7  | 53.0  |\\n| RoBERTa-base   | 71.6  | 62.5  | 55.3  |\\n| Longformer-base| 73.4  | 62.4  | 58.3  |\\n| Longformer-large| 75.2  | 65.9  | 57.6  |\\n\\nQ, S represent query, stance and evidence page indices, respectively.\\n\\nA.11 Evaluation Results\\n\\nTable 8 shows the evaluation results for the validation set.\"}"}
{"id": "GF5l0F19Bt", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.12 Dataset Documentation\\n\\nWe follow the existing dataset sheets [16] to provide information of our dataset with the dataset consumers.\\n\\nA.12.1 Motivation\\n\\nFor what purpose was the dataset created?\\n\\nThis dataset is designed for the task of evidence-based assessment of corporate climate policy engagement. The task takes the text extracted from a PDF file, such as a corporate sustainability report, as input and outputs a set of (query, stance, evidence page indices) triplets. We anticipate that our dataset will stimulate research on NLP and climate change, steering it towards more accurate detection of greenwashing.\\n\\nWho created the dataset (for example, which team, research group) and on behalf of which entity (for example, company, institution, organization)?\\n\\nThe authors of this paper created the dataset. GM is a Visiting Scholar at Stanford University, and this work is supervised by CDM at the Stanford NLP Group.\\n\\nWho funded the creation of the dataset?\\n\\nThis research was done in the Stanford Data Science (SDS) Affiliates Program.\\n\\nA.12.2 Composition\\n\\nWhat do the instances that comprise the dataset represent (for example, documents, photos, people, countries)?\\n\\nThe dataset represents documents, e.g., corporate-related reports and screenshots of web media. The origin document could contain not only text but images.\\n\\nHow many instances are there in total (of each type, if appropriate)?\\n\\nSee Table 1 and Table 5.\\n\\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?\\n\\nMany samples are removed when the dataset construction system failed to parse PDF files or found any issues. All samples are retained except the those that have been technically removed.\\n\\nWhat data does each instance consist of?\\n\\nOur dataset mainly consists of text data extracted from a PDF file, evidences (i.e., (P, Q, S) triplets and its comment text.) The dataset also contains metadata (e.g., origin third party data and PDF parser name.) Also, we provide company information and origin PDF data.\\n\\nIs there a label or target associated with each instance?\\n\\nYes. See Section 4.\\n\\nIs any information missing from individual instances?\\n\\nN/A.\\n\\nAre relationships between individual instances made explicit (for example, users' movie ratings, social network links)?\\n\\nThe PDF file name is assumed as an ID in our dataset, and each instance can be resolved mainly by the PDF file names.\\n\\nAre there recommended data splits (for example, training, development/validation, testing)?\\n\\nWe recommend using our split described in the paper.\\n\\nAre there any errors, sources of noise, or redundancies in the dataset?\\n\\nYes, there are. See Appendix A.2.\\n\\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (for example, websites, tweets, other datasets)?\\n\\nThe dataset is self-contained for the tasks described in the paper; links for the origin web pages are included to clarify the source. a) We can not ensure the linked resources exist for a long time and consistent. b) There is no official archive related to the linked resource.\"}"}
{"id": "GF5l0F19Bt", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Does the dataset contain data that might be considered confidential (for example, data that is protected by legal privilege or by doctor\u2013patient confidentiality, data that includes the content of individuals' non-public communications)?\\nNo.\\n\\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?\\nNo.\\n\\nDoes the dataset identify any subpopulations (for example, by age, gender)?\\nNo.\\n\\nIs it possible to identify individuals (that is, one or more natural persons), either directly or indirectly (that is, in combination with other data) from the dataset?\\nNo.\\n\\nDoes the dataset contain data that might be considered sensitive in any way (for example, data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)?\\nNo.\\n\\nA.12.3 Collection process\\n\\nHow was the data associated with each instance acquired? Was the data directly observable (for example, raw text, movie ratings), reported by subjects (for example, survey responses), or indirectly inferred/derived from other data (for example, part-of-speech tags, model-based guesses for age or language)?\\nSee Section 4.\\n\\nWhat mechanisms or procedures were used to collect the data (for example, hardware apparatuses or sensors, manual human curation, software programs, software APIs)?\\nSee Section 4 and Appendix A.5.\\n\\nIf the dataset is a sample from a larger set, what was the sampling strategy (for example, deterministic, probabilistic with specific sampling probabilities)?\\nNo probabilistic sampling has been done. We removed items that could not be technically included in the dataset.\\n\\nWho was involved in the data collection process (for example, students, crowdworkers, contractors) and how were they compensated (for example, how much were crowdworkers paid)?\\nN/A.\\n\\nOver what timeframe was the data collected?\\nThe data collection was conducted during February to March 2023. Each evidence item has its own timeframe (i.e., creation date or update date).\\n\\nWere any ethical review processes conducted (for example, by an institutional review board)?\\nNo.\\n\\nDid you collect the data from the individuals in question directly, or obtain it via third parties or other sources (for example, websites)?\\nVia third party websites.\\n\\nWere the individuals in question notified about the data collection?\\nN/A.\\n\\nDid the individuals in question consent to the collection and use of their data?\\nN/A.\\n\\nIf consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses?\\nN/A.\\n\\nHas an analysis of the potential impact of the dataset and its use on data subjects (for example, a data protection impact analysis) been conducted?\\nN/A.\"}"}
{"id": "GF5l0F19Bt", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.12.4 Preprocessing/cleaning/labeling\\n\\nWas any preprocessing/cleaning/labeling of the data done (for example, discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?\\n\\nYes. See Section 4 and Appendix A.5.\\n\\nWas the \u201craw\u201d data saved in addition to the preprocessed/cleaned/labeled data (for example, to support unanticipated future uses)?\\n\\nYes, origin evidence item information can be found in the meta field in the dataset.\\n\\nIs the software that was used to preprocess/clean/label the data available?\\n\\nNot yet available.\\n\\nA.12.5 Uses\\n\\nHas the dataset been used for any tasks already?\\n\\nNo.\\n\\nIs there a repository that links to any or all papers or systems that use the dataset?\\n\\nSee Appendix A.4.\\n\\nWhat (other) tasks could the dataset be used for?\\n\\nAny form using the dataset can be considered, such as multi-modal climate policy detection task and evidence generation task.\\n\\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?\\n\\nSee Appendix A.1.\\n\\nAre there tasks for which the dataset should not be used?\\n\\nSee Appendix A.1.\\n\\nA.12.6 Distribution\\n\\nWill the dataset be distributed to third parties outside of the entity (for example, company, institution, organization) on behalf of which the dataset was created?\\n\\nYes.\\n\\nHow will the dataset be distributed (for example, tarball on website, API, GitHub)?\\n\\nSee Appendix A.4.\\n\\nWhen will the dataset be distributed?\\n\\nAfter this paper is accepted, as soon as possible.\\n\\nWill the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?\\n\\nSee Appendix A.1. Except for third-party works, the extent to which they are attributed to the authors is distributed under specific license that can be found under the dataset repository.\\n\\nHave any third parties imposed IP-based or other restrictions on the data associated with the instances?\\n\\nThe third party data is not for commercial use and is otherwise subject to the terms and conditions of the organization.\\n\\nDo any export controls or other regulatory restrictions apply to the dataset or to individual instances?\\n\\nNo.\\n\\nA.12.7 Maintenance\\n\\nWho will be supporting/hosting/maintaining the dataset?\\n\\nThe authors will be.\\n\\nHow can the owner/curator/manager of the dataset be contacted (for example, email address)?\\n\\nBy the email address.\\n\\nIs there an erratum?\\n\\nN/A.\"}"}
{"id": "GF5l0F19Bt", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Will the dataset be updated (for example, to correct labeling errors, add new instances, delete instances)?\\n\\nIt is possible that the dataset be updated on the website or codebase to correct errors or to delete instances upon requests.\\n\\nIf the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (for example, were the individuals in question told that their data would be retained for a fixed period of time and then deleted)?\\n\\nN/A.\\n\\nWill older versions of the dataset continue to be supported/hosted/maintained?\\n\\nIt depends on the nature of the dataset update. We may notify the dataset consumers via a website or codebase.\\n\\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?\\n\\nNo. We consider that the dataset is used only for our task. Any extended work should be a separate contribution with ours.\"}"}
