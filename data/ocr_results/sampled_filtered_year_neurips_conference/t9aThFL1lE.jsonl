{"id": "t9aThFL1lE", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"After Attacking\\n\\nAfter Unlearning\\n\\nBefore Unlearning\\n\\nTest Prompt:\\n\\n\\\"An image of frogs in Van Gogh style\\\"\\n\\n\\\"An image of rabbits in Comic Etch style\\\"\\n\\nFigure A12: Visualization of the images generated by the unlearned DMs using different unlearning methods in the absence or presence of adversarial prompts.\"}"}
{"id": "t9aThFL1lE", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Although UNLEAN CANVAS is originally designed for benchmarking MU methods, we would like to demonstrate its broader use cases of benchmarking more generative modeling tasks, thanks to its good properties discussed in Sec. 3. In this section, we start with a case study on the task of style transfer, which is a much more well-studied topic than MU, but surprisingly also faces great challenges in building up a comprehensive and precise evaluation framework. In the next, we will first dissect the key challenges of the current style transfer evaluation framework, and then demonstrate how UNLEAN CANVAS efficiently resolves these challenges and further proposes a comprehensive and automated benchmark. Through extensive experiments, we draw demonstrate new insights from these results and illuminate the challenges of the future research directions. In the end, we will discuss the possibility of using UNLEAN CANVAS to benchmark more generative modeling tasks.\\n\\nF.1 Benchmarking Style Transfer using UNLEAN CANVAS\\n\\nStyle transfer. Style transfer is a long-standing topic and focuses on transferring the artistic style from one style image (also known as the reference image) $x_s$ to a target content image $x_c$. The most recent methods typically employ a neural network, denoted by $\\\\theta_s$, to extract style features and perform stylization in a single inference step, expressed as $\\\\hat{x}_o = f_{\\\\theta_s}(x_s, x_c)$. Current state-of-the-art (SOTA) style transfer techniques exhibit remarkable generalization capabilities, successfully transferring styles not encountered during training and not requiring any further back-propagations. Figure 1 illustrates the pipeline of this task. Most existing literature [74\u201379] utilize WILIA [66] to provide different styles for training the stylization network $\\\\theta_s$. During the evaluation, the validation set of WILIA will serve as the style (reference) images, together with the content images from the COCO dataset [80], to form a test bed for style transfer and style learning methods. However, such an evaluation scheme has some inherent limitations, which will be detailed below.\\n\\nIssues and challenges in the evaluating methods for style transfer Although the task of style transfer has been widely studied, its evaluations are still based on very limited quantitative or even sorely qualitative assessments, potentially leading to incomplete and inaccurate assessments [81]. Upon examining the evaluation pipelines of over 10 state-of-the-art (SOTA) style transfer methods, three significant challenges are identified within the current widely accepted evaluation frameworks.\\n\\n\u2022 Challenge I (C1): The lack of the ground truth images for style similarity evaluation. Unlike other vision tasks like classification [82], detection [83], and segmentation [84], one of the key shortcomings of the current style transfer evaluation lies in the lack of the ground truth images $x_g$ for the given reference style image $x_s$ and the content image $x_c$. Consequently, existing evaluation metrics, such as the style loss $\\\\ell_{style}$ [85, 86], has to be calculated with the reference style image $x_s$ as the ground truth $x_g$, namely $\\\\ell_{style}(x_s, \\\\hat{x}_o)$, rather than directly using the ground truth $\\\\ell_{style}(x_o, x_g)$. Obviously, such an indirect evaluation may lead to inaccurate results due to the different contents held in $x_s$ and $\\\\hat{x}_o$. Existing work has demonstrated that such evaluation metrics can lead to very different conclusion from that of the user study [79]. Therefore, the creation of a supervised dataset with ground truth stylized images for every content image under each style is a timely remedy.\\n\\n\u2022 Challenge II (C2): The lack of algorithm stability evaluation against varied reference images $x_s$. The evaluation of algorithm stability in style transfer has often been neglected. This assessment requires consistent performance of a method on a content image $x_c$ across different style reference images $x_s$ representing the same target artistic style. Ideally, an algorithm should maintain uniform quality across various references within a style, avoiding significant performance variations. Current challenges in such evaluations stem from the lack of reference sets that exhibit high stylistic consistency within each style. Figure A1 showcases examples from the widely-used WILIA dataset. Despite sharing the same artistic label, images within a row show considerable divergence in visual appearance and style. Consequently, using these images as style references can lead to stable algorithms producing stylistically varied outputs, leading to misleading assessment results. Therefore, creating a dataset...\"}"}
{"id": "t9aThFL1lE", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"with high stylistic uniformity within each style category and clear differentiation between styles is crucial for accurate measurement of algorithm stability.\\n\\n**Method I**\\n\\n**Method II**\\n\\n**Content Image**\\n\\n**Style Image**\\n\\n(a) Style Authenticity (b) Content Integrity (c) Algorithm Stability\\n\\nFigure A14: An illustration of comprehensive performance metrics for style transfer tasks. (a) Style authenticity measures the style similarity between the generated image and the style image. (b) Content integrity measures the content preservation between the generated image and the content image. (c) Algorithm stability reflects the sensitivity of the algorithm to different style images. For all the metrics in the illustration, \u201cMethod II\u201d is always better than \u201cMethod I\u201d.\\n\\nBuilding up a comprehensive evaluation pipeline with U\\\\textsubscript{NLEARN}C\\\\textsubscript{ANVAS} for style transfer.\\n\\nTo revolutionize the evaluation framework with U\\\\textsubscript{NLEARN}C\\\\textsubscript{ANVAS}, it is crucial to first understand the components that form a comprehensive evaluation pipeline, ensuring a thorough and impartial assessment of performance. In the realm of style transfer, three pivotal metrics stand paramount, as depicted in Figure A14:\\n\\n1. **Style Authenticity**: Assesses the extent to which the style of the generated image aligns with that of the provided reference style image(s).\\n\\n2. **Content Integrity**: Measures how well the content features are preserved post style transfer.\\n\\n3. **Algorithm Stability**: Evaluates the algorithm\u2019s robustness against variations in content subjects, target styles, or style reference image selections, while keeping other parameters constant.\\n\\nIn addressing the challenges (C\\\\textsubscript{1}-C\\\\textsubscript{2}), U\\\\textsubscript{NLEARN}C\\\\textsubscript{ANVAS} proves to be inherently advantageous.\\n\\nFirst, the style-specific supervision embedded in U\\\\textsubscript{NLEARN}C\\\\textsubscript{ANVAS} enables the provision of ground truth for quantitative evaluations of stylized images. Second, the extensive image collection within the same style category in U\\\\textsubscript{NLEARN}C\\\\textsubscript{ANVAS} allows for the assessment of algorithm stability by applying style transfer methods to varied reference images. To encompass the aforementioned aspects of style transfer performance, we propose the following quantitative evaluation metrics:\\n\\n1. **Style Loss**: Utilizes a feature map-based style loss [87] to quantify the stylistic dissimilarity between image pairs, effectively representing the inverse of style authenticity.\\n\\n2. **Content Loss**: Employs a VGG-based, feature-map content loss [87, 88] to measure the visual dissimilarity between the reference and generated images, essentially mirroring content integrity.\\n\\n3. **Averaged Standard Deviation (STD)**: Computes the average STD of style and content loss w.r.t. the same reference image, reflecting algorithm stability.\\n\\nFurthermore, similar to the MU task, we also consider efficiency metrics for each method, including:\\n\\n4. **Average Time Consumption**: Measures the time required for performing style transfer.\\n\\n5. **Peak GPU Memory Consumption**: Records the maximum GPU memory usage during the style transfer process.\\n\\n6. **Model Storage Memory Consumption**: Assesses the memory requirement for storing the style transfer model.\\n\\nWith the introduction of evaluation metrics (\u2460-\u2465), we establish a comprehensive evaluation pipeline for style transfer. The process is as follows:\\n\\nFor each style transfer method, style transfer is executed within each object class. Specifically, for every style, images indexed from 1 to 18 serve as style reference images, while seed images corresponding to indices 19 and 20 are used as content images. Style and content loss are computed for each pair of style reference and content images, with the stylized images derived from the used seed content images serving as the ground truth for style loss. This results in a total of 60 \u00d7 20 \u00d7 18 \u00d7 2 experimental trials for each method. For a specific style and content image pair, 18 experiments are performed to calculate the Standard Deviation (STD) values for both style and content loss.\"}"}
{"id": "t9aThFL1lE", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"STD values are then averaged over all content images (amounting to \\\\(60 \\\\times 20 \\\\times 2\\\\) cases). The findings from these comprehensive evaluations are presented in Table A6.\\n\\nFollowing this evaluation pipeline, we scrutinized 9 prominent style transfer methods, including SANET\\\\[78\\\\], MCC\\\\[89\\\\], MAST\\\\[90\\\\], ARTLOW with its two variants (AF-A\\\\[79\\\\], IEC\\\\ONTRAST\\\\[91\\\\], C\\\\AST\\\\[88\\\\], STY\\\\T\\\\[87\\\\], and BLIP\\\\[92\\\\].\\n\\nTable A6: Performance overview of different style transfer methods evaluated with UNLEARN\\\\-VAS dataset. The performance are assessed from the perspectives of stylistic authenticity (style loss), content integrity (content loss), algorithm stability (standard deviations from different dimensions), and efficiency. For all the metrics, smaller values are always preferred for better performance. The best performance per each metric is highlighted in bold. The standard deviations are first calculated with respect to different styles, object classes, or tested content images and then averaged in order to depict the algorithm stability from different perspectives.\\n\\n| Method   | Style Loss  | Content Loss | Efficiency |\\n|----------|-------------|--------------|------------|\\n|          | Mean        | STD (Averaged over) | Mean | STD (Averaged over) | Time(s/image) | Memory(GB) | Storage(GB) |\\n| SANET    | 23.48       | 2.73         | 2.87       | 1.87         | 0.85         | 0.12       | 0.17        | 0.09        | 0.29       | 2.3       | 0.11M |\\n| MCC      | 17.92       | 4.59         | 4.82       | 2.14         | 0.96         | 0.14       | 0.21        | 0.07        | 0.38       | 5.4       | 0.10M |\\n| MAST     | 24.10       | 2.87         | 3.16       | 1.74         | 1.42         | 0.33       | 0.34        | 0.18        | 2.86       | 4.8       | 0.16AF-A\\\\[79\\\\] |\\n| SANET    | 20.78       | 2.96         | 3.13       | 1.65         | 1.09         | 0.11       | 0.19        | 0.05        | 0.53       | 6.3       | 0.08AF-WCT |\\n| IE-C\\\\ONTRAST | 21.27     | 3.01         | 3.32       | 2.05         | 1.08         | 0.29       | 0.31        | 0.15        | 0.05       | 3.8       | 0.11C |\\n| C\\\\AST    | 24.01       | 2.78         | 2.90       | 1.35         | 1.38         | 0.32       | 0.40        | 0.16        | 0.32       | 6.7       | 0.19S |\\n| STY\\\\T    | 19.75       | 3.04         | 3.30       | 1.91         | 0.62         | 0.10       | 0.12        | 0.04        | 0.58       | 3.9       | 0.21BLIP |\\n| BLIP     | 25.43       | 2.90         | 3.06       | 2.03         | 1.61         | 0.30       | 0.34        | 0.16        | 8.87       | 7.2       | 7.2 |\\n\\nExperiment results analysis.\\n\\nTab. A6 provides a systematic evaluation of the performance of various methods tested. From the analysis, we can derive several crucial insights:\\n\\nFirst, it is evident that no single method excels across all evaluation metrics. Notably, MCC demonstrates superior performance in maintaining stylistic authenticity, as indicated by a low style loss. Conversely, STY\\\\T\\\\ stands out in preserving content integrity, reflected by its minimal content loss.\\n\\nSecond, the assessment of standard deviation is indispensable for a comprehensive evaluation. The method with the optimal performance does not necessarily exhibit the greatest stability. This is particularly apparent in the style loss evaluation, where MCC, despite achieving the best result in terms of style loss, exhibits the least stability, denoted by the highest standard deviation.\\n\\nF.2 Other Possible Applications of UNLEARN\\\\-VAS\\n\\nIn the preceding section, we demonstrated the application of UNLEARN\\\\-VAS in refining evaluation metrics and frameworks for style transfer. Beyond this, we recognize the potential of UNLEARN\\\\-VAS in diverse domains. Here, we delve into two illustrative examples:\\n\\nBias mitigation.\\n\\nBias mitigation in DMs, which are now gaining popularity, can also benefit from UNLEARN\\\\-VAS. Its hierarchical and balanced architecture enables the deliberate introduction of artificial biases by selectively omitting data from specific groups. For instance, by predominantly excluding images from styles other than the 'Van Gogh Style' within the 'Dogs' class, DMs finetuned on this dataset will inherently exhibit a tendency to generate images of dogs in the Van Gogh style, particularly when the style is not explicitly specified in the prompt. This approach not only allows for the manipulation and quantification of biases but also paves the way for UNLEARN\\\\-VAS to become a standardized benchmark for bias mitigation, similar to the role of MU for DMs.\\n\\nVision in-context learning (V-ICL).\\n\\nV-ICL\\\\[93\u201396\\\\] is another domain where UNLEARN\\\\-VAS can be effectively applied. The field of V-ICL is in urgent need of robust, comprehensive methods for the fair assessment of existing models. In this context, the image pairs from UNLEARN\\\\-VAS are ideally suited for evaluating various tasks such as style transfer, image inpainting, and image segmentation, offering a rich resource for nuanced and quantitative analyses.\"}"}
{"id": "t9aThFL1lE", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This work helps improve the assessment and further promotes the advancement of MU (machine unlearning) methods for DMs (diffusion models), which are known to be effective in relieving or mitigating the various negative societal influences brought by the prevalent usage of DMs, which include but are not limited to the following aspects.\\n\\n\u2022 Avoiding Copyright Issues. There is an urgent need for the generative model providers to scrub the influence of certain data on an already-trained model. In January 2023, a notable lawsuit targeted two leading AI art generators, Stable Diffusion [1] and Midjourney [2], for alleged copyright infringement. Concurrently, incidents with the recently released Midjourney V6 [2] also highlighted a visual plagiarism issue on famous film scenes. These instances illuminate the broad copyright challenges inherent in the way of training data collection method of those foundation generative models\u2019 training datasets. MU methods can be used as an effective method to remove the influence of the private data and avoid unnecessary retraining.\\n\\n\u2022 Mitigating biases and stereotypes. Generative AI systems are known to have tendencies towards bias, stereotypes, and reductionism, when it comes to gender, race and national identities [17]. For example, a recent study on the images generated with Midjourney revealed, that images associated with higher-paying job titles featured people with lighter skin tones, and that results for most professional roles were male-dominated [16]. MU is known to be effective in eliminating biases rooted in the training data. Moreover, U2C2 offers a flexible framework to benchmark MU techniques against bias removal, allowing for the creation and quantitative control of biases across different object classes for comprehensive bias removal studies.\"}"}
{"id": "t9aThFL1lE", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nThe technological advancements in diffusion models (DMs) have demonstrated unprecedented capabilities in text-to-image generation and are widely used in diverse applications. However, they have also raised significant societal concerns, such as the generation of harmful content and copyright disputes. Machine unlearning (MU) has emerged as a promising solution, capable of removing undesired generative capabilities from DMs. However, existing MU evaluation systems present several key challenges that can result in incomplete and inaccurate assessments.\\n\\nTo address these issues, we propose UNLEARN-CANVAS, a comprehensive high-resolution stylized image dataset that facilitates the evaluation of the unlearning of artistic styles and associated objects. This dataset enables the establishment of a standardized, automated evaluation framework with 7 quantitative metrics assessing various aspects of the unlearning performance for DMs. Through extensive experiments, we benchmark 9 state-of-the-art MU methods for DMs, revealing novel insights into their strengths, weaknesses, and underlying mechanisms. Additionally, we explore challenging unlearning scenarios for DMs to evaluate worst-case performance against adversarial prompts, the unlearning of finer-scale concepts, and sequential unlearning. We hope that this study can pave the way for developing more effective, accurate, and robust DM unlearning methods, ensuring safer and more ethical applications of DMs in the future. The dataset, benchmark, and codes are publicly available at https://unlearn-canvas.netlify.app/.\\n\\n1 Introduction\\n\\nThe recent technological breakthroughs in text-to-image generation, driven by diffusion models (DMs), have shown an unprecedented capability to produce high-resolution, high-quality images across a diverse range of subjects [1\u201310]. These models have become widely accessible to the public and are applied across various sectors, including advertising, the creative arts [11], and forensic sketching [12]. One reason for DMs being able to generate a broad spectrum of content is their reliance on diverse internet-sourced data [13]. However, this inclusiveness comes with risks, such as harmful generation [14], copyright issues [15], and biases or stereotypes [16, 17].\\n\\nTo alleviate the negative social impacts associated with DMs, machine unlearning (MU) techniques are catching increasing attention and have been studied in the field of text-to-image generation via DMs [18\u201333]. MU for DMs, also referred to as DM unlearning [27], aims to prevent the model from generating images when conditioned on an undesired concept (typically specified by a text prompt to be forgotten). Therefore, the problem of DM unlearning is also known as concept erasing [18\u201326].\\n\\nBased on the types of unlearning requests, the targeted concept to be erased can be diverse, including not-safe-for-work (NSFW) prompts [14, 23], concrete object entities [18\u201333], and copyrighted information like artistic styles [23\u201326, 28\u201330, 32]. In Fig. 1 (a), we provide an illustration of DM\"}"}
{"id": "t9aThFL1lE", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: (a) An illustration of MU for DMs. (b) Overview of experiment settings and benchmark results. This benchmark focuses on three categories of quantitative metrics: the unlearning effectiveness (UA, Rob., FU, SU); the retainability of innocent knowledge (IRA, CRA, FR, SR); and the image generation quality (FID). Results are normalized to $0\\\\% \\\\sim 100\\\\%$ per metric. No single method excels across all metrics. See a summary of these metrics in Tab. A1 and more results in Sec. 4.\\n\\nUnlearning by demonstrating the removal of an undesired artistic style from DM-generated images. Additionally, we refer readers to Sec. 2 for more related work on DM unlearning.\\n\\nCompared to MU for DMs, the unlearning studies in computer vision have primarily focused on image classification models [34\u201348]. Given an image classification dataset, benchmarking unlearning performance for discriminative models is simpler, because MU typically involves either class-wise forgetting [49, 35] or data-wise forgetting [27, 40]. The former aims to eliminate the influence of a specific image class, while the latter removes the influence of specific data points from the entire training set. In contrast, DM unlearning is often applied to scenarios involving the removal of higher-level and more abstract concepts, rather than specific training data points, which can also be difficult to localize within the DM training set. The primary method for assessing the performance of DM unlearning is using the I2P (inappropriate image prompts) dataset [14]. This dataset mainly focuses on the safety assessment of unlearned DMs (i.e., DMs post-unlearning), designed to erase specific harmful concepts like 'nudity' and 'violence'. Although I2P specifies which concepts to unlearn, evaluating the unlearning effectiveness and generation retainability of unlearned DMs under I2P remains challenging. This difficulty arises primarily from the lack of ground-truth data or objective criteria to precisely define safety versus non-safety. Furthermore, I2P cannot evaluate the performance of DM unlearning in other scenarios, such as object unlearning [19\u201333] and style unlearning [23\u201330].\\n\\nTo enhance the assessment of machine unlearning in DMs and establish a standardized evaluation framework, we propose the development of a new benchmark dataset, referred to as UNLEARN CANVAS. Unlike I2P, UNLEARN CANVAS is designed to evaluate the unlearning of artistic painting styles along with associated image objects (see an illustration in Fig. 2). It also provides ground-truth data annotations to precisely define the criteria for the effectiveness of unlearning and the retained model utility post-unlearning (see a result overview in Fig. 1 (b)). We summarize our contributions below.\\n\\n\u2022 We conduct a systematic review of existing MU methods for DMs and identified three unresolved challenges in their evaluations. We further provide an in-depth analysis of why a standardized benchmark for DM unlearning evaluation is crucial.\"}"}
{"id": "t9aThFL1lE", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We propose UNLEARN-CANVAS, a large-scale, high-resolution stylized image dataset with diverse styles and objects, to address current challenges in DM unlearning evaluation. Its dual supervision of styles and objects ensures stylistic consistency, enabling a comprehensive, standardized evaluation approach to precisely characterize unlearning efficacy, generation utility preservation, and efficiency.\\n\\nWe benchmark 9 state-of-the-art DM unlearning methods using UNLEARN-CANVAS, covering standard assessments and examining more challenging scenarios, such as adversarial robustness, grouped object-style unlearning, and sequential unlearning. This comprehensive evaluation provides previously unknown insights into their strengths and weaknesses.\\n\\n2 Related Work, Problem Statement, and Open Challenges\\n\\nRelated work. MU (machine unlearning) was originally developed to mitigate the (potentially detrimental) influence of specific data points in a pretrained ML model, without necessitating a complete retraining of the model after removing these unlearning data [34\u201352]. The significance of MU has emerged with the purpose of data privacy protection, in response to regulations such as 'the right to be forgotten' [53]. However, it has rapidly gained recognition for its role in promoting security, safety, and trustworthiness of ML models. Examples include defense against backdoor attacks [38, 54], fairness enhancement [55, 56], and controllable federated learning [57, 58]. More recently, DM unlearning has proven crucial in text-to-image generation to prevent the production of harmful, private, or illegal content [23\u201332]. For example, DMs have demonstrated a susceptibility to generating NSFW (not-safe-for-work) images when conditioned on inappropriate text prompts (e.g., 'nudity' and 'violence') [14, 59, 60]. Similarly, MU has also found applications in preventing the generation of copyrighted information, such as the misuse of artistic painting styles [23, 28]. In brief, DM unlearning can be regarded as a model edit operation, aimed at preventing DMs from producing undesired or inappropriate images.\\n\\nAs unlearning approaches continue to evolve, the need for benchmarking their performance becomes increasingly critical. Standard datasets like CIFAR-10 [61] and ImageNet [62] have long been used to evaluate unlearning performance when forgetting a specific subset of data points [27, 40] or a targeted image class [49, 35]. Additionally, a recent benchmark specialized for unlearning in face recognition has been developed [63]. However, crafting effective benchmarks for generative models introduces additional challenges. In the language domain, recent initiatives have specifically aimed to evaluate the efficacy of MU for large language models (LLMs). Notable examples include TOFU [64], which explores the unlearning of fictitious elements, and WMDP [65], aimed at removing hazardous knowledge from LLMs. In contrast, to the best of our knowledge, there is currently no effective benchmark dataset designed for DM unlearning in text-to-image generation. The predominant dataset, I2P [14], targets the removal of harmful concepts for safe image generation. However, I2P only covers a specialized unlearning scenario and lacks precise evaluation criteria to support a comprehensive, quantitative, and precise assessment of unlearning performance. This gap inspires us to develop a new benchmark dataset for DM unlearning.\\n\\n2 Related Work, Problem Statement, and Open Challenges\\n\\nRelated work. MU (machine unlearning) was originally developed to mitigate the (potentially detrimental) influence of specific data points in a pretrained ML model, without necessitating a complete retraining of the model after removing these unlearning data [34\u201352]. The significance of MU has emerged with the purpose of data privacy protection, in response to regulations such as 'the right to be forgotten' [53]. However, it has rapidly gained recognition for its role in promoting security, safety, and trustworthiness of ML models. Examples include defense against backdoor attacks [38, 54], fairness enhancement [55, 56], and controllable federated learning [57, 58]. More recently, DM unlearning has proven crucial in text-to-image generation to prevent the production of harmful, private, or illegal content [23\u201332]. For example, DMs have demonstrated a susceptibility to generating NSFW (not-safe-for-work) images when conditioned on inappropriate text prompts (e.g., 'nudity' and 'violence') [14, 59, 60]. Similarly, MU has also found applications in preventing the generation of copyrighted information, such as the misuse of artistic painting styles [23, 28]. In brief, DM unlearning can be regarded as a model edit operation, aimed at preventing DMs from producing undesired or inappropriate images.\\n\\nAs unlearning approaches continue to evolve, the need for benchmarking their performance becomes increasingly critical. Standard datasets like CIFAR-10 [61] and ImageNet [62] have long been used to evaluate unlearning performance when forgetting a specific subset of data points [27, 40] or a targeted image class [49, 35]. Additionally, a recent benchmark specialized for unlearning in face recognition has been developed [63]. However, crafting effective benchmarks for generative models introduces additional challenges. In the language domain, recent initiatives have specifically aimed to evaluate the efficacy of MU for large language models (LLMs). Notable examples include TOFU [64], which explores the unlearning of fictitious elements, and WMDP [65], aimed at removing hazardous knowledge from LLMs. In contrast, to the best of our knowledge, there is currently no effective benchmark dataset designed for DM unlearning in text-to-image generation. The predominant dataset, I2P [14], targets the removal of harmful concepts for safe image generation. However, I2P only covers a specialized unlearning scenario and lacks precise evaluation criteria to support a comprehensive, quantitative, and precise assessment of unlearning performance. This gap inspires us to develop a new benchmark dataset for DM unlearning.\\n\\nTable 1: Overview of existing MU evaluations for DMs, which covers two unlearning scenarios: object unlearning and style unlearning. Each case is demonstrated by the number of targeted unlearning concepts (target #), qualitative evaluation (Qual.) via generated image visualization, and quantitative evaluation (Quant.) of unlearning effectiveness (UE) and retainability (RT).\\n\\n| Related Work | Object Unlearning | Style Unlearning |\\n|--------------|------------------|------------------|\\n| Target #     | Quant. | Qual. | Target # | Quant. | Qual. |\\n| ESD [23]     | \u2713     | \u2713     | 5       | \u2713     | \u2713     |\\n| CA [16]      | \u2713     | \u2713     | 4       | \u2713     | \u2713     |\\n| FMN [28]     | \u2713     | \u2713     | 7       | \u2713     | \u2713     |\\n| UCE [24]     | \u2713     | \u2713     | 7       | \u2713     | \u2713     |\\n| SA [29]      | \u2713     | \u2713     | 31      | \u2713     | \u2713     |\\n| SalUn [27]   | \u2713     | \u2713     | 11      | \u2713     | \u2713     |\\n| SPM [26]     | \u2713     | \u2713     | 24      | \u2713     | \u2713     |\\n| SEOT [30]    | \u2713     | \u2713     | 22      | \u2713     | \u2713     |\\n| EDiff [31]   | \u2713     | \u2713     | 2       | \u2713     | \u2713     |\\n| SHS [32]     | \u2713     | \u2713     | 1       | \u2713     | \u2713     |\\n|               |        |       |         |        |       |\"}"}
{"id": "t9aThFL1lE", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Motivation: Unresolved challenges in MU evaluation.\\n\\nWhen building a rigorous quantitative evaluation framework for DM unlearning, we identified three urgent challenges after carefully examining existing studies as shown in Tab. 1.\\n\\n(C1) The absence of a consensus on a diverse unlearning target test repository. As shown in Tab. 1, assessments of DM unlearning in various studies, in terms of both effectiveness (i.e., concept erasure performance) and retainability (i.e., preserved generation quality under non-forgotten concepts), typically use manually-selected unlearning targets from a limited pool and focus on object-centric evaluation. Precise evaluation for style unlearning and grouped object-style unlearning is lacking.\\n\\nAlthough the existing dataset WIKIART[66] features a collection of real-world artworks by various artists, it does not apply to object unlearning.\\n\\n(C2) The lack of a systematic study on 'retainability' of DMs post-unlearning. As highlighted in the 'retainability' column of Tab. 1, there is a notable deficiency in the quantitative assessment of retainability for DMs post-unlearning. Retainability is crucial for measuring the potential side effects of a MU method. For instance, when unlearning an artistic style, it is essential to assess the DM's ability to generate images in other styles and all objects. This makes the WIKIART dataset less suitable for benchmarking DM unlearning, as it lacks object labels needed to characterize the side effects of style unlearning on object recognition.\\n\\n(C3) The precision challenge in evaluating DM-generated images. Artistic styles are inherently complex to define and differentiate precisely, complicating the quantitative evaluation of forgetting and retaining performance in style unlearning. For example, the WIKIART dataset was recently used in[67] to train a classifier for detecting copyright infringement, merely achieving an accuracy of 72.80%. The difficulty in precisely recognizing artistic styles in WIKIART may further hamper the evaluation of DMs' post-unlearning generation. Tab. A3 illustrates the challenge of accurately recognizing the styles of images generated by DMs, even when finetuned on WIKIART. This is evidenced by the test-time classification accuracy using the style classifier, ViT-L/16[68], finetuned on WIKIART. As we can see, only about half of the images are correctly classified into their corresponding style classes, e.g., 56.7% accuracy for classification on generated images using finetuned stable diffusion (SD) v2.0[1], which is significantly lower than that on test images from the original WIKIART (85.4%).\\n\\nThe above challenges (C1)\u2013(C3) drive us to develop the UNLEARNCANVAS dataset as a solution for the systematic and comprehensive evaluation of DM unlearning in Sec. 3.\\n\\n3 Our Proposal: UNLEARNCANVAS Dataset\\n\\nConstruction of UNLEARNCANVAS. As motivated in Sec. 2, UNLEARNCANVAS is created for ease of MU evaluation in DMs. Its construction involves two main steps: seed image collection and subsequent image stylization; see Fig. 3 for a schematic overview. More information about the dataset is provided in Appx. A.\\n\\nStep 1: Seed Image Collection\\n\\nHuman...\\n\\nDogs...\\n\\nArchitecture...\\n\\n20 Object Classes\\n\\nSeed Images\\n\\nStylized Images (60 styles)\\n\\nHigh Stylistic Consistency\\n\\nSketch Crayon Fauvism Van Gogh\\n\\n...\\n\\n...\\n\\n...\\n\\nFigure 3: Illustration of curating UNLEARNCANVAS.\\n\\nStep 2: Stylization\\n\\nUkiyoe\\n\\nStep 1: Seed Image Collection\\n\\nHuman...\\n\\nDogs...\\n\\nArchitecture...\\n\\n20 Object Classes\\n\\nSeed Images\\n\\nStylized Images (60 styles)\\n\\nHigh Stylistic Consistency\\n\\nSketch Crayon Fauvism Van Gogh\\n\\n...\\n\\n...\\n\\n...\\n\\nFigure 3: Illustration of curating UNLEARNCANVAS.\\n\\nSeed image collection. UNLEARN-CANVAS includes images in 60 unique artistic styles across 20 distinct objects. The style categories are built upon a set of high-resolution seed images, given by real-world photos from Pexels[69]. There are 20 seed images collected for each of the 20 object classes. See step 1 in Fig. 3.\\n\\nImage stylization. After collecting the seed images, the next step is the stylization process. During this phase, each seed image is transformed into 60 predetermined artistic styles, ensuring high stylistic consistency while preserving the original content details. This stylization process is carried out using services provided by Fotor[11]. Once all seed images have been stylized in all 60 styles, the dataset is constructed with super-high-resolution images and organized in a hierarchical structure that balances both style and object categories. See step 2 in Fig. 3.\"}"}
{"id": "t9aThFL1lE", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Advantages of UNLEARN CANVAS.\\n\\nNext, we will elaborate on the advantages (A1)\u2013(A3) of UNLEARN CANVAS to address the MU evaluation challenges (C1)\u2013(C3) discussed in Sec. 2.\\n\\n(A1) Style-object dual supervision enables a rich unlearning target bank for comprehensive evaluation.\\n\\nTo address (C1), UNLEARN CANVAS offers a diverse range of unlearning targets by annotating each image with both style and object labels. We categorized the unlearning targets into two main domains: a style domain comprising 60 styles, and an object domain with 20 object classes. This setup allows us to investigate the impact of style unlearning on the fidelity of object generation and vice versa.\\n\\n(A2) Enabling both 'in-domain' and 'cross-domain' retainability analyses for MU evaluation.\\n\\nTo address (C2), UNLEARN CANVAS facilitates us to assess DMs' retainability post-unlearning from two perspectives: in-domain retainability, which evaluates the DM's generation performance within the same domain as the unlearning target, and cross-domain retainability, which assesses the model's generation ability under conditions from a different domain. For example, as depicted in Fig. 4, we can define the 'domain' as either 'artistic style' or 'object class'. If the unlearning target is the 'Van Gogh' style, then generating images in all styles except 'Van Gogh' within the same object class (e.g., 'Dog' in Fig. 4) represents the in-domain case. Conversely, cross-domain retainability refers to generating images of different object classes (e.g., 'Horses' in Fig. 4), corresponding to an object domain different from the unlearning target within the style domain.\\n\\n(Before Unlearning\\n\\nAfter Unlearning\\n\\nTest Prompt: A painting of a dog in Van Gogh Style.\\n\\n\u2713 and \u2717 indicate satisfactory and undesired results post unlearning.\\n\\n(A3) High stylistic consistency ensures precise style definitions and enables accurate quantitative evaluations.\\n\\nIn UNLEARN CANVAS, images labeled with the same style exhibit high intra-style consistency and distinct inter-style differences. This facilitates the distinction between different styles, effectively addressing challenge (C2). In a similar study to that presented in Tab. A3, we observe that the styles within UNLEARN CANVAS can be readily classified using ViT-Large [68], achieving an accuracy of 98% for images generated by DM finetuned on UNLEARN CANVAS. For detailed results, please refer to Tab. A4.\\n\\nThe above advantages (A1)\u2013(A3) make UNLEARN CANVAS uniquely suitable for assessing the performance of DM unlearning. For instance, UNLEARN CANVAS includes a greater number of high-resolution images (15M) compared to the existing dataset WIKIART (2M). Moreover, UNLEARN CANVAS surpasses WIKIART in terms of both intra-style coherence and inter-style distinctiveness. See Appx. C for more detailed comparisons.\\n\\nEvaluation pipeline via UNLEARN CANVAS.\\n\\nWe next introduce how UNLEARN CANVAS can be used to benchmark the performance of DM unlearning and the corresponding evaluation metrics. Fig. 5 illustrates the proposed evaluation pipeline with a designated unlearning target. It comprises four phases (I\u2013IV) to evaluate unlearning effectiveness, retainability, generation quality, and efficiency.\\n\\nPhase I: Testbed preparation (Fig. 5 (a)). Prior to unlearning, we commence by finetuning a specific DM, given by SD (StableDiffusion) v1.5, on UNLEARN CANVAS for text-to-image generation. We also train a vision transformer (ViT-Large) [68] for the purposes of style and object classification after unlearning. We ensure that both the SD model and classifiers attain high performance in image generation and classification across all styles and objects. See detailed testbed preparation in Appx. B.\\n\\nPhase II: Machine unlearning (Fig. 5 (b)). Next, we utilize a given MU method to update the DM acquired in Phase I, aiming to eliminate a designated unlearning target (e.g., Van Gogh style). This results in the unlearned DM (denoted as $\\\\theta_u$), which avoids generating images in response to Van Gogh style-specific prompts.\\n\\nPhase III: Answer set generation (Fig. 5 (c)). Further, we utilize the unlearned model $\\\\theta_u$ to generate a set of images conditioned on both the unlearning-related prompts and other benign prompts. To comprehensively evaluate unlearning performance, three types of answer sets are generated.\\n\\n(a) To assess unlearning effectiveness, images are generated in response to prompts that involve the unlearning target (e.g., any prompt like 'An image in Van Gogh style' for the case in Fig. 5).\\n\\n(b) To assess retainability, images are generated in response to prompts that involve other benign prompts in the same domain as the unlearning target (e.g., 'An image in Van Gogh style' for the case in Fig. 5).\\n\\n(c) To assess cross-domain retainability, images are generated in response to prompts that involve other benign prompts in a different domain from the unlearning target (e.g., 'An image in Van Gogh style' for the case in Fig. 5).\"}"}
{"id": "t9aThFL1lE", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, our benchmarking results are divided into two main parts. In Sec. 4.1, we comprehensively evaluate the existing DM unlearning methods on the established tasks of style and object unlearning in a sequential unlearning fashion (Tab. A5). The settings for each method can be found in Appx. B.\\n\\nTo assess in-domain retainability (i.e., IRA), the answer set undergoes unlearning in a sequential unlearning fashion (Fig. 5 (d)). This is the classification accuracy of images generated by the unlearned DM using innocent prompts in different domains. In addition to the accuracy metrics, the performance when facing unlearning targets with finer granularity, formed by style-object concept combinations (Tab. 3). Further, we leverage UANVAS to measure cross-domain retainability (Tab. 2 & Fig. A2). We also show that preserving CRA except the one to be unlearned.\\n\\nExtensive studies following the proposed evaluation pipeline (Fig. 5) reveal that unlearning effectiveness and retainability) are quantitatively assessed (marked in blue) to accurately reflect the unlearning performance portrait. The unlearning target of the pipeline could traverse all the styles and objects to achieve a comprehensive evaluation.\\n\\nFigure 5: An illustration of the evaluation pipeline proposed in this work using UANVAS.\\n\\n**Table:**\\n\\n| Style/Object Classifiers | Cross-Domain Retainability |\\n|--------------------------|----------------------------|\\n| Van Gogh Classifier      | (a)                        |\\n| Classifier               | (b)                        |\\n\\nWe remark that SA [29], a recently proposed method, is excluded from our evaluation due to its excessive computational resource requirements and time consumption. Unless specified otherwise, SD v1.5 is the model used for unlearning. Detailed training settings for each method can be found in Appx. B.\\n\\nIn this work, we assess DM unlearning methods to be benchmarked. For instance, in the case of style unlearning as target (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods retainability is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA is not concurrently assessed (Tab. 2 & Fig. A2). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore"}
{"id": "t9aThFL1lE", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Performance overview of different DM unlearning methods evaluated on U\\\\textsuperscript{NEARN}C\\\\textsuperscript{ANVAS}.\\n\\n| Method        | Effectiveness | Time (s) | Memory (GB) | Storage (GB) |\\n|---------------|---------------|----------|--------------|--------------|\\n|               |               |          |              |              |\\n|               | Style         | Object   | FID          |\\n|               | Unlearning    | Unlearning|              |\\n|               | UA (\u2191)        | IRA (\u2191)  | CRA (\u2191)      |\\n| ESD [23]      | 98.58%        | 80.97%   | 93.96%       |\\n|               | 92.15%        | 55.78%   | 44.23%       |\\n|               | 65.55        | 6163     | 17.8         |\\n|               |              |          |              |\\n| FMN [28]      | 88.48%        | 56.77%   | 46.60%       |\\n|               | 45.64%        | 90.63%   | 73.46%       |\\n|               | 131.37       | 350      | 17.9         |\\n|               |              |          |              |\\n| UCE [24]      | 98.40%        | 60.22%   | 47.71%       |\\n|               | 94.31%        | 39.35%   | 34.67%       |\\n|               | 182.01       | 434      | 4.1         |\\n|               |              |          |              |\\n| CA [25]       | 60.82%        | 96.01%   | 92.70%       |\\n|               | 46.67%        | 90.11%   | 81.97%       |\\n|               | 54.21        | 734      | 10.1        |\\n|               |              |          |              |\\n| SalUn [27]    | 86.26%        | 90.39%   | 95.08%       |\\n|               | 86.91%        | 96.35%   | 99.59%       |\\n|               | 61.05        | 667      | 30.8        |\\n|               |              |          |              |\\n| SEOT [30]     | 56.90%        | 94.68%   | 84.31%       |\\n|               | 23.25%        | 95.57%   | 82.71%       |\\n|               | 62.38        | 95.74    | 0.0         |\\n|               |              |          |              |\\n| SPM [26]      | 60.94%        | 92.39%   | 84.33%       |\\n|               | 71.25%        | 90.79%   | 81.65%       |\\n|               | 59.79        | 29700    | 6.9         |\\n|               |              |          |              |\\n| EDiff [31]    | 92.42%        | 73.91%   | 98.93%       |\\n|               | 86.67%        | 94.03%   | 86.48%       |\\n|               | 48.48        | 81.42    | 27.8        |\\n|               |              |          |              |\\n| SHS [32]      | 95.84%        | 80.42%   | 43.27%       |\\n|               | 43.27%        | 80.73%   | 81.15%       |\\n|               | 119.34       | 1223     | 31.2        |\\n\\n4.1 Benchmarking Current DM Unlearning Methods for Style and Object Unlearning\\n\\nOverall performance of DM unlearning. In Tab. 2, we provide an overview of the performance of existing unlearning methods, using the evaluation metrics associated with U\\\\textsuperscript{NEARN}C\\\\textsuperscript{ANVAS}.\\n\\nFirst, as seen in Tab. 2, retainability is essential for a comprehensive assessment of DM unlearning. For example, in the scenario of style unlearning, ESD and UCE achieve similar UA (both around 98%), but their retainability (IRA and CRA) differs significantly, with one over 80% and the other around 60%. Therefore, relying solely on UA can provide a skewed view of the performance of DM unlearning. Second, we observe that CRA is harder to retain than IRA. For example, UCE exhibits a significant gap between its IRA and CRA, both in style (60.22% vs. 47.71%) and object (39.35% vs. 34.67%) unlearning scenarios. Similar patterns are observed with other methods like FMN, CA, SEOT, SPM, and SHS. This pattern suggests that while MU methods are somewhat effective at preserving concepts within the same domain, they face greater challenges in maintaining performance on unlearning target-unrelated concepts across domains. This issue has been overlooked in previous studies due to the absence of a multi-label dataset like U\\\\textsuperscript{NEARN}C\\\\textsuperscript{ANVAS} for systematic unlearning analyses. Third, we find that a single unlearning method can perform differently across various domains, and no single method excels in all aspects. For example, FMN achieves a UA of 88.48% in style unlearning but a much lower UA of 45.64% in object unlearning. A similar phenomenon can be observed with the unlearning methods SEOT, SPM, and SHS, which exhibit significantly large performance gaps between style and object unlearning. Furthermore, each method exhibits unique strengths and weaknesses. For example, while ESD typically shows improvement in UA, it may lag in terms of IRA and CRA, particularly in the case of object unlearning. Some methods, such as SEOT and SPM, achieve extremely high storage efficiency, requiring no additional storage, but suffer from inferior UA (below 80%). These observations underscore the challenges of DM unlearning and highlight the need for further advancements in the field.\\n\\nA closer look into retainability: A case study on ESD.\\n\\nWe next perform an in-depth analysis of DMs' retainability after unlearning, focusing on the most popular ESD method [23]. In Fig. 6 (left), we present a heatmap illustrating the per-style/object unlearning accuracy (diagonal) and retain accuracy (off-diagonal) for ESD. The heatmap segments are labeled to indicate different regions of interest (ROIs) in style and object unlearning related to our evaluation metrics: A1/A2 for UA, B1/B2 for IRA, and C1/C2 for CRA. Key observations are highlighted below.\\n\\nFirst, we observe distinct behaviors of ESD in unlearning styles compared to objects. For example, ROI C2 appears darker than C1, suggesting a reduced ability to retain styles during object unlearning. Similar patterns are seen between B2 and B1. For a clearer explanation, Fig. 6 (right) provides image generation examples of the DM before and after unlearning within different ROIs. Second, we observe that style/object unlearning is relatively easier compared to retaining the generation performance of unlearned DMs conditioned on unlearning-unrelated prompts. This is evident in the contrast between ROIs A1/A2 and ROIs B1/B2 or C1/C2. One exception is the case of unlearning the object 'Sea', which exhibits relatively low UA in A2. This may be attributed to post-unlearning image generation still resembling sea waves, as shown by the image examples in Fig. 6 (right) for\"}"}
{"id": "t9aThFL1lE", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Before MU | After MU\\n--- | ---\\nA1, A2: Unlearning Effectiveness | B1, B2, C1, C2: Retainability\\n\\nDesirable Case | Undesirable Case\\n--- | ---\\n\\nFigure 6: Left: Heatmap visualization of the unlearning accuracy and retainability of ESD on UNLEARN CANVAS. The x-axis shows the tested concepts for image generation using the unlearned model, while the y-axis indicates the unlearning target. Concept types are distinguished by color: styles in blue and objects in orange. The figure is divided into regions representing corresponding evaluation metrics and unlearning scopes (A for UA, B for IRA, C for CRA; '1' for style unlearning, '2' for object unlearning). Higher values in lighter colors denote better performance. The first row serves as a reference for comparison before unlearning. Zooming into the figure is recommended for detailed observation.\\n\\nRight: Representative cases illustrating each region with images generated before and after unlearning a specific concept.\\n\\nA2. Third, we observe an inherent trade-off between unlearning effectiveness and retainability. For example, while SalUn exhibits more consistent performance across different scenarios than ESD, it does not achieve the same level of UA, as indicated by the lighter diagonal values in Fig. A3. For the heatmap performance of other unlearning methods, please refer to Appx. D.\\n\\nUnderstanding unlearning method's behavior via unlearning directions. As noted earlier, different unlearning methods display distinct unlearning behaviors. To gain insights into the underlying reasons for these differences, Fig. 7 (a) and (b) visualize the 'unlearning directions' for ESD and SalUn.\\n\\nFigure 7: Visualization of the unlearning directions of (a) ESD and (b) SalUn. This figure illustrates the conceptual shift of the generated images of an unlearned model conditioned on the unlearning target. Images generated by the post-unlearning models are classified and used to understand this shift. Edges leading from the object in the left column to the right signify that images generated conditioned on unlearning targets are instead classified as the shifted concepts after unlearning. This reveals the primary unlearning direction for each unlearning method. The most dominant unlearning direction for an object is visualized. Figure (c) provides visualizations of generated images using the prompt template 'A painting of {object} in Sketch style.' with object being each unlearning target.\"}"}
{"id": "t9aThFL1lE", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SalUn, respectively. These unlearning directions are determined by connecting the unlearning target with the predicted label of the generated image from the unlearned DM conditioned on the unlearning target. As shown in Fig. 7 (a), ESD demonstrates a focused shift in image generation after object unlearning, with a predominant transition towards generating images labeled by 'Sea' and 'Trees'. This behavior arises from ESD's optimization process, designed to steer the generation of the DM away from a predefined concept. Consequently, images generated by the ESD-induced unlearned model consistently lack clearly identifiable objects, resembling waves and trees, which leads to their classification into the 'Sea' and 'Trees' classes; see Fig. 7 (c) for examples of generated images. In contrast, SalUn exhibits a more diverse range of unlearning directions, shifting images to 11 different objects. This diversity results from SalUn's requirement to replace the unlearning target with a random concept. As shown in Fig. 7 (c), images generated by SalUn post-object unlearning still maintain some object contours (different from the original unlearning target) and better retain style information compared to ESD.\\n\\n4.2 Benchmarking Current DM Unlearning Methods in More Challenging Scenarios\\n\\n| Method  | Object Unlearning | Style Unlearning |\\n|---------|-------------------|-----------------|\\n| ESD     | 50.0%             | 57.2%           |\\n| FMN     | 25.6%             | 45.6%           |\\n| UCE     | 46.7%             | 94.3%           |\\n| CA      | 60.8%             | 60.9%           |\\n| SalUn   | 86.9%             | 92.4%           |\\n| SEOT    | 23.2%             | 71.2%           |\\n| SPM     | 64.3%             | 95.8%           |\\n| EDiff   | 11.2%             | 99.0%           |\\n| SHS     | 29.8%             | 49.8%           |\\n\\nFigure 8: UA of DM unlearning against adversarial prompts [59]. Unlearned models in Tab. 2 are used as victim models to generate adversarial prompts. Recent studies [59, 60] have highlighted the vulnerabilities of DM unlearning to adversarial prompts, such as jailbreak attacks. In Fig. 8, we use the state-of-the-art attacking method, UnlearnDiffAtk [59], to craft adversarial prompts based on $U_{NLEARN}$ and evaluate the robustness of unlearned DMs shown in Tab. 2. See Appx. B.4 for detailed setup. As we can see, all the DM unlearning methods experience a significant drop in UA, falling below 60% when confronted with adversarial prompts. Notably, methods like UCE, SEOT, and SHS exhibit particularly steep declines. Moreover, a high UA under normal conditions does not necessarily imply robustness to adversarial attacks. For instance, although UCE achieves a UA over 90% in normal settings, its performance plummets to below 50% against adversarial prompts. This stark contrast highlights the importance of the worst-case evaluation for MU methods.\\n\\nTable 3: Performance of unlearning style-object combinations. The assessment includes UA and retainability in three contexts: SC (style consistency), OC (object consistency), and UP (unrelated prompting). The best result in each metric is highlighted in green.\\n\\n| Method  | UA  | SC  | OC  | UP  |\\n|---------|-----|-----|-----|-----|\\n| ESD     | 91.42% | 4.88% | 4.88% | 84.38% |\\n| FMN     | 45.37% | 68.73% | 62.74% | 83.25% |\\n| UCE     | 75.97% | 4.53% | 5.72% | 35.42% |\\n| CA      | 47.92% | 10.08% | 56.35% | 81.54% |\\n| SalUn   | 42.21% | 62.45% | 70.93% | 87.28% |\\n| SEOT    | 29.32% | 45.31% | 53.64% | 85.45% |\\n| SPM     | 45.72% | 41.34% | 36.32% | 67.82% |\\n| EDiff   | 71.33% | 35.23% | 26.32% | 51.52% |\\n| SHS     | 55.32% | 14.34% | 24.32% | 83.95% |\\n\\nUnlearning at a finer scale: Style-object combinations as unlearning targets. Next, we consider the fine granularity of the unlearning target, defined by a style-object combination, such as \\\"An image of dogs in Van Gogh style\\\". This unlearning challenge requires the unlearned model to avoid affecting image generation for dogs in non-Van Gogh styles and Van Gogh-style images with non-dog objects. See Appx. B.5 for detailed setup. Here, besides UA, the retainability performance is assessed in three contexts. (a) Style consistency (SC): retainability under prompts with different objects but in the same style, e.g., \\\"An image of cats in Van Gogh style\\\". (b) Object consistency (OC): retainability under prompts featuring the same object in different styles, e.g., \\\"An image of dogs in Picasso style\\\". (c) Unrelated prompting (UP): retainability for all other prompts. Tab. 2 presents the performance of unlearning style-object combinations. Style-object combinations are more challenging to unlearn than individual objects or styles, as evidenced by a significant drop in UA\u2014over 20% lower compared to values in Tab. 2. Retainability drops to below 20% for top-performing methods like ESD and UCE, originally highlighted for their efficacy. This is presumably due to the increased difficulty in targeting individual objects within a more complex visual context.\"}"}
{"id": "t9aThFL1lE", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"due to ESD's underlying unlearning mechanism, which requires only a single prompt, resulting in a poor ability to precisely define the unlearning scope. Evaluation in sequential unlearning. Furthermore, we evaluate the unlearning performance in sequential unlearning (SU) [70, 71], where unlearning requests arrive sequentially. This parallels the continual learning (CL) task, which requires models to not only unlearn new targets effectively but also maintain the unlearning of previous targets while retaining all other knowledge. Here, we consider unlearning 6 styles sequentially and the results are presented in Tab. A5. We remark that the method SEOT does not support sequential unlearning in its original implementation and thus is not included in Tab. A5. Our findings reveal significant insights. (1) Degraded retainability: Sequential unlearning requests generally degrade retainability across all methods, with RA values frequently dropping below the average levels previously seen in Tab. 2. Here RA is given by the average of IRA and CRA. (2) Unlearning rebound effect: Knowledge previously unlearned can be inadvertently reactivated by new unlearning requests. This is evidenced by decreasing UA values for earlier objectives as more unlearning tasks are introduced. This suggests that residual knowledge remains within the model and can be reactivated, aligning with findings from Fig. 8. This indicates the unlearned models by some MU methods do not essentially lose the generation ability of the unlearning target. (3) Catastrophic retaining failure: RA significantly drops at a certain request, exemplified by a sudden decrease in RA of UCE from 81.42% to 29.38% after the second request, T_2. This indicates that the seemingly acceptable side effects generated by some unlearning methods will drastically modify the knowledge representations when accumulated. This experiment illuminates the complex dynamics of knowledge removal and retention within DMs and highlights the potential pitfalls of existing unlearning methods when faced with sequential unlearning tasks. The observation of the 'unlearning rebound effect' and 'catastrophic retaining failure' particularly emphasizes the need for a more nuanced understanding of how knowledge is managed within DMs. Visualizations. We provide plenty of visualizations, illustrations, and qualitative results. These visualizations are intended to deepen the understanding of the effects of different MU methods and clearly illustrate the challenges identified in previous sections. Specifically, in Fig. A11, we provide abundant generation examples of all the 9 methods benchmarked in this work in a case study of unlearning the 'Cartoon' style. Both the successful and failure cases are demonstrated in the context of unlearning effectiveness, in-domain retainability, and cross-domain retainability. Besides, in Fig. A12, we provide visualizations for the effect of adversarial prompts enabled by UnlearnDiffAtk [59].\\n\\n5 Conclusion, Discussion, and Limitation\\nIn this paper, we systematically reviewed existing MU (machine unlearning) methods on DMs (diffusion models) and identified key challenges in their evaluation systems that could lead to incomplete, inaccurate, and biased assessments. In response, we propose \\\\textsc{UNLEARNCANNAS}, a high-resolution stylized image dataset designed to facilitate comprehensive evaluation of MU methods. We also introduce novel systematic evaluation metrics. By benchmarking nine state-of-the-art MU methods, we reveal novel insights into their strengths and weaknesses and also deepen the understanding of their underlying mechanisms. Our analysis of three challenging unlearning tasks highlights significant shortcomings of current methods, including a lack of robustness, difficulties in fine-scale unlearning, and issues arising from sequential unlearning tasks. These findings also point to meaningful future research directions aimed at developing satisfactory and practical MU methods for real-world applications. Furthermore, we recognize limitations in our benchmark, such as its focus on specific Stable Diffusion models and the text-to-image task in DMs. We hope this work serves as a foundation for future research to broaden MU evaluations across a wider range of model architectures and tasks in DMs.\\n\\nAcknowledgement\\nThe work of Y. Zhang, C. Fan, Y. Zhang, Y. Yao, J. Jia, J. Liu, and S. Liu was supported by the Cisco Research Award, the ARO Award W911NF2310343, the National Science Foundation (NSF) CISE Core Program Award IIS-2207052, and the NSF CAREER Award IIS-2338068, and the Amazon Research Award for AI in Information Security.\"}"}
{"id": "t9aThFL1lE", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, \u201cHigh-resolution image synthesis with latent diffusion models,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 10 684\u201310 695.\\n\\n[2] \u201cMidjourney,\u201d https://www.midjourney.com/, 2023, accessed: 2024-01-19.\\n\\n[3] M. Kang, J.-Y. Zhu, R. Zhang, J. Park, E. Shechtman, S. Paris, and T. Park, \u201cScaling up gans for text-to-image synthesis,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 10 124\u201310 134.\\n\\n[4] M. Tao, B.-K. Bao, H. Tang, and C. Xu, \u201cGalip: Generative adversarial clips for text-to-image synthesis,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 14 214\u201314 223.\\n\\n[5] M. Tao, H. Tang, F. Wu, X.-Y. Jing, B.-K. Bao, and C. Xu, \u201cDf-gan: A simple and effective baseline for text-to-image synthesis,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 16 515\u201316 525.\\n\\n[6] B. Li, X. Qi, T. Lukasiewicz, and P. Torr, \u201cControllable text-to-image generation,\u201d Advances in Neural Information Processing Systems, vol. 32, 2019.\\n\\n[7] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee, \u201cGenerative adversarial text to image synthesis,\u201d in International conference on machine learning. PMLR, 2016, pp. 1060\u20131069.\\n\\n[8] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, \u201cLearning to prompt for vision-language models,\u201d International Journal of Computer Vision, vol. 130, no. 9, pp. 2337\u20132348, 2022.\\n\\n[9] B. Kawar, S. Zada, O. Lang, O. Tov, H. Chang, T. Dekel, I. Mosseri, and M. Irani, \u201cImagic: Text-based real image editing with diffusion models,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2023, pp. 6007\u20136017.\\n\\n[10] M. Zhang, Z. Cai, L. Pan, F. Hong, X. Guo, L. Yang, and Z. Liu, \u201cMotiondiffuse: Text-driven human motion generation with diffusion model,\u201d arXiv preprint arXiv:2208.15001, 2022.\\n\\n[11] \u201cFotor: Online photo editor for everyone.\u201d https://www.fotor.com/, 2023.\\n\\n[12] \u201cForensic sketch artist at lablab\u2019s openai event,\u201d 2023.\\n\\n[13] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman et al., \u201cLaion-5b: An open large-scale dataset for training next generation image-text models,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 25 278\u201325 294, 2022.\\n\\n[14] P. Schramowski, M. Brack, B. Deiseroth, and K. Kersting, \u201cSafe latent diffusion: Mitigating inappropriate degeneration in diffusion models,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 22 522\u201322 531.\\n\\n[15] I. Krietzberg, \u201cUsers of midjourney text-to-image site claim issues with new update,\u201d The Street, 2023.\\n\\n[16] Y. Vinker, A. Voynov, D. Cohen-Or, and A. Shamir, \u201cConcept decomposition for visual exploration and inspiration,\u201d ACM Transactions on Graphics (TOG), vol. 42, no. 6, pp. 1\u201313, 2023.\\n\\n[17] V. Turk, \u201cHow ai reduces the world to stereotypes,\u201d Rest of World, 2023. [Online]. Available: https://restofworld.org/2023/ai-image-stereotypes/?utm_source=pocket-newtab-en-us\\n\\n[18] Y.-L. Tsai, C.-Y. Hsu, C. Xie, C.-H. Lin, J.-Y. Chen, B. Li, P.-Y. Chen, C.-M. Yu, and C.-Y. Huang, \u201cRing-a-bell! how reliable are concept removal methods for diffusion models?\u201d arXiv preprint arXiv:2310.10012, 2023.\"}"}
{"id": "t9aThFL1lE", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"P. Dong, S. Guo, J. Wang, B. Wang, J. Zhang, and Z. Liu, \u201cTowards test-time refusals via concept negation,\u201d Advances in Neural Information Processing Systems, vol. 36, 2024.\\n\\nS. Hong, J. Lee, and S. S. Woo, \u201cAll but one: Surgical concept erasing with model preservation in text-to-image diffusion models,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 19, 2024, pp. 21 143\u201321 151.\\n\\nM. Zhao, L. Zhang, T. Zheng, Y. Kong, and B. Yin, \u201cSeparable multi-concept erasure from diffusion models,\u201d arXiv preprint arXiv:2402.05947, 2024.\\n\\nC.-P. Huang, K.-P. Chang, C.-T. Tsai, Y.-H. Lai, and Y.-C. F. Wang, \u201cReceler: Reliable concept erasing of text-to-image diffusion models via lightweight erasers,\u201d arXiv preprint arXiv:2311.17717, 2023.\\n\\nR. Gandikota, J. Materzynska, J. Fiotto-Kaufman, and D. Bau, \u201cErasing concepts from diffusion models,\u201d arXiv preprint arXiv:2303.07345, 2023.\\n\\nR. Gandikota, H. Orgad, Y. Belinkov, J. Materzy\u0144ska, and D. Bau, \u201cUnified concept editing in diffusion models,\u201d arXiv preprint arXiv:2308.14761, 2023.\\n\\nN. Kumari, B. Zhang, S.-Y. Wang, E. Shechtman, R. Zhang, and J.-Y. Zhu, \u201cAblating concepts in text-to-image diffusion models,\u201d arXiv preprint arXiv:2303.13516, 2023.\\n\\nM. Lyu, Y. Yang, H. Hong, H. Chen, X. Jin, Y. He, H. Xue, J. Han, and G. Ding, \u201cOne-dimensional adapter to rule them all: Concepts, diffusion models and erasing applications,\u201d arXiv preprint arXiv:2312.16145, 2023.\\n\\nC. Fan, J. Liu, Y. Zhang, D. Wei, E. Wong, and S. Liu, \u201cSalun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation,\u201d arXiv preprint arXiv:2310.12508, 2023.\\n\\nE. Zhang, K. Wang, X. Xu, Z. Wang, and H. Shi, \u201cForget-me-not: Learning to forget in text-to-image diffusion models,\u201d arXiv preprint arXiv:2303.17591, 2023.\\n\\nA. Heng and H. Soh, \u201cSelective amnesia: A continual learning approach to forgetting in deep generative models,\u201d arXiv preprint arXiv:2305.10120, 2023.\\n\\nS. Li, J. van de Weijer, T. Hu, F. S. Khan, Q. Hou, Y. Wang, and J. Yang, \u201cGet what you want, not what you don\u2019t: Image content suppression for text-to-image diffusion models,\u201d arXiv preprint arXiv:2402.05375, 2024.\\n\\nJ. Wu, T. Le, M. Hayat, and M. Harandi, \u201cErasediff: Erasing data influence in diffusion models,\u201d arXiv preprint arXiv:2401.05779, 2024.\\n\\nJ. Wu and M. Harandi, \u201cScissorhands: Scrub data influence via connection sensitivity in networks,\u201d arXiv preprint arXiv:2401.06187, 2024.\\n\\nY. Zhang, X. Chen, J. Jia, Y. Zhang, C. Fan, J. Liu, M. Hong, K. Ding, and S. Liu, \u201cDefensive unlearning with adversarial training for robust concept erasure in diffusion models,\u201d arXiv preprint arXiv:2405.15234, 2024.\\n\\nY. Cao and J. Yang, \u201cTowards making systems forget with machine unlearning,\u201d in 2015 IEEE symposium on security and privacy. IEEE, 2015, pp. 463\u2013480.\\n\\nA. Golatkar, A. Achille, and S. Soatto, \u201cEternal sunshine of the spotless net: Selective forgetting in deep networks,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 9304\u20139312.\\n\\nA. Becker and T. Liebig, \u201cEvaluating machine unlearning via epistemic uncertainty,\u201d arXiv preprint arXiv:2208.10836, 2022.\\n\\nA. Thudi, G. Deza, V. Chandrasekaran, and N. Papernot, \u201cUnrolling sgd: Understanding factors influencing machine unlearning,\u201d in 2022 IEEE 7th European Symposium on Security and Privacy (EuroS&P). IEEE, 2022, pp. 303\u2013319.\"}"}
{"id": "t9aThFL1lE", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"J. Jia, J. Liu, P. Ram, Y. Yao, G. Liu, Y. Liu, P. Sharma, and S. Liu, \\\"Model sparsity can simplify machine unlearning,\\\" in Annual Conference on Neural Information Processing Systems, 2023.\\n\\nM. Chen, W. Gao, G. Liu, K. Peng, and C. Wang, \\\"Boundary unlearning: Rapid forgetting of deep networks via shifting the decision boundary,\\\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 7766\u20137775.\\n\\nA. Warnecke, L. Pirch, C. Wressnegger, and K. Rieck, \\\"Machine unlearning of features and labels,\\\" arXiv preprint arXiv:2108.11577, 2021.\\n\\nC. Fan, J. Liu, A. Hero, and S. Liu, \\\"Challenging forgets: Unveiling the worst-case forget sets in machine unlearning,\\\" arXiv preprint arXiv:2403.07362, 2024.\\n\\nA. Ginart, M. Guan, G. Valiant, and J. Y. Zou, \\\"Making AI forget you: Data deletion in machine learning,\\\" Advances in neural information processing systems, vol. 32, 2019.\\n\\nS. Neel, A. Roth, and S. Sharifi-Malvajerdi, \\\"Descent-to-delete: Gradient-based methods for machine unlearning,\\\" in Algorithmic Learning Theory. PMLR, 2021, pp. 931\u2013962.\\n\\nE. Ullah, T. Mai, A. Rao, R. A. Rossi, and R. Arora, \\\"Machine unlearning via algorithmic stability,\\\" in Conference on Learning Theory. PMLR, 2021, pp. 4126\u20134142.\\n\\nA. Sekhari, J. Acharya, G. Kamath, and A. T. Suresh, \\\"Remember what you want to forget: Algorithms for machine unlearning,\\\" Advances in Neural Information Processing Systems, vol. 34, pp. 18 075\u201318 086, 2021.\\n\\nL. Bourtoule, V. Chandrasekaran, C. A. Choquette-Choo, H. Jia, A. Travers, B. Zhang, D. Lie, and N. Papernot, \\\"Machine unlearning,\\\" in 2021 IEEE Symposium on Security and Privacy (SP). IEEE, 2021, pp. 141\u2013159.\\n\\nT. T. Nguyen, T. T. Huynh, P. L. Nguyen, A. W.-C. Liew, H. Yin, and Q. V. H. Nguyen, \\\"A survey of machine unlearning,\\\" arXiv preprint arXiv:2209.02299, 2022.\\n\\nZ. Izzo, M. A. Smart, K. Chaudhuri, and J. Zou, \\\"Approximate data deletion from machine learning models,\\\" in International Conference on Artificial Intelligence and Statistics. PMLR, 2021, pp. 2008\u20132016.\\n\\nL. Graves, V. Nagisetty, and V. Ganesh, \\\"Amnesiac machine learning,\\\" in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 13, 2021, pp. 11 516\u201311 524.\\n\\nC. Guo, T. Goldstein, A. Hannun, and L. Van Der Maaten, \\\"Certified data removal from machine learning models,\\\" arXiv preprint arXiv:1911.03030, 2019.\\n\\nH. Xu, T. Zhu, L. Zhang, W. Zhou, and P. S. Yu, \\\"Machine unlearning: A survey,\\\" ACM Computing Surveys, vol. 56, no. 1, pp. 1\u201336, 2023.\\n\\nJ. Jia, Y. Zhang, Y. Zhang, J. Liu, B. Runwal, J. Diffenderfer, B. Kailkhura, and S. Liu, \\\"Soul: Unlocking the power of second-order optimization for llm unlearning,\\\" arXiv preprint arXiv:2404.18239, 2024.\\n\\nC. J. Hoofnagle, B. van der Sloot, and F. Z. Borgesius, \\\"The European Union general data protection regulation: what it is and what it means,\\\" Information & Communications Technology Law, vol. 28, no. 1, pp. 65\u201398, 2019.\\n\\nY. Liu, M. Fan, C. Chen, X. Liu, Z. Ma, L. Wang, and J. Ma, \\\"Backdoor defense with machine unlearning,\\\" arXiv preprint arXiv:2201.09538, 2022.\\n\\nR. Chen, J. Yang, H. Xiong, J. Bai, T. Hu, J. Hao, Y. Feng, J. T. Zhou, J. Wu, and Z. Liu, \\\"Fast model debias with machine unlearning,\\\" arXiv preprint arXiv:2310.12560, 2023.\\n\\nA. Oesterling, J. Ma, F. P. Calmon, and H. Lakkaraju, \\\"Fair machine unlearning: Data removal while mitigating disparities,\\\" arXiv preprint arXiv:2307.14754, 2023.\\n\\nA. Halimi, S. Kadhe, A. Rawat, and N. Baracaldo, \\\"Federated unlearning: How to efficiently erase a client in fl?\\\" arXiv preprint arXiv:2207.05521, 2022.\"}"}
{"id": "t9aThFL1lE", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"R. Jin, M. Chen, Q. Zhang, and X. Li, \\\"Forgettable federated linear learning with certified data removal,\\\" arXiv preprint arXiv:2306.02216, 2023.\\n\\nY. Zhang, J. Jia, X. Chen, A. Chen, Y. Zhang, J. Liu, K. Ding, and S. Liu, \\\"To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now,\\\" arXiv preprint arXiv:2310.11868, 2023.\\n\\nZ.-Y. Chin, C.-M. Jiang, C.-C. Huang, P.-Y. Chen, and W.-C. Chiu, \\\"Prompting4debugging: Red-teaming text-to-image diffusion models by finding problematic prompts,\\\" arXiv preprint arXiv:2309.06135, 2023.\\n\\nA. Krizhevsky and G. Hinton, \\\"Learning multiple layers of features from tiny images,\\\" Master's thesis, Department of Computer Science, University of Toronto, 2009.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \\\"Imagenet: A large-scale hierarchical image database,\\\" in Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009, pp. 248\u2013255.\\n\\nD. Choi and D. Na, \\\"Towards machine unlearning benchmarks: Forgetting the personal identities in facial recognition systems,\\\" arXiv preprint arXiv:2311.02240, 2023.\\n\\nP. Maini, Z. Feng, A. Schwarzschild, Z. C. Lipton, and J. Z. Kolter, \\\"Tofu: A task of fictitious unlearning for llms,\\\" arXiv preprint arXiv:2401.06121, 2024.\\n\\nN. Li, A. Pan, A. Gopal, S. Yue, D. Berrios, A. Gatti, J. D. Li, A.-K. Dombrowski, S. Goel, L. Phan et al., \\\"The wmdp benchmark: Measuring and reducing malicious use with unlearning,\\\" arXiv preprint arXiv:2403.03218, 2024.\\n\\nF. Phillips and B. Mackintosh, \\\"Wiki art gallery, inc.: A case for critical thinking,\\\" Issues in Accounting Education, vol. 26, no. 3, pp. 593\u2013608, 2011.\\n\\nM. Moayeri, S. Basu, S. Balasubramanian, P. Kattakinda, A. Chengini, R. Brauneis, and S. Feizi, \\\"Rethinking artistic copyright infringements in the era of text-to-image generative models,\\\" arXiv preprint arXiv:2404.08030, 2024.\\n\\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., \\\"An image is worth 16x16 words: Transformers for image recognition at scale,\\\" arXiv preprint arXiv:2010.11929, 2020.\\n\\n\\\"Pexels: The best free stock photos, royalty free images and videos shared by creators.\\\" https://www.pexels.com/, 2023.\\n\\nJ. Jang, D. Yoon, S. Yang, S. Cha, M. Lee, L. Logeswaran, and M. Seo, \\\"Knowledge unlearning for mitigating privacy risks in language models,\\\" arXiv preprint arXiv:2210.01504, 2022.\\n\\nJ. Chen and D. Yang, \\\"Unlearn what you want to forget: Efficient unlearning for llms,\\\" arXiv preprint arXiv:2310.20150, 2023.\\n\\nT. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. D. Iii, and K. Crawford, \\\"Datasheets for datasets,\\\" Communications of the ACM, vol. 64, no. 12, pp. 86\u201392, 2021.\\n\\nK. Nichol, \\\"Painter by numbers,\\\" WikiArt, 2016, accessed: [insert date of access].\\n\\nS. Liu, T. Lin, D. He, F. Li, M. Wang, X. Li, Z. Sun, Q. Li, and E. Ding, \\\"Adaattn: Revisit attention mechanism in arbitrary neural style transfer,\\\" in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 6649\u20136658.\\n\\nX. Huang and S. Belongie, \\\"Arbitrary style transfer in real-time with adaptive instance normalization,\\\" in Proceedings of the IEEE international conference on computer vision, 2017, pp. 1501\u20131510.\\n\\nX. Li, S. Liu, J. Kautz, and M.-H. Yang, \\\"Learning linear transformations for fast image and video style transfer,\\\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 3809\u20133817.\"}"}
{"id": "t9aThFL1lE", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"T. Q. Chen and M. Schmidt, \u201cFast patch-based style transfer of arbitrary style,\u201d arXiv preprint arXiv:1612.04337, 2016.\\n\\nD. Y. Park and K. H. Lee, \u201cArbitrary style transfer with style-attentional networks,\u201d in proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 5880\u20135888.\\n\\nJ. An, S. Huang, Y. Song, D. Dou, W. Liu, and J. Luo, \u201cArtflow: Unbiased image style transfer via reversible neural flows,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 862\u2013871.\\n\\nT.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick, \u201cMicrosoft coco: Common objects in context,\u201d in European conference on computer vision. Springer, 2014, pp. 740\u2013755.\\n\\nQ. Cai, M. Ma, C. Wang, and H. Li, \u201cImage neural style transfer: A review,\u201d Computers and Electrical Engineering, vol. 108, p. 108723, 2023.\\n\\nA. Singh and P. Singh, \u201cImage classification: a survey,\u201d Journal of Informatics Electrical and Electronics Engineering (JIEEE), vol. 1, no. 2, pp. 1\u20139, 2020.\\n\\nZ. Zou, K. Chen, Z. Shi, Y. Guo, and J. Ye, \u201cObject detection in 20 years: A survey,\u201d Proceedings of the IEEE, 2023.\\n\\nS. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, and D. Terzopoulos, \u201cImage segmentation using deep learning: A survey,\u201d IEEE transactions on pattern analysis and machine intelligence, vol. 44, no. 7, pp. 3523\u20133542, 2021.\\n\\nL. Gatys, A. S. Ecker, and M. Bethge, \u201cTexture synthesis using convolutional neural networks,\u201d Advances in neural information processing systems, vol. 28, 2015.\\n\\nL. A. Gatys, A. S. Ecker, and M. Bethge, \u201cA neural algorithm of artistic style,\u201d arXiv preprint arXiv:1508.06576, 2015.\\n\\nY. Deng, F. Tang, W. Dong, C. Ma, X. Pan, L. Wang, and C. Xu, \u201cStytr2: Image style transfer with transformers,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 11 326\u201311 336.\\n\\nY. Zhang, F. Tang, W. Dong, H. Huang, C. Ma, T.-Y. Lee, and C. Xu, \u201cDomain enhanced arbitrary image style transfer via contrastive learning,\u201d in ACM SIGGRAPH 2022 Conference Proceedings, 2022, pp. 1\u20138.\\n\\nY. Deng, F. Tang, W. Dong, H. Huang, C. Ma, and C. Xu, \u201cArbitrary video style transfer via multi-channel correlation,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, 2021, pp. 1210\u20131217.\\n\\nJ. Huo, S. Jin, W. Li, J. Wu, Y.-K. Lai, Y. Shi, and Y. Gao, \u201cManifold alignment for semantically aligned style transfer,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 861\u201314 869.\\n\\nH. Chen, Z. Wang, H. Zhang, Z. Zuo, A. Li, W. Xing, D. Lu et al., \u201cArtistic style transfer with internal-external learning and contrastive learning,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 26 561\u201326 573, 2021.\\n\\nD. Li, J. Li, and S. C. Hoi, \u201cBlip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing,\u201d arXiv preprint arXiv:2305.14720, 2023.\\n\\nZ. Wang, Y. Jiang, Y. Lu, Y. Shen, P. He, W. Chen, Z. Wang, and M. Zhou, \u201cIn-context learning unlocked for diffusion models,\u201d arXiv preprint arXiv:2305.01115, 2023.\\n\\nX. Wang, W. Wang, Y. Cao, C. Shen, and T. Huang, \u201cImages speak in images: A generalist painter for in-context visual learning,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 6830\u20136839.\"}"}
{"id": "t9aThFL1lE", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Y. Bai, X. Geng, K. Mangalam, A. Bar, A. Yuille, T. Darrell, J. Malik, and A. A. Efros, \\\"Sequential modeling enables scalable learning for large vision models,\\\" arXiv preprint arXiv:2312.00785, 2023.\\n\\nQ. Wang, X. Bai, H. Wang, Z. Qin, and A. Chen, \\\"Instantid: Zero-shot identity-preserving generation in seconds,\\\" arXiv preprint arXiv:2401.07519, 2024.\\n\\nChecklist\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] The introduction (Sec. 1) clearly specifies the scope of this paper and the major contributions.\\n   (b) Did you describe the limitations of your work? [Yes] The limitations are discussed in Sec. 5.\\n   (c) Did you discuss any potential negative societal impacts of your work? [No] This work aims to improve the evaluation of machine unlearning technique, which will bring many positive societal impacts, such as mitigating copyright infringement, avoiding sexual or violent content generation, and mitigating the biases and stereotypes of the diffusion models. Given the technical focus of this work, there are no specific negative societal consequences directly stemming from it that need to be highlighted here.\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A] There are no theoretical results in this paper.\\n   (b) Did you include complete proofs of all theoretical results? [N/A] There are no theoretical results in this paper.\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] There are detailed instructions of the codes in the github repo to facilitate reproducing the results. The detailed experiment settings are also disclosed in Appx. B.\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Detailed training settings are disclosed in Sec. 4 and Appx. D.\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] The results reported in this work were averaged over dozens of individual unlearning cases, and the variance confirmed to be below 1%. We believe this indicates negligible variability. Given the large scale of the experiments, error bars were omitted to enhance readability and maintain focus on the key findings.\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] The compute resources are disclosed in Appx. B.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes] The seed images used to curate U\\\\textsc{Nlearn}Canvas belong to the existing assets, and their sources are properly cited in Sec. 3.\\n   (b) Did you mention the license of the assets? [Yes]\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] This paper includes a new dataset U\\\\textsc{Nlearn}Canvas and the link is released both in the abstract as well as in Appx. A.\\n   (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] The data being used is collected from an open-source photography website and this is mentioned in Sec. 3.\"}"}
{"id": "t9aThFL1lE", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] The data does not contain any personally identifiable information or offensive content, as discussed in Appx. A.\\n\\nIf you used crowdsourcing or conducted research with human subjects...\\n\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] No human subjects are involved.\\n\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] No human subjects are involved.\\n\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] No human subjects are involved.\"}"}
{"id": "t9aThFL1lE", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table of Contents\\n\\nA.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nA.2 Composition: Styles and Object Classes . . . . . . . . . . . . . . . . . . . . . . . 19\\nA.3 Collection and Labeling Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nA.4 Uses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nA.5 Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nA.6 Maintenance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nA.7 Author Statements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\nB Reproducibility Statement and Detailed Experiment Settings 20\\nB.1 Finetuning Style and Object Classifier with U NLEARN CANVAS . . . . . . . . 20\\nB.2 Finetuning Stable Diffusion with U NLEARN CANVAS . . . . . . . . . . . . . . . 20\\nB.3 Implementation of DM Unlearning Methods Studied in This Work . . . . . . . . . 20\\nB.4 Evaluation Details of the Adversarial Prompt Generation for Unlearning Robustness 21\\nB.5 Experiment Details of the Style-Object Combination Unlearning . . . . . . . . . . 22\\nB.6 Experiment Details of Sequential Unlearning . . . . . . . . . . . . . . . . . . . . . 23\\nB.7 Metrics Summary in All Unlearning Settings . . . . . . . . . . . . . . . . . . . . . 24\\nC A Detailed Comparison between U NLEARN CANVAS and WIKI ART 25\\nD Additional Experiment Results for Unlearning Evaluation 26\\nD.1 Visualization of the Style and Object Unlearning Performance . . . . . . . . . . . 26\\nD.2 A Fine-Grained Comparison per Unlearning Target: ESD vs. S ALUN . . . . . . . . 26\\nD.3 Unlearning Heatmaps of More Unlearning Methods . . . . . . . . . . . . . . . . . . 26\\nE Visualizations 29\\nE.1 Illustrations of the Styles and Objects in U NLEARN CANVAS . . . . . . . . . . . . 29\\nE.2 Visualization of Style Unlearning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\nE.3 Visualization of the Unlearning Performance in the Presence of Adversarial Prompts 30\\nF Broader Use Cases of U NLEARN CANVAS 34\\nF.1 Benchmarking Style Transfer using U NLEARN CANVAS . . . . . . . . . . . . . . . 34\\nF.2 Other Possible Applications of U NLEARN CANVAS . . . . . . . . . . . . . . . . . 36\\nG Impact Statement 37\"}"}
{"id": "t9aThFL1lE", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A U N L E A R N C A N V A S\\n\\nDataset Details\\n\\nIn this section, we provide a detailed description of the dataset following 'datasheet for datasets' [72].\\n\\nA.1 Motivation\\n\\nThis dataset is collected to enable a precise, comprehensive, and automated quantitative evaluation framework for MU (machine unlearning) methods in DMs (diffusion models). The current evaluation plans used in the existing literature have exposed several weaknesses, which may lead to incomplete, inaccurate or even biased results, see a more detailed discussion in Sec. 2. To the best of our knowledge, there are no datasets specifically designed to meet the assessing requirements of DM unlearning. Therefore, U N L E A R N C A N V A S is designed, collected, and made to fill in this gap.\\n\\nA.2 Composition: Styles and Object Classes\\n\\nThere are 60 predetermined artistic styles provided by Fotor [11]. The images in the same artistic style all share high stylistic consistency, which enables a high-precision style classifier to be trained on them. In Fig. A9, we list some examples of the images in each style to illustrate these styles.\\n\\nThere are 20 distinct object classes in U N L E A R N C A N V A S. In Fig. A10, we list some examples of the images in each object to illustrate these classes.\\n\\nA.3 Collection and Labeling Process\\n\\nThe construction of U N L E A R N C A N V A S involves two main steps: seed image collection and subsequent image stylization; see Fig. 3 for an illustration. For seed image collection, a set of high-resolution real-world photos are collected from Pexels [69], providing open-sourced photographs. There are 20 seed images collected for each of the 20 object classes; see Fig. A10. After collecting the seed images, we stylize each and every seed image into all 60 predetermined artistic styles; see Fig. A9 with Fotor [11]. After the stylization of all the images, the dataset is structured in a hierarchical manner, and each image is labeled with both its style and object classes. In order to support text-to-image training, each image is annotated with the prompt 'An image of object in style'.\\n\\nA.4 Uses\\n\\nU N L E A R N C A N V A S can be used to evaluate MU methods in different unlearning scenarios. Please see Sec. 4 for more details. In addition, we stress that U N L E A R N C A N V A S can be used for more real-world tasks than unlearning, and we provide an example of how U N L E A R N C A N V A S can be used to systematically evaluate another generative task, style transfer, in Appx. F. We provide very detailed instructions with codes in the GitHub code repository.\\n\\nA.5 Distribution\\n\\nU N L E A R N C A N V A S is an open-sourced dataset and is based on the existing open-sourced data [69]. We make access to the dataset public under the MIT license. We remark that no personally identifiable information or offensive content is included in this dataset. The dataset can be accessed either through Google Drive and HuggingFace. More resources on the dataset, such as the introduction video and the benchmark, can be found in the official project webpage.\\n\\nA.6 Maintenance\\n\\nThe dataset will be maintained by the lead author Yihua Zhang. If needed, the email for contacting is zhan1908@msu.edu. The dataset may be updated if needed (with the inclusion of more seed images, more artistic styles, and more objects). The updates will be ad-hoc and will not be periodical. Each time the dataset is updated, the updates will be reflected in the same GitHub code repository.\"}"}
{"id": "t9aThFL1lE", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.7 Author Statements\\n\\nThe collector and the lead author of this dataset, Yihua Zhang, bears full responsibility for any violation of rights that may arise from the collection of the data included in this research.\\n\\nB. Reproducibility Statement and Detailed Experiment Settings\\n\\nIn this section, we provide detailed instructions on the reproduction of our results in Sec. 4, including the settings of training, the implementation details of the tested machine unlearning methods, and the evaluation details in each unlearning scenario. We denote in Sec. 4, 50 out of the 60 styles proposed in UNLEARN CANVAS are studied and evaluated, with the other 10 styles being intentionally reserved for future explorations, such as generalizations.\\n\\nB.1 Finetuning Style and Object Classifier with UNLEARN CANVAS\\n\\nStyle and object classifiers need to be trained as part of the testbed proposed in our evaluation pipeline (Fig. 5). Here, we adopted a ViT-L/16 model [68] pretrained on ImageNet and finetune it on UNLEARN CANVAS. UNLEARN CANVAS are split into the train set and test set with a ratio of 9:1. After hyper-parameter tuning, the classifiers are trained with Adam optimizer at a learning rate of 0.01 for 10 epochs.\\n\\nB.2 Finetuning Stable Diffusion with UNLEARN CANVAS\\n\\nThe other part of the testbed is a diffusion model capable of generating high quality images in all the styles associated with all the objects encompassed in UNLEARN CANVAS in order to guarantee a trustworthy and unbiased evaluation.\\n\\nTraining settings.\\n\\nPractically, we finetune the pretrained Stable Diffusion (SD) v1.5 on UNLEARN CANVAS for 20k steps with a learning rate of $1 \\\\times 10^{-6}$. Unless otherwise stated, we strictly follow the training configurations used in Stable Diffusion [1]. For each image in UNLEARN CANVAS, we annotate the data with text prompt \\\"An image of \\\\{object\\\\} in \\\\{style\\\\}\\\", where the object and style are the corresponding object and style label. For seed images, the style label we use is 'photo' style. We use the training scripts provided by Diffuser official tutorial (https://huggingface.co/docs/diffusers/v0.13.0/en/training/text2image) and the pretraining model card is runwayml/stable-diffusion-v1-5. During training, the checkpoints will be saved every 1000 steps.\\n\\nEvaluation.\\n\\nTo evaluate the quality of the saved checkpoints and select the best one for unlearning study, the checkpoints are first used to generate an image set with the same prompt as training (\\\"An image of \\\\{object\\\\} in \\\\{style\\\\}\\\") by traversing all the possible style and object labels. Each prompt are used to generate 5 images with different random seed. Each image are sampled with 100 steps with a guidance coefficient of 9. The image set for each checkpoint are fed into the style and object classifier trained in Appx. B.1. The model with the highest average performance on all the styles and objects are selected as the testbed for MU study. The classification performance are also used as a reference for later IRA/CRA comparison, which are disclosed in the first row of Fig. 6 (left).\\n\\nComputing resource.\\n\\nIn this work, we employ $40 \\\\times$ NVIDIA RTX A6000 GPUs to conduct all the model training, unlearning, image generation, and evaluations. When we finetuned the StableDiffusion on UNLEARN CANVAS, $8 \\\\times$ GPUs were used for parallel computing. Other experiments were all carried out in a single-GPU environment. Around 60,000 GPU hours in total were spent to complete all the experiments.\\n\\nB.3 Implementation of DM Unlearning Methods Studied in This Work\\n\\nIn this work, we inspected a series of stateful MU methods for DMs. For each method, we use their publicly released source codes as code bases, which are listed below:\\n\\n- ESD [23]: https://github.com/rohitgandikota/erasing\"}"}
{"id": "t9aThFL1lE", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In particular, we adopt the following training settings to adapt the methods to our dataset:\\n\\n- **ESD**: Based on the suggestions from the authors, ESD is used to only finetune the cross-attention-related model weights (ESD-x). Other settings strictly follow the ones used in the paper.\\n\\n- **CA**: In order to ablate concepts using CA, we first use ChatGPT to generate a list of simple prompts for each concept (including the styles and the objects), namely anchor prompts. Each anchor prompt is a simple one-sentence description of the unlearning target.\\n\\n- **UCE**: This method requires a guided concept (prompt) for each unlearning concept. For style unlearning, we use the prompt `An image in {style*}` as the guided concept, where `style*` represents the next style in UNLEARN Canvas in alphabetical order. Similarly, `An image of object*` is used for object unlearning, where `object*` is the next object in UNLEARN Canvas in alphabetical order.\\n\\n- **FMN**: This method requires the images associated with the unlearning target. For simplicity and best performance, we randomly select 20 images associated with the unlearning concept. For the first stage of FMN, we run text inversion for 500 steps with a learning rate of $1 \\\\times 10^{-4}$, and for the second step, we used the inversed text to unlearn the cross attention layers of the model for 100 steps. The hyper-parameters of learning rate, maximum steps, and tunable parameters (cross-attention or non-cross-attention) are carefully tuned with grid search.\\n\\n- **SalUn**: This method involves two steps, the mask finding (weight saliency analysis) and the model unlearning. For mask finding, we tuned the mask ratio, while for unlearning, we tune the hyper-parameter learning rate and unlearning intensity. All the parameters are tuned with grid search. For both steps, the mask or model is trained with 10 epochs.\\n\\n- **SEOT**: To generate unlearned images, we use the prompt `An {object*} image in {style*}`. We then suppress either `object*` or `style*` individually. Other settings strictly follow the ones used in the paper.\\n\\n- **SPM**: Following the hyperparameters provided by the authors, we trained and obtained Pre-tuned SPMs for all `object*` and `style*`. During image generation, we combine the pre-tuned SPMs with the DM. By calculating the association between words in the prompt and the target word, we determine whether to allow the specified word to preserve, and generate the corresponding image.\\n\\n- **EDiff**: Based on the authors' suggestions, EDiff is used to finetune only the cross-attention-related model weights (EraseDiff-x). During the unlearning process, we adjusted the hyperparameters, specifically the learning rate and the number of unlearning epochs. The model is trained with 5 epochs.\\n\\n- **SHS**: SHS consists of two stages: trimming and repairing. During the trimming stage, certain weights are re-initialized. The repairing stage then restores the model's utility. Throughout the unlearning process, we finetuned the hyperparameters, focusing on the learning rate and the number of unlearning epochs. Ultimately, we selected 2 epochs as the optimal number.\\n\\n**B.4 Evaluation Details of the Adversarial Prompt Generation for Unlearning Robustness**\\n\\nIn Sec. 4, we evaluated the robustness of different MU methods against adversarial prompts. Here, we use the state-of-the-art method, UnlearnDiffAtk [59] to generate adversarial prompts. We set the prepended prompt perturbations by $N = 5$ tokens for both style and object unlearning. Following the\"}"}
{"id": "t9aThFL1lE", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"original attack setting in UnlearnDiffAtk \\\\cite{59}, to optimize the adversarial perturbations, we sample 50 diffusion time steps and perform PGD running for 40 iterations with a learning rate of 0.01 at each step. Prior to projection onto the discrete text space, we utilize the AdamW optimizer.\\n\\nB.5 Experiment Details of the Style-Object Combination Unlearning\\n\\nUnlearning targets.\\n\\nIn Sec. 4, we evaluate the capability of different MU methods on performing unlearning at a finer scale, and we use the style-object combinations as unlearning targets for evaluation. Ideally, the UNLEARNCANVAS dataset can generate 1200 ($60 \\\\times 20$) style-object combinations.\\n\\nIn this work, we randomly select 50 of these combinations for evaluation and we list these combinations below. For each method, the same hyper-parameters are used for each MU method as the ones for style and object unlearning in Tab. 2. The unlearning targets include:\\n\\n\u2022 'An image of Architectures in Abstractionism style.'\\n\u2022 'An image of Bears in Artist Sketch style.'\\n\u2022 'An image of Birds in Blossom Season style.'\\n\u2022 'An image of Butterfly in Bricks style.'\\n\u2022 'An image of Cats in Byzantine style.'\\n\u2022 'An image of Dogs in Cartoon style.'\\n\u2022 'An image of Fishes in Cold Warm style.'\\n\u2022 'An image of Flame in Color Fantasy style.'\\n\u2022 'An image of Flowers in Comic Etch style.'\\n\u2022 'An image of Frogs in Crayon style.'\\n\u2022 'An image of Horses in Cubism style.'\\n\u2022 'An image of Human in Dadaism style.'\\n\u2022 'An image of Jellyfish in Dapple style.'\\n\u2022 'An image of Rabbits in Defoliation style.'\\n\u2022 'An image of Sandwiches in Early Autumn style.'\\n\u2022 'An image of Sea in Expressionism style.'\\n\u2022 'An image of Statues in Fauvism style.'\\n\u2022 'An image of Towers in French style.'\\n\u2022 'An image of Trees in Glowing Sunset style.'\\n\u2022 'An image of Waterfalls in Gorgeous Love style.'\\n\u2022 'An image of Architectures in Greenfield style.'\\n\u2022 'An image of Bears in Impressionism style.'\\n\u2022 'An image of Birds in Ink Art style.'\\n\u2022 'An image of Butterfly in Joy style.'\\n\u2022 'An image of Cats in Liquid Dreams style.'\\n\u2022 'An image of Dogs in Magic Cube style.'\\n\u2022 'An image of Fishes in Meta Physics style.'\\n\u2022 'An image of Flame in Meteor Shower style.'\\n\u2022 'An image of Flowers in Monet style.'\\n\u2022 'An image of Frogs in Mosaic style.'\\n\u2022 'An image of Horses in Neon Lines style.'\\n\u2022 'An image of Human in On Fire style.'\\n\u2022 'An image of Jellyfish in Pastel style.'\\n\u2022 'An image of Rabbits in Pencil Drawing style.'\"}"}
{"id": "t9aThFL1lE", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An image of Sandwiches in Picasso style.\\n\\nAn image of Sea in Pop Art style.\\n\\nAn image of Statues in Red Blue Ink style.\\n\\nAn image of Towers in Rust style.\\n\\nAn image of Waterfalls in Sketch style.\\n\\nAn image of Architectures in Sponge Dabbed style.\\n\\nAn image of Bears in Structuralism style.\\n\\nAn image of Birds in Superstring style.\\n\\nAn image of Butterfly in Surrealism style.\\n\\nAn image of Cats in Ukiyoe style.\\n\\nAn image of Dogs in Van Gogh style.\\n\\nAn image of Fishes in Vibrant Flow style.\\n\\nAn image of Flame in Warm Love style.\\n\\nAn image of Flowers in Warm Smear style.\\n\\nAn image of Frogs in Watercolor style.\\n\\nAn image of Horses in Winter style.\\n\\nEvaluation.\\n\\nThe evaluation of the style-object combination unlearning concerns four quantitative metrics, one for unlearning effectiveness and three for retainability. Before the evaluation, an answer set will be generated exactly following the same procedure introduced in Sec. 3 and Fig. 5 after unlearning each target. First, the UA (unlearning accuracy) will be evaluated for each answer set, which stands for the ratio of images generated by the target prompt that are neither classified into the target object nor the target style class. A high UA denotes a better ability to successfully unlearn the target combination. Second, the retainability of generation associated with those prompts close to the unlearning target prompt will be evaluated. These prompts can be divided into two groups, the ones sharing the same style but not the object class and the ones sharing the object but not the style class. The classification accuracy of the former corresponds to the retainability of the style, i.e., style consistency (SC), while the latter one denotes the object consistency (OC). These two quantitative metrics evaluate how well the unlearning method precisely define the unlearning scope and retain the generation ability of those close but innocent concept. Thirdly, the retainability of the rest unrelated prompts (UP) are evaluated, which is the last quantitative evaluation metric. The results reported in Tab. 3 are averaged over all the unlearning cases shown above.\\n\\nB.6 Experiment Details of Sequential Unlearning\\n\\nIn Sec. 4, we also evaluated the MU methods with the task of sequential unlearning (SU), where the efficacy of MU methods in handling multiple sequential unlearning requests \\\\{T_i\\\\} are evaluated. This requires models not only to unlearn new targets effectively but also to maintain the unlearning of previous targets, while retaining all other knowledge. In the experiments, 6 styles are randomly selected as the unlearning targets and excluded from the RA evaluation. The UA of all the already unlearned target will be assessed each time a new request is accomplished. The selected 6 styles include:\\n\\n- Abstractionism\\n- Byzantine\\n- Cartoon\\n- Cold Warm\\n- Ukiyoe\\n- Van Gogh\\n\\n23\"}"}
{"id": "t9aThFL1lE", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"After each unlearning request, the unlearning effectiveness and retainability are evaluated. Specifically, the unlearning accuracy of all the unlearning targets in the previous requests are evaluated to evaluate how the unlearning effect lasts when new unlearning requests arrive. In the meantime, the retainability of all the other concepts that are not selected as unlearning targets are evaluated, and to ease the presentation, the retain accuracy of all the concepts (styles and objects) are averaged and reported.\\n\\nB.7 Metrics Summary in All Unlearning Settings\\n\\nBesides the UNLEARNCANVAS dataset, the various quantitative evaluation metrics proposed in this work are part of the major contributions to a comprehensive and precise evaluation for DM unlearning methods. As there are various unlearning scenarios studied in this work, we provide a summary of these metrics in Tab. A1, including their abbreviations, descriptions, and related tables or figures.\\n\\nTable A1: A summary of the quantitative metrics used in this work, including their abbreviations, meanings and where they are used.\\n\\n| Metrics                  | Description                                                                 | Usages (Table & Figure) |\\n|--------------------------|-----------------------------------------------------------------------------|--------------------------|\\n| Style/Object Unlearning  | UA: Unlearning accuracy                                                     | Fig. 1, Tab. 1           |\\n|                          | IRA: In-domain unlearning accuracy                                          | Fig. 1, Tab. 1           |\\n|                          | CRA: Cross-domain unlearning accuracy                                       | Fig. 1, Tab. 1           |\\n| Unlearning Robustness against Adversarial Prompts | Rob.: Unlearning robustness, unlearning accuracy in the presence of adversarial prompts | Fig. 1                   |\\n| Style-Object Combination | FU/UA: Unlearning accuracy in finer-scale unlearning                        | Fig. 1, Tab. 3           |\\n|                          | SC: Retainability evaluation of style consistency                           | Tab. 3                   |\\n|                          | OC: Retainability evaluation of object consistency                          | Tab. 3                   |\\n|                          | UP: Retainability evaluation of unrelated prompts                           | Tab. 3                   |\\n|                          | FR: Retainability evaluation in finer-scale unlearning, averaged by SC, OC, and UP | Fig. 1                   |\\n| Sequential or Continual Unlearning | SU or CU: Unlearning accuracy in the context of sequential unlearning | Fig. 1, Tab. A5          |\\n|                          | SR or CR: Retainability in the context of sequential unlearning            | Fig. 1, Tab. A5          |\"}"}
{"id": "t9aThFL1lE", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Detailed Comparison between UNLEARN CANVAS and WikiArt\\n\\nFigure A1: Image examples with the same style label from WikiArt and UNLEARN CANVAS.\\n\\nImages of the same artistic style in UNLEARN CANVAS exhibit high stylistic consistency compared to WikiArt.\\n\\nUNLEARN CANVAS vs. WikiArt\\n\\nTo the best of our knowledge, WikiArt is the most relevant baseline dataset to ours. In Tab. A2, we provide a direct comparison of the key attributes of these two datasets. UNLEARN CANVAS differs from WikiArt in the following aspects.\\n\\nFirst, UNLEARN CANVAS includes a greater number of high-resolution images (15 M) compared to WikiArt (2 M), a factor that may enhance the training of state-of-the-art DMs.\\n\\nTable A2: Comparison with WikiArt, the most relevant dataset containing stylized images to ours.\\n\\n| Dataset     | Resolution (Pixels/Image) | Style-wise Supervised | High Stylistic Consistency | Object-wise Supervised | Class-wise Balanced |\\n|-------------|---------------------------|-----------------------|----------------------------|------------------------|--------------------|\\n| WikiArt [73]| ~2 M                      | \u2717 \u2717                   | Style-wise \u2717               | Object-wise \u2717          |                    |\\n| UNLEARN CANVAS | ~15 M                   | \u2713 \u2713                    | Style-wise \u2713               | Object-wise \u2713          |                   |\\n\\nSecond, UNLEARN CANVAS surpasses WikiArt in terms of both intra-style coherence and inter-style distinctiveness, as illustrated in Fig. A1, where images labeled with \u2018Van Gogh Style\u2019 from both datasets are compared. In UNLEARN CANVAS, the images exhibit high stylistic consistency, while WikiArt lacks the necessary clarity for precise assessment. This will hamper the MU evaluation as discussed in the challenges (C2) and (C3). This benefit can also be reflected in the training performance using UNLEARN CANVAS and WikiArt. The results are reported in Tab. A3 and Tab. A4, respectively. As we can see, the classifier is much more easily trained on UNLEARN CANVAS, justifying the higher discernible features within each style in UNLEARN CANVAS.\\n\\nTable A3: Art style reproduction quality using SD v1.5 and SD v2.0 finetuned on WikiArt. Images are generated with the prompt \u201cA painting in artist style\u201d, where artist refers to those included in WikiArt. The test accuracy on DM-generated images and original WikiArt test images is reported using the style classifier finetuned from the pretrained ViT-L/16 on WikiArt.\\n\\n| Image Source | Images by SD v1.5 | Images by SD v2.0 |\\n|--------------|-------------------|-------------------|\\n| WikiArt Test Set | 41.2% | 56.7% |\\n|                | 85.4% |      |\\n\\nThird, the images in UNLEARN CANVAS are style-wise supervised. For each seed image, a stylized counterpart can be found in each style class. This is beneficial for tasks other than unlearning for text-to-image, such as image editing, image stylization, and style transfer, which can provide a ground truth image for precise and robust evaluation. This will be detailed in Appx. F.\"}"}
{"id": "t9aThFL1lE", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table A4: Style classification results of a ViT-Large [68] as a style classifier trained on UNLEARN-CANVAS. After convergence, the classifier is tested on the test set and the image set generated by SD v1.5 finetuned on UNLEARN-CANVAS.\\n\\n|           | Train Set | Test Set |\\n|-----------|-----------|----------|\\n| Images by SD v1.5 tuned on UNLEARN-CANVAS | | |\\n| Accuracy  | 100.0%  | 99.9%  | 98.8% |\\n\\nD Additional Experiment Results for Unlearning Evaluation\\n\\nD.1 Visualization of the Style and Object Unlearning Performance\\n\\nTo make a more direct comparison among different MU methods reported in Tab. 1, the results are visualized in the radar chart Fig. A2. This figure illustrates that no method dominates across all assessment dimensions. This underscores the complexity of unlearning in generative models and the need for further improvement.\\n\\n![Figure A2: Performance visualization for various unlearning methods as summarized in Table 2.](source)\\n\\nFor UA, IRA, and CRA, the results are averaged over the style and object unlearning scenarios. Other metrics undertake the inverse operation as a smaller values represent better performance. Results are normalized to $0\\\\% \\\\sim 100\\\\%$ per metric.\\n\\nD.2 A Fine-Grained Comparison per Unlearning Target: ESD vs. SalUN\\n\\nFollowing the analysis of ESD in Fig. 6, we next turn our focus to a comparative analysis with SalUN, a method that demonstrated a better balance between unlearning and retaining according to Tab. 2. A similar accuracy heatmap for SalUN is presented in Fig. A3. Compared to ESD, SalUN exhibits more consistent performance across various unlearning scenarios, as indicated by the more uniform color distribution in the heatmap. This also suggests enhanced retainability. However, it is noticeable that SalUN does not reach the same level of UA (Unlearning Accuracy) as ESD, as evidenced by the darker diagonal values in Fig. A3. This observation reinforces the existence of a trade-off between unlearning effectiveness and retainability in the visual generative MU task, a phenomenon paralleled in other tasks such as classification.\\n\\nD.3 Unlearning Heatmaps of More Unlearning Methods\\n\\nIn Fig. A4 \u223c Fig. A8, we provide more unlearning heatmap visualizations in the same format as Fig. 6 and Fig. A3 in order to provide a more detailed unlearning performance dissection for all the DM unlearning methods studied in this work.\"}"}
{"id": "t9aThFL1lE", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"**Abstractionism**\\n**Artist Sketch**\\n**Blossom Season**\\n**Bricks**\\n**Byzantine**\\n**Cartoon**\\n**Cold Warm**\\n**Color Fantasy**\\n**Comic Etch**\\n**Crayon**\\n**Cubism**\\n**Dadaism**\\n**Dapple**\\n**Defoliation**\\n**Early Autumn**\\n**Expressionism**\\n**Fauvism**\\n**French**\\n**Glowing Sunset**\\n**Gorgeous Love**\\n**Greenfield**\\n**Impressionism**\\n**Ink Art**\\n**Joy**\\n**Liquid Dreams**\\n**Magic Cube**\\n**Meta Physics**\\n**Meteor Shower**\\n**Monet**\\n**Mosaic**\\n**Neon Lines**\\n**On Fire**\\n**Pastel**\\n**Pencil Drawing**\\n**Picasso**\\n**Pop Art**\\n**Red Blue Ink**\\n**Rust**\\n**Sketch**\\n**Sponge Dabbed**\\n**Structuralism**\\n**Superstring**\\n**Surrealism**\\n**Ukiyoe**\\n**Van Gogh**\\n**Vibrant Flow**\\n**Warm Love**\\n**Warm Smear**\\n**Watercolor**\\n**Winter**\\n**Architectures**\\n**Bears**\\n**Birds**\\n**Butterfly**\\n**Cats**\\n**Dogs**\\n**Fishes**\\n**Flame**\\n**Flowers**\\n**Frogs**\\n**Horses**\\n**Human**\\n**Jellyfish**\\n**Rabbits**\\n**Sandwiches**\\n**Sea**\\n**Statues**\\n**Towers**\\n**Trees**\\n**Waterfalls**\\n\\n**Figure A3:** Heatmap visualization of SalUn. The plot setting is identical to Figure 6.\\n\\n**Figure A4:** Heatmap visualization of FMN. The plot setting is identical to Figure 6.\\n\\n**Figure A5:** Heatmap visualization of SEOT. The plot setting is identical to Figure 6.\"}"}
{"id": "t9aThFL1lE", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure A6: Heatmap visualization of SPM. The plot setting is identical to Figure 6.\\n\\nFigure A7: Heatmap visualization of Ediff. The plot setting is identical to Figure 6.\\n\\nFigure A8: Heatmap visualization of SHS. The plot setting is identical to Figure 6.\"}"}
{"id": "t9aThFL1lE", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table A5: Performance comparison of different DM unlearning methods in the sequential unlearning setting.\\n\\nEach column represents a new unlearning request, denoted by $T_i$, where $T_1$ is the oldest.\\n\\nEach row represents the UA for a specific unlearning objective or the retaining accuracy (RA), given by the average of IRA and CRA. Results indicating unlearning rebound effect are highlighted in orange, and those signifying catastrophic retaining failure are marked in red.\\n\\n| Method          | Metrics | $T_1$ | $T_1 \\\\sim T_2$ | $T_1 \\\\sim T_3$ | $T_1 \\\\sim T_4$ | $T_1 \\\\sim T_5$ | $T_1 \\\\sim T_6$ |\\n|-----------------|---------|-------|----------------|----------------|----------------|----------------|----------------|\\n| **Method: ESD** | UA      | 100%  | 99%            | 95%            | 87%            | 81%            | 75%            |\\n| Method: FMN     | UA      | 88%   | 99%            | 99%            | 98%            | 99%            | 99%            |\\n| **Method: UCE** | RA      | 77.46%| 52.94%         | 35.99%         | 24.86%         | 18.69%         | 12.95%         |\\n| Method: CA      | RA      | 82.39%| 14.56%         | 13.34%         | 10.42%         | 9.83%          | 8.76%          |\\n| **Method: SalUn**| UA      | 93%   | 95%            | 98%            | 96%            | 97%            | 98%            |\\n| Method: SPM     | UA      | 58%   | 55%            | 59%            | 45%            | 44%            | 40%            |\\n| **Method: EDiff**| RA      | 81.43%| 80.32%         | 71.42%         | 65.24%         | 63.24%         | 63.24%         |\\n| Method: SHS     | RA      | 72.39%| 70.42%         | 67.89%         | 60.45%         | 55.32%         | 51.12%         |\\n\\n#### E Visualizations\\n\\nIn this section, we aim to provide plenty of visualizations, illustrations, and qualitative results of the dataset and the quantitative results shown in Sec. 4 and Appx. D. These visualizations are intended to deepen the understanding of the effects of different MU methods and clearly illustrate the challenges identified in previous sections. We hope these visual aids will enable readers to more effectively grasp the nuances of MU methods and their implications for DMs (Diffusion Models).\\n\\n**E.1 Illustrations of the Styles and Objects in U\\\\textsc{NLEARN Canvas}**\\n\\nWe first provide an illustration of the styles and object classes included in U\\\\textsc{NLEARN Canvas}. Specifically, we show the styles in Fig. A9 and object classes in Fig. A10 with the style and object names disclosed in the captions.\"}"}
{"id": "t9aThFL1lE", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure A9: An illustration of the images in each style in UNLEARN CANVAS used in, which are stylized from the same seed image from the 'Dogs' object class. The seed image is presented in Fig. A10 (6). Images are cropped and down-scaled for illustration purpose. The name of the styles are: (1) Abstractionism; (2) Artist Sketch; (3) Blossom Season; (4) Blue Blooming; (5) Bricks; (6) Byzantine; (7) Cartoon; (8) Cold Warm; (9) Color Fantasy; (10) Comic Etch; (11) Crayon; (12) Crypto Punks; (13) Cubism; (14) Dadaism; (15) Dapple; (16) Defoliation; (17) Dreamweave; (18) Early Autumn; (19) Expressionism; (20) Fauvism; (21) Foliage Patchwork; (22) French; (23) Glowing Sunset; (24) Gorgeous Love; (25) Greenfield; (26) Impasto; (27) Impressionism; (28) Ink Art; (29) Joy; (30) Liquid Dreams; (31) Palette Knife; (32) Magic Cube; (33) Meta Physics; (34) Meteor Shower; (35) Monet; (36) Mosaic; (37) Neon Lines; (38) On Fire; (39) Pastel; (40) Pencil Drawing; (41) Picasso; (42) Pointillism; (43) Pop Art; (44) Rainwash; (45) Realistic Watercolor; (46) Red Blue Ink; (47) Rust; (48) Sketch; (49) Sponge Dabbed; (50) Structuralism; (51) Superstring; (52) Surrealism; (53) Techno; (54) Ukiyoe; (55) Van Gogh; (56) Vibrant Flow; (57) Warm Love; (58) Warm Smear; (59) Watercolor; (60) Winter.\\n\\nE.2 Visualization of Style Unlearning\\n\\nIn Fig. A11, we provide abundant generation examples of all the 9 methods benchmarked in this work in a case study of unlearning the 'Cartoon' style. Both the successful and failure cases are demonstrated in the context of unlearning effectiveness, in-domain retainability, and cross-domain retainability.\\n\\nE.3 Visualization of the Unlearning Performance in the Presence of Adversarial Prompts\\n\\nIn Fig. A12, we provide visualizations for the effect of adversarial prompts. As revealed in Fig. 8, all the DM unlearning methods experience a significant drop in unlearning effectiveness when attacked by the adversarial prompt, enabled by UnlearnDiffAtk [59]. We provide the image generation in four unlearning cases (two for style unlearning and two for object unlearning), and show the images of the unlearning target successfully generated in the presence of adversarial prompts.\"}"}
{"id": "t9aThFL1lE", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure A10: An illustration of the seed images in each object class in UNLEARN CANVAS. Images are cropped and down-scaled for illustration purpose. The name of the object classes are: (1) Architecture; (2) Bear; (3) Bird; (4) Butterfly; (5) Cat; (6) Dog; (7) Fish; (8) Flame; (9) Flowers; (10) Frog; (11) Horse; (12) Human; (13) Jellyfish; (14) Rabbits; (15) Sandwich; (16) Sea; (17) Statue; (18) Tower; (19) Tree; (20) Waterfalls.\"}"}
{"id": "t9aThFL1lE", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure A11: visualization of the unlearning performance of different methods on the task of style unlearning. Three text prompt templates are used to evaluate the unlearning effectiveness, in-domain retainability, and cross-domain retainability of each method. Images with green frame denote desirable results, while the ones with red frame denote unlearning or retaining failures.\"}"}
