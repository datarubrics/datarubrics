{"id": "iTUlYblV0K", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MQuAKE-Remastered: Multi-Hop Knowledge Editing Can Only Be Advanced With Reliable Evaluations\\n\\nShaochen (Henry) Zhong*, Yifan Lu*, Lize Shao\u2663, Bhargav Bhushanam\u221e, Xiaocong Du\u221e, Louis Feng\u221e, Yixin Wan\u2020, Yiwei Wang\u2020, Daochen Zha\u2663, Yucheng Shi\u2662, Ninghao Liu\u2662, Kaixiong Zhou\u2661, Shuai Xu\u2660, Vipin Chaudhary\u2660, and Xia Hu\u2663\\n\\nDepartment of Computer Science, Rice University\\n\u2662School of Computing, University of Georgia\\n\u2661Department of Electrical and Computer Engineering, North Carolina State University\\n\u2660Department of Computer and Data Sciences, Case Western Reserve University\\n\u2020Department of Computer Science, University of California, Los Angeles\\n\u221eMeta Platforms, Inc.\\n\\nAbstract\\n\\nLarge language models (LLMs) can give out erroneous answers to factually rooted questions either as a result of undesired training outcomes or simply because the world has moved on after a certain knowledge cutoff date. Under such scenarios, knowledge editing often comes to the rescue by delivering efficient patches for such erroneous answers without significantly altering the rests, where many editing methods have seen reasonable success when the editing targets are simple and direct (e.g., \u201cwhat club does Lionel Messi currently play for?\u201d). However, knowledge fragments like this are often deeply intertwined in the real world, making effectively propagating the editing effect to non-directly related questions a practical challenge (to entertain an extreme example: \u201cWhat car did the wife of the owner of the club that Messi currently plays for used to get to school in the 80s?\u201d). Prior arts have coined this task as multi-hop knowledge editing with the most popular dataset being MQUAKE, serving as the sole evaluation benchmark for many later proposed editing methods due to the expensive nature of making knowledge editing datasets at scale. In this work, we reveal that up to 33% or 76% of MQUAKE\u2019s questions and ground truth labels are, in fact, corrupted in various fashions due to some unintentional clerical or procedural oversights. Our work provides a detailed audit of MQUAKE\u2019s error pattern and a comprehensive fix without sacrificing its dataset capacity. Additionally, we benchmarked almost all proposed MQUAKE-remastered evaluated editing methods on our post-fix dataset, MQUAKE-Remastered. It is our observation that many methods try to overfit the original MQUAKE by exploiting some data-specific properties of MQUAKE. We provide a guideline on how to faithfully approach such datasets and show that a simple, minimally invasive approach can bring excellent editing performance without such exploitation. Please refer to https://github.com/henryzhongsc/MQuAKE-Remastered and supplemental material for assets.\\n\\n* Equal contribution. Work corresponds to Shaochen (Henry) Zhong<shaochen.zhong@rice.edu>.\\n\\nSubmitted to the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. Do not distribute.\"}"}
{"id": "iTUlYblV0K", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1 Introduction\\n\\nGiven the widespread public-facing popularity of various Large Language Model-powered (LLM) products [Zhao et al., 2023, Yang et al., 2024], even an occasional user has likely experienced LLMs giving out erroneous answers to factually rooted, knowledge-intensive questions. While the reasons why LLMs would hallucinate such kind of misinformation is complex and still an open problem \u2014 noisy training data, model bias, out-of-distribution questions, or even simply because the world has moved on after a certain knowledge cutoff date, all likely contributed their fair share to this rather undesired character of LLMs [Huang et al., 2023, Zhang et al., 2023]\u2014 under a practical context, knowledge editing is often considered the go-to remedy by delivering efficient patches for such erroneous answers without significantly altering the LLM's output on unrelated queries [Sinitsin et al., 2020, Mitchell et al., 2022].\\n\\nWith the growing need to have more credible and trustworthy LLMs, a vast amount of LLM-specific knowledge editing methods have been proposed, and many of them have seen reasonable success in addressing editing targets that are simple and direct. For example, most modern knowledge editing methods can reliably edit the answer of \u201cWhat club does Lionel Messi currently play for?\u201d from \u201cParis Saint-Germain\u201d to \u201cInter Miami CF\u201d and therefore correctly reflecting the occupation status of Messi [Zhong et al., 2023].  \\n\\n1.1 Multi-hop knowledge editing poses practical significance and non-trivial challenges. However, due to the intertwined nature of different knowledge fragments, a small change in one knowledge fragment can produce ripple-like effects on a vast amount of related questions [Zhong et al., 2023, Cohen et al., 2023]. It is often a non-trivial challenge to efficiently propagate the editing effect to non-directly related questions with proper precision and locality. E.g., for a \u2014 in this case intensionally extreme \u2014 question like \u201cWhat car did the wife of the owner of the club that Messi currently plays for used to get to school in the 80s?\u201d Many knowledge-edited LLMs can still struggle while being fully aware of Messi\u2019s abovementioned club transfer [Zhong et al., 2023].\\n\\nPrior arts have realized the practical significance of being able to edit such complex/non-direct questions upon a certain knowledge update, as different knowledge fragments are almost always deeply entangled with each other in the real world [Zhong et al., 2023, Cohen et al., 2023, Wei et al., 2024]. Meanwhile, exhausting all potential combinations of questions related to one or a few updated knowledge fragments is impractical, if not totally impossible: imagining editing an LLM for every possible question influenced by the abovementioned club transfer of Messi. Even if it is feasible, this poses high operational costs and comes with the intrinsic risks of editing a mass amount of targets; not to mention a repeated effort would be required should Messi ever opt to transfer again.\\n\\nIt is intuitive that a practical knowledge editing method should be able to produce correct answers to relevant factual questions with only a few updated knowledge fragments available. This task has been coined as multi-hop knowledge editing with the founding, largest, as well as the most popular dataset to date being MQUAKE by Zhong et al. [2023]; serving as the sole evaluation backbone due to the expensive nature of making counterfactual and temporal datasets at such a scale (>10,000 cases provided, more about the dataset statistics in Table 6). Note that such expansiveness is further multiplied given the abovementioned ripple effect of multi-hop question answering, as one knowledge update of a subquestion can potentially lead to multiple updated answers across a large number of cases.\\n\\n1.2 Unfortunately, MQUAKE is flawed due to unintentional clerical and procedural errors \u2014 we fixed/remade it and re-benchmarked almost all proposed multi-hop knowledge editing methods. While MQUAKE is the founding dataset of multi-hop knowledge editing tasks and very much brings life to this vital subject, through a comprehensive audit, we reveal that up to 33% or 76% of MQUAKE questions and ground truth labels are, in fact, corrupted in various fashions due to some unintentional clerical or procedural errors; which inevitably cast doubts on the effectiveness of developed methods (especially the ones that solely) evaluated on MQUAKE, and present as a\"}"}
{"id": "iTUlYblV0K", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We highlight that MQ_UAKE is an already massive yet constantly growing issue, as MQ_UAKE is one of the fastest-growing datasets in terms of adaptation yet, the task it is trying to tackle \u2014 building more reliable LLM \u2014 is without a doubt crucial aspect of NLP development. To pave the way for future advancement of multi-hop knowledge editing, we present our work with the following contributions:\\n\\n- A comprehensive audit of MQ_UAKE: We are the first to present a comprehensive audit of the existing errors within MQ_UAKE [Zhong et al., 2023], bringing awareness to the knowledge editing community regarding this popular dataset with significant task importance attached.\\n\\n- Fix/remake MQ_UAKE to MQ_UAKE-Remastered: We present the only available fix/remake that not only patches all discovered errors, and done so without sacrificing the intended intensity and capacity of the original MQ_UAKE whenever possible.\\n\\n- Extensively re-benchmark of almost all existing multi-hop knowledge editing methods: Given the currently existing reports based upon the original MQ_UAKE are flawed reflections of such proposed methods' capability, we additionally re-benchmark almost all existing multi-hop knowledge editing methods that are available against our MQ_UAKE-RMASTERED datasets.\\n\\n- Guidance for future multi-hop knowledge editing development. Upon our extensive re-benchmark results, we observe that many proposed multi-hop knowledge editing methods intentionally or unintentionally overfit the original MQ_UAKE dataset by applying data-specific operations that are largely unique to the MQ_UAKE dataset family. We provide guidance on how to faithfully approach these datasets and additionally show that a simple, minimally invasive approach with no such operations can also achieve excellent editing performance.\\n\\n2 Preliminary\\n\\n2.1 Background of MQ_UAKE\\n\\nMQ_UAKE (Multi-hop Question Answering for Knowledge Editing) is a knowledge editing dataset focusing on the abovementioned multi-hop question answering tasks proposed in Zhong et al. [2023], where every case of MQ_UAKE is a multi-hop question made by a chain of single-hop subquestions. Specifically, MQ_UAKE is constructed based on the Wikidata:RDF dataset [Vrande\u02c7ci\u00b4c and Kr\u00f6tzsch, 2014], which, in its rawest format, is a knowledge graph consisting 15+ trillion of Resource Description Framework (RDF) triples. MQ_UAKE essentially builds a much more concise subgraph with only 37 manually elected common relations and top 20% of the most common entities, where a walk of \\\\{2, 3, 4\\\\}-hop on this subgraph can form a case (which is a chain of \\\\{2, 3, 4\\\\} single-hop subquestions connected together) in the MQ_UAKE dataset.\\n\\nMQ_UAKE is presented as two (but in practice, it is essentially three) sub-datasets: MQ_UAKE-CF and MQ_UAKE-T. The former focuses on counterfactual tasks, while the latter on temporal changes. We highlight that there is also a MQ_UAKE-CF-3K dataset, which is a subset of MQ_UAKE-CF that only contains 3,000 cases in total (with 1,000 cases for \\\\{2, 3, 4\\\\}-hop questions respectively). Authors of MQ_UAKE evaluate their proposed method, MeLLo [Zhong et al., 2023], upon this MQ_UAKE-CF-3K dataset, citing limited compute resources; which then become an unspoken standard practice for the majority of the later proposed multi-hop knowledge editing methods [Gu et al., 2024, Shi et al., 2024, Wang et al., 2024, Anonymous, 2024, Cheng et al., 2024]. Due to the very popularity of this sub-sampled dataset, we provide our error analysis mostly based on MQ_UAKE-CF-3K and MQ_UAKE in the following \u00a73. For interested readers, we additionally provide the same error analysis upon the full MQ_UAKE-CF in the Appendix B.2, which is only more drastic than MQ_UAKE-CF-3K due to MQ_UAKE-CF being a much larger superset of the already compromised MQ_UAKE-CF-3K. We also collect the dataset statistics in Table 6 to provide a numerical overview of the composition of all three MQ_UAKE datasets.\"}"}
{"id": "iTUlYblV0K", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.2 Evaluating using MQ\\n\\nDatasets like MQ-2CF or MQ-3K are often evaluated against different \u201cediting intensity,\u201d which is controlled by how many cases among all tested cases are considered \u201cedited,\u201d mimicking different levels of deviation between the learned knowledge stored in the LLM and the desire edited knowledge. This is a sound practice because proper knowledge editing methods should perform well when different numbers of knowledge fragments are edited, as it is equally important to navigate when a significant amount of knowledge is updated, as well as to recognize the few edited knowledge and limit their influence from unrelated unedited knowledge with proper editing locality.\\n\\nIn its original paper, MQ-CF-3K is evaluated when \\\\{1, 100, 1000, 3000\\\\} of its 3,000 cases are edited, similarly, MQ-T is evaluated when \\\\{1, 100, 500, 1868\\\\} of its 1,868 cases being edited, forming an experiment report like Table 5. This kind of report granularity (a gradual coverage from a few edits to all cases being edited) is also adopted by the majority of later proposed multi-hop knowledge editing methods, either in full [Anonymous, 2024] or in spirit with different subsample settings [Gu et al., 2024, Wang et al., 2024, Shi et al., 2024, Cheng et al., 2024, Mengqi et al., 2024].\\n\\nIn this work, we report at an even finer level of granularity for maximum cross-reference potentials.\\n\\n3 Auditing MQ-2\\n\\nIn this section, we present a comprehensive audit of the error pattern that existed in MQ-2 and MQ-T [Zhong et al., 2023]. We specifically note that our audit is there to provide a better understanding to the knowledge editing community, especially when digesting methods evaluated on these datasets.\\n\\nOur audit is not to discredit the contribution of MQ-2, or any of the proposed methods evaluated on MQ-2. We recognize the fact that no dataset can be perfect, especially when it is intrinsically hard to collect large-scale counterfactual and temporal datasets.\\n\\n3.1 Intra Contamination between Edited Cases and Unedited Cases\\n\\nAs discussed in \u00a72.2, having a gradual evaluation coverage from a few to all cases being edited like Table 5 makes sense for as an evaluation granularity. However, one critical issue is that \\\\(k \\\\in \\\\{1, 100, 1000, 3000\\\\}\\\\) edited cases (supposed MQ-CF-3K) are randomly sub-sampled from the 3,000 total cases. Thus, there is no guarantee that the \\\\(k\\\\)-edited cases and \\\\((3000 - k)\\\\) unedited cases would require two disjoint sets of knowledge and, therefore, risk contamination.\\n\\nFor a concrete example, consider the following two multi-hop questions from MQ-CF-3K (we also additionally provide the subquestion breakdown and intermediate answers of the two questions for better presentation, we note that such auxiliary information is not part of the instruction visible to the question-answering LLM):\\n\\n- case_id:245 (unedited):\\n  - What is the official language of the country where Karl Alvarez holds citizenship?\\n  - What is the country of citizenship of Karl Alvarez? USA.\\n  - What is the official language of United States of America? American English.\\n\\n- case_id:323 (unedited):\\n  - What language is the official language of the country where Wendell Pierce holds citizenship?\\n  - What is the country of citizenship of Wendell Pierce? USA.\\n  - What is the official language of United States of America? American English.\\n\\nFor both questions, the correct pre-edited answer should be \u201cAmerican English.\u201d As both Karl Alvarez and Wendell Pierce are US citizens, and the official language of the US is American English. However, suppose case_id:323 is sampled as an edited case while case_id:245 remains unedited, we will be provided with the additional triple containing the knowledge of \u201cThe official language of United States of America is Arabic.\u201d Since the unedited case_id:245 and the edited case_id:323 share the same subquestion of \u201cWhat is the official language of United States of America?\u201d The answer of case_id:323 will be rightfully updated to \u201cArabic\u201d per the new knowledge. However, the unedited case_id:245 still considers the\"}"}
{"id": "iTUlYblV0K", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to be correct, and is therefore contaminated by the edited case in an unintended fashion. This is problematic because a successful knowledge editing method should be able to retrieve the edited knowledge \u2014 \\\"The official language of United States of America is Arabic\\\" \u2014 upon the relevant questions (in this case the shared one), and thus answering \\\"Arabic\\\". This is technically correct, but in conflict with MQ UAKE-CF-3K\u2019s label, causing inaccurate experiment readings.\\n\\nWe further note the above-illustrated contamination is not a cherry-picked fluke, but rather a widespread error. Here, we sample \\\\{1, 100, 1000, 2000, 3000\\\\} -editing targets from MQ UAKE-CF-3K using random seed 100, and find the following error statistics in Table 1.\\n\\n| # of Contaminated Cases | MQ UAKE-CF-3K | MQ UAKE-T [Zhong et al., 2023] |\\n|-------------------------|---------------|-------------------------------|\\n| 1-edit                 | 0             | 2,013                         |\\n| 100-edit               | 2,706         | 1,772                         |\\n| 1000-edit              | 1,664         | 910                           |\\n| 2000-edit              | 0             | 0                             |\\n| 3000-edit              | 0             | 29                            |\\n\\nIt is observable from Table 1 that even a small number of edited cases will cause a concerningly large contamination to unedited cases and subquestions, where 67% and 76% of all cases from MQ UAKE-CF-3K and MQ UAKE-T are contaminated with just 100 cases being edited, introducing a significant distortion to the reported experiment results.\\n\\nWe additionally note while this edited-to-unedited intra-contamination is reducing with \\\\(k\\\\)-edit growing, this does not imply a diminishing of issue, but rather a simple by-product of a larger \\\\(k\\\\) implies a lesser \\\\((3000 - k)\\\\), leaving fewer unedited cases as potential contamination victims. In the extreme case of 3000-edit, there is 0 edited-to-unedited contamination because there is no unedited case left in MQ UAKE-CF-3K to be the victim. But 3000-edit has the most edited-to-edited inner contamination, more on this in the following \u00a73.2.\\n\\n### 3.2 Inner Contamination between Different Edited Cases\\n\\nOther than edited cases contaminating unedited cases (\u00a73.1), contamination might also happen among multiple edited cases because a certain subquestion presented in different edited cases can be edited in some but unedited in others. For brevity, we leave the example walkthrough in Appendix B.1.\\n\\n| # of Contaminated Subquestions | MQ UAKE-CF-3K | MQ UAKE-T [Zhong et al., 2023] |\\n|-------------------------------|---------------|-------------------------------|\\n| 1-edit                        | 0             | 14                            |\\n| 100-edit                      | 14            | 265                           |\\n| 1000-edit                     | 337           | 619                           |\\n| 2000-edit                     | 854           | 998                           |\\n| 3000-edit                     | 1,399         | 0                             |\\n\\nThis type of contamination is, once again, universally visible in MQ UAKE, as shown in Table 2; which is very much a flipped version of Table 1. With \\\\(k\\\\)-edit growing, there are more edited cases, thus more edited-to-edited contamination, as there are more potential victims. Notably, under the 3000-edit tasks, almost one-third (998/3000, \\\\(\\\\approx\\\\) 33\\\\%) of the evaluated cases are contaminated, which again introduces distortion to the reported experiment results. We omit the report on MQ UAKE-T here because there is only one edit-to-edit contamination when all 1,868 cases from MQ UAKE-T are edited (case_id:424).\\n\\nWe note that in Zhong et al. [2023], \\\"\\\\(k\\\\)-edit\\\" means only \\\\(k\\\\) of edited cases are evaluated, without any uneeded cases. We evaluated both to better reflect the locality of different knowledge editing methods.\\n\\nNote, an edited case does not require all of its subquestions being edited, but merely one or more of it (Table 6).\"}"}
{"id": "iTUlYblV0K", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.3 Conflicting Edits\\n\\nThe two types of contamination introduced in \u00a73.1 and \u00a73.2 are indeed subtle and hard to detect, as they hide between the retrieval scope of different edited cases, which is further complicated when only a subset of cases are edited. However, MQ\\\\_AKE-CF\\\\_3 also includes some straightforward conflicts, such as for the subquestion \\\"Which company is Ford Mustang produced by?\\\" we have the following edits:\\n\\n- case_id:2566 (edited): Ford Motor Company Nintendo.\\n- case_id:231/2707 (edited): Ford Motor Company Fiat S.p.A.\\n\\nThis is going to cause a direct conflict when case_id:2566 and any of the case_id:231/2707 are both selected as edited cases, as they shall confuse any knowledge edited LLM for having two answers to the same questions. Fortunately, such types of errors are rather minuscule in MQ\\\\_AKE-CF\\\\_3, with the abovementioned Ford Mustang question and three cases being the only affected data samples.\\n\\n3.4 Missing Information in Multi-hop Question Instructions\\n\\nAs mentioned in \u00a72, the MQ\\\\_AKE dataset is built upon a severely filtered Wikidata:RDF knowledge graph [Vrande\u02c7ci\u00b4c and Kr\u00f6tzsch, 2014]. Specifically, the triples of a certain \\\\{2,3,4\\\\}-hop walk on this subgraph are then fed into a gpt-3.5-turbo model to generate the multi-hop question instruction in a natural language format; such generation are repeated for three different times in case any of the generated question instructions becomes incomprehensible. For every case evaluation, an LLM is considered right should it correctly answer against any three of the multi-hop question instructions [Zhong et al., 2023].\\n\\nHowever, while repeating generation three times definitely reduces the chances of having incomprehensible question instructions, we noticed some of such instructions in MQ\\\\_AKE are still incomplete. We take the following triple set and its generated 3-questions as an example:\\n\\n- case_id:546 (unedited): We have a 2-hop triple chain of (Albert Mohler, employer, Southern Baptist Theological Seminary) and (Southern Baptist Theological Seminary, religion or worldview, Southern Baptist Convention).\\n- MQ\\\\_AKE-CF\\\\_3 provides the following generated multi-hop questions:\\n  - Generation #1: What religion is Albert Mohler associated with?\\n  - Generation #2: Which religion does Albert Mohler follow?\\n  - Generation #3: With which religious faith does Albert Mohler identify?\\n\\nIt is clear that all three generated questions omit the part mentioning which company/institution Albert Mohler is employed by and essentially reduce themselves to single-hop questions, where a correct generation should read like \\\"What religion is Albert Mohler's employer associated with?\\\" Without the complete question, suppose there is an edit on Albert Mohler's employer (which there indeed is one), the final answer would likely change. However, with question instruction omitting such information, even the best knowledge-edited LLM cannot answer the question correctly with a faithful approach.\\n\\nAs a general analysis, we find the natural language question instructions of 672 cases in MQ\\\\_AKE-CF\\\\_3 are missing information in comparison to their raw triplet chain. This number is counted in the sense that one or more pieces of information present in the triple chain are missing from all three variants of the generated natural language instruction. Similarly, there are 2,830 and 233 cases of erroneous instructions in MQ\\\\_AKE-CF and MQ\\\\_AKE-T, respectively.\\n\\n3.5 Duplicated Cases\\n\\nThe last kind of error we discovered in MQ\\\\_AKE is simply unintended duplication \u2014 i.e., two or more cases sharing the same start subjects, edited facts, chain of triples, and final answer. We discovered 47, 4, and 4 cases of duplication, respectively, in MQ\\\\_AKE-CF, MQ\\\\_AKE-CF\\\\_3, and MQ\\\\_AKE-T.\"}"}
{"id": "iTUlYblV0K", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we illustrate how we modified and improved the MQ UAKE dataset to MQ UAKE-MASTERED with various fixes on the data samples themselves, as well as providing utility modules to facilitate how one interacts with such datasets.\\n\\n4.1 Hard Corrections\\n\\nThree types of error existing in MQ UAKE can be fixed once and for all with some careful hard corrections, they are namely Conflicting Edits (\u00a73.3), Missing Information in Multi-hop Question Instructions (\u00a73.4), and Duplicated Cases (\u00a73.5). For Conflicting Edits and Duplicated Cases, since there are only a few such errors (<50 per type per dataset), we employ some manual corrections to address these errors: in the former case, we flip the minority edits to align with the majority edits (and adjust their answers to their subsequence subquestions, should there be any); in the latter case, we simply remove such duplicated cases (except for MQ UAKE-CF-3K, which we manually select 4 more cases from MQ UAKE-CF to keep the dataset having 3,000 cases in total and a 1,000 cases for \\\\{2, 3, 4\\\\}-hops). For the Missing Information in Multi-hop Question Instructions errors, we rewrite such natural language question instructions and then replace the original information-missing instructions.\\n\\n4.2 Dynamic Masking for Maximum Coverage: MQ UAKE-MASTERED-CF, MQ UAKE-MASTERED-CF-3K, and MQ UAKE-MASTERED-T\\n\\nDue to the contamination count of Intra Edited-to-Unedited Contamination (\u00a73.1) and Inner Edited-to-Edited Contamination (\u00a73.2) tend to grow in the opposite direction as shown in Table 1 and 2, it is impossible to find a fix within the current MQ UAKE that can address both issues without significantly decreasing the dataset size. As an alternative, we develop an API that will take a case_id and an edited_flag as input, respectfully indicating the evaluating case-in-question and whether this case is considered edited; our API shall then return a set of triples that are contamination free by dynamically masking out the conflicting edits from other cases. After such, the user may build up an editing knowledge bank upon such triplets and conduct evaluations for any memory-based knowledge editing methods without losing any of the 9,218 cases from MQ UAKE-CF or 1,868 cases from MQ UAKE-T.\\n\\nSpecifically, once case_id-of-interest is given, our API would loop through all of its subquestions and identify if any of such subquestions is considered edited under another case. If there is a hit, the triple with respect to such edited subquestions is then removed from the bank of edited triples. This dynamic masking mechanism would ensure all cases within the original MQ UAKE be usable against memory-based knowledge editing methods. However, the drawback of masking is it won't support parameter-based knowledge editing methods, where weight update is required. We additionally provide a MQ UAKE-MASTERED-CF-6334 to address the need for such methods (Appendix C.1).\\n\\n5 Benchmark and Discussion\\n\\nGiven almost all proposed multi-hop knowledge editing methods are evaluated on the original, error-contained, MQ UAKE datasets. Here, we provide a re-benchmark of those methods against post-fix MQ UAKE-MASTERED datasets for a more reliable reporting of each method's performance.\\n\\n5.1 Experiment Coverage\\n\\nCompared Methods\\n\\nIn this work, we aim to cover most, if not all, open-sourced knowledge editing methods evaluated on the original MQ UAKE. To the best of our knowledge, this screening criteria include MeLLo [Zhong et al., 2023] and PokeMQA [Gu et al., 2024] as methods specifically proposed to target this multi-hop knowledge editing problem and evaluated on MQ UAKE. We additionally include ICE [Cohen et al., 2023] and IKE [Zheng et al., 2023a] as these are also methods purposed for the (single-edit) multi-hop knowledge editing task, though not specifically evaluated.\"}"}
{"id": "iTUlYblV0K", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We note that methods like GMeLLo [Anonymous, 2024], GLAME [Mengqi et al., 2024], RAE [Shi et al., 2024], StableKE [Wei et al., 2024], and Temple-MQA [Cheng et al., 2024] are also evaluated on MQ.UAKE, but they are purposely omitted from our re-benchmark coverage due to lack of open-sourced implementation, likely because most of these works are still in submission. Last, we note DeepEdit [Wang et al., 2024] is also an open-sourced MQ.UAKE-evaluated method, but we excluded it due to its lack of inference optimization (>200 A100 GPU hours needed for 1-edit on MQ.UAKE-REMASTERED-CF-3K).\\n\\nCovered Models\\nWe opt to use lmsys/vicuna-7b-v1.5 [Zheng et al., 2023b], mistralai/Mistral-7B-Instruct-v0.2 [Jiang et al., 2023], and meta-llama/Meta-Llama-3-8B-Instruct [AI@Meta, 2024] as the choice of question-answering models, both for alignment with existing works [Zhong et al., 2023, Shi et al., 2024, Gu et al., 2024] as well as providing coverage the most recent language models. For methods that require a text-embedding model as a retriever, we use facebook/contriever-msmarco [Izacard et al., 2022] for alignment with MeLLo [Zhong et al., 2023].\\n\\nCovered Datasets\\nWe will provide coverage on our post-fix dataset, namely MQ.UAKE-REMASTERED-CF, MQ.UAKE-REMASTERED-CF-3K, and MQ.UAKE-REMASTERED-CF-3K in the masking fashion illustrated in \u00a74.2; as well as MQ.UAKE-REMASTERED-CF-6334 in its vanilla form. These datasets are respectively corresponding to the original MQ.UAKE-CF, MQ.UAKE-CF-3K, and MQ.UAKE-T from Zhong et al. [2023] (with 6334 as an extra for parameter-based methods), but with the types of error mentioned in \u00a73 fixed in the via means illustrated in \u00a74. We emphasize that such modification is legitimate, and our MQ.UAKE-REMASTERED is free for the scholarly community to adopt, as the original MQ.UAKE dataset was published under the MIT license. Where MQ.UAKE-REMASTERED will be released under CC BY 4.0. All experiments are conducted with an 80G NVIDIA A100 from a DGX A100 server.\\n\\nTable 3: Performance Comparison of Original MQ.UAKE and our MQ.UAKE-REMASTERED datasets\\n\\n| Method          | MQ.UAKE-CF-3k Original | MQ.UAKE-CF-3k Remastered |\\n|-----------------|------------------------|--------------------------|\\n| MeLLo [Zhong et al., 2023] | 6.7                    | 6.77                     |\\n| GWalk           | 36.23                  | 66.33                    |\\n\\nTable 4: Experiments on MQ.UAKE-REMASTERED-CF with numbers of edited cases and methods.\\n\\nResults inside ( ) are edited cases accuracy and unedited cases accuracy, respectively.\\n\\n| Method          | MQ.UAKE-REMASTERED-CF 1-edit | MQ.UAKE-REMASTERED-CF 1000-edit | MQ.UAKE-REMASTERED-CF 3000-edit | MQ.UAKE-REMASTERED-CF 6000-edit | MQ.UAKE-REMASTERED-CF 9171-edit |\\n|-----------------|-----------------------------|---------------------------------|---------------------------------|---------------------------------|--------------------------------|\\n| vicuna-7b-v1.5  | [22.55(100, 22.54)          | 21.54(8, 23.2)                  | 17.79(7.43, 22.83)              | 12.62(7.28, 22.58)              | 6.95(6.95, N/A)                 |\\n| MeLLo [Zhong et al., 2023] | <1 OOM                  | <1 OOM                          | <1 OOM                          | <1 OOM                          | <1 OOM                          |\\n| ICE [Cohen et al., 2023]  | <1 OOM                  | <1 OOM                          | <1 OOM                          | <1 OOM                          | <1 OOM                          |\\n| IKE [Zheng et al., 2023a] | <1 OOM                  | <1 OOM                          | <1 OOM                          | <1 OOM                          | <1 OOM                          |\\n| GWalk (Ours)    | 61.89 (100, 61.89)        | 56.98 (56.2, 57.07)             | 56.37 (53.97, 57.54)            | 54.93 (53.27, 58.06)            | 54.15 (54.15, N/A)              |\\n| Mistral-7B-Instruct-v0.2 [Jiang et al., 2023] | 19.83(<1, 19.84)            | 19.08(20.6, 18.9)               | 18.9(19.47, 18.62)              | 18.27(19.02, 16.87)              | 18.09(18.09, N/A)               |\\n| ICE [Cohen et al., 2023]  | <1 OOM                  | <1 OOM                          | <1 OOM                          | <1 OOM                          | <1 OOM                          |\\n| IKE [Zheng et al., 2023a] | <1 OOM                  | <1 OOM                          | <1 OOM                          | <1 OOM                          | <1 OOM                          |\\n| GWalk (Ours)    | 74.09 (100, 74.09)        | 73.67 (71.1, 73.98)             | 72.4 (70.9, 73.13)              | 71.62 (70.33, 74.05)            | 70.08 (70.08, N/A)              |\\n| Meta-Llama-3-8B-Instruct [AI@Meta, 2024] | <1 OOM                  | <1 OOM                          | <1 OOM                          | <1 OOM                          | <1 OOM                          |\\n\\nGiven our MQ.UAKE-REMASTERED are mostly provided as a fix to MQ.UAKE, we would like to first highlight the drastic results difference when the same method is evaluated on these two datasets. Table 3 shows our fixing can indeed result in drastically different experiment reports. Where such difference is especially significant for stronger methods, suggesting all previous reporting on MQ.UAKE has room for reliability improvements, which we filled here with MQ.UAKE-REMASTERED.\"}"}
{"id": "iTUlYblV0K", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Due to page limitation, we only present the benchmark results on MQ UAKE-R EMASTERED - CF in the main text and refer our readers to Appendix D.2 for benchmarks of MQ UAKE-R EMASTERED - 324 3K, MQ UAKE-R EMASTERED - T, and MQ UAKE-R EMASTERED - CF-6334. Given the dominance of GWalk \u2014 a demo method we proposed as guidance to future scholars of this MHKE task \u2014 we leave more discussion on this method below.\\n\\n5.3 Making Faithful Approach to MQ UAKE and MQ UAKE-R EMASTERED\\n\\nAdditionally, it is also our observation that many multi-hop knowledge editing methods with decent accuracy reports on MQ UAKE or MQ UAKE-R EMASTERED are utilizing designs that leverage specific data properties unique to MQ UAKE. For example, methods like GLAME [Mengqi et al., 2024] utilize Wikidata [Vrande\u02c7ci\u00b4c and Kr\u00f6tzsch, 2014] as the external knowledge graph to better detect the edit-induced conflicts, which happen to be the source of MQ UAKE as discussed in \u00a72.1. While these methods might have decent performance on MQ UAKE, the cost of maintaining a positive knowledge graph on the correct \u2014 but not just edited \u2014 knowledge facts is undoubtedly a non-trivial operation cost. Yet, whether sourcing the same Wikidata knowledge graph as MQ UAKE might bring them data-specific advantages remains unanswered. Similarly, PokeMQA [Gu et al., 2024] utilizes the 6,218 cases included in MQ UAKE-CF but not in MQ UAKE-CF-3 as the train set to train its auxiliary components. Given MQ UAKE is a dataset with relatively low diversity (e.g., it only includes 37 types of relations), whether having a heavily overlapped train and test set will result in data-specific advantages unique MQ UAKE and its variants, again remains unanswered.\\n\\nA Minimally Invasive but Performant Approach: GWalk\\n\\nHere, we provide a brief walkthrough of a simple method we designed, namely GraphWalk. It does not leverage any data-specific property unique to MQ UAKE or MQ UAKE-R EMASTERED, yet still presents pleasant performance surpassing many established baselines. We illustrate this method as a simple guidance and potential inspiration to our future multi-hop knowledge editing scholars. Due to page limitation, we introduce the technical details and design intuition of GWalks in Appendix D.1, and only present the performance of GWalks in the main text.\\n\\nWe hope the performant nature of GWalk \u2014 in its most vanilla form, without employing any data-specific property unique to MQ UAKE or MQ UAKE-R EMASTERED \u2014 can inspire more multi-hop knowledge editing methods that leverage the graph topology of edited facts, without converting such facts to natural language descriptions (at least for retrieval).\\n\\n6 Related Works\\n\\nOur work mainly conducts an audit and provides fixes to the MQ UAKE dataset. To the best of our knowledge, only two prior arts have touched on the errors existing in MQ UAKE: GMeLLo [Anonymous, 2024] (an anonymous submission to ACL ARR 2024 February) and DeepEdit [Wang et al., 2024]. As an overview, GMeLLo briefly discussed the same type of error we discussed in \u00a73.4 without providing any quantitative error analysis or any fix. DeepEdit discovered the same inner contamination error as we discussed in \u00a73.2, but specific to 3000-edit setup. DeepEdit's proposed fix is simply removing the 998 inner contaminated cases from the MQ UAKE-CF-3 dataset, so this fix is custom 3000-edit and done so by sacrificing 1/3 of the dataset capacity. We leave more details in Appendix E due to page limitation.\\n\\nAdditionally, our work provides a re-benchmark of most, if not all, open-sourced knowledge editing methods evaluated on MQ UAKE, and sets guidance on how to faithfully approach such datasets. To the best of our knowledge, no other work provides the same benchmark nor touches on the same issue.\\n\\n7 Conclusion\\n\\nOur work provides a comprehensive audit and fix of the MQ UAKE dataset. We further re-benchmarked all open-sourced knowledge editing methods evaluated on MQ UAKE with our MQ UAKE-R EMASTERED datasets and provided guidance and examples on how to faithfully approach these datasets with our GWalk.\"}"}
{"id": "iTUlYblV0K", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"While our work comprehensively addressed many errors in MQAKE, we caution our reader to perform further analysis and evaluation on our MQAKE-R to ensure our fixes are indeed exhaustive. We also note that multi-hop knowledge editing only represents one aspect of a language model's ability, so any actual deployment of a language model should undergo more, and if possible, deployment-specific evaluations.\\n\\nReferences\\n\\nAI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md.\\n\\nAnonymous. Graph memory-based editing for large language models. Submission to ACL ARR 2024 February, 2024.\\n\\nK. Cheng, G. Lin, H. Fei, Y. zhai, L. Yu, M. A. Ali, L. Hu, and D. Wang. Multi-hop question answering under temporal knowledge editing. arXiv, 2024.\\n\\nR. Cohen, E. Biran, O. Yoran, A. Globerson, and M. Geva. Evaluating the ripple effects of knowledge editing in language models. Transactions of the Association for Computational Linguistics, 2023.\\n\\nH. Gu, K. Zhou, X. Han, N. Liu, R. Wang, and X. Wang. Pokemqa: Programmable knowledge editing for multi-hop question answering. arXiv, 2024.\\n\\nL. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, and T. Liu. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv, 2023.\\n\\nG. Izacard, M. Caron, L. Hosseini, S. Riedel, P. Bojanowski, A. Joulin, and E. Grave. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research, 2022.\\n\\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b. arXiv, 2023.\\n\\nZ. Mengqi, Y. Xiaotian, L. Qiang, R. Pengjie, W. Shu, and C. Zhumin. Knowledge graph enhanced large language model editing. arXiv, 2024.\\n\\nE. Mitchell, C. Lin, A. Bosselut, C. Finn, and C. D. Manning. Fast model editing at scale. In International Conference on Learning Representations, 2022.\\n\\nY. Shi, Q. Tan, X. Wu, S. Zhong, K. Zhou, and N. Liu. Retrieval-enhanced knowledge editing for multi-hop question answering in language models. arXiv, 2024.\\n\\nA. Sinitsin, V. Plokhotnyuk, D. Pyrkin, S. Popov, and A. Babenko. Editable neural networks. In International Conference on Learning Representations, 2020.\\n\\nD. Vrande\u02c7ci\u00b4c and M. Kr\u00f6tzsch. Wikidata: a free collaborative knowledgebase. Communications of the ACM, 2014.\\n\\nY. Wang, M. Chen, N. Peng, and K.-W. Chang. Deepedit: Knowledge editing as decoding with constraints. arXiv, 2024.\\n\\nZ. Wei, L. Pang, H. Ding, J. Deng, H. Shen, and X. Cheng. Stable knowledge editing in large language models. arXiv, 2024.\"}"}
{"id": "iTUlYblV0K", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, S. Zhong, B. Yin, and X. Hu. Harnessing the power of llms in practice: A survey on chatgpt and beyond. ACM Trans. Knowl. Discov. Data, 2024.\\n\\nY. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y. Chen, L. Wang, A. T. Luu, W. Bi, F. Shi, and S. Shi. Siren's song in the ai ocean: A survey on hallucination in large language models. arXiv, 2023.\\n\\nW. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, Y. Du, C. Yang, Y. Chen, Z. Chen, J. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu, P. Liu, J.-Y. Nie, and J.-R. Wen. A survey of large language models. arXiv, 2023.\\n\\nC. Zheng, L. Li, Q. Dong, Y. Fan, Z. Wu, J. Xu, and B. Chang. Can we edit factual knowledge by in-context learning? arXiv, 2023a.\\n\\nL. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv, 2023b.\\n\\nZ. Zhong, Z. Wu, C. D. Manning, C. Potts, and D. Chen. MQuAKE: Assessing knowledge editing in language models via multi-hop questions. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.\\n\\nChecklist\\nThe checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [TODO] to [Yes], [No], or [N/A]. You are strongly encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description. For example:\\n\\n\u2022 Did you include the license to the code and datasets? [Yes] See Section ??.\\n\\nPlease do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] We provide an audit and remake of a dataset, as well as a benchmark of all available methods.\\n   (b) Did you describe the limitations of your work? [Yes]\\n   (c) Did you discuss any potential negative societal impacts of your work? [Yes]\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] We have read and ensured the paper conforms to the guidelines.\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A] No theoretical result included in the paper.\\n   (b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [N/A]\\n   (b) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [N/A]\\n   (c) Did you mention the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [N/A]\\n   (d) Did you include the datasets? [N/A]\\n   (e) Did you include the datasets? [N/A]\\n   (f) Did you include the datasets? [N/A]\\n   (g) Did you include the datasets? [N/A]\\n   (h) Did you include the datasets? [N/A]\\n   (i) Did you include the datasets? [N/A]\\n   (j) Did you include the datasets? [N/A]\\n   (k) Did you include the datasets? [N/A]\\n   (l) Did you include the datasets? [N/A]\\n   (m) Did you include the datasets? [N/A]\\n   (n) Did you include the datasets? [N/A]\\n   (o) Did you include the datasets? [N/A]\\n   (p) Did you include the datasets? [N/A]\\n   (q) Did you include the datasets? [N/A]\\n   (r) Did you include the datasets? [N/A]\\n   (s) Did you include the datasets? [N/A]\\n   (t) Did you include the datasets? [N/A]\\n   (u) Did you include the datasets? [N/A]\\n   (v) Did you include the datasets? [N/A]\\n   (w) Did you include the datasets? [N/A]\\n   (x) Did you include the datasets? [N/A]\\n   (y) Did you include the datasets? [N/A]\\n   (z) Did you include the datasets? [N/A]\\n   (AA) Did you include the datasets? [N/A]\\n   (BB) Did you include the datasets? [N/A]\\n   (CC) Did you include the datasets? [N/A]\\n   (DD) Did you include the datasets? [N/A]\\n   (EE) Did you include the datasets? [N/A]\\n   (FF) Did you include the datasets? [N/A]\\n   (GG) Did you include the datasets? [N/A]\\n   (HH) Did you include the datasets? [N/A]\\n   (II) Did you include the datasets? [N/A]\\n   (JJ) Did you include the datasets? [N/A]\\n   (KK) Did you include the datasets? [N/A]\\n   (LL) Did you include the datasets? [N/A]\\n   (MM) Did you include the datasets? [N/A]\\n   (NN) Did you include the datasets? [N/A]\\n   (OO) Did you include the datasets? [N/A]\\n   (PP) Did you include the datasets? [N/A]\\n   (QQ) Did you include the datasets? [N/A]\\n   (RR) Did you include the datasets? [N/A]\\n   (SS) Did you include the datasets? [N/A]\\n   (TT) Did you include the datasets? [N/A]\\n   (UU) Did you include the datasets? [N/A]\\n   (VV) Did you include the datasets? [N/A]\\n   (WW) Did you include the datasets? [N/A]\\n   (XX) Did you include the datasets? [N/A]\\n   (YY) Did you include the datasets? [N/A]\\n   (ZZ) Did you include the datasets? [N/A]\\n   (AAA) Did you include the datasets? [N/A]\\n   (BBB) Did you include the datasets? [N/A]\\n   (CCC) Did you include the datasets? [N/A]\\n   (DDD) Did you include the datasets? [N/A]\\n   (EEE) Did you include the datasets? [N/A]\\n   (FFF) Did you include the datasets? [N/A]\\n   (GGG) Did you include the datasets? [N/A]\\n   (HHH) Did you include the datasets? [N/A]\\n   (III) Did you include the datasets? [N/A]\\n   (JJJ) Did you include the datasets? [N/A]\\n   (KKK) Did you include the datasets? [N/A]\\n   (LLL) Did you include the datasets? [N/A]\\n   (MMM) Did you include the datasets? [N/A]\\n   (NNN) Did you include the datasets? [N/A]\\n   (OOO) Did you include the datasets? [N/A]\\n   (PPP) Did you include the datasets? [N/A]\\n   (QQQ) Did you include the datasets? [N/A]\\n   (RRR) Did you include the datasets? [N/A]\\n   (SSS) Did you include the datasets? [N/A]\\n   (TTT) Did you include the datasets? [N/A]\\n   (UUU) Did you include the datasets? [N/A]\\n   (VVV) Did you include the datasets? [N/A]\\n   (WWW) Did you include the datasets? [N/A]\\n   (XXX) Did you include the datasets? [N/A]\\n   (YYY) Did you include the datasets? [N/A]\\n   (ZZZ) Did you include the datasets? [N/A]\\n   (AAA) Did you include the datasets? [N/A]\\n   (BBB) Did you include the datasets? [N/A]\\n   (CCC) Did you include the datasets? [N/A]\\n   (DDD) Did you include the datasets? [N/A]\\n   (EEE) Did you include the datasets? [N/A]\\n   (FFF) Did you include the datasets? [N/A]\\n   (GGG) Did you include the datasets? [N/A]\\n   (HHH) Did you include the datasets? [N/A]\\n   (III) Did you include the datasets? [N/A]\\n   (JJJ) Did you include the datasets? [N/A]\\n   (KKK) Did you include the datasets? [N/A]\\n   (LLL) Did you include the datasets? [N/A]\\n   (MMM) Did you include the datasets? [N/A]\\n   (NNN) Did you include the datasets? [N/A]\\n   (OOO) Did you include the datasets? [N/A]\\n   (PPP) Did you include the datasets? [N/A]\\n   (QQQ) Did you include the datasets? [N/A]\\n   (RRR) Did you include the datasets? [N/A]\\n   (SSS) Did you include the datasets? [N/A]\\n   (TTT) Did you include the datasets? [N/A]\\n   (UUU) Did you include the datasets? [N/A]\\n   (VVV) Did you include the datasets? [N/A]\\n   (WWW) Did you include the datasets? [N/A]\\n   (XXX) Did you include the datasets? [N/A]\\n   (YYY) Did you include the datasets? [N/A]\\n   (ZZZ) Did you include the datasets? [N/A]\\n   (AAA) Did you include the datasets? [N/A]\\n   (BBB) Did you include the datasets? [N/A]\\n   (CCC) Did you include the datasets? [N/A]\\n   (DDD) Did you include the datasets? [N/A]\\n   (EEE) Did you include the datasets? [N/A]\\n   (FFF) Did you include the datasets? [N/A]\\n   (GGG) Did you include the datasets? [N/A]\\n   (HHH) Did you include the datasets? [N/A]\\n   (III) Did you include the datasets? [N/A]\\n   (JJJ) Did you include the datasets? [N/A]\\n   (KKK) Did you include the datasets? [N/A]\\n   (LLL) Did you include the datasets? [N/A]\\n   (MMM) Did you include the datasets? [N/A]\\n   (NNN) Did you include the datasets? [N/A]\\n   (OOO) Did you include the datasets? [N/A]\\n   (PPP) Did you include the datasets? [N/A]\\n   (QQQ) Did you include the datasets? [N/A]\\n   (RRR) Did you include the datasets? [N/A]\\n   (SSS) Did you include the datasets? [N/A]\\n   (TTT) Did you include the datasets? [N/A]\\n   (UUU) Did you include the datasets? [N/A]\\n   (VVV) Did you include the datasets? [N/A]\\n   (WWW) Did you include the datasets? [N/A]\\n   (XXX) Did you include the datasets? [N/A]\\n   (YYY) Did you include the datasets? [N/A]\\n   (ZZZ) Did you include the datasets? [N/A]\\n   (AAA) Did you include the datasets? [N/A]\\n   (BBB) Did you include the datasets? [N/A]\\n   (CCC) Did you include the datasets? [N/A]\\n   (DDD) Did you include the datasets? [N/A]\\n   (EEE) Did you include the datasets? [N/A]\\n   (FFF) Did you include the datasets? [N/A]\\n   (GGG) Did you include the datasets? [N/A]\\n   (HHH) Did you include the datasets? [N/A]\\n   (III) Did you include the datasets? [N/A]\\n   (JJJ) Did you include the datasets? [N/A]\\n   (KKK) Did you include the datasets? [N/A]\\n   (LLL) Did you include the datasets? [N/A]\\n   (MMM) Did you include the datasets? [N/A]\\n   (NNN) Did you include the datasets? [N/A]\\n   (OOO) Did you include the datasets? [N/A]\\n   (PPP) Did you include the datasets? [N/A]\\n   (QQQ) Did you include the datasets? [N/A]\\n   (RRR) Did you include the datasets? [N/A]\\n   (SSS) Did you include the datasets? [N/A]\\n   (TTT) Did you include the datasets? [N/A]\\n   (UUU) Did you include the datasets? [N/A]\\n   (VVV) Did you include the datasets? [N/A]\\n   (WWW) Did you include the datasets? [N/A]\\n   (XXX) Did you include the datasets? [N/A]\\n   (YYY) Did you include the datasets? [N/A]\\n   (ZZZ) Did you include the datasets? [N/A]\\n   (AAA) Did you include the datasets? [N/A]\\n   (BBB) Did you include the datasets? [N/A]\\n   (CCC) Did you include the datasets? [N/A]\\n   (DDD) Did you include the datasets? [N/A]\\n   (EEE) Did you include the datasets? [N/A]\\n   (FFF) Did you include the datasets? [N/A]\\n   (GGG) Did you include the datasets? [N/A]\\n   (HHH) Did you include the datasets? [N/A]\\n   (III) Did you include the datasets? [N/A]\\n   (JJJ) Did you include the datasets? [N/A]\\n   (KKK) Did you include the datasets? [N/A]\\n   (LLL) Did you include the datasets? [N/A]\\n   (MMM) Did you include the datasets? [N/A]\\n   (NNN) Did you include the datasets? [N/A]\\n   (OOO) Did you include the datasets? [N/A]\\n   (PPP) Did you include the datasets? [N/A]\\n   (QQQ) Did you include the datasets? [N/A]\\n   (RRR) Did you include the datasets? [N/A]\\n   (SSS) Did you include the datasets? [N/A]\\n   (TTT) Did you include the datasets? [N/A]\\n   (UUU) Did you include the datasets? [N/A]\\n   (VVV) Did you include the datasets? [N/A]\\n   (WWW) Did you include the datasets? [N/A]\\n   (XXX) Did you include the datasets? [N/A]\\n   (YYY) Did you include the datasets? [N/A]\\n   (ZZZ) Did you include the datasets? [N/A]\\n   (AAA) Did you include the datasets? [N/A]\\n   (BBB) Did you include the datasets? [N/A]\\n   (CCC) Did you include the datasets? [N/A]\\n   (DDD) Did you include the datasets? [N/A]\\n   (EEE) Did you include the datasets? [N/A]\\n   (FFF) Did you include the datasets? [N/A]\\n   (GGG) Did you include the datasets? [N/A]\\n   (HHH) Did you include the datasets? [N/A]\\n   (III) Did you include the datasets? [N/A]\\n   (JJJ) Did you include the datasets? [N/A]\\n   (KKK) Did you include the datasets? [N/A]\\n   (LLL) Did you include the datasets? [N/A]\\n   (MMM) Did you include the datasets? [N/A]\\n   (NNN) Did you include the datasets? [N/A]\\n   (OOO) Did you include the datasets? [N/A]\\n   (PPP) Did you include the datasets? [N/A]\\n   (QQQ) Did you include the datasets? [N/A]\\n   (RRR) Did you include the datasets? [N/A]\\n   (SSS) Did you include the datasets? [N/A]\\n   (TTT) Did you include the datasets? [N/A]\\n   (UUU) Did you include the datasets? [N/A]\\n   (VVV) Did you include the datasets? [N"}
{"id": "iTUlYblV0K", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] In supplemental material.\\n\\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] In supplemental material.\\n\\n(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] Given the massive amount of experiments, we fix the seeds and run each experiment entry by once.\\n\\n(d) Did you include the total amount of compute and the type of resources used? [Yes] See \u00a75.1 for resource and supplementary materials for amount of compute.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n\\n(a) If your work uses existing assets, did you cite the creators? [Yes] All works are properly cited in-text and afterward.\\n\\n(b) Did you mention the license of the assets? [Yes] At \u00a75.1\\n\\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We include the dataset in supplemental materials\\n\\n(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] Data used are open-sourced in MIT license, as showed in \u00a75.1.\\n\\n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] In \u00a72.1, we discussed the MQuAKE dataset is constructed based on the Wikidata: RDF dataset\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] No applicable\\n\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] No applicable\\n\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] No applicable\"}"}
{"id": "iTUlYblV0K", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 5: Standard reporting format of MQAKE-CF and MQAKE-T demoed with MeLLo on Vicuna-7B [Zheng et al., 2023b]; k-edited means k cases out of the total cases are edited. Abbreviated table courtesy of Zhong et al. [2023] (Table 3).\\n\\n| Model          | Method | 1-edit | 100-edit | 1000-edit | 3000-edit |\\n|----------------|--------|--------|----------|-----------|-----------|\\n| MQAKE-CF-3K    | Vicuna-7B MeLLo [Zhong et al., 2023] | 20.3   | 11.9     | 11.0      | 10.2      |\\n| MQAKE-T        |        | 84.4   | 56.3     | 52.6      | 51.3      |\\n\\n### A.2 Dataset Statistics\\n\\nTable 6: Dataset Statistics of MQAKE. Numbers are in terms of cases (a case in MQAKE is a chain consisting of multiple subquestions).\\n\\n| Dataset   | # of Edits | 2-hop | 3-hop | 4-hop | Total |\\n|-----------|------------|-------|-------|-------|-------|\\n| MQAKE-CF-3K | 1          | 513   | 356   | 224   | 1,093 |\\n|           | 2          | 487   | 334   | 246   | 1,067 |\\n|           | 3          | -     | 310   | 262   | 572   |\\n|           | 4          | -     | -     | 268   | 268   |\\n|           | All        | 1,000 | 1,000 | 1,000 | 3,000 |\\n\\n| Dataset   | # of Edits | 2-hop | 3-hop | 4-hop | Total |\\n|-----------|------------|-------|-------|-------|-------|\\n| MQAKE-CF  | 1          | 2,454 | 855   | 446   | 3,755 |\\n|           | 2          | 2,425 | 853   | 467   | 3,745 |\\n|           | 3          | -     | 827   | 455   | 1,282 |\\n|           | 4          | -     | -     | 436   | 436   |\\n|           | All        | 4,879 | 2,535 | 1,804 | 9,218 |\\n\\n| Dataset   | # of Edits | 2-hop | 3-hop | 4-hop | Total |\\n|-----------|------------|-------|-------|-------|-------|\\n| MQAKE-CF-EMASTERED-3K | 1          | 1,971 | 77    | 0     | 2,048 |\\n|           | 2          | 2,415 | 476   | 14    | 2,905 |\\n|           | 3          | -     | 823   | 128   | 951    |\\n|           | 4          | -     | -     | 430   | 430    |\\n|           | All        | 4,386 | 1,376 | 572   | 6,334  |\\n\\n| Dataset   | # of Edits | 2-hop | 3-hop | 4-hop | Total |\\n|-----------|------------|-------|-------|-------|-------|\\n| MQAKE-CF-EMASTERED-6334 | 1          | 1,971 | 77    | 0     | 2,048 |\\n|           | 2          | 2,415 | 476   | 14    | 2,905 |\\n|           | 3          | -     | 823   | 128   | 951    |\\n|           | 4          | -     | -     | 430   | 430    |\\n|           | All        | 4,386 | 1,376 | 572   | 6,334  |\"}"}
{"id": "iTUlYblV0K", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B Extended Auditing\\n\\nB.1 Example of Inner Contamination between Different Edited Cases (\u00a73.2)\\n\\nAgain, we walk through two cases from MQ with KE-3 as a concrete example. First, we show them in their unedited format (again, subquestion breakdowns and intermediate answers are here for demonstration purposes and are not visible to the question-answering LLM during evaluation):\\n\\n\u2022 case_id:1570 (unedited):\\n  - Who was the creator of the official language used in the work location of Matti Vanhanen?\\n    - Which city did Matti Vanhanen work in? Helsinki.\\n    - What is the official language of Helsinki? Finnish.\\n    - Who was Finnish created by? Mikael Agricola.\\n\\n\u2022 case_id:1968 (unedited):\\n  - Who created the official language of Housemarque's headquarters location?\\n    - Which city is the headquarter of Housemarque located in? Helsinki.\\n    - What is the official language of Helsinki? Finnish.\\n    - Who was Finnish created by? Mikael Agricola.\\n\\nSuppose case_id:1570 and case_id:1968 are both selected as editing cases, two triples containing the following knowledge will be available: \u201cThe official language of Helsinki is Black Speech\u201d (intended for case_id:1570), and \u201cFinnish was created by William Shakespeare\u201d (intended for case_id:1968), leading to the following edited breakdown.\\n\\n\u2022 case_id:1570 (edited):\\n  - Who was the creator of the official language used in the work location of Matti Vanhanen?\\n    - Which city did Matti Vanhanen work in? Helsinki.\\n    - What is the official language of Helsinki? Finnish Black Speech.\\n    - Who was Finnish Black Speech created by? J. R. R. Tolkien.\\n\\n\u2022 case_id:1968 (edited):\\n  - Who created the official language of Housemarque's headquarters location?\\n    - Which city is the headquarter of Housemarque located in? Helsinki.\\n    - What is the official language of Helsinki? Finnish.\\n    - Who was Finnish created by? Mikael Agricola William Shakespeare.\\n\\nMuch like the previous conflict between unedited and edited cases, these two edited cases share a common subquestion: \u201cWhat is the official language of Helsinki?\u201d However, such subquestion is edited in case_id:1570 while unedited in case_id:1968, causing unintended contamination.\"}"}
{"id": "iTUlYblV0K", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.2 Error Analysis of MQ\\n\\nTable 8: Error statistics of MQAKE-CF [Zhong et al., 2023] in terms of edited cases contaminating unedited cases \u00a73.1.\\n\\n| # of Contaminated Cases | 1-edit | 100-edit | 1000-edit | 2000-edit | 3000-edit | 5000-edit | 9218-edit |\\n|------------------------|--------|----------|-----------|-----------|-----------|-----------|-----------|\\n| Cases                  | 62     | 3307     | 5275      | 5110      | 4578      | 3346      | 0         |\\n| Subquestions           | 62     | 4525     | 8751      | 8989      | 8326      | 6364      | 0         |\\n\\nTable 9: Error statistics of MQAKE-CF [Zhong et al., 2023] in terms edited cases contaminating each others \u00a73.2.\\n\\n| # of Contaminated Cases | 1-edit | 100-edit | 1000-edit | 2000-edit | 3000-edit | 5000-edit | 9218-edit |\\n|------------------------|--------|----------|-----------|-----------|-----------|-----------|-----------|\\n| Cases                  | 0      | 8        | 192       | 441       | 732       | 1397      | 2873      |\\n| Subquestions           | 0      | 12       | 270       | 606       | 1027      | 1986      | 4250      |\\n\\nC Extended Remastering\\n\\nC.1 Contamination Free Subset: MQAKE-REMASTERED-CF-6334\\n\\nWhile MQAKE-R EMastered-CF with masking operation can well support memory-based knowledge editing methods, it will not be compatible with parameter-based methods. This is because, for parameter-based methods, the set of edited facts used for training and evaluation needs to be constant yet consistent with each other at all times; whereas dynamic masking cannot suffice as it is essentially adjusting the dataset on the fly during inference time.\\n\\nTo effectively evaluate parameter-based knowledge editing methods, we present MQAKE-REMASTERED-CF-6334. MQAKE-REMASTERED-CF-6334 is a dataset extracted from MQAKE-CF, where all 6,334 cases are edited cases; and they are completely contamination-free from each other. This dataset is suitable for LLM editing with parameter-based approaches, as one can make careful splits among the 6,334 cases of MQAKE-REMASTERED-CF-6334 to serve as train, validation, and evaluation sets.\"}"}
{"id": "iTUlYblV0K", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D. Extended Benchmark and Discussion\\n\\n533\\n\\n534\\n\\n535\\n\\n536\\n\\n537\\n\\n538\\n\\n539\\n\\n540\\n\\n541\\n\\n542\\n\\n543\\n\\n544\\n\\n545\\n\\n546\\n\\n547\\n\\n548\\n\\n549\\n\\n550\\n\\n551\\n\\n552\\n\\n553\\n\\n554\\n\\n555\\n\\n556\\n\\n557\\n\\n558\\n\\n559\\n\\n560\\n\\n561\\n\\n562\\n\\n563\\n\\n564\\n\\n565\\n\\n566\\n\\n567\\n\\n568\\n\\n569\\n\\n570\\n\\n571\\n\\n572\\n\\n573\\n\\n574\\n\\n575\\n\\n576\\n\\n577\\n\\n578\\n\\n579\\n\\n580\\n\\n581\\n\\n582\\n\\n583\\n\\n584\\n\\n585\\n\\n586\\n\\n587\\n\\n588\\n\\n589\\n\\n590\\n\\n591\\n\\n592\\n\\n593\\n\\n594\\n\\n595\\n\\n596\\n\\n597\\n\\n598\\n\\n599\\n\\n600\\n\\n601\\n\\n602\\n\\n603\\n\\n604\\n\\n605\\n\\n606\\n\\n607\\n\\n608\\n\\n609\\n\\n610\\n\\n611\\n\\n612\\n\\n613\\n\\n614\\n\\n615\\n\\n616\\n\\n617\\n\\n618\\n\\n619\\n\\n620\\n\\n621\\n\\n622\\n\\n623\\n\\n624\\n\\n625\\n\\n626\\n\\n627\\n\\n628\\n\\n629\\n\\n630\\n\\n631\\n\\n632\\n\\n633\\n\\n634\\n\\n635\\n\\n636\\n\\n637\\n\\n638\\n\\n639\\n\\n640\\n\\n641\\n\\n642\\n\\n643\\n\\n644\\n\\n645\\n\\n646\\n\\n647\\n\\n648\\n\\n649\\n\\n650\\n\\n651\\n\\n652\\n\\n653\\n\\n654\\n\\n655\\n\\n656\\n\\n657\\n\\n658\\n\\n659\\n\\n660\\n\\n661\\n\\n662\\n\\n663\\n\\n664\\n\\n665\\n\\n666\\n\\n667\\n\\n668\\n\\n669\\n\\n670\\n\\n671\\n\\n672\\n\\n673\\n\\n674\\n\\n675\\n\\n676\\n\\n677\\n\\n678\\n\\n679\\n\\n680\\n\\n681\\n\\n682\\n\\n683\\n\\n684\\n\\n685\\n\\n686\\n\\n687\\n\\n688\\n\\n689\\n\\n690\\n\\n691\\n\\n692\\n\\n693\\n\\n694\\n\\n695\\n\\n696\\n\\n697\\n\\n698\\n\\n699\\n\\n700\\n\\n701\\n\\n702\\n\\n703\\n\\n704\\n\\n705\\n\\n706\\n\\n707\\n\\n708\\n\\n709\\n\\n710\\n\\n711\\n\\n712\\n\\n713\\n\\n714\\n\\n715\\n\\n716\\n\\n717\\n\\n718\\n\\n719\\n\\n720\\n\\n721\\n\\n722\\n\\n723\\n\\n724\\n\\n725\\n\\n726\\n\\n727\\n\\n728\\n\\n729\\n\\n730\\n\\n731\\n\\n732\\n\\n733\\n\\n734\\n\\n735\\n\\n736\\n\\n737\\n\\n738\\n\\n739\\n\\n740\\n\\n741\\n\\n742\\n\\n743\\n\\n744\\n\\n745\\n\\n746\\n\\n747\\n\\n748\\n\\n749\\n\\n750\\n\\n751\\n\\n752\\n\\n753\\n\\n754\\n\\n755\\n\\n756\\n\\n757\\n\\n758\\n\\n759\\n\\n760\\n\\n761\\n\\n762\\n\\n763\\n\\n764\\n\\n765\\n\\n766\\n\\n767\\n\\n768\\n\\n769\\n\\n770\\n\\n771\\n\\n772\\n\\n773\\n\\n774\\n\\n775\\n\\n776\\n\\n777\\n\\n778\\n\\n779\\n\\n780\\n\\n781\\n\\n782\\n\\n783\\n\\n784\\n\\n785\\n\\n786\\n\\n787\\n\\n788\\n\\n789\\n\\n790\\n\\n791\\n\\n792\\n\\n793\\n\\n794\\n\\n795\\n\\n796\\n\\n797\\n\\n798\\n\\n799\\n\\n800\\n\\n801\\n\\n802\\n\\n803\\n\\n804\\n\\n805\\n\\n806\\n\\n807\\n\\n808\\n\\n809\\n\\n810\\n\\n811\\n\\n812\\n\\n813\\n\\n814\\n\\n815\\n\\n816\\n\\n817\\n\\n818\\n\\n819\\n\\n820\\n\\n821\\n\\n822\\n\\n823\\n\\n824\\n\\n825\\n\\n826\\n\\n827\\n\\n828\\n\\n829\\n\\n830\\n\\n831\\n\\n832\\n\\n833\\n\\n834\\n\\n835\\n\\n836\\n\\n837\\n\\n838\\n\\n839\\n\\n840\\n\\n841\\n\\n842\\n\\n843\\n\\n844\\n\\n845\\n\\n846\\n\\n847\\n\\n848\\n\\n849\\n\\n850\\n\\n851\\n\\n852\\n\\n853\\n\\n854\\n\\n855\\n\\n856\\n\\n857\\n\\n858\\n\\n859\\n\\n860\\n\\n861\\n\\n862\\n\\n863\\n\\n864\\n\\n865\\n\\n866\\n\\n867\\n\\n868\\n\\n869\\n\\n870\\n\\n871\\n\\n872\\n\\n873\\n\\n874\\n\\n875\\n\\n876\\n\\n877\\n\\n878\\n\\n879\\n\\n880\\n\\n881\\n\\n882\\n\\n883\\n\\n884\\n\\n885\\n\\n886\\n\\n887\\n\\n888\\n\\n889\\n\\n890\\n\\n891\\n\\n892\\n\\n893\\n\\n894\\n\\n895\\n\\n896\\n\\n897\\n\\n898\\n\\n899\\n\\n900\\n\\n901\\n\\n902\\n\\n903\\n\\n904\\n\\n905\\n\\n906\\n\\n907\\n\\n908\\n\\n909\\n\\n910\\n\\n911\\n\\n912\\n\\n913\\n\\n914\\n\\n915\\n\\n916\\n\\n917\\n\\n918\\n\\n919\\n\\n920\\n\\n921\\n\\n922\\n\\n923\\n\\n924\\n\\n925\\n\\n926\\n\\n927\\n\\n928\\n\\n929\\n\\n930\\n\\n931\\n\\n932\\n\\n933\\n\\n934\\n\\n935\\n\\n936\\n\\n937\\n\\n938\\n\\n939\\n\\n940\\n\\n941\\n\\n942\\n\\n943\\n\\n944\\n\\n945\\n\\n946\\n\\n947\\n\\n948\\n\\n949\\n\\n950\\n\\n951\\n\\n952\\n\\n953\\n\\n954\\n\\n955\\n\\n956\\n\\n957\\n\\n958\\n\\n959\\n\\n960\\n\\n961\\n\\n962\\n\\n963\\n\\n964\\n\\n965\\n\\n966\\n\\n967\\n\\n968\\n\\n969\\n\\n970\\n\\n971\\n\\n972\\n\\n973\\n\\n974\\n\\n975\\n\\n976\\n\\n977\\n\\n978\\n\\n979\\n\\n980\\n\\n981\\n\\n982\\n\\n983\\n\\n984\\n\\n985\\n\\n986\\n\\n987\\n\\n988\\n\\n989\\n\\n990\\n\\n991\\n\\n992\\n\\n993\\n\\n994\\n\\n995\\n\\n996\\n\\n997\\n\\n998\\n\\n999\\n\\n1000\\n\\n1001\\n\\n1002\\n\\n1003\\n\\n1004\\n\\n1005\\n\\n1006\\n\\n1007\\n\\n1008\\n\\n1009\\n\\n1010\\n\\n1011\\n\\n1012\\n\\n1013\\n\\n1014\\n\\n1015\\n\\n1016\\n\\n1017\\n\\n1018\\n\\n1019\\n\\n1020\\n\\n1021\\n\\n1022\\n\\n1023\\n\\n1024\\n\\n1025\\n\\n1026\\n\\n1027\\n\\n1028\\n\\n1029\\n\\n1030\\n\\n1031\\n\\n1032\\n\\n1033\\n\\n1034\\n\\n1035\\n\\n1036\\n\\n1037\\n\\n1038\\n\\n1039\\n\\n1040\\n\\n1041\\n\\n1042\\n\\n1043\\n\\n1044\\n\\n1045\\n\\n1046\\n\\n1047\\n\\n1048\\n\\n1049\\n\\n1050\\n\\n1051\\n\\n1052\\n\\n1053\\n\\n1054\\n\\n1055\\n\\n1056\\n\\n1057\\n\\n1058\\n\\n1059\\n\\n1060\\n\\n1061\\n\\n1062\\n\\n1063\\n\\n1064\\n\\n1065\\n\\n1066\\n\\n1067\\n\\n1068\\n\\n1069\\n\\n1070\\n\\n1071\\n\\n1072\\n\\n1073\\n\\n1074\\n\\n1075\\n\\n1076\\n\\n1077\\n\\n1078\\n\\n1079\\n\\n1080\\n\\n1081\\n\\n1082\\n\\n1083\\n\\n1084\\n\\n1085\\n\\n1086\\n\\n1087\\n\\n1088\\n\\n1089\\n\\n1090\\n\\n1091\\n\\n1092\\n\\n1093\\n\\n1094\\n\\n1095\\n\\n1096\\n\\n1097\\n\\n1098\\n\\n1099\\n\\n1100\\n\\n1101\\n\\n1102\\n\\n1103\\n\\n1104\\n\\n1105\\n\\n1106\\n\\n1107\\n\\n1108\\n\\n1109\\n\\n1110\\n\\n1111\\n\\n1112\\n\\n1113\\n\\n1114\\n\\n1115\\n\\n1116\\n\\n1117\\n\\n1118\\n\\n1119\\n\\n1120\\n\\n1121\\n\\n1122\\n\\n1123\\n\\n1124\\n\\n1125\\n\\n1126\\n\\n1127\\n\\n1128\\n\\n1129\\n\\n1130\\n\\n1131\\n\\n1132\\n\\n1133\\n\\n1134\\n\\n1135\\n\\n1136\\n\\n1137\\n\\n1138\\n\\n1139\\n\\n1140\\n\\n1141\\n\\n1142\\n\\n1143\\n\\n1144\\n\\n1145\\n\\n1146\\n\\n1147\\n\\n1148\\n\\n1149\\n\\n1150\\n\\n1151\\n\\n1152\\n\\n1153\\n\\n1154\\n\\n1155\\n\\n1156\\n\\n1157\\n\\n1158\\n\\n1159\\n\\n1160\\n\\n1161\\n\\n1162\\n\\n1163\\n\\n1164\\n\\n1165\\n\\n1166\\n\\n1167\\n\\n1168\\n\\n1169\\n\\n1170\\n\\n1171\\n\\n1172\\n\\n1173\\n\\n1174\\n\\n1175\\n\\n1176\\n\\n1177\\n\\n1178\\n\\n1179\\n\\n1180\\n\\n1181\\n\\n1182\\n\\n1183\\n\\n1184\\n\\n1185\\n\\n1186\\n\\n1187\\n\\n1188\\n\\n1189\\n\\n1190\\n\\n1191\\n\\n1192\\n\\n1193\\n\\n1194\\n\\n1195\\n\\n1196\\n\\n1197\\n\\n1198\\n\\n1199\\n\\n1200\\n\\n1201\\n\\n1202\\n\\n1203\\n\\n1204\\n\\n1205\\n\\n1206\\n\\n1207\\n\\n1208\\n\\n1209\\n\\n1210\\n\\n1211\\n\\n1212\\n\\n1213\\n\\n1214\\n\\n1215\\n\\n1216\\n\\n1217\\n\\n1218\\n\\n1219\\n\\n1220\\n\\n1221\\n\\n1222\\n\\n1223\\n\\n1224\\n\\n1225\\n\\n1226\\n\\n1227\\n\\n1228\\n\\n1229\\n\\n1230\\n\\n1231\\n\\n1232\\n\\n1233\\n\\n1234\\n\\n1235\\n\\n1236\\n\\n1237\\n\\n1238\\n\\n1239\\n\\n1240\\n\\n1241\\n\\n1242\\n\\n1243\\n\\n1244\\n\\n1245\\n\\n1246\\n\\n1247\\n\\n1248\\n\\n1249\\n\\n1250\\n\\n1251\\n\\n1252\\n\\n1253\\n\\n1254\\n\\n1255\\n\\n1256\\n\\n1257\\n\\n1258\\n\\n1259\\n\\n1260\\n\\n1261\\n\\n1262\\n\\n1263\\n\\n1264\\n\\n1265\\n\\n1266\\n\\n1267\\n\\n1268\\n\\n1269\\n\\n1270\\n\\n1271\\n\\n1272\\n\\n1273\\n\\n1274\\n\\n1275\\n\\n1276\\n\\n1277\\n\\n1278\\n\\n1279\\n\\n1280\\n\\n1281\\n\\n1282\\n\\n1283\\n\\n1284\\n\\n1285\\n\\n1286\\n\\n"}
{"id": "iTUlYblV0K", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method                      | MQUAKE-R | EMASTERED-CF | 3K                |\\n|----------------------------|----------|--------------|-------------------|\\n| vicuna-7b-v1.5 [Zheng et al., 2023b] | 16.54 (100, 16.51) | 18 (9.0, 18.31) | 14.63 (8.0, 17.95) |\\n| MeLLo [Zhong et al., 2023]  | 16.51    | 18.31        | 17.95             |\\n| ICE [Cohen et al., 2023]    | <1       | OOM          | OOM               |\\n| IKE [Zheng et al., 2023a]   | <1       | OOM          | OOM               |\\n| GWalk (Ours)                | 54.89 (100, 54.87) | 60.9 (54, 61.14) | 57.37 (54.4, 58.85) |\\n| Mistral-7B-Instruct-v0.2 [Jiang et al., 2023] | 19.73 (100, 19.71) | 18.6 (21, 18.52) | 15.93 (17.8, 15.6) |\\n| MeLLo [Zhong et al., 2023]  | <1       | OOM          | OOM               |\\n| ICE [Cohen et al., 2023]    | <1       | OOM          | OOM               |\\n| IKE [Zheng et al., 2023a]   | <1       | 4.43 (4, 4.49) | OOM               |\\n| GWalk (Ours)                | 56.57 (100, 56.55) | 61.93 (47, 62.45) | 57.17 (51.5, 60.0) |\\n| Meta-Llama-3-8B-Instruct     | <1       | <1           | 2.3               |\\n| ICE [Cohen et al., 2023]    | <1       | OOM          | OOM               |\\n| IKE [Zheng et al., 2023a]   | <1       | OOM          | OOM               |\\n| GWalk (Ours)                | 69.0 (100, 68.99) | 76.73 (67, 77.07) | 75.47 (74.2, 76.1) |\\n\\n*Results inside the parenthesis are edited cases accuracy and unedited cases accuracy, respectively.*\"}"}
{"id": "iTUlYblV0K", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method                                  | MQ UAKE-R | EMASTERED-T | 1-edit | 100-edit | 500-edit | 1864-edit |\\n|----------------------------------------|-----------|-------------|--------|----------|----------|-----------|\\n| vicuna-7b-v1.5 [Zheng et al., 2023b]   | 19.31     | (100, 19.27)| 18.88  | (45.0, 17.4)| 22.16   | (40.4, 15.47)|\\n| MeLLo [Zhong et al., 2023]             |           |             | 44.37  | (44.37, N/A) |         |           |\\n| ICE [Cohen et al., 2023]               |           |             | <1     | <1        | <1       | OOM       |\\n| IKE [Zheng et al., 2023a]              |           |             | <1     | <1        | <1       | OOM       |\\n| GWalk (Ours)                           | 35.52     | (100, 35.48)| 46.51  | (49.0, 46.37)| 48.93   | (56.0, 46.33)|\\n| Mistral-7B-Instruct-v0.2 [Jiang et al., 2023] | 10.3     | (0, 10.31)  | 10.25  | (59.0, 7.48) | 18.78   | (48.4, 7.92) |\\n| MeLLo [Zhong et al., 2023]             |           |             | <1     | <1        | <1       | OOM       |\\n| ICE [Cohen et al., 2023]               |           |             | <1     | <1        | <1       | OOM       |\\n| IKE [Zheng et al., 2023a]              |           |             | <1     | <1        | <1       | OOM       |\\n| GWalk (Ours)                           | 34.07     | (0, 34.08)  | 45.76  | (47, 45.69)| 46.78   | (51.2, 45.16)|\\n| Meta-Llama-3-8B-Instruct [AI@Meta, 2024] | <1     | 1.13        | (17, 0.23)| 4.72     | (17.4, <1)| 16.58    | (16.58, N/A) |\\n| MeLLo [Zhong et al., 2023]             |           |             | <1     | <1        | <1       | OOM       |\\n| ICE [Cohen et al., 2023]               |           |             | <1     | <1        | <1       | OOM       |\\n| IKE [Zheng et al., 2023a]              |           |             | <1     | <1        | <1       | OOM       |\\n| GWalk (Ours)                           | 70.12     | (100, 70.1)| 73.28  | (84.0, 72.68)| 76.61   | (87, 72.8) |\\n|                                        |           |             |        |           |          |           |\\n\\n*Results inside the parenthesis are edited cases accuracy and unedited cases accuracy, respectively.*\"}"}
{"id": "iTUlYblV0K", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method                        | 100-edit | 1000-edit | 3000-edit | 6344-edit |\\n|-------------------------------|----------|-----------|-----------|-----------|\\n| vicuna-7b-v1.5 [Zheng et al., 2023b] |          |           |           |           |\\n| MeLLo [Zhong et al., 2023]    | 19.16    | (0, 10.99, 19.37) | 19.27    | (5.1, 9.58, 24.53) |\\n| ICE [Cohen et al., 2023]      |          |           |           |           |\\n| IKE [Zheng et al., 2023a]     |          |           |           |           |\\n| PokeMQA [Gu et al., 2024]     |          |           | - - -     | 21.77     |\\n| GWalk (Ours)                  | 57.55    | (22.22, 64.84, 57.48) | 61.79    | (29.08, 66.17, 63.23) |\\n| Mistral-7B-Instruct-v0.2 [Jiang et al., 2023] | 27.5    | (<1, 23.08, 27.65) | 27.54    | (12.76, 24.0, 30.40) |\\n| ICE [Cohen et al., 2023]      |          |           |           |           |\\n| IKE [Zheng et al., 2023a]     | 8.82     | (11.11, 6.59, 8.86) | OOM OOM OOM |          |\\n| PokeMQA [Gu et al., 2024]     |          |           | - - -     | 20.38     |\\n| GWalk (Ours)                  | 56.25    | (33.33, 57.14, 56.28) | 58.9     | (34.69, 60.57, 60.60) |\\n| Meta-Llama-3-8B-Instruct      |          |           |           |           |\\n| MeLLo [Zhong et al., 2023]    | <1 <1 1.12 | (1.17, 1.48, 0.22) | 1.27     | (<1, 1.4, 1.59) |\\n| ICE [Cohen et al., 2023]      |          |           |           |           |\\n| IKE [Zheng et al., 2023a]     | <1 OOM OOM OOM |                 |         |\\n| PokeMQA [Gu et al., 2024]     |          |           | - - -     | 20.38     |\\n| GWalk (Ours)                  | 67.01    | (33.33, 74.73, 66.92) | 71.89    | (47.45, 80.94, 70.65) |\\n|                                | 73.76    | (54.05, 81.60, 71.12) | 74.22    | (61.02, 80.47, 73.02) |\\n\\n*Results inside the parenthesis are edited cases (unique in the test set) accuracy, edited cases (overlap of the test and train set) accuracy, and unedited cases accuracy, respectively.\"}"}
{"id": "iTUlYblV0K", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Specifically, GMeLLo [Anonymous, 2024] briefly discusses the inconsistency between the triple chain and the generated multi-hop questions in its \u00a74.5.1, which is the same type of error we discussed in \u00a73.4. We note that GMeLLo merely highlights such errors but does not provide a quantified measurement of its scale nor any fix. We did both in \u00a73.4 and \u00a74.1.\\n\\nDeepEdit [Wang et al., 2024] discovered the same inner contamination error as we discussed in \u00a73.2. DeepEdit does provide a quantified measurement of the scale of such error but only pertains to the MQUAKE-CF-3K dataset, and such quantifiable results are only valid when all 3,000 cases of MQUAKE-CF-3K are considered edited; which, as shown in Table 5, only constitute one column of MQUAKE-CF-3K\u2019s reporting. Further, DeepEdit provides a rather hardcore fix to this problem by removing the 998 inner contaminated cases from the MQUAKE-CF-3K dataset \u2014 which is (supposedly) the same 998 cases we detect in Table 2 under the 3000-edit column \u2014 with the post-fix dataset denoted as MQUAKE-2002 for having 2,002 out of 3,000 cases left. While this fix is, of course, helpful, we argue our post-fix MQUAKEREMASTERED-CF, MQUAKEREMASTERED, and MQUAKEREMASTERED-T are much more comprehensive and effective since they patched many more errors revealed in \u00a73 (which still exists in MQUAKE-2002), works outside the MQUAKE-CF-3K dataset, do not require the number of edits to be 2,002 cases, and most importantly, done so without scarifying almost 1/3 of the capacity of the original dataset.\"}"}
