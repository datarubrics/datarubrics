{"id": "jSKtxmxc0M", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VideoGUI: A Benchmark for GUI Automation from Instructional Videos\\n\\nKevin Qinghong Lin, Linjie Li, Difei Gao, Qinchen Wu, Mingyi Yan, Zhengyuan Yang, Lijuan Wang, Mike Zheng Shou\\n\\n1 Show Lab, National University of Singapore\\n2 Microsoft\\n\\nAbstract\\n\\nGraphical User Interface (GUI) automation holds significant promise for enhancing human productivity by assisting with computer tasks. Existing task formulations primarily focus on simple tasks that can be specified by a single, language-only instruction, such as \u201cInsert a new slide.\u201d In this work, we introduce VideoGUI, a novel multi-modal benchmark designed to evaluate GUI assistants on visual-centric GUI tasks. Sourced from high-quality web instructional videos, our benchmark focuses on tasks involving professional and novel software (e.g., Adobe Photoshop or Stable Diffusion WebUI) and complex activities (e.g., video editing). VideoGUI evaluates GUI assistants through a hierarchical process, allowing for identification of the specific levels at which they may fail: (i) high-level planning: reconstruct procedural subtasks from visual conditions without language descriptions; (ii) middle-level planning: generate sequences of precise action narrations based on visual state (i.e., screenshot) and goals; (iii) atomic action execution: perform specific actions such as accurately clicking designated elements. For each level, we design evaluation metrics across individual dimensions to provide clear signals, such as individual performance in clicking, dragging, typing, and scrolling for atomic action execution. Our evaluation on VideoGUI reveals that even the SoTA large multimodal model GPT4o performs poorly on visual-centric GUI tasks, especially for high-level planning. The data and code are available at https://github.com/showlab/videogui.\\n\\n1 Introduction\\n\\nIn the digital age, individuals rely on computers for a vast array of daily activities (e.g., web browsing, entertainment etc.). These activities often necessitate the use of diverse software, which are accessed primarily through Graphical User Interfaces (GUI). Large language models (LLMs) [1], which excel in understanding complex language instructions and integrating various tools seamlessly, have shown great potential in GUI automation [2, 3, 4, 5]. They could streamline the navigation of digital interfaces and significantly enhance productivity, e.g., assisting slide template creation in Powerpoint with just a few keywords [2].\\n\\nRecently, notable efforts have been made in GUI automation evaluation, benchmarking model performances on Web [4, 6, 7] or Smartphone GUI navigation [8, 9], given screenshots or HTML codes [10, 11]. Follow-up works [12, 13, 14] develop executable environments with well-defined action spaces, which removes the dependencies on pre-defined inputs. Nonetheless, most existing GUI benchmarks [15, 16] restrict their applications to simpler domains and tasks that can be described with a single text instruction (e.g., \u201cInsert a new slide on the second page\u201d). In real-world scenarios, users rarely struggle with basic operations that can be clearly described in text. Rather, they often encounter difficulties in performing novel and advanced tasks (e.g., \u201cCreate a special animation effects in powerpoint\u201d), which extend far beyond basic operations, and rely more on visual signals than text instructions to complete such tasks.\"}"}
{"id": "jSKtxmxc0M", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: A brief illustration of VideoGUI. VideoGUI focuses on professional and novel software like PR, AE for video editing, and Stable Diffusion, Runway for visual creation. We source tasks from high-quality instructional videos (see an example here), with annotators replicating these to reproduce effects; We provide detailed annotations with planning procedures and recorded actions for hierarchical evaluation.\\n\\nInspired by the abundant instructional videos that teach average users for performing novel and complex GUI tasks, we introduce VideoGUI, a new multi-modal GUI benchmark derived from high-quality web instructional videos. As shown in Fig. 1, VideoGUI provides high-quality annotations by having participants reproducing the instructional videos, capturing multi-level labels from procedural planning to atomic actions with element locations. VideoGUI covers 11 visual-centric software applications and features 86 complex tasks (averaging 22.7 actions each) and 463 subtasks, alongside hierarchical manual planning and 2.7K manual action annotations (Tab. 1).\\n\\nWith VideoGUI, we propose a comprehensive evaluation suite for GUI assistants via a hierarchical process: (i) high-level planning involves reconstructing procedural subtasks from visual cues without language descriptions; (ii) middle-level planning details the steps for completing a subtask with a sequence of precise action narrations based on visual state and textual query; (iii) atomic action execution is to perform the target actions (e.g., click on the designated element). For each level, we design evaluation metrics across individual dimensions to assess model performance, which help to pinpoint model limitations.\\n\\nWe conduct comprehensive evaluation of SoTA large multimodal models (LMMs) on VideoGUI, and find that even the current best model GPT-4 fails to complete a single full task in our benchmark. Our empirical results show that the bottleneck surprisingly lies in planning rather than action execution, even though GPT-4 is not known for grounding. Moreover, planning from textual queries is much easier than planning from visual previews for almost all models evaluated, which further implies the difficulty of visual-centric GUI tasks. Our findings shed lights on the directions for developing the next generation of models or agent systems towards GUI automation.\"}"}
{"id": "jSKtxmxc0M", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison of VideoGUI with existing GUI datasets. VideoGUI differs from existing benchmarks in: (i) sourcing from instructional videos with human demonstrations; (ii) featuring 86 challenging full tasks averaging 22.7 actions, and 463 subtasks; (iii) offering comprehensive evaluation with hierarchical planning and action categories.\\n\\nRecent studies have highlighted the promising potential of LLMs beyond language modeling. Notable advancements in Chain of Thought (CoT) and ReAct strategies have demonstrated LLMs' capabilities as autonomous agents, capable of completing complex tasks through dynamic programming. Motivated by these progresses, follow-up works connect LLMs with visual experts to enable multimodal applications, such as visual question answering or image editing applications. In the realm of Embodied GUI tasks, the primary challenges involve understanding complex UI elements and planning to execute diverse tasks. This has led to the development of approaches such as: (i) Training-free agent systems, which primarily consist of two stages: the first involves semantically understanding UI elements, either by transforming the GUI into HTML representations or language descriptions, or using off-the-shelf visual models like OCR and SoM. The second stage involves utilizing LLMs to integrate information and generate responses. This method heavily relies on closed-source LLMs, incurring significant costs. Additionally, it limits the model's UI visual perception abilities, such as demonstrating goals or state transitions visually rather than linguistically. (ii) Vision-Language-Action models, which are pretrained on large-scale GUI vision-text corpus (e.g., screenshots). This enables the LLMs to obtain more abilities such as element grounding and reasoning in unified responses. However, it remains unclear when and how to employ different types of GUI agents or tools. VideoGUI provides a comprehensive suite for studying and benchmarking these models.\\n\\n3 VideoGUI Benchmarks\\n\\n3.1 Data Construction\\n\\nData source. VideoGUI consists of 11 software applications, categorized into: (i) media creation, featuring visual and animation tools like PowerPoint, Runway, and Stable Diffusion; (ii) media editing, including Adobe Photoshop, Premiere Pro, After Effects, CapCut, and DaVinci Resolve; (iii) media browsing, with platforms like YouTube, VLC Player, and Web Stock.\\n\\nPipeline. The VideoGUI creation pipeline is illustrated in Fig.2. For each software, (i) we manually select instructional videos paired with high-quality transcripts from YouTube, focusing on those teaching practical and novel usages. To collect the human manipulation trajectory, we build a simulated environment to monitor user behaviors including Click, Drag, Type/Press, and Scroll. (ii) We invite five participants who first watch the selected video and then try to reproduce the effects shown using our simulator, which records all cursor and keyboard activities (e.g., [x, y] coordinates).\"}"}
{"id": "jSKtxmxc0M", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Illustration of VideoGUI creation pipeline, encompassing four phases:\\n\\n(i) High-quality instruction videos are manually selected,\\n(ii) Participants replicate skills demonstrated in videos,\\n(iii) Participants annotate task elements and procedures,\\n(iv) Annotated data is validated manually for VideoGUI benchmarking use.\\n\\nAfterward, they provide a brief description of the overall goal for the full task, which can be optionally used as text query during evaluation. Then the operations shown in the video is broken down into several subtasks and annotated with textual descriptions, each focusing on a main functionality operation (e.g., inserting a figure).\\n\\n(iii) We also instruct the annotators to identify the active elements (e.g., buttons 'Insert') for each action, as they are not automatically identified and recorded by our simulator. After the demonstration, we retain all available files, including material, project files, and visual outcomes (the latter being our full-task's visual query).\\n\\n(iv) The participants cross-validate the annotations, and remove unclear/incorrect ones.\\n\\nData statistic. Overall, VideoGUI includes 178 tasks across 11 software applications (Fig. 3a) on Windows and Web browsers (Chrome, Edge, Firefox). It comprises 86 complex tasks (i.e., full task) and 92 simple tasks (i.e., subtask) that do not require high-level planning, where those 86 full tasks can be further divided into 371 subtasks, resulting in a total of 463 subtasks. Fig. 3b shows the distribution of number of actions per task. In total, we collect 2,712 atomic manual actions. As shown in Fig. 3c, the most common action is LeftClick (66.2%), while RightClick and Scroll are the least common actions (approximately 2%).\\n\\n(a) Dist. of Software taxonomy.\\n\\n(b) Num. of Action per task.\\n\\n(c) Dist. of Atomic actions.\\n\\nFigure 3: Data statistics of VideoGUI.\\n\\n3.2 Evaluation and Metrics\\nOverview. Imagine a human to complete the complex task illustrated in Tab. 2, we often first break down the full task into sub-tasks, and then sequentially perform the actions required to complete each subtask. Existing GUI benchmarks [6, 17, 18] predominantly use a boolean metric (i.e., Success Rate) to measure the success of completing a task. It may work okay for simpler tasks involving only a few actions, but is clearly not sufficient in providing feedback on where the models fall short, especially as the complexity of the task increases (e.g., a full task with over 100 actions), and nonetheless to say to guide future improvements in modeling for GUI navigation.\\n\\nTo address this, we propose a hierarchical decomposition of tasks into three key stages: A. High-level Planning, which translates task instructions or reverse engineers final outcomes into several key milestones. B. Middle-level Planning, which converts each milestone into detailed action narrations. C. Atomic-level Execution, which focuses on accurately executing specific actions, such as clicking and typing, as dictated by the narration. The whole evaluation scheme is shown in Fig. 4. Each part is discussed in detail subsequently.\\n\\n\u25cf High-level Planning. This method translates instructions or outcomes into key milestones (i.e., subtasks). Unlike previous approaches that start with explicit textual queries, practical scenarios often rely on final visual demonstrations like animations, requiring the reconstruction of past procedural tasks. Accordingly, we develop three categories based on different modal inputs:\"}"}
{"id": "jSKtxmxc0M", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Visual query: How to transform from [start] to [end] in Premiere Pro?\\n\\nTextual query: Change the blue sky in the background of the picture to the red color at sunset.\\n\\na. Add ultra key effect to the video\\nb. Get the color of the background\\nc. Adjust the track order to the second track\\nd. Add the new background photo to the first video track;\\n\\na1. Click on Effects panel\\na2. Click on search bar in Effects panel\\na3. Keyboard Type \u2018ultra\u2019\\na4. Click on \u2018Ultra Key\u2019\\na5. Drag Ultry key effect from effects panel to the video. (Purpose: add ultra key to the video)\\n\\nTable 2: Hierarchical annotations in VideoGUI (Premiere Pro). The top row displays the video input and the desired task outcome as the visual query, with an optional textual query describing the video editing effect. The model is expected to \u201creverse-engineer\u201d this outcome through a hierarchical process: first by planning high-level milestones (i.e., sub-tasks), then detailing each milestone into step-by-step narrations at the middle level, and finally translating these narrations into executable actions.\\n\\n\u2022 Visual query is our primary setting, with only a visual preview are provided, for example, two photos before and after editing with Photoshop or an animation effect created in PowerPoint.\\n\u2022 Textual query explicitly defines the objectives using detailed descriptions.\\n\u2022 Visual query + Textual query, which provides the most complete information.\\n\\nMetrics: Planning involves open-ended question-answering with multiple correct approaches, making traditional metrics insufficient. To adaptively evaluate model responses, we define a critic using GPT-4-Turbo [37] inspired by [38], prompting LLM to focus on key elements and operations such as 3D shape and specific animation types. We score the model\u2019s generated procedure steps against the ground truth on a scale from 0 (totally unrelated) to 5 (almost correct).\\n\\nMiddle-level Planning. Given a milestone task, the agent should perform appropriate UI operations based on its observation state (e.g., screenshots). This stage aims to generate a sequence of precise action narrations (i.e., desired action type with an accurate element) by combining textual milestones and visual observations. We devise three modes:\\n\\n\u2022 Visual initial state + Textual query: Our main setting, as it accepts the output from the previous high-level planning, and the initial state (i.e., screenshot) can be straightforwardly obtained.\\n\u2022 Textual query: A common setting in most existing works.\\n\u2022 Visual state transition (initial and end state): the most challenging setting requiring the model to understand differences by screenshot transition and reverse its fine-grained actions.\\n\\nMetrics: similar to the high-level planning phrase, we use the LLM as the critic for scoring.\\n\\nAtomic-action Execution. After planning, the agent should respond to the action narrations with middle-level planning. We evaluate whether the model can accurately interpret narrations and perform the corresponding actions. Specifically, we consider the four most common action categories:\\n\\n\u2022 Click: For a narration like Click on the Insert button, the model must accurately localize desired element on the screenshot by providing a bounding box or click position [x, y].\\n\\nMetrics: (i) Dist := \u2206L, where \u2206 is the pixel difference between the predicted location and the ground-truth coordinate. L is the farthest distance from the ground-truth location to any of the four screenshot vertices for adaptive normalization. (ii) Recall@d: In practice, a click is usually valid when it falls within a very short distance, such as within a button area. We calculate the recall score with a threshold d, which we empirically set to 100.\\n\\n\u2022 Drag: can be approximated as a combination of a Click with a short-phrase movement, where the model must infer both the initial and end coordinates.\\n\\nMetrics: (i) Dist := 1/2 \u2206sLs + \u2206eLe by jointly measuring the start and end distance, then averaging them. (ii) Recall@d: This metric is more stricter than click, requiring both the start and end predictions to be within a distance d; otherwise, the score is 0.\\n\\n\u2022 Scroll: The scroll action assesses whether the desired elements are visible in the current screenshot, determining if a scroll action is needed (e.g., scroll up, scroll down, or no need).\"}"}
{"id": "jSKtxmxc0M", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Click: Where is the 'Effects'?\\n2. Scroll: Should I scroll up / down / not to find the text box?\\n3. Drag: How to drag SAMPLE TEXT from the last letter to the first letter.\\n4. Type / Press: Please type \\\"The cat scared the dog away\\\"\\n\\nHigh-level Planning:\\n\\nGT: A. Insert the title effect 'Clean and Simple Lower Third' with text 'The cat scared the dog away' at the beginning of the first clip.\\n\\nMiddle-level Planning:\\n\\nGT: A1. Click on Effects\\nA2. Click on Titles\\nA3. Drag Clean and Simple Lower Third from original position to the beginning of the cameraB.mp4. (Purpose: insert the title effect)\\nA4. Click on Inspector\\nA5. scroll up\\nA6. Drag SAMPLE TEXT from the last letter to the first letter. (Purpose: select all text)\\nA7. Keyboard Type 'The cat scared the dog away'\\n\\nGT_Start: [1645, 393]\\nGT_End: [1415, 381]\\n\\nMetrics: We frame this QA as a multiple-choice question: 'Could you advise whether I need to scroll to see [target element]?' with options: [A] No need to scroll; [B] Scroll up; [C] Scroll down. To prevent bias, we shuffle the choices randomly and calculate the accuracy score.\\n\\nType & Press: For type actions such as Type 'Hello world!', the agent must accurately produce the string through keystrokes. For commands like Ctrl+C, it must execute multiple keystrokes and button presses. Most GUI agents utilize PyAutoGUI for these operations, framing them as coding challenges that require verification for correctness.\\n\\nMetrics: We design a Sandbox scheme by developing a mini simulator that executes the code produced by the agent. Additionally, we use a monitor to listen for the keys pressed or typed. We then compare the monitored results with the ground-truth results to check for matches. This setting is evaluated using Recall (i.e., whether the GT is produced) and Precision (i.e., the count number of GT and actual outputs to study redundancy).\\n\\nExperiments:\\n\\n4.1 Baseline settings\\nWe evaluate leading Multi-modal Large Language Models (MLLMs) including GPT-4-Turbo, GPT-4o, Claude-3-Opus, Gemini-Pro-V, Qwen-VL-Max, and the open-source CogAgent. We also include text-only LLMs such as GPT-3.5-Turbo, LLama3-70B, and Mixtral-8x22B. Tab. 3 summarizes all evaluated models and their supported modalities.\"}"}
{"id": "jSKtxmxc0M", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"4.2 Main Results on VideoGUI\\n\\nIn Tab. 3, we provide a comprehensive evaluation of baseline models on VideoGUI. Scores are reported for high-level planning (visual query), middle-level planning (visual+text for MLLMs, or text only for LLMs), and atomic action (covering four categories), as well as an overall score summing these three. The planning score was originally evaluated on a scale from 0 to 5. We aimed to provide an overall score that considers both planning and action execution, with the latter being evaluated on a scale of 0 to 1. To achieve this, we normalized the planning scores by dividing by the maximum value of 5. The lowest scores in high-level planning across all models highlight the challenge posed by vision preview instructions. Overall, GPT-4o achieved the highest score of 39.4, excelling in all three tracks. In addition, we incorporate a few simple agent baselines, which use GPT-4T/GPT-4o for high-level/middle-level plan, while incorporate additional tools (i.e., OCR or SoM) to aid its action execution. The use of tools further boosts the overall model performance by $\\\\sim 3$ points for GPT-4o and $\\\\sim 5$ points for GPT-4T. We next dive into the detailed evaluation of procedural planning and action execution for a deeper analysis.\\n\\n| Model                  | High Plan | Mid. Plan | Action | Overall |\\n|------------------------|-----------|-----------|--------|---------|\\n| LLama3-70B [44]        | \u2713         | \u2013         | 40.5   | 20.3    |\\n| Mixtral-8x22B [45]     | \u2713         | \u2013         | 36.0   | 19.6    |\\n| GPT-3.5-Turbo [43]     | \u2713         | \u2013         | 49.1   | 22.3    |\\n| CogAgent [35]          | \u2713 \u2713       |           | 4.4    | 21.8    |\\n| Qwen-VL-Max [42]       | \u2713 \u2713 \u2713     |           | 5.1    | 35.7    |\\n| Gemini-Pro-Vision [41] | \u2713 \u2713 \u2713     |           | 7.9    | 28.6    |\\n| Claude-3-Opus [40]     | \u2713 \u2713 \u2713     |           | 9.7    | 45.6    |\\n| GPT-4-Turbo [37]       | \u2713 \u2713 \u2713     |           | 14.3   | 52.9    |\\n| GPT-4o [37]            | \u2713 \u2713 \u2713     |           | 17.1   | 53.5    |\\n| GPT-4T + OCR           | \u2713 \u2713 \u2713     |           | 14.3   | 52.9    |\\n| GPT-4T + SoM [32]      | \u2713 \u2713 \u2713     |           | 14.3   | 52.9    |\\n| GPT-4o + OCR           | \u2713 \u2713 \u2713     |           | 17.1   | 53.5    |\\n| GPT-4o + SoM [32]      | \u2713 \u2713 \u2713     |           | 17.1   | 53.5    |\\n\\nTable 3: Full evaluation on VideoGUI with Baselines and their supported interleaved instructions, which might be a text query, an image (1 frame), or a media (more than 1 frame) such as two photos, one or two videos. The bottom block features 4 simple agent baseline, which use GPT-4T/GPT-4o for high-level/middle-level plan, while incorporate additional tools (i.e., OCR or SoM) for action execution.\\n\\n| Model                  | High-level Planning (0\u22125) | Middle-level Planning (0\u22125) | Vision & Text Vision & Text Vision Text |\\n|------------------------|---------------------------|-----------------------------|----------------------------------------|\\n| LLama3-70B [44]        | \u2013 2.62                    | \u2013                           | \u2013                                      |\\n| Mixtral-8x22B [45]     | \u2013 2.43                    | \u2013                           | \u2013                                      |\\n| GPT-3.5-Turbo [43]     | \u2013 2.67                    | \u2013                           | \u2013                                      |\\n| CogAgent [35]          | 0.22 1.12                 | 1.23                        | 1.32                                   |\\n| Qwen-VL-Max [42]       | 0.25 2.30                 | 1.96                        | 0.70 1.72                              |\\n| Gemini-Pro-Vision [41] | 0.39 2.35                 | 1.45                        | 0.34 1.61                              |\\n| Claude-3-Opus [40]     | 0.48 2.54                 | 2.17                        | 0.66 2.26                              |\\n| GPT-4-Turbo [37]       | 0.71 2.57                 | 2.55                        | 1.49 2.57                              |\\n| GPT-4o [37]            | 0.86 2.68                 | 2.46                        | 1.78 2.45                              |\\n| Avg. by models         | 0.49 2.37                 | 1.97                        | 0.99 2.02                              |\\n\\nTable 4: Detailed evaluation on Procedural Planning, including both high-level and middle-level planning. Each level is evaluated across three types of query formulation as discussed in \u00a73.2 (i.e., vision, text, and vision & text). Columns highlighted with colors are the primary evaluation settings. The maximum score is 5.\\n\\nProcedural planning. Tab. 4 studies the impact of different query formulations for planning. On both high and middle-level:\\n\\n(i) The vision-only setting is significantly challenging (especially for high-level, 0.49 versus 2.37 for textual). Among the models, GPT-4o demonstrates the strongest visual reasoning ability.\\n(ii) All models, except CogAgent [35] with a small LLM [46], exhibit similar performance on textual-only inputs, as the textual query concretely indicates the key operations or effects type. This suggests that if we have clear and detailed textual instructions, a text LLM may be sufficient for this stage.\\n(iii) We do not observe a significant gain in the vision+text setting compared to text-only, which requires strong interleaved UI perception abilities.\\n\\nAction executions. Tab. 5 examines the impact of different atomic actions on model performance. We summarize our findings as below.\\n\\n(i) Click: We prompt multi-modal LLMs to output coordinates...\"}"}
{"id": "jSKtxmxc0M", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Detailed evaluation on Actions Executions.\\n\\nWe report model performance on four types of atomic action execution. The full score is the sum of Click recall, Drag recall, Type precision, and Scroll accuracy. Grd. indicates whether the model has explicit grounding ability such as output element's coordinates. In the bottom half, we equip LLMs with tools like SoM [32] and OCR [47].\\n\\n(ii) Drag: To perform Drag, it requires models to accurately localize the movement at both the start and end points. The best model, GPT4-o with OCR, yields only 11.3 recall. For LLMs with tools, OCR brings 8.8 recall gain over the base model, which is even more helpful than SoM as it helps to precisely localize text for the button, while SoM often suffers from poor segmentation results.\\n\\n(iii) Type / Press: Regarding keyboard activity, most models achieve good scores, as large-scale instruction-tuned LLMs generally is decent at coding, making the LLMs even more competent for this task.\\n\\n(iv) Scroll: For Scroll, models must infer not only whether an element appears but also its order relative to other elements. GPT-4o is the top-performing model, while Gemini scores extremely low, often preferring outputs without scrolling.\\n\\n4.3 Performance by Task Difficulty.\\n\\nHigh planning by different software.\\n\\nFig. 5 (top) shows mid-level plan scores (visual query) across different software. Models perform highest on Powerpoint, which is more commonly used than others. On \u00e6 and Photoshop, model performance drops significantly as they are professional software. It is worth mentioning that being web-based, Runway and Stable Diffusion remains challenging because these novel applications are relatively new to the MLLMs.\\n\\nMiddle planning by action number.\\n\\nFig. 5 (bottom) shows the mid-level planning scores (visual + text query) by the number of actions per task. Scores tend to decrease as the number of actions increases, demonstrating the difficulty of long procedural GUI tasks.\\n\\n4.4 Qualitative Results\\n\\nIn Fig. 6, we visualize model performance and failure cases. In (a) High-level planning, GPT-4o and Gemini-Pro-V successfully predict the sub-tasks for the slide with the 3D model. GPT-4o also accurately identifies the Morph transition effect, achieving the best score. In (b) Mid-level planning, both models inserted and adjusted the 3D Shiba Inu model. However, Gemini-Pro-V introduces unnecessary operations,\"}"}
{"id": "jSKtxmxc0M", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**High-level Planning (Final visual effect)**\\n\\n**How to create such effect in PowerPoint?**\\n\\n**GT:**\\n1. Set up a blank slide with a gradient fill background.\\n2. Insert a 3D model of a Shiba Inu dog and adjust its size and position.\\n3. Add a text 'Shiba Inu' in title box.\\n4. Duplicate the first slide.\\n5. Drag the title box to bottom with text 'Side view'.\\n6. Drag the dog to center and rotate to right.\\n7. Apply the Morph transition to all slides.\\n\\n**GPT:**\\n1. Insert a 3D model of a Shiba Inu.\\n2. Add text 'Shiba Inu' and position it next to the 3D model.\\n3. Apply the 'Fly in' animation to the 3D model from the left.\\n4. Add a second slide with a side view of the Shiba Inu 3D model.\\n5. Add text 'Side view' and position it below the 3D model.\\n6. Apply the 'Morph' transition between the two slides.\\n\\n**Gemini-Pro-V:**\\n1. Insert a 3D model of a Shiba Inu dog.\\n2. Apply a \\\"Rotate\\\" animation to the dog, specifying the \\\"Side\\\" direction.\\n3. Insert a text box with the word \\\"Side view\\\" and position it below the dog.\\n\\n**Claude-3-Opus:**\\n1. Insert an image of a Shiba Inu dog.\\n2. Add the text \\\"Shiba Inu\\\" next to the image.\\n3. Apply a 3D rotation effect to the dog image, rotating it from a side view to a front view.\\n\\n---\\n\\n**Middle-level Planning (Initial visual state + Text)**\\n\\n**Insert a 3D model of a Shiba Inu dog and adjust its size and position.**\\n\\n**GT:**\\n1. Click on 'Insert'.\\n2. Click on '3D Models'.\\n3. Click on 'Stock 3D Models'.\\n4. Keyboard: Type dog then Enter.\\n5. Click on 'Shiba Inu dog with motion'.\\n6. Click on 'Insert'.\\n7. Drag the the dog to top-left.\\n8. Drag the lower right corner to enlarge the dog.\\n\\n**GPT:**\\n1. Click the 'Insert' tab in the ribbon at the top of the screen.\\n2. Click the '3D Models' button in the 'Illustrations' group.\\n3. Select 'From Online Sources...' from the dropdown menu.\\n4. In the search bar, type 'shiba inu dog' and press Enter.\\n5. Browse the results and click on the desired 3D model of a shiba inu dog.\\n6. Click the 'Insert' button to add the 3D model to the slide.\\n7. Click on the 3D model to select it.\\n8. Use the corner handles to adjust the size of the 3D model by clicking and dragging.\\n9. Click and drag the 3D model to position it in the desired location on the slide.\\n\\n**Gemini-Pro-V:**\\n1. Click the 'Insert' tab.\\n2. In the '3D Models' group, click the 'From File' button.\\n3. In the 'Insert 3D Model' dialog box, navigate to the folder containing the shiba inu dog model.\\n4. Select the 'shiba_inu.fbx' file and click the 'Insert' button.\\n5. The shiba inu dog model will be inserted into the slide.\\n6. Click and drag the model to reposition it.\\n7. Use the 'Scale' tool to resize the model.\\n8. Click the 'Format' tab.\\n9. In the 'Shape Styles' group, click the '3D Model Styles' button.\\n10. In the '3D Model Styles' gallery, select the desired style for the model.\\n11. Click the 'Close' button to close the 'Format Shape' dialog box.\\n12. The shiba inu dog model will be updated with the selected style.\\n\\n**Claude-3-Opus:**\\n1. Click the 'Insert' tab.\\n2. Add the text \\\"Shiba Inu\\\" next to the image.\\n3. Apply a 3D rotation effect to the dog image, rotating it from a side view to a front view.\\n\\n---\\n\\n*(b) Middle-level Planning*\\n\\nAction Execution (Click)\\n\\nClick on '3D Models'\\n\\n**GT:**\\nDist: 0.02, Recall: 1\\n\\n**GPT:**\\nDist: 0.16, Recall: 0\\n\\n**Gemini-Pro-V:**\\nDist: 0.32, Recall: 0\\n\\n**CogAgent:**\\nDist: 0.2, Recall: 0\\n\\nAction Execution (Drag)\\n\\nDrag to select the text 'Shiba Inu' from right to left\\n\\n**GT:**\\nDist: 0.19, Recall: 0\\n\\n**GPT-4V:**\\nDist: 0.8, Recall: 0\\n\\nAction Execution (Scroll)\\n\\nShould I scroll up / down / not to find the 'Calibri font type'?\\n\\n**GT:** Down\\n\\n**GPT-4V:**\\nDown\\n\\n**GPT-4o:**\\nDown\\n\\n**Gemini-Pro-V:**\\nNo need\\n\\n**Claude-3-Opus:**\\nNo need\\n\\n---\\n\\n*(c) Atomic Execution with Click, Drag and Scroll.*\\n\\nFigure 6: Qualitative Results on VideoGUI with PowerPoint software.\\n\\nThe color green indicates the human references (GT), while red indicates wrong model predictions. Such as shape styles and formatting, leading to discrepancies in positioning and scaling. In (c) Atomic execution, models are assessed on precise actions. In Drag, GPT-4V selects part of the letters, but as the pixel distance is larger than the threshold, it still receives a recall of 0.\\n\\n4.5 Simulator Experiments\\n\\nTo simulate the real application scenario, we use the best performing LLM GPT-4o and build a simple agent baseline as shown in Fig. 7. We evaluate this agent on the most popular software (Powerpoint) to study its behavior.\"}"}
{"id": "jSKtxmxc0M", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our Minimalist GUI Agent Framework consists of three components: a Planner, a Parser, and an Actor. The Planner receives input queries, which may be either vision previews or text instructions. It then conducts high-level planning and generates mid-level plans for the Actor. The Actor executes these plans by performing a sequence of actions. After action execution, the current state (screenshot) is captured and sent back to the Parser to gather observations. These observations are then relayed to the Planner for subsequent planning.\\n\\n| Model Settings | VideoGUI Eval. | Subtask Eval. |\\n|----------------|----------------|--------------|\\n|                | High Plan.     | Mid Plan.    |\\n| GUI Agent w/ GPT-4o | 17.1 53.5 56.3 0 2.50 | 53.5 56.3 20.0 5.4 |\\n| Orig. Query (V) | 100.0 53.5 56.3 0 1.88 | 100 56.3 50.0 3.3 |\\n| w. GT High Plan. | 100.0 100.0 56.3 0 1.38 | |\\n| w. GT High & Mid Plan. | 100.0 100.0 56.3 0 1.38 | |\\n\\nTable 6 presents the model performance on full task execution in our simulator environment. We see that completing the full task is extremely challenging for the GPT-4o agent, with a notable 0 success rate for all variants. This again supports the design of our hierarchical evaluation, as the zero success rate simply implies the model/agent fail to execute the full task, without enough information in where they succeed or fail, or even how these models/agents perform relatively to each other. Therefore, we introduce another metric, Rank (Arena), which compares the final outcome of their execution. Specifically, we ask human to perform manual inspection, and rank the comparing models by the similarities between the final results and the GT. We found that when injected with GT planning (both high or mid.-level), the full-task execution can be significantly improved. These results echoes our observations of low model performance in high-level and mid-level planning in the main paper, which are the bottlenecks of successful full-task executions.\\n\\nWe visualize the final outcome of the three agent variants in Fig. 9 and Fig. 11.\\n\\nIn Tab. 7, we examine the performance of the GPT-4o agent in subtask competitions. Since subtasks do not necessitate high-level planning, we primarily investigate two variants: one with and one without manually provided middle-level planning, referred to as action sequences. Our study yields two key findings: (i) Despite the simplicity of these tasks, the original GPT-4o agent achieves a success rate of only 20.0%. With the assistance of manual plans, there is a 30% increase in success rate. (ii) For simple subtasks, the agent typically requires more extensive procedural execution compared to manual demonstrations (+2.1), which often represent the optimal pathway. This redundancy is exacerbated in complex tasks. Therefore, enhancing planning capabilities is essential for achieving efficient system with accurate success rates.\\n\\n5 Conclusion\\n\\nIn this work, we introduced VideoGUI, a multi-modal benchmark for advanced GUI tasks sourced from high-quality instructional videos targeting professional and novel software. VideoGUI, with its long procedural tasks, hierarchical manual annotations, and well-established evaluation metrics, provides clear signals for existing limitations and areas for improvement. By comparing state-of-the-art models, we highlight the challenges of visual-oriented GUI automation and the potential of instructional videos for advancing GUI task automation.\"}"}
{"id": "jSKtxmxc0M", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: Final effect in PowerPoint files.\\n(a) GPT-4o\\n(b) GPT-4o w. GT High Plan\\n(c) GPT-4o w. GT High+Mid. Plan\\n\\nFigure 9: Example of final outcome with our simple GPT-4o agent in simulated environment. When provided with GT planning (c), the GUI agent successfully inserts the 3D model. However, it still fails to match the background color.\\n\\nAcknowledgement\\nThis research is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG3-RP-2022-030).\"}"}
{"id": "jSKtxmxc0M", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: Final effect in Powerpoint files.\\n(a) GPT-4o\\n(b) GPT-4o w. GT High Plan\\n(c) GPT-4o w. GT High+Mid. Plan\\n\\nFigure 11: Example of final outcome with our simple GPT-4o agent in simulated environment. Guided by the GT planning, both (b) and (c) successfully insert the textual background, while the (c) can accurately type \u201898%\u2019.\"}"}
{"id": "jSKtxmxc0M", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Experimental Settings\\n\\nA.1 Data Collection Settings\\n\\nWe use OBS Studio software to record the demonstration videos and capture the user's screenshots. Notably, in the screenshots, the user's cursor is not recorded, which is beneficial as the screenshots can be used directly without revealing the target coordinates. We use pynput to monitor detailed user activity metadata, such as click location \\\\([x, y]\\\\), typed content, and scroll distance.\\n\\nIn Fig. 12, we display our manually labeled interface. Here, the annotator watches their key recording screenshots, with active regions such as the cursor coordinates highlighted in red. The annotators are then asked to enter the element name (e.g., \\\"Drop-down menu of font color\\\").\\n\\nFigure 12: Illustration of Manual annotation tools. The user are asked to watch their keyframe in their recording, and prompt to provide the element name regarding action.\\n\\nDefinition of Action Narration (e.g., Drag)\\n\\nWe ask annotators to provide a textual quadruple for each drag action: \\\\([\\\\text{start position}, \\\\text{end position}, \\\\text{element}, \\\\text{purpose}]\\\\). The narration follows the format: Drag the \\\\([\\\\text{element}]\\\\) from \\\\([\\\\text{start position}]\\\\) to \\\\([\\\\text{end position}]\\\\) to \\\\([\\\\text{purpose}]\\\\). Here, the start and end positions guide the annotators in identifying locations within the screenshot, with the element representing the object being dragged and the purpose defining the goal of the action (mainly movement or resizing). For movement, the start point is usually the element's original parent element (or \\\"original position\\\" if unspecified), while the end point is determined by the target parent element (e.g., a panel). For resizing, the start point is based on the specific part of the element being dragged (e.g., \\\"top-left corner of the circle\\\"), and the end point is identified by relevant elements in the screenshot. Additionally, our focus is on making predictions that closely approximate the intended points, rather than matching exact coordinates, which is why the narration serves as an effective guide.\\n\\nPrinciple for selecting instruction videos.\\n\\nWe selected the videos based on both topic relevance and quality:\\n\\nBy Topic:\\n(i) Videos introducing novel concepts or features with visual preview effects, primarily for visual creation and editing software. (ii) Videos offering advanced knowledge beyond basic usage, such as \\\"Top tips\\\" for VLC Player.\\n\\nBy Quality:\\n(iii) High-resolution videos with clear, step-by-step instructions; (iv) High-quality, accessible transcripts that users can easily follow.\"}"}
{"id": "jSKtxmxc0M", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.2 Baseline Details\\n\\nModel Ref. link Version (e.g., model id)\\n\\n- LLama3-70B [44] deepinfra meta-llama/Meta-Llama-3-70B-Instruct\\n- Mixtral-8x22B [45] deepinfra mistralai/Mixtral-8x22B-Instruct-v0.1\\n- GPT-3.5-Turbo [43] OpenAI gpt-3.5-turbo\\n- CogAgent [35] CogAgent CogAgent-18B\\n- Qwen-VL-Max [42] Aliyun qwen-vl-max\\n- Claude-3-Opus [40] Anthropic claude-3-opus-20240229\\n- Gemini-Pro-V [41] Google gemini-pro-vision\\n- GPT-4-Turbo [37] OpenAI gpt-4-turbo\\n- GPT-4o [37] OpenAI gpt-4o\\n\\nA.3 Evaluation Settings\\n\\nWe detail how we calculate the distance metric. Assume we have a ground-truth point \\\\([x_o, y_o]\\\\) while the screenshot size is \\\\(H \\\\times W\\\\).\\n\\n- If the model prediction is a bounding box \\\\([x_1, y_1, x_2, y_2]\\\\) (e.g., CogAgent [35] or Qwen-VL-Max [42]):\\n  We cannot only take the center of the bounding box as the click target for evaluation because it does not account for the area of the bounding box. As illustrated in Fig. 13 (a), if the center point is very close to the ground truth but the bounding box cover a large area, the distance between the center point and the ground truth would be small. Therefore, we design our metric to penalize for the area of the bounding box. Specifically, we calculate the distance between the ground truth and the four corners of the bounding box and then take the average. For the predicted bounding box, the average distance \\\\(d\\\\) is calculated as follows:\\n\\n  \\\\[\\n  d = \\\\frac{1}{4} p (x_o - x_1)^2 + (y_o - y_1)^2 + p (x_o - x_2)^2 + (y_o - y_2)^2 + p (x_o - x_1)^2 + (y_o - y_2)^2 + p (x_o - x_2)^2 + (y_o - y_1)^2\\n  \\\\]\\n\\n- If the model prediction is a coordinate \\\\([x_1, y_1]\\\\) (e.g., as in GPT4V+SoM [32]):\\n  We directly adopt the distance \\\\(d\\\\) calculated by:\\n\\n  \\\\[\\n  d = p (x_o - x_1)^2 + (y_o - y_1)^2\\n  \\\\]\\n\\nTo normalize the pixel-level distance \\\\(d\\\\) to \\\\(0 - 1\\\\), a simple way is to divide \\\\(d\\\\) by the maximum length in the screenshot, such as \\\\(\\\\sqrt{H^2 + W^2}\\\\). But in practice, the maximum length should be the distance between the ground-truth point and the farthest vertices, so we use that for normalization. The comparison between the two normalization methods is illustrated in Fig. 13 (b).\\n\\nDrag.\\n\\nDrag is a combination of Clicks, so we simply adopt the click metric for the start and end point of drag, and take the average. The score is calculated as \\\\(\\\\text{Dist} = \\\\frac{1}{2} d_s D_s + d_e D_e\\\\) where \\\\(d_s\\\\) is the pixel difference between predict start and GT start, while \\\\(D_s\\\\) is the farthest vertices for the GT start; \\\\(d_e\\\\) is the pixel difference between predict end and GT end, while \\\\(D_e\\\\) is the farthest vertices for the GT end.\\n\\nRecall.\\n\\nFor Recall, it is calculated by:\\n\\n\\\\[\\n\\\\text{Recall}(\\\\text{start, end}) = \\\\begin{cases} 1 & \\\\text{if Recall}(\\\\text{start}) \\\\& \\\\text{Recall}(\\\\text{end}) \\\\\\\\ 0 & \\\\text{otherwise} \\\\end{cases}\\n\\\\]\\n\\nType / Press.\\n\\nFor type/press, we evaluate whether the model can generate correct and efficient code to control keyboard activity. First, we prompt LLMs to write code for typing activity, and then we use pynput to monitor the keyboard outputs by executing the code. In Fig. 14, we show the pipeline for evaluating type/press activity. The model must generate the correct actions (e.g., Ctrl+F) with high precision, avoiding unnecessary actions such as redundant Ctrl presses.\\n\\nScroll.\\n\\nFig. 15 illustrates how we construct QA pairs to evaluate on scroll action. Before scrolling, the target element is assumed to be outside of the visible area, prompting for a scroll action.\"}"}
{"id": "jSKtxmxc0M", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Calculate distance by four corners or center point?\\n\\nFigure 13: (a) Illustration of why taking the distance between the center point of a bounding box and groundtruth is not a proper measure of model performance on click. As shown, the predicted bounding box center point is quite close to the ground-truth point, but the predicted bounding box area is large.\\n\\n(b) Illustration of distance normalization. To normalize the distance \\\\( d \\\\) to \\\\( 0 - 1 \\\\), a more proper term should be \\\\( D_1 \\\\) (farthest vertices) rather than \\\\( D_2 \\\\).\\n\\nModel Query\\n\\nI'm working on a computer task involving typing or pressing keys. Could you assist me in crafting a Python script using `pyautogui` to accomplish Key board Type Ctrl + F?\\n\\n```python\\nimport pyautogui\\n\\n# Press and hold the 'ctrl' key\\npyautogui.keyDown('ctrl')\\n\\n# Press the 'f' key\\npyautogui.press('f')\\n\\n# Release the 'ctrl' key\\npyautogui.keyUp('ctrl')\\n```\\n\\nB More Discussion\\n\\nPros Cons Environment\\n\\nHuman-only\\n\\nHigh-quality, Interpretability Extremely cost Real Simulator\\n\\nLLM-only\\n\\nFully automatic Hallucinations, might be unreliable No required\\n\\nHuman & LLM (Human anno. + LLM judge)\\n\\nSufficient signals for each stage; Automatic once we collected all annotations Require annotations for each task in advance. No required\\n\\nHuman & LLM (Agent exec. + Human verify)\\n\\nCheck whether agent indeed complete the full-task Require human check output (but is fast) Real Simulator\\n\\nTable 8: Comparison between Human evaluation and LLM as Judge.\\n\\nIn evaluation, accuracy is the most important. As shown in the below Table, while Human-only annotation ensures high-quality results, it is extremely time-consuming. On\"}"}
{"id": "jSKtxmxc0M", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Before Scrolling, we assume that \\\\([\\\\text{the target element}]\\\\) is not within the screenshot. After Scrolling, we assume that \\\\([\\\\text{the target element}]\\\\) should be inside the screenshot.\\n\\n**Question:** Should I scroll to find the maiandra GD button?\\n\\n**Answer:** No.\\n\\n**Question:** Should I scroll to find the maiandra GD button?\\n\\n**Answer:** Scroll down.\"}"}
{"id": "jSKtxmxc0M", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Software Platform | # Full Task | # Action per full task | # Action per subtask |\\n|-------------------|------------|-----------------------|---------------------|\\n| Powerpoint Windows 8 | 52 | 47.6 | 8.5 |\\n| StableDiffusion Web + Windows 10 | 69 | 19.0 | 4.0 |\\n| Runway Web | 11 | 63 | 24.3 | 4.7 |\\n| Photoshop Windows 10 | 69 | 19.0 | 4.0 |\\n| After Effects Windows 13 | 44 | 18.8 | 4.7 |\\n| Premiere Pro Windows 7 | 38 | 15.4 | 4.5 |\\n| Capcut Web + Windows 10 | 46 | 9.4 | 3.6 |\\n| DaVinci Windows 11 | 44 | 18.8 | 4.7 |\\n| YouTube Web | 0 | 13 | 0 | 4.3 |\\n| Web Stock Web | 0 | 12 | 0 | 9.7 |\\n| VLC player Windows | 0 | 12 | 0 | 9.2 |\\n| **Total** | **82** | **463** | **23.7** | **5.8** |\\n\\nTable 9: VideoGUI\u2019s software distribution.\\n\\nIn Fig. 16b, we present the distribution of manual recording time per subtask, with an average of 55 sec.\\n\\n1600 x 2560 59.1%\\n1080 x 1920 38%\\n2160 x 3840 2.83%\\n\\n(a) Screenshot resolution distribution.\\n\\nFigure 16: Distribution of (a) Screenshot resolution and (b) Human recording time.\\n\\nIn Fig.17, we present VideoGUI\u2019s Word Cloud, where the most frequent words are atomic actions (e.g., click, drag, type) and commonly used proper nouns (e.g., layer, background, panel) in the GUI.\"}"}
{"id": "jSKtxmxc0M", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":null}"}
{"id": "jSKtxmxc0M", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we display the visual-preview data samples, which are mainly focused on visual creation or editing.\\n\\nVisual query:\\nHow to create this effect in Powerpoint?\\n\\nTextual query:\\nCreate a slide that displays a large percentage figure of \\\"98%\\\" against a textured, beige background that appears to be fabric or canvas. The numerals are rendered in a bold, stylized font. The visual effect in this image is a wave-like effect. The blue percentage numerals appear to be rising out of the beige fabric-like background, creating a dynamic appearance. This gradient of wave creates a sense of depth and dimensionality, making the wave appear to have volume and curvature. The lighter blue at the top catches the light more, giving an illusion of the wave crest rising up, while the darker blue below suggests shadow and recession.\\n\\na. Format the background for the canvas\\nb. Change the background texture to parchment. Add a text box, add 98%, increase the font size and bold effect\\nc. Change the background texture to papyrus, increase the font size of 98%, change color to white, center it in the middle\\nd. Add a rectangle, remove outline, change the texture to papyrus\\ne. Send the rectangle to the back\\nf. Select the rectangle and the text. Merge shape and subtract, add buttom right shadow\\ng. Add shapes (e.g. Ovals) in between the two layers\\nh. Duplicate the slide, place it nicely and add Morph transition effect\\n\\nFull task\\n\\n\u25cf High-level Plans\\n\u25cf Mid.-level Plans\\n\u25cf Atomic Actions\\n\\nVisual query:\\nHow to transform from [start] to [end] in Premiere Pro?\\n\\nTextual query:\\nAdd a rectangle mosaic mask to the red billboard and track it.\\n\\na. Drag the timestamp to the beginning of the video\\nb. Add Mosaic effect on the top clip\\nc. Adjust the granularity of the Mosaic to 120\\nd. Add a rectangle mask to cover the billboard and track it\\n\\nb1. Click on Effects\\nb2. Click on Search box in Effects panel\\nb3. Keyboard Type Mosaic\\nb4. Click on 'Mosaic' effect\\nb5. Drag the Mosaic effect to the top clip.\\n\\nTable 10: Video Creation (i.e., animation) example with Powerpoint.\\n\\nTable 11: Video Editing example with Premiere Pro.\"}"}
{"id": "jSKtxmxc0M", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**High-level Plans**\\n\\n**Mid.-level Plans**\\n\\n**Atomic Actions**\\n\\nVisual query: How to create this effect in Runway?\\n\\nTextual query: Create a video about \u201cA man in a dark green jacket stands in the center of a futuristic industrial setting with yellow machines and monitors, under bright overhead lights, creating a cinematic portrait effect.\u201d with the dolly zoom effect.\\n\\na. Open Text/Image to Video\\n\\nb. Generate preview picture with text \u201cA man in a dark green jacket stands in the center of a futuristic industrial setting with yellow machines and monitors, under bright overhead lights, creating a cinematic portrait effect.\u201d\\n\\nc. Select the third image as the image input\\n\\nd. Adjust camera settings.\\n\\nd1. Click on Camera Settings.\\nd2. Click on the value of Zoom.\\nd3. Keyboard Type -3\\n\\ne. Select the background in Motion Brush. Set its Proximity to 10\\n\\nf. Select the subject in Motion Brush. Set its Proximity to 2\\n\\ng. Generate the video\\n\\nda1. Click, [50, 840]\\n\\n**Full task**\\n\\n**Table 12:** Video Creation example with Runway.\\n\\n| Start | End |\\n|-------|-----|\\n\\nVisual query: How to transform from [start] to [end] in StableDiffusion-WebUI?\\n\\nTextual query: Replace the 512*512 photo of a cat to a 720*720 photo of dragon by DPM++ method.\\n\\na. Open img2img Tool and drag photo of cat into the file upload box\\n\\nb. Put \u201cimage of a dragon\u201d into prompt box\\n\\nc. Put \u201ccartoon\u201d into negative prompt box\\n\\nd. Set \u201cSampling method\u201d to \u201cDPM++ 2M Karras\u201d\\n\\ne. Set Width to 720 and Height to 720\\n\\nf. Set Sampling steps to 25, Batch Size to 4 and CFG Scale to 4\\n\\ng. Generate the image\\n\\nda1. Scroll down 7\\nda2. Click on options of Sampling method.\\nda3. Click on \u201cDPM++ 2M\u201d\\nda4. Click on options of Schedule type.\\nda5. Click on Karras.\\n\\nd3. Click, [229, 277]\\n\\n**Table 13:** Image Editing example with StableDiffusion-WebUI.\"}"}
{"id": "jSKtxmxc0M", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Visual query:\\n\\n**How to transform from [start] to [end] in Adobe Effects?**\\n\\n**Textual query:** Isolate the dog with Green Screen.\\n\\n### Atomic Actions\\n\\n- **Select and apply Keylight effect to the BostonTerrier.mov layer**\\n- **Use the eyedropper tool to select the green background**\\n- **Adjust Keylight view mode to Screen Matte**\\n- **Modify Screen Gain and Screen Balance parameters**\\n- **Adjust Clip Black and Clip White parameters in Screen Matte**\\n- **Switch view mode back to Final Result and hide background layer**\\n\\n### Table 14: Video Editing example with Adobe Effects.\\n\\n| Step | Action |\\n|------|--------|\\n| 1     | Select and apply Keylight effect to the BostonTerrier.mov layer |\\n| 2     | Use the eyedropper tool to select the green background |\\n| 3     | Adjust Keylight view mode to Screen Matte |\\n| 4     | Modify Screen Gain and Screen Balance parameters |\\n| 5     | Adjust Clip Black and Clip White parameters in Screen Matte |\\n| 6     | Switch view mode back to Final Result and hide background layer |\\n\\n## Visual query:\\n\\n**How to transform from [start] to [end] in Photoshop?**\\n\\n**Textual query:** Use quick selection tool to put the pencil in the black background.\\n\\n### Atomic Actions\\n\\n- **Use quick selection tool to select the pencil**\\n- **Create a mask**\\n- **Create a solid black background layer**\\n- **Refine the mask. Set the smooth to 8, Feather to 7 px, Contrast to 72%, and Shift Edge to -3%;**\\n\\n### Table 15: Image Editing example with Photoshop.\\n\\n| Step | Action |\\n|------|--------|\\n| 1     | Use quick selection tool to select the pencil |\\n| 2     | Create a mask |\\n| 3     | Create a solid black background layer |\\n| 4     | Refine the mask. Set the smooth to 8, Feather to 7 px, Contrast to 72%, and Shift Edge to -3%; |\\n\\n21\"}"}
{"id": "jSKtxmxc0M", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Visual query: How to transform from [start] to [end] in DaVinci?\\n\\nTextual query: Use Depth Map to blur the background.\\n\\n- Add a serial node with depth map\\n- Add a serial node with lens blur\\n- Connect nodes and inverse the depth map node\\n- Disable Depth Map Preview\\n\\n**Table 16:** Video Editing example with DaVinci.\\n\\n---\\n\\nVisual query: How to transform from [start] to [end] in CapCut?\\n\\nTextual query: Add Stickers \\\"Heart\\\", Effects \\\"Blur\\\" and Filters \\\"Glow\\\" to the video.\\n\\n- Add \\\"Heart\\\" Sticker to the video\\n- Add \\\"Blur\\\" Effect to the video\\n- Add \\\"Glow\\\" Filter to the video\\n\\n**Table 17:** Video Editing example with CapCut.\"}"}
{"id": "jSKtxmxc0M", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Tab. 18 and Tab. 19, we present the prompt templates for high-level and mid-level planning, respectively. These templates are conditioned on the query formulation, such as whether the start or end visual effects are provided, or paired with the textual query.\\n\\n```python\\ndef get_high_prompt(vis_start=True, vis_end=True, txt=None, software=None):\\n    PROMPT = f\"You are a software assistant professional at {software}.\\n\\n    if vis_start and vis_end:\\n        PROMPT += f\\\"Given two sequence of image frames about the initial visual effect and the final visual effect\\\"\\n    elif vis_end:\\n        PROMPT += f\\\"Given a sequence of image frames about the final visual effect\\\"\\n    else:\\n        PROMPT += f\\\"You are provided with a task textual description\\\"\\n\\n    PROMPT += \\\"Your goal is to recognize the effect software demonstrates and pinpoint the key functions or operations, necessary to replicate this distinctive pattern.\\\"\\n\\n    PROMPT += \\\"**High-Level Planning**:\\n    Distill the process into essential stages or components, emphasizing the unique functions or operations, such as a specific design technique. Concentrate on brevity and precision in describing each stage, highlighting the unique aspects that contribute to the overall effect.\\\"\\n\\n    PROMPT += \\\"Please format your response as follows (we use Powerpoint as an example):\\n    \\n    1: Insert a Circle and Change its color as black.\\n    2: Add Text 'Happy' inside the Circle.\\n    3: Apply the 'Fly-in' animation for the Circle.\\\"\\n\\n    Each stage should be concise yet comprehensive, focusing on the key functionalities or operations that lead to the visual outcome in PowerPoint. Notably, avoid detailed step-by-step actions. Strive to keep the number of stages as few as possible, only including those that are crucial for achieving the unique effect.\\\"\\n\\n    if txt:\\n        PROMPT += f\\\"**This is the textual descriptions** {txt}\\\"\\n\\n    return PROMPT\\n```\\n\\nTable 18: High-level Planning Prompt conditioned on the interleaved instruction query.\\n\\nAction \u2013 Click. In Tab. 20, we show the template used by LLM to estimate click coordinates based on image resolution. With SoM\u2019s assistance, we use the Tab. 21 template to predict the mark index. With OCR\u2019s assistance, we use the Tab. 22 template.\\n\\nAction \u2013 Drag. In Tab. 23, we show the template used by LLM to estimate drag coordinates based on image resolution. With SoM\u2019s assistance, we use the Tab. 24 template to predict the start and end mark index. With OCR\u2019s assistance, we use the Tab. 25 template.\"}"}
{"id": "jSKtxmxc0M", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def get_prompt(vis=True, txt=None, software=None):\\nPROMPT =\\n\\\"You have been assigned the task of planning a sequence of actions in {software} software to achieve a desired goal state based on certain conditions. Your objective is to outline the fundamental actions needed.\\\" \\nif vis and not txt:\\nPROMPT += \\\"**You are provided with two screenshots which indicate the initial state as well as goal state.**\\\"\\nelif vis and txt:\\nPROMPT += \\\"**You are provided with a screenshot to indicate your initial state.**\\\"\\nif txt:\\nPROMPT += f\\\"**The goal is: {txt}**\\\"\\nPROMPT += \\\"\\nPlease format your response as follows:\\n'\\n1. Click the 'xxx'.\\n2. Type 'yyy'.\\n3. Click the 'zzz'.\\n'\\\"\\nEnsure that each step is clearly described to facilitate step-by-step reproduction of the actions.\\nreturn PROMPT\\n\\nTable 19: Middle-level Planning Prompt conditioned on the interleaved instruction query.\\n\\nI'm working on a computer task that involves clicking on some elements (like a button). You are provided with a screenshot with a resolution of width: {width} and height: {height}. Could you assist me in navigating to the {element}?\\n\\nPlease provide the location in the following format:\\n\\\"' [x, y] '\\nEnsure that your response contains only the coordinates.\\n\\nTable 20: Click action template that prompts LLMs output click's coordinate [x,y]\\nThe screenshot has been divided into areas and marked with numbers. Where is {element}?\\nAnswer by mark index like [x].\\n\\nTable 21: Click action template that prompts LLMs (with SoM [32]) output coordinate.\\n\\nAction \u2013 Type / Press.\\nIn Tab. 26, we present the template used by LLM to generate pyautogui code for keyboard actions.\\n\\nAction \u2013 Scroll.\\nIn Tab. 27, we present the template used by LLM to predict scroll action, which is used for high-level planning. For mid-level planning, we remove the commentary component.\\n\\nEvaluation.\\nIn Tab. 28, we display the evaluation template for GPT-4-Turbo [37].\\n\\n24\"}"}
{"id": "jSKtxmxc0M", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I'm working on a computer task that involves clicking on some elements (like a button).\\n\\nBelow are the OCR detection results (element name - bounding coordinates \\\\([x_1, y_1], [x_2, y_2]\\\\)), which are separated by a colon \\\";\\\".\\n\\n{ocr_result}\\n\\nCould you assist me in navigating to the \\\"\\\\{element\\\\}\\\"?\\n\\nPlease provide the location in the following format:\\n\\n\\\"' \\\\([x, y]\\\\) '\\\"\\n\\nEnsure that your response contains only the coordinates.\\n\\nTable 22: Click action template\\n\\nthat prompts LLMs (with OCR [47]) output click's coordinate \\\\([x, y]\\\\).\\n\\nI am working on a computer task that involves dragging elements from one place to another\\n\\nYou are provided with a screenshot with a resolution of width: {width} and height: {height}.\\n\\nCould you assist me in navigating for action \\\"\\\\{narration\\\\}\\\"\\n\\nPlease provide the location in the following format:\\n\\n\\\"' \\\\([x_1, y_1] \\\\rightarrow [x_2, y_2]\\\\) '\\\"\\n\\nwhere \\\\([x_1, y_1]\\\\) are the start coordinates and \\\\([x_2, y_2]\\\\) are the destination coordinates. Ensure that your response contains only the coordinates.\\n\\nTable 23: Drag action template\\n\\nthat prompts LLMs output drag's coordinate \\\\([x_1, y_1] \\\\rightarrow [x_2, y_2]\\\\).\\n\\nThe screenshot has been divided into areas and marked with numbers.\\n\\nTo assist with dragging an item, please provide the start and end mark numbers.\\n\\nHow to \\\\{element\\\\}? Provide the mark indices as follows:\\n\\n\\\"' \\\\([x] \\\\rightarrow [y]\\\\) '\\\"\\n\\nwhere \\\\([x]\\\\) represents the starting index and \\\\([y]\\\\) represents the ending index.\\n\\nTable 24: Drag action template\\n\\nthat prompts LLMs (with SoM [32]) output SoM mark.\\n\\nI am working on a computer task that involves dragging elements from one place to another\\n\\nBelow are the OCR detection results (element name - bounding coordinates \\\\([x_1, y_1], [x_2, y_2]\\\\)), which are separated by a colon \\\";\\\".\\n\\n{ocr_result}\\n\\nCould you assist me in navigating for action \\\"narration\\\"?\\n\\nPlease provide the location in the following format:\\n\\n\\\"' \\\\([x_1, y_1] \\\\rightarrow [x_2, y_2]\\\\) '\\\"\\n\\nwhere \\\\([x_1, y_1]\\\\) are the start coordinates and \\\\([x_2, y_2]\\\\) are the destination coordinates. Ensure that your response contains only the coordinates.\\n\\nTable 25: Drag action template\\n\\nthat prompts LLMs (with OCR [47]) output drag's coordinate \\\\([x_1, y_1] \\\\rightarrow [x_2, y_2]\\\\).\"}"}
{"id": "jSKtxmxc0M", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I'm working on a computer task involving typing or pressing keys. Could you assist me in crafting a Python script using pyautogui to accomplish {goal} where the key input element is \\\"{element}\\\". I've already set up the environment. Please provide the executable code directly and refrain from including other outputs or additional code blocks. Ensure that your response contains only one code block formatted as follows:\\n\\n'python\\nimport pyautogui\\npyautogui.press('ctrl')'\\n\\nTable 26: Type / Press action template that prompts LLMs output\\n\\nI'm currently engaged in a computer-based task and need your assistance. You are provided with an image of my screenshot. Could you advise whether I need to scroll to see the complete element \\\"{element}\\\"? Please note that even if the element appears partially, I still need to scroll to see it completely.\\n\\n'A': 'No need to scroll.', 'B': 'Scroll down.', 'C': 'Scroll up.'\\n\\nPlease select the appropriate option and format your response as follows (Wrap options in square brackets):\\n\\n' \\n\\n**Notably, only output options with square brackets**\"}"}
{"id": "jSKtxmxc0M", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You are tasked with evaluating the quality of a software procedure plan. Assess the prediction provided by an AI model against the human-generated ground truth and assign a correctness score to the prediction.\\n\\nEvaluation Criteria:\\n\\n1. Conciseness and Clarity: The procedure plan should be straightforward and to the point.\\n2. Element Accuracy: Pay attention to the precision of specific details like types of animation, text content, and design elements (e.g., 3d shape, color, shape). The prediction should accurately reflect these aspects as mentioned in the ground truth.\\n3. Commentary: Provide a brief commentary in your response summarizing the accurate and inaccurate aspects of the prediction as evidence to support your scoring decision.\\n\\nCorrectness Score (must be an integer):\\n- 0: Completely incorrect\\n- 1 to 3: Partially correct (with 1 being least accurate and 3 being more accurate)\\n- 4 to 5: Fully correct (with 4 being good and 5 being perfect)\\n\\nGround truth: {GT}\\nPrediction: {Pred}\\n\\nConsidering the detailed elements and the overall process, please format your response as follows:\\n\\n[comment]: Summary of evaluation.\\n[score]: x\"}"}
{"id": "jSKtxmxc0M", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] OpenAI. Gpt-4 technical report, 2023.\\n\\n[2] Microsoft copilot. https://copilot.microsoft.com/. Accessed: 2024-04-15.\\n\\n[3] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023.\\n\\n[4] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023.\\n\\n[5] Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo, Shixiang Shane Gu, and Izzeddin Gur. Multimodal web navigation with instruction-finetuned foundation models. arXiv preprint arXiv:2305.11854, 2023.\\n\\n[6] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. Advances in Neural Information Processing Systems, 36, 2024.\\n\\n[7] Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In International Conference on Machine Learning, pages 3135\u20133144. PMLR, 2017.\\n\\n[8] Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the zoo: Chain-of-action-thought for gui agents. arXiv preprint arXiv:2403.02713, 2024.\\n\\n[9] Xinbei Ma, Zhuosheng Zhang, and Hai Zhao. Comprehensive cognitive llm agent for smart-phone gui automation. arXiv preprint arXiv:2402.11941, 2024.\\n\\n[10] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild: A large-scale dataset for android device control. arXiv preprint arXiv:2307.10088, 2023.\\n\\n[11] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. Empowering llm to use smartphone for intelligent task automation. arXiv preprint arXiv:2308.15272, 2023.\\n\\n[12] Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, et al. Openagents: An open platform for language agents in the wild. arXiv preprint arXiv:2310.10634, 2023.\\n\\n[13] Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang, Bo An, and Shuicheng Yan. Agentstudio: A toolkit for building general virtual agents. arXiv preprint arXiv:2403.17918, 2024.\\n\\n[14] Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina N Toutanova. From pixels to ui actions: Learning to follow instructions via graphical user interfaces. Advances in Neural Information Processing Systems, 36, 2024.\\n\\n[15] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744\u201320757, 2022.\\n\\n[16] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024.\\n\\n[17] Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language instructions to mobile ui action sequences. arXiv preprint arXiv:2005.03776, 2020.\\n\\n[18] Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qinchen Wu, Weichen Zhang, Peiyi Wang, Xiangwu Guo, et al. Assistgui: Task-oriented desktop graphical user interface automation. arXiv preprint arXiv:2312.13108, 2023.\"}"}
{"id": "jSKtxmxc0M", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[108x710]\\n\\nTianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. arXiv preprint arXiv:2404.07972, 2024.\\n\\n[20] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024.\\n\\n[21] Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, and Doina Precup. Androidenv: A reinforcement learning platform for android. arXiv preprint arXiv:2105.13231, 2021.\\n\\n[22] Juyeon Yoon, Robert Feldt, and Shin Yoo. Autonomous large language model agents enabling intent-driven mobile gui testing. arXiv preprint arXiv:2311.08649, 2023.\\n\\n[23] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022.\\n\\n[24] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023.\\n\\n[25] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.\\n\\n[26] Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou. Assistgpt: A general multi-modal assistant that can plan, execute, inspect, and learn. arXiv preprint arXiv:2306.08640, 2023.\\n\\n[27] Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor C\u02d8arbune, Jason Lin, Jindong Chen, and Abhanshu Sharma. Screenai: A vision-language model for ui and infographics understanding. arXiv preprint arXiv:2402.04615, 2024.\\n\\n[28] Xinyu Zhang, Mengxue Kang, Fei Wei, Shuang Xu, Yuhe Liu, and Lin Ma. Tie: Revolutionizing text-based image editing for complex-prompt following and high-fidelity editing. arXiv preprint arXiv:2405.16803, 2024.\\n\\n[29] Zecheng He, Srinivas Sunkara, Xiaoxue Zang, Ying Xu, Lijuan Liu, Nevan Wichers, Gabriel Schubiner, Ruby Lee, and Jindong Chen. Actionbert: Leveraging user actions for semantic understanding of user interfaces. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 5931\u20135938, 2021.\\n\\n[30] Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, et al. Uibert: Learning generic multimodal representations for ui understanding. arXiv preprint arXiv:2107.13731, 2021.\\n\\n[31] Pratyay Banerjee, Shweti Mahajan, Kushal Arora, Chitta Baral, and Oriana Riva. Lexi: Self-supervised learning of the ui language. arXiv preprint arXiv:2301.10165, 2023.\\n\\n[32] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4, 2023.\\n\\n[33] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding. In International Conference on Machine Learning, pages 18893\u201318912. PMLR, 2023.\\n\\n[34] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562, 2023.\\n\\n[35] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: A visual language model for gui agents. arXiv preprint arXiv:2312.08914, 2023.\"}"}
{"id": "jSKtxmxc0M", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: Grounded mobile UI understanding with multimodal LLMs. arXiv preprint arXiv:2404.05719, 2024.\\n\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\\n\\nWeihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. MM-Vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023.\\n\\nPyAutoGUI. Pyautogui. 2024. https://pyautogui.readthedocs.io/en/latest/.\\n\\nAI Anthropic. The Claude 3 model family: Opus, Sonnet, Haiku. Claude-3 Model Card, 2024.\\n\\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\\n\\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\\n\\nOpenAI. Introducing chatgpt. OpenAI Blog, 09 2021.\\n\\nMeta. Introducing Meta Llama 3: The most capable openly available LLM to date, 2024. Accessed: 2024-04-18.\\n\\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.\\n\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing GPT-4 with 90%* chatgpt quality, March 2023.\\n\\nAzure OCR. Azure OCR. 2024. https://azure.microsoft.com/en-us/products/ai-services/ai-vision.\\n\\nOBS Studio. OBS Studio. 2024. https://obsproject.com/.\"}"}
{"id": "jSKtxmxc0M", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] All the claims made in the abstract and introduction are supported by the benchmark dataset, a new metric, and empirical evaluation.\\n   (b) Did you describe the limitations of your work? [Yes] Given the expensive manual collection cost, VideoGUI is limited by a smaller scale. Additionally, we provide final files but do not yet offer automatic success rate evaluation scripts, which require human validation.\\n   (c) Did you discuss any potential negative societal impacts of your work? [No] We do not see significant negative societal impacts from VideoGUI.\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] We have reviewed the Code of Ethics.\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [NA] The paper does not include theoretical results.\\n   (b) Did you include complete proofs of all theoretical results? [NA]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] We will provide code, data samples and instructions in supplementary material.\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Training details are described in Appendix.\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] We did not include error bars because experiments based on closed-source LLMs (i.e., API) are costly. Instead, we controlled parameters such as setting the temperature in LLM to 0 and provided all parameter settings and prompt template details.\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Our work primarily focuses on data curation and evaluation. We conduct experiments on a machine with 4 \u00d7 RTX6000 GPUs. The evaluation of VideoGUI for all models costs approximately 150 USD.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes] We properly cited all code, data, and models used in this paper.\\n   (b) Did you mention the license of the assets? [Yes] We provide MIT License software to be used with our data.\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We provide data samples of our dataset in supplementary material.\\n   (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] Our instructional videos are primarily sourced from YouTube with the consent of the video owners. We do not own the videos; instead, we provide the URL to refer to the original video link.\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [NA] The publicly available datasets from which our data is constructed do not have any PII instances and are devoid of toxic content.\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] Yes, since we focus on GUI software applications, we provide instructions to participants during data collection to ensure they understand the process and details.\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [NA] We didn't conduct research with human subjects.\"}"}
{"id": "jSKtxmxc0M", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [NA]\"}"}
