{"id": "47qVX2pa-2", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A new dataset for French and multilingual keyphrase generation\\n\\nFr\u00e9d\u00e9ric Piedboeuf\\nRALI, Diro\\nUniversit\u00e9 de Montr\u00e9al\\nfrederic.piedboeuf@umontreal.ca\\n\\nPhilippe Langlais\\nRALI, Diro\\nUniversit\u00e9 de Montr\u00e9al\\nfelipe@iro.umontreal.ca\\n\\nAbstract\\nKeyphrases are key components in efficiently dealing with the ever-increasing amount of information present on the internet. While there are many recent papers on English keyphrase generation, keyphrase generation for other languages remains vastly understudied, mostly due to the absence of datasets. To address this, we present a novel dataset called Papyrus, composed of 16427 pairs of abstracts and keyphrases. We release four versions of this dataset, corresponding to different subtasks. Papyrus-e considers only English keyphrases, Papyrus-f considers French keyphrases, Papyrus-m considers keyphrase generation in any language (mostly French and English), and Papyrus-a considers keyphrase generation in several languages. We train a state-of-the-art model on all four tasks and show that they lead to better results for non-English languages, with an average improvement of 14.2% on keyphrase extraction and 2.0% on generation. We also show an improvement of 0.4% on extraction and 0.7% on generation over English state-of-the-art results by concatenating Papyrus-e with the Kp20K training set.\\n\\n1 Introduction\\nAs of 2017, approximately 2.5 million new papers are published each year in the scientific field alone. With this ever-increasing flow of information, technologies that attempt to condense information, such as keyphrase generation, are becoming increasingly important. Keyphrases are defined as single or multi-word lexical units that summarize documents, and they can be used for indexing documents, document clustering, summarization, or even opinion mining.\\n\\nDue to the fast evolution of generative models, keyphrase generation (KPG) has gained attention in recent years. However, most of the models that are available today are trained exclusively in English, despite the fact that English represents only about 25% of internet use. This paper attacks this problem by collecting a multilingual dataset for KPG, composed of mostly French and English documents but occasionally of documents in other languages. To do so, we look at Papyrus, the institutional repository of Universit\u00e9 de Montr\u00e9al which hosts theses as well as other types of documents. We collect all documents from Papyrus and separate them into pairs of input sentences (abstracts) and keyphrases. We split the collected data into four KPG tasks; English, French, as well as two multilingual versions: KPG in a one2many way (one abstract in one language generating many keyphrases).\"}"}
{"id": "47qVX2pa-2", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"in that same language), as well as KPG in a many2many way (multiple abstracts of the same document generating several keyphrases in various languages). The difference between the two multilingual tasks is illustrated in Table 1. To the best of our knowledge, only English KPG has been presented in the literature as a task so far. We run generative baselines on each task based on the BAR T architecture, which achieved state-of-the-art results in English KPG, and we highlight several limitations that these systems have. We also run a version where we concatenate our English version of Papyrus with the main KPG English dataset, Kp20K, and show that it helps outperform state-of-the-art results on English KPG.\\n\\nThe paper is organized as follows. In Section 2, we present the related work that has been done in the field. In Section 3, we present the methodology for collecting the datasets, as well as some analysis of their contents. Sections 4 and 5 demonstrate the efficacy of the models trained on our new datasets and discuss the results on the different tasks. Then in Section 6, we discuss broader implications of our work, and Section 7 concludes the paper.\\n\\n2 Related Work\\n\\nThis work bridges two branches of automatic indexation: KPG in English, which has received a lot of attention in recent years, and keyphrase extraction for other languages. The difference between extraction and generation lies in the fact that extractive methods can only predict keyphrases present in the input (present keyphrases), while generative methods can also predict keyphrases not explicitly written (absent keyphrases).\\n\\nThe recent success of English KPG can be mostly attributed to the paper of Meng et al. [34], which proposes the first large corpus for keyphrase generation, Kp20K. The authors collect 567,830 abstracts/keyphrases pairs of scientific articles in computer science, and show that an encoder-decoder architecture pretrained on Kp20K can generate accurate keyphrases for other, small datasets.\\n\\nMost KPG papers following Meng et al. [34] use Kp20K as a training corpus. In Yuan et al. [46], the authors show that training the system to generate a sequence of keyphrases (one2many) is more efficient than training to generate one keyphrase and using a beam-search for multiple keyphrases (one2one), because the network could use the information of preceding generated keyphrases for more diversity. This sparked many other works, including the papers of Meng et al. [32, 33], which show that it is more efficient to train to generate first the present keyphrases (those in the input), followed by the absent ones. Other improvements have also been reported, such as the use of reinforcement learning to encourage the generation of more keyphrases [8], the study of generation from longer documents [1; 29], the generation of sets instead of sequences (one2set) [45], the use of summarization techniques [48], or the use of GANs for better generation of absent keyphrases [42; 43].\\n\\nRecently, Chowdhury et al. [9] reviewed papers and commented on the difficulty of implementing the proposed methods. They show that by simply fine-tuning a pre-trained transformer, BAR T, they could achieve results equating state-of-the-art.\\n\\nKeyphrase extraction has been explored in the past for several languages, mostly with unsupervised approaches and small datasets. Giarelis et al. [17] recently reviewed and compared several multilingual extractive methods on five languages: French, Spanish, Polish, Portuguese, and English. They show that three techniques perform better: YAKE, a statistical method relying on several features such as the position of the term or its normalized frequency, KeyBert, an extraction method using BERT to get document representation and calculating similarity with n-gram embeddings, and singleRank, a modification of TextRank where the weighting is adapted for keyphrase extraction. These three models, as well as the five non-English datasets, will serve to evaluate the efficacy of our generative approach in Section 4.\\n\\nOther efforts at keyphrase extraction on languages other than English include DEFT2012 and DEFT2016 challenges, where keyphrases extraction and indexation are tested on French articles from the humanities [38; 2; 14]. From what we can infer, this dataset is no longer maintained, and we weren't successful in obtaining it. Many other techniques have been reported, but they are generally unsupervised approaches, and the limited size of the datasets makes them less effective.\"}"}
{"id": "47qVX2pa-2", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"developed for keyphrase extraction in other languages, such as [21] where the authors introduce a TF-IDF method tailored for multilingual texts, the development of statistical methods for language independent keyphrase extraction [39], and the development of KPG systems for specific languages [13;11]. Finally, the recent paper of [16] introduces two datasets for multilingual KPG, one from the e-commerce domain, composed of entries in Spanish, German, Italian, and French, and one in the academic domain, composed of entries in Korean and Chinese. However, their paper presents significant differences with ours. First, the focus is on a new technique they name Retriever-Generator Iterative Training, where the examples are augmented with English keyphrases retrieved from semantically close examples. Second, while they show the efficacy of their technique on their dataset, we focus on showing that having a multilingual corpus to pretrain a KPG system leads to downstream improvements on various test datasets from different languages and domains, which can be more generally useful to practitioners. While we wanted to compare their dataset to ours, we were unfortunately unable to access their datasets at the time of writing.\\n\\n3 Data collection\\nWe collect our multilingual corpus from Papyrus, which is a repository of documents from the Universit\u00e9 de Montr\u00e9al. Papyrus hosts various types of documents, mostly theses but also institutional reports or other documents written by faculty members. While official policy is that theses should be written in French, most have abstracts in several languages, mainly French and English. For the remaining of this paper, we\u2019ll denote a document as it appears in Papyrus, with all its abstracts and keyphrases. The documents can be accessed by their index, so we could easily scrape the 26508 pages that were available at the date of collection. Of these, 657 corresponded to error pages, and 9602 to documents with no abstracts or keyphrases, leaving us with a total of 16249 documents composed of one or more abstracts as well as multiple keyphrases. The next step is then to conduct language identification on both the abstracts and keyphrases. We use langdetect on the abstracts, and a simple heuristic on the keyphrases to find their languages, among all the languages identified from the abstracts of that document: for present keyphrases, we assign them to all the languages of the abstracts that contains them. For absent keyphrases, we use fastText [23;24] and assign them to the most likely language. If fastText fails to identify the language, that is, if the top-15 languages from fastText are not concordant with the languages of the abstracts, such as for the keyphrase \\\"\u9593ma\\\", \\\"1000\\\", or \\\"Leptoquark\\\", we assign it to all possible languages for that entry. This corresponds to 119 keyphrases out of 208730. This approach is not perfect, as some keyphrases are sometimes erroneously tagged, and so as a sanity check we inspected 100 randomly selected examples from the training set and checked whether the abstracts and keyphrases are labelled with the correct language. The sanity check was done by the first author who speaks French, English, Spanish, and Dutch. We measure an accuracy of labelling of 100% for the abstracts, and 98.9% for the keyphrases. The selected examples and results of the evaluation are available on github.\\n\\nFinally, we split the entries into four different KPG datasets/tasks and drop examples that are present more than once:\\n\\n- **Papyrus-f**: From the French abstracts, generate French keyphrases,\\n- **Papyrus-e**: From the English abstracts, generate English keyphrases,\\n- **Papyrus-m**: From one abstract in any language, generate keyphrases in that same language (one language to one language),\\n\\nThe latest version of the dataset was collected on April 7, 2022. All our code for preprocessing and running experiments is available at https://github.com/smolPixel/French-keyphrase-generation. The data is distributed with the authorization from Universit\u00e9 de Montr\u00e9al.\"}"}
{"id": "47qVX2pa-2", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"From the multiple abstracts of a document, generate keyphrases in the same languages as the abstracts (many to many languages).\\n\\nWe show a toy example in Table 1. For Papyrus-a, we simply concatenate the abstracts together in the order they were present in Papyrus, and we do the same for keyphrases.\\n\\n| Task | Abstract | Keyphrases |\\n|------|----------|------------|\\n| Papyrus-f | Ce document parle de l'extraction et de la g\u00e9n\u00e9ration de mots-cl\u00e9s francophone et multilingue. | Extraction de mots-cl\u00e9s, g\u00e9n\u00e9ration de mots-cl\u00e9s, t\u00e2che francophone, t\u00e2che multilingue |\\n| Papyrus-e | This document is about keyphrase generation and extraction, for French and multilingual corpora. | Keyphrase generation, keyphrases extraction, Multilingual keyphrases, document indexing |\\n\\nTable 1: A toy example of a bilingual document and how it is processed for the various tasks. The present keyphrases are underlined. For Papyrus-m, each language is separated and passed as input separately, while in Papyrus-a the input text is a concatenation of all of the abstracts of that document.\\n\\nWe separate the documents with a 70/10/20 ratio train, dev, and test, prior to separating the dataset into the four tasks, so that the same documents find themselves in the same split for each task. We report in Table 2 the number of examples in each split and for each task.\\n\\nIf we look at the number of languages in each document, we find 1761 unilingual documents, 9391 bilingual ones, 134 with three languages, three with four, and one with six languages.\\n\\nIf we look instead at the languages present in the dataset, we find 15289 English abstracts, 14826 in French, 172 Spanish, 29 German, 20 Italian, 17 Portuguese, 7 Arabic, 5 Tagalog, 3 Catalan/Greek, 2 Turkish/Russian, and 1 in Polish/Farsi/Indonesian/Lingala/Swedish/Finnish/Romanian/Korean. There are also two abstracts written in a language that langdetect could not identify. One is, as far as we can judge, Inuktitut. The other was a mistaken entry where, in addition to the French and English abstracts, the term \u201c2002-10\u201d had been marked as an abstract.\\n\\nTable 3 presents some statistics about the abstracts and keyphrases in our datasets as well as for Kp20K. The abstracts in Papyrus are longer than those in Kp20K, with more keyphrases, and a larger proportion of present keyphrases.\\n\\n| Dataset | #Train | #Dev | #Test |\\n|---------|--------|------|-------|\\n| Papyrus-f | 10299 | 1488 | 2981 |\\n| Papyrus-e | 10508 | 1539 | 3046 |\\n| Papyrus-m | 20963 | 3040 | 6061 |\\n| Papyrus-a | 11290 | 1638 | 3261 |\\n\\nTo make sure that no other artifacts were introduced, we sorted the entries by length and manually checked them. The next shortest entry after \u201c2002-10\u201d is \u201c\u00c9ditorial en r\u00e9ponse \u00e0 un article d'Andr\u00e9e Quiviger.\u201d, which is a correct abstract for the document.\"}"}
{"id": "47qVX2pa-2", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Statistics of various tasks. Values represent the average over all examples of the training sets.\\n\\n| Dataset   | #abstract | #kp | % present | % br.present |\\n|-----------|-----------|-----|-----------|--------------|\\n| Kp20K     | 148       | 5.3 | 2.1       | 50.9         | 60.8         |\\n| Papyrus-f | 323       | 7.0 | 1.9       | 65.0         | 72.2         |\\n| Papyrus-e | 290       | 7.4 | 1.7       | 60.8         | 67.6         |\\n| Papyrus-m | 307       | 7.2 | 1.8       | 62.8         | 69.8         |\\n| Papyrus-a | 573       | 13.4| 1.9       | 62.8         | 70.4         |\\n\\n4 Baselines\\n\\nOur work allows to train large abstractive models for French and Multilingual KPG. Until now, the only option was to use unsupervised, language independent, extractive approaches. In this section, we first train four models, for the four tasks, and run them on their own test sets. Then, we compare their performances on various KPG tasks in several languages and show that using pretraining is more efficient than unsupervised approaches.\\n\\nWe follow the footsteps of Chowdhury et al. [9] in opting for a simple approach that is efficient and reproducible. We extend their proposed method, which uses BAR T for KPG, to a multilingual setting. Similarly to them, we apply no special preprocessing to the sentences, and do not order keyphrases in any specific way. Because BAR T is a model trained on English sentences, it would not work optimally on Papyrus-f, Papyrus-m, and Papyrus-a. As such, we use BAR Thez [12] (a French version of BAR T) for Papyrus-f, and the multilingual BAR T model mBAR T-large-50 [28] for Papyrus-a and Papyrus-m. We also rerun and report performances on Kp20K for comparison. It is to note that for mBAR T-large-50, the language has to be specified when tokenizing. For languages that are not available with mBAR T-large-50, or for the multilingual inputs of Papyrus-a, we default to English. For the rest of this paper we denote the BAR T systems fine-tuned on Papyrus-f, Papyrus-e, Papyrus-m, Papyrus-a, and Kp20K, as respectively Bart-f, Bart-e, Bart-m, Bart-a, and Bart-k. Finally, we include in the Appendix results on T5 [41], which has shown good results in tasks like summarization [18]. However, we found it underperformed in the KPG task when compared to BAR T.\\n\\nWe use a learning rate of 5e-5 with the Adam W optimizer, a beam search of 10 for generation, and a maximum input length of 1024. The only notable difference with Chowdhury et al. [9] is that we emulate a batch size of 128 by using a small batch size and gradient accumulation so that it fits on our GPU, while they report directly using a batch size of 128. We also use 16-bits precision, and train for 10 epochs for all Papyrus datasets, which seems to give optimal results, and 3 for Kp20K, which is the value used in Chowdhury et al. [9]. All experiments are run on a computer with the following configuration - Processor: AMD Ryzen 7 5800X 8-Core Processor @ 3.8GHz, RAM: 126G, GPU: GeForce RTX 3090, 24 G. We run all experiments three times and report the average and standard deviations. It is to note that in the case of Papyrus-m, we have a skewed distribution, with most entries being in French or English, and only a minority being in other languages. Because it is so extreme, we decided not to make use of it while training nor change the evaluation protocol. We discuss in further details the impact of the skewed distribution in Section 5.\\n\\nWe use the evaluation protocol from [46;34;32], which calculates recall, precision, and F1, for all keyphrases present in the abstract, as well as for the absent ones. Typical evaluation in KPG is done at thresholds where we take the X best keyphrases generated, denoted M@X, where M is the metric of interest. We report Precision, Recall, and F1@5 and @10 for present keyphrases, and @10 for absent keyphrases, similarly to Chowdhury et al. [9], but remove the stemming process in the evaluation to accommodate for non-English languages. Table 4 shows the results for present and absent keyphrases on all five datasets for the system trained on that dataset. We see that the results are comparable to state-of-the-art ones, at least on Kp20K where Chowdhury et al. [9] reported 33.5 F@5/31.1 F@10 for present keyphrases. Fine-tuning time (without testing) is 1h for Bart-f, 2h for Bart-e, 8h for Bart-m, 5h for Bart-a, and 15h for Bart-k.\"}"}
{"id": "47qVX2pa-2", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Evaluation of the generative models (respectively Bart-k, Bart-f, Bart-e, Bart-m, and Bart-a) on their own test sets. In listed order, the standard deviations for each model are in the range [0.4, 0.8], [0.2, 0.9], [0.1, 0.9], [0.7, 4.0], [1.4, 4.9].\\n\\n| Test set | Model   | P| R| F |\\n|----------|---------|---|---|---|\\n|          | Bart-f  | 6.6/0 | 8.1/0 | 2.4/0 |\\n|          | Bart-e  | 8.6/0 | 13.2/0 | 2.2/0 |\\n|          | Bart-m  | 8.2/0 | 11.8/0 | 2.5/0 |\\n|          | Bart-a  | 1.4/0 | 9.7/0 | 1.9/0 |\\n| Papyrus-f|         | 25.7/2.8 | 24.8/0.7 | 30.4/3.6 |\\n| Papyrus-e|         | 19.2/0.2 | 34.6/4.3 | 30.9/3.2 |\\n| Papyrus-m|         | 22.3/1.5 | 29.6/2.5 | 30.6/3.4 |\\n| Papyrus-a|         | 17.2/1.3 | 22.0/1.9 | 22.6/2.5 |\\n| Wikinews (fr) |         | 23.1/0.3 | 27.4/0.2 | 26.4/0.5 |\\n| cacic57 (es) |         | 29.1/0.3 | 47.5/0.2 | 48.8/0.2 |\\n| wicc 78 (es) |         | 14.6/0.3 | 26.4/0.2 | 29.9/0.2 |\\n| 110ptbnkp (pt) |         | 18.1/1.0 | 22.2/1.0 | 21.9/3.0 |\\n| pak2018 (pl) |         | 5.5/0 | 18.1/0.5 | 10.3/0.5 |\\n| Average |         | 6.8/0 | 12.2/0 | 2.1/0 |\\n\\nTable 5: Evaluation of the systems on various test sets. We report F1@10/R@10 for present and absent keyphrases respectively. SR stands for SingleRank, and KB for KeyBERT. Bart-a, Bart-f, Bart-e, Bart-m, and Bart-k correspond to our Bart models trained with respectively Papyrus-a, Papyrus-f, Papyrus-e, Papyrus-m, and Kp20K. Best scores for each test set are in bold.\\n\\nWe observe that the performance on Papyrus-f is much lower than for Papyrus-e and -m. As we discussed in Section 5, this is because BAR Thez underperforms globally and not because the task is more difficult, as we find that training mBAR T-large-50 on Papyrus-f yields better results. We also note that while Papyrus-m and Papyrus-a correspond to two versions of the same task, the performance on Papyrus-a is lower. This suggests that the many2many setting is more difficult than the one2many one, and that further research on the subject is necessary. Finally, we observe that the standard deviation is higher for Bart-m and Bart-a, the two multilingual systems using mBAR T-large-50. While fine-tuning, we noticed that mBAR T-large-50 was more difficult to correctly fine-tune, sometimes randomly collapsing and getting 0 on all metrics.\\n\\nWe now check if our models help in downstream KPG. We take as our starting point the paper of Giarelis et al. [17] which compares multiple keyphrase extraction methods on datasets of various languages. We report in Table 5 the results for all five BAR T s trained on the different tasks as well as the top three systems from Giarelis et al. [17], that is SingleRank, Y AKE, and KeyBERT, on the same five non-English datasets they use. Standard deviations follow the tendencies expressed in Table 4.\\n\\nThe French dataset is WikiNews [6], composed of 100 news articles annotated by students. For Spanish, they use subsets of Cacic and Wicc, two datasets composed of scientific articles as well as their keyphrases [3]. For Polish, they use pak2018, which is also composed of abstracts of scientific articles [7], and the Portuguese one is 110ptbnkp, composed of transcribed texts from broadcast news programs [30]. Despite some of these tasks having a low number of present keyphrases, they were all previously considered only for extractive techniques.\\n\\nThe lower performance is in part due to the removal of the stemming step in the evaluation protocol which will not accept variations of the same word as one, and in part due to the difference due to randomness of the training process. As far as we could gather, Chowdhury et al. [9] runs only one experiment, while we average over three. When we evaluate with the stemming step, we obtain 33.2 F1@5 and 33.3 F1@10 present, and 3.4 R@10 absent.\\n\\nThey have respectively 94.5%, 70.8%, 62.6%, 15.4%, and 97.8% of present keyphrases.\"}"}
{"id": "47qVX2pa-2", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We note that except for 110ptbnkp, using a generative model allows for a better performance for both present and absent keyphrases. The performance on the absent keyphrases is still low, but better than the extractive systems that cannot generate new keyphrases. It is both surprising and encouraging however to notice that the generative systems outperform generally the extractive systems even on present keyphrases. We also see that Bart-m is the best system overall, surpassing the other systems on 10 out of the 18 results reported. This is not surprising, given the multilingual aspect of the task, and demonstrates the advantage of having a multilingual corpus to train on. Finally, while Papyrus-f and Papyrus-a are useful for objective comparison, they are of limited use for pretraining, Bart-f and Bart-a being vastly outperformed by the other models. For Papyrus-f, it seems to be due to the weak general performance of Barthez, which is also supported by the fact that Bart-e outperforms Bart-f on Wikinews. For Papyrus-a, it calls for further research on the many2many setting.\\n\\n5 Discussion\\n\\nIn this section, we analyze the results from three points of view. We first take a look at French KPG, then multilingual KPG, and finally we look at the difference between our dataset and Kp20K. We conclude by showing an example of an input and the keyphrases generated by each of the generative models.\\n\\n5.1 French keyphrase generation\\n\\nAs mentioned, we believe that the poor performance of Bart-f when compared to Bart-e or Bart-m is due to the poor performance of Barthez rather than a more difficult task. To see if we could get a better system for French KPG, we train two other systems on papyrus-f: mBAR T, which consists in the mBAR T system that was further trained on a French corpus, as well as mBAR T. We report results on our two French corpus in Table 6. We can see that with these systems, performance is equivalent and even surpassing Bart-m, confirming that the problem was that Barthez underperforms. The good performance on Wikinews also shows that our systems can be used for downstream KPG in French.\\n\\n| System        | Papyrus-f  | Wikinews |\\n|---------------|------------|----------|\\n| Bart-f        | 25.7/2.8   |          |\\n| mBAR T        | 32.4/4.0   | 28.6/0.2 |\\n| mBAR T        | 31.6/3.8   | 28.5/1.2 |\\n| Bart-m        | 30.4/3.6   | 26.4/0.5 |\\n\\nTable 6: Results for three systems trained on Papyrus-f on the two French corpus, compared to Bart-m. We can see that when mBAR T and mBAR T are used, systems trained on Papyrus-f outperform Bart-m.\\n\\n5.2 Multilingual keyphrase generation\\n\\nAs mentioned, the best overall system for multilingual KPG is Bart-m. Some surprising results are that Bart-e (and to some extent, Bart-f) can successfully extract keyphrases from corpora in other languages. By looking at 100 keyphrases correctly predicted by Bart-e on Papyrus-f, we found 39 which could be considered anglophones (gpr55, queer, etc.), and the rest were clearly francophone (imaginaire social, coop\u00e9ration r\u00e9glementaire, etc.), supporting the hypothesis that the models use spurious cues for KPG.\\n\\n| Language | Present R@10 | Absent R@10 |\\n|----------|--------------|-------------|\\n| en       | 9.2          | 10.2        |\\n| es       | 12.0         | 15.3        |\\n| fr       | 15.0         | 20.7        |\\n| it       | 2.8          | 23.5        |\\n| pl       | 15.1         | 29.0        |\\n| de       | 15.0         | 45.0        |\\n| pt       | 0.0          | 11.1        |\\n\\nTable 7: R@10 for both present and absent keyphrases and for each language present in respectively the test sets of Papyrus-a and Papyrus-m. Language codes follow the ISO 639-1 format, with fr being French, en-English, es-Spanish, pl-Polish, it-Italian, de-German, pt-Portuguese, and tl-Tagalog.\\n\\nTo better appreciate the performances of our multilingual systems, we report in Table 7 the R@10 by languages for Bart-a and Bart-m, which indicates how many of the keyphrases of...\"}"}
{"id": "47qVX2pa-2", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: F1@10 present/R@10 absent, on all English test sets for all the generative systems.\\n\\nWe can see that Bart-m performs much better on the test set than Bart-a, being able to generate keyphrases for most of the rare languages. In this specific case, we could elect to simply split by languages and generate keyphrases that way, but there are many contexts where two or more languages might be used in a random order in a text (due to multilingual writers using \\\"code-switching\\\" [37]), and the Papyrus-a task reflects that phenomenon.\\n\\nSomething that is noteworthy is that even if the abstractive models perform globally better, they still struggle with their generative capabilities. A huge part of the problem is the evaluation protocol, which at the moment demands exact match for an absent keyphrase to be deemed correct. However, for a same concept there are many valid ways to express it, and authors do not write all the possible keyphrases that could be a valid match.\\n\\nAs a sanity check that the poor performance on absent keyphrases is not due to the models collapsing to pure extraction, we measure what percentage of keyphrases are absent keyphrases. Bart-f outputs 24% of absent keyphrases, Bart-e, 19.8%, Bart-m, 21.3%, Bart-a, 27.2%, and finally Bart-k outputs 23.2% of absent keyphrases. This confirms that the systems do learn to generate new keyphrases, but these keyphrases are simply not evaluated by the current evaluation protocol as correct.\\n\\nFinally, an important factor to consider for Bart-m and Bart-a is the skewed distribution. Having such a distribution is generally problematic, as deep learning models cannot learn correctly from classes for which there are few data points. Standard solutions include data preprocessing such as synthetic data generation or upsampling, or algorithm approaches such as cost-sensitive learning. However, these are often ineffective when dealing with such a skewed distribution [47]. Nevertheless, our results in Table 7 show that Bart-m and Bart-a correctly learn to generate keyphrases independently of the language.\\n\\n5.3 Difference with Kp20K\\n\\nWe compared, up to now, our new datasets to Kp20K in an informal way, using it as a way to validate our results against state-of-the-art English KPG. In this section, we take a closer look at the performance of the models trained on our new datasets, on English KPG. We report in Table 8 the performance of our models on all the test sets used in Chowdhury et al. [9], namely Inspec [22], NUS [36], SemEval [25], Kp20K [34], and Krapivin [26].\\n\\nIt is surprising that Bart-e is not far behind Bart-k, because Kp20K contains 527,091 data points, compared to the 10,602 examples of Papyrus-e. This prompts us to train a version of BAR T where we extend the Kp20K dataset with Papyrus-e: Bart-(k+e). Bart-(k+e) overall provides a slight improvement in performance in both extractive and abstractive capacities, surpassing Bart-k by an average of 0.4% on extraction and 0.7% on abstraction.\\n\\nIt is also surprising that Bart-f obtains decent scores for present keyphrases, given that all the test sets are in English. This, combined with results in the preceding section, reinforce the idea that BAR T generally relies on spurious cues for its KPG instead of on a real comprehension of the task.\\n\\nIf we train on only 10,602 points of Kp20K, we get present/absent score of 26.3/1.3 for Inspec, 31.4/2.1 for NUS, 21.8/1.5 for SemEval, 25.3/2.8 for Kp20K, 25.7/3.4 for Krapivin, and 28.5/1.9 for Papyrus-e.\"}"}
{"id": "47qVX2pa-2", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Title: Challenges to reintegration: barriers to reentry encountered by ex-convicts in halfway houses\\n\\nAbstract: Reintegration is a difficult process and individuals who have been affected by the justice system can benefit from interventions that help them to be autonomous, law-abiding, and integrated into their communities. Current research, however, seldom goes beyond identifying the difficulties in this process. Our qualitative study was an in-depth exploration of the challenges encountered by ex-convicts who had been provided with lodging in halfway houses as part of their reentry process. Six categories of challenges were identified following a content analysis of the interviews (n=16): return to society, financial and occupational situation, social network, personal development, stigmatization, and living in halfway houses. Experiences specific to certain profiles (length of sentence, sexual delinquency) are noted. Results are discussed in relation to desistance theories. Among other things, the study highlights the ways in which legal conditions and the stigmatization of offenders can lead to counter-productive outcomes.\\n\\nKeyphrases: R\u00e9insertion sociale, Interventions, Lib\u00e9ration conditionnelle, Entretiens semi-dirig\u00e9s, D\u00e9linquance sexuelle, Reentry, Parole, Intervention, Semi-structured interviews, Sex offenders, Reinserci\u00f3n social, Intervenciones, Liberaci\u00f3n condicional, Entrevistas semi-directivas, Delincuencia sexual\\n\\n5.4 Examples of generated keyphrases.\\n\\nWe show in Figure 1 an example of a document from the development dataset. The abstracts are available in English, French, and Spanish, but for reasons of space we only show the English one. In Table 9, we show the generated keyphrases by all five generative models. All of them are quite good, but generate ultimately few keyphrases. Furthermore, we also point out that Bart-a, trained on the Papyrus-a task, did not generate any Spanish keyphrases, which shows again that the Papyrus-a task is a more demanding setting than Papyrus-m and could benefit from more research.\\n\\nBart-e: Reintegration, Desistance, Criminal reentry, Halfway house, Stigmatization\\n\\nBart-f: r\u00e9insertion sociale, retour \u00e0 la libert\u00e9, situation financi\u00e8re et occupationnelle, r\u00e9seau social, stigmatisation, h\u00e9bergement en maison de transition\\n\\nBart-m (f): D\u00e9sistement criminel, Intervention, R\u00e9insertion sociale, Stigmatisation\\n\\nBart-m (e): Stigmatization, Return to society, Social network, Personal development, Sexual delinquency, Halfway houses\\n\\nBart-m (es): reinserci\u00f3n social, red social, desistimiento delictivo, estigmatizaci\u00f3n, desaf\u00edos contra productivos\\n\\nBart-a: D\u00e9sistement criminel, H\u00e9bergement en maison de transition, Stigmatisation, Retour \u00e0 la soci\u00e9t\u00e9, R\u00e9insertion sociale, Criminal desistimiento, Halfway houses, Return to society, Social reintegration, Stigmatization\\n\\nBart-k: reinsertion, justice system, desistance theory, halfway houses\\n\\nTable 9: Keyphrases produced by each of the generative models for the example of Figure 1.\"}"}
{"id": "47qVX2pa-2", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implications\\n\\nWe do not see any direct negative implications of the new tasks and datasets, and rather we believe that multilingual datasets are a good thing that allow for a fairer use of artificial intelligence. There is, however, the danger of misinterpreting the results, in part due to the skewed distribution. The classic evaluation metrics do not really reflect the performance on the languages that are rare in the dataset, so researchers should take care to interpret correctly the performance lest it erases the performance on minority languages. We believe that the best way to evaluate the performance on papyrus-m is to test on external test sets on other languages as well as output the performance per language to verify that it performs well on all languages.\\n\\nConclusion\\n\\nKeyphrase generation is an important topic for machine learning, due to the increasing amount of information that is accessible on the internet. Until now, however, research has mostly been focussed on English KPG, which represents only a subset of the need for KPG. In this work, we first collect and curate a dataset for multilingual KPG, which we separate into four distinct tasks, among them three that have never been presented before. Papyrus-f and Papyrus-e are unilingual KPG tasks, in French and English respectively. Papyrus-m and Papyrus-a are both multilingual KPG tasks, the former in a one-to-one way and the later in a many-to-many way.\\n\\nWe train a state-of-the-art KPG model based on BAR T, on all four datasets and test the models on test sets from various languages. We show that while BAR T provides a solid baseline, there is still a lot of work that remains to be done. In particular, most universities over the world maintain a repository of theses similar to Papyrus. We hope this work will encourage others to do so and will help kickstart multilingual KPG, which is without a doubt a very important field in modern machine learning.\\n\\nAcknowledgments\\n\\nWe thank LexRock AI, MIT ACS, Universit\u00e9 de Montr\u00e9al, and the Papyrus team for supporting this work, as well as the six reviewers of this paper for their insightful comments.\\n\\nReferences\\n\\n[1] Wasi Uddin Ahmad, Xiao Bai, Soomin Lee, and Kai-Wei Chang. 2021. Select, Extract and Generate: Neural Keyphrase Generation with Layer-wise Coverage Attention. arXiv:2008.01739 [cs]. ArXiv: 2008.01739.\\n[2] Chedi Bechikh Ali, Rui Wang, and Hatem Haddad. 2015. A Two-Level Keyphrase Extraction Approach. In Computational Linguistics and Intelligent Text Processing, Lecture Notes in Computer Science, pages 390\u2013401, Cham. Springer International Publishing.\\n[3] Germ\u00e1n Aquino and Laura Lanzarini. 2015. Keyword Identification in Spanish Documents using Neural Networks. Journal of Computer Science and Technology, 15(2):55\u201360. Publisher: Facultad de Inform\u00e1tica Section: Journal of Computer Science and Technology.\\n[4] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610\u2013623, Virtual Event Canada. ACM.\\n\\nThe majority of such repositories provides a standardized OAI-PMH metadata harvesting mechanism (https://www.openarchives.org/pmh/) and several mutualized sites already exist (eg https://oatd.org/).\"}"}
{"id": "47qVX2pa-2", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"G\u00e1bor Berend. 2011. Opinion Expression Mining by Exploiting Keyphrase Extraction. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 1162\u20131170, Chiang Mai, Thailand. Asian Federation of Natural Language Processing.\\n\\nAdrien Bougouin, Florian Boudin, and B\u00e9atrice Daille. 2013. TopicRank: Graph-Based Topic Ranking for Keyphrase Extraction. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 543\u2013551, Nagoya, Japan. Asian Federation of Natural Language Processing.\\n\\nRicardo Campos, V\u00edtor Mangaravite, Arian Pasquali, Al\u00edpio Jorge, C\u00e9lia Nunes, and Adam Jatowt. 2020. YAKE! Keyword extraction from single documents using multiple local features. Information Sciences, 509:257\u2013289.\\n\\nHou Pong Chan, Wang Chen, Lu Wang, and Irwin King. 2019. Neural Keyphrase Generation via Reinforcement Learning with Adaptive Rewards. arXiv:1906.04106 [cs]. ArXiv:1906.04106.\\n\\nMd Faisal Mahbub Chowdhury, Gaetano Rossiello, Michael Glass, Nandana Mihindukulasooriya, and Alfio Gliozzo. 2022. Applying a Generic Sequence-to-Sequence Model for Simple and Effective Keyphrase Generation. arXiv:2201.05302 [cs]. ArXiv:2201.05302.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BER T: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nLiangping Ding, Zhixiong Zhang, Huan Liu, Jie Li, and Gaihong Yu. 2021. Automatic Keyphrase Extraction from Scientific Chinese Medical Abstracts Based on Character-Level Sequence Labeling. Journal of Data and Information Science, 6(3):35\u201357.\\n\\nMoussa Kamal Eddine, Antoine J.-P. Tixier, and Michalis Vazirgiannis. 2021. BAR Thez: a Skilled Pretrained French Sequence-to-Sequence Model. arXiv:2010.12321 [cs]. ArXiv:2010.12321.\\n\\nSamhaa R. El-Beltagy and Ahmed Rafea. 2009. KP-Miner: A keyphrase extraction system for English and Arabic documents. Information Systems, 34(1):132\u2013144.\\n\\nNazanin Firoozeh, Adeline Nazarenko, Fabrice Alizon, and B\u00e9atrice Daille. 2020. Keyword extraction: Issues and methods. Natural Language Engineering, 26(3):259\u2013291. Publisher: Cambridge University Press.\\n\\nYgor Gallina, Florian Boudin, and B\u00e9atrice Daille. 2019. KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents. arXiv:1911.12559 [cs]. ArXiv:1911.12559.\\n\\nYifan Gao, Qingyu Yin, Zheng Li, Rui Meng, Tong Zhao, Bing Yin, Irwin King, and Michael R. Lyu. 2022. Retrieval-Augmented Multilingual Keyphrase Generation with Retriever-Generator Iterative Training. ArXiv:2205.10471 [cs].\\n\\nNikolaos Giarelis, Nikos Kanakaris, and Nikos Karacapilidis. 2021. A Comparative Assessment of State-Of-The-Art Methods for Multilingual Unsupervised Keyphrase Extraction. In Artificial Intelligence Applications and Innovations, pages 635\u2013645. Springer, Cham.\\n\\nTravis Goodwin, Max Savery, and Dina Demner-Fushman. 2020. Towards Zero-Shot Conditional Summarization with Adaptive Multi-Task Fine-Tuning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3215\u20133226, Online. Association for Computational Linguistics.\\n\\nMaarten Grootendorst. 2020. KeyBERT: Minimal keyword extraction with BER T. Version Number: v0.3.0.\\n\\nKhaled M. Hammouda, Diego N. Matute, and Mohamed S. Kamel. 2005. CorePhrase: Keyphrase Extraction for Document Clustering. In Machine Learning and Data Mining in Pattern Recognition, Lecture Notes in Computer Science, pages 265\u2013274, Berlin, Heidelberg. Springer.\"}"}
{"id": "47qVX2pa-2", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "47qVX2pa-2", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chad Nilep. 2006. \u201cCode Switching\u201d in Sociocultural Linguistics. Colorado Research in Linguistics.\\n\\nPatrick Paroubek, Pierre Zweigenbaum, Dominic Forest, and Cyril Grouin. 2012. Indexation libre et contr\u00f4l\u00e9e d\u2019articles scientifiques. Pr\u00e9sentation et r\u00e9sultats du d\u00e9fi fouille de textes [in French]. In JEP-T ALN-RECIT AL 2012, Workshop DEFT 2012: D\u00e9fi Fouil le de Textes (DEFT 2012 Workshop: Text Mining Challenge), pages 1\u201313, Grenoble, France. A T ALA/AFCP.\\n\\nMari-Sanna Paukkeri, Ilari T. Nieminen, Matti P\u00f6ll\u00e4, and Timo Honkela. 2008. A Language-Independent Approach to Keyphrase Extraction and Evaluation. In Coling 2008: Companion volume: Posters, pages 83\u201386, Manchester, UK. Coling 2008 Organizing Committee.\\n\\nVahed Qazvinian, Dragomir R. Radev, and Arzucan \u00d6zg\u00fcr. 2010. Citation Summarization Through Keyphrase Extraction. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 895\u2013903, Beijing, China. Coling 2010 Organizing Committee.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research, 21(140):1\u201367.\\n\\nAvinash Swaminathan, Raj Kuwar Gupta, Haimin Zhang, Debanjan Mahata, Rakesh Gosangi, and Rajiv Ratn Shah. 2020. Keyphrase Generation for Scientific Articles Using GANs (Student Abstract). Proceedings of the AAAI Conference on Artificial Intelligence, 34(10):13931\u201313932. Number: 10.\\n\\nAvinash Swaminathan, Haimin Zhang, Debanjan Mahata, Rakesh Gosangi, Rajiv Ratn Shah, and Amanda Stent. 2020. A Preliminary Exploration of GANs for Keyphrase Generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8021\u20138030, Online. Association for Computational Linguistics.\\n\\nXiaojun Wan and Jianguo Xiao. 2008. Single document keyphrase extraction using neighborhood knowledge. In Proceedings of the 23rd national conference on Artificial intelligence - Volume 2, AAAI\u201908, pages 855\u2013860, Chicago, Illinois. AAAI Press.\\n\\nJiacheng Ye, Tao Gui, Yichao Luo, Yige Xu, and Qi Zhang. 2021. One2Set: Generating Diverse Keyphrases as a Set. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4598\u20134608, Online. Association for Computational Linguistics.\\n\\nXingdi Yuan, Tong Wang, Rui Meng, Khushboo Thaker, Peter Brusilovsky, Daqing He, and Adam Trieschler. 2020. One Size Does Not Fit All: Generating and Evaluating Variable Number of Keyphrases. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7961\u20137975, Online. Association for Computational Linguistics.\\n\\nYucan Zhou, Qinghua Hu, and Yu Wang. 2018. Deep super-class learning for long-tail distributed image classification. Pattern Recognition, 80:118\u2013128.\\n\\nErion \u00c7ano and Ond\u0159ej Bojar. 2019. Keyphrase Generation: A Text Summarization Struggle. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 666\u2013672, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nChecklist\\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? [Yes]\\n(b) Did you describe the limitations of your work? [Yes]\\n(c) Did you discuss any potential negative societal impacts of your work? [Yes] We mentionned that we did not see any potential direct negative societal impacts.\"}"}
{"id": "47qVX2pa-2", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Have you read the ethics review guidelines and ensured that your paper conforms to them?\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n   (b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes]\\n   We gave the link to the github for the experiments, as well as for the downloading of data. The github also includes all the necessary code for complete recollection and reprocessing of the data.\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes]\\n   See Section 3 and 4.\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A]\\n   It is not standard in the domain of keyphrase generation to rerun several times the experiments with various seeds and report std.\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]\\n   See section 4.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes]\\n   (b) Did you mention the license of the assets? [Yes]\\n   See the datasheet for details on licenses.\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\\n   All new assets are available at https://github.com/smolPixel/French-keyphrase-generation\\n   (d) Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? [Yes]\\n   See the datasheet for details on how consent was obtained.\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]\\n   See the datasheet for details on potentially offensive content and identifiable information.\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\"}"}
