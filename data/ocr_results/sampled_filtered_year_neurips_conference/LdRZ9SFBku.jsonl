{"id": "LdRZ9SFBku", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Addition Statement for Our New Dataset\\n\\nA.1 Dataset Documentation and Intended Use\\n\\nWe offer a detailed overview of our dataset statistics in Sec. 3.3. To facilitate better understanding and ease of access, we have made our dataset project available on ModelScope at: https://www.modelscope.cn/datasets/yutong/UKnow/summary, which includes dataset summary, data preview, quickstart and data files.\\n\\nThe detailed data organization and corresponding download links are listed below:\\n\\n\u2022 Original data: We gather our data from publicly available international news sources, accumulating a substantial volume of images and text. Subsequently, we compress the collected data into several zip archives and store them in original_data: UKnow/raw_data/*.\\n\\n\u2022 Processed data:\\n  \u2022 Pre-node $N_p$: Building upon Phase-1, we leverage pre-trained deep learning models to extract valuable information from various domains. The resultant output from Phase-1 is structured as a dictionary and is then stored and saved to pre_node: UKnow/processed_data/pre_node*.\\n\\n  \u2022 Node index $N_n$ and Edge index $N_e$: As the outcomes acquired in Phase-1 (e.g., $N_p$) are not directly applicable for graph construction, we employ an information symbolization strategy to organize them into indices, namely $N_n$ and $N_e$, which are subsequently saved to index: UKnow/processed_data/*_index*.pickle.\\n\\n  \u2022 Knowledge graph $G_m$: Finally, we consolidate two types of internal knowledge ($I_{in}$, $T_{in}$) and three types of associative knowledge ($I_{cross}$, $T_{cross}$, $IT_{cross}$) into into one knowledge graph ($G_m$), which is stored as a dictionary in graph: UKnow/processed_data/graph*.pickle.\\n\\nOur dataset is intended for academic use and the corresponding license is based on: https://www.contributor-covenant.org/zh-cn/version/1/4/code-of-conduct.html, which was created by Coraline Ada Ehmke in 2014 and is released under the CC BY-NC-ND 4.0.\\n\\nA.2 Author statement\\n\\nWe confirm the data licenses and that we bear all responsibility in case of violation of rights.\\n\\nA.3 Hosting, licensing, and maintenance plan\\n\\nHosting and Licensing.\\n\\nOur dataset is hosted on ModelScope. Moreover, we furnish the relevant licenses in accordance with ModelScope at: https://www.contributor-covenant.org/zh-cn/version/1/4/code-of-conduct.html, which was created by Coraline Ada Ehmke in 2014 and is released under the CC BY 4.0 License.\\n\\nIntroduction to ModelScope.\\n\\nModelScope is a platform designed for managing and optimizing machine learning models. It provides various tools and features to streamline the model development process, including version control, performance monitoring, and collaboration capabilities. As for managing datasets, ModelScope offers robust functionality for organizing, storing, and accessing data. Users can upload datasets to the platform, where they are securely stored and can be easily accessed by authorized team members. ModelScope also supports versioning of datasets, allowing users to track changes over time and ensure reproducibility in their experiments. Additionally, the platform provides tools for data preprocessing, visualization, and analysis, helping users to efficiently prepare their data for model training and evaluation. Overall, ModelScope offers comprehensive support for managing datasets throughout the machine learning lifecycle. Therefore, we choose ModelScope as our hosting platform.\\n\\nUsage of ModelScope.\\n\\nTo enable users to directly utilize all models on the ModelScope platform without configuring the environment, ModelScope integrates an online Notebook programming environment on its website and offers official mirrors for developers. These official mirrors allow users to bypass all installation and configuration steps, providing immediate access to the models. Currently the latest version of the CPU mirror and GPU mirror can be obtained from the office ModelScope repository.\\n\\nUsers also can setup local python environment using following commands:\\n\\n1. conda create -n modelscope python=3.8\\n2. conda activate modelscope\\n3. pip install modelscope\\n\\nThen, users can access and enjoy our dataset by:\\n\\n1. from modelscope.msdatasets import MsDataset\\n2. ds = MsDataset.load('yutong/UKnow', subset_name='default', split='train')\"}"}
{"id": "LdRZ9SFBku", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Besides, we strongly recommend that users read the official documents for optimal use.\\n\\nMaintenance Plan.\\n\\nIn future work, we will persistently augment the dataset across various scales following the UKnow protocol. This endeavor aims to furnish a comprehensive, diverse, and resilient multimodal knowledge graph, thereby facilitating subsequent research endeavors.\\n\\nB Preliminaries\\n\\nMultimodal Knowledge Graph.\\n\\nAn intuitive interpretation of multimodal knowledge graph is that the ordinary knowledge graph only consists of \\\\(<\\\\text{head}, \\\\text{relation}, \\\\text{tail}>\\\\) triples like \\\\(<\\\\text{\u201cJony\u201d}, \\\\text{Citizen}, \\\\text{\u201cNew York\u201d}>\\\\), but the multimodal knowledge graph consists of the following:\\n\\n\\\\(<\\\\text{\u201cJony\u201d}, \\\\text{Citizen}, \\\\text{\u201cNewYork\u201d}>\\\\),\\n\\\\(<\\\\text{\u201cJony\u201d}, \\\\text{Appearance}, \\\\text{\u201c[Face]\u201d}>\\\\),\\n\\\\(<\\\\text{\u201cNewYork\u201d}, \\\\text{Landmark}, \\\\text{\u201c[Statueofliberty]\u201d}>\\\\),\\n\\\\(<\\\\text{\u201c[AirForceOne]\u201d}, \\\\text{Similarity}, \\\\text{\u201c[AirForceTwo]\u201d}>\\\\),\\n\\nwhere \\\\((\\\\cdot)\\\\) means a text node and \\\\([\\\\cdot]\\\\) means a image node. The machine cannot understand what \u201cAn old man with white hair\u201d is without establishing the connection between each word and its physical world meaning. However, with the help of multimodal knowledge graph, as a simple example, it is possible to generate a more informative entity-level sentence (e.g., \u201cBiden is making a speech\u201d) instead of a vague concept-level description (e.g., \u201cAn old man with white hair is making a speech\u201d).\\n\\nTo evaluate the effectiveness of multimodal knowledge graph (MMKG), several downstream tasks are often performed on the MMKGs, including common-sense reasoning, vision-language pre-training.\\n\\nCommon-sense Reasoning.\\n\\nCommon-sense reasoning means answering queries by logic permutations. The specific task in this work is the link prediction. In the inference phase, feeding \\\\(<\\\\text{\u201cAmerica\u201d}, \\\\text{Capital}>\\\\) to a reasoning model, the output should be \\\\(<\\\\text{\u201cWashington\u201d}>\\\\). Various works \\\\([\\\\ 3, 70, 68, 60, 94, 53]\\\\) achieve reasoning by embedding entities and relations in knowledge graph into low-dimensional vector space. For instance, GQE \\\\([15]\\\\) encodes queries through a computation graph with relational projection and conjunction (\\\\(\\\\land\\\\)) as operators. Path-based methods \\\\([\\\\ 27, 82, 63, 51]\\\\) start from anchor entities and determine the answer set by traversing the intermediate entities via relational path. There are also GCN \\\\([25]\\\\) based methods \\\\([\\\\ 61, 16]\\\\) pass message to iterate graph representation for reasoning. Common-sense reasoning is an extremely popular task in the field of knowledge graph. Since our dataset is based on the knowledge graph, the performance validation on common-sense reasoning is indispensable.\\n\\nVision-Language Pre-training\\n\\nVision-language pre-training (VLP) can be divided into three categories based on how they encode images \\\\([10]\\\\): OD-based region features \\\\([\\\\ 5, 31, 34, 41, 66, 69]\\\\), CNN-based grid feature \\\\([62, 19, 20]\\\\) and ViT-based patch features \\\\([84, 30, 24]\\\\). Pre-training objectives are usually: masked language/image modeling (MLM/MIM) \\\\([\\\\ 2, 9, 39]\\\\), image-text matching (ITM) \\\\([34, 19, 10]\\\\), and image-text contrastive learning (ITC) \\\\([30, 50, 35]\\\\). In this work, we concentrate on the study of the how to introduce our UKnow into ITC method based on ViT-based patch features.\\n\\nImage-Text Contrastive Learning.\\n\\nThe recent CLIP \\\\([\\\\ 50]\\\\) and ALIGN \\\\([\\\\ 21]\\\\) perform pre-training using a crossmodal contrastive loss on millions of image-text pairs, which achieves remarkable performance on various downstream tasks \\\\([\\\\ 42, 62, 64]\\\\). MDETR \\\\([\\\\ 23]\\\\) trains on multi-modal datasets which have explicit alignment between phrases and objects. GLIP \\\\([\\\\ 32]\\\\) generates grounding boxes in a self-training fashion, and makes the learned representations semantic-rich. We implement these mainstream methods on our dataset, and also design a basic knowledge-based ITC method with UKnow.\\n\\nC Experimental Details\\n\\nIn this section, we give more details about the computation complexity, training, fine-tuning hyperparameters and evaluation for reference.\\n\\nC.1 Common-sense Reasoning\\n\\nDatasets.\\n\\nSince our dataset is a knowledge graph, we benchmark the performance of KG-reasoning models on our dataset by completing KG-triples. The partitioning of the dataset is illustrated in the upper segment of Tab. 4.\\n\\nEvaluation.\\n\\nThe specific task of common-sense reasoning in this work is the link prediction. Given a test query \\\\(q\\\\) (e.g., \\\\(<\\\\text{\u201cJony\u201d}, \\\\text{Citizen}, (?)>\\\\)), we are interested in discovering non-trivial answers (e.g., \u201cNew York\u201d). That is, answer entities where at least one edge needs to be imputed in order to create an answer path to that entity. Each entity in our multimodal knowledge graph is not limited to a text entity but a multimodal node. Following \\\\([56]\\\\), for each non-trivial answer \\\\(t\\\\) of test query \\\\(q\\\\), we rank it against non-answer entities \\\\(E\\\\text{\\\\[q\\\\]}\\\\). Then the rank of each answer is labeled as \\\\(r\\\\).\\n\\nWe use Mean Reciprocal Rank (MRR): \\\\(\\\\frac{1}{r}\\\\) and Hits-at-\\\\(N\\\\) (\\\\(\\\\text{H}@N\\\\)) : \\\\(1\\\\leq N\\\\) as quantitative metrics.\"}"}
{"id": "LdRZ9SFBku", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We report four metrics of each model on the validation and test sets. All experiments were repeated five times and the variance is shown in the table.\\n\\n| Model          | Val-H@1 \u00b1 | Val-H@3 \u00b1 | Val-H@10 \u00b1 | Val-MRR \u00b1 |\\n|----------------|-----------|-----------|------------|-----------|\\n| TransE [3]     | 11.75     | 29.04     | 31.76      | 14.77     |\\n| Q2B [54]      | 14.99     | 25.78     | 36.76      | 18.80     |\\n| BETAE [56]    | 18.04     | 33.02     | 41.97      | 21.16     |\\n| QA-GNN [87]   | 21.69     | 38.11     | 45.97      | 22.83     |\\n\\n| Model          | Test-H@1 \u00b1 | Test-H@3 \u00b1 | Test-H@10 \u00b1 | Test-MRR \u00b1 |\\n|----------------|------------|------------|-------------|------------|\\n| TransE [3]     | 11.26      | 21.68      | 31.57       | 14.66      |\\n| Q2B [54]      | 14.48      | 25.17      | 36.32       | 18.46      |\\n| BETAE [56]    | 16.35      | 28.67      | 38.45       | 19.27      |\\n| QA-GNN [87]   | 17.65      | 32.75      | 41.67       | 20.75      |\\n\\nWe consider four baselines: TransE [3], Q2B [54] and BETAE [56]. Since the UKnow based plug-in module can be attached to any reasoning models, we implement the Q2B\u2217 with our module based on Q2B and BETAE\u2217 based on BETAE. As shown in Tab. 9, BETAE\u2217 achieves on average 21.64% and 21.23% MRR on the validation and testing set of our dataset, respectively. For a fair comparison (e.g., TransE), our dataset does not construct complex logic such as FOL [14] to evaluate the performance of multi-hop logical reasoning.\\n\\nC.2 Multimodal Event Classification\\nWe propose a novel task called multimodal event classification, leveraging event annotations (Tab. 3) from both Wiki's event categories and our own manual tagging. The event annotation helps intelligent machines understand human activities and history, offering the possibility to identify which type of event or which real historical event a picture or a text is relevant to. As shown in Tab. 10, TCL [85] achieves on 66.80% and 55.87% on ACC@1 when using the image-input on the Event-11 and Event-9185, respectively. We simply modify all the baseline methods and add a late-fusion module after the image/text encoder to support multimodal classification. Results show that TCL with multimodal inputs obtains gains of 1.89% and 5.02% compared with the single-modal, which demonstrates that multimodal pre-training is more helpful for downstream multimodal tasks.\\n\\nC.3 Single- & Cross-Modal Retrieval\\nWe design four kinds of single- & cross-modal retrieval tasks: image-to-image, text-to-text, image-to-text, and text-to-image. The construction of GT is based on the event annotations in $G_m$ (Fig. 4). We treat images or texts belonging to the same news event as a similar semantic cluster, and the goal of retrieval is to recall the nearest neighbors within this cluster. The features used for retrieval are derived from the output of the previous layer of the classifier.\\n\\nAs shown in Tab. 11, TCL [85] achieves on 33.24%, 43.37%, and 45.22% R@1, R@5, R@10 on the zero-shot setting of image retrieval. The results are 58.89%, 68.47%, and 73.91% when fine-tuning the pre-trained parameters, which means the pre-training $\\\\rightarrow$ fine-tuning strategy is extremely beneficial for downstream retrieval.\\n\\nWe provide more details about hyperparameters in Sec. C.5.\\n\\nC.4 Visual Task Adaptation\\nVisual Task Adaptation Benchmark (VTAB) [89] is a diverse, realistic, and challenging vision representation benchmark, containing 19 tasks and covering a broad spectrum of domains and semantics. These tasks are grouped into three sets: NATURAL, SPECIALIZED, and STRUCTURED which utilize natural world, professional technology and artificial environment images respectively. We benchmark models on VTAB with ACC@1. We fine-tune models for 10 epoch in each task and compute the inner product between outputs of the pre-trained models on each task.\"}"}
{"id": "LdRZ9SFBku", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11: A new benchmark of the retrieval task. Zero-shot means freezing the pre-trained parameters then transfer to the test set for inference. Fine-tune means tuning the pre-trained parameters in the training set before inference.\\n\\n| Model        | Retrieval | Zero-Shot | Fine-Tune |\\n|--------------|-----------|-----------|-----------|\\n|              | [R@1]     | [R@5]     | [R@10]    |\\n| CLIP [50]    | IMAGE     | 32.41     | 41.96     | 43.92     |\\n|              |           | 55.97     | 67.44     | 71.28     |\\n| DeCLIP [35]  | IMAGE     | 32.75     | 42.36     | 44.38     |\\n|              |           | 56.96     | 66.59     | 70.95     |\\n| ALBEF [30]   | IMAGE     | 32.88     | 42.76     | 44.79     |\\n|              |           | 58.56     | 67.83     | 72.24     |\\n| TCL [85]     | IMAGE     | 33.24     | 43.37     | 45.22     |\\n|              |           | 58.89     | 68.47     | 73.91     |\\n| CLIP [50]    | TEXT      | 33.02     | 42.56     | 46.03     |\\n|              |           | 56.50     | 65.12     | 70.20     |\\n| DeCLIP [35]  | TEXT      | 34.00     | 43.97     | 47.11     |\\n|              |           | 55.87     | 65.20     | 70.35     |\\n| ALBEF [30]   | TEXT      | 33.87     | 43.86     | 46.82     |\\n|              |           | 56.77     | 65.91     | 71.15     |\\n| TCL [85]     | TEXT      | 34.67     | 44.25     | 47.67     |\\n|              |           | 56.60     | 65.50     | 70.54     |\\n| CLIP [50]    | IMG-to-TXT| 32.73     | 42.64     | 44.72     |\\n|              |           | 56.32     | 66.93     | 70.61     |\\n| DeCLIP [35]  | IMG-to-TXT| 32.96     | 42.84     | 45.17     |\\n|              |           | 57.21     | 66.80     | 71.26     |\\n| ALBEF [30]   | IMG-to-TXT| 33.20     | 42.97     | 45.32     |\\n|              |           | 58.43     | 67.59     | 71.95     |\\n| TCL [85]     | IMG-to-TXT| 33.37     | 43.25     | 46.04     |\\n|              |           | 58.70     | 67.88     | 72.33     |\\n| CLIP [50]    | TXT-to-IMG| 31.78     | 41.04     | 42.51     |\\n|              |           | 55.74     | 64.38     | 69.56     |\\n| DeCLIP [35]  | TXT-to-IMG| 32.13     | 41.55     | 42.99     |\\n|              |           | 55.84     | 65.12     | 70.32     |\\n| ALBEF [30]   | TXT-to-IMG| 31.95     | 41.32     | 42.85     |\\n|              |           | 57.21     | 66.04     | 71.50     |\\n| TCL [85]     | TXT-to-IMG| 32.56     | 42.04     | 43.74     |\\n|              |           | 57.17     | 65.92     | 71.47     |\\n\\nTable 12: The comparison of w/ and w/o UKnow pre-training. Zero means the model is initialized with all-zero parameters w/o pre-training. CLIP \u2217 means pre-training with origin CLIP contrast loss on our dataset. Ours means UKnow pre-training.\\n\\n| Dataset       | Zero      | CLIP \u2217     | Ours      |\\n|---------------|-----------|------------|-----------|\\n| CIFAR100      | 58.39     | 75.25      | 76.79     |\\n| Caltech101    | 53.54     | 71.74      | 72.73     |\\n| DTD           | 49.26     | 58.39      | 60.44     |\\n| Flowers102    | 52.51     | 77.54      | 78.48     |\\n| Pets          | 58.93     | 74.40      | 76.33     |\\n| SVHN          | 64.24     | 79.42      | 80.56     |\\n| Sun397        | 48.96     | 61.72      | 62.37     |\\n| Camelyon      | 52.44     | 70.42      | 72.23     |\\n| EuroSAT       | 63.95     | 81.56      | 83.27     |\\n| Resisc45      | 60.03     | 76.43      | 77.26     |\\n| Retinopathy   | 58.62     | 67.85      | 65.91     |\\n| ClevrCount    | 62.78     | 81.25      | 82.46     |\\n| ClevrDist     | 62.59     | 80.48      | 81.34     |\\n| DMLab         | 44.27     | 60.03      | 63.37     |\\n| KITTIDist     | 75.89     | 84.33      | 85.61     |\\n| dSprLoc       | 74.48     | 82.66      | 85.12     |\\n| dSprOri       | 67.54     | 83.68      | 85.12     |\\n| sNORBAzim     | 60.89     | 76.57      | 76.64     |\\n| NORBElev      | 74.09     | 74.09      | 76.64     |\\n\\nImages and label texts with prompts [50] through pre-trained image encoders and text encoders as the similarity score. As shown in Tab. 12, our approach obtains gains of avg. 1.14% compared with the origin CLIP when fairly using the same UKnow's data for the upstream pre-training. For the suboptimal performance on the Retinopathy and NORBElev datasets, we carefully examine the composition of both dataset. The Diabetic Retinopathy dataset consists of image-label pairs with high-resolution retinal images labeled to indicate the presence of diabetic retinopathy (DR) on a scale from 0 to 4. Similarly, the NORBElev dataset contains jittered texture images. It is evident that these data significantly differ from the natural images collected in UKnow. In contrast, commonly used general image datasets in practical applications, such as CIFAR-10, tend to show greater improvements when utilizing UKnow. This observation suggests that researchers, when designing advanced knowledge-based pre-training methods with UKnow, should carefully consider balancing data domains according to specific downstream tasks. Additionally, accurate node construction is essential for building a robust multimodal knowledge graph to fully leverage the advantages of UKnow. This underscores the importance of designing effective pre-processing functions, particularly in specialized subfields such as the Retinopathy dataset. In these domains, more dedicated data pre-processing models, such as medical image segmentation and detection models, can be employed to enhance feature extraction.\\n\\nThe backbone of CLIP is ViT-B/32. The cost of pre-train is 26h / 30epoch. The key hyperparameters are bs: 512, lr: 0.001, warmup: 1e4, eps: 1e-8, beta1: 0.9, beta2: 0.999, dim: 512, AdamW. The detailed setting can be found in Sec. C.5. It is essential to highlight that the image-text PAIR constitutes only one type of data in our protocol. By leveraging the capabilities of UKnow, our pre-trained CLIP model can effectively comprehend the inherent knowledge ingrained within the data, resulting in superior performance than the original CLIP model (as observed in Tab. 12, Row2, utilizing image-text PAIR only).\\n\\nC.5 Hyperparameters\\n\\nTab. 13 and Tab. 14 list the hyperparameters that differ on each models and are determined with the validation performance on our dataset. In particular, Tab. 13 lists 7 common hyperparameters, such as learning rate, batch size, warmup, epoch number, etc., employed during pre-training. The pre-trained model is evaluated using a standard pipeline consisting of pre-training on Dataset1, fine-tuning on Dataset2-Train, and testing on either Dataset2-Test/Val. Therefore, we list the hyperparameters used during fine-tuning in Tab. 14, which are slightly different from pre-training. The pre-trained model is evaluated using a standard pipeline consisting of pre-training on Dataset1, fine-tuning on Dataset2-Train, and testing on either Dataset2-Test/Val. Therefore, we list the hyperparameters used during fine-tuning in Tab. 14, which are slightly different from pre-training.\"}"}
{"id": "LdRZ9SFBku", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We omit some of the model results, since ALBEF and TCL share the same set of hyperparameters, and the original CLIP and CLIP-UKnow share the same set of parameters.\\n\\n### Table 13: Hyperparameters for models of pre-training.\\n\\n| Hyperparameter | ALBEF | DeCLIP | CLIP-UKnow |\\n|----------------|-------|--------|------------|\\n| Learning Rate  | 0.0001| 0.001  | 0.001      |\\n| Batch Size     | 128   | 128    | 512        |\\n| Number of Epochs | 30   | 30     | 30         |\\n| Weight Decay   | 0.02  | 0.1    | 0.1        |\\n| Optimizer      | AdamW | AdamW  | AdamW      |\\n| Feature Dim    | 256   | 512    | 512        |\\n| Warmup         | 20 ep | 5000 ep| 10000 ep   |\\n\\n### Table 14: Hyperparameters for models of fine-tuning.\\n\\n| Hyperparameter | ALBEF | DeCLIP | CLIP-UKnow |\\n|----------------|-------|--------|------------|\\n| Learning Rate  | 0.0001| 5e-5   | 5e-5       |\\n| Batch Size     | 128   | 256    | 256        |\\n| Number of Epochs | 128  | 20     | 20         |\\n| Weight Decay   | 0.02  | 0.02   | 0.02       |\\n| Optimizer      | AdamW | AdamW  | AdamW      |\\n| Feature Dim    | 256   | 512    | 512        |\\n| Warmup         | 4 ep  | 6 ep   | 6 ep       |\\n\\n### C.6 Computation Complexity\\n\\nHere we detail the time cost of pre-training and fine-tuning. The GPU is NVIDIA(R) A100, the memory of GPU is 81,251MiB, driver version is 470.154, CUDA version is 11.4. The CPU is Intel(R) Xeon(R) Platinum 8369B @ 2.90GHz with 15 physical computation cores. The environment is Python 3.6.12 with Torch 1.10.1. Results are as shown in Tab. 15 and Tab. 16.\\n\\n### Table 15: The time cost of pre-training.\\n\\n| Model Backbone | Epoch | Batch | Time/h |\\n|----------------|-------|-------|--------|\\n| DeCLIP ViT-B/32 | 30    | 128   | 91     |\\n| ALBEF ViT-B/16  | 30    | 128   | 69     |\\n| TCL ViT-B/16    | 30    | 128   | 67     |\\n| CLIP\u2217 ViT-B/32  | 30    | 512   | 25     |\\n| CLIP-UKnow ViT-B/32 | 30  | 512   | 26     |\\n\\n### Table 16: The time cost of downstream fine-tuning.\\n\\n| Model Backbone | Tasks | Epoch | Batch | Time/h |\\n|----------------|-------|-------|-------|--------|\\n| DeCLIP ViT-B/32 | VTAB  | 20    | 128   | 12     |\\n| ALBEF ViT-B/16  | VTAB  | 20    | 128   | 10     |\\n| TCL ViT-B/16    | VTAB  | 20    | 128   | 10     |\\n| Zero\u2217 ViT-B/32  | VTAB  | 15    | 128   | 3      |\\n| CLIP\u2217 ViT-B/32  | VTAB  | 20    | 256   | 8      |\\n| CLIP-UKnow ViT-B/32 | VTAB | 20    | 256   | 8      |\\n\\n### D Discussion\\n\\n#### D.1 Complexity\\n\\nWe notice that the detailed pipeline and protocol may appear complex and require effort to implement and understand fully. However, this complexity is necessary to ensure that the pipeline is robust, flexible, and capable of handling diverse and multimodal datasets.\\n\\nTo mitigate the implementation challenges, we have designed the pipeline to be modular, like Phase-1/2/3, allowing each phase to be independently replaced, added, or disabled based on specific needs. Moreover, we present an extra dataset documentation and construct a website in Sec. A.1. It provides a detailed data organization, corresponding download links, and an example code to guide users through the process, making the protocol more accessible and easier to adopt. Our goal is to balance complexity with practicality, ensuring that the benefits of a thorough and versatile approach outweigh the initial learning curve.\"}"}
{"id": "LdRZ9SFBku", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D.2 Correlation between the Knowledge View and Phase 1\\n\\nIn Phase 1, Content Extraction is designed to preprocess raw data (such as images and texts) using pre-trained deep learning models, which extract essential information that serves as the foundation for our knowledge view. The extracted content provides a rich, structured collection of attributes and features that capture both global and semantic-level details from the input. It transforms raw data into a set of key-value pairs that represent various aspects of the input content. These key-value pairs encapsulate knowledge at different levels, which are critical for constructing meaningful nodes in the subsequent phases. This structured output essentially forms the knowledge view of our system, where each extracted piece of information is treated as a node attribute. These attributes are later symbolized and linked in Phase 2, leading to the construction of the multimodal knowledge graph in Phase 3. Thus, the content extracted in Phase 1 is directly correlated with the knowledge view, serving as the core data that the entire graph construction process relies upon.\\n\\nD.3 Limitation and Future Work\\n\\nDespite the strides made, our research bears certain limitations. First of all, our current dataset primarily centers on text and image modalities which serve as fundamental pillars for information storage and representation, but lack other useful modalities. In future work, we aim to diversify modalities by augmenting our dataset with a broader range of modalities (e.g., audio, video, 3D, etc.) to facilitate exploration across various downstream tasks. Second, for each downstream task, we selected several basic yet most suitable methods for our work as our baseline, resulting in slight deviations with current state-of-the-art (SOTA) performance. Our primary objective lies in validating the efficacy of our proposed dataset and protocols, and demonstrating the most straightforward and intuitive approach for utilizing our dataset. Hence, we made certain trade-offs, sacrificing some performance by opting for a more rudimentary approach instead of pursuing the SOTA method to enhance understanding and usage. We anticipate that our simplified demonstration will stimulate the community to delve deeper into the potential enhancements that UKnow can offer in improving performance.\\n\\nD.4 Societal Impact\\n\\nAs stated in Sec. 3.2, our dataset originates from publicly accessible international news sources via the Wikipedia API. These sources only contain events that are publicly available and do not include any sensitive information. Consequently, we confidently affirm that our research carries no potential negative societal impacts.\"}"}
{"id": "LdRZ9SFBku", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This work presents a unified knowledge protocol, called UKnow, which facilitates knowledge-based studies from the perspective of data. Particularly focusing on visual and linguistic modalities, we categorize data knowledge into five unit types, namely, in-image, in-text, cross-image, cross-text, and image-text, and set up an efficient pipeline to help construct the multimodal knowledge graph from any data collection. Thanks to the logical information naturally contained in knowledge graph, organizing datasets under UKnow format opens up more possibilities of data usage compared to the commonly used image-text pairs. Following UKnow protocol, we collect, from public international news, a large-scale multimodal knowledge graph dataset that consists of 1,388,568 nodes (with 571,791 vision-related ones) and 3,673,817 triplets. The dataset is also annotated with rich event tags, including 11 coarse labels and 9,185 fine labels. Experiments on 4 benchmarks demonstrate the potential of UKnow in supporting common-sense reasoning and boosting vision-language pre-training with a single dataset, benefiting from its unified form of knowledge organization. See Appendix A to download the dataset.\\n\\n1 Introduction\\n\\nRecent efforts have been attracted to leverage the multimodal knowledge graph [95] for data-driven intelligence. Inspired by the human mastery knowledge network [49], we consider that the multi-modal knowledge graph, which naturally accommodates heterogeneous data based on its format of complex network [93, 77], is well suited for constructing a unified knowledge criterion from the perspective of data. Driven by the multimodal knowledge graph, models can easily introduce external knowledge [57], discover long-range relations [82] and understand more logical semantics [52]. However, existing datasets of the multimodal knowledge graph commonly focus on only one task like common-sense reasoning [81, 46] due to their limited scale and irregular data organization. Therefore, it is imperative to construct a well-organized multimodal knowledge graph dataset with large-scale and rich-logic, which enables delving into deeper foundational problems in lower layers, such as the knowledge based vision-language pre-training.\\n\\nTo this end, we propose UKnow, a Unified Knowledge protocol, which facilitates knowledge-based studies from data perspective. Particularly focusing on visual and linguistic modalities, we categorize data knowledge into five unit types, namely, in-image $I_{in}$, in-text $T_{in}$, cross-image $I_{cross}$, cross-text $T_{cross}$, and image-text $I_{T_{cross}}$. As shown in Fig. 1, these knowledge types are together named Knowledge-View which can be easily used to construct a multimodal knowledge graph ($G_m$).\"}"}
{"id": "LdRZ9SFBku", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fishermen reported seeing the Granville-based Aliz\u00e9 3 on marine-tracker websites working in a bream-spawning ground off the north coast. Some attempted to stop the boat while the Norman Le Brocq fisheries protection vessel was also deployed.\\n\\nFigure 1: Overview of UKnow protocol, consisting of five unit knowledge types, namely, in-image (e.g., object), in-text (e.g., entity), cross-image (e.g., image similarity), cross-text (e.g., text continuity), and image-text (e.g., description).\\n\\nTo verify that UKnow can serve as a standard protocol, we further set up an efficient data processing pipeline, consisting of Phase-1/2/3, to reorganize existing datasets into UKnow\u2019s format. Please note that, this pipeline is also able to automatically extend an existing image-text dataset like LAION-5B [59] with more useful information to build a new dataset. A brief description of each Phase is as follows:\\n\\nPhase-1: Content Extraction. We use pre-trained models to preprocess data and extract useful content. Note that pre-trained models can be replaced/added/disabled freely as needed.\\n\\nPhase-2: Information Symbolization. Since the results obtained in Phase-1 (e.g., images and texts) cannot be used directly for graph construction, we adopt information symbolization strategy to arrange all of them into the index in this phase. This information symbolization strategy numbers all original or generated data by a certain rule, which links the nodes from Phase-1 to make a multimodal graph.\\n\\nPhase-3: Knowledge Construction. Two kinds of internal knowledge (I in, T in) and three kinds of associative knowledge (I cross, T cross, IT cross) are aggregated into one graph (Gm) in this phase as shown in Fig. 1. Following UKnow protocol and above pipeline, we build a novel large-scale multimodal knowledge graph. Considering that a large-scale event dataset is of practical significance for real-world applications, such as information retrieval and public sentiment analysis, our data are collected from public international news. Overall, our dataset contains 1,388,568 nodes of which 571,791 are vision relevant (i.e., news images or visual objects). The number of triples in the entire graph is 3,673,817. To the best of our knowledge, this dataset has become the largest multimodal knowledge graph dataset of international news events. Moreover, to organize data in a more structured way and enhance dataset with more category labels, our dataset introduces a hierarchical event annotation for each news, including Event-11 and Event-9185. Specifically, the former contains general event categories such as \u201cSports, Ceremony, ...\u201d, while the latter consists of real human activity in the history such as \u201c2019 NBA All-Star Game, 2019 Daytona 500, ...\u201d. More details about the annotation are shown in Sec. 3.2, Fig 3, and Tab. 3.\\n\\nIn summary, our contributions are as follows:\\n\\n\u2022 We propose UKnow to introduce the multimodal knowledge graph into the vision field as a new standard of data organization, which features the relation inside data in addition to the original data format. Such a protocol opens up the possibilities of data usage such that more logic-rich downstream tasks can be expected in the future.\\n\\n\u2022 We design an efficient data processing pipeline for constructing dataset following our UKnow protocol, together with a large-scale multimodal knowledge graph dataset collected from public international news. We also equip the dataset with hierarchical event annotations, which can help models understand human activities and history. See Appendix A to download the dataset.\\n\\n\u2022 We provide some examples of the usage of UKnow in practical applications. Experiments on four benchmarks showcase the advantages of UKnow in supporting common-sense reasoning and boosting vision-language pre-training with a unified form of data organization, making it possible to evaluate various tasks on a single dataset.\"}"}
{"id": "LdRZ9SFBku", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Statistics of various multimodal knowledge graph datasets. TRIPLE is the basic component of knowledge graph (Sec. 2.1), WEB and GIT indicate homepage and Github repository respectively. EVENT indicates the news event.\\n\\n| DATASET     | YEAR | MULTIMODAL INFO. | SOURCE                  | NODE | IMAGE | TRIPLE | WEB | GIT | EVENT |\\n|-------------|------|------------------|-------------------------|------|-------|--------|-----|-----|-------|\\n| WN9-IMG-TXT | 2016 | ENT.             | WN18, ImageNet          | 6,555| 63,225| 14,397 | \u2713   |     |       |\\n| ImageGraph  | 2017 | ENT./CONCEPT     | FB15k                   | 14,870| 829,931| 564,010| \u2713  |     |       |\\n| VisualGenome| 2017 | ENT.             | MSCOCO                  | 75,729| 108,077| 1,531,448| \u2713  |     |       |\\n| GAIA        | 2018 | ENT./CONCEPT     | Freebase, Geonames      | 457,000| - | 38,000 | \u2713  |     |       |\\n| MMKG-FB15k  | 2019 | ENT./CONCEPT     | FB15k, Search Engine    | 14,951| 13,444| 592,213| \u2713  | \u2713  |       |\\n| MMKG-DB15k  | 2019 | ENT./CONCEPT     | DB15k, Search Engine    | 14,777| 12,842| 99,028 | \u2713  | \u2713  |       |\\n| MMKG-Y AGO15k | 2019 | ENT./CONCEPT     | Y AGO15k, Search Engine | 15,283| 11,194| 122,886| \u2713  | \u2713  |       |\\n| Richpedia   | 2020 | ENT./REL./CONCEPT| Wikipedia               | 29,985| 2,914,770| 2,708,511| \u2713  |     |       |\\n| VisualSem   | 2020 | ENT./CONCEPT     | BabelNet                | 89,896| 930,000| 1,500,000| \u2713  |     |       |\\n| RESIN       | 2021 | ENT./REL./CONCEPT| News                    | 51,422| 6,399 | 150,220 | \u2713  | \u2713  | \u2713    |\\n| MKG-W       | 2022 | ENT./REL./CONCEPT| Open EA, Search Engine  | 15,000| 14,463| -       | \u2713  |     |       |\\n| MKG-Y       | 2022 | ENT./REL./CONCEPT| Open EA, Search Engine  | 15,000| 14,244| -       | \u2713  |     |       |\\n| MMKB-DB15K  | 2022 | ENT./REL./CONCEPT| Open EA, Search Engine  | 12,842| 12,818| -       | \u2713  |     |       |\\n| MarKG       | 2023 | ENT./CONCEPT     | Wikidata, Search Engine | 11,292| 76,424 | 34,420 | \u2713  |     |       |\\n| Multi-OpenEA| 2023 | ENT./CONCEPT     | Open EA, Search Engine  | 920,000| 2,705,688| -       | \u2713  |     |       |\\n| UMVM        | 2023 | ENT./CONCEPT     | DBpedia, Multi-OpenEA   | 238,208| 1,073,671| 982,626 |     |     |       |\\n| AspectMMKG  | 2023 | ENT./CONCEPT     | Wikipedia, Search Engine| 2,380| 645,456| -       | \u2713  |     |       |\\n| TIV A-KG    | 2023 | ENT./REL./CONCEPT| Wikipedia, Search Engine| 443,580| 1,695,688| 1,382,358| \u2713  |     |       |\\n| VTKG-C      | 2023 | ENT./CONCEPT     | ConceptNet, WordNet     | 43,267| 461,007| 111,491| \u2713  |     |       |\\n| UKnow       | 2024 | ENT./REL./CONCEPT| News, Wikipedia         | 1,388,568| 1,073,671| 3,673,817| \u2713  | \u2713  | \u2713    |\\n\\nexploration. Existing knowledge-based deep learning models are broadly divided into two aspects: (1) external knowledge introduction [12], (2) internal knowledge mining [22]. The former leverages expert knowledge by introducing external data [44, 28, 4] or pre-trained models [76, 58, 11, 86]. The latter means constructing correlations of training data by similarity [48, 13, 17] or discovering favorable substructures of internal models [32, 7, 33, 78]. However, from the perspective of data organization, existing studies often claim to be knowledge-based only using one piece of them, which is actually incomplete and cannot be analogous to the complex knowledge network held by humans. In this work, we build a unified knowledge protocol based on the multimodal knowledge graph to define the unified knowledge on multimodal data.\\n\\n2.2 Multimodal Knowledge Graph Datasets\\n\\nThe Multimodal Knowledge Graph (MMKG) serves as a potent means to store and leverage multimodal knowledge explicitly, which bolsters and enhances model performances across diverse domains. In Tab. 1, we list mainstream multimodal knowledge graph datasets [72, 47, 26, 1, 92, 81, 38, 79, 83, 36, 6, 90, 74, 88, 29], constructed by texts and images with detailed information. In terms of data scale, VisualGenome [26] is a multimodal knowledge graph which contains 40,480 relations, 108,077 image nodes with objects. The ImageGraph [47] further pushed up the number of image nodes to 829,931 but missing the extraction of visual objects. Recently, VisualSem [1] implements a multimodal knowledge graph with 938K image nodes and 89,896 entity nodes, but it only uses 15 types of relation to build the graph. On the route of increasing the number of entity nodes, while Multi-OpenEA [36] boasts 920,000 entity nodes, surpassing prior methods, our endeavor has achieved 1,388,568 nodes, establishing the largest graph thus far. Besides, most of existing multimodal knowledge graphs are more like a vision-similarity-based image library [40, 65] with image descriptions and meta information, it lacks the most valuable feature of the knowledge graph: \\\"The Logical Connection\\\". This logic refers to the additional association between two nodes that were originally unrelated, triggered by a news event involving these two nodes. For example, prior to the news event \\\"Celebrity 1 visits Area 1,\\\" there was no relation between Celebrity 1 and the Area 1. The newly added \\\"visit\\\" relation in <(Celebrity1), visit, (Area1)> tuple exemplifies this logic, which is highly beneficial for downstream tasks. Generally speaking, the above news refer to international news, which carries the most complex event logic as well as plentiful multimodal information [75]. To completely exploit the advantages of multimodal knowledge graphs, building a dataset using event logic from international news is a natural approach. However, there is not yet a large multimodal knowledge graph of news events. RESIN [79] is a recently published multimodal knowledge graph containing 24 types of entities, 46 types of relations and 67 types of events. The larger and fresher CLIP-Event [33] is a event rich dataset with 106,875 images and 187 types of events extracted by a text information extraction system [92, 37]. Actually, CLIP-Event is not a knowledge graph and its definition of \\\"event\\\" is not a news event but an action. In summary, one of goals of our work is to build a large, and realistic news-event rich, multimodal knowledge graph dataset from international news.\"}"}
{"id": "LdRZ9SFBku", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.3 Knowledge-based Downstream Tasks\\n\\nThanks to the innovative unified knowledge proposed by our UKnow protocol, our dataset can readily accommodate a variety of downstream tasks. In this study, we opt for common-sense reasoning and vision-language pre-training as experimental domains to validate our dataset. Common-sense reasoning is an extremely popular task in the field of knowledge graph. Since our dataset is based on the knowledge graph, the performance validation on common-sense reasoning is indispensable. Moreover, the representations from Vision-Language Pre-training models are capable of diminishing the necessity for intricate task-specific architectures [9], which allows the knowledge to further flow into various downstream tasks. By incorporating these two tasks, we are able to maximize the assessment of the dataset's knowledge validity.\\n\\nCommon-sense Reasoning.\\n\\nCommon-sense reasoning means answering queries by logic permutations. The specific task in this work is the link prediction. Various works [3, 70, 68, 60, 94, 53] achieve reasoning by embedding entities and relations in knowledge graph into low-dimensional vector space. Path-based methods [27, 82, 63, 51] start from anchor entities and determine the answer set by traversing the intermediate entities via relational path. There are also GCN [25] based methods [61, 16] pass message to iterate graph representation for reasoning.\\n\\nVision-Language Pre-training\\n\\nVision-language pre-training (VLP) can be divided into three categories based on how they encode images [10]: OD-based region features [5, 31, 34, 41, 66, 69], CNN-based grid feature [62, 19, 20] and ViT-based patch features [84, 30, 24]. Pre-training objectives are usually: masked language/image modeling (MLM/MIM) [2, 9, 39], image-text matching (ITM) [34, 19, 10], and image-text contrastive learning (ITC) [30, 50, 35].\\n\\n3 UKnow\\n\\nWe commence by introducing the overall architecture of UKnow in Sec. 3.1. Then the detailed exposition of the data collection process for the new dataset and statistics are presented in Sec. 3.2 and Sec. 3.3. In Sec. 4, we lastly provide the guidance to researchers on how to integrate the multimodal knowledge graph and effectively design a UKnow-based model.\\n\\nCompared to previous libraries-like methods [40, 65] with simple descriptions and meta-information, which lack the logical connection, the most valuable feature of our data processing pipeline is to endow with more logical connections to achieve superior performance in various tasks. As shown in Fig. 2, particularly focusing on visual and linguistic modalities, we categorize data knowledge into five unit types. Then we devise an efficient data processing pipeline to help reorganize existing datasets or create a new one under UKnow format. The construction process of UKnow can be invoked separately for any multimodal data to standardize the knowledge. As shown in Fig. 3, the whole pipeline is mainly empowered by three parts: content extraction, information symbolization, and knowledge construction.\"}"}
{"id": "LdRZ9SFBku", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Detailed data organization under UKnow protocol, which builds the multimodal (image & text) graph $G_m$ based on the Knowledge-View ($I_{in}, T_{in}, I_{cross}, T_{cross},$ and $IT_{cross}$). Each node owns up to 22 attributes shown as $N_p$ in Fig. 3. The original output $N_{ori}$ into a $K:V$ dictionary $N_p$. The KEY of $N_p$ are shown in top right corner of Fig. 3 (Phase-1). $N_p$ is also used as the attribute of each node in the final output multimodal knowledge graph $G_m$.\\n\\nPhase-2: Information Symbolization. Since Images and texts cannot be used directly for graph construction, we design the Phase-2 to number all original or generated data by a certain rule, then Phase-3 links these nodes to make a multimodal graph. Information Symbolization is used to subscript $N_p$ to edge index $N_e$ or node index $N_n$: (1) The symbolization for edges $N_e$ is based on the category or visual / semantic similarity. For example, \\\\[111\\\\] title_title_clip is a kind of parallelism edge which is constructed by the cosine similarity of clip features of news titles. (2) The symbolization for nodes $N_n$ is divided into three levels: \\\\[fact, image / text, object / entity\\\\]. As shown in Fig. 3, \\\\[L1.\u2217\\\\] means fact-level which is an abstraction of a piece of news. The real index used in our multimodal knowledge graph would be \\\\{\\\\[L1.0, L1.1, L1.2, ..., \\\\}. Similarly, \\\\[L2.\u2217\\\\] means image / text-level which is the symbolization of images or texts from news, \\\\[L3.\u2217\\\\] is the object in image or entity in text. The index for all nodes is eventually shuffled, that is, the real index would be \\\\{\\\\[L1.0, L2.1, L1.2, L3.3, L3.4, ..., \\\\}.\\n\\nWe provide the clearer explanations about the motivation of Phase-2. As stated in Sec. 3.2, our data are collected from international news, which encompasses a wide variety of text and images. Although Phase-1 preprocesses the data like detection and segmentation, the resulting features are still a huge volume as it contains detailed information extracted from the news. While this detailed information is valuable for constructing a knowledge graph, the computational demands and complexity far exceed available resources. Thus, a common approach in knowledge graph construction is to store data and their relationships as indices, as done in the Phase-2 Information Symbolization stage. This means it has the following benefits: efficiency in storage and retrieval, fast lookup and traversal, uniqueness and consistency, scalability, and simplification of graph operations.\\n\\n| Phrase Construction Method View | Num. |\\n|---------------------------------|------|\\n| Phrase-2 Detection Category     | 648,871 |\\n| NER Category                    | 1,606,936 |\\n| Similarity&Manual Annotation    | 684,207 |\\n| Similarity&Manual Annotation    | 140,133 |\\n| Phrase-3 Manual Event Annotation| 593,670 |\\n\\nTable 2: Edge ($N_e$) construction and statistics.\\n\\nPhase-3: Knowledge Construction. We categorize data knowledge into five unit types, namely, in-text ($T_{in}$), in-image ($I_{in}$), inter-text ($T_{cross}$), inter-image ($I_{cross}$), and image-text ($IT_{cross}$) which are together called Knowledge-View detailed in Fig. 2(a) and Fig. 2(b). In this phase, we aggregate two kinds of internal knowledge ($I_{in}, T_{in}$) and three kinds of associative knowledge ($I_{cross}, T_{cross}, IT_{cross}$) in one graph $G_m$, which are usually introduced independently in previous studies.\"}"}
{"id": "LdRZ9SFBku", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Pipeline of dataset construction following UKnow protocol. Phase-1: Content Extraction (**Np**), Phase-2: Information Symbolization (**Nn**, **Ne**), and Phase-3: Knowledge Construction (**Gm**).\\n\\n**Nn** hides the real node index for easy understanding, the actual number is much more than **Ne**.\\n\\nKnowledge graph **Gm** (Fig. 2(c)). Since **Ne** and **Nn** are both isolated, we use four kinds of correlation methods including semantic similarity, visual similarity, annotations, and categories to make connections between **Nn** by **Np** shown in Tab. 2.\\n\\n### 3.2 Dataset Collection\\n\\nFollowing the proposed protocol and three phases, we collect a new dataset, a large-scale multimodal knowledge graph from public international news. Specifically, based on the Wikipedia API [43] and our crawler system, we grab all the data of \\\"Worldwide Current Events\\\" from Wikipedia. As demonstrated in the top of Fig. 4, we propose two category sets of news event called: Event-11 and Event-9185, which is coarse-grained and fine-grained respectively. For example, \\\"Sports\\\" is a kind of coarse-grained event label in Event-11 and \\\"2019 Daytona 500\\\" is a fine-grained label in Event-9185, detailed in Tab. 3. Since Wikipedia only records the news URL (downward black arrow in Fig. 4) and the HTML of original news from different news platforms is inconsistent, it is difficult to design a uniform crawler to get the well-structured raw data of news. Thus, we manually read each news and collect the original data (rightward black arrow). By this way, each news in our dataset is marked with extremely clean title, content, time, image, image description, event description, [hierarchical] event name (e.g., \\\"Armed conflicts and attacks \u2192 War in Donbass\\\"), and event attribute (location, date, etc.). Subsequently, as shown in the bottom right of Fig. 4, we apply the designed pipeline to sequentially undergo phases 1/2/3 to restructure the above extracted raw data, resulting in the knowledge graph under the UKnow format.\"}"}
{"id": "LdRZ9SFBku", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The 2019 Daytona 500, the 61st running of the event, was a Monster Energy NASCAR Cup Series race held on February 17, 2019. Contested over 207 laps, the race was won by Denny Hamlin, who posted a time of 2 hours, 56 minutes, and 14.681 seconds. It was his second win in the event, the first coming in 2016. Jamie McMurray (40), Erik Jones (20), William Byron (24), Brad Keselowski (2), Brendan Gaughan (62), Clint Bowyer (14) and Chase Elliott (9) collided in Turn 3 during the NASCAR Daytona 500 auto race at Daytona International Speedway Sunday in Daytona Beach.\\n\\nDenny Hamlin wins the 2019 Daytona 500. It is his second win in the event, the first coming in 2016. (Fox News)\\n\\nFurthermore, in addition to utilizing intricate annotation files (e.g., Fig. 4) as inputs mentioned above, another major advantage of the proposed conversion pipeline is its ability to accommodate common image-text pair annotations expressed in the format of \\n\\n```\\n[<image description>] ./xxx.jpg\\n```\\n\\nas the fundamental input. This design allows UKNow to automatically construct a new dataset with more useful information from an existing image-text pair dataset. Taking LAION-5B [59] as an example, which solely comprises pairs of images and text, our pipeline can extract more features from them like objects, and thus expand LAION-5B into a larger and more practical dataset. However, given the absence of high-level event logic, this type of input does not lend itself to the creation of L1.* nodes and event-related edges.\"}"}
{"id": "LdRZ9SFBku", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Histogram of the number of indexes in our dataset. The x-axis in the upper left corner (Node Index) corresponds to the order of the $I_{in}$ in Fig. 3.\\n\\n| Node Index ($N_n$) | Edge Index ($N_e$) | Edge Index ($N_e$) | Number |\\n|-------------------|-------------------|-------------------|--------|\\n| 0.1               | 0.5               | 1.0              | 2.0    | 3.0   | 4.0   | 5.0   | 6.0   | 7.0   | 8.0   | 9.0   | 10.0  | 11.0  | 12.0  | 13.0  | 14.0  | 15.0  | 16.0  | 17.0  | 18.0  | 19.0  | 20.0  | 21.0  | 22.0  | 23.0  | 24.0  | 25.0  | 26.0  | 27.0  | 28.0  | 29.0  | 30.0  | 31.0  | 32.0  | 33.0  | 34.0  | 35.0  | 36.0  | 37.0  | 38.0  | 39.0  | 40.0  | 41.0  | 42.0  | 43.0  | 44.0  | 45.0  | 46.0  | 47.0  | 48.0  | 49.0  | 50.0  | 51.0  | 52.0  | 53.0  | 54.0  | 55.0  | 56.0  | 57.0  | 58.0  | 59.0  | 60.0  | 61.0  | 62.0  | 63.0  | 64.0  | 65.0  | 66.0  | 67.0  | 68.0  | 69.0  | 70.0  | 71.0  | 72.0  | 73.0  | 74.0  | 75.0  | 76.0  | 77.0  | 78.0  | 79.0  | 80.0  | 81.0  | 82.0  | 83.0  | 84.0  | 85.0  | 86.0  | 87.0  | 88.0  | 89.0  | 90.0  | 91.0  | 92.0  | 93.0  | 94.0  | 95.0  | 96.0  | 97.0  | 98.0  | 99.0  | 100.0 | 101.0 | 102.0 | 103.0 | 104.0 | 105.0 | 106.0 | 107.0 | 108.0 | 109.0 | 110.0 | 111.0 | 112.0 | 113.0 | 114.0 | 115.0 | 116.0 | 117.0 | 118.0 | 119.0 | 120.0 | 121.0 | 122.0 | 123.0 | 124.0 | 125.0 | 126.0 | 127.0 | 128.0 | 129.0 | 130.0 | 131.0 | 132.0 | 133.0 | 134.0 | 135.0 | 136.0 | 137.0 | 138.0 | 139.0 | 140.0 | 141.0 | 142.0 | 143.0 | 144.0 | 145.0 | 146.0 | 147.0 | 148.0 | 149.0 | 150.0 | 151.0 | 152.0 | 153.0 | 154.0 | 155.0 | 156.0 | 157.0 | 158.0 | 159.0 | 160.0 | 161.0 | 162.0 | 163.0 | 164.0 | 165.0 | 166.0 | 167.0 | 168.0 | 169.0 | 170.0 | 171.0 | 172.0 | 173.0 | 174.0 | 175.0 | 176.0 | 177.0 | 178.0 | 179.0 | 180.0 | 181.0 | 182.0 | 183.0 | 184.0 | 185.0 | 186.0 | 187.0 | 188.0 | 189.0 | 190.0 | 191.0 | 192.0 | 193.0 | 194.0 | 195.0 | 196.0 | 197.0 | 198.0 | 199.0 | 200.0 | 201.0 | 202.0 | 203.0 | 204.0 | 205.0 | 206.0 | 207.0 | 208.0 | 209.0 | 210.0 | 211.0 | 212.0 | 213.0 | 214.0 | 215.0 | 216.0 | 217.0 | 218.0 | 219.0 | 220.0 | 221.0 | 222.0 | 223.0 | 224.0 | 225.0 | 226.0 | 227.0 | 228.0 | 229.0 | 230.0 | 231.0 | 232.0 | 233.0 | 234.0 | 235.0 | 236.0 | 237.0 | 238.0 | 239.0 | 240.0 | 241.0 | 242.0 | 243.0 | 244.0 | 245.0 | 246.0 | 247.0 | 248.0 | 249.0 | 250.0 | 251.0 | 252.0 | 253.0 | 254.0 | 255.0 | 256.0 | 257.0 | 258.0 | 259.0 | 260.0 | 261.0 | 262.0 | 263.0 | 264.0 | 265.0 | 266.0 | 267.0 | 268.0 | 269.0 | 270.0 | 271.0 | 272.0 | 273.0 | 274.0 | 275.0 | 276.0 | 277.0 | 278.0 | 279.0 | 280.0 | 281.0 | 282.0 | 283.0 | 284.0 | 285.0 | 286.0 | 287.0 | 288.0 | 289.0 | 290.0 | 291.0 | 292.0 | 293.0 | 294.0 | 295.0 | 296.0 | 297.0 | 298.0 | 299.0 | 300.0 | 301.0 | 302.0 | 303.0 | 304.0 | 305.0 | 306.0 | 307.0 | 308.0 | 309.0 | 310.0 | 311.0 | 312.0 | 313.0 | 314.0 | 315.0 | 316.0 | 317.0 | 318.0 | 319.0 | 320.0 | 321.0 | 322.0 | 323.0 | 324.0 | 325.0 | 326.0 | 327.0 | 328.0 | 329.0 | 330.0 | 331.0 | 332.0 | 333.0 | 334.0 | 335.0 | 336.0 | 337.0 | 338.0 | 339.0 | 340.0 | 341.0 | 342.0 | 343.0 | 344.0 | 345.0 | 346.0 | 347.0 | 348.0 | 349.0 | 350.0 | 351.0 | 352.0 | 353.0 | 354.0 | 355.0 | 356.0 | 357.0 | 358.0 | 359.0 | 360.0 | 361.0 | 362.0 | 363.0 | 364.0 | 365.0 | 366.0 | 367.0 | 368.0 | 369.0 | 370.0 | 371.0 | 372.0 | 373.0 | 374.0 | 375.0 | 376.0 | 377.0 | 378.0 | 379.0 | 380.0 | 381.0 | 382.0 | 383.0 | 384.0 | 385.0 | 386.0 | 387.0 | 388.0 | 389.0 | 390.0 | 391.0 | 392.0 | 393.0 | 394.0 | 395.0 | 396.0 | 397.0 | 398.0 | 399.0 | 400.0 | 401.0 | 402.0 | 403.0 | 404.0 | 405.0 | 406.0 | 407.0 | 408.0 | 409.0 | 410.0 | 411.0 | 412.0 | 413.0 | 414.0 | 415.0 | 416.0 | 417.0 | 418.0 | 419.0 | 420.0 | 421.0 | 422.0 | 423.0 | 424.0 | 425.0 | 426.0 | 427.0 | 428.0 | 429.0 | 430.0 | 431.0 | 432.0 | 433.0 | 434.0 | 435.0 | 436.0 | 437.0 | 438.0 | 439.0 | 440.0 | 441.0 | 442.0 | 443.0 | 444.0 | 445.0 | 446.0 | 447.0 | 448.0 | 449.0 | 450.0 | 451.0 | 452.0 | 453.0 | 454.0 | 455.0 | 456.0 | 457.0 | 458.0 | 459.0 | 460.0 | 461.0 | 462.0 | 463.0 | 464.0 | 465.0 | 466.0 | 467.0 | 468.0 | 469.0 | 470.0 | 471.0 | 472.0 | 473.0 | 474.0 | 475.0 | 476.0 | 477.0 | 478.0 | 479.0 | 480.0 | 481.0 | 482.0 | 483.0 | 484.0 | 485.0 | 486.0 | 487.0 | 488.0 | 489.0 | 490.0 | 491.0 | 492.0 | 493.0 | 494.0 | 495.0 | 496.0 | 497.0 | 498.0 | 499.0 | 500.0 | 501.0 | 502.0 | 503.0 | 504.0 | 505.0 | 506.0 | 507.0 | 508.0 | 509.0 | 510.0 | 511.0 | 512.0 | 513.0 | 514.0 | 515.0 | 516.0 | 517.0 | 518.0 | 519.0 | 520.0 | 521.0 | 522.0 | 523.0 | 524.0 | 525.0 | 526.0 | 527.0 | 528.0 | 529.0 | 530.0 | 531.0 | 532.0 | 533.0 | 534.0 | 535.0 | 536.0 | 537.0 | 538.0 | 539.0 | 540.0 | 541.0 | 542.0 | 543.0 | 544.0 | 545.0 | 546.0 | 547.0 | 548.0 | 549.0 | 550.0 | 551.0 | 552.0 | 553.0 | 554.0 | 555.0 | 556.0 | 557.0 | 558.0 | 559.0 | 560.0 | 561.0 | 562.0 | 563.0 | 564.0 | 565.0 | 566.0 | 567.0 | 568.0"}
{"id": "LdRZ9SFBku", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"task adaptation. While these applications benefit from the structured nature of KGs, the underlying datasets may not be flexible enough to support a wide range of tasks, particularly those that require cross-modal reasoning or dynamic context adaptation.\\n\\nIn contrast, UKnow is inherently designed to handle multimodal data (e.g., images, text) in a unified structure. This allows for seamless integration and interaction between different types of data, making it particularly well-suited for tasks that require cross-modal reasoning, such as vision-language pre-training and complex event understanding. Besides, UKnow introduces a hierarchical structure that organizes nodes into levels and incorporates logical connections between them. The unified structure and logical richness of UKnow make it highly versatile for a wide range of downstream tasks. The ability to evaluate various tasks on a unified Knowledge Graph also reduces the complexity of model development and evaluation, leading to more efficient and effective AI solutions.\\n\\n4 Usage of UKnow\\n\\n4.1 UKnow for Common-sense Reasoning\\n\\nSince UKnow is reasoning compatible, i.e., it naturally supports all KG-reasoning models, we directly implemented the commonly used KG-reasoning models (e.g., TransE [3], Q2B [54]) on UKnow. We propose a plug-in module which aggregates node features within a small sub-graph region to achieve a better central node features. We briefly introduce how to implement this module. Suppose $N(e) \\\\equiv \\\\{e_{neib} | r(e_{neib}, e) \\\\lor r(e, e_{neib}), r \\\\in R\\\\}$ is the collection of neighbors of each central node $e$. The calculation expression of the new representation $e'$ of $e$ is as follow:\\n\\n$$e' = \\\\text{MLP}(\\\\text{Flatten}(\\\\text{ReLU}(\\\\omega_n \\\\star (\\\\tau'(e, N_e') + b_n))) + b_n),$$\\n\\nwhere $e \\\\in \\\\mathbb{R}^d$ is the node feature before enhancement, $e'$ is the new feature, $\\\\star$ denotes a 2D convolution operation, $\\\\omega_n$ is the filter, $b_n$ is the bias and the specification of MLP is $\\\\mathbb{R}^{m_1 \\\\times m_2 \\\\times \\\\mathbb{R}^d}$.\\n\\n4.2 UKnow for Vision-Language Pre-training\\n\\nFollowing the recent works [33], our work applies CLIP [50] as the pre-trained backbone benefit from its strong downstream performance. Specifically, the text encoder first tokenize the input text description into the word sequence, and then projects them into word embeddings $W_0 = \\\\{w_{10}, w_{20}, \\\\ldots, w_{N0}\\\\} \\\\in \\\\mathbb{R}^{N \\\\times d_t}$. $W_0$ is fed into a $L$-layer Transformer [71] with the architecture modifications described in BERT [9]. And the final text embedding $z_T$ is obtained by projecting the last token, which corresponds to the $[\\\\text{EOS}]$ (the end of sequence) token, from the last layer of the text encoder, i.e., $z_T = \\\\text{TextProj}(w_{NL})$, $z_T \\\\in \\\\mathbb{R}^{d_t}$. As for the vision encoder, the input image $I$ is first split into $M$ non-overlapping patches, and projected into a sequence of patch tokens $E_0 \\\\in \\\\mathbb{R}^{M \\\\times d_v}$. Then, $E_0$ is fed into a $L$-layer Transformer-based architecture along with a learnable $[\\\\text{CLS}]$ token $c_0$. The final image embedding $z_I$ is obtained by projecting the $[\\\\text{CLS}]$ token from the last layer of the vision encoder, i.e., $z_I = \\\\text{VisProj}(c_{vL}, E_{vL})$, $z_I \\\\in \\\\mathbb{R}^{d_v}$. Since we have Knowledge-View, a new dimension $z_k$ which is used to represent knowledge is introduced:\\n\\n$$z_k = \\\\text{Concat}(I_{in}(z_I), T_{in}(z_T), I_{cross}(z_I), T_{cross}(z_T)),$$\\n\\nwhere $I_{in}(\\\\cdot)$ and $T_{in}(\\\\cdot)$ mean to get the embedding of the $[L_3 \\\\ast]$ nodes ($N_n$) from $G_m$ via $N_{e}$, $I_{cross}(\\\\cdot)$ and $T_{cross}(\\\\cdot)$ mean to get the embedding of $[L_2 \\\\ast]$ from $G_m$. Therefore, the similarity score between the image, text and knowledge can be calculated with the cosine similarity as follow:\\n\\n$$s(T, I, k) = \\\\frac{z_T^\\\\top z_I}{\\\\|z_T\\\\|\\\\|z_I\\\\|} + \\\\frac{z_k^\\\\top z_I}{\\\\|z_k\\\\|\\\\|z_I\\\\|} + \\\\frac{z_k^\\\\top z_T}{\\\\|z_k\\\\|\\\\|z_T\\\\|}.$$\"}"}
{"id": "LdRZ9SFBku", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for our dataset. Specifically, Common-sense Reasoning is a conventional and fundamental task in our domain, aligning closely with our dataset. Then we perform multiple downstream tasks to verify the performance of the pretrained model trained with our dataset. For more details about task description/training setting/evaluation metric/analysis, please refer to Sec. C.\\n\\nCommon-sense Reasoning. We implement the Q2B\\\\textsuperscript{*} with our UKnow based plug-in module based on Q2B \\\\cite{54} and BETAE\\\\textsuperscript{*} based on BETAE \\\\cite{55}. As shown in Tab. 9, BETAE\\\\textsuperscript{*} achieves on average 21.64\\\\% and 21.23\\\\% MRR on the validation and testing set of our dataset. It indicates that our UKnow based module can significantly improve the performance of existing methods.\\n\\nMultimodal Event Classification. As shown in Tab. 10, TCL \\\\cite{85} achieves on 66.80\\\\% and 55.87\\\\% on ACC@1 when using the image-input on the Event-11 and Event-9185, respectively. We add a late-fusion module after the image/text encoder for all methods to support multimodal classification. Results show that TCL obtains gains of 1.89\\\\% and 5.02\\\\% compared with the singlemodal input, which demonstrates that multimodal pre-training is more helpful for downstream multimodal tasks.\\n\\nSingle- & Cross-Modal Retrieval. As shown in Tab. 11, TCL \\\\cite{85} achieves on 33.24\\\\%, 43.37\\\\% and 45.22\\\\% R@1, R@5, R@10 on the zero-shot setting of image retrieval. The results are 58.89\\\\%, 68.47\\\\% and 73.91\\\\% when fine-tuning the pre-trained parameters, which means the pre-training \u2192 fine-tuning strategy is extremely beneficial for downstream retrieval.\\n\\nVisual Task Adaptation. As shown in Tab. 12, our approach obtains gains of avg. 1.14\\\\% compared with the origin CLIP when fairly using the same UKnow\u2019s data for the upstream pre-training. It is essential to highlight that the image-text PAIR constitutes only one type of data in our protocol. By leveraging the capabilities of UKnow, our pre-trained CLIP model can effectively comprehend the inherent knowledge, resulting in superior performance than original CLIP model (Tab. 12, Row2).\\n\\n4.4 Practical Applications in Other Domains\\n\\nUKnow is a general multimodal knowledge graph construction protocol that can be easily adapted to different domains by adjusting P in Phase-1 to the relevant processing modules required. Due to issues such as time and effort and difficulty of data acquisition, in this paper, we only use international news as an example, given its significance in the multimodal field and its ability to highlight UKnow\u2019s strengths in handling multimodal data. In the future, as we mentioned in Sec. D.3, we aim to diversify modalities by augmenting our dataset with a broader range of modalities (e.g., audio, video, 3D, etc.) to facilitate exploration across various downstream tasks. Here\u2019s an example of how to extend UKnow to the video domain and modality: (1) Phase-1: Replace P with operations like Video Captioning, Action Recognition, Video Summarization, or Object Detection and Tracking to process the video content. (2) Phase-2: Organize the processed video features into the node index and construct relationship edges with other modalities such as text, images, and audio. (3) Phase-3: Utilize Phase 3 to build the knowledge graph, which can then be applied to various knowledge-based downstream tasks.\\n\\n5 Conclusion\\n\\nThis paper presents a unified knowledge protocol called UKnow to establish the standard of knowledge from the perspective of data. Following this protocol, we collect a novel and the largest multimodal knowledge graph dataset from public international news with rich news event annotations, which can help intelligent machines understand human activities and history. The specific tasks addressed in this paper are the common-sense reasoning and vision-language pre-training. The former is a typical task in the knowledge graph field, and the latter brings knowledge to various downstream tasks. We also present a series of novel logic-rich downstream tasks to showcase the advantages of UKnow. In future work, we will continuously expand the data of different scales based on the UKnow protocol.\"}"}
{"id": "LdRZ9SFBku", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Houda Alberts, Teresa Huang, Yash Deshpande, Yibo Liu, Kyunghyun Cho, Clara Vania, and Iacer Calixto. Visualsem: a high-quality knowledge graph for vision and language. arXiv preprint arXiv:2008.09150, 2020.\\n\\n[2] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021.\\n\\n[3] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems, 2013.\\n\\n[4] Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu. Recall and learn: Fine-tuning deep pretrained language models with less forgetting. arXiv preprint arXiv:2004.12651, 2020.\\n\\n[5] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104\u2013120, 2020.\\n\\n[6] Zhuo Chen, Lingbing Guo, Yin Fang, Yichi Zhang, Jiaoyan Chen, Jeff Z Pan, Yangning Li, Huajun Chen, and Wen Zhang. Rethinking uncertainly missing and ambiguous visual modality in multi-modal entity alignment. In International Semantic Web Conference, pages 121\u2013139. Springer, 2023.\\n\\n[7] Zewen Chi, Li Dong, Bo Zheng, Shaohan Huang, Xian-Ling Mao, Heyan Huang, and Furu Wei. Improving pretrained cross-lingual language models via self-labeled word alignment. arXiv preprint arXiv:2106.06381, 2021.\\n\\n[8] Jaemin Cho, Seunghyun Yoon, Ajinkya Kale, Franck Dernoncourt, Trung Bui, and Mohit Bansal. Fine-grained image captioning with clip reward. In Findings of NAACL, 2022.\\n\\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n\\n[10] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, et al. An empirical study of training end-to-end vision-and-language transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18166\u201318176, 2022.\\n\\n[11] Sepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei Shu. Zero-shot out-of-distribution detection based on the pretrained model clip. In Proceedings of the AAAI conference on artificial intelligence, 2022.\\n\\n[12] Yuxia Geng, Jiaoyan Chen, Xiang Zhuang, Zhuo Chen, Jeff Z Pan, Juan Li, Zonggang Yuan, and Huajun Chen. Benchmarking knowledge-driven zero-shot learning. Journal of Web Semantics, page 100757, 2023.\\n\\n[13] Tianyu Guo, Hong Liu, Zhan Chen, Mengyuan Liu, Tao Wang, and Runwei Ding. Contrastive learning from extremely augmented skeleton sequences for self-supervised action recognition. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 762\u2013770, 2022.\\n\\n[14] Kelvin Guu, John Miller, and Percy Liang. Traversing knowledge graphs in vector space. arXiv preprint arXiv:1506.01094, 2015.\\n\\n[15] Will Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, and Jure Leskovec. Embedding logical queries on knowledge graphs. Advances in neural information processing systems, 2018.\\n\\n[16] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 2017.\\n\\n[17] Tengda Han, Weidi Xie, and Andrew Zisserman. Self-supervised co-training for video representation learning. Advances in Neural Information Processing Systems, pages 5679\u20135690, 2020.\\n\\n[18] Xiangteng He, Yulin Pan, Mingqian Tang, Yiliang Lv, and Yuxin Peng. Learn from unlabeled videos for near-duplicate video retrieval. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1002\u20131011, 2022.\\n\\n[19] Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu. Seeing out of the box: End-to-end pre-training for vision-language representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12976\u201312985, 2021.\"}"}
{"id": "LdRZ9SFBku", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849, 2020.\\n\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904\u20134916, 2021.\\n\\nLonglong Jing and Yingli Tian. Self-supervised visual feature learning with deep neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence, pages 4037\u20134058, 2020.\\n\\nAishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1780\u20131790, 2021.\\n\\nWonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583\u20135594, 2021.\\n\\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.\\n\\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, pages 32\u201373, 2017.\\n\\nNi Lao, Tom Mitchell, and William Cohen. Random walk inference and learning in a large scale knowledge base. In Proceedings of the 2011 conference on empirical methods in natural language processing, pages 529\u2013539, 2011.\\n\\nAnne Lauscher, Olga Majewska, Leonardo FR Ribeiro, Iryna Gurevych, Nikolai Rozanov, and Goran Glava\u0161. Common sense or world knowledge? investigating adapter-based knowledge injection into pretrained transformers. arXiv preprint arXiv:2005.11787, 2020.\\n\\nJaejun Lee, Chanyoung Chung, Hochang Lee, Sungho Jo, and Joyce Whang. Vista: Visual-textual knowledge graph representation learning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7314\u20137328, 2023.\\n\\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems, pages 9694\u20139705, 2021.\\n\\nLiunian Harold Li*, Pengchuan Zhang*, Haotian Zhang*, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded language-image pre-training. In IEEE Conf. Comput. Vis. Pattern Recog., 2022.\\n\\nManling Li, Ruochen Xu, Shuohang Wang, Luowei Zhou, Xudong Lin, Chenguang Zhu, Michael Zeng, Heng Ji, and Shih-Fu Chang. Clip-event: Connecting text and images with event structures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16420\u201316429, 2022.\\n\\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137, 2020.\\n\\nYangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. arXiv preprint arXiv:2110.05208, 2021.\\n\\nYangning Li, Jiaoyan Chen, Yinghui Li, Yuejia Xiang, Xi Chen, and Hai-Tao Zheng. Vision, deduction and alignment: An empirical study on multi-modal knowledge graph alignment. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE, 2023.\\n\\nYing Lin, Heng Ji, Fei Huang, and Lingfei Wu. A joint neural model for information extraction with global features. In Proceedings of the 58th annual meeting of the association for computational linguistics, pages 7999\u20138009, 2020.\"}"}
{"id": "LdRZ9SFBku", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ye Liu, Hui Li, Alberto Garcia-Dur\u00e1n, Mathias Niepert, Daniel Onoro-Rubio, and David S Rosenblum. Mmkg: multi-modal knowledge graphs. In European Semantic Web Conference, pages 459\u2013474, 2019.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\\n\\nZiwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1096\u20131104, 2016.\\n\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 2019.\\n\\nRuotian Luo, Brian Price, Scott Cohen, and Gregory Shakhnarovich. Discriminability objective for training descriptive captions. arXiv preprint arXiv:1803.04376, 2018.\\n\\nMartin Majlis. Wikipedia-api. https://pypi.org/project/Wikipedia-API/.\\n\\nKenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, and Marcus Rohrbach. Krisp: Integrating implicit and symbolic knowledge for open-domain knowledge-based vqa. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14111\u201314121, 2021.\\n\\nRon Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021.\\n\\nHatem Mousselly-Sergieh, Teresa Botschen, Iryna Gurevych, and Stefan Roth. A multimodal translation-based approach for knowledge graph representation learning. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 225\u2013234, 2018.\\n\\nDaniel O\u00f1oro-Rubio, Mathias Niepert, Alberto Garc\u00eda-Dur\u00e1n, Roberto Gonz\u00e1lez, and Roberto J L\u00f3pez-Sastre. Answering visual-relational queries in web-extracted knowledge graphs. arXiv preprint arXiv:1709.02314, 2017.\\n\\nXingjia Pan, Fan Tang, Weiming Dong, Yang Gu, Zhichao Song, Yiping Meng, Pengfei Xu, Oliver Deussen, and Changsheng Xu. Self-supervised feature augmentation for large image object detection. IEEE Transactions on Image Processing, pages 6745\u20136758, 2020.\\n\\nHeiko Paulheim. Knowledge graph refinement: A survey of approaches and evaluation methods. Semantic web, pages 489\u2013508, 2017.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763, 2021.\\n\\nBrandon Reagen, Paul Whatmough, Robert Adolf, Saketh Rama, Hyunkwang Lee, Sae Kyu Lee, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Gu-Yeon Wei, and David Brooks. Minerva: Enabling low-power, highly-accurate deep neural network accelerators. In 2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA), pages 267\u2013278, 2016.\\n\\nThomas Rebele, Fabian Suchanek, Johannes Hoffart, Joanna Biega, Erdal Kuzey, and Gerhard Weikum. Yago: A multilingual knowledge base from wikipedia, wordnet, and geonames. In International semantic web conference, pages 177\u2013185, 2016.\\n\\nFeiliang Ren, Juchen Li, Huihui Zhang, Shilei Liu, Bochao Li, Ruicheng Ming, and Yujia Bai. Knowledge graph embedding with atrous convolution and residual learning. arXiv preprint arXiv:2010.12121, 2020.\\n\\nHongyu Ren, Weihua Hu, and Jure Leskovec. Query2box: Reasoning over knowledge graphs in vector space using box embeddings. arXiv preprint arXiv:2002.05969, 2020.\\n\\nHongyu Ren and Jure Leskovec. Beta embeddings for multi-hop logical reasoning in knowledge graphs. Advances in Neural Information Processing Systems, pages 19716\u201319726, 2020.\\n\\nAndrea Rossi, Denilson Barbosa, Donatella Firmani, Antonio Matinata, and Paolo Merialdo. Knowledge graph embedding for link prediction: A comparative analysis. ACM Transactions on Knowledge Discovery from Data (TKDD), pages 1\u201349, 2021.\"}"}
{"id": "LdRZ9SFBku", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dan Ruta, Andrew Gilbert, Pranav Aggarwal, Naveen Marri, Ajinkya Kale, Jo Briggs, Chris Speed, Hailin Jin, Baldo Faieta, Alex Filipkowski, et al. Stylebabel: Artistic style tagging and captioning. arXiv preprint arXiv:2203.05321, 2022.\\n\\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\\n\\nChao Shang, Yun Tang, Jing Huang, Jinbo Bi, Xiaodong He, and Bowen Zhou. End-to-end structure-aware convolutional networks for knowledge base completion. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 3060\u20133067, 2019.\\n\\nSheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. How much can clip benefit vision-and-language tasks? arXiv preprint arXiv:2107.06383, 2021.\\n\\nYing Shen, Ning Ding, Hai-Tao Zheng, Yaliang Li, and Min Yang. Modeling relation paths for knowledge graph completion. IEEE Transactions on Knowledge and Data Engineering, pages 3607\u20133617, 2020.\\n\\nHaoyu Song, Li Dong, Wei-Nan Zhang, Ting Liu, and Furu Wei. Clip models are few-shot learners: Empirical studies on vqa and visual entailment. arXiv preprint arXiv:2203.07190, 2022.\\n\\nWenzheng Song, Masanori Suganuma, Xing Liu, Noriyuki Shimobayashi, Daisuke Maruta, and Takayuki Okatani. Matching in the dark: a dataset for matching image pairs of low-light scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6029\u20136038, 2021.\\n\\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic representations. arXiv preprint arXiv:1908.08530, 2019.\\n\\nZequn Sun, Qingheng Zhang, Wei Hu, Chengming Wang, Muhao Chen, Farahnaz Akrami, and Chengkai Li. A benchmarking study of embedding-based entity alignment for knowledge graphs. arXiv preprint arXiv:2003.07743, 2020.\\n\\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by relational rotation in complex space. arXiv preprint arXiv:1902.10197, 2019.\\n\\nHao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490, 2019.\\n\\nTh\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In International conference on machine learning, pages 2071\u20132080, 2016.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pages 5998\u20136008, 2017.\\n\\nMeng Wang, Haofen Wang, Guilin Qi, and Qiushuo Zheng. Richpedia: a large-scale, comprehensive multi-modal knowledge graph. Big Data Research, page 100159, 2020.\\n\\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers, 2020.\\n\\nXin Wang, Benyuan Meng, Hong Chen, Yuan Meng, Ke Lv, and Wenwu Zhu. Tiva-kg: A multimodal knowledge graph with text, image, video and audio. In Proceedings of the 31st ACM International Conference on Multimedia, pages 2391\u20132399, 2013.\\n\\nYaqing Wang, Fenglong Ma, Zhiwei Jin, Ye Yuan, Guangxu Xun, Kishlay Jha, Lu Su, and Jing Gao. Eann: Event adversarial neural networks for multi-modal fake news detection. In Proceedings of the 24th acm sigkdd international conference on knowledge discovery & data mining, pages 849\u2013857, 2018.\\n\\nZhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong Guo, Mingming Gong, and Tongliang Liu. Cris: Clip-driven referring image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11686\u201311695, 2022.\"}"}
{"id": "LdRZ9SFBku", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zikang Wang, Linjing Li, Qiudan Li, and Daniel Zeng. Multimodal data enhanced representation learning for knowledge graphs. In 2019 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138, 2019.\\n\\nFangyun Wei, Yue Gao, Zhirong Wu, Han Hu, and Stephen Lin. Aligning pretraining for detection via object-level contrastive learning. Advances in Neural Information Processing Systems, pages 22682\u201322694, 2021.\\n\\nHaoyang Wen, Ying Lin, Tuan Lai, Xiaoman Pan, Sha Li, Xudong Lin, Ben Zhou, Manling Li, Haoyu Wang, Hongming Zhang, et al. Resin: A dockerized schema-guided cross-document cross-lingual cross-media information extraction and event tracking system. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations, pages 133\u2013143, 2021.\\n\\nYuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2, 2019.\\n\\nRuobing Xie, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. Image-embodied knowledge representation learning. arXiv preprint arXiv:1609.07028, 2016.\\n\\nWenhan Xiong, Thien Hoang, and William Yang Wang. Deeppath: A reinforcement learning method for knowledge graph reasoning. arXiv preprint arXiv:1707.06690, 2017.\\n\\nDerong Xu, Tong Xu, Shiwei Wu, Jingbo Zhou, and Enhong Chen. Relation-enhanced negative sampling for multimodal knowledge graph completion. In Proceedings of the 30th ACM international conference on multimedia, pages 3857\u20133866, 2022.\\n\\nHongwei Xue, Yupan Huang, Bei Liu, Houwen Peng, Jianlong Fu, Houqiang Li, and Jiebo Luo. Probing inter-modality: Visual parsing with self-attention for vision-and-language pre-training. Advances in Neural Information Processing Systems, pages 4514\u20134528, 2021.\\n\\nJinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang. Vision-language pre-training with triple contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15671\u201315680, 2022.\\n\\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 3081\u20133089, 2022.\\n\\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. Qa-gnn: Reasoning with language models and knowledge graphs for question answering. arXiv preprint arXiv:2104.06378, 2021.\\n\\nZhiwei Zha, Jiaan Wang, Zhixu Li, Xiangru Zhu, Wei Song, and Yanghua Xiao. M2conceptbase: A fine-grained aligned multi-modal conceptual knowledge base. arXiv preprint arXiv:2312.10417, 2023.\\n\\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019.\\n\\nJingdan Zhang, Jiaan Wang, Xiaodan Wang, Zhixu Li, and Yanghua Xiao. Aspectmmkg: A multi-modal knowledge graph with aspect-aware entities. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pages 3361\u20133370, 2023.\\n\\nNingyu Zhang, Lei Li, Xiang Chen, Xiaozhuan Liang, Shumin Deng, and Huajun Chen. Multimodal analogical reasoning over knowledge graphs. In The Eleventh International Conference on Learning Representations, 2022.\\n\\nTongtao Zhang, Ananya Subburathinam, Ge Shi, Lifu Huang, Di Lu, Xiaoman Pan, Manling Li, Boliang Zhang, Qingyun Wang, Spencer Whitehead, et al. Gaia-a multi-media multi-lingual knowledge extraction and hypothesis generation system. In TAC, 2018.\\n\\nShangfei Zheng, Weiqing Wang, Jianfeng Qu, Hongzhi Yin, Wei Chen, and Lei Zhao. Mmkgr: Multi-hop multi-modal knowledge graph reasoning. arXiv preprint arXiv:2209.01416, 2022.\\n\\nZhehui Zhou, Can Wang, Yan Feng, and Defang Chen. Jointe: Jointly utilizing 1d and 2d convolution for knowledge graph embedding. Knowledge-Based Systems, page 108100, 2022.\\n\\nXiangru Zhu, Zhixu Li, Xiaodan Wang, Xueyao Jiang, Penglei Sun, Xuwu Wang, Yanghua Xiao, and Nicholas Jing Yuan. Multi-modal knowledge graph construction and application: A survey. arXiv preprint arXiv:2202.05786, 2022.\"}"}
{"id": "LdRZ9SFBku", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Checklist\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n   (b) Did you describe the limitations of your work? [Yes] See Sec. D.3\\n   (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Sec. D.4\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [NA]\\n   (b) Did you include complete proofs of all theoretical results? [NA]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Sec. A and Sec. C\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Refer to Tab. 4 for data splits and refer to Sec. C for hyperparameters and other training details.\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No]\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Sec. C.6.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes] In Sec. 4.3, we implement several methods with our UKnow, including Q2B [54], BETAE [55], CLIP [50]. Please note that we exclusively utilize the official implementation of Q2B to train our model, while we reimplement the codes for BETAE and CLIP based on the guidelines provided in their respective original papers. Additionally, due credit is given to all creators.\\n   (b) Did you mention the license of the assets? [NA]\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] See Sec. A\\n   (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [NA]\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [NA]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [NA]\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [NA]\\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [NA]\\n\\n---\\n\\n1 https://github.com/hyren/query2box\"}"}
