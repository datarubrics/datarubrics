{"id": "5ADv5OfQgU", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"trajdata: A Unified Interface to Multiple Human Trajectory Datasets\\n\\nBoris Ivanovic 1\\nGuanyu Song 2\\nIgor Gilitschenski 2\\nMarco Pavone 1, 3\\n\\n1 NVIDIA Research\\n2 University of Toronto\\n3 Stanford University\\n\\nAbstract\\nThe field of trajectory forecasting has grown significantly in recent years, partially owing to the release of numerous large-scale, real-world human trajectory datasets for autonomous vehicles (AVs) and pedestrian motion tracking. While such datasets have been a boon for the community, they each use custom and unique data formats and APIs, making it cumbersome for researchers to train and evaluate methods across multiple datasets. To remedy this, we present trajdata: a unified interface to multiple human trajectory datasets. At its core, trajdata provides a simple, uniform, and efficient representation and API for trajectory and map data. As a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights.\\n\\ntrajdata is permissively licensed (Apache 2.0) and can be accessed online at https://github.com/NVlabs/trajdata.\\n\\n1 Introduction\\nResearch in trajectory forecasting (i.e., predicting where an agent will be in the future) has grown significantly in recent years, partially owing to the success of deep learning methods on the task [1]; availability of new large-scale, real-world datasets (see Fig. 1); and investment in its deployment within domains such as autonomous vehicles (AVs) [2, 3, 4, 5, 6, 7, 8, 9] and social robots [10, 11, 12]. In addition, recent dataset releases have held associated prediction challenges which have periodically benchmarked the field and spurred new developments [13, 14, 15, 16]. While this has been a boon for research progress, each dataset has a unique data format and development API, making it cumbersome for researchers to train and evaluate methods across multiple datasets. For instance, the recent Waymo Open Motion dataset employs binary TFRecords [17] which differ significantly from nuScenes\u2019 foreign-key format [18] and Woven Planet (Lyft) Level 5\u2019s compressed zarr files [19]. The variety of data formats has also hindered research on topics which either require or greatly benefit from multi-dataset comparisons, such as prediction model generalization (e.g., [20, 21]). To remedy this, we present trajdata: a unified interface to multiple human trajectory datasets.\\n\\nContributions.\\nOur key contributions are threefold. First, we introduce a standard and simple data format for trajectory and map data, as well as an extensible API to access and transform such data for research use. Second, we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a richer understanding of the data underpinning much of pedestrian and AV motion forecasting research. Finally, we leverage insights from these analyses to provide suggestions for future dataset releases.\"}"}
{"id": "5ADv5OfQgU", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Recent datasets provide access to thousands of hours of autonomous driving data, albeit with different data formats and APIs, complicating the use of multiple datasets in research projects.\\n\\n2 Related Work\\n\\nHuman Trajectory Datasets.\\n\\nInitial trajectory forecasting research employed video motion tracking datasets for benchmarking, primarily due to the availability of annotated agent positions over time. Of these, the ETH and UCY pedestrian datasets were among the most widely-used, containing a total of 1536 pedestrians and challenging behaviors such as couples walking together, groups crossing each other, and groups forming and dispersing. Soon after the successful application of deep learning models to pedestrian trajectory forecasting, and as data needs grew in autonomous driving research and industry, numerous large-scale datasets have emerged containing significantly more heterogeneous-agent interactive scenarios (e.g., between vehicles and pedestrians) in urban environments. Fig. 1 visualizes the scale, collection, and annotation strategy of such datasets, with a comprehensive review of earlier human motion datasets available in. In particular, the gradual shift from human annotation to autolabeling can be seen, with the recent large-scale Yandex Shifts, Waymo Open Motion, and nuPlan datasets employing powerful autolabeling pipelines to accurately label sensor data collected by vehicle fleets at scale.\\n\\nMulti-Dataset Benchmarking.\\n\\nWhile the increase in datasets and associated challenges has bolstered research, their unique formats increase the complexity of evaluating methods across datasets, complicating efforts to analyze, e.g., prediction model generalization. To address this issue for pedestrian motion data, OpenTraj created dataloaders for different pedestrian motion datasets as part of its effort to evaluate and compare motion complexity across pedestrian datasets. More recently, TrajNet++ and Atlas present multi-dataset benchmarks to systematically evaluate human motion trajectory prediction algorithms in a unified framework. While these efforts have provided the community with multi-dataset benchmarks, they are primarily focused on pedestrian data. In contrast, trajdata tackles the standardization of both pedestrian and autonomous vehicle datasets, including additional data modalities such as maps.\"}"}
{"id": "5ADv5OfQgU", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Datasets currently supported by trajdata. More details can be found in the appendix.\\n\\n| Dataset  | Size  | Locations | Maps? |\\n|----------|-------|-----------|-------|\\n| ETH [22] | 0.4h  | No        |       |\\n| INTERACTION [39] | 16.5h | Yes       |       |\\n| UCY [23] | 0.3h  | No        |       |\\n| Lyft Level 5 [19] | 1118h | Yes       |       |\\n| SDD [40]  | 5h    | No        |       |\\n| Waymo Open [17] | 570h  | Yes       |       |\\n| nuScenes [18] | 5.5h  | Yes       |       |\\n| nuPlan [27] | 1500h | Yes       |       |\\n\\nIndex: Optional\\n\\n| Field  | Description |\\n|--------|-------------|\\n| agent_id |            |\\n| scene_ts |            |\\n| x       |            |\\n| y       |            |\\n| z       |            |\\n| v       |            |\\n| a_x     |            |\\n| a_y     |            |\\n| a_z     |            |\\n| l       |            |\\n| h       |            |\\n| w       |            |\\n| h       |            |\\n\\nFigure 2: Left: trajdata adopts a tabular representation for trajectory data, leveraging advanced indexing to satisfy user data queries. Right: Agent trajectories from the nuScenes [18] dataset visualized on the scene\u2019s VectorMap, containing all of trajdata\u2019s core map elements.\\n\\nMaps. To retain the most information from high-definition (HD) dataset maps, trajdata adopts a polyline representation for map data. This choice matches the vast majority of modern trajectory datasets which provide vector map data and makes them immediately compatible with our format. Currently, there are four core map elements: RoadLane, RoadArea, PedCrosswalk, and PedWalkway. As illustrated in Fig. 2, a RoadLane represents a driveable road lane with a centerline and optional left and right boundaries. A RoadArea represents other driveable areas of roads which are not part of lanes, e.g., parking lots or shoulders. A PedCrosswalk denotes a marked area where pedestrians can cross the road. Finally, a PedWalkway marks sidewalks adjacent to roads. Of these, only RoadLane elements are required to be extracted, other elements are optional (they are absent in some datasets). Our map format additionally supports lane connectivity information in the form of left/right adjacent lanes (i.e., lanes accessible by left/right lane changes) and successor/predecessor lanes (i.e., lanes that continue from / lead into the current lane following the road direction). Each map element is designed to be compatible with popular computational geometry packages, such as Shapely [37], enabling efficient set-theoretic queries to calculate, e.g., road boundary violations. By default, trajdata serializes map data using Google protocol buffers [38], and, in particular, only stores neighboring position differences for efficiency, similar to the implementation used in [19]. Dynamic traffic light information is also supported, and trajdata makes use of a separate data frame to link the traffic signal shown per timestep with the lane ID being controlled.\\n\\n3.2 Core trajdata Functionalities\\n\\nMulti-dataset training and evaluation. One of trajdata\u2019s core functionalities is aggregating data from multiple datasets in a UnifiedDataset object (a PyTorch [30] Dataset subclass by default).\\n\\n```python\\nfrom trajdata import UnifiedDataset\\ndataset = UnifiedDataset(desired_data=['nusc_mini-boston', 'sdd-train'], desired_dt=0.1, centric='agent', history_sec=(1.0, 3.0), future_sec=(4.0, 4.0))\\n```\\n\\nThese settings were used to create Figure 2.\\n\\nDetailed demonstrations of trajdata\u2019s capabilities can be found in our repository\u2019s examples/folder.\"}"}
{"id": "5ADv5OfQgU", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: trajdata can provide agent-centric (or scene-centric) batches of trajectory data for model training and evaluation in associated AgentBatch (or SceneBatch) objects. The indexing and padding strategy of a few core AgentBatch tensors are visualized here.\\n\\nThe example above creates a dataset that provides agent-centric data batches (i.e., each batch element contains data for one agent at one timestep, see Fig. 3) sourced from only Boston in the nuScenes mini dataset (\\\"nusc_mini-boston\\\") as well as the Stanford Drone Dataset's entire training split (\\\"sdd-train\\\"), with time upsampling ensuring all data is at 10Hz (desired_dt=0.1).\\n\\nhistory_sec=(1.0, 3.0) specifies that the predicted agent's trajectory must have at least 1.0s of history available, with padding for any missing data up to 3.0s (see Fig. 3). Similarly, future_sec=(4.0, 4.0) requires that the predicted agent's trajectory have 4.0s of future available.\\n\\ntrajdata provides many other capabilities in addition to the above, including scene-centric batches (i.e., data for all agents in a scene at the same timestep), semantic search (e.g., nuScenes [18] provides text descriptions for each scene), agent filtering (e.g., only vehicles), coordinate frame standardization (i.e., making trajectories relative to the predicted agent's frame at the current timestep), map rasterization (e.g., if encoding scene context with a convolutional architecture), data augmentations (e.g., additive Gaussian noise to past trajectories), and general data transforms via custom functions.\\n\\nMap API. trajdata's standardized vector map object is VectorMap. In addition to providing access to individual map elements (e.g., lanes, sidewalks), it also leverages precomputed spatial indices to make nearest neighbor queries very efficient.\\n\\nIn the example above, the polyline map of Boston's seaport neighborhood (from nuScenes [18]) is loaded from the user's trajdata cache (its path would be specified instead of <=>) and queried for the closest RoadLane to a given x, y, z position.\\n\\nSimulation Interface. trajdata also provides a simulation interface that enables users to initialize a scene from real-world data and simulate agents from a specific timestep onwards. Simulated agent motion is recorded by trajdata and can be analyzed with a library of evaluation metrics (e.g., collision and offroad rates, statistical differences to real-world data distributions) or exported to disk. This functionality was extensively used to benchmark learning-based traffic models in [32, 33].\\n\\n4 Dataset Comparisons and Analyses\\nIn this section, we leverage trajdata's standardized trajectory and map representations to directly compare many popular AV and pedestrian trajectory datasets along a variety of metrics. Our goal is to provide a deeper understanding of the datasets underpinning much of human motion research by analyzing their data distributions, motion complexity, and annotation quality.\"}"}
{"id": "5ADv5OfQgU", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4:\\nLeft: Number of unique agents per dataset.\\nRight: Distribution of agent types per dataset.\\n\\nNote that we only analyze dataset training and validation splits, since these are the splits predominantly used by methods for development. We explicitly do not analyze test splits since they are either not available publicly or because doing so may harm existing benchmark validity. Further, while trajdata supports data frequency up- and down-scaling via interpolation and down-sampling, all of the following analyses were conducted in each dataset's native data resolution. All analyses were performed using the latest version of trajdata at the time of writing (v1.3.2) on a desktop computer with 64 GB of RAM and an AMD Ryzen Threadripper PRO 32-core CPU. For larger datasets, an NVIDIA DGX-1 server with 400 GB of RAM and 64 CPU cores was used.\\n\\n4.1 Agent Distributions\\nPopulation.\\nTo build a fundamental understanding of the considered datasets, we first analyze and compare agent populations. Fig. 4 visualizes overall agent counts and proportions per dataset. As can be expected, modern large-scale AV datasets such as Waymo [17] and Lyft Level 5 [19] contain multiple orders of magnitude more agents than earlier pedestrian datasets SDD [40], ETH [22], or UCY [23]. However, as we will show later, pedestrian datasets still provide value in terms of agent diversity, density, and motion complexity in popular social robotics settings such as college campuses.\\n\\nAs can be seen in Fig. 4 (right), the vast majority of agents in AV datasets are vehicles or pedestrians, with the exception of Lyft Level 5 [19] where 71.8% of agents have unknown types. In contrast, bicycles (a relatively niche category in many datasets) account for 41% of all agents in SDD [40] (indeed, biking is a popular method of transportation around Stanford's large campus). Such imbalances in agent populations are indicative of real-world distributions, e.g., motorcycles make up only 3.5% of vehicles in the USA [42], similar to their proportion in nuScenes [18] (1.6%).\\n\\nDensity and Observation Duration.\\nIn addition to which agent types are captured in scenes, the amount and density of agents can be an important desiderata (e.g., for research on crowd behavior) or computational consideration (e.g., for methods whose runtime scales with the number of agents). Fig. 5 visualizes the distribution of the number of agents observed per scene per timestep (left), as well as the maximum number of simultaneous agents per scene (right). As can be seen, urban scenarios captured in modern AV datasets frequently contain 100+ detected agents (with a long tail extending to 250+ agents). In this respect, ETH [22], UCY [23], and INTERACTION [39] are limited by their fixed-camera and drone-based data-collection strategies compared to the comprehensive on-vehicle sensors used in nuScenes [18], Waymo [17], Lyft [19], and nuPlan [27]. However, while ETH [22], UCY [23], and INTERACTION [39] do not contain as many agents, they consistently provide the highest-density scenes (see Fig. 6), especially for pedestrians and bicycles. We compute agent density by dividing the number of agents in a scene by their overall bounding rectangle area, as in [25].\\n\\nEach dataset supported by trajdata adopts different scenario lengths and corresponding agent observation durations. As can be seen in Fig. 7, AV datasets are comprised of scenarios with lengths ranging from 4 s in INTERACTION [39] to 25 s in Lyft Level 5 [19]. The peaks at the right of each AV dataset duration distribution are caused by the always-present ego-vehicle (for Vehicles) as well as other agents detected throughout the scene (common in steady traffic, parking lots, or at an intersection with stopped traffic and pedestrians waiting to cross). One can also see that Lyft Level 5 [19] agent detections are much shorter-lived compared to other AV datasets' relatively uniform distributions (Waymo [17], nuScenes [18], and nuPlan [27]). This could be caused by Lyft's...\"}"}
{"id": "5ADv5OfQgU", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Left: Number of agents present per timestamp and scene. Right: Maximum number of agents present at the same time per scene.\\n\\nAnnotations being collected from an onboard perception system [19] (which are affected by noise and occlusions) vs human annotators [18] or autolabeling [27,17] which can leverage data from past and future timesteps be more robust to such errors. We conduct additional comparisons between data collection methodologies in Section 4.3.\\n\\nEgo-Agent Distances.\\n\\nWhen developing AV perception systems, an important consideration is the sensor range(s) necessary to facilitate the desired prediction and planning horizons as well as provide advanced warning of critical situations (e.g., stopped traffic on a highway). In Fig. 8, we compare the distribution of ego-agent distances and find that, while nuScenes [18] and Lyft Level 5 [19] have long-tailed distributions extending past 200 m, Waymo [17] and nuPlan [27] appear to have artificial cut-offs at 75\u201380 m, potentially to maintain data quality by avoiding poor data from distant agents. However, it would be more useful to maintain distant detections and add uncertainty outputs from the autolabeler to support uncertain long-range detection research in addition to improving autolabeling.\\n\\nMapped Areas.\\n\\nHD maps are a core component of many AV datasets, frequently leveraged in trajectory forecasting and motion planning research to provide scene context and geometric lane information (e.g., for global search-based planning and trajectory optimization). Current AV dataset maps are very large (see Table 2 in the appendix) and comprehensive, spanning multiple neighborhoods in different cities. However, not all HD maps are created equal, commonly differing along three axes: Area completeness, lane definitions, and traffic lights. While most AV datasets provide complete HD maps of neighborhoods, Waymo [17] differs by only providing local map crops per scenario without a common reference frame across scenarios 2. This also significantly increases the storage requirements of Waymo [17] maps compared to other datasets.\\n\\nLane definitions can also differ significantly between datasets, with intersections being a notable differentiator. For instance, the nuScenes dataset [18] does not annotate intersections fully, opting for only lane centerlines without associated edges (Fig. 2 shows an example). Lyft Level 5 [19] and nuPlan [27] both include full lane center and edge information for all possible motion paths through an intersection. Waymo [17] maps are unique in that they provide full lane center and boundary information, but there are many gaps in the associations between lane centerlines and boundaries, making it difficult to construct lane edge polylines or lane area polygons 3. As a result, we exclude Waymo maps from map-based analyses in this work.\\n\\n4.2 Motion Complexity\\n\\nMeasuring the complexity of driving scenarios is an important open problem in the AV domain, with a variety of proposed approaches ranging from heuristic methods [25] to powerful conditional behavior 2.\\n\\nSee https://github.com/waymo-research/waymo-open-dataset/issues/394 for visualizations.\\n\\n3 See https://github.com/waymo-research/waymo-open-dataset/issues/389 for visualizations.\"}"}
{"id": "5ADv5OfQgU", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Distributions of the length of time agents are observed in each scene.\\n\\nFigure 8: Distribution of distances between agents and data-collecting ego-vehicle in AV datasets.\\n\\nPrediction models [43]. To avoid potential biases in analyzing datasets with an externally-trained model, we employ simple and interpretable heuristics similar to [25].\\n\\nMotion Diversity. We first analyze distributions of dynamic agent quantities (e.g., speed, acceleration, jerk). As can be seen in Fig. 9, the majority of speed distributions have high peaks at zero (no motion). This is corroborated by Table 3 in the appendix, which shows that a significant portion of agents are stationary in many datasets, especially for nuScenes [18] (17.5%) and Waymo [17] (53.6%). After the initial peak, agent speed distributions drop sharply to a roughly uniform plateau (up to 20 m/s for vehicles) before dropping completely around 30 m/s (a common highway speed around the world).\\n\\nWhile SDD [40] and INTERACTION [39] have sensible vehicle speeds, their pedestrian speeds can be too high. Such high speeds may be caused by annotations near the edge of drone camera view or by rectification artifacts near the image border. Additionally, the very long-tailed distribution of Lyft [19] and Waymo [17] vehicle, pedestrian, and bicycle speeds (exceeding 60 m/s) show a remaining area of improvement for state-of-the-art AV perception systems and autolabeling pipelines.\\n\\nComparisons of acceleration and jerk can be found in the appendix. Overall, from dynamic quantities alone, Waymo [17] and Lyft [19] provide the most diversity in agent motion. If such long-tailed data is undesirable, the INTERACTION [39] dataset provides the most realistic set of vehicle speeds.\\n\\nTrajectory Nonlinearity. To analyze the spatial diversity of agent trajectories, we first compare each agent's heading to their initial timestep. As can be seen in Fig. 10, and reiterating earlier analyses, the vast majority of human movement is straight and linear ($h=0$). Moving away from the center, we also see repeated symmetric peaks at $\\\\pm \\\\pi$ (capturing left and right turns) and $\\\\pm k\\\\pi$ in some datasets.\\n\\nOne possible reason for these periodic peaks in the distribution is an artifact of the autolabeling methods used in the datasets (since only datasets that autolabel sensor data are affected), another is that their respective scene geometries contain more roundabouts, cul-de-sacs, and repeated turns than other datasets (more detailed heading distributions can be found in the appendix). We can also see that pedestrians' distributions are more uniform as they do not have to adhere to rigid road geometry.\\n\\nPath Efficiency. Lastly, we also measure agent path efficiencies, defined as the ratio of the distance between trajectory endpoints to the trajectory length [25]. Intuitively, the closer to 100%, the closer the trajectory is to a straight line. As can be seen in Fig. 15 in the appendix, most path efficiency distributions are uniformly distributed, with peaks near 100%, echoing earlier straight-line findings. However, the INTERACTION [39] dataset is an outlier in that its agent trajectories are predominantly straight lines with much less curved motion than other AV and pedestrian datasets.\\n\\n4.3 Annotation Quality\\n\\nWhile analyzing datasets' true annotation accuracy would be best, neither we nor the original data annotators have access to the underlying real-world ground truth. As a proxy, we instead analyze the self-consistency of annotations in the form of incidence rates of collisions between agents, off-road driving, and uncomfortable high-acceleration events (using $0.4g$ as a standard threshold [44, 45]).\"}"}
{"id": "5ADv5OfQgU", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Virtually all observed agent data is free of collisions and off-road driving, save for rare one-offs (e.g., the INTERACTION dataset contains a minor car accident). We denote bounding box intersections between agents as collisions, and agent center-of-mass exiting the road boundary as off-road driving. Collisions typically indicate errors in bounding box annotations, whereas off-road driving can indicate erroneous bounding box dimensions, missing map coverage, or harsh driving that, e.g., cuts corners during a right turn.\\n\\nAs can be seen in Fig. 11 (left), most vehicles in datasets experience collision rates below 5%. Of particular note is the fact that state-of-the-art autolabeling systems (e.g., used in Waymo) are nearly matching the accuracy of human annotations (e.g., used in nuscenes) in terms of resulting collision rates. However, detecting agents from a near-ground perspective (even with 3D LiDAR) is a very challenging task, and current performance still lags behind high altitude viewpoints. In particular, The INTERACTION dataset achieves orders of magnitude lower vehicle collision, off-road, and harsh acceleration rates owing to its drone-based data collection strategy. In theory, SDD should enjoy a similar advantage, but it only provides axis-aligned bounding box annotations (which overestimate agent extents) and Stanford's college campus contains much more interactive agents than other urban environments. More generally, the notion of bounding box intersections as collisions does not transfer exactly to pedestrians as they can enter/exit cars and walk in close groups, and further study is needed to robustly distinguish between errant motion and normal interactive motion.\\n\\nIn Fig. 11 (middle), we find that vehicles in general experience very few (<1%) harsh acceleration events, with Waymo, Lyft, and nuScenes all having the highest incidence, commensurate with their earlier-discussed long-tail acceleration distributions. Lastly, we find in Fig. 11 (right) that the INTERACTION and nuPlan agent annotations are well-aligned onto their maps, whereas nuScenes suffers from poor map coverage away from main roads (there are many annotated parked cars next to the main road) and Lyft suffers from high false positive detections next to the main road (the majority of which take the Unknown class).\\n\\n5 Conclusions and Recommendations\\n\\nThe recent releases of large-scale human trajectory datasets have significantly accelerated the field of AV research. However, their unique data formats and custom developer APIs have complicated multi-dataset research efforts (e.g., [20, 21]). In this work, we present trajdata, a unified trajectory data loader that aims to harmonize data formats, standardize data access APIs, and simplify the process of using multiple AV datasets within the AV research community with a simple, uniform, and efficient data representation and development API. We used trajdata to comprehensively compare existing trajectory datasets, finding that, in terms of annotation self-consistency, drone-based data collection methods yield significantly more accurate birds-eye view bounding box annotations than even state-of-the-art AV perception stacks with LiDAR (albeit with much less spatial coverage), modern\"}"}
{"id": "5ADv5OfQgU", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11: Self-consistency failure rates per dataset and agent type, in the form of collision (left), high vehicle acceleration (middle), and off-road (right) rates.\\n\\nAutolabeling pipelines are nearing human annotation performance, and smaller-scale pedestrian datasets can still be useful for investigations requiring high-agent-density scenarios. As concrete recommendations, we saw that some datasets artificially limit the distance agents are autolabeled. Instead, it would be more useful to the long-range detection community to remove such restrictions, but add autolabeler-output uncertainties to long-range detections, supporting uncertain perception research along the way. Further, incorporating explicit self-consistency checks within autolabeling pipelines and catching, e.g., collisions, prior to release can both improve the autolabeling method as well as the resulting data labels.\\n\\nMore broadly, providing researchers with access to more data comprised of various agent types from diverse geographies should help in modeling rare agent types and behaviors, in addition to aiding in the generalization of methods to multiple geographies. However, as we have seen in prior sections, there is an overwhelming bias towards straight line driving, and one capability missing from trajdata is the ability to (re)balance data on a semantic (behavioral) level. Finally, even if lower-level trajectory classes (e.g., driving straight, turning left/right, slowing down, speeding up, etc) are balanced, an important higher-level consideration during original dataset curation time is to ensure that AV datasets explore all geographic regions within an environment, and not only those of certain socioeconomic statuses or transportation access.\\n\\nFuture work will address the current limitations of trajdata (e.g., expanding the number of supported datasets and new capabilities such as geometric map element associations to support Waymo-like map formats [17]). Further, incorporating sensor data would also enable perception research as well as joint perception-prediction-planning research, an exciting emerging AV research field.\\n\\nAcknowledgments and Disclosure of Funding\\nWe thank all past and present members of the NVIDIA Autonomous Vehicle Research Group for their code contributions to trajdata and feedback after using it in projects. We additionally thank Leon De Andrade, Alex Naumann, and Stepan Konev for their contributions to trajdata on GitHub.\\n\\nReferences\\n[1] A. Rudenko, L. Palmieri, M. Herman, K. M. Kitani, D. M. Gavrila, and K. O. Arras, \u201cHuman motion trajectory prediction: A survey,\u201d Int. Journal of Robotics Research, vol. 39, no. 8, pp. 895\u2013935, 2020.\\n[2] General Motors, \u201cSelf-driving safety report,\u201d 2018, Available at https://www.gm.com/content/dam/company/docs/us/en/gmcom/gmsafetyreport.pdf.\\n[3] Uber Advanced Technologies Group, \u201cA principled approach to safety,\u201d 2020, Available at https://uber.app.box.com/v/UberATGSafetyReport.\\n[4] Lyft, \u201cSelf-driving safety report,\u201d 2020, Available at https://2eg1kz1onwfq1djllo2xh4bb-wpengine.netdna-ssl.com/wp-content/uploads/2020/06/Safety_Report_2020.pdf.\\n[5] Waymo, \u201cSafety report,\u201d Waymo LLC, 2021, Available at https://waymo.com/safety/safety-report.\"}"}
{"id": "5ADv5OfQgU", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "5ADv5OfQgU", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"J. Amirian, B. Zhang, F. V. Castro, J. J. Baldelomar, J.-B. Hayet, and J. Pettr\u00e9, \\\"OpenTraj: Assessing prediction complexity in human trajectories datasets,\\\" in Asian Conference on Computer Vision, 2020.\\n\\nA. Malinin, N. Band, Y. Gal, M. Gales, A. Ganshin, G. Chesnokov, A. Noskov, A. Ploskonosov, L. Prokhorenkova, I. Provilkov, V. Raina, D. Roginskiy, M. Shmatova, P. Tigas, and B. Yangel, \\\"Shifts: A dataset of real distributional shift across multiple large-scale tasks,\\\" in Conf. on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. [Online]. Available: https://openreview.net/forum?id=qM45LHaWM6E\\n\\nH. Caesar, J. Kabzan, K. S. Tan, W. K. Fong, E. Wolff, A. Lang, L. Fletcher, O. Beijbom, and S. Omari, \\\"nuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles,\\\" 2021, Available at https://arxiv.org/abs/2106.11810.\\n\\nP. Kothari, S. Kreiss, and A. Alahi, \\\"Human trajectory forecasting in crowds: A deep learning perspective,\\\" IEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 7, pp. 7386\u20137400, 2022.\\n\\nA. Rudenko, L. Palmieri, W. Huang, A. J. Lilienthal, and K. O. Arras, \\\"The atlas benchmark: An automated evaluation framework for human motion prediction,\\\" in IEEE Int. Conf. on Robot and Human Interactive Communication, 2022.\\n\\nA. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, \\\"Automatic differentiation in PyTorch,\\\" in Conf. on Neural Information Processing Systems - Autodiff Workshop, 2017.\\n\\nD. Rempe, Z. Luo, X. B. Peng, Y. Yuan, K. Kitani, K. Kreis, S. Fidler, and O. Litany, \\\"Trace and Pace: Controllable pedestrian animation via guided trajectory diffusion,\\\" in IEEE Conf. on Computer Vision and Pattern Recognition, 2023.\\n\\nD. Xu, Y. Chen, B. Ivanovic, and M. Pavone, \\\"BITS: Bi-level imitation for traffic simulation,\\\" in IEEE Int. Conf. on Robotics and Automation, 2023.\\n\\nZ. Zhong, D. Rempe, D. Xu, Y. Chen, S. Veer, T. Che, B. Ray, and M. Pavone, \\\"Guided conditional diffusion for controllable traffic simulation,\\\" in IEEE Int. Conf. on Robotics and Automation, 2023.\\n\\nF. Christianos, P. Karkus, B. Ivanovic, S. V. Albrecht, and M. Pavone, \\\"Planning with occluded traffic agents using bi-level variational occlusion models,\\\" in IEEE Int. Conf. on Robotics and Automation, 2023.\\n\\nY. Chen, P. Karkus, B. Ivanovic, X. Weng, and M. Pavone, \\\"Tree-structured policy planning with learned behavior models,\\\" in IEEE Int. Conf. on Robotics and Automation, 2023.\\n\\nThe Apache Software Foundation, \\\"Apache arrow,\\\" 2023, Available at https://github.com/apache/arrow.\\n\\nS. Gillies, C. van der Wel, J. Van den Bossche, M. W. Taves, J. Arnott, B. C. Ward, and others, \\\"Shapely,\\\" 2023, Available at https://github.com/shapely/shapely.\\n\\nGoogle Inc., \\\"Protocol buffers - google's data interchange format,\\\" 2023, Available at https://github.com/protocolbuffers/protobuf.\\n\\nW. Zhan, L. Sun, D. Wang, H. Shi, A. Clausse, M. Naumann, J. K\u00fcmmerle, H. K\u00f6nigshof, C. Stiller, A. de La Fortelle, and M. Tomizuka, \\\"INTERACTION Dataset: An international, adversarial and cooperative motion dataset in interactive driving scenarios with semantic maps,\\\" 2019, Available at https://arxiv.org/abs/1910.03088.\\n\\nA. Robicquet, A. Sadeghian, A. Alahi, and S. Savarese, \\\"Learning social etiquette: Human trajectory prediction in crowded scenes,\\\" in European Conf. on Computer Vision, 2016.\\n\\nG. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. (2016) OpenAI Gym. Available at https://arxiv.org/abs/1606.01540.\"}"}
{"id": "5ADv5OfQgU", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bureau of Transportation Statistics, \u201cNational Transportation Statistics. Number of U.S. Aircraft, Vehicles, Vessels, and Other Conveyances,\u201d U.S. Dept. of Transportation, Tech. Rep., 2023.\\n\\nE. Tolstaya, R. Mahjourian, C. Downey, B. Varadarajan, B. Sapp, and D. Anguelov, \u201cIdentifying driver interactions via conditional behavior prediction,\u201d in *IEEE Int. Conf. on Robotics and Automation*, 2021.\\n\\nB. G. Simons-Morton, J. D. Ouimet, J. Wang, S. G. Klauer, S. E. Lee, and T. A. Dingus, \u201cHard braking events among novice teenage drivers by passenger characteristics,\u201d *Driving Assessment Conference*, vol. 5, pp. 236\u2013242, 6 2009. [Online]. Available: https://pubs.lib.uiowa.edu/driving/article/id/28044/\\n\\nS. G. Klauer, T. A. Dingus, V. L. Neale, J. D. Sudweeks, and D. J. Ramsey, \u201cComparing real-world behaviors of drivers with high versus low rates of crashes and near-crashes,\u201d National Highway Traffic Safety Administration, Tech. Rep. DOT HS 811 091, 2009.\"}"}
