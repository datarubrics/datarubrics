{"id": "bBff294gqLp", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Graph neural architecture search (GraphNAS) has recently aroused considerable attention in both academia and industry. However, two key challenges seriously hinder the further research of GraphNAS. First, since there is no consensus for the experimental setting, the empirical results in different research papers are often not comparable and even not reproducible, leading to unfair comparisons. Secondly, GraphNAS often needs extensive computations, which makes it highly inefficient and inaccessible to researchers without access to large-scale computation. To solve these challenges, we propose NAS-Bench-Graph, a tailored benchmark that supports unified, reproducible, and efficient evaluations for GraphNAS. Specifically, we construct a unified, expressive yet compact search space, covering 26,206 unique graph neural network (GNN) architectures and propose a principled evaluation protocol. To avoid unnecessary repetitive training, we have trained and evaluated all of these architectures on nine representative graph datasets, recording detailed metrics including train, validation, and test performance in each epoch, the latency, the number of parameters, etc. Based on our proposed benchmark, the performance of GNN architectures can be directly obtained by a look-up table without any further computation, which enables fair, fully reproducible, and efficient comparisons. To demonstrate its usage, we make in-depth analyses of our proposed NAS-Bench-Graph, revealing several interesting findings for GraphNAS. We also showcase how the benchmark can be easily compatible with GraphNAS open libraries such as AutoGL and NNI. To the best of our knowledge, our work is the first benchmark for graph neural architecture search.\"}"}
{"id": "bBff294gqLp", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The experimental settings, such as dataset splits, hyper-parameter settings, and evaluation protocols differ greatly from paper to paper. As a result, the experimental results cannot be guaranteed comparable and reproducible, making fair comparisons of different methods extremely difficult.\\n\\nGraphNAS often requires extensive computations and therefore is highly inefficient, especially for large-scale graphs. Besides, the computational bottleneck makes GraphNAS research inaccessible to those without abundant computing resources.\\n\\nSimilar challenges have arisen in other domains of NAS research [86], which gives birth to the idea of tabular NAS Benchmarks [72, 12, 27, 75]. Tabular NAS benchmarks provide pre-computed evaluations for all possible architectures in the search space by a table lookup. These benchmarks dramatically boost NAS research, for example, by speeding up the experiments since no architecture training is needed during the search procedure and creating fair comparisons of different NAS algorithms [35]. Inspired by the success of tabular NAS benchmarks and to solve the challenges of GraphNAS, we propose NAS-Bench-Graph, the first tabular NAS benchmark that supports unified, reproducible, and efficient evaluations for GraphNAS. Specifically, we first construct a unified GraphNAS search space by formulating the macro space of message-passing as a constrained directed acyclic graph and carefully choose operations from seven GNN candidates. Our proposed search space is expressive yet compact, resulting in 26,206 unique architectures and covering many representative GNNs. We further propose a principled protocol for dataset splits, choosing hyper-parameters, and evaluations. We have trained and evaluated all of these architectures on nine representative graph datasets with different sizes and application domains and recorded detailed metrics during the training and testing process. All the code and evaluation results have been open-sourced. Therefore, our proposed benchmark can enable fair, fully reproducible, and efficient comparisons for different GraphNAS methods.\\n\\nTo explore our proposed NAS-Bench-Graph, we make in-depth analyses from four perspectives with several interesting observations. First, the performance distribution shows that though reasonably effective architectures are common, architectures with extremely good results are rare, and these powerful architectures have diverse efficiencies, as measured by the model latency. Therefore, how to find architectures with both high efficiency and effectiveness is challenging. Second, the architecture distribution suggests that different graph datasets differ greatly in the macro space and operation choices. The cross-datasets correlations further suggest that different graph datasets exhibit complicated patterns and simply transferring the best-performing architectures from similar graph datasets cannot lead to the optimal result. Lastly, detailed architecture explorations demonstrate that architecture space exhibits certain degrees of smoothness, which supports the mutation process in evolutionary search strategies, and deeper parts of architectures are more influential than lower parts, which may inspire more advanced reinforcement learning based search strategies.\\n\\nTo demonstrate the usage of NAS-Bench-Graph, we have integrated it with two representative open libraries: AutoGL [18], the first dedicated library for GraphNAS, and NNI [35], a widely adopted library for general NAS. Experiments demonstrate that NAS-Bench-Graph can be easily compatible with different search strategies including random search, reinforcement learning based methods, and evolutionary algorithms.\\n\\nOur contributions are summarized as follows.\\n\\n\u2022 We propose NAS-Bench-Graph, a tailored GraphNAS benchmark that enables fair, fully reproducible, and efficient empirical comparisons for GraphNAS research. We are the first to study benchmarking GraphNAS research to the best of our knowledge.\\n\\n\u2022 We have trained and evaluated all GNN architectures in our tailored search space on nine common graph datasets with a unified and principled evaluation protocol. Based on our proposed benchmark, the performance of architectures can be directly obtained without repetitive training.\\n\\n\u2022 We make in-depth analyses for our proposed benchmark and showcase how it can be easily compatible with GraphNAS open libraries such as AutoGL and NNI.\\n\\n2 https://github.com/THUMNLab/NAS-Bench-Graph\\n3 https://github.com/microsoft/nni\"}"}
{"id": "bBff294gqLp", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2 Related Works\\n\\n2.1 Graph Neural Architecture Search\\n\\nGNNs have shown impressive performance for graph machine learning in the past few years [25, 1, 59, 69, 68, 30, 31, 32, 39, 37, 38]. However, the existing GNNs are manually designed, which require expert knowledge, are labor-intensive, and unadaptable to changes in graph datasets and tasks. Motivated by these problems, automated graph learning has drawn increasing attention in the past few years, including hyper-parameter optimization on graphs [57, 61], and GraphNAS [16, 84, 48, 18, 82, 23, 10, 73, 22, 62, 64, 5, 17]. GraphNAS aims to automatically discover the optimal GNN architectures. Similar to general NAS [14], GraphNAS can be categorized based on the search space, the search strategy, and the performance estimation strategy [78]. For the search space, both micro [16, 80, 34] and macro [63, 15] spaces for the message-passing functions in GNNs are studied, as well as other functions such as pooling [23, 65], heterogeneous graphs [10], and spatial-temporal graphs [45]. Search strategies include reinforcement learning based methods [16, 84, 40], evolutionary algorithms [44, 53, 18], and differentiable methods [82, 79, 81]. Performance estimation strategies can be generally divided into training from scratch [16], hand-designed weight-sharing mechanisms [84, 8], using supernets [48, 6, 18, 49], and prediction-based methods [56]. Despite these progresses, how to properly evaluate and compare GraphNAS methods receives less attention. Our proposed benchmark can support fair and efficient comparisons of different GraphNAS methods as well as motivating new GraphNAS research.\\n\\n2.2 NAS Benchmarks\\n\\nSince the introduction of NAS-Bench-101 [72], many different benchmarks have been introduced for NAS. However, most previous works focus on computer vision tasks such as NAS-Bench-101 [72], NAS-Bench-201 [12], NATS-Bench [11], NAS-Bench-1shot1 [74], Surr-NAS-Bench [75], HW-NAS-Bench [28], NAS-HPO-Bench-II [20], TransNAS-Bench-101 [13], NAS-Bench-Zero [7], NAS-Bench-x11 [70], and NAS-Bench-360 [58]. Some recent benchmarks also study tabular data (NAS-HPO-Bench [26]), natural language processing (NAS-Bench-NLP [27] and NAS-Bench-x11 [70]), acoustics (NAS-Bench-ASR [41]), and sequence (NAS-Bench-360 [58]). We draw inspiration from these benchmarks and propose the first tailored NAS benchmark for graphs. We provide more comparisons with the existing NAS benchmarks in Appendix D.2.\\n\\n3 Benchmark Design\\n\\nIn this section, we describe our design for the NAS benchmark construction, including the search space (Section 3.1), the datasets used (Section 3.2), and the experiment settings (Section 3.3). We provide some preliminaries of GNNs and GraphNAS in Appendix D.1.\\n\\n3.1 Search Space Design\\n\\nTo balance the effectiveness and efficiency, we design a tractable yet expressive search space. Specifically, we consider the macro search space of GNN architectures as a directed acyclic graph (DAG) to formulate the computation, i.e., each computing node indicates a representation of vertices and each edge indicates an operation. Concretely, the DAG contains six nodes (including the input and output node) and we constrain that each intermediate node has only one incoming edge. The resulted DAG has 9 choices, as illustrated in Fig 1. Those intermediate nodes without successor nodes are connected to the output node by concatenation. Besides this macro space, we also consider optional fully-connected pre-process and post-process layers as GraphGym [73] and PasCa [76]. Notice that to avoid exploding the search space, we consider the numbers of pre-process and post-process layers as hyper-parameters, which will be discussed in Section 3.3. Finally, we adopt a task-specific fully connected layer to obtain the model prediction.\\n\\nFor the candidate operations, we consider 7 most widely adopted GNN layers: GCN [25], GAT [59], GraphSAGE [19], GIN [68], ChebNet [9], ARMA [4], and k-GNN [43]. Besides, we also consider Identity to support residual connection and fully connected layer that does not use graph structures.\"}"}
{"id": "bBff294gqLp", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: An illustration of the 9 different choices of our macro search space. Each node indicates a representation of vertices and each edge indicates an operation. We omit the output node for clarity.\\n\\nTable 1: The statistics of the adopted datasets.\\n\\n| Dataset      | #Vertices | #Links  | #Features | #Classes | Metric       |\\n|--------------|-----------|---------|-----------|----------|--------------|\\n| Cora         | 2,708     | 5,429   | 1,433     | 7        | Accuracy     |\\n| CiteSeer     | 3,327     | 4,732   | 3,703     | 6        | Accuracy     |\\n| PubMed       | 19,717    | 44,338  | 500       | 3        | Accuracy     |\\n| Coauthor-CS  | 18,333    | 81,894  | 6,805     | 15       | Accuracy     |\\n| Coauthor-Physics | 34,493 | 247,962 | 8,415     | 5        | Accuracy     |\\n| Amazon-Photo | 7,487     | 119,043 | 745       | 8        | Accuracy     |\\n| Amazon-Computers | 13,381 | 245,778 | 767       | 10       | Accuracy     |\\n| ogbn-arxiv   | 169,343   | 1,166,243 | 128     | 40       | Accuracy     |\\n| ogbn-proteins| 132,534   | 39,561,252 | 8      | 112      | ROC-AUC      |\\n\\nIn summary, our designed search space contains 26,206 different architectures (after removing isomorphic structures, i.e., architectures that appear differently but have the same functionality, see Appendix A.5), which covers many representative GNN variants, including methods mentioned above as well as more advanced architectures such as JK-Net [69], residual- and dense-like GNNs [29].\\n\\n3.2 Datasets\\n\\nWe adopt nine publicly available datasets commonly used in GraphNAS: Cora, CiteSeer, and PubMed [51], Coauthor-CS, Coauthor-Physics, Amazon-Photo, and Amazon-Computer [52], ogbn-arxiv and ogbn-proteins [21]. The statistics and evaluation metrics of the datasets are summarized in Table 1. These datasets cover different sizes from thousands of vertices and links to millions of links, and various application domains including citation graphs, e-commerce graphs, and protein graphs. More details about the datasets are provided in Appendix A.1.\\n\\nThe dataset splits are as follows. For Cora, CiteSeer, and PubMed, we use the public semi-supervised setting by [71], i.e., 20 nodes per class for training and 500 nodes for validation. For two Amazon and two Coauthor datasets, we follow [52] and randomly split the train/validation/test set in a semi-supervised setting, i.e., 20 nodes per class for training, 30 nodes per class for validation, and the rest for testing. For ogbn-arxiv and ogbn-proteins, we follow the official splits of the dataset.\\n\\nFor ogbn-proteins, we find through preliminary studies that using GIN and k-GNN operations consistently makes the model parameters converge to explosion and therefore results in meaningless results. Besides, GAT and ChebNet will result in out-of-memory errors for our largest GPUs with 32GB of memories. Therefore, to avoid wasting computational resources, we restrict the candidate operations for ogbn-proteins to be GCN, ARMA, GraphSAGE, Identity and fully connected layer. After such changes, there are 2,021 feasible architectures for ogbn-proteins.\\n\\n3.3 Experimental Setting\\n\\nHyper-parameters\\n\\nTo ensure fair and reproducible comparisons, we propose a unified evaluation protocol. Specifically, we consider the following hyper-parameters with tailored ranges:\"}"}
{"id": "bBff294gqLp", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 Number of pre-process layers: 0 or 1.\\n\u2022 Number of post-process layers: 0 or 1.\\n\u2022 Dimensionality of hidden units: 64, 128, or 256.\\n\u2022 Dropout rate: 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8.\\n\u2022 Optimizer: SGD or Adam.\\n\u2022 Learning Rate (LR): 0.5, 0.2, 0.1, 0.05, 0.02, 0.01, 0.005, 0.002, 0.001.\\n\u2022 Weight Decay: 0 or 0.0005.\\n\u2022 Number of training epochs: 200, 300, 400, 500.\\n\\nFor each dataset, we fix the hyper-parameters for all architectures to ensure a fair comparison. Notice that jointly enumerating architectures and hyper-parameters will result in billions of architecture-hyper-parameter pairs and is infeasible in practice. Therefore, we first optimize the hyper-parameters to a proper value which can accommodate different GNN architectures, and then focus on the GNN architectures. Specifically, we adopt 30 GNN architectures from our search space as \u201canchors\u201d and adopt random search for hyper-parameter optimization [3]. The 30 anchor architectures are composed of 20 randomly selected architectures from our search space and 10 classic GNN architectures including GCN, GAT, GIN, GraphSAGE, and ARMA with 2 and 3 layers. We optimize the hyper-parameters by maximizing the average performance of the anchor architectures. The detailed selected hyper-parameters for each dataset are shown in the Appendix A.2.\\n\\n**Metrics**\\n\\nDuring training each architecture, we record the following metric covering both model effectiveness and efficiency: train/validation/test loss value and evaluation metric at each epoch, the model latency, and the number of parameters. The hardware and software configurations are provided in Appendix A.3. Besides, all experiments are repeated three times with different random seeds. The total time cost of creating our benchmark is approximately 8,000 GPU hours (see Appendix A.4).\\n\\n**4 Analyses**\\n\\nIn this section, we carry out empirical analyses to gain insights for our proposed benchmark. All the following analyses are based on the average performances of three random seeds.\\n\\n**4.1 Performance Distribution**\\n\\nWe first visualize the distribution of performances, including the accuracy, the latency, and the numbers of parameters, of all architectures in Figure 2. We make several interesting observations. For the effectiveness aspect, many architectures can obtain a reasonably good result, but architectures with exceptionally strong results are still rare. On the other hand, the latency and the numbers of parameters of architectures differ greatly. Since both model effectiveness and efficiency are critical for GraphNAS, we mark the Pareto-optimal with respect to accuracy and latency in the figure. The results show that, even for top-ranking architectures with similar accuracy, their latency varies greatly. In addition, we observe that the number of parameters and the latency are positively correlated in general (details are shown in the Appendix B.1).\\n\\n**4.2 Architecture Distribution**\\n\\nAs introduced in Section 3.1, our proposed search space mainly consists of the macro space (i.e., the DAG) and candidate GNN operations. To gain insights of how different macro space and GNN operation choices contribute to the model effectiveness, we select the top 5% architectures on each dataset and plot the frequency of the macro search space and operation choices. The results are shown in Figure 3. We make the following observations.\\n\\nFirst, there exist significant differences in the macro space choices for different datasets, indicating that different GNN architectures suit different graph data. For example, Cora, CiteSeer and PubMed tend to select a 2-layer DAG, i.e., (E), (F), (G), and (H) (please refer to Figure 1 for the detailed DAGs). PudMed and CS also prefer the 1-layer DAG (I), which is hardly selected in other datasets. Physics, Photo, and Computers show more balanced distributions on the macro space. ogbn-arXiv\"}"}
{"id": "bBff294gqLp", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The distribution of accuracy, latency, and the numbers of parameters of all architectures.\\nThe Pareto-optimal architectures w.r.t. accuracy and latency are marked with red crosses.\\n\\n(a) Cora  (b) CiteSeer  (c) PubMed  \\n(d) Coauthor-CS  (e) Coauthor-Physics  \\n(f) Amazon-Photo  (g) Amazon-Computers  (h) ogbn-arXiv  (i) ogbn-proteins\\n\\nFigure 3: The frequency of the macro space and operation choices in the top 5% architectures of different datasets. Please refer to Figure 1 for the macro space choices.\\n\\nand ogbn-proteins select deeper architectures more frequently, e.g., the 4-layer DAG (A) and the 3-layer DAG (B), (C), and (D).\\n\\nAs for the operation distribution, different datasets show more similar patterns. GCN and GAT are selected most frequently in almost all datasets. Surprisingly, even though GIN and k-GNN are shown to be theoretically more expressive in terms of the Weifeiler-Lehman test, they are only selected in the relatively small datasets, i.e., Cora, CiteSeer, and PubMed. A plausible reason is that GIN and k-GNN adopt the summation aggregation function in the GNN layers, which is not suitable for node-level tasks in large-scale graphs. Moreover, different from NAS in computer vision where...\"}"}
{"id": "bBff294gqLp", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.3 Cross-datasets Correlations\\n\\nTo measure how architectures perform across different datasets, we calculate the performance correlation of all architectures on dataset pairs as You et al. [73]. Specifically, we adopt three metrics: Pearson correlation coefficient, Kendall rank correlation, and the overlapping ratio of the top 5% architectures, i.e., if there are $N$ architectures belonging to the top 5% of both two different datasets in terms of accuracy, then the overlapping ratio is $N/(26 \\\\times 206 \\\\times 5\\\\%)$. We show the results in Figure 4.\\n\\nWe can observe that the correlation matrix has roughly block structures, indicating that there exist groups of datasets in which architectures share more correlations. For example, Cora, CiteSeer and PubMed generally show strong correlations. The correlations are also relatively high between Physics, Photo, Computers, and ogbn-arXiv. Notice that even for these datasets, only the Pearson correlation coefficient and Kendall rank correlation have large values, while the overlapping ratio of the top 5% architectures is considerably lower (e.g., no larger than 0.3). Since we usually aim to discover best architectures, the results indicate that best-performing architectures in different graph datasets exhibit complicated patterns, and directly transferring the best architecture from a similar dataset as You et al. [73] may not lead to the optimal result. More analyses about the transferability of the optimal architectures on different datasets can be found in the Appendix B.3.\\n\\n4.4 Detailed Explorations in Architectures\\n\\nSince enumerating all possible architectures to find the best-performing one is infeasible in practice, NAS search strategies inevitably need prior assumptions on the architectures. These explicit or implicit assumptions largely determine the effectiveness of the NAS methods. Next, we explore the architectures in details to verify some common assumptions.\\n\\nEvolutionary Algorithm is one of the earliest adopted optimization methods for NAS [50]. In the mutation process of evolutionary algorithms, i.e., randomly changing choices in the search space, a common assumption is that similar architectures have relatively similar performance so that smoothly mutating architectures is feasible [84]. To verify this assumption, we calculate the performance difference between architectures with different number of mutations together with the average performance difference between randomly chosen architectures as a reference line. The results are shown in Figure 5. We can observe that the performance difference between mutated architectures is considerably smaller than two random architecture, verifying the smoothness assumption in mutations. Besides, we observe that the performance difference increases as the number of mutations in general and changing operation choices usually leads to smaller differences than changing the macro space choices. These observations may inspire further research of evolution algorithms for GraphNAS.\\n\\nReinforcement Learning (RL) is also widely adopted in NAS [87, 47, 85]. In RL-based NAS, architectures are usually generated by a Markov Decision Process, i.e., deciding the architectures\"}"}
{"id": "bBff294gqLp", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 5: The performance difference between architectures with different number of mutations. The red lines indicate the average performance difference between two random architectures.\\n\\nFigure 6: $\\\\eta^2$ for the architecture performance with different lengths of prefixes/suffixes as groups. Larger $\\\\eta^2$ means that the groups can better explain the variance and the corresponding architectures choices are more important for the performance.\\n\\nUsing a sequential order [87]. Most GraphNAS methods simply assume a natural order, i.e., generating architectures from lower parts to deeper parts [16]. To gain insights for such methods, we analyze the importance of different positions of architectures. Specifically, we group architectures using their prefixes (i.e., lower-parts choices) and their suffixes (i.e., deeper-parts choices), corresponding to generating architectures using the natural order and the reserve order. Then, we adopt the following metric from analysis of variance [46]:\\n\\n$$\\\\eta^2 = \\\\frac{SSB}{SST} = \\\\frac{\\\\sum_{k=1}^{K} n_k (\\\\mu_k - \\\\bar{\\\\mu})^2}{\\\\sum_{i=1}^{N} (x_i - \\\\bar{\\\\mu})^2},$$\\n\\nwhere $SSB$ and $SST$ are the between-group variations and the total variation, $N$ is the number of architectures, $x_i$ is the accuracy of an architecture $i$, $\\\\mu$ is the mean accuracy, $K$ is the number of groups, $n_k$ is the number of architectures in the group $k$, and $\\\\mu_k$ is the mean performance of architectures in the group $k$. In short, a larger $\\\\eta^2$ means the groups can better explain the variances in the samples so that the factors for the group (i.e., architecture choices) are more important. The results of varying the length of the prefixes and suffixed are shown in Figure 6. Somewhat surprisingly, in most datasets, $\\\\eta^2$ in the reverse order is larger than that in the natural order, indicating that deeper parts of the architecture are more influential for the performance than the lower parts. With the length of prefixes and suffixes growing, $\\\\eta^2$ grows significantly. The results indicate that the natural order...\"}"}
{"id": "bBff294gqLp", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: The performance of NAS methods in AutoGL and NNI using NAS-Bench-Graph. The best performance for each dataset is marked in bold. We also show the performance of the top 5% architecture (i.e., 20-quantiles) as a reference line. The results are averaged over five experiments with different random seeds and the standard errors are shown in the bottom right.\\n\\n| Library | Method | cora  | citeseer | pubmed | cs    | physics | photo | computers | arXiv |\\n|---------|--------|------|----------|--------|-------|---------|-------|-----------|-------|\\n| AutoGL  | GNAS   | 82.04| 0.17     | 70.89  | 0.16  | 77.79   | 0.02  | 90.97     | 0.06  |\\n| AutoGL  | Auto-GNN| 81.80| 0.00     | 70.76  | 0.12  | 77.69   | 0.16  | 91.04     | 0.04  |\\n| NNI     | Random | 82.09| 0.08     | 70.49  | 0.08  | 77.91   | 0.07  | 90.93     | 0.07  |\\n| NNI     | EA     | 81.85| 0.20     | 70.48  | 0.12  | 77.96   | 0.12  | 90.60     | 0.07  |\\n| NNI     | RL     | 82.27| 0.21     | 70.66  | 0.12  | 77.96   | 0.09  | 90.98     | 0.01  |\\n\\nThe top 5% performances:\\n- cora: 80.63\\n- citeseer: 69.07\\n- pubmed: 76.60\\n- cs: 90.01\\n- physics: 91.67\\n- photo: 91.57\\n- computers: 82.77\\n- arXiv: 71.69\\n\\nTo convert architectures into sequences may not be the optimal solution for GraphNAS and more research could be explored in this direction.\\n\\nFigure 7: The average weight of different operations obtained using DARTS as the search algorithm. Compared to the weights distribution of the top 5% architectures shown in Figure 3(b), we can find that in some datasets, DARTS dispatches the largest weight to the most frequent operations in best-performing architecture, e.g., GAT for Cora and GCN for Amazon-Computers, indicating the effectiveness of DARTS in searching best-performing architectures.\\n\\n5 Example Usages\\nIn this section, we showcase the usage of NAS-Bench-Graph with existing open libraries including AutoGL [18] and NNI [42] (detailed example codes are provided in the Appendix C). Specifically, we run two NAS algorithms through AutoGL: GNAS [16] and Auto-GNN [84]. For NNI, we adopt Random Search [33], Evolutionary Algorithm (EA), and Policy-based Reinforcement Learning (RL). To ensure fair comparisons, we only let each algorithm access the performances of 2% of the all architectures in the search space. We report the results in Table 2. We also show the performances of the top 5% architecture, i.e, the 20-quantiles of each dataset in the table.\\n\\nFrom the results, we can observe that all algorithms outperform the top 5% performance, indicating that they can learn informative patterns in NAS-Bench-Graph. However, no algorithm can consistently win on all datasets. Surprisingly, Random Search is still a strong baseline when compared with other methods and even performs the best on two datasets, partially corroborating the findings in [33] for general NAS. The results indicate that further research on GraphNAS is still urgently needed.\\n\\nTo investigate the learning of different GraphNAS methods, we plot the curves of the optimal performance with respect to the number of architectures. The results are shown in Figure 8. We can find that different algorithms behave differently. For example, EA and AGNN show a few \\\"jumps\\\", i.e., the performance largely increases, while RL shows more smooth increasing patterns. Taking closer looks at the learning curve may provide inspirations for developing new algorithms for GraphNAS.\\n\\n6 Conclusion and Future Work\\nIn this paper, we propose NAS-Bench-Graph, the first tailored NAS benchmark for graph neural networks. We have trained all 26,206 GNN architectures in our designed search space on nine representative graph datasets with a unified evaluation protocol. NAS-Bench-Graph can support\"}"}
{"id": "bBff294gqLp", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8: The learning curve of the optimal performance with respect to the number of searched architectures. All results are averaged over five experiments with different random seeds. The standard errors are shown in the background.\\n\\nunified, reproducible, and efficient evaluations for GraphNAS. We also provide in-depth analyses for NAS-Bench-Graph and show how it can be easily compatible with the existing GraphNAS libraries. Since constructing tabular NAS benchmarks consumes extensive computational resources, our proposed NAS-Bench-Graph has a relatively limited search space (i.e., 26,206 unique GNN architectures). A possible direction is constructing surrogate benchmarks for GraphNAS to allow larger search spaces. We also plan to extend our proposed benchmark to other graph tasks besides node classification, such as link prediction and graph classification.\\n\\nAcknowledgments and Disclosure of Funding\\n\\nThis work was supported in part by the National Key Research and Development Program of China No. 2020AAA0106300, National Natural Science Foundation of China (No. 62250008, 62222209, 62102222), China National Postdoctoral Program for Innovative Talents No. BX20220185, and China Postdoctoral Science Foundation No. 2022M711813.\"}"}
{"id": "bBff294gqLp", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.\\n\\n[2] Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimization: a methodological tour d'horizon. European Journal of Operational Research, 290(2):405\u2013421, 2021.\\n\\n[3] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of machine learning research, 13(2), 2012.\\n\\n[4] Filippo Maria Bianchi, Daniele Grattarola, Lorenzo Livi, and Cesare Alippi. Graph neural networks with convolutional arma filters. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\\n\\n[5] Jie Cai, Xin Wang, Chaoyu Guan, Yateng Tang, Jin Xu, Bin Zhong, and Wenwu Zhu. Multi-modal continual graph learning with neural architecture search. In Proceedings of the ACM Web Conference 2022, pages 1292\u20131300, 2022.\\n\\n[6] Shaofei Cai, Liang Li, Jincan Deng, Beichen Zhang, Zheng-Jun Zha, Li Su, and Qingming Huang. Rethinking graph neural architecture search from message-passing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6657\u20136666, 2021.\\n\\n[7] Hanlin Chen, Ming Lin, Xiuyu Sun, and Hao Li. Nas-bench-zero: A large scale dataset for understanding zero-shot neural architecture search. In OpenReview, 2021.\\n\\n[8] Jiamin Chen, Jianliang Gao, Yibo Chen, Moctard Babatounde Oloulade, Tengfei Lyu, and Zhao Li. Graphpas: Parallel architecture search for graph neural networks. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2182\u20132186, 2021.\\n\\n[9] Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. Advances in neural information processing systems, 29:3844\u20133852, 2016.\\n\\n[10] Yuhui Ding, Quanming Yao, Huan Zhao, and Tong Zhang. Diffmg: Differentiable meta graph search for heterogeneous graph neural networks. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 279\u2013288, 2021.\\n\\n[11] Xuanyi Dong, Lu Liu, Katarzyna Musial, and Bogdan Gabrys. Nats-bench: Benchmarking nas algorithms for architecture topology and size. IEEE transactions on pattern analysis and machine intelligence, 2021.\\n\\n[12] Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search. In International Conference on Learning Representations, 2019.\\n\\n[13] Yawen Duan, Xin Chen, Hang Xu, Zewei Chen, Xiaodan Liang, Tong Zhang, and Zhenguo Li. Transnas-bench-101: Improving transferability and generalizability of cross-task neural architecture search. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5251\u20135260, 2021.\\n\\n[14] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The Journal of Machine Learning Research, 20(1):1997\u20132017, 2019.\\n\\n[15] Guosheng Feng, Chunnan Wang, and Hongzhi Wang. Search for deep graph neural networks. arXiv preprint arXiv:2109.10047, 2021.\\n\\n[16] Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu. Graph neural architecture search. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, volume 20, pages 1403\u20131409, 2020.\"}"}
{"id": "bBff294gqLp", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chaoyu Guan, Xin Wang, Hong Chen, Ziwei Zhang, and Wenwu Zhu. Large-scale graph neural architecture search. In International Conference on Machine Learning, pages 7968\u20137981. PMLR, 2022.\\n\\nChaoyu Guan, Xin Wang, and Wenwu Zhu. Autoattend: Automated attention representation search. In International Conference on Machine Learning, pages 3864\u20133874. PMLR, 2021.\\n\\nWilliam L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 1025\u20131035, 2017.\\n\\nYoichi Hirose, Nozomu Yoshinari, and Shinichi Shirakawa. Nas-hpo-bench-ii: A benchmark dataset on joint optimization of convolutional neural network architecture and training hyperparameters. In Asian Conference on Machine Learning, pages 1349\u20131364. PMLR, 2021.\\n\\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Neural Information Processing Systems, 2020.\\n\\nZHAO Huan, YAO Quanming, and TU Weiwei. Search to aggregate neighborhood for graph neural network. In 2021 IEEE 37th International Conference on Data Engineering, pages 552\u2013563. IEEE, 2021.\\n\\nShengli Jiang and Prasanna Balaprakash. Graph neural network architecture search for molecular property prediction. In 2020 IEEE International Conference on Big Data, pages 1346\u20131353. IEEE, 2020.\\n\\nWeiwei Jiang and Jiayun Luo. Graph neural network for traffic forecasting: A survey. arXiv preprint arXiv:2101.11174, 2021.\\n\\nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017.\\n\\nAaron Klein and Frank Hutter. Tabular benchmarks for joint architecture and hyperparameter optimization. arXiv preprint arXiv:1905.04970, 2019.\\n\\nNikita Klyuchnikov, Ilya Trofimov, Ekaterina Artemova, Mikhail Salnikov, Maxim Fedorov, and Evgeny Burnaev. Nas-bench-nlp: neural architecture search benchmark for natural language processing. arXiv preprint arXiv:2006.07116, 2020.\\n\\nChaojian Li, Zhongzhi Yu, Yonggan Fu, Yongan Zhang, Yang Zhao, Haoran You, Qixuan Yu, Yue Wang, Cong Hao, and Yingyan Lin. Hw-nas-bench: Hardware-aware neural architecture search benchmark. In International Conference on Learning Representations, 2020.\\n\\nGuohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9267\u20139276, 2019.\\n\\nHaoyang Li, Xin Wang, Ziwei Zhang, Jianxin Ma, Peng Cui, and Wenwu Zhu. Intention-aware sequential recommendation with structured intent transition. IEEE Transactions on Knowledge and Data Engineering, 2021.\\n\\nHaoyang Li, Xin Wang, Ziwei Zhang, Zehuan Yuan, Hang Li, and Wenwu Zhu. Disentangled contrastive learning on graphs. Advances in Neural Information Processing Systems, 34:21872\u201321884, 2021.\\n\\nHaoyang Li, Ziwei Zhang, Xin Wang, and Wenwu Zhu. Disentangled graph contrastive learning with independence promotion. IEEE Transactions on Knowledge and Data Engineering, 2022.\\n\\nLiam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. In Uncertainty in artificial intelligence, pages 367\u2013377. PMLR, 2020.\\n\\nYanxi Li, Zean Wen, Yunhe Wang, and Chang Xu. One-shot graph neural architecture search with dynamic search space. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8510\u20138517, 2021.\"}"}
{"id": "bBff294gqLp", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Marius Lindauer and Frank Hutter. Best practices for scientific research on neural architecture search. Journal of Machine Learning Research, 21(243):1\u201318, 2020.\\n\\nHanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In International Conference on Learning Representations, 2018.\\n\\nZemin Liu, Yuan Fang, Chenghao Liu, and Steven CH Hoi. Relative and absolute location embedding for few-shot node classification on graph. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 4267\u20134275, 2021.\\n\\nZemin Liu, Qiheng Mao, Chenghao Liu, Yuan Fang, and Jianling Sun. On size-oriented long-tailed graph classification of graph neural networks. In Proceedings of the ACM Web Conference 2022, pages 1506\u20131516, 2022.\\n\\nZemin Liu, Trung-Kien Nguyen, and Yuan Fang. Tail-gnn: Tail-node graph neural networks. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 1109\u20131119, 2021.\\n\\nQing Lu, Weiwen Jiang, Meng Jiang, Jingtong Hu, Sakyasingha Dasgupta, and Yiyu Shi. Fgnas: Fpga-aware graph neural architecture search. OpenReview, 2020.\\n\\nAbhinav Mehrotra, Alberto Gil CP Ramos, Sourav Bhattacharya, \u0141ukasz Dudziak, Ravichander Vipperla, Thomas Chau, Mohamed S Abdelfattah, Samin Ishtiaq, and Nicholas Donald Lane. Nas-bench-asr: Reproducible neural architecture search for speech recognition. In International Conference on Learning Representations, 2020.\\n\\nMicrosoft. Neural Network Intelligence, 1 2021.\\n\\nChristopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 4602\u20134609, 2019.\\n\\nMatheus Nunes and Gisele L Pappa. Neural architecture search in graph neural networks. In Brazilian Conference on Intelligent Systems, pages 302\u2013317. Springer, 2020.\\n\\nZheyi Pan, Songyu Ke, Xiaodu Yang, Yuxuan Liang, Yong Yu, Junbo Zhang, and Yu Zheng. Autostg: Neural architecture search for predictions of spatio-temporal graph. In Proceedings of the Web Conference 2021, pages 1846\u20131855, 2021.\\n\\nCharles A Pierce, Richard A Block, and Herman Aguinis. Cautionary note on reporting eta-squared values from multifactor anova designs. Educational and psychological measurement, 64(6):916\u2013924, 2004.\\n\\nYijian Qin, Xin Wang, Peng Cui, and Wenwu Zhu. Gqnas: Graph q network for neural architecture search. In 21st IEEE International Conference on Data Mining, 2021.\\n\\nYijian Qin, Xin Wang, Zeyang Zhang, and Wenwu Zhu. Graph differentiable architecture search with structure learning. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nYijian Qin, Xin Wang, Ziwei Zhang, Pengtao Xie, and Wenwu Zhu. Graph neural architecture search under distribution shifts. In International Conference on Machine Learning, pages 18083\u201318095. PMLR, 2022.\\n\\nEsteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V Le, and Alexey Kurakin. Large-scale evolution of image classifiers. In International Conference on Machine Learning, pages 2902\u20132911. PMLR, 2017.\\n\\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93\u201393, 2008.\\n\\nOleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Pitfalls of graph neural network evaluation. Relational Representation Learning Workshop, NeurIPS 2018, 2018.\"}"}
{"id": "bBff294gqLp", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Min Shi, David A Wilson, Xingquan Zhu, Yu Huang, Yuan Zhuang, Jianxun Liu, and Yufei Tang. Evolutionary architecture search for graph neural networks. arXiv preprint arXiv:2009.10199, 2020.\\n\\nJonathan Shlomi, Peter Battaglia, and Jean-Roch Vlimant. Graph neural networks in particle physics. Machine Learning: Science and Technology, 2(2):021001, 2020.\\n\\nChang Su, Jie Tong, Yongjun Zhu, Peng Cui, and Fei Wang. Network embedding in biomedical data science. Briefings in bioinformatics, 21(1):182\u2013197, 2020.\\n\\nJunwei Sun, Bai Wang, and Bin Wu. Automated graph representation learning for node classification. In 2021 International Joint Conference on Neural Networks, pages 1\u20137. IEEE, 2021.\\n\\nKe Tu, Jianxin Ma, Peng Cui, Jian Pei, and Wenwu Zhu. Autone: Hyperparameter optimization for massive network embedding. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 216\u2013225, 2019.\\n\\nRenbo Tu, Mikhail Khodak, Nicholas Carl Roberts, Nina Balcan, and Ameet Talwalkar. Nas-bench-360: Benchmarking diverse tasks for neural architecture search. In OpenReview, 2021.\\n\\nPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018.\\n\\nXingchen Wan. On redundancy and diversity in cell-based neural architecture search. In Presented at the 10th International Conference on Learning Representations, volume 25, page 29, 2022.\\n\\nXin Wang, Shuyi Fan, Kun Kuang, and Wenwu Zhu. Explainable automated graph representation learning with hyperparameter importance. In International Conference on Machine Learning, pages 10727\u201310737. PMLR, 2021.\\n\\nZhili Wang, Shimin Di, and Lei Chen. Autogel: An automated graph neural network with explicit link information. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nLanning Wei, Huan Zhao, and Zhiqiang He. Learn layer-wise connections in graph neural networks. arXiv preprint arXiv:2112.13585, 2021.\\n\\nLanning Wei, Huan Zhao, and Zhiqiang He. Designing the topology of graph neural networks: A novel feature fusion perspective. In Proceedings of the ACM Web Conference 2022, pages 1381\u20131391, 2022.\\n\\nLanning Wei, Huan Zhao, Quanming Yao, and Zhiqiang He. Pooling architecture search for graph classification. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 2091\u20132100, 2021.\\n\\nShiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. Graph neural networks in recommender systems: a survey. ACM Computing Surveys (CSUR), 2020.\\n\\nZonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 32(1):4\u201324, 2020.\\n\\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019.\\n\\nKeyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, pages 5453\u20135462. PMLR, 2018.\\n\\nShen Yan, Colin White, Yash Savani, and Frank Hutter. Nas-bench-x11 and the power of learning curves. Advances in Neural Information Processing Systems, 34, 2021.\"}"}
{"id": "bBff294gqLp", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pages 40\u201348. PMLR, 2016.\\n\\nChris Ying, Aaron Klein, Eric Christiansen, Esteban Real, Kevin Murphy, and Frank Hutter. Nas-bench-101: Towards reproducible neural architecture search. In International Conference on Machine Learning, pages 7105\u20137114. PMLR, 2019.\\n\\nJiaxuan You, Zhitao Ying, and Jure Leskovec. Design space for graph neural networks. Advances in Neural Information Processing Systems, 33, 2020.\\n\\nArber Zela, Julien Siems, and Frank Hutter. Nas-bench-1shot1: Benchmarking and dissecting one-shot neural architecture search. In International Conference on Learning Representations, 2019.\\n\\nArber Zela, Julien Niklas Siems, Lucas Zimmer, Jovita Lukasik, Margret Keuper, and Frank Hutter. Surrogate nas benchmarks: Going beyond the limited search spaces of tabular nas benchmarks. In International Conference on Learning Representations, 2021.\\n\\nWentao Zhang, Yu Shen, Zheyu Lin, Yang Li, Xiaosen Li, Wen Ouyang, Yangyu Tao, Zhi Yang, and Bin Cui. Pasca: a graph neural architecture search system under the scalable paradigm. Proceedings of the Web Conference 2022, 2022.\\n\\nZiwei Zhang, Peng Cui, and Wenwu Zhu. Deep learning on graphs: A survey. IEEE Transactions on Knowledge and Data Engineering, 2020.\\n\\nZiwei Zhang, Xin Wang, and Wenwu Zhu. Automated machine learning on graphs: A survey. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, 2021.\\n\\nHuan Zhao, Lanning Wei, quanming yao, and Zhiqiang He. Efficient graph neural architecture search, 2021.\\n\\nHuan Zhao, Lanning Wei, and Quanming Yao. Simplifying architecture search for graph neural network. arXiv preprint arXiv:2008.11652, 2020.\\n\\nYiren Zhao, Duo Wang, Daniel Bates, Robert Mullins, Mateja Jamnik, and Pietro Lio. Learned low precision graph neural networks. In The 2nd Workshop on Machine Learning and Systems, 2021.\\n\\nYiren Zhao, Duo Wang, Xitong Gao, Robert Mullins, Pietro Lio, and Mateja Jamnik. Probabilistic dual network architecture search on graphs. arXiv preprint arXiv:2003.09676, 2020.\\n\\nJie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications. AI Open, 1:57\u201381, 2020.\\n\\nKaixiong Zhou, Qingquan Song, Xiao Huang, and Xia Hu. Auto-gnn: Neural architecture search of graph neural networks. arXiv preprint arXiv:1909.03184, 2019.\\n\\nYuwei Zhou, Xin Wang, Hong Chen, Xuguang Duan, Chaoyu Guan, and Wenwu Zhu. Curriculum-nas: Curriculum weight-sharing neural architecture search. In Proceedings of the 30th ACM International Conference on Multimedia, MM '22, page 6792\u20136801, 2022.\\n\\nWenwu Zhu and Xin Wang. Automated machine learning and meta-learning for multimedia, 2021.\\n\\nBarret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In International Conference on Learning Representations, 2017.\"}"}
{"id": "bBff294gqLp", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n   (b) Did you describe the limitations of your work? [Yes]\\n   (c) Did you discuss any potential negative societal impacts of your work? [Yes]\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n   (b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes]\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes]\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes]\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes]\\n   (b) Did you mention the license of the assets? [Yes]\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\\n   (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\"}"}
