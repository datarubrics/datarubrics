{"id": "ChWo6qLgILf", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning\\n\\nChangan Chen 1, 4,\u2217 Carl Schissler 2,\u2217 Sanchit Garg 2,\u2217 Philip Kobernik 2, Alexander Clegg 4, Paul Calamia 2, Dhruv Batra 3, 4, Philip Robinson 2, Kristen Grauman 1, 4\\n\\n1 UT Austin\\n2 Reality Labs at Meta\\n3 Georgia Tech\\n4 Meta AI\\n\\nAbstract\\nWe introduce SoundSpaces 2.0, a platform for on-the-fly geometry-based audio rendering for 3D environments. Given a 3D mesh of a real-world environment, SoundSpaces can generate highly realistic acoustics for arbitrary sounds captured from arbitrary microphone locations. Together with existing 3D visual assets, it supports an array of audio-visual research tasks, such as audio-visual navigation, mapping, source localization and separation, and acoustic matching. Compared to existing resources, SoundSpaces 2.0 has the advantages of allowing continuous spatial sampling, generalization to novel environments, and configurable microphone and material properties. To our knowledge, this is the first geometry-based acoustic simulation that offers high fidelity and realism while also being fast enough to use for embodied learning. We showcase the simulator's properties and benchmark its performance against real-world audio measurements. In addition, we demonstrate two downstream tasks\u2014embodied navigation and far-field automatic speech recognition\u2014and highlight sim2real performance for the latter. SoundSpaces 2.0 is publicly available to facilitate wider research for perceptual systems that can both see and hear.\\n\\n1 Introduction\\nWhat we see and hear dominates our perceptual experience, and there is often a strong relationship between the two modalities. At the object level, we can anticipate the sounds an object makes based on how it looks, and vice versa (a dog barks, a door slams, a baby cries). At the environment level, materials and geometry of the surrounding 3D space that we see transform the sounds that reach our ears. For example, a person speaking in a marble-floored, high-ceiling museum sounds distinct from one speaking in a cozy carpeted bookshop.\\n\\nModeling the correspondence between visuals and acoustics in 3D spaces is of vital importance for many applications in embodied AI and augmented/virtual reality (AR/VR). For instance, a rescue robot needs to localize the person who is calling for help; a service robot needs to look and listen to know if the espresso machine is running properly; an AR system needs to generate sounds that are consistent with the user's acoustical environment for an immersive experience.\\n\\nRealistic simulations of the first-person perceptual experience are a valuable resource for AI research. They allow training and evaluating models at scale and in a replicable manner. On the visual side, fast visual simulators [60, 70] coupled with 3D assets from scanned real-world environments [11, 78, 68, 56] have facilitated substantial work in visual navigation and related tasks in recent years [77, 12, 6, 30, 57, 34], enabling rigorous benchmarks [1] and even successful \u201csim2real\u201d transfer to agents that move in the real world [76, 73, 36]. On the audio side, acoustic simulation has been traditionally\\n\\n\u2217 Equal contribution\\n\\nhttps://github.com/facebookresearch/sound-spaces\\n\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks.\"}"}
{"id": "ChWo6qLgILf", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Direct sound\\nReflection\\nHRTF\\nTransmission\\nHardwood\\nGypsum\\nCarpet\\nVisual Render\\nAcoustic Render\\nReverb RGB\\nLeft Ear\\nRight Ear\\n\\n**Figure 1:** Illustration of SoundSpaces 2.0 rendering in a multi-room multi-floor HM3D environment. In this scenario, a boy is watching TV in the living room while his mom calls him to have dinner from the kitchen downstairs. We model various frequency-dependent acoustic phenomena for sound propagation from all sources (TV and mom) to him, including direct sound, reflection, reverb, transmission, diffraction and air absorption. The sound propagation is based on a bidirectional path-tracing algorithm that takes the geometry of the scene as well as materials of objects in the space as input. The received sound is spatialized to binaural with the head-related transfer function (HRTF). As a result, SoundSpaces 2.0 renders the visual and audio observations with spatial and acoustic correspondence. For example, the TV being situated more towards the right results in right-ear signals that are stronger than those in the left ear.\\n\\nPursued for physical models [9], gaming [43] and auralization for architectural design [75], typically restricted to simple parametric geometries and in isolation from visual context. Towards bringing the two modalities together in joint audio-visual simulations, recent work offers initial steps [14, 21]. SoundSpaces [14] provides highly realistic room impulse responses (RIRs) obtained via bidirectional path tracing for 100 real multi-room environment meshes from Replica [68] and Matterport3D [11], while ThreeDWorld [21] uses physics simulations in the Unity3D video game platform to model object collisions, impact sounds, and environment reverberation. These tools support an array of new research in navigation [22, 17, 14, 15, 12, 45], floorplan reconstruction [55], feature learning [23], audio-visual (de)reverberation [16, 13], and audio-visual collision detection [20].\\n\\nThough inspiring, these early platforms have several limitations. SoundSpaces\u2019s foremost limitation is its pre-computed, discretized nature. The provided RIRs are pre-computed for all source and receiver pairs on a 0.5m grid, and for a fixed list of 100 total environments. This prevents sampling data at new locations. This in turn means that 1) an agent in the simulator can only move or hop between discrete grid points in the space, which abstracts away some difficult parts of the navigation task; 2) the simulations do not generalize to novel environments\u2014just the 100 provided; and 3) the pre-computed data itself is on the order of TBs, impeding the ability to change configurations, e.g., of the microphone types or materials. ThreeDWorld [21] offers continuous-space rendering, yet it only supports audio rendering for simple 3D environment geometry, namely an oversimplified \u201cshoebox\u201d (rectangular parallelepiped) model, and thus is not applicable to real-scan datasets [78, 56]. In sum, today\u2019s audio-visual rendering platforms fall short in accuracy, speed, and flexibility, which in turn constrains the scope of research tasks they can support within audio-visual embodied learning [12, 17, 79] and visual-acoustic learning [67, 44, 13].\\n\\nIn this work, we introduce SoundSpaces 2.0, which performs on-the-fly geometry-based audio rendering for arbitrary environments. It allows highly realistic rendering of arbitrary camera views and arbitrary microphone placements for waveforms of the user\u2019s choosing, accounting for all major real-world acoustic factors: direct sounds, early specular/diffuse reflections, reverberation, binaural spatialization, and frequency-dependent effects from materials and air absorption (illustrated in Fig. 1). Furthermore, SoundSpaces 2.0 generalizes audio simulation to any input mesh, making it possible for...\"}"}
{"id": "ChWo6qLgILf", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison with existing non-commercial datasets/simulation platforms.\\n\\n| Platform      | Audio-Visual | Geometric | Configurable | Arbitrary Env |\\n|---------------|--------------|-----------|--------------|---------------|\\n| SoundSpaces   | \u2713            | \u2713         | \u2713            | \u2713             |\\n| GW A          | \u2713            | \u2713         | \u2713            | \u2713             |\\n| ThreeDWorld   | \u2713            | \u2713         | \u2713            | \u2713             |\\n| Pyroomacoustics| \u2713            | \u2713         | \u2713            | \u2713             |\\n\\nSoundSpaces 2.0 (Ours) allows for the first time to import sound into well-used environment assets like Gibson [78], HM3D [56], and Matterport3D [11], as well as any future or emerging one like Ego4D [25]. In addition, SoundSpaces 2.0 allows users to configure various properties of the simulation such as source-receiver locations, simulation parameters, material properties, and the microphone configuration. The rendering platform and associated research codebase are publicly available.\\n\\nIn this paper, we describe the new platform and its functionality, and we illustrate its flexibility with various concrete examples (please see also the Supplementary video). In addition, we perform systematic experiments to answer two questions: 1) how accurate are the audio-visual simulations? and 2) how well can machine learning models trained in SoundSpaces 2.0 generalize to real world data? For this purpose, we collect real-world audio RIR measurements for a public scene dataset Replica [68] and benchmark the simulation accuracy. We also benchmark two downstream tasks: continuous audio-visual navigation and far-field speech recognition. For speech recognition, we show the machine-learning models trained on our synthetic data can generalize when tested on real data. We propose an acoustic randomization technique that models the real-world distribution of materials' acoustic properties, and we show that this strategy leads to better sim2real generalization. Finally, aside from the rendering engine itself, which is readily integrated with Habitat [60], we also release SoundSpaces-PanoIR: a large-scale dataset of images paired with RIRs computed in SoundSpaces 2.0; this prepared dataset can facilitate future research on visual-acoustic learning in a stand-alone manner (without interfacing with the simulators themselves).\\n\\n2 Related Work\\n\\nWe overview related work on simulations, audio(-visual) learning, and sim2real transfer.\\n\\nAcoustic simulation. Sounds are first produced by vibrating objects and then propagate in space before reaching human ears. Modeling sound propagation has a long history in the literature, the goal of which is to simulate realistic high-fidelity audio that is consistent with the given environment specification. Interactive acoustic simulation systems have been extensively used in games and AR/VR applications. Sound propagation algorithms typically fall into two main categories: wave-based [3, 35, 50] and geometric [19, 42, 64]. Wave-based methods aim to solve the wave equation numerically, resulting in high computation expense. In the geometric method family, the Image-Source Methods [4] solve the specular reflection of sounds deterministically but have low accuracy for late reverb, while path-tracing based approaches offer both high accuracy and efficiency [59]. Aside from sound propagation, some simulators like TDW [21] model impact sounds between objects. Our work builds on the SoundSpaces [14] dataset in that we use their bidirectional path-tracing algorithm and simulation framework as a starting point; however, as discussed above, we overcome its core limitations by enabling on-the-fly rendering, and we also augment the propagation algorithm by adding diffraction and improving reverberation level accuracy. Compared to existing public platforms, SoundSpaces 2.0 adds significant generality and flexibility\u2014accepting arbitrary scene geometry, generalizing to new 3D meshes on-the-fly, rendering in real-time, and allowing configuration of materials and microphones\u2014all of which we demonstrate. See Table 1 for comparisons.\\n\\nAudio-visual learning. Recent advances in audio-visual learning include self-supervised cross-modal feature learning from video [5, 40, 49], object localization [28], and audio-visual speech [3].\"}"}
{"id": "ChWo6qLgILf", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"enhancement and source separation [18, 53, 80, 2, 48, 81, 27]. Besides learning from video, audio-visual simulation supports the study of embodied tasks like navigation [14, 22, 17, 45], where an agent moves intelligently based on the visual and auditory observations. Another line of research facilitated by simulation is visual-acoustic learning [67, 16, 13, 44, 46], where the goal is to either match or remove the room acoustics implied by the image. Acoustic rendering also facilitates audio-only research, such as far-field speech recognition [38, 47, 71], sound source separation [31, 7], localization [26, 31] and sound synthesis [33, 72]. Our work revisits multiple audio(-visual) learning tasks to showcase the advantages of having a continuous, configurable, and generalizable simulation.\\n\\nSimulation-to-reality transfer.\\n\\nSeveral large-scale datasets of real-world 3D scans of buildings have been released in the past few years [78, 11, 56]. In parallel, multiple simulation environments [78, 60, 39] have been created in order to simulate embodied motion in these 3D scans. These advances allow large-scale training, fast experimentation, consistent benchmarking, and replicable research compared to physical experimentation. Transferring the model trained in simulation to the real world is thus of great interest. The mostly widely used approaches are domain randomization [73, 74], system identification [36, 41], and transfer learning and domain adaptation [82]. Most sim2real transfer research studies transferring a policy from simulation to the real world based on visual input. While some work leverages synthetic audio data for speech tasks [71, 31] or builds a multisensory object dataset for sim2real [24], transferring models trained on acoustic simulation has been understudied due to the lack of real-world benchmarks. To the best of our knowledge, this is the first work to both benchmark simulation performance with real measurements (Sec. 5.2) as well conduct sim2real transfer for machine learning models (Sec. 5.4).\\n\\n3 SoundSpaces 2.0 Audio-Visual Rendering Platform\\n\\nIn this section, we detail the audio-visual rendering pipeline for SoundSpaces 2.0.\\n\\n3.1 Rendering Pipeline and Simulation Enhancements\\n\\nThe core of SoundSpaces 2.0 is the audio propagation engine (RLR-Audio-Propagation) we are releasing for research purposes. We integrate this engine into the existing visual simulator Habitat-Sim [60], which offers fast visual rendering. In addition, we provide high-level APIs for various downstream tasks (e.g., navigation) and training scripts at the SoundSpaces repo.\\n\\nFig. 1 illustrates the propagation pipeline. SoundSpaces 2.0 takes the scene mesh data processed by Habitat, together with source and receiver locations specified by the user, and computes a room impulse response (RIR) using a bidirectional path-tracing algorithm [10]. This module models various acoustic phenomena, including reflection, transmission, and diffraction, as well as spatialization. The simulation operates in logarithmically-spaced frequency bands (configurable), where it computes an energy-time histogram at the audio sampling rate. This histogram incorporates spatial information using spherical harmonics for each time sample that represents the directional distribution of arriving sound energy. This representation is then spatialized to either an ambisonic or binaural pressure impulse response [66], which can be convolved with the source audio signals to generate the sound at the receiver position. See Supp. for more details.\\n\\nCompared to the original SoundSpaces, we have improved the simulation in a few ways. SoundSpaces did not include any simulation of acoustic diffraction, and thus exhibited abrupt occlusion of sources. We have removed this limitation using the fast diffraction approach from [65], which is able to efficiently compute smooth diffraction effects for occluded sources. We also improved the accuracy of the direct-to-reverberant ratio (DRR), the ratio of the sound pressure level of a direct sound from a directional source to the reverberant sound pressure level, by fixing a bias of $\\\\sqrt{\\\\frac{4\\\\pi}{\\\\pi}}$ that was present in the indirect sound pressure of the original SoundSpaces.\\n\\nIn the following, we overview modeling advances in SoundSpaces 2.0 that promote continuity, configurability, generalizability, and performance.\\n\\n4\\n\\n4\\n\\nhttps://github.com/facebookresearch/rlr-audio-propagation\\n\\n5\\n\\nhttps://github.com/facebookresearch/habitat-sim/blob/main/docs/AUDIO.md\\n\\n6\\n\\nhttps://github.com/facebookresearch/sound-spaces\\n\\n4\"}"}
{"id": "ChWo6qLgILf", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 Continuity\\n\\nSpatial continuity.\\n\\nHumans move around in the real world continuously while hearing. Given an arbitrary source location \\\\( s \\\\), receiver location \\\\( r \\\\), and receiver's heading direction \\\\( \\\\theta \\\\) in a given mesh environment, we render the impulse response between the source and receiver as \\\\( R(s, r, \\\\theta) \\\\).\\n\\nThe sound received by the receiver is computed as \\\\( A_r = A_s * R(s, r, \\\\theta) \\\\), where \\\\( A_s \\\\) is the sound emitted from the source and \\\\( * \\\\) denotes convolution. Whereas SoundSpaces [14] restricts the \\\\( s \\\\) and \\\\( r \\\\) locations to a 0.5m discrete grid due to its pre-computed approach and hefty storage requirements, SoundSpaces 2.0 allows arbitrary placements.\\n\\nAcoustic continuity.\\n\\nWhile an agent moves in the environment, it moves smoothly from point A to point B (even with a small step size). With the spatial continuity property, we can render \\\\( R_s(s, r_A, \\\\theta_A) \\\\) and \\\\( R_r(s, r_B, \\\\theta_B) \\\\) for these two locations respectively. However, the original SoundSpaces takes the rendered IR for each location and convolves it with the source sound directly as the audio observation. This calculation implicitly assumes the source does not emit sound continuously, i.e., it starts to emit when the agent moves to a new location, stops after one second, and resumes at the agent's next location.\\n\\nIn SoundSpaces 2.0, we introduce acoustic continuity for both the source sound and listener. More specifically, given a sampling rate \\\\( F \\\\) and the time between two steps \\\\( \\\\Delta t \\\\), the number of received audio samples is \\\\( N = F \\\\Delta t \\\\) per step. Assuming a listener is at location \\\\( x_i \\\\) at time \\\\( t_i \\\\), the audio signal received by the listener at time \\\\( t_i \\\\) emitted from the source at time \\\\( t_p \\\\) is \\\\( t_i - R(s, x_i, \\\\theta_{x_i}) + 1 \\\\). We take the corresponding source sound segment \\\\( A_s[t_p:t_p+N] \\\\) and convolve it with \\\\( R(s, x_i, \\\\theta_{x_i}) \\\\) without zero padding to compute \\\\( A_{x_i}t_i \\\\). Following the common practice [51], we apply linear crossfading between \\\\( A_{x_i-1}t_i \\\\) and \\\\( A_{x_i}t_i \\\\) to smooth out the transition from \\\\( x_{i-1} \\\\) to \\\\( x_i \\\\) with an overlap time window of \\\\( T \\\\) seconds. See Supp. video for the impact on perceptual quality.\\n\\n3.3 Configurability\\n\\nDue to its pre-computed nature, it is impossible to change any simulation setup (parameters, microphones, or materials) for the original SoundSpaces. All are configurable in SoundSpaces 2.0, as summarized below and in more detail in Supp.\\n\\nSimulation parameters.\\n\\nWe expose many useful parameters for users to configure, including the sampling rate, the number of frequency bands, number of rays for direct/indirect sounds, whether reflection, transmission or diffraction is enabled, etc.\\n\\nMicrophone types.\\n\\nWe provide several types of built-in microphone configurations, including monaural single-channel audio, binaural (modeling a human listener), and ambisonics (full sphere surround sound). In addition, users are also able to configure their own microphone array by specifying an array of monaural microphone locations.\\n\\nCustom HRTFs.\\n\\nWe allow users to load their own head-related transfer functions (HRTFs), which incorporate customized human perception in the acoustic rendering simulation.\\n\\nMaterial modeling.\\n\\nMaterials of objects/surfaces have a big impact on how humans perceive the sound in an environment. Consider the difference between sound in a recording studio versus a living room of the same size. Due to the absorptive materials in the recording studio, the sound will consist primarily of direct sound without reverberation, whereas in the living room, the sound will consist of a mixture of direct sound and reverberation.\\n\\nExisting real-scan datasets have semantic annotations at the level of object categories, e.g., chair, table, couch and floor, while lacking material annotations of what these objects are made of, e.g., wood or steel for tables. SoundSpaces coped with this issue by defining a fixed mapping from object categories to acoustic materials, e.g., floors are always mapped to the carpet material, which is very absorptive. However, this fixed mapping fails to reflect the fact in the real world, different instances of the same object category could have very different acoustic properties, e.g., a floor could be carpet or wood or concrete materials depending on the home type.\\n\\nTo account for this variation, we expose an API to let users define their own acoustic material configurations. We provide 29 built-in acoustic materials, e.g., wood, concrete, curtain, soil, water. Every...\"}"}
{"id": "ChWo6qLgILf", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: An example of the SoundSpaces-PanoIR dataset rendered on Gibson [78]. We render panoramic RGB and depth images to capture the scene geometry. We provide the images, impulse responses, and the coordinates of the point source location with respect to the current camera pose.\\n\\nThe acoustic material has a list of candidate object categories to be mapped from. It also has a set of coefficients for absorption, scattering, and transmission in the following format:\\n\\n$$[f_1, c_1, f_2, c_2, ..., f_n, c_n]$$\\n\\nwhere $f_i$ is a frequency and $c_i$ is the coefficient for a certain acoustic phenomenon at frequency $f_i$.\\n\\nThis allows modeling the frequency-dependent acoustic properties of different acoustic materials. For example, high-frequency waves are absorbed more compared to low frequencies when reflecting from carpets. See Supp. for details.\\n\\nWe also model distance-dependent damping of the sound propagation media. This includes air absorption as well as transmission losses through materials. Air absorption is calculated using an analytical model [8]. Users can specify the frequency-dependent damping coefficients for each material, expressed as dB per meter, in a similar format to the other material properties.\\n\\n### 3.4 Generalizability\\n\\n**Generalization to scene datasets.** Our new simulator accommodates arbitrary 3D meshes as input. This makes it compatible with all available scene datasets (e.g., Gibson [78], HM3D [56], Ego4D [25], Matterport3D [11], Replica [68]), as well as any future assets that become available, such as if a user scans their own lab or home environment. This is an important advance over SoundSpaces, which was restricted to Replica and Matterport3D alone. See Supp. for videos of generated examples.\\n\\n**Generalization to shoebox rooms.** We expose APIs for creating shoebox rooms with different materials for walls, which simulates simpler setups as in Pyroomacoustics [61] and TDW [21].\\n\\n**Generalization to the real world.** The fidelity and flexibility of our simulation platform also supports generalization to the real world. In Sec. 5, we score the simulator output against real-world RIRs and show how machine learning models trained on SoundSpaces 2.0 can generalize to real data.\\n\\n### 3.5 Rendering Modes and Rendering Performance\\n\\nOur simulation generates high-quality audio rendering based on mesh and materials, and this fidelity can be instrumental for certain research areas. On the other hand, in tasks like embodied navigation with reinforcement learning, which typically require millions (or even billions [77]) of training iterations, rendering speed is of vital importance. Thus, we offer two built-in rendering modes: high-speed and high-quality.\\n\\nIn high-speed mode, we reduce the number of rays and improve the accuracy by leveraging previously computed impulse responses [63], under the assumption that movements are spatially continuous. Our algorithms use information computed on previous simulation frames, such as sound propagation paths and RIRs, to reduce the number of rays and ray bounces that are needed on each frame for sufficient sound quality (see Sec. 5.1).\\n\\nIn high-quality mode, we set all rendering parameters to max and turn off the temporal coherence feature to ensure that every impulse response is accurate without temporal blurring. Our engine is multi-threaded and users can set the number of threads when using either mode. See Sec. 5.1 for analysis of the simulation performance in terms of speed and accuracy.\"}"}
{"id": "ChWo6qLgILf", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Simulation speed vs. quality tradeoff. We report mean and standard deviation over 5 runs.\\n\\n| Relative RT60 Error (%) | 1 Thread (FPS) | 5 Threads (FPS) |\\n|-------------------------|----------------|-----------------|\\n| High-quality            | 0.0 \u00b1 0.0      | 0.9 \u00b1 0.0       |\\n| High-speed              | 9.5 \u00b1 0.2      | 7.7 \u00b1 0.2       |\\n\\n4 Large-scale SoundSpaces-PanoIR Dataset\\n\\nWe are releasing SoundSpaces 2.0 as a general-purpose platform with which a user can generate observations on-the-fly (particularly relevant for embodied AI models), or populate a new offline dataset of their own design. As an example of the latter, and to ease adoption for researchers who wish to work with visual-acoustic scene data without a layer of agent interaction, we next use SoundSpaces 2.0 to compose a large-scale dataset called SoundSpaces-PanoIR that couples IRs with images.\\n\\nFor visual-acoustic learning tasks, such as audio-visual dereverberation [16] and synthesizing acoustics based on visuals [13, 67, 44], there are no existing large-scale accurate image-IR datasets due to the high expense and complexity of data collection. Our SoundSpaces-PanoIR dataset has 10M panoramic image and IR pairs rendered from 750 environments across the Matterport3D, Gibson, and HM3D datasets. We provide the data in the following format: panorama (RGB/Depth), IR, polar coordinates of the source with respect of the center of the panorama. Fig. 2 shows one example in Gibson. See Supp. for more examples and statistics.\\n\\n5 Evaluation and Benchmarks\\n\\nNext we evaluate both the simulation quality and its value for downstream tasks with two machine learning benchmarks. Fig. 3a illustrates these two tasks.\\n\\n5.1 Simulation Speed vs. Quality Tradeoff\\n\\nTo understand the tradeoff between the quality versus speed of rendering, we report the accuracy and speed of different modes by rendering RIRs along random trajectories with an average length of 15m across 20 Matterport3D environments. We profile the speed on a Xeon(R) Gold 6230 CPU with 2.10GHz. See Table 2. For accuracy, we measure the relative RT60 error of RIRs generated in high-speed mode compared to RIRs generated in high-quality mode. RT60 is a standard acoustic measurement that is defined as the time it takes for the sound pressure level to reduce by 60 dB [29].\\n\\nWe see high-speed greatly improves efficiency over the high-quality mode, by 8\u00d7 with single thread and 33\u00d7 with 5 threads, while only losing 9.5% accuracy despite RT60 calculation being noisy. When coupled with distributed training, it meets the requirement of today's RL agent training. In addition, we test the navigation model trained in high-speed mode on high-quality mode; the performance difference is smaller than 1% compared to the test performance in high-speed mode in Table 3. In comparison, TDW [21] runs at 60 FPS and SoundSpaces runs at 500+ FPS (bottleneck on I/O) at the cost of simplified room models or not being configurable, respectively, c.f. Table 1. We treat high-quality mode as the gold-standard and benchmark its quality against real-world IRs next.\\n\\n5.2 Validating Simulation Accuracy with Real IRs\\n\\nHow realistic are our audio simulations? To quantify this, we collect real acoustic measurements of the FRL apartment from the Replica dataset [68] and compare them to SoundSpaces 2.0 outputs. IR measurements were captured at seven different source/receiver positions throughout the real-world apartment using an omnidirectional B&K Type 4295 speaker (100Hz to 8kHz frequency response) and Earthworks M30 microphone with the exponential sine sweep method. These measurements are publicly available to assist future research.\\n\\nFigure 3b compares the measurements to the corresponding simulations at the same source/receiver positions, for both the original SoundSpaces and the proposed SoundSpaces 2.0 (high-quality mode). The measurements were scaled to match the direct sound level of the simulations. The acoustic material properties of the mesh were optimized to match the measurements following [62].\"}"}
{"id": "ChWo6qLgILf", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this example, a person's phone rings in the dining room while she is in the living room and she asks the robot to bring her the phone. Upon receiving the audio signal with the binaural microphone, the robot needs to figure out two things: 1) what she is saying (far-field automatic speech recognition) and 2) how to navigate to her and the phone (audio-visual navigation). Note that far-field ASR is not limited to robotics; it has various applications such as video captioning.\\n\\nComparing real measurements and simulations in the Replica apartment [68] for 7 measurement positions and the 250Hz to 4000Hz frequency band. SoundSpaces 2.0 has much lower error for the direct-to-reverberant ratio (DRR) compared to SoundSpaces.\\n\\nEnergy decay curve comparisons. The energy decay curve of SoundSpaces 2.0 is much closer to the real measurements than SoundSpaces.\\n\\nTable 3: Continuous audio-visual navigation benchmark. DTG stands for distance to goal. We report the mean and standard deviation by training on 1 random seed, and evaluating on 3 random seeds.\\n\\n|                | Train SPL (%) | Test SPL (%) | Train DTG (m) | Test DTG (m) |\\n|----------------|---------------|--------------|---------------|--------------|\\n| SoundSpaces    | 64.2 \u00b1 0.8    | 27.5 \u00b1 0.4   | 27.5 \u00b1 0.4    | 5.6 \u00b1 0.2    |\\n| SoundSpaces    | 64.7 \u00b1 3.9    | 49.3 \u00b1 3.0   | 49.3 \u00b1 3.0    | 5.9 \u00b1 0.5    |\\n\\nWe report the direct-to-reverberant ratio (DRR) acoustic parameters derived from the impulse responses [29] in Figure 3b. SoundSpaces 2.0 has a better match of direct-to-reverberant ratio, where the error compared to measurements is reduced from 11.0 dB to 0.98 dB on average, while preserving the same relative RT60 error of 12.4% (see Supp.). Figure 3c reinforces that advantage, plotting the energy-time curves of the simulations versus the real measurements from 250Hz to 4000Hz. Overall, the proposed new features and improvements lead to higher realism for the acoustic simulation.\\n\\n5.3 Benchmark 1: Continuous Audio-Visual Navigation\\n\\nNavigating to localize the sound source in an unmapped environment has many real world applications, such as rescue robot or service robot (e.g., find the person calling for help or the ringing phone). The audio-visual navigation task (AV-Nav), originally introduced in [14], is gaining attention from the broader community via public competitions at CVPR 2021 and CVPR 2022.\\n\\nHowever, due to its reliance on SoundSpaces, AV-Nav thus far must assume the agent travels along the discrete grid. The navigation task is thus easier due to the lack of collisions and implied perfect localization.\\n\\nHere we introduce the continuous AV-Nav task, enabled by SoundSpaces 2.0 simulation. In this task, the agent can either move forward 0.15 m per step at a speed of 1m/s or turn left/right 10 degrees. If the agent issues a stop action within 1m radius of the goal, the episode is regarded as successful. Importantly, the agent not only moves in continuous space but also receives acoustically continuous audio signals (cf. Sec. 3.2). We use the high-speed rendering mode.\\n\\nWe generalize the existing audio-visual navigation (AV-Nav) agent [14] to a distributed audio-visual navigation (DAV-Nav) agent equipped with DD-PPO [77] to speed up the training process. We train and test on the AudioGoal navigation dataset [14]. To ablate the simulation improvement as detailed in Sec. 3.1, for the SoundSpaces baseline, we train DAV-Nav on SoundSpaces' discrete setup (agent only moving between grid points) with data rendered from the enhanced simulation; the action space is either moving forward 1 m, turning left/right 90 degrees or issuing a stop action.\"}"}
{"id": "ChWo6qLgILf", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Far-field automatic speech recognition benchmark.\\n\\n|                      | Word Error Rate (%) |\\n|----------------------|---------------------|\\n| Pretrained           | 29.10               |\\n| Finetuned on real IRs [38] | 13.32               |\\n| Finetuned on Pyroomacoustics [61] | 16.24               |\\n| Finetuned on SoundSpaces 1.0 [14] | 18.48               |\\n| Finetuned on SoundSpaces 2.0 | 12.48               |\\n\\nTable 3 shows the results using the standard metrics of success rate, success rate normalized by path length (SPL), and distance to goal. If only the space is continuous, the DA V-Nav agent trained on SoundSpaces has 64.2% success rate and 27.5% SPL on average compared to 64.7% success rate and 49.3% SPL of the agent trained on SoundSpaces 2.0. This shows spatial continuity mostly harms the agent\u2019s efficiency rather than its success rate; the agent can still navigate to the source despite having more collisions. However, when the sound is acoustically continuous, the baseline\u2019s performance drops. This is likely because the agent relies on the direct-sound cue that is (inaccurately) always present in the audio, while in the continuous-sound rendering, direct sound is always mixed with the reverberation in the environment, making navigation more difficult. In comparison, the agent trained on SoundSpaces 2.0 achieves a much higher success rate and is much closer to the goal location on average. This shows it is essential to model both spatial and acoustic continuity for AV-Nav, which SoundSpaces 2.0 enables. Furthermore, recall that SoundSpaces 2.0 opens up any other 3D scene dataset for exploring AV-Nav, whereas previously only Replica or Matterport3D were applicable.\\n\\n5.4 Benchmark 2: Far-Field Automatic Speech Recognition\\n\\nSpeech recognition is critical for many applications, including far-field scenarios where the speaker is far from the microphone (e.g., speaking to a smart home assistant device). When speech recognition models are trained on a clean speech corpus, such as LibriSpeech [54], they generalize poorly to far-field cases with unanticipated reverberation. Due to the high expense of collecting real IRs, synthetic impulse responses are thus often used to augment speech for far-field ASR [38, 47, 71]. Here, we propose to benchmark far-field ASR systems augmented by our generated impulse responses.\\n\\nWe take the pretrained transformer-based ASR system from SpeechBrain [58], an open-sourced speech toolkit, as the base model. For finetuning, we augment speech in the train-clean-100 split of LibriSpeech [54] with IRs generated in different systems and finetune 60 epochs. For testing, we augment speech from a real RIR dataset [69], where IRs are recorded in real environments, e.g., home, conference rooms, auditoriums. In this way, we test the sim2real generalization for models trained on the synthetic data. We compare the pretrained model with the ASR model finetuned on IRs generated with Pyroomacoustics, SoundSpaces 1.0, and SoundSpaces 2.0 (high-quality mode). We ensured the simulated RIRs have matching RT60 distributions. In addition, we compare with the ASR model finetuned on real IRs [38] from the RWCP sound scene database [52], the 2014 REVERB challenge database [37], and the Aachen impulse response database (AIR) [32].\\n\\nTable 4 shows the results. As we can see, the pretrained model generalizes poorly to far-field speech with word error rate (WER) of 29.1%, compared to 2.4% WER on a clean test set lacking any reverberation. Finetuning with synthetic IRs leads to a dramatic improvement. Comparing Pyroomacoustics and SoundSpaces 1.0 to SoundSpaces 2.0, our generated IRs lead to much lower WER. Finetuning on real IRs also reduces the error substantially, but still not as much as our simulated data, which can be generated at scale across a wide variety of environments. Our simulation generates realistic IRs that help machine learning models generalize better to reality.\\n\\nAcoustic randomization.\\n\\nIn the real world, instances of a given object category need not share identical material profiles. While existing simulations do not model such nuances, in SoundSpaces 2.0 we can manipulate the materials in a more subtle way. Inspired by domain randomization techniques [73, 74] that randomize simulation parameters for better sim2real generalization, we explore if acoustic randomization offers similar benefits. Specifically, we define a set of possible acoustic materials for each object category. When rendering, a random material is picked for a category to simulate the category-level variation. In addition, to model the differences of acoustic...\"}"}
{"id": "ChWo6qLgILf", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We add $N(0, 0.1)$ Gaussian noise to each coefficient. Altogether, this strategy models both the category-level and instance-level material nuances.\\n\\nWhen we use the proposed acoustic randomization technique to generate the same amount of data for fine-tuning, the ASR model has even lower WER on the test set, reduced from 12.48% to 12.04%, while uniform randomization, i.e., uniformly sampling coefficients between 0 and 1, leads to a higher WER of 12.58%. This not only shows the benefit of acoustic randomization but also how SoundSpaces 2.0\u2019s configurability facilitates research on acoustic sim2real.\\n\\n6 Discussion on Limitations and Future Work\\n\\nWe believe SoundSpaces 2.0 can facilitate significant new work in embodied AI, multimodal perception, and audio research. The platform is general and accessible, and our experiments offer concrete examples of its potential.\\n\\nLike any research tool, there are certain limitations and assumptions that are important to recognize. Our simulation platform supports audio rendering for arbitrary environments with a state-of-the-art path tracing algorithm. For this algorithm to render accurately, the scene meshes need to have high quality, i.e., no large open holes on the mesh, otherwise the rays will leak from the holes, resulting in inaccurate simulation. For example, Section 5 in Supp. shows Matteport3D has lower RT60s compared to other datasets on average due to some of its broken mesh environments. To aid users in checking the mesh quality for audio rendering, we expose an API to let users check the percentage of rays leaked from the mesh; users can repair the mesh accordingly if the ray efficiency is low.\\n\\nPath tracing is also vulnerable to the standard shortcomings of geometrical-acoustics techniques, e.g., room modes, though our implementation takes care to eliminate the typical lack of diffraction as described in Sec. 3.\\n\\nMaterials have impact on audio simulation, and one of the open challenges of material modeling is that it is infeasible to accurately know the acoustic material properties only given the environment meshes, e.g., we cannot estimate how much energy the floor absorbs purely based on the mesh or rendered visuals. Currently, we tackle that by assigning common material properties to objects (Sec. 3.3), which allows our simulator to operate with fairly lightweight assumptions about the incoming mesh. For more in-depth treatment of materials, one could perform acoustic measurements into the environment scanning pipeline when creating a digital replica of a real-world environment.\\n\\nIn this work, we validate the simulation accuracy with real IRs collected in the apartment from the Replica dataset. To improve the simulation accuracy and further understand its difference with the real world, future work could collect acoustic measurements in diverse environments with varying geometry and materials, which is supported by our simulation platform (Sec. 3.4).\\n\\n7 Conclusion\\n\\nWe introduced SoundSpaces 2.0, a platform for on-the-fly geometry-based audio rendering for 3D environments. We collected real measurements in public 3D scenes to validate the simulation accuracy. We benchmarked two tasks and showed encouraging evidence that systems trained on this simulation can generalize to the real world. Notably, beyond those two tasks, our platform will support and upgrade many other tasks currently explored in the literature [12, 17, 44, 31, 55, 67].\\n\\nLastly, we are releasing a large-scale SoundSpaces-PanoIR dataset that is ready to use for research. We have made the simulation and real measurements publicly available to facilitate research on visual-acoustic learning.\\n\\nAcknowledgements: UT Austin is supported in part by a gift from Google, DARPA L2M, and the IFML NSF AI Institute.\"}"}
{"id": "ChWo6qLgILf", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Embodied AI workshop challenges. [1](https://embodied-ai.org).\\n\\n[2] Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. The conversation: Deep audio-visual speech enhancement. In *INTERSPEECH*, 2018.\\n\\n[3] Andrew Allen and Nikunj Raghuvanshi. Aerophones in flatland: Interactive wave simulation of wind instruments. *ACM Transactions on Graphics (TOG)* 34, 4 (2015), 1\u201311.\\n\\n[4] Jont B. Allen and David A. Berkley. Image method for efficiently simulating small-room acoustics. *J. Acoust. Soc. Am.*, vol. 65, no. 4, pp. 943\u2013950, 1979.\\n\\n[5] Humam Alwassel, Dhruv Mahajan, Bruno Korbar, Lorenzo Torresani, Bernard Ghanem, and Du Tran. Self-supervised learning by cross-modal audio-video clustering. In *NeurIPS*, 2020.\\n\\n[6] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2018.\\n\\n[7] Rohith Aralikatti, Anton Ratnarajah, Zhenyu Tang, and Dinesh Manocha. Improving reverberant speech separation with synthetic room impulse responses. *IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)*. IEEE, 900\u2013906, 2021.\\n\\n[8] HE Bass, LC Sutherland, and AJ Zuckerwar. Atmospheric absorption of sound: Update. *The Journal of the Acoustical Society of America*, 88(4):2019\u20132021, 1990.\\n\\n[9] Stefan Bilbao, Charlotte Desvages, Michele Ducceschi, Brian Hamilton, Reginald Harrison-Harsley, Alberto Torin, and Craig Webb. Physical Modeling, Algorithms, and Sound Synthesis: The NESS Project. *MIT Press*, 2020.\\n\\n[10] Chunxiao Cao, Zhong Ren, Carl Schissler, Dinesh Manocha, and Kun Zhou. Interactive sound propagation with bidirectional path tracing. *ACM Transactions on Graphics (TOG)*, 35(6):1\u201311, 2016.\\n\\n[11] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. *3DV*, 2017. MatterPort3D dataset license available at: [http://kaldir.vc.in.tum.de/matterport/MP_TOS.pdf](http://kaldir.vc.in.tum.de/matterport/MP_TOS.pdf).\\n\\n[12] Changan Chen, Ziad Al-Halah, and Kristen Grauman. Semantic audio-visual navigation. In *CVPR*, 2021.\\n\\n[13] Changan Chen, Ruohan Gao, Paul Calamia, and Kristen Grauman. Visual acoustic matching. In *CVPR*, 2022.\\n\\n[14] Changan Chen, Unnat Jain, Carl Schissler, Sebastia Vicenc Amengual Gari, Ziad Al-Halah, Vamsi Krishna Ithapu, Philip Robinson, and Kristen Grauman. SoundSpaces: Audio-visual navigation in 3d environments. In *ECCV*, 2020.\\n\\n[15] Changan Chen, Sagnik Majumder, Ziad Al-Halah, Ruohan Gao, Santhosh Kumar Ramakrishnan, and Kristen Grauman. Learning to set waypoints for audio-visual navigation. In *ICLR*, 2021.\\n\\n[16] Changan Chen, Wei Sun, David Harwath, and Kristen Grauman. Learning audio-visual dereverberation. *arXiv:2106.07732*, 2021.\\n\\n[17] Victoria Dean, Shubham Tulsiani, and Abhinav Gupta. See, hear, explore: Curiosity via audio-visual association. In *NeurIPS*, 2020.\\n\\n[18] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T Freeman, and Michael Rubinstein. Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation. In *SIGGRAPH*, 2018.\\n\\n[19] Thomas Funkhouser, Ingrid Carlbom, Gary Elko, Gopal Pingali, Mohan Sondhi, and Jim West. A beam tracing approach to acoustic modeling for interactive virtual environments. In *Proceedings of the 25th annual conference on Computer graphics and interactive techniques*, 1998.\\n\\n[20] Chuang Gan*, Yi Gu*, Siyuan Zhou, Jeremy Schwartz, Seth Alter, James Traer, Dan Gutfreund, Joshua B. Tenenbaum, Josh McDermott*, and Antonio Torralba*. Finding fallen objects via asynchronous audio-visual integration. In *CVPR*, 2022.\"}"}
{"id": "ChWo6qLgILf", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano, Kuno Kim, Elias Wang, Damian Mrowca, Michael Lingelbach, Aidan Curtis, Kevin T. Feigelis, Daniel M. Bear, Dan Gutfreund, David D. Cox, James J. DiCarlo, Josh H. McDermott, Joshua B. Tenenbaum, and Daniel L. K. Yamins. Threedworld: A platform for interactive multi-modal physical simulation. In NeurIPS Track on Datasets and Benchmarks, 2021.\\n\\nChuang Gan, Yiwei Zhang, Jiajun Wu, Boqing Gong, and Joshua B Tenenbaum. Look, listen, and act: Towards audio-visual embodied navigation. In ICRA, 2020.\\n\\nRuohan Gao, Changan Chen, Ziad Al-Halah, Carl Schissler, and Kristen Grauman. Visualechoes: Spatial image representation learning through echolocation. In ECCV, 2020.\\n\\nRuohan Gao, Zilin Si, Yen-Yu Chang, Samuel Clarke, Jeannette Bohg, Li Fei-Fei, Wenzhen Yuan, and Jiajun Wu. Objectfolder 2.0: A multisensory object dataset for sim2real transfer. In CVPR, 2022.\\n\\nKristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Christian Fuegen, Abraham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the world in 3,000 hours of egocentric video. In CVPR, 2022.\\n\\nPierre-Amaury Grumiaux, Srdan Kitic, Laurent Girin, , and Alexandre Gu\u00e9rin. A survey of sound source localization with deep learning methods. CoRR abs/2109.03465 (2021).\\n\\nJen-Cheng Hou, Syu-Siang Wang, Ying-Hui Lai, Yu Tsao, Hsiu-Wen Chang, and Hsin-Min Wang. Audio-visual speech enhancement using multimodal deep convolutional neural networks. In IEEE Transactions on Emerging Topics in Computational Intelligence, 2017.\\n\\nDi Hu, Rui Qian, Minyue Jiang, Xiao Tan, Shilei Wen, Errui Ding, Weiyao Lin, and Dejing Dou. Discriminative sounding objects localization via self-supervised audiovisual matching. In NeurIPS, 2020.\\n\\nISO. ISO 3382, Acoustics\u2014Measurement of room acoustic parameters. International Organization, (3382), 2012.\\n\\nUnnat Jain, Luca Weihs, Eric Kolve, Mohammad Rastegari, Svetlana Lazebnik, Ali Farhadi, Alexander G. Schwing, and Aniruddha Kembhavi. Two body problem: Collaborative visual task completion. In CVPR, 2019. first two authors contributed equally.\\n\\nTeerapat Jenrungrot, Vivek Jayaram, Steve Seitz, and Ira Kemelmacher-Shlizerman. The cone of silence: Speech separation by localization. In Advances in Neural Information Processing Systems, 2020.\\n\\nMarco Jeub, Magnus Schafer, and Peter Vary. A binaural room impulse response database for the evaluation of dereverberation algorithms. In 2009 16th International Conference on Digital Signal Processing, pages 1\u20135, 2009.\\n\\nShulei Ji, Jing Luo, and Xinyu Yang. A comprehensive survey on deep music generation: Multi-level representations, algorithms, evaluations, and future directions. arXiv preprint arXiv:2011.06801, 2020.\\n\\nMohit Bansal Jialu Li, Hao Tan. Envedit: Environment editing for vision-and-language navigation. In CVPR, 2022.\\n\\nIII Julius O. Smith. Physical modeling using digital waveguides. Computer Music Journal Vol. 16, No. 4 (Winter, 1992), pp. 74-91 (18 pages), 1992.\\n\\nAbhishek Kadian, Joanne Truong, Aaron Gokaslan, Alexander Clegg, Erik Wijmans, Stefan Lee, Manolis Savva, Sonia Chernova, and Dhruv Batra. Sim2real predictivity: Does evaluation in simulation predict real-world performance? In RA-L, 2020.\"}"}
{"id": "ChWo6qLgILf", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Keisuke Kinoshita, Marc Delcroix, Sharon Gannot, Emanu\u00ebl Habets, Reinhold Haeb-Umbach, Walter Kellermann, Volker Leutnant, Roland Maas, Tomohiro Nakatani, Bhiksha Raj, Armin Sehr, and Takuya Yoshioka. A summary of the reverb challenge: State-of-the-art and remaining challenges in reverberant speech processing research. EURASIP Journal on Advances in Signal Processing, 2016.\\n\\nTom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L. Seltzer, and Sanjeev Khudanpur. A study on data augmentation of reverberant speech for robust speech recognition. In ICASSP, 2017.\\n\\nEric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv, 2017.\\n\\nBruno Korbar, Du Tran, and Lorenzo Torresani. Cooperative learning of audio and video models from self-supervised synchronization. In NeurIPS, 2018.\\n\\nKristinn Kristinsson and Guy Albert Dumont. System identification and control using genetic algorithms. IEEE Transactions on Systems, Man, and Cybernetics, 22(5), 1992.\\n\\nAsbj\u00f8rn Krokstad, S Strom, and Svein S\u00f8rsdal. Calculating the acoustical room response by the use of a ray tracing technique. Journal of Sound and Vibration 8, 1 (1968), 118\u2013125.\\n\\nShiguang Liu and Dinesh Manocha. Sound synthesis, propagation, and rendering: A survey. arxiv, 2020.\\n\\nAndrew Luo, Yilun Du, Michael J Tarr, Joshua B Tenenbaum, Antonio Torralba, and Chuang Gan. Learning neural acoustic fields. In NeurIPS 2022, 2022.\\n\\nSagnik Majumder, Ziad Al-Halah, and Kristen Grauman. Move2hear: Active audio-visual source separation. In ICCV, 2021.\\n\\nSagnik Majumder, Changan Chen, Ziad Al-Halah, and Kristen Grauman. Few-shot audio-visual learning of environment acoustics. 2022.\\n\\nMishaim Malik, Muhammad Kamran Malik, Khawar Mehmood, and Imran Makhdoom. Automatic speech recognition: a survey. 2021.\\n\\nDaniel Michelsanti, Zheng-Hua Tan, Shi-Xiong Zhang, Yong Xu, Meng Yu, Dong Yu, and Jesper Jensen. An overview of deep-learning-based audio-visual speech enhancement and separation. In arXiv, 2020.\\n\\nPedro Morgado, Yi Li, and Nuno Vasconcelos. Learning representations from audio-visual spatial alignment. In NeurIPS, 2020.\\n\\nD.T. Murphy, Antti Kelloniemi, Jack Mullen, and Simon Shelley. Acoustic modeling using the digital waveguide mesh. Signal Processing Magazine, IEEE, 24:55 \u2013 66, 04 2007.\\n\\nChristian M\u00fcller-Tomfelde. Time-varying filter in non-uniform block convolution. In Proceedings of the COST G-6 Conference on Digital Audio Effects (DAFX-01), 2001.\\n\\nSatoshi Nakamura, Kazuo Hiyane, Futoshi Asano, Takanobu Nishiura, and Takeshi Yamada. Acoustical sound database in real environments for sound scene understanding and hands-free speech recognition. In LREC, 2000.\\n\\nAndrew Owens and Alexei A Efros. Audio-visual scene analysis with self-supervised multisensory features. In ECCV, 2018.\\n\\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 5206\u20135210. IEEE, 2015.\\n\\nSenthil Purushwalkam, Sebastian Vicenc Amengual Gari, Vamsi Krishna Ithapu, Carl Schissler, Philip Robinson, Abhinav Gupta, and Kristen Grauman. Audio-visual floorplan reconstruction. In ICCV, 2021.\\n\\nSanthosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg, John M Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, Manolis Savva, Yili Zhao, and Dhruv Batra. Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments for embodied AI. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.\\n\\nRam Ramrakhya, Eric Undersander, Dhruv Batra, and Abhishek Das. Habitat-web: Learning embodied object-search strategies from human demonstrations at scale. In CVPR, 2022.\"}"}
{"id": "ChWo6qLgILf", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku Rouhe, Samuele Cornell, Loren Lugosch, Cem Subakan, Nauman Dawalatabad, Abdelwahab Heba, Jianyuan Zhong, Ju-Chieh Chou, Sung-Lin Yeh, Szu-Wei Fu, Chien-Feng Liao, Elena Rastorgueva, Fran\u00e7ois Grondin, William Aris, Hwidong Na, Yan Gao, Renato De Mori, and Yoshua Bengio. SpeechBrain: A general-purpose speech toolkit, 2021. arXiv:2106.04624.\\n\\nLauri Savioja and U Peter Svensson. Overview of geometrical room acoustic modeling techniques. The Journal of the Acoustical Society of America, 138(2):708\u2013730, 2015.\\n\\nManolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A Platform for Embodied AI Research. In ICCV, 2019.\\n\\nRobin Scheibler, Eric Bezzam, and Ivan Dokmani\u00b4c. Pyroomacoustics: A python package for audio room simulations and array processing algorithms. arXiv, 2017.\\n\\nCarl Schissler, Christian Loftin, and Dinesh Manocha. Acoustic classification and optimization for multi-modal rendering of real-world scenes. IEEE transactions on visualization and computer graphics, 24(3):1246\u20131259, 2017.\\n\\nCarl Schissler and Dinesh Manocha. Adaptive impulse response modeling for interactive sound propagation. In Proceedings of the 20th ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, pages 71\u201378, 2016.\\n\\nCarl Schissler, Ravish Mehra, and Dinesh Manocha. High-order diffraction and diffuse reflections for interactive sound propagation in large environments. ACM Transactions on Graphics (TOG) 33, 4 (2014), 39.\\n\\nCarl Schissler, Gregor M\u00fcckl, and Paul Calamia. Fast diffraction pathfinding for dynamic sound propagation. ACM Transactions on Graphics (TOG), 40(4):1\u201313, 2021.\\n\\nCarl Schissler, Peter Stirling, and Ravish Mehra. Efficient construction of the spatial room impulse response. In 2017 IEEE Virtual Reality (VR), pages 122\u2013130. IEEE, 2017.\\n\\nNikhil Singh, Jeff Mentch, Jerry Ng, Matthew Beveridge, and Iddo Drori. Image2reverb: Cross-modal reverb impulse response synthesis. In ICCV, 2021.\\n\\nJulian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, et al. The Replica dataset: A digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2019.\\n\\nMIgor Szoke, Miroslav Skacel, Ladislav Mosner, Jakub Paliesek, and Jan \u201cHonza\u201d Cernocky. Building and evaluation of a real room impulse response dataset. arXiv, 2018.\\n\\nAndrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir VonDrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to rearrange their habitat. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\\n\\nZhenyu Tang, Rohith Aralikatti, Anton Ratnarajah, , and Dinesh Manocha. Gwa: A large geometric-wave acoustic dataset for audio processing. In Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings (SIGGRAPH '22 Conference Proceedings), 2022.\\n\\nZhenyu Tang, Nicholas J Bryan, Dingzeyu Li, Timothy R Langlois, and Dinesh Manocha. Scene-aware audio rendering via deep acoustic analysis. IEEE Transactions on Visualization and Computer Graphics (2020), 2020.\\n\\nJosh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In IROS, 2017.\\n\\nJuliano Vacaro, Guilherme Marques, Bruna Oliveira, Gabriel Paz, Thomas Paula, Wagston Staehler, and David Murphy. Sim-to-real in reinforcement learning for everyone. LARS-SBR-WRE, 2019.\\n\\nMichael Vorl\u00e4nder. Auralization. Germany: Springer International Publishing, 2020.\\n\\nTomi Westerlund Wenshuai Zhao, Jorge Pe\u00f1a Queralta. Sim-to-real transfer in deep reinforcement learning for robotics: a survey. arxiv, 2020.\"}"}
{"id": "ChWo6qLgILf", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra. DD-PPO: Learning near-perfect PointGoal navigators from 2.5 billion frames. In ICLR, 2020.\\n\\nFei Xia, Amir R. Zamir, Zhi-Yang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson Env: real-world perception for embodied agents. In Computer Vision and Pattern Recognition (CVPR), 2018 IEEE Conference on. IEEE, 2018.\\n\\nYinfeng Yu, Wenbing Huang, Fuchun Sun, Changan Chen, Yikai Wang, and Xiaohong Liu. Sound adversarial audio-visual navigation. In ICLR, 2022.\\n\\nHang Zhao, Chuang Gan, Wei-Chiu Ma, and Antonio Torralba. The sound of motions. In ICCV, 2019.\\n\\nHang Zhou, Ziwei Liu, Xudong Xu, Ping Luo, and Xiaogang Wang. Vision-infused deep audio inpainting. In ICCV, 2019.\\n\\nFuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 2020.\\n\\nChecklist\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n   (b) Did you describe the limitations of your work? [Yes] See discussion on limitations in Section 6.\\n   (c) Did you discuss any potential negative societal impacts of your work? [Yes] See discussion on societal impacts in the supplemental material.\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n   (b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes]\\n   The binaries of RLR-Audio-Propagation are released at https://github.com/facebookresearch/rlr-audio-propagation.\\n   The integration with Habitat-Sim is available at https://github.com/facebookresearch/habitat-sim/blob/main/docs/AUDIO.md.\\n   The high-level APIs for tasks and training scripts are available at https://github.com/facebookresearch/sound-spaces. Instructions for reproducing the results are included in the supplemental material.\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] They are specified in the supplemental material.\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] We report error bars in Table 1 and 2.\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] They are specified in the supplemental material.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes] We used and cited the Matterport3D dataset.\\n   (b) Did you mention the license of the assets? [Yes] The license for Matterport3D is included in the reference.\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] The SoundSpaces-PanoIR dataset and real measurements for Replica dataset is available at the SoundSpaces repo.\"}"}
{"id": "ChWo6qLgILf", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]\\n\\n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\\n\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\"}"}
