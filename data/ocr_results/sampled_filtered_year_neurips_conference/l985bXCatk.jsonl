{"id": "l985bXCatk", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LRVS-Fashion: Extending Visual Search with Referring Instructions\\n\\nSimon Lepage\\nJ\u00e9r\u00e9mie Mary\\nDavid Picard\\n\\n1 CRITEO AI Lab, Paris, France\\n2 LIGM, \u00c9cole des Ponts, Marne-la-Vall\u00e9e, France\\n{s.lepage, j.mary}@criteo.com david.picard@enpc.fr\\n\\nAbstract\\nThis paper introduces a new challenge for image similarity search in the context of fashion, addressing the inherent ambiguity in this domain stemming from complex images. We present Referred Visual Search (RVS), a task allowing users to define more precisely the desired similarity, following recent interest in the industry. We release a new large public dataset, LRVS-Fashion, consisting of 272k fashion products with 842k images extracted from fashion catalogs, designed explicitly for this task. However, unlike traditional visual search methods in the industry, we demonstrate that superior performance can be achieved by bypassing explicit object detection and adopting weakly-supervised conditional contrastive learning on image tuples. Our method is lightweight and demonstrates robustness, reaching Recall at one superior to strong detection-based baselines against 2M distractors.\\n\\nRetrieved Items\\n\\nQuery\\nCategorical Conditioning\\nLOWER BODY OUTWEAR\\nTextual Conditioning\\n\\\"Same handbag\\\" \\\"I want her t-shirt\\\"\\n\\nFigure 1: Overview of the Referred Visual Search task. Given a query image and conditioning information, the goal is to retrieve a target instance from a large gallery. Note that a query is made of an image and an additional text or category, precisiong what aspect of the image is relevant.\\n\\n1 Introduction\\n\\nImage embeddings generated by deep neural networks play a crucial role in a wide range of computer vision tasks. Image retrieval has gained substantial prominence, leading to the development of dedicated vector database systems [22]. These systems facilitate efficient retrieval by comparing embedding values and identifying the most similar images within the database. Image similarity search in the context of fashion presents a unique challenge due to the inherently ill-founded nature of the problem. The primary issue arises from the fact that two images can be considered similar in various ways, leading to ambiguity in defining a single similarity metric. For\\n\\n1 The dataset is available at https://huggingface.co/datasets/Slep/LAION-RVS-Fashion\\n\\nSubmitted to the 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks. Do not distribute.\"}"}
{"id": "l985bXCatk", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In practice, object selection in complex scenes is classically tackled using object detection and crops \\\\cite{21,17,12,42}. Some recent approaches use categorical attributes \\\\cite{8} or text instead \\\\cite{6}, and automatically crop the image based on learned attention to input attributes. It is also possible to ask the user to perform the crop himself, yet in all the situations the performance of the retrieval will be sensitive to this extraction step making it costly to build a generic retrieval tool. Recently, Jiao et al. \\\\cite{20} went a step further, incorporating prior knowledge about the taxonomy of fashion attributes and classes without using crops. They use a multi-granularity loss and two sub-networks to learn attribute and class-specific representations, resulting in improved robustness for fashion retrieval, yet without providing any code.\\n\\nIn this work, we seek to support these efforts by providing a dataset dedicated to RVS. We extracted a subset of LAION 5B \\\\cite{41} focused on pairs of images sharing a labeled similarity in the domain of fashion, and propose a method to eliminate the need for explicit detection or segmentation, while still producing similarities in the embedding space specific to the conditioning. We think that such end-to-end approach has the potential to be more generalizable and robust, whereas localization-dependent approaches hinge on multi-stage processing heuristics specific to the dataset.\\n\\nThis paper presents two contributions to the emerging field of Referred Visual Search, aiming at defining image similarity based on conditioning information.\\n\\n1. The introduction of a new dataset, referred to as LRVS-Fashion, which is derived from the LAION-5B dataset and comprises 272k fashion products with nearly 842k images. This dataset features a test set with an addition of more than 2M distractors, enabling the evaluation of method robustness in relation to gallery size. The dataset's pairs and additional metadata are designed to necessitate the extraction of particular features from complex images.\\n\\n2. An innovative method for learning to extract referred embeddings using weakly-supervised training. Our approach demonstrates superior accuracy against a strong detection-based baseline and existing published work. Furthermore, our method exhibits robustness against a large number of distractors, maintaining high R @ 1 even when increasing the number of distractors to 2M.\\n\\n### Related Work\\n\\n#### Retrieval Datasets.\\n\\nStandard datasets in metric learning literature consider that the images are object-centric, and focus on single salient objects \\\\cite{49,25,45}. In the fashion domain there exist multiple datasets dedicated to product retrieval, with paired images depicting the same product and additional labeled attributes. A recurrent focus of such datasets is cross-domain retrieval, where the goal is to retrieve images of a given product taken in different situations, for example consumer-to-shop \\\\cite{31,50,32,12}, or studio-to-shop \\\\cite{32,27}. The domain gap is in itself a challenge, with issues stemming from irregular lighting, occlusions, viewpoints, or distracting backgrounds. However, the query domain (consumer images for example) often contains scenes with multiple objects, making queries ambiguous. This issue has been circumvented with the use of object detectors and landmarks detectors \\\\cite{23,18,32,12}. Some are not accessible anymore \\\\cite{23,32,50}.\\n\\nWith more than 272k distinct training product identities captured in multi-instance scenes, our new dataset proposes an exact matching task similar to the private Zalando dataset \\\\cite{27}, while being larger than existing fashion retrieval datasets and publicly available. We also create an opportunity for new multi-modal approaches, with captions referring to the product of interest in each complex image, and for robustness to gallery size with 2M added distractors at test time.\"}"}
{"id": "l985bXCatk", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the last decade, content-based image retrieval has changed because of the arrival of deep learning, which replaced many handcrafted heuristics (keypoint extraction, descriptors, geometric matching, re-ranking...). In the industry, this technology has been of interest to retail companies and search engines to develop visual search solutions, with new challenges stemming from the large scale of such databases. Initially using generic pretrained backbones to extract embeddings with minimal retraining, methods have evolved toward domain-specific embeddings supervised by semantic labels, and then multi-task domain-specific embeddings, leveraging additional product information. The latest developments in the field incorporate multi-modal features for text-image matching, with specific vision-language pretext tasks. However, these methods often consider that the query image is unambiguous, and often rely on a region proposal system to crop the initial image. In our work, we bypass this step and propose an end-to-end framework, leveraging the Transformer architecture to implicitly perform this detection step conditionally to the referring information.\\n\\nReferring Tasks. Referring tasks are popular in vision-language processing, in particular Referring Expression Comprehension and Segmentation, where a sentence designates an object in a scene, that the network has to localize. For the comprehension task (similar to open-vocabulary object detection), the goal is to output a bounding box. The segmentation task aims at producing an instance mask for images and recently videos. In this paper, we propose a referring expression task, where the goal is to embed the designated object of interest into a representation that can be used for retrieval. We explore the use of Grounding DINO and Segment Anything to create a strong baseline on our task.\\n\\nConditional Embeddings. Conditional similarity search has been studied through the retrieval process and the embedding process. On one hand, for the retrieval process, Hamilton et al. propose to use a dynamically pruned random projection tree. On the other hand, previous work in conditional visual similarity learning focused on attribute-specific retrieval, defining different similarity spaces depending on chosen discriminative attributes. They use either a mask applied on the features, or different projection heads, and require extensive data labeling. In Fashion, ASEN uses spatial and channel attention to an attribute embedding to extract specific features in a global branch. Dong et al. and Das et al. build upon this model and add a local branch working on an attention-based crop. Recently, Jiao et al. incorporated prior knowledge about fashion taxonomy in this process to create class-conditional embeddings based on known fine-grained attributes, using multiple attribute-conditional attention modules. In a different domain, Asai et al. tackle a conditional document retrieval task, where the user intent is made explicit by concatenating instructions to the query documents. In our work, we use Vision Transformers to implicitly pool features depending on the conditioning information, without relying on explicit ROI cropping or labeled fine-grained attributes.\\n\\nComposed Image Retrieval (CIR) is another retrieval task where the embedding of an image must be modified following a given instruction. Recent methods use a composer network after embedding the image and the modifying text. While CIR shares similarities with RVS in terms of inputs and outputs, it differs conceptually. Our task focuses on retrieving items based on depicted attributes and specifying a similarity computation method, rather than modifying the image. In Fashion, CIR has been extended to dialog-based interactive retrieval, where an image query is iteratively refined following user instructions.\\n\\n3 Dataset\\nMetric learning methods work by extracting features that pull together images labeled as similar. In our case, we wanted to create a dataset where this embedding has to focus on a specific object in a scene to succeed. We found such images in fashion, thanks to a standard practice in this field consisting in taking pictures of the products alone on neutral backgrounds, and worn by models in scenes involving other clothing items (see Fig. 3). We created LAION-RVS-Fashion (abbreviated LRVS-F) from LAION-5B by collecting images of products isolated and in context, which we respectively call simple and complex. We grouped them using extracted product identifiers. We also gathered and created a set of metadata to be used as...\"}"}
{"id": "l985bXCatk", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Overview of the data collection.\\n\\n- **a)** Selection of a subset of domains belonging to known fashion retailers.\\n- **b)** Extraction of product identifiers in the URLs using domain-specific regular expressions.\\n- **c)** Generation of synthetic metadata for the products (categories, captions, ...) using both pretrained and finetuned models.\\n- **d)** Deduplication of the images, and assignment to subsets.\\n\\n```\\n{ Images: Category: Lower Body  LAION Alt Text: Michael Kors 'Samantha' skinny trousers Caption: A women's beige trousers }\\n{ Images: Category: Bags  LAION Alt Text: BARK - striped tote 7 Caption: A handbag with navy stripes}\\n```\\n\\nFigure 3: Samples from LRVS-F. Each product is represented on at least a simple and a complex image, and is associated with a category. The simple images are also described by captions from LAION and BLIP2. Please refer to Appendix A.1 for more samples.\\n\\nThe process is depicted Fig. 2, presented in Section 3.1 with additional details in Appendix A.3.\\n\\n### 3.1 Construction\\n\\n#### Image Collection.\\n\\nThe URLs in LRVS-F are a subset of LAION-5B, curated from content delivery networks of fashion brands and retailers. By analyzing the URL structures we identified product identifiers, which we extracted with regular expressions to recreate groups of images depicting the same product. URLs without distinct identifiers or group membership were retained as distractors.\\n\\n#### Annotations.\\n\\nWe generated synthetic labels for the image complexity, the category of the product, and added new captions to replace the noisy LAION alt-texts. For the complexity labels, we employed active learning to incrementally train a classifier to discern between isolated objects on neutral backdrops and photoshoot scenes. The product categories were formed by aggregating various fine-grained apparel items into 10 coarse groupings. This categorization followed the same active learning protocol. Furthermore, the original LAION captions exhibited excessive noise, including partial translations or raw product identifiers. Therefore, we utilized BLIP-2 [29] to generate new, more descriptive captions.\\n\\n#### Dataset Split.\\n\\nWe grouped together images associated to the same product identifier and dropped the groups that did not have at least a simple and a complex image. We manually selected 400 of them for the validation set, and 2,000 for the test set. The distractors are all the images downloaded previously that were labeled as \\\"simple\\\" but not used in product groups. This mostly includes images for which it was impossible to extract any product identifier.\\n\\n#### Dataset Cleaning.\\n\\nIn order to mitigate false negatives in our results, we utilized Locality Sensitive Hashing and OpenCLIP ViT-B/16 embeddings to eliminate duplicates. Specifically, we removed duplicates between the test targets and test distractors, as well as between the validation targets and validation distractors. Throughout our experiments, we did not observe any false negatives in the results. However, there remains a small quantity of near-duplicates among the distractor images.\"}"}
{"id": "l985bXCatk", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 Composition\\n\\nIn total, we extracted 272,451 products for training, represented in 841,718 images. This represents 581,526 potential simple/complex positive pairs. We additionally extracted 400 products (800 images) to create a validation set, and 2,000 products (4,000 images) for a test set. We added 99,541 simple images in the validation gallery as distractors, and 2,000,014 in the test gallery.\\n\\nWe randomly sampled images and manually verified the quality of the labels. For the complexity labels, we measured an empirical error rate of $\\\\frac{1}{1000}$ on the training set and $\\\\frac{3}{1000}$ for the distractors.\\n\\nFor the product categories, we measured a global empirical error rate of 1%, with confusions mostly arising from semantically similar categories and images where object scale was ambiguous in isolated settings (e.g. long shirt vs. short dress, wristband vs. hairband). The BLIP2 captions we provided exhibit good quality, increasing the mean CLIP similarity with the image by 7.4%. However, as synthetic captions, they are not perfect and may contain occasional hallucinations.\\n\\nPlease refer to Appendix A.4 for metadata details, A.5 for considerations regarding privacy and biases and C for metadata details and a datasheet [13].\\n\\n3.3 Benchmark\\n\\nWe define a benchmark on LRVS-F to evaluate different methods on a held-out test set with a large number of distractors. The test set contains 2,000 unseen products, and up to 2M distractors. Each product in the set is represented by a pair of images - a simple one and a complex one. The objective of the retrieval task is to retrieve the simple image of each product from among a vast number of distractors and other simple test images, given the complex image and conditioning information.\\n\\nFor this dataset, we propose to frame the benchmark as an asymmetric task: the representation of simple images (the gallery) should not be computed conditionally. This choice is motivated by three reasons. First, when using precise free-form conditioning (such as LAION texts, which contain hashed product identifiers and product names) a symmetric encoding would enable a retrieval based solely on this information, completely disregarding the image query. Second, for discrete (categorical) conditioning it allows the presence of items of unknown category in the gallery, which is a situation that may occur in distractors. Third, these images only depict a single object, thus making referring information unnecessary. A similar setting is used by Asai et al. [1].\\n\\nAdditionally, we provide a list of subsets sampled with replacement to be used for boostrapped estimation of confidence intervals on the metrics. We created 10 subsets of 1000 test products, and 10 subsets of 10K, 100K and 1M distractors. We also propose a validation set of 400 products with nearly 100K other distractors to monitor the training and for hyperparameter search.\\n\\n4 Conditional Embedding\\n\\nTask Formulation.\\n\\nLet $x_q$ be a query image containing several objects of interest (e.g., a person wearing many different clothes and items), and $c_q$ the associated referring information that provides cues about what aspect of $x_q$ is relevant for the query (e.g., a text describing which garment is of interest, or directly the class of the garment of interest). Similarly, let $x_t$ be a target image, described by the latent information $c_t$. The probability of $x_t$ to be relevant for the query $x_q$ is given by the conditional probability $P(x_t, c_t | x_q, c_q)$. When working with categories for $c_q$ and $c_t$, a filtering strategy consists in assuming independence between the images and their category, $P(x_t, c_t | x_q, c_q) = P(x_t | x_q) P(c_t | c_q)$.\\n\\nIn this work, we remove those assumptions and instead assume that $P(x_t, c_t | x_q, c_q)$ can be directly inferred by a deep neural network model. More specifically, we propose to learn a flexible embedding function $h(x_q, c_q)$, $(x_t, c_t)/P(x_t, c_t | x_q, c_q)$.\"}"}
{"id": "l985bXCatk", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Overview of our method on LRVS-F. For each element in a batch, we embed the scene conditionally and the isolated item unconditionally. We optimize an InfoNCE loss over the cosine similarity matrix.\\n\\ndenotes concatenation to the patch sequence.\\n\\nto provide localized information about the content of interest (like a bounding box) and can be as imprecise as a free-form text, as shown in Fig. 1.\\n\\nMethod: We implement by modifying the Vision Transformer (ViT) architecture \\\\[9\\\\]. The conditioning is an additional input token with an associated learnable positional encoding, concatenated to the sequence of image patches. The content of this token can either be learned directly (e.g. for discrete categorical conditioning), or be generated by another network (e.g. for textual conditioning).\\n\\nAt the end of the network, we linearly project the \\\\([\\\\text{CLS}]\\\\) token to map the features to a metric space. We experimented with concatenating at different layers in the transformer, and found that concatenating before the first layer is the most sensible choice (see Appendix B.1).\\n\\nWe train the network with the InfoNCE loss \\\\[44, 38\\\\], following CLIP \\\\[40\\\\], which is detailed in the next paragraph. However, we hypothesize that even though our method relies on a contrastive loss, it does not explicitly require a specific formulation of it. We choose the InfoNCE loss because of its popularity and scalability. During training, given a batch of \\\\(N\\\\) pairs of images and conditioning \\\\(((x_A^i, c_A^i); (x_B^i, c_B^i))_{i=1}^N\\\\), we compute their conditional embeddings \\\\((z_A^i, z_B^i)_{i=1}^N\\\\) with \\\\(z = (x, c)\\\\). We compute a similarity matrix \\\\(S\\\\) where \\\\(S_{ij} = s(z_A^i, z_B^j)\\\\), with \\\\(s\\\\) the cosine similarity.\\n\\nWe then optimize the similarity of the correct pair with a cross-entropy loss, effectively considering the \\\\(N-1\\\\) other products in the batch as negatives:\\n\\n\\\\[\\n\\\\mathcal{L}(S) = \\\\frac{1}{N^2} \\\\sum_{i=1}^{N} \\\\log \\\\left( \\\\frac{\\\\exp(S_{ii} \\\\tau)}{\\\\sum_{j=1}^{N} \\\\exp(S_{ij} \\\\tau)} \\\\right),\\n\\\\]\\n\\nwith \\\\(\\\\tau\\\\) a learned temperature parameter, and the final loss is \\\\(L = \\\\mathcal{L}(S) / 2 + \\\\mathcal{L}(S^>) / 2\\\\). Please refer to Fig. 4 for an overview of the method. The \\\\(\\\\tau\\\\) parameter is used to follow the initial formulation of CLIP \\\\([40]\\\\] and is optimized by gradient during the training. At test time, we use FAISS \\\\([22]\\\\] to create a unique index for the entire gallery and perform fast similarity search on GPUs.\\n\\nWe compare our method to various baselines on LRVS-F, using both category- and caption-based settings. We report implementation details before analyzing the results.\\n\\n5.1 Implementation details\\n\\nAll our models take as input images of size \\\\(224 \\\\times 224\\\\), and output an embedding vector of 512 dimensions. We use CLIP weights as initialization, and then train our models for 30 epochs with AdamW \\\\([33]\\\\] and a maximum learning rate of \\\\(10^{-5}\\\\) determined by a learning rate range test \\\\([43]\\\\]. To avoid distorting pretrained features \\\\([26]\\\\], we start by only training the final projection and new input\"}"}
{"id": "l985bXCatk", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"embeddings (conditioning and positional) for a single epoch, with a linear warm-up schedule. We then train all parameters for the rest of the epochs with a cosine schedule.\\n\\nWe pad the images to a square with white pixels, before resizing the largest side to 224 pixels. During training, we apply random horizontal flip, and random resized crops covering at least 80% of the image area. We evaluate the Recall at 1 (R @ 1) of the model on the validation set at each epoch, and report test metrics (recall and categorical accuracy) for the best performing validation checkpoint.\\n\\nWe used mixed precision and sharded loss to run our experiments on multiple GPUs. B/32 models were trained for 6 hours on 2 V100 GPUs, with a total batch size of 360. B/16 were trained for 9 hours on 12 V100, with a batch size of 420. Batch sizes were chosen to maximize GPU memory use.\\n\\n5.2 Results\\n\\nDetection-based Baseline\\n\\nWe leveraged the recent Grounding DINO \\\\[30\\\\] and Segment Anything \\\\[24\\\\] to create a baseline approach based on object detection and segmentation. In this setting, we feed the model the query image and conditioning information, which can be either the name of the category or a caption. Subsequently, we use the output crops or masks to train a ViT following the aforementioned procedure. Please refer to Tab. 1 for the results.\\n\\nInitial experiments conducted with pretrained CLIP features showed a slight preference toward segmenting the object. However, training the image encoder revealed that superior performances can be attained by training the network on crops. Our supposition is that segmentation errors lead to definitive loss of information, whereas the network's capacity is sufficient for it to learn to disregard irrelevant information and recover from a badly cropped image.\\n\\nOverall, using Grounding DINO makes for a strong baseline. However, it is worth highlighting that the inherent imprecision of category names frequently results in overly large bounding boxes, which in turn limits the performances of the models. Indeed, adding more information into the dataset such as bounding boxes with precise categories would help, yet this would compromise the scalability of the model as such data is costly to obtain. Conversely, the more precise boxes produced by the caption-based model reach 67.8% R @ 1 against 2M distractors.\\n\\n| Distractors | Condi. Preprocessing | Embedding | %R@1 | %Cat@1 |\\n|-------------|----------------------|-----------|------|--------|\\n| +10K        | Gr. DINO-T + SAM-B   | CLIP ViT-B/32 | 16.9 \u00b1 1.45 | 67.4 \u00b1 1.70 |\\n|             |                      |           | 8.9 \u00b1 0.79  | 65.6 \u00b1 1.93  |\\n| +100K       | Gr. DINO-T + SAM-B   | ViT-B/32  | 83.0 \u00b1 1.06 | 94.6 \u00b1 0.75  |\\n|             |                      |           | 69.4 \u00b1 1.36 | 92.0 \u00b1 0.67  |\\n| +1M         | Gr. DINO-T + SAM-B   | ViT-B/32  | 88.7 \u00b1 0.74 | 96.4 \u00b1 0.55  |\\n|             |                      |           | 77.0 \u00b1 1.79 | 94.3 \u00b1 0.82  |\\n| +2M         | Gr. DINO-B           | ViT-B/16  | 89.9 \u00b1 0.87 | 96.2 \u00b1 0.77  |\\n|             |                      |           | 80.8 \u00b1 1.35 | 94.5 \u00b1 0.73  |\\n\\nCategorical Conditioning\\n\\nWe compare our method with categorical detection-based approaches, and unconditional ViTs finetuned on our dataset. To account for the extra conditioning information used in our method, we evaluated the latter on filtered indexes, with only products belonging to the correct category. We did not try to predict the item of interest from the input picture, and instead consider it as a part of the query. We also report unfiltered metrics for reference. Results are in Tab. 2.\\n\\nTraining the ViTs on our dataset greatly improves their performances, both in terms of R @ 1 and categorical accuracy. Filtering the gallery brings a modest mean gain of 4% R @ 1 across all quantities of distractors (Fig. 4b), reaching 62.4% R @ 1 for 2M distractors with a ViT-B/16 architecture.\\n\\nIn practice, this approach is impractical as it necessitates computing and storing an index for each category to guarantee a consistent quantity of retrieved items. Moreover, a qualitative evaluation of the filtered results reveals undesirable behaviors. When filtering on a category divergent from the network's intrinsic focus, we observe the results displaying colors and textures associated with the automatically focused object rather than the requested one.\"}"}
{"id": "l985bXCatk", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Comparisons of results on LRVS-F for unconditional, category-based and caption-based models. For 0, 10K, 100K and 1M distractors, we report bootstrapped means and standard deviations from 10 randomly sampled sets. Our CondViT-B/16 outperforms other methods for both groups.\\n\\n| Distractors | ViT-B/32 | 85.6 \u00b1 1.08 | 93.7 \u00b1 0.31 | 73.4 \u00b1 1.35 | 90.9 \u00b1 0.78 | 58.5 \u00b1 1.37 | 87.8 \u00b1 0.86 |\\n|-------------|---------|-------------|-------------|-------------|-------------|-------------|-------------|\\n|             | %R@1    | %Cat@1      | %R@1        | %Cat@1      | %R@1        | %Cat@1      |\\n| +10K        | 93.7 \u00b1 0.31 | 90.9 \u00b1 0.78 | 58.5 \u00b1 1.37 | 87.8 \u00b1 0.86 |\\n| +100K       | 73.4 \u00b1 1.35 | 92.3 \u00b1 0.73 | 66.1 \u00b1 1.21 | 90.2 \u00b1 0.92 |\\n| +1M         | 90.9 \u00b1 0.78 | 92.3 \u00b1 0.73 | 66.1 \u00b1 1.21 | 90.2 \u00b1 0.92 |\\n| +2M         | 58.5 \u00b1 1.37 | 90.2 \u00b1 0.92 | 66.1 \u00b1 1.21 | 90.2 \u00b1 0.92 |\\n\\n| Model       | %R@1    | %Cat@1      | %R@1        | %Cat@1      | %R@1        | %Cat@1      |\\n|-------------|---------|-------------|-------------|-------------|-------------|-------------|\\n| ViT-B/16    | 88.4 \u00b1 0.88 | 94.8 \u00b1 0.52 | 79.0 \u00b1 1.02 | 92.3 \u00b1 0.73 | 66.1 \u00b1 1.21 | 90.2 \u00b1 0.92 |\\n| ASEN        | 63.1 \u00b1 1.50 | 76.3 \u00b1 1.26 | 46.1 \u00b1 1.21 | 68.5 \u00b1 0.84 | 29.8 \u00b1 1.86 | 62.9 \u00b1 1.27 |\\n| ViT-B/32 + Filt. | 88.9 \u00b1 1.01 | \u2014           | 76.8 \u00b1 1.24 | \u2014           | 62.0 \u00b1 1.31 | \u2014           |\\n| CondViT-B/32 - Category | 90.9 \u00b1 0.98 | 99.2 \u00b1 0.31 | 80.2 \u00b1 1.55 | 98.8 \u00b1 0.39 | 65.8 \u00b1 1.42 | 98.4 \u00b1 0.65 |\\n| ViT-B/16 + Filt. | 90.9 \u00b1 0.88 | \u2014           | 81.9 \u00b1 0.87 | \u2014           | 62.4 \u00b1 1.11 | \u2014           |\\n| CondViT-B/16 - Category | 93.3 \u00b1 1.04 | 99.5 \u00b1 0.25 | 85.6 \u00b1 1.06 | 99.2 \u00b1 0.35 | 74.2 \u00b1 1.82 | 99.0 \u00b1 0.42 |\\n| CoSMo       | 88.3 \u00b1 1.30 | 97.6 \u00b1 0.45 | 76.1 \u00b1 1.85 | 96.0 \u00b1 0.32 | 59.1 \u00b1 1.42 | 94.7 \u00b1 0.40 |\\n| CLIP4CIR    | 92.9 \u00b1 0.64 | 99.0 \u00b1 0.33 | 81.9 \u00b1 1.63 | 98.1 \u00b1 0.68 | 66.9 \u00b1 2.05 | 96.5 \u00b1 0.67 |\\n| CondViT-B/32 - Caption | 92.7 \u00b1 0.77 | 99.1 \u00b1 0.30 | 82.8 \u00b1 1.22 | 98.7 \u00b1 0.40 | 68.4 \u00b1 1.50 | 98.1 \u00b1 0.43 |\\n| CondViT-B/16 - Caption | 94.2 \u00b1 0.90 | 99.4 \u00b1 0.37 | 86.4 \u00b1 1.13 | 98.9 \u00b1 0.49 | 74.6 \u00b1 1.65 | 98.4 \u00b1 0.58 |\\n\\nFigure 5: R@1 with respect to the number of added distractors, evaluated on the entire test set. Please refer to Tab. 1 and 2 for bootstrapped metrics and confidence intervals. Our categorical CondViT-B/16 reaches the performances of the best caption-based models, while using a sparser conditioning. We also compare with ASEN [8] trained on our dataset using the authors\u2019 released code. This conditional architecture uses a global and a local branch with conditional spatial attention modules, respectively based on ResNet50 and ResNet34 backbones, with explicit ROI cropping. However, in our experiments, the performances decrease with the addition of the local branch in the second training stage, even after tuning the hyperparameters. We report results for the global branch. We train our CondViT using the categories provided in our dataset, learning an embedding vector for each of the 10 clothing categories. For the \\\\( i \\\\)-th product in the batch, we randomly select in the associated data a simple image \\\\( x_s \\\\) and its category \\\\( c_s \\\\), and a complex image \\\\( x_c \\\\). We then compute their embeddings \\\\( z_A^i = (x_c, c_s) \\\\), \\\\( z_B^i = x_s \\\\). We also experimented with symmetric conditioning, using a learned token for the gallery side (see Appendix B.1). Our categorical CondViT-B/16, with 68.4% R@1 against 2M distractors, significantly outperforms all other category-based approaches (see Fig. 5, left) and maintains a higher categorical accuracy. Furthermore, it performs similarly to the detection-based method conditioned on richer captions, while requiring easy-to-acquire coarse categories. It does so without making any assumption on the semantic nature of these categories, and adding only a few embedding weights (7.7K parameters) to the network, against 233M parameters for Grounding DINO-B. We confirm in Appendix B.2 that its attention is localized on different objects depending on the conditioning.\"}"}
{"id": "l985bXCatk", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Outwear Lower Body Upper Body\\n\\n\\\"Long dress\\\" \\\"Sandals\\\" \\\"Ankle boots\\\"\\n\\n\\\"Her blouse\\\" \\\"Same shorts\\\" \\\"Puffer vest\\\"\\n\\nFigure 6: Qualitative results for our categorical (first 2 rows) and textual (last 2 rows) CondViT-B/16.\\n\\nWe use free-form textual queries instead of BLIP2 captions to illustrate realistic user behavior, and retrieve from the whole test gallery. See Fig. 13 and 14 in the Appendix for more qualitative results.\\n\\nTextual Conditioning\\n\\nTo further validate our approach, we replaced the categorical conditioning with referring expressions, using our generated BLIP2 captions embedded by a Sentence T5-XL model. We chose this model because it embeds the sentences in a 768-dimensional vector, allowing us to simply replace the categorical token. We pre-computed the caption embeddings, and randomly used one of them instead of the product category at training time. At test time, we used the first caption.\\n\\nIn Tab. 2, we observe a gain of 3.1% R @ 1 for the CondViT-B/32 architecture, and 0.9% R @ 1 for CondViT-B/16, compared to categorical conditioning against 2M distractors, most likely due to the additional details in the conditioning sentences. When faced with users, this method allows for more natural querying, with free-form referring expressions. See Figure 6 for qualitative results.\\n\\nWe compare these models with CIR methods: CoSMo and CLIP4CIR. Both use a compositor network to fuse features extracted from the image and accompanying text. CoSMo reaches performances similar to an unconditional ViT-B/32, while CLIP4CIR performs similarly to our textual CondViT-B/32. We hypothesize that for our conditional feature extraction task, early conditioning is more effective than modifying embeddings through a compositor at the network's end. Our CondViT-B/16 model significantly outperforms all other models and achieves results comparable to our caption-based approach using Grounding DINO-B (see Fig. 5, right). As the RVS task differs from CIR, despite both utilizing identical inputs, this was anticipated. Importantly, CondViT-B/16 accomplishes this without the need for explicit detection steps or dataset-specific preprocessing.\\n\\nNotably, we observe that our models achieve a categorical accuracy of 98% against 2M distractors, surpassing the accuracy of the best corresponding detection-based model, which stands at 94.3%.\\n\\n6 Conclusion & Limitations\\n\\nWe studied an approach to image similarity in fashion called Referred Visual Search (RVS), which introduces two significant contributions. Firstly, we introduced the LAION-RVS-Fashion dataset, comprising 272K fashion products and 842K images. Secondly, we proposed a simple weakly-supervised learning method for extracting referred embeddings. Our approach outperforms strong detection-based baselines. These contributions offer valuable resources and techniques for advancing image retrieval systems in the fashion industry and beyond.\\n\\nHowever, one limitation of our approach is that modifying the text description to refer to something not present or not easily identifiable in the image does not work effectively. For instance, if the image shows a person carrying a green handbag, a refined search with \\\"red handbag\\\" as a condition would only retrieve a green handbag. The system may also ignore the conditioning if the desired item is small or absent in the database. Examples of such failures are illustrated in Appendix B.3. Additionally, extending the approach to more verticals would be relevant.\"}"}
{"id": "l985bXCatk", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions. arXiv preprint arXiv:2211.09260, 2022.\\n\\n[2] Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Alberto Del Bimbo. Composed image retrieval using contrastive learning and task-oriented clip-based features. ACM Transactions on Multimedia Computing, Communications and Applications, 2023.\\n\\n[3] Sean Bell, Yiqun Liu, Sami Alsheikh, Yina Tang, Edward Pizzi, M. Henning, Karun Singh, Omkar Parkhi, and Fedor Borisyuk. GrokNet: Unified Computer Vision Model Trunk and Embeddings For Commerce. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2020. ISBN 978-1-4503-7998-4. doi: 10.1145/3394486.3403311.\\n\\n[4] Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin. End-to-end referring video object segmentation with multimodal transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\n[5] Yiyang Chen, Zhedong Zheng, Wei Ji, Leigang Qu, and Tat-Seng Chua. Composed image retrieval with text feedback via multi-grained uncertainty regularization, 2022.\\n\\n[6] Nilotpal Das, Aniket Joshi, Promod Yenigalla, and Gourav Agrwal. MAPS: Multimodal Attention for Product Similarity. Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2022.\\n\\n[7] Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang. Vision-Language Transformer and Query Generation for Referring Segmentation. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, 2021. ISBN 978-1-66542-812-5. doi: 10.1109/ICCV48922.2021.01601.\\n\\n[8] Jianfeng Dong, Zhe Ma, Xiaofeng Mao, Xun Yang, Yuan He, Richang Hong, and Shouling Ji. Fine-Grained Fashion Similarity Prediction by Attribute-Specific Embedding Learning. IEEE Transactions on Image Processing, 2021. ISSN 1057-7149, 1941-0042. doi: 10.1109/TIP.2021.3115658.\\n\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.\\n\\n[10] Ming Du, Arnau Ramisa, Amit Kumar K C, Sampath Chanda, Mengjiao Wang, Neelakandan Rajesh, Shasha Li, Yingchuan Hu, Tao Zhou, Nagashri Lakshminarayana, Son Tran, and Doug Gray. Amazon Shop the Look: A Visual Search System for Fashion and Home. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. ACM, 2022. ISBN 978-1-4503-9385-0. doi: 10.1145/3534678.3539071.\\n\\n[11] Shiv Ram Dubey. A Decade Survey of Content Based Image Retrieval using Deep Learning. IEEE Transactions on Circuits and Systems for Video Technology, 2022. ISSN 1051-8215, 1558-2205. doi: 10.1109/TCSVT.2021.3080920.\\n\\n[12] Yuying Ge, Ruimao Zhang, Xiaogang Wang, Xiaoou Tang, and Ping Luo. DeepFashion2: A Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2019. ISBN 978-1-72813-293-8. doi: 10.1109/CVPR.2019.00548.\\n\\n[13] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 III, and Kate Crawford. Datasheets for datasets, 2021.\\n\\n[14] Xiaoxiao Guo, Hui Wu, Yu Cheng, Steven Rennie, Gerald Tesauro, and Rogerio Feris. Dialog-based Interactive Image Retrieval. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2018.\\n\\n[15] Mark Hamilton, Stephanie Fu, Mindren Lu, Johnny Bui, Darius Bopp, Zhenbang Chen, Felix Tran, Margaret Wang, Marina Rogers, Lei Zhang, Chris Hoder, and William T. Freeman. MosAIc: Finding Artistic Connections across Culture with Conditional Image Retrieval. In Proceedings of the NeurIPS 2020 Competition and Demonstration Track. PMLR, 2021.\\n\\n[16] Xiao Han, Sen He, Li Zhang, Yi-Zhe Song, and Tao Xiang. UIGR: Unified Interactive Garment Retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\"}"}
{"id": "l985bXCatk", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Houdong Hu, Yan Wang, Linjun Yang, Pavel Komlev, Li Huang, Xi Chen, Jiapei Huang, Ye Wu, Meenaz Merchant, and Arun Sacheti. Web-scale responsive visual search at bing. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, 2018.\\n\\nJunshi Huang, Rogerio Feris, Qiang Chen, and Shuicheng Yan. Cross-Domain Image Retrieval with a Dual Attribute-Aware Ranking Network. In 2015 IEEE International Conference on Computer Vision (ICCV). IEEE, 2015. ISBN 978-1-4673-8391-2. doi: 10.1109/ICCV.2015.127.\\n\\nShaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, and Bo Li. Referring Image Segmentation via Cross-Modal Progressive Comprehension. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2020. ISBN 978-1-72817-168-5. doi: 10.1109/CVPR42600.2020.01050.\\n\\nYang (Andrew) Jiao, Yan Gao, Jingjing Meng, Jin Shang, and Yi Sun. Learning attribute and class-specific representation duet for fine-grained fashion analysis. In CVPR 2023, 2023.\\n\\nYushi Jing, David Liu, Dmitry Kislyuk, Andrew Zhai, Jiajing Xu, Jeff Donahue, and Sarah Tavel. Visual search at pinterest. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015.\\n\\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 2019.\\n\\nM. Hadi Kiapour, Xufeng Han, Svetlana Lazebnik, Alexander C. Berg, and Tamara L. Berg. Where to Buy It: Matching Street Clothing Photos in Online Shops. In 2015 IEEE International Conference on Computer Vision (ICCV). IEEE, 2015. ISBN 978-1-4673-8391-2. doi: 10.1109/ICCV.2015.382.\\n\\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023.\\n\\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3D Object Representations for Fine-Grained Categorization. In 2013 IEEE International Conference on Computer Vision Workshops. IEEE, 2013. ISBN 978-1-4799-3022-7. doi: 10.1109/ICCVW.2013.77.\\n\\nAnanya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In International Conference on Learning Representations, 2022.\\n\\nJulia Lasserre, Katharina Rasch, and Roland Vollgraf. Studio2Shop: from studio photo shoots to fashion articles. In Proceedings of the 7th International Conference on Pattern Recognition Applications and Methods, 2018. doi: 10.5220/0006544500370048.\\n\\nSeungmin Lee, Dongwan Kim, and Bohyung Han. Cosmo: Content-style modulation for image retrieval with text feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\\n\\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding DINO: Marrying DINO with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023.\\n\\nSi Liu, Zheng Song, Guangcan Liu, Changsheng Xu, Hanqing Lu, and Shuicheng Yan. Street-to-shop: Cross-scenario clothing retrieval via parts alignment and auxiliary set. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, 2012.\\n\\nZiwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2016. ISBN 978-1-4673-8851-1. doi: 10.1109/CVPR.2016.124.\\n\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019.\\n\\nGen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji. Multi-Task Collaborative Network for Joint Referring Expression Comprehension and Segmentation. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2020. ISBN 978-1-72817-168-5. doi: 10.1109/CVPR42600.2020.01005.\"}"}
{"id": "l985bXCatk", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zhe Ma, Jianfeng Dong, Zhongzi Long, Yao Zhang, Yuan He, Hui Xue, and Shouling Ji. Fine-grained fashion similarity learning by attribute-specific embedding network. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020.\\n\\nEmily Mu and John Guttag. Conditional Contrastive Networks. In NeurIPS 2022 First Table Representation Workshop, 2022.\\n\\nJianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Findings of the Association for Computational Linguistics: ACL 2022. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-acl.146.\\n\\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\\n\\nMaxime Oquab, Timoth\u00e9e Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 2021.\\n\\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\\n\\nRaymond Shiau, Hao-Yu Wu, Eric Kim, Yue Li Du, Anqi Guo, Zhiyuan Zhang, Eileen Li, Kunlong Gu, Charles Rosenberg, and Andrew Zhai. Shop The Look: Building a Large Scale Visual Shopping System at Pinterest. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020. doi: 10.1145/3394486.3403372.\\n\\nLeslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter conference on applications of computer vision (WACV). IEEE, 2017.\\n\\nKihyuk Sohn. Improved Deep Metric Learning with Multi-class N-pair Loss Objective. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2016.\\n\\nHyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep Metric Learning via Lifted Structured Feature Embedding. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2016. ISBN 978-1-4673-8851-1. doi: 10.1109/CVPR.2016.434.\\n\\nSon Tran, R. Manmatha, and C. J. Taylor. Searching for fashion products from images in the wild. In KDD 2019 Workshop on AI for Fashion, 2019.\\n\\nAndreas Veit, Serge Belongie, and Theofanis Karaletsos. Conditional similarity networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2017.\\n\\nNam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. Composing text and image for image retrieval-an empirical odyssey. In CVPR, 2019.\\n\\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The Caltech-UCSD Birds-200-2011 Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.\\n\\nXi Wang, Zhenfeng Sun, Wenqiang Zhang, Yu Zhou, and Yu-Gang Jiang. Matching User Photos to Online Products with Robust Deep Features. In Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval. ACM, 2016. ISBN 978-1-4503-4359-6. doi: 10.1145/2911996.2912002.\\n\\nHui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, and Rogerio Feris. Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language Feedback. In CVPR, 2019.\\n\\nJiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping Luo. Language as Queries for Referring Video Object Segmentation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2022. ISBN 978-1-66546-946-3. doi: 10.1109/CVPR52688.2022.00492.\\n\\nFan Yang, Ajinkya Kale, Yury Bubnov, Leon Stein, Qiaosong Wang, Hadi Kiapour, and Robinson Piramuthu. Visual Search at eBay. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2017. doi: 10.1145/3097983.3098162.\"}"}
{"id": "l985bXCatk", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Licheng Yu, Jun Chen, Animesh Sinha, Mengjiao Wang, Yu Chen, Tamara L Berg, and Ning Zhang. Com-472\\nmercemm: Large-scale commerce multimodal representation learning with omni retrieval. In\\nProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining\\n3\\n474\\nYifei Yuan and Wai Lam. Conversational Fashion Image Retrieval via Multiturn Natural Language\\nFeedback. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '21), 2021. 3\\n475\\nYan Zeng, Xinsong Zhang, and Hang Li. Multi-Grained Vision Language Pre-Training: Aligning Texts\\nwith Visual Concepts. In Proceedings of the 39th International Conference on Machine Learning. PMLR,\\n479\\n2022. 3\\n480\\nYan Zeng, Xinsong Zhang, Hang Li, Jiawei Wang, Jipeng Zhang, and Wangchunshu Zhou. X\\n2-VLM: All-in-one pre-trained model for vision-language tasks. arXiv preprint arXiv:2211.12402, 2022. 3\\n481\\nAndrew Zhai, Hao-Yu Wu, Eric Tzeng, Dong Huk Park, and Charles Rosenberg. Learning a unified em-\\nbedding for visual search at pinterest. In Proceedings of the 25th ACM SIGKDD International Conference\\non Knowledge Discovery & Data Mining, 2019. 3\\n484\\nXunlin Zhan, Yangxin Wu, Xiao Dong, Yunchao Wei, Minlong Lu, Yichi Zhang, Hang Xu, and Xiaodan\\nLiang. Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-Modal\\nPretraining. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, 2021. ISBN\\n978-1-66542-812-5. doi: 10.1109/ICCV48922.2021.01157. 3\\n485\\nYanhao Zhang, Pan Pan, Yun Zheng, Kang Zhao, Yingya Zhang, Xiaofeng Ren, and Rong Jin. Visual\\nSearch at Alibaba. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge\\nDiscovery & Data Mining, 2018. doi: 10.1145/3219819.3219820. 3\\n488\\nYuting Zhang, Luyao Yuan, Yijie Guo, Zhiyuan He, I-An Huang, and Honglak Lee. Discriminative\\nBimodal Networks for Visual Localization and Detection with Natural Language Queries. In 2017 IEEE\\nConference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017. ISBN 978-1-5386-0457-1.\\ndoi: 10.1109/CVPR.2017.122. 3\\n489\\nXiaoyang Zheng, Zilong Wang, Ke Xu, Sen Li, Tao Zhuang, Qingwen Liu, and Xiaoyi Zeng. MAKE:\\nVision-Language Pre-training based Product Retrieval in Taobao Search. In Companion Proceedings of the\\nACM Web Conference 2023, 2023. doi: 10.1145/3543873.3584627. 3\\n490\\nChecklist\\n1.\\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper's\\ncontributions and scope? [Yes]\\n(b) Did you describe the limitations of your work? [Yes]\\n(c) Did you discuss any potential negative societal impacts of your work? [Yes]\\nSee Appendix A.5.\\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to\\nthem? [Yes]\\nSee Appendix A.5.\\n2.\\nIf you are including theoretical results...\\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n(b) Did you include complete proofs of all theoretical results? [N/A]\\n3.\\nIf you ran experiments (e.g. for benchmarks)...\\n(a) Did you include the code, data, and instructions needed to reproduce the main ex-\\nperimental results (either in the supplemental material or as a URL)? [Yes]\\nSee Appendix A.2.\\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they\\nwere chosen)? [Yes]\\nSee Section 5.1.\\n(c) Did you report error bars (e.g., with respect to the random seed after running experi-\\nments multiple times)? [Yes]\\nSee Table 1 and 2.\\n(d) Did you include the total amount of compute and the type of resources used (e.g., type\\nof GPUs, internal cluster, or cloud provider)? [Yes]\\nSee Section 5.1.\"}"}
{"id": "l985bXCatk", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n\\n(a) If your work uses existing assets, did you cite the creators?\\n\\n[Yes]\\n\\n(b) Did you mention the license of the assets?\\n\\n[Yes]\\n\\n(c) Did you include any new assets either in the supplemental material or as a URL?\\n\\n[Yes]\\n\\nWe provide the URLs to the assets in Appendix A.2.\\n\\n(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating?\\n\\n[Yes]\\n\\n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?\\n\\n[Yes]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable?\\n\\n[N/A]\\n\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?\\n\\n[N/A]\\n\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?\\n\\n[N/A]\"}"}
