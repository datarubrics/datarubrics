{"id": "yCZRdI0Y7G", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sample Efficiency Matters: A Benchmark for Practical Molecular Optimization\\n\\nWenhao Gao, Tianfan Fu, Jimeng Sun, Connor W. Coley\\n\\n1 Department of Chemical Engineering, Massachusetts Institute of Technology,\\n2 Department of Computational Science and Engineering, Georgia Institute of Technology,\\n3 Department of Computer Science, University of Illinois at Urbana-Champaign,\\n4 Carle Illinois College of Medicine, University of Illinois at Urbana-Champaign,\\n5 Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology,\\n\\n\u21e4 Equal Contributions\\n\\n{whgao,ccoley}@mit.edu, tfu42@gatech.edu, jimeng@illinois.edu\\n\\nAbstract\\n\\nMolecular optimization is a fundamental goal in the chemical sciences and is of central interest to drug and material design. In recent years, significant progress has been made in solving challenging problems across various aspects of computational molecular optimizations, emphasizing high validity, diversity, and, most recently, synthesizability. Despite this progress, many papers report results on trivial or self-designed tasks, bringing additional challenges to directly assessing the performance of new methods. Moreover, the sample efficiency of the optimization\u2014the number of molecules evaluated by the oracle\u2014is rarely discussed, despite being an essential consideration for realistic discovery applications.\\n\\nTo fill this gap, we have created an open-source benchmark for practical molecular optimization, PMO, to facilitate the transparent and reproducible evaluation of algorithmic advances in molecular optimization. This paper thoroughly investigates the performance of 25 molecular design algorithms on 23 single-objective (scalar) optimization tasks with a particular focus on sample efficiency. Our results show that most \u201cstate-of-the-art\u201d methods fail to outperform their predecessors under a limited oracle budget allowing 10K queries and that no existing algorithm can efficiently solve certain molecular optimization problems in this setting. We analyze the influence of the optimization algorithm choices, molecular assembly strategies, and oracle landscapes on the optimization performance to inform future algorithm development and benchmarking.\\n\\nPMO provides a standardized experimental setup to comprehensively evaluate and compare new molecule optimization methods with existing ones. All code can be found at https://github.com/wenhao-gao/mol_opt.\\n\\n1 Introduction\\n\\nDesigning new functional molecules is a constrained multi-objective optimization problem that aims to find molecules with desired properties such as selective inhibition against a disease target, with additional desiderata and constraints to ensure the structures are stable and synthesizable. The importance of molecular design problems has attracted significant efforts to develop systematical molecular design methodologies instead of exhaustive searches, leveraging combinatorial optimization algorithms [1, 2], predictive machine learning models [3, 4], and generative models [5, 6]. Especially in recent years, we have witnessed significant progress in solving challenging problems across\\n\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks.\"}"}
{"id": "yCZRdI0Y7G", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"various aspects of computational molecular optimizations, such as achieving high validity \\\\cite{7,8,9}, diversity \\\\cite{10}, and, most recently, synthesizability \\\\cite{11,12}.\\n\\nDespite the exciting progress in the field and the abundance of new methods proposed, how these algorithms compare against each other remains unclear. Most method development papers and existing benchmarks such as Guacamol \\\\cite{13}, Therapeutics Data Commons (TDC) \\\\cite{14} and Tripp et al.'s \\\\cite{15} suffer from at least one of three problems: (1) Lack of consideration of the oracle budget: Many papers \\\\cite{16,17,18} do not report how many times the oracle function is called to achieve the reported results (i.e., how many candidate molecules were evaluated), except in rare cases \\\\cite{19,20,21,22,23}, despite this range spanning orders of magnitude. As most valuable oracles\u2014experiments or high-accuracy simulations\u2014require substantial costs, it is vital to identify the desired compound with as few oracle calls as possible. (2) Trivial oracles: Some papers only report results on trivial oracles \\\\cite{17} like quantitative estimate of drug-likeness (QED) \\\\cite{24} or penalized octanol-water partition coefficient (LogP); other papers even introduce new self-designed tasks \\\\cite{18,21}, which obfuscates a comparison to prior work. (3) Randomness: Another complication is that many algorithms are not deterministic and exhibit significant run-to-run variation, so reporting results from several independent trials is essential. All of the existing benchmarks examined no more than five methods due to the significant variation between molecular optimization algorithms. Thus we still lack a unified benchmark to assess which methods are beneficial in a realistic discovery scenario.\\n\\nThis paper presents a new reproducible large-scale experimental study with a sound experimental protocol for molecular design, PMO. We have benchmarked 25 methods across 23 various widely-used oracle functions, with each of them tuned and run for multiple independent trials. To consider a combination of optimization ability and sample efficiency, we limit the number of maximum oracle calls up to 10,000 queries and measure model performance with the area under the curve (AUC) of the top-10 average performance versus oracle calls. Our results show that none of the existing molecular optimization algorithms are efficient enough to solve a de novo molecular optimization problem within a realistic oracle budget of hundreds of experiments, and \u201cstate-of-the-art\u201d methods often fail to outperform their predecessors. We analyze the algorithmic contribution and the influence of oracle landscapes on optimization performance to inform future algorithm development and benchmarking.\\n\\nOur results highlight the necessity of standardized experimental reporting, including independent replicates and extensive hyperparameter tuning. We envision that the PMO benchmark will make molecular optimizations more accessible and reproducible, thereby facilitating algorithmic advances and, ultimately, the broader adoption of molecular optimization techniques in experimental drug and materials discovery workflows.\\n\\n2 Algorithms\\n\\nA molecular optimization method has two major components: (1) a molecular assembly strategy that defines the chemical space by assembling a digital representation of compounds, and (2) an optimization algorithm that navigates this chemical space. This section will first introduce common strategies to assemble molecules, then introduce the benchmarked molecular optimization methods based on the core optimization algorithms. Table 1 summarizes current molecular design methods categorized based on assembly strategy and optimization method, including but not limited to the methods included in our baseline. We emphasize that our goal is not to make an exhaustive list but to include a group of methods that are representative enough to obtain meaningful conclusions.\\n\\n2.1 Preliminaries\\n\\nIn this paper, we limit our scope to general-purpose single-objective molecular optimization methods focusing on small organic molecules with scalar properties with some relevance to therapeutic design. As demonstrated in this benchmark later, QED is likely to have a global maximum of 0.948 and even random sampling could reach that value. It is disabled to meaningfully distinguish different algorithms. LogP is unbounded and the relationship between LogP values and molecular structures is fairly simple: adding carbons monotonically increases the estimated LogP value \\\\cite{25,20}. This simple strategy makes the performance in LogP highly depend on the chemical space definition and the number of steps allowed, and provides no insights for distinguish algorithms\u2019 optimization ability. Besides, simply maximizing LogP is not a meaningful goal in drug design. Therefore, we exclude LogP in this benchmark.\"}"}
{"id": "yCZRdI0Y7G", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Representative molecule generation methods, categorised based on the molecular assembly strategies and the optimization algorithms. Columns are various molecular assembly strategies while rows are different optimization algorithms.\\n\\n| SMILES | SELFIES | Graph (atom) | Graph (fragment) | Synthesis |\\n|--------|---------|--------------|------------------|------------|\\n| GA     | GA+D    | STONED       | -                | SynNet     |\\n|        |         | [1]          | [13]             | [29]       |\\n| MCTS   | BO      | BOSS         | -                | GPBO       |\\n|        |         | [30]         | [15]             | ChemBO     |\\n| V AE   | SMILES-V AE | SELFIES-V AE | -                | JTV AE     |\\n|        |         | [6]          | [22]             | [8]        |\\n| GAN    | ORGAN   | -            | MolGAN           | -          |\\n|        |         | [31]         | [32]             | [37]       |\\n| HC     | SMILES LSTM | SELFIES LSTM | -                | MIMOSA     |\\n|        |         | [13]         | [11]             | DoG-Gen    |\\n| RL     | REINVENT | SELFIES-REINVENT | -              | MolDQN     |\\n|        |         | [5]          | [16]             | GPBN       |\\n|        |         |              | [25]             | RationaleRL|\\n|        |         |              | [35]             | PGFS       |\\n|        |         |              | [18]             | REACTOR    |\\n|        |         |              |                  | Pasithea   |\\n|        |         |              | [37]             | DST        |\\n\\nFormally, we can formulate such a molecular design problem as an optimization problem:\\n\\n$$m^\\\\ast = \\\\arg\\\\max_{m \\\\in M} O(m),$$  \\n\\nwhere $m$ is a molecular structure, $M$ denotes the design space called chemical space that comprises all possible candidate molecules. The size of $M$ is impractically large, e.g., $10^{60}$. We assume we have access to the ground truth value of a property of interest denoted by $O(m)$: $M \\\\rightarrow \\\\mathbb{R}$, where an oracle, $O$, is a black-box function that evaluates certain chemical or biological properties of a molecule $m$ and returns the ground truth property $O(m)$ as a scalar. Note that neither the analytic form of oracles nor the derivatives of the properties are accessible. The most practical oracles\u2014experiments or high-accuracy simulations\u2014typically require substantial costs. An algorithm able to optimize the oracle within a reasonable budget is thus necessary for automating the design of molecules to achieve high-level automated chemical design (ACD) or function-oriented autonomous synthesis.\\n\\n### 2.2 Molecular assembly strategies\\n\\n#### String-based\\n\\nString-based assembly strategies represent molecules as strings and explore chemical space by modifying strings directly: character-by-character, token-by-token, or through more complex transformations based on a specific grammar. We include two types of string representations: (1) Simplified Molecular-Input Line-Entry System (SMILES) [38], a linear notation describing the molecular structure using short ASCII strings based on a graph traversal algorithm; (2) SELF-referencing Embedded Strings (SELFIES) [9], which avoids syntactical invalidity by enforcing the chemical validity rules in a formal grammar table.\\n\\n#### Graph-based\\n\\nTwo-dimensional (2D) graphs can intuitively define molecular identities to a first approximation (ignoring stereochemistry): the nodes and edges represent the atoms and bonds. There are two main assembling strategies for molecular graphs: (1) an atom-based assembly strategy [16] that adds or modifies atoms and bonds one at a time, which covers all valid chemical space; (2) a fragment-based assembling strategy [8] that summarizes common molecular fragments and operates one fragment at a time. Note that fragment-based strategy could also include atom-level operation.\\n\\n#### Synthesis-based\\n\\nMost of the above assembly strategies can cover a large chemical space, but an eventual goal of molecular design is to physically test the candidate; thus, a desideratum is to explore synthesizable candidates only. Designing molecules by assembling synthetic pathways from commercially-available starting materials and reliable chemical transformation adds a constraint of synthesizability to the search space. This class can be divided into template-free [11] and template-based [12] based on how to define reliable chemical transformations, but we will not distinguish between them in this paper as synthesis-based strategy is relatively less explored in general.\\n\\nIncorporating certain stereochemical information in 2D molecular graphs is possible through various approaches [39, 40, 41].\"}"}
{"id": "yCZRdI0Y7G", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.3 Optimization algorithms\\n\\nScreening (a.k.a. virtual screening) involves searching over a pre-enumerated library of molecules. We include Screening as a baseline, which randomly samples ZINC 250k. Model-based screening instead trains a surrogate model and prioritizes molecules that are scored highly by the surrogate to accelerate screening. We adopt the implementation from the original paper of MolPAL and treat it as a model-based version of screening.\\n\\nGenetic Algorithm (GA) is a popular heuristic algorithm inspired by natural evolutionary processes. It combines mutation and/or crossover perturbing a mating pool to enable exploration in the design space. We include SMILES GA that defines actions based on SMILES context-free grammar and a modified version of STONED that directly manipulates tokens in SELFIES strings. Unlike the string-based GAs that only have mutation steps, Graph GA derives crossover rules from graph matching and includes both atom- and fragment-level mutations. Finally, we include SynNet as a synthesis-based example that applies a genetic algorithm on binary fingerprints and decodes to synthetic pathways. We adopt the implementation of SMILES GA and Graph GA from Guacamol, STONED, and SynNet from the original paper. We also include the original implementation of a deep learning enhanced version of SELFIES-based GA from and label it as GA+D.\\n\\nMonte-Carlo Tree Search (MCTS) locally and randomly searches each branch of the current state (e.g., a molecule or partial molecule) and selects the most promising ones (those with highest property scores) for the next iteration. Graph MCTS is an MCTS algorithm based on atom-level searching over molecular graphs. We adopt the implementation from Guacamol.\\n\\nBayesian optimization (BO) is a large class of methods that builds a surrogate for the objective function using a Bayesian machine learning technique, such as Gaussian process (GP) regression, then uses an acquisition function combining the surrogate and uncertainty to decide where to sample, which is naturally model-based. However, as BO usually leverages a non-parametric model, it scales poorly with sample size and feature dimension. We included a string-based model, BO over String Space (BOSS) and a synthesis-based model, ChemBO, but do not obtain meaningful results even with early stopping potentially due to the poor scaling of the string subsequence kernel (SSK) (see Section B.3 for early stopping setting, and Section B.33 for more analysis). Finally, we adopt Gaussian process Bayesian optimization (GP BO) that optimizes the GP acquisition function with Graph GA methods in an inner loop. The implementation is from the original paper, and we treat it as a model-based version of Graph GA. Note that we categorize methods that apply BO to optimize molecules in latent space as a separate class below.\\n\\nVariational autoencoders (VAEs) are a class of generative methods that maximize a lower bound of the likelihood (evidence lower bound (ELBO)) instead of estimating the likelihood directly. A VAE typically learns to map molecules to and from real space to enable the indirect optimization of molecules by numerically optimizing latent vectors, most commonly with BO. SMILES-V AE uses a VAE to model molecules represented as SMILES strings, and is implemented in MOSES. We adopt the identical architecture to model SELFIES strings and denote it as SELFIES-V AE.\\n\\nJT-V AE abstracts a molecular graph into a junction tree (i.e., a cycle-free structure), and desig message passing network as the encoder and tree-RNN as the decoder. DoG-AE uses Wasserstein autoencoder (WAE) to learn the distribution of synthetic pathways. Note that we include a set of vanilla methods for each kind while many variants have emerged, such as and . We leave the validation of variants for the future development of this benchmark.\\n\\nScore-based modeling (SBM) formulates the problem of molecule design as a sampling problem where the target distribution is a function of the target property, featured by Markov-chain Monte Carlo (MCMC) methods that construct Markov chains with the desired distribution as their equilibrium distribution. MARkov molecular Sampling (MARS) is such an example that leverages a graph neural network to propose action steps adaptively in an MCMC with an annealing scheme. Generative Flow Network (GFlowNet) views the generative process as a flow network and trains it with a temporal difference-like loss function based on the conservation of flow. By matching the property of interest with the volume of the flow, generation can sample a distribution proportional to the target distribution.\\n\\nHill climbing (HC) is an iterative learning method that incorporates the generated high-scored molecules into the training data and fine-tunes the generative model for each iteration. It is a variant\"}"}
{"id": "yCZRdI0Y7G", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of the cross-entropy method \\\\[54\\\\], and can also be seen as a variant of REINFORCE \\\\[55\\\\] with a particular reward shaping. We adopt SMILES-LSTM-HC from Guacamol \\\\[13\\\\] that leverages a LSTM to learn the molecular distribution represented in SMILES strings, and modifies it to a SELFIES version denoted as SELFIES-LSTM-HC. MultI-constraint MOlecule SAmpling (MIMOSA) \\\\[33\\\\] leverages a graph neural network to predict the identity of a masked fragment node and trains it with a HC algorithm. DoG-Gen \\\\[11\\\\] instead learn the distribution of synthetic pathways as Directed Acyclic Graph (DAGs) with an RNN generator. Reinforcement Learning (RL) learns how intelligent agents take actions in an environment to maximize the cumulative reward by transitioning through different states. In molecular design, a state is usually a partially generated molecule; actions are manipulations at the level of graphs or strings; rewards are defined as the generated molecules' property of interest. REINVENT \\\\[5\\\\] adopts a policy-based RL approach to tune RNNs to generate SMILES strings. We adopt the implementation from the original paper, and modify it to generate SELFIES strings, SELFIES-REINVENT. MolDQN \\\\[16\\\\] uses a deep Q-network to generate molecular graph in an atom-wise manner.\\n\\nGradient ascent (GRAD) methods learn to estimate the gradient direction based on the landscape of the molecular property over the chemical space, and back-propagate to optimize the molecules. Pasithea \\\\[37\\\\] exploits an MLP to predict properties from SELFIES strings, and back-propagate to modify tokens. Differentiable scaffolding tree (DST) \\\\[20\\\\] abstracts molecular graphs to scaffolding trees and leverages a graph neural network to estimate the gradient. We adopted the implementation from the original papers and modify them to update the surrogates online as data are acquired.\\n\\n3 Experiments\\n3.1 Benchmark setup\\nThis section introduces the setup of PMO benchmark. The main idea behind PMO is the pursuit of an ideal de novo molecular optimization algorithm that exhibits strong optimization ability, sample efficiency, generalizability to various optimization objectives, and robustness to hyperparameter selection and random seeds.\\n\\nOracle: To examine the generalizability of methods, we aim to include a broad range of pharmaceutically-relevant oracle functions. Systematic categorization of oracles based on their landscape is still challenging due to the complicated relationship between molecular structure and function. We have included the most commonly used oracles (see a recent discussion of commonly-used oracles in \\\\[56\\\\]). Several have been described as \\\"trivial\\\", but we assert this is only true when the number of oracle queries is not controlled. In total, PMO includes 23 oracle functions: QED \\\\[24\\\\], DRD2 \\\\[5\\\\], GSK3, JNK3 \\\\[57\\\\], and 19 oracles from Guacamol \\\\[13\\\\]. QED is a relatively simple heuristic function that estimates if a molecule is likely to be a drug based on if it contains some \\\"red flags\\\". DRD2, GSK3, and JNK3 are machine learning models (support vector machine (SVM), random forest (RF)) fit to experimental data to predict the bioactivities against their corresponding disease targets. Guacamol oracles are designed to mimic the drug discovery objectives based on multiple considerations, called multi-property objective (MPO), including similarity to target molecules, molecular weights, CLogP, etc. All oracle scores are normalized from 0 to 1, where 1 is optimal. Recently, docking scores that estimate the binding affinity between ligands and proteins have been adopted as oracles \\\\[58,14,59\\\\]. However, as the simulations are more costly than above ones but are still coarse estimates that do not reflect true bioactivity, we leave it to future work.\\n\\nMetrics: To consider the optimization ability and sample efficiency simultaneously, we report the area under the curve (AUC) of top-K average property value versus the number of oracle calls (AUC top-K) as the primary metric to measure the performance. Unlike using top-K average property, AUC rewards methods that reach high values with fewer oracle calls. We use \\\\(K = 10\\\\) in this paper as it is useful to identify a small number of distinct molecular candidates to progress to later stages of development. We limit the number of oracle calls to 10000, though we expect methods to optimize well within hundreds of calls when using experimental evaluations. The reported values of AUCs are min-max scaled to \\\\([0, 1]\\\\).\"}"}
{"id": "yCZRdI0Y7G", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Performance of ten best performing molecular optimization methods based on mean AUC\\n\\n| Method         | Rank | Rank | Rank | Rank | Rank | Rank | Rank | Rank | Rank | Rank | Rank | Rank | Rank | Rank | Rank | Rank | Rank | Rank |\\n|----------------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|\\n|               | Median1 | Median2 | REINVENT | LSTM HC | SMILES | Graph GA | SynNet | DoG-Gen | Synthesis | SELFIES | Fragments | Fragments | Fragments | Fragments | Fragments | Fragments | Fragments | Fragments | Fragments | Fragments |\\n|                |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |       |\\n| gsk3b          | 0.123 | 0.341 | 0.045 | 0.334 | 0.176 | 0.198 | 0.206 | 0.011 | 0.016 | 0.019 | 0.018 | 0.019 | 0.019 | 0.019 | 0.019 | 0.019 | 0.019 | 0.019 | 0.019 | 0.019 | 0.019 | 0.019 |\\n| jnk3           | 0.416 | 0.416 | 0.416 | 0.416 | 0.416 | 0.416 | 0.416 | 0.416 | 0.416 | 0.416 | 0.416 | 0.416 | 0.416 | 0.416 | 0.416 | 0.416 | 0.416 | 0.416 | 0.416 | 0.416 | 0.416 | 0.416 |\\n| drd2           | 0.375 | 0.375 | 0.375 | 0.375 | 0.375 | 0.375 | 0.375 | 0.375 | 0.375 | 0.375 | 0.375 | 0.375 | 0.375 | 0.375 | 0.375 | 0.375 | 0.375 | 0.375 | 0.375 | 0.375 | 0.375 | 0.375 |\\n| qed            | 0.515 | 0.515 | 0.515 | 0.515 | 0.515 | 0.515 | 0.515 | 0.515 | 0.515 | 0.515 | 0.515 | 0.515 | 0.515 | 0.515 | 0.515 | 0.515 | 0.515 | 0.515 | 0.515 | 0.515 | 0.515 | 0.515 |\\n| deco_hop       | 0.711 | 0.711 | 0.711 | 0.711 | 0.711 | 0.711 | 0.711 | 0.711 | 0.711 | 0.711 | 0.711 | 0.711 | 0.711 | 0.711 | 0.711 | 0.711 | 0.711 | 0.711 | 0.711 | 0.711 | 0.711 | 0.711 |\\n| Assembly       | 0.934 | 0.934 | 0.934 | 0.934 | 0.934 | 0.934 | 0.934 | 0.934 | 0.934 | 0.934 | 0.934 | 0.934 | 0.934 | 0.934 | 0.934 | 0.934 | 0.934 | 0.934 | 0.934 | 0.934 | 0.934 | 0.934 |\\n| REINVENT       | 0.580 | 0.580 | 0.580 | 0.580 | 0.580 | 0.580 | 0.580 | 0.580 | 0.580 | 0.580 | 0.580 | 0.580 | 0.580 | 0.580 | 0.580 | 0.580 | 0.580 | 0.580 | 0.580 | 0.580 | 0.580 | 0.580 |\\n| GP BO          | 0.787 | 0.787 | 0.787 | 0.787 | 0.787 | 0.787 | 0.787 | 0.787 | 0.787 | 0.787 | 0.787 | 0.787 | 0.787 | 0.787 | 0.787 | 0.787 | 0.787 | 0.787 | 0.787 | 0.787 | 0.787 | 0.787 |\\n| STONED         | 0.851 | 0.851 | 0.851 | 0.851 | 0.851 | 0.851 | 0.851 | 0.851 | 0.851 | 0.851 | 0.851 | 0.851 | 0.851 | 0.851 | 0.851 | 0.851 | 0.851 | 0.851 | 0.851 | 0.851 | 0.851 | 0.851 |\\n| SELFIES        | 0.722 | 0.722 | 0.722 | 0.722 | 0.722 | 0.722 | 0.722 | 0.722 | 0.722 | 0.722 | 0.722 | 0.722 | 0.722 | 0.722 | 0.722 | 0.722 | 0.722 | 0.722 | 0.722 | 0.722 | 0.722 | 0.722 |\\n\\n*Note: All results are reported for the same set of conditions and parameters.*\\n\\nDetails are described in Appendix. Note that the implementation of sitagliptin_mpo and zaleplon_mpo are taken from the Therapeutic Data Commons (TDC) from 5 independent runs with various random seeds. All data, oracle functions, and metric evaluations for JT-V AE, MIMOSA, DST are extracted from this database. Other details for REINVENT, GP BO, STONED are extracted from this database. Generative models such as VAEs, LSTMs are pretrained on this database; fragments required for scaffold_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco_hop, deco"}
{"id": "yCZRdI0Y7G", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: The ranking of each methods based on different metrics.\\n\\n| Method             | AUC Top-1 | AUC Top-10 | AUC Top-100 | Top-1 Mean |\\n|--------------------|-----------|------------|-------------|------------|\\n| REINVENT           | 1         | 1          | 1           | 1          |\\n| Graph GA           | 2         | 2          | 2           | 3          |\\n| SELFIES-REINVENT   | 3         | 3          | 4           | 3          |\\n| SMILES-LSTM-HC     | 5         | 6          | 7           | 4          |\\n| GP BO              | 4         | 4          | 5           | 6          |\\n| STONED             | 6         | 5          | 3           | 7          |\\n| DoG-GEN            | 7         | 9          | 11          | 5          |\\n| SMILES GA          | 9         | 7          | 6           | 8          |\\n| DST                | 11        | 10         | 9           | 9          |\\n| SynNet             | 8         | 8          | 8           | 14         |\\n| SELFIES-LSTM-HC    | 13        | 14         | 13          | 11         |\\n| MIMOSA             | 14        | 12         | 10          | 12         |\\n| MARS               | 12        | 11         | 12          | 13         |\\n| MolPAL             | 10        | 13         | 15          | 16         |\\n| GA+D               | 23        | 17         | 14          | 12         |\\n| DoG-AE             | 15        | 15         | 17          | 17         |\\n| GFlowNet           | 20        | 16         | 16          | 15         |\\n| SELFIES-VAE        | 16        | 18         | 21          | 21         |\\n| Screening          | 17        | 19         | 19          | 19         |\\n| SMILES-VAE         | 18        | 20         | 20          | 20         |\\n| GFlowNet-AL        | 22        | 22         | 18          | 21         |\\n| Pasithea           | 19        | 21         | 23          | 22         |\\n| JT-V AE            | 21        | 23         | 22          | 23         |\\n| Graph MCTS         | 24        | 24         | 24          | 24         |\\n| MolDQN             | 25        | 25         | 25          | 25         |\\n\\n3.2 Results & Analysis\\n\\nThe primary results are summarized in Table 2 and 3. For clarity, we only show the ten best-performing models in the table. We show a selective set of optimization curves in Figure 1. The remaining results are in the Appendix A and D.\\n\\nSample efficiency matters. A first observation from the results is that none of the methods we implemented can optimize the simple toy objectives within hundreds of oracle calls under our experimental settings, except some trivial ones like QED, DRD2, and osimertinib_mpo, which emphasize the need for more efficient molecular optimization algorithms. By comparing the ranking of AUC Top-10 and Top-100, we notice some methods have significantly different relative performances. For example, SMILES LSTM HC, which used to be seen as comparable to Graph GA, actually requires more oracle queries to achieve the same level of performance, while a related algorithm, REINVENT, requires far fewer (see Figure 1). These differences indicate the training algorithm of REINVENT is more efficient than HC, emphasizing the importance of AUC Top-10 as an evaluation metric. In addition, methods that assemble molecules either token-by-token or atom-by-atom from a single start point, such as GA+D, MolDQN, and Graph MCTS, are most data-inefficient. Those methods potentially cover broader chemical space and include many undesired candidates, such as unstable or unsynthesizable ones, which wastes a significant portion of the oracle budget and also imposes a strong requirement on the oracles' quality.\"}"}
{"id": "yCZRdI0Y7G", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) Comparison between SMILES- and SELFIES-based methods. Note GA is not a head-to-head comparison.\\n\\n(b) Comparison between model-free and corresponding model-based methods.\\n\\nFigure 2: Each point represents the AUC Top-10 of one task, with x-axis the SMILES variant and y-axis the SELFIES variant of the same method. Colors are labeled by the optimization algorithms. The fractions of the tasks above the parity line are in parentheses.\\n\\nOlder algorithms are still powerful.\\n\\nAs shown in Table 2 and 3, the best-performing algorithms are REINVENT and Graph GA among all the compared methods, despite both of them being released several years ago. However, we rarely see model development papers list these two methods as baselines. The absence of a thorough benchmark has obfuscated the fact that newer models published in top AI conferences do not seem to offer an improvement in performance by our metrics. Of course, we should acknowledge that some of the methods are developed to solve other problems in molecular optimization, such as strings' validity or synthesizability, and some might have opened new avenues to tackle the problem that could potentially be more efficient when mature. Still, some of the field's efforts and resources might be wasted due to a lack of a thorough and standardized benchmark.\\n\\nThere are no obvious shortcomings of SMILES.\\n\\nSELFIES was designed as a substitute of SMILES to solve the syntactical invalidity problem met in SMILES representation and has been adopted by a number of recent studies. However, our head-to-head comparison of string-based methods, especially the ones leveraging language models, shows that most SELFIES variants cannot outperform their corresponding SMILES-based methods in terms of optimization ability and sample efficiency (Figure 2a). We do observe some early methods like the initial version of SMILES V AE\\\\[^6\\\\] (2016) and ORGAN\\\\[^31\\\\] (2017) struggle to propose valid SMILES strings, but this is not an issue for more recent methods. We believe this is partially because current language models are better able to learn the grammar of SMILES strings, which has flattened the advantage of SELFIES. Further, as shown in Appendix D.1, more combinations of SELFIES tokens don't necessarily explore larger chemical space but might map to a small number of valid molecules that can be represented by truncated SELFIES strings, which implies that there are still syntax requirements in generating SELFIES strings to achieve effective exploration.\\n\\nOn the other hand, we observe a clear advantage of SELFIES-based GA compared to SMILES-based one, which indicates that SELFIES has an advantage over SMILES when we need to design the rules to manipulate the sequence. However, we should note that the comparison is not head-to-head, as GAs' performances highly depend on the mutation and crossover rule design, but not the representation. Graph GA's mutation rules are also encoded in SMARTS strings and operate on SMILES strings, which can also be seen as SMILES modification steps. Overall, when we need to design the generative action manually, the assembly strategy that could derive desired transformation more intuitively should be preferred.\\n\\nModel-based methods are potentially more efficient but need careful design.\\n\\nIt is widely recognized in the RL community that model-based optimization methods that explicitly leverage a predictive model (\u201cworld model\u201d) are more sample efficient than the model-free ones\\\\[^60\\\\]. Our results on MolPAL and screening verify the principle that training a predictive model is beneficial compared to random sampling (see Figure 2b). However, the results of Graph GA (model-based variant: GP BO) and GFlowNet (model-based variant: GFlowNet-AL) indicate that simply adding a predictive model might not necessarily be helpful. GP BO outperformed Graph GA in 12 tasks among 23, but Graph GA outperformed GP BO in the summation. GFlowNet outperformed GFlowNet-AL in almost every task. From the step-wise increment behavior (see Figure 1) and hyper-parameter\"}"}
{"id": "yCZRdI0Y7G", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: The heatmap and the clustering of oracles based on relative AUC Top-10. Relative AUC Top-10 is computed by normalizing AUC Top-10 values to a range from the lowest and the highest value within the task. The zaleplon_mpo and sitagliptin_mpo are multi-objective versions of isomer functions [13], while all other MPOs are based on similarity. Clear patterns emerge between a large cluster of similarity-based oracles, four isomer-based oracles, and other non-clustered ones. Different types of landscape are more suitable for different kinds of methods to explore. The cluster tree was calculated with unweighted pair group method with arithmetic mean (UPGMA) using Euclidean distance.\\n\\nAs shown from Figure 3 and Table 2, we find that there are some clear clusters of oracles based on the relative performance of methods. One clear pattern is that string-based GAs, such as SMILES GA and STONED, reach superior relative performance in tasks involving isomer functions, including isomer_c7h8n2o2, isomer_c9h10n2o2pf2cl, sitagliptin_mpo, and zaleplon_mpo. Isomer-type oracles are summations of atomic contribution, while all other MPOs are mainly based on similarity measured by fingerprints, and they generally have closer relative performance. Among similarity-based oracles, the ones including logP and TPSA, such as fexofenadine_mpo and osimertinib_mpo, are clustered together against more naive similarities such as the rediscovery and median ones. The machine learning oracles predicting bioactivities belong to the same cluster of similarity-based oracles. While QED is too trivial that almost all methods reach very close values, deco_hop, valsartan_smarts, scaffold_hop that are designed based on whether a molecule contains a substructure have varied performance. The results suggest that different types of landscape are better explored by different kinds of methods, such as string-based GA on isomer-type oracles. It is not evident which type of oracle is closest to a \u201ctrue\u201d pharmaceutical design objective, which is likely more complex and challenging to optimize; we leave further investigation on oracle landscapes and their influence on optimization to future work.\\n\\nHyperparameter reoptimization and multiple runs are required when reporting results. We also observed that the optimal set of hyper-parameters is always not the default ones suggested by a method's original paper (see Appendix D.2). For example, REINVENT's performance is highly dependent on \\\\( \\\\_\\\\_ \\\\); we found the best-performing value to be much larger than the values suggested in the original paper (see Figure 15 and 14) [5]. We conclude that this is due to unique demands of our setting of limited oracle budget, which was not a goal of the original study, and thus suggest reoptimizing the hyper-parameters whenever the testing environment is changed. Another challenge is the non-determinism of most algorithms. For example, Graph GA suffers from a relatively\"}"}
{"id": "yCZRdI0Y7G", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"large variance due to its random-walk-like exploration, as does GP BO. If the oracle were a costly experimental evaluation, we might consider the worst-case performance as an endpoint to reduce the risk rather than the average performance, highlighting the importance of running multiple independent runs and reporting the distribution of outcomes.\\n\\n4 Conclusions\\n\\nThis paper proposes PMO: a standardized molecular design benchmark focusing on sample efficiency as a key impediment to experimental adoption. We conduct a thorough investigation across 25 methods and 23 objectives to determine the current state-of-the-art, investigate problems, and draw insights for future studies. Our primary observations are that (1) methods considered to be strong baselines, like LSTM HC, may be inefficient in data usage; (2) several older methods, like REINVENT and Graph GA, outperform more recent ones; (3) SELFIES does not seem to offer an immediate benefit in optimization performance compared to SMILES except in GA; (4) model-based methods have the potential to be more sample efficient but require careful design of the inner-loop, outer-loop, and the predictive model; and (5) different optimization algorithms may excel at different tasks, determined by the landscapes of oracle functions; which algorithm to select is still dependent on the use case and the type of tasks.\\n\\nWe acknowledge several limitations of the current study: we cannot exhaustively explore every method and thoroughly tune every hyperparameter, the representative methods we implement might not be the best-in-class among all possible variants, our conclusion might be biased toward similarity-based oracles, and we are not thoroughly investigating other important quantities such as synthesizability and diversity. We also emphasize that our experiments consider the number of oracle calls from scratch, i.e., the data used to train the surrogate models in model-based methods are counted in the total budget. If a dataset has been collected previously, it may be prudent to train a surrogate model on this information and use a model-based method as illustrated by Tripp et al. We will support the continued development of this benchmark to minimize the wasted effort caused by non-reproducibility and poor baselines to boost the field's growth toward solving practical molecular design problems.\\n\\nWe would like to conclude with recommendations for subsequent studies: (1) When comparing baselines, it is important to run algorithms under the same oracle budgets; (2) For general-purpose molecular design algorithms, one should test on multiple types of oracles; (3) Conducting multiple independent runs and reporting the distribution of outcomes is critical for non-deterministic methods; (4) Whenever the tasks and testing environment are changed, hyperparameter tuning is necessary.\\n\\nAcknowledgments and Disclosure of Funding\\n\\nThis research was supported by the Office of Naval Research under grant number N00014-21-1-2195 and the Machine Learning for Pharmaceutical Discovery and Synthesis consortium. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Office of Naval Research. W.G. received additional funding from MIT-Takeda fellowship. T.F. and J.S. were supported by NSF award SCH-2205289, SCH-2014438, IIS-1838042, NIH award R01 1R01NS107291-01. We thank Samuel Goldman and John Bradshaw for commenting on the manuscript.\\n\\nReproducibility Statement\\n\\nAll code, parameters, and releasable data can be found at https://github.com/wenhao-gao/mol_opt, including instructions in a README file. All results generated in this experiment can be found at https://figshare.com/articles/dataset/Results_for_practival_molecular_optimization_PMO_benchmark/20123453. Appendix B describe the experimental setup, implementation details, datasets used, and hardware configuration.\"}"}
{"id": "yCZRdI0Y7G", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Jan H Jensen. A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. *Chemical science*, 10(12):3567\u20133572, 2019.\\n\\n[2] Yutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu, and Lei Li. MARS: Markov molecular sampling for multi-objective drug discovery. In *ICLR*, 2021.\\n\\n[3] David E Graff, Eugene I Shakhnovich, and Connor W Coley. Accelerating high-throughput virtual screening through molecular pool-based active learning. *Chemical science*, 12(22):7866\u20137881, 2021.\\n\\n[4] Francesco Gentile, Jean Charle Yaacoub, James Gleave, Michael Fernandez, Anh-Tien Ton, Fuqiang Ban, Abraham Stern, and Artem Cherkasov. Artificial intelligence\u2013enabled virtual screening of ultra-large chemical libraries with deep docking. *Nature Protocols*, pages 1\u201326, 2022.\\n\\n[5] Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de-novo design through deep reinforcement learning. *Journal of cheminformatics*, 9(1):1\u201314, 2017.\\n\\n[6] Rafael G\u00f3mez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Benjam\u00edn S\u00e1nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al\u00e1n Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. *ACS central science*, 2018.\\n\\n[7] Matt J Kusner, Brooks Paige, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Grammar variational autoencoder. In *International Conference on Machine Learning*, pages 1945\u20131954. PMLR, 2017.\\n\\n[8] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. *ICML*, 2018.\\n\\n[9] Mario Krenn, Florian H\u00e4se, Akshat Kumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation. *Machine Learning: Science and Technology*, 1(4):045024, 2020.\\n\\n[10] Yoshua Bengio, Tristan Deleu, Edward J. Hu, Salem Lahlou, Mo Tiwari, and Emmanuel Bengio. GFlowNet foundations. *CoRR*, abs/2111.09266, 2021.\\n\\n[11] John Bradshaw, Brooks Paige, Matt J Kusner, Marwin Segler, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Barking up the right tree: an approach to search over molecule synthesis dags. *Advances in Neural Information Processing Systems*, 33:6852\u20136866, 2020.\\n\\n[12] Wenhao Gao, Roc\u00edo Mercado, and Connor W Coley. Amortized tree generation for bottom-up synthesis planning and synthesizable molecular design. *International Conference on Learning Representations*, 2022.\\n\\n[13] Nathan Brown, Marco Fiscato, Marwin HS Segler, and Alain C Vaucher. GuacaMol: benchmarking models for de novo molecular design. *Journal of chemical information and modeling*, 59(3):1096\u20131108, 2019.\\n\\n[14] Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Connor W Coley, Cao Xiao, Jimeng Sun, and Marinka Zitnik. Therapeutics data commons: Machine learning datasets and tasks for therapeutics. *NeurIPS Track Datasets and Benchmarks*, 2021.\\n\\n[15] Austin Tripp, Gregor NC Simm, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. A fresh look at de novo molecular design benchmarks. In *NeurIPS 2021 AI for Science Workshop*, 2021.\\n\\n[16] Zhenpeng Zhou, Steven Kearnes, Li Li, Richard N Zare, and Patrick Riley. Optimization of molecules via deep reinforcement learning. *Scientific reports*, 9(1):1\u201310, 2019.\\n\\n[17] Akshat Kumar Nigam, Pascal Friederich, Mario Krenn, and Al\u00e1n Aspuru-Guzik. Augmenting genetic algorithms with deep neural networks for exploring the chemical space. In *ICLR*, 2020.\"}"}
{"id": "yCZRdI0Y7G", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sai Krishna Gottipati, Boris Sattarov, Sufeng Niu, Yashaswi Pathak, Haoran Wei, Shengchao Liu, Simon Blackburn, Karam Thomas, Connor Coley, Jian Tang, et al. Learning to navigate the synthetically accessible chemical space using reinforcement learning. In International Conference on Machine Learning, pages 3668\u20133679. PMLR, 2020.\\n\\nKsenia Korovina, Sailun Xu, Kirthevasan Kandasamy, Willie Neiswanger, Barnabas Poczos, Jeff Schneider, and Eric Xing. ChemBO: Bayesian optimization of small organic molecules with synthesizable recommendations. In International Conference on Artificial Intelligence and Statistics, pages 3393\u20133403. PMLR, 2020.\\n\\nTianfan Fu, Wenhao Gao, Cao Xiao, Jacob Yasonik, Connor W Coley, and Jimeng Sun. Differentiable scaffolding tree for molecular optimization. International Conference on Learning Representations, 2022.\\n\\nEmmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nNatalie Maus, Haydn T Jones, Juston S Moore, Matt J Kusner, John Bradshaw, and Jacob R Gardner. Local latent space bayesian optimization over structured inputs. arXiv preprint arXiv:2201.11872, 2022.\\n\\nAntoine Grosnit, Rasul Tutunov, Alexandre Max Maraval, Ryan-Rhys Griffiths, Alexander I Cowen-Rivers, Lin Yang, Lin Zhu, Wenlong Lyu, Zhitang Chen, Jun Wang, et al. High-dimensional Bayesian optimisation with variational autoencoders and deep metric learning. arXiv preprint arXiv:2106.03609, 2021.\\n\\nG Richard Bickerton, Gaia V Paolini, J\u00e9r\u00e9my Besnard, Sorel Muresan, and Andrew L Hopkins. Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):90, 2012.\\n\\nJiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. Advances in neural information processing systems, 31, 2018.\\n\\nRegine S Bohacek, Colin McMartin, and Wayne C Guida. The art and practice of structure-based drug design: a molecular modeling perspective. Medicinal research reviews, 16(1):3\u201350, 1996.\\n\\nBrian Goldman, Steven Kearnes, Trevor Kramer, Patrick Riley, and W Patrick Walters. Defining levels of automated chemical design. Journal of Medicinal Chemistry, 2022.\\n\\nWenhao Gao, Priyanka Raghavan, and Connor W Coley. Autonomous platforms for data-driven organic synthesis. Nature Communications, 13(1):1\u20134, 2022.\\n\\nAkshatKumar Nigam, Robert Pollice, Mario Krenn, Gabriel dos Passos Gomes, and Alan Aspuru-Guzik. Beyond generative models: superfast traversal, optimization, novelty, exploration and discovery (STONED) algorithm for molecules using SELFIES. Chemical science, 12(20):7079\u20137090, 2021.\\n\\nHenry Moss, David Leslie, Daniel Beck, Javier Gonzalez, and Paul Rayson. BOSS: Bayesian optimization over string spaces. Advances in neural information processing systems, 33:15476\u201315486, 2020.\\n\\nBenjamin Sanchez-Lengeling, Carlos Outeiral, Gabriel L Guimaraes, and Alan Aspuru-Guzik. Optimizing distributions over molecular space. an objective-reinforced generative adversarial network for inverse-design chemistry (ORGANIC). 2017.\\n\\nNicola De Cao and Thomas Kipf. MolGAN: An implicit generative model for small molecular graphs. arXiv preprint arXiv:1805.11973, 2018.\\n\\nTianfan Fu, Cao Xiao, Xinhao Li, Lucas M Glass, and Jimeng Sun. MIMOSA: Multi-constraint molecule sampling for molecule optimization. AAAI, 2021.\"}"}
{"id": "yCZRdI0Y7G", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Multi-objective molecule generation using interpretable substructures. In International Conference on Machine Learning, pages 4849\u20134859. PMLR, 2020.\\n\\nSoojung Yang, Doyeong Hwang, Seul Lee, Seongok Ryu, and Sung Ju Hwang. Hit and lead discovery with explorative RL and fragment-based molecule generation. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nJulien Horwood and Emmanuel Noutahi. Molecular design in synthetically accessible chemical space via deep reinforcement learning. ACS omega, 5(51):32984\u201332994, 2020.\\n\\nCynthia Shen, Mario Krenn, Sagi Eppel, and Alan Aspuru-Guzik. Deep molecular dreaming: Inverse machine learning for de-novo molecular design and interpretability with surjective representations. Machine Learning: Science and Technology, 2021.\\n\\nDavid Weininger. SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):31\u201336, 1988.\\n\\nJakob Lykke Andersen, Christoph Flamm, Daniel Merkle, and Peter F Stadler. Chemical graph transformation with stereo-information. In International Conference on Graph Transformation, pages 54\u201369. Springer, 2017.\\n\\nLagnajit Pattanaik, Octavian-Eugen Ganea, Ian Coley, Klavs F Jensen, William H Green, and Connor W Coley. Message passing networks for molecules with tetrahedral chirality. arXiv preprint arXiv:2012.00094, 2020.\\n\\nKeir Adams, Lagnajit Pattanaik, and Connor W Coley. Learning 3D representations of molecular chirality with invariance to bond rotations. International Conference on Learning Representations, 2022.\\n\\nTeague Sterling and John J Irwin. ZINC 15\u2013ligand discovery for everyone. Journal of chemical information and modeling, 55(11):2324\u20132337, 2015.\\n\\nFredrik Svensson, Ulf Norinder, and Andreas Bender. Improving screening efficiency through iterative screening using docking and conformal prediction. Journal of chemical information and modeling, 57(3):439\u2013444, 2017.\\n\\nJos\u00e9 Miguel Hern\u00e1ndez-Lobato, James Requeima, Edward O Pyzer-Knapp, and Al\u00e1n Aspuru-Guzik. Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space. In International conference on machine learning, pages 1470\u20131479. PMLR, 2017.\\n\\nLaeeq Ahmed, Valentin Georgiev, Marco Capuccini, Salman Toor, Wesley Schaal, Erwin Laure, and Ola Spjuth. Efficient iterative virtual screening with Apache Spark and conformal prediction. Journal of cheminformatics, 10(1):1\u20138, 2018.\\n\\nFrancesco Gentile, Vibudh Agrawal, Michael Hsing, Anh-Tien Ton, Fuqiang Ban, Ulf Norinder, Martin E Gleave, and Artem Cherkasov. Deep docking: A deep learning platform for augmentation of structure based drug discovery. ACS central science, 6(6):939\u2013949, 2020.\\n\\nDavid E Graff, Matteo Aldeghi, Joseph A Morrone, Kirk E Jordan, Edward O Pyzer-Knapp, and Connor W Coley. Self-focusing virtual screening with active design space pruning. arXiv preprint arXiv:2205.01753, 2022.\\n\\nNaruki Yoshikawa, Kei Terayama, Masato Sumita, Teruki Homma, Kenta Oono, and Koji Tsuda. Population-based de novo molecule generation, using grammatical evolution. Chemistry Letters, 47(11):1431\u20131434, 2018.\\n\\nBobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104(1):148\u2013175, 2015.\"}"}
{"id": "yCZRdI0Y7G", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "yCZRdI0Y7G", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. \\\\textit{arXiv preprint arXiv:1511.05493}, 2015.\\n\\nGabriel Lima Guimaraes, Benjamin Sanchez-Lengeling, Carlos Outeiral, Pedro Luis Cunha Farias, and Al\u00e1n Aspuru-Guzik. Objective-reinforced generative adversarial networks (ORGAN) for sequence generation models. \\\\textit{arXiv preprint arXiv:1705.10843}, 2017.\\n\\nJasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of machine learning algorithms. \\\\textit{Advances in neural information processing systems}, 25, 2012.\\n\\nChoon Hui Teo and S.V .N. Vishwanathan. Fast and space efficient string kernels using suffix arrays. In \\\\textit{Proceedings of the 23rd international conference on Machine learning}, pages 929\u2013936, 2006.\\n\\nLukas Biewald. Experiment tracking with weights and biases, 2020. Software available from \\\\textit{wandb.com}.\\n\\nChecklist\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n   (b) Did you describe the limitations of your work? [Yes] See Section 4.\\n   (c) Did you discuss any potential negative societal impacts of your work? [N/A]\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n   (b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes]\\n   All code, parameters, and releasable data can be found at \\\\textit{https://github.com/wenhao-gao/mol_opt}, including instructions in a README file. Appendix B describe the experimental setup, implementation details, datasets used, and hardware configuration.\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Please see Section B.\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] Please see Table 2.\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Please see Section C.2.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... \\n   (a) If your work uses existing assets, did you cite the creators? [Yes]\\n   (b) Did you mention the license of the assets? [Yes] Please see Section C.3 for details.\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We release the code repository at \\\\textit{https://github.com/wenhao-gao/mol_opt}, including instructions in a README file. Appendix B and C describe the experimental setup, implementation details, datasets used, and hardware configuration.\\n   (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes]\\n   All the data/codes we use are publicly available. Please see Section B for details.\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]\\n   Our paper does not involve human subjects research. It also does not contain any personally identifiable information or offensive content.\"}"}
{"id": "yCZRdI0Y7G", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. If you used crowdsourcing or conducted research with human subjects...\\n\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\\n\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\"}"}
