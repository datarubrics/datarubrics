{"id": "VtEEpi-dGlt", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kantorovich Strikes Back!\\n\\nWasserstein GANs are not Optimal Transport?\\n\\nAlexander Korotin\\nSkolkovo Institute of Science and Technology\\nArtificial Intelligence Research Institute\\nMoscow, Russia\\na.korotin@skoltech.ru\\n\\nAlexander Kolesov\\nSkolkovo Institute of Science and Technology\\nMoscow, Russia\\na.kolesov@skoltech.ru\\n\\nEvgeny Burnaev\\nSkolkovo Institute of Science and Technology\\nArtificial Intelligence Research Institute\\nMoscow, Russia\\ne.burnaev@skoltech.ru\\n\\nAbstract\\n\\nWasserstein Generative Adversarial Networks (WGANs) are the popular generative models built on the theory of Optimal Transport (OT) and the Kantorovich duality. Despite the success of WGANs, it is still unclear how well the underlying OT dual solvers approximate the OT cost (Wasserstein-1 distance, $\\\\mathcal{W}_1$) and the OT gradient needed to update the generator. In this paper, we address these questions.\\n\\nWe construct 1-Lipschitz functions and use them to build ray monotone transport plans. This strategy yields pairs of continuous benchmark distributions with the analytically known OT plan, OT cost and OT gradient in high-dimensional spaces such as spaces of images. We thoroughly evaluate popular WGAN dual form solvers (gradient penalty, spectral normalization, entropic regularization, etc.) using these benchmark pairs. Even though these solvers perform well in WGANs, none of them faithfully compute $\\\\mathcal{W}_1$ in high dimensions. Nevertheless, many provide a meaningful approximation of the OT gradient. These observations suggest that these solvers should not be treated as good estimators of $\\\\mathcal{W}_1$, but to some extent they indeed can be used in variational problems requiring the minimization of $\\\\mathcal{W}_1$.\\n\\nThe Wasserstein-1 distance $\\\\mathcal{W}_1$ is a popular loss function to learn generative models. It has numerous advantages compared to the vanilla GAN loss $\\\\mathcal{L}_{gan}$. For example, $\\\\mathcal{W}_1$ is correctly defined if the distributions' supports differ. Besides, it correlates with the sample quality, provides improved stability of the optimization process and does not suffer from the vanishing gradients issue.\\n\\nGenerative models which employ $\\\\mathcal{W}_1$ as the loss to update the generator are called the Wasserstein GANs (WGANs). To compute $\\\\mathcal{W}_1$, they use its variational approximation based on the Kantorovich duality and the Optimal Transport (OT) theory. Since the introduction of the original WGANs with the weight clipping method, a lot of alternative techniques (neural dual OT solvers) to compute $\\\\mathcal{W}_1$ have been proposed: gradient penalties, entropic regularization, architectural constraints, batch-based methods, maximin methods, etc.\\n\\nDespite the popularity of WGANs, it still remains unclear to what extent their success is connected to OT and $\\\\mathcal{W}_1$ rather than, e.g., to a good choice of regularization. Due to the limited amount of pairs of distributions with known $\\\\mathcal{W}_1$, it is challenging to evaluate existing dual OT solvers.\\n\\nContributions.\\n\\nWe develop a generic methodology based on the transport rays to evaluate dual OT solvers for the Wasserstein-1 distance $\\\\mathcal{W}_1$. Our main contributions are as follows:\"}"}
{"id": "VtEEpi-dGlt", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use $1$-Lipschitz functions to construct pairs of continuous distributions that we use as a benchmark with analytically-known OT cost, map and gradient for $W_1$ transport (M3.2, M3.3).\\n\\nWe use these benchmark distributions to evaluate (M4) popular WGAN dual OT solvers (M2) in high-dimensional spaces, including the spaces of $32 \\\\times 32$ CIFAR-10 images, $64 \\\\times 64$ CelebA faces.\\n\\nRelated works [32, 49] consider discrete distributions and show that some solvers fail to estimate $W_1$. In contrast to them, we study how well the solvers compute the gradient of $W_1$ (OT gradient), as it is the OT gradient which is used to update the generator in WGANs, not the value of $W_1$. We use continuous distributions since in the discrete case the OT gradient may be ill-defined (M1).\\n\\nNotation. We work in the $\\\\mathbb{R}^D$ space that is endowed with the Euclidean norm $|| \\\\cdot ||_2$. We use $\\\\mu_L$ to denote the Lesbegue measure on $\\\\mathbb{R}^D$. For a measurable map $T: \\\\mathbb{R}^D \\\\to \\\\mathbb{R}^D$, we denote the associated pushforward operator by $T^\\\\#$. We consider Borel probability distributions $P, Q$ on $\\\\mathbb{R}^D$ with finite first moments. We use $\\\\Pi(P, Q)$ to denote the set of probability distributions on $\\\\mathbb{R}^D \\\\times \\\\mathbb{R}^D$ with marginals $P$ and $Q$ (transport plans). All the integrals are computed over $\\\\mathbb{R}^D$, if not stated otherwise.\\n\\nWe write $|| f ||_{L_\\\\leq C}$ if $f: \\\\mathbb{R}^D \\\\to \\\\mathbb{R}$ is $C$-Lipschitz.\\n\\n1 Background on Optimal Transport\\n\\nPrimal Formulation. For distributions $P, Q$, the Monge's formulation of the Wasserstein-1 ($W_1$) distance, i.e., OT with the distance cost function $|| x - y ||_2$, is given by (Figure 1a)\\n\\n$$W_1(P, Q) \\\\defeq \\\\min_{T^\\\\#P = Q} \\\\int || x - T(x) ||_2 dP(x),$$\\n\\nwhere $\\\\min$ is taken over measurable functions $T: \\\\mathbb{R}^D \\\\to \\\\mathbb{R}^D$ (transport maps) that map $P$ to $Q$. The optimal $T^*$ is called the optimal transport map (OT map). Note that (1) is not symmetric, and this formulation does not allow for mass splitting, i.e., for some $P, Q$, there is no map $T$ that satisfies $T^\\\\#P = Q$ [40, Remark 2.4]. Thus, Kantorovich [20] proposed the following relaxation (Figure 1b):\\n\\n$$W_1(P, Q) \\\\defeq \\\\min_{\\\\pi \\\\in \\\\Pi(P, Q)} \\\\int_{\\\\mathbb{R}^D \\\\times \\\\mathbb{R}^D} || x - y ||_2 d\\\\pi(x, y),$$\\n\\nwhere $\\\\min$ is taken over transport plans $\\\\pi \\\\in \\\\Pi(P, Q)$. The optimal $\\\\pi^* \\\\in \\\\Pi(P, Q)$ is called the optimal transport plan (OT plan). If $\\\\pi^* = [id_{\\\\mathbb{R}^D}, T^*]^\\\\#P$ for some map $T^*: \\\\mathbb{R}^D \\\\to \\\\mathbb{R}^D$, then $T^*$ minimizes formulation (1). In general, there might exist more than one OT plan $\\\\pi^*$ or OT map $T^*$.\\n\\n(a) Monge's OT formulation (1).\\n(b) Kantorovich's OT formulation (2).\\n\\nFigure 1: Monge's and Kantorovich's OT formulations of the Wasserstein-1 distance ($W_1$).\\n\\nDual formulation. For distributions $P, Q$, the dual formulation of $W_1$ is given by [52, Thm. 5.10]:\\n\\n$$W_1(P, Q) = \\\\max_{f \\\\oplus g \\\\leq ||\\\\cdot||_2} \\\\int f(x) dP(x) + \\\\int g(y) dQ(y),$$\\n\\nwhere $\\\\max$ is taken over $f, g: \\\\mathbb{R}^D \\\\to \\\\mathbb{R}$ satisfying $f(x) + g(y) \\\\leq ||x - y||_2$ for all $x, y \\\\in \\\\mathbb{R}^D$. By using the $c$-transform $f_c(y) \\\\defeq \\\\min_{x \\\\in \\\\mathbb{R}^D} \\\\{ ||x - y||_2 - f(x) \\\\}$ [52, M5], one rewrites (3) as\\n\\n$$W_1(P, Q) = \\\\max_{f} \\\\int f(x) dP(x) + \\\\int f_c(y) dQ(y).$$\\n\\n(4)\\n\\nIn accordance with [52, Case 5.16], dual form (4) can be further restricted to $1$-Lipschitz functions. In this case, it holds $f_c(y) = -f(y)$ [52, Case 5.4], and the alternative duality formula for $W_1$ is\\n\\n$$W_1(P, Q) = \\\\max || f ||_{L \\\\leq 1} \\\\int f(x) dP(x) - \\\\int f(y) dQ(y).$$\\n\\n(5)\\n\\nIn WGAN literature [17, 2], function $f$ is typically called the critic (or discriminator). In OT literature [52, 47, 51], functions $f, f_c, g$ are commonly refered as the (Kantorovich) potentials.\"}"}
{"id": "VtEEpi-dGlt", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The anti-gradient $-\\\\nabla f^* (x)$ shows where to move the mass of each $x = G(\\\\alpha)(z)$ to make the generated $P_{\\\\alpha}$ closer to $Q$ in $W_1$.\\n\\nOptimal transport in GANs. Derivatives of $W_1$ are used implicitly in generative modeling [3, 39, 35, 17, 38, 46, 48, 28] that incorporates $W_1$ loss, in which $P_{\\\\alpha} = P_{\\\\alpha}^G$ is the distribution generated from a fixed latent distribution $S$ by a generator network $G(\\\\alpha)$. The goal is to find parameters $\\\\alpha$ that minimize $W_1(P_{\\\\alpha}, Q)$ via gradient descent. The loss function for the generator is:\\n\\n$$W_1(P_{\\\\alpha}, Q) = Z_z f^* (G(\\\\alpha)(z)) dS(z) - Z_y f^*(y) dQ(y),$$\\n\\nwhere $f^*$ is the optimal potential in (5). The loss derivative (Figure 2) is given by [13, Eq. 3]:\\n\\n$$\\\\frac{\\\\partial W_1(P_{\\\\alpha}, Q)}{\\\\partial \\\\alpha} = Z_z J_{\\\\alpha} G(\\\\alpha)(z)^T \\\\nabla f^* G(\\\\alpha)(z) dS(z),$$\\n\\nwhere $J_{\\\\alpha} G(\\\\alpha)(z)^T$ is the transpose of the Jacobian matrix of $G(\\\\alpha)(z)$ w.r.t. parameters $\\\\alpha$. This result still holds without assuming the potentials are fixed [3, Theorem 3] by the envelope theorem [33].\\n\\nIn practice, the optimal potential $f^*$ is unknown. Therefore, WGANs approximate it with a network $f_{\\\\theta}$: $R_D \\\\rightarrow R$ (potential) by maximizing (5), (4) or (3) via the stochastic gradient ascent (SGA). This is usually associated with evaluating the $W_1$ loss. However, note that the loss value plays no role in generator updates. Only the gradient $\\\\nabla f^*$ of the potential is needed. We call it the OT gradient. We use a generic phrase OT solver to refer to any algorithm which is capable of recovering $\\\\nabla f^*$.\\n\\nQuantitative evaluation of OT solvers. Existing solvers are typically tested as the loss in WGANs without evaluating the actual OT performance. The quality of the generated samples is evaluated by standard metrics such as FID [18] or IS [45]. These metrics do not provide understanding about the quality of the solver itself since they depend on components of the model that are not related to OT.\\n\\nIn [41, 32, 49], the authors use discrete $P, Q$ to show that some solvers imprecisely compute $W_1$. Their approach is not applicable to evaluation of the OT gradient, as $\\\\nabla f^*$ is ill-defined in the discrete case. For example, when $P = \\\\delta_0, Q = \\\\delta_1$, it holds that $f^* = -[x] + 0$ is an optimal potential, but it is not even differentiable at $x = 0 = \\\\text{Supp}(P)$.\\n\\n2 Neural Dual Solvers for the Wasserstein-1 Distance\\n\\nOur proposed benchmark is useful for testing any OT solver that computes $\\\\nabla f^*$ or $W_1$. We evaluate only neural solvers which are based on (5), (4), or (3) and used in WGANs. We provide an overview of these methods below. We group them by the dual formulations which they use.\\n\\nMost solvers approximate the potential by a network $f_{\\\\theta}$: $R_D \\\\rightarrow R$ and learn it via maximizing (5) with SGA on batches from $P, Q$. The main challenge is to enforce the 1-Lipschitz constraint for $f_{\\\\theta}$.\\n\\n\u230aWC\u230b In [3], the space $\\\\Theta$ of parameters is restricted to a compact, e.g., to a hypercube $[-c, c]^{\\\\dim \\\\theta}$.\\n\\nWith mild assumptions on the architecture, $f_{\\\\theta}$ is provably Lipschitz continuous with some unknown constant $C$, i.e., $||f_{\\\\theta}||_L \\\\leq C$. The main practical issue is tuning the boundary $c$ of the set.\\n\\n\u230aGP\u230b The authors of [17] prove that with mild assumptions on the OT plan $\\\\pi^*$, the equation $||\\\\nabla f^*(z)||^2 = 1$ holds almost surely for $z = tx + (1-t)y$ with $(x, y)$ distributed as the OT plan $\\\\pi^*$ and $t \\\\sim \\\\text{Uniform}[0, 1]$. Thus, they softly penalize $f$ for being not 1-Lipschitz and optimize $W_1(P, Q) \\\\approx \\\\max f Z_f(x) dP(x) - Z_f(y) dQ(y) - \\\\lambda R(f), \\\\lambda > 0$.\\n\\nThe gradient penalty $R_{\\\\text{GP}}(f)$ equals $R(||\\\\nabla f(r)||^2 - 1)^2 d\\\\mu(r)$, where $\\\\mu$ is the distribution of a random variable $r = xt + (1-t)y$ with $t \\\\sim \\\\text{Uniform}[0, 1]$ and $(x, y) \\\\sim P \\\\times Q$ (as $\\\\pi^*$ is unknown).\\n\\nIn [34], it is proved that (6) with $R = R_{\\\\text{GP}}$ is a specific OT formulation called congested OT.\"}"}
{"id": "VtEEpi-dGlt", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In [35], the authors optimize (5) and use the power iteration method [14] to normalize the weight matrices of linear layers of net $f_\\\\theta$ by their spectral norms. This provably makes $f$ globally Lipschitz continuous, which does not hold for [17, 39] which are based on the soft penalization (6).\\n\\nThe authors of [1] claim that spectral normalization negatively affects the expressiveness of the network. They propose to ortho-normalize weight matrices and use GroupSort activations [5]. Such networks $f_\\\\theta$ are $1$-Lipschitz and universally approximate $1$-Lipschitz functions [1, M4].\\n\\nBelow we overview methods based on (4) or (3) which do not require enforcing $1$-Lipschitz continuity for the potential. Unlike the above-mentioned methods, they mostly require 2 neural networks.\\n\\nIn [46, 48, 12, 6], the authors optimize the following unconstrained regularized form (3):\\n\\n$$W_1(P, Q) \\\\approx \\\\max_{f,g} Z_f(x) dP(x) + Z_g(y) dQ(y) - R(f, g),$$\\n\\nwhere $R(f, g)$ is the entropic or quadratic regularizer [46, Eq. 5] which softly penalizes the potentials $f, g$ for disobeying $f \\\\oplus g \\\\leq || \\\\cdot ||_2$. In practice, $f, g$ are neural networks $f_\\\\theta, g_\\\\omega: \\\\mathbb{D} \\\\to \\\\mathbb{R}$.\\n\\nThe authors of [31, 32] expand the dual formulation (4) with the $c$-transform:\\n\\n$$W_1(P, Q) = \\\\max_f Z_f(x) dP(x) + \\\\min_x \\\\in \\\\mathbb{D} [\\\\|x - y\\\\|_2 - f(y)] dQ(y).$$\\n\\nDuring optimization, they restrict the inner minimization to the current mini-batch from $P$. This leads to overestimation of the inner problem's solution since the minimum is taken over a restricted subset.\\n\\nRecently, a more tricky version of this approach appeared [27, M3]. We call it $\\\\text{MM:Bv2}$. In [38], the authors use a saddle point formulation equivalent to (8):\\n\\n$$W_1(P, Q) = \\\\max_f Z_f(x) dP(x) + \\\\min_H Z_{\\\\|H(y) - y\\\\|_2} - f(H(y)) dQ(y),$$\\n\\nwhere the minimization is performed over functions $H: \\\\mathbb{D} \\\\to \\\\mathbb{D}$. The authors use neural networks $f_\\\\theta$ and $H_\\\\omega$ to parametrize the potential and the minimizer of the inner problem ($\\\\text{mover}$). To train $\\\\theta, \\\\omega$, the authors apply stochastic gradient ascent/descent (SGAD) over mini-batches from $P, Q$.\\n\\nOne may also recover the OT gradient $\\\\nabla f^*$ from the OT map $T^*$. Consider the form\\n\\n$$W_1(P, Q) = \\\\max_g \\\\min_T Z_{\\\\|T(x) - x\\\\|_2} - g(T(x)) dP(x) + Z_g(y) dQ(y),$$\\n\\nwhich is a reversed [23] version of (9), i.e., the roles of $P, Q$ are swapped and $T: \\\\mathbb{D} \\\\to \\\\mathbb{D}$. For some optimal saddle points $(g^*, T^*)$ of (10) it holds that mover $T^*$ is an OT map [25, Lemma 4], [11, Lemma 2], [50], [8]. With mild assumptions on $P, Q$, one may recover $T^*$ and use the identity $\\\\nabla f^*(x) = x - T^*(x) \\\\|x - T^*(x)\\\\|_2$[7, M1] to obtain the OT gradient from $T^*$.\\n\\n3 Constructing Benchmark Distributions for Dual Solvers\\n\\nIn this section, we develop a generic methodology to construct pairs $(P, Q)$ with computable ground truth OT plan, OT cost and OT gradient. Our approach is inspired by the insights about OT plans in [47, M3.1], [16], [7, M3-7]. In M3.1, we give the required preliminaries. In M3.2, we provide our method to build benchmark pairs. We construct them in M3.3. The proofs are given in Appendix A.\\n\\n3.1 Ray Monotone Transport Plans and 1-Lipschitz Functions\\n\\nLet $u: \\\\mathbb{D} \\\\to \\\\mathbb{R}$ be a $1$-Lipschitz function. Recall that due to the Rademacher's theorem, $u$ is differentiable $\\\\mu_L$-almost everywhere. For every $x, y \\\\in \\\\mathbb{D}$ satisfying $u(x) - u(y) = \\\\|x - y\\\\|_2$ it holds that $u$ is affine on the segment $[x, y]$, i.e.,\\n\\n$$u(z) = tu(x) + (1 - t)u(y) \\\\text{ for } z = tx + (1 - t)y \\\\text{ with } t \\\\in [0, 1],$$\\n\\nMoreover, $u$ is differentiable at all $z \\\\in (x, y)$ and $\\\\nabla u(z) = x - y \\\\|x - y\\\\|_2$.\\n\\nFollowing [47, Definition 3.7], we call a transport ray any non-trivial (different from a singleton) segment $[x, y]$ such that $u(x) - u(y) = \\\\|x - y\\\\|_2$, which is maximal for the inclusion among segments of this form. The unit vector $\\\\frac{x - y}{\\\\|x - y\\\\|_2}$ is called the direction of a transport ray. Two transport rays can only intersect at their boundary points [46, Corollary 3.8]. In general, not all points...\"}"}
{"id": "VtEEpi-dGlt", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) Samples \\\\(x \\\\sim P\\\\).\\n(b) 1-Lipschitz potential \\\\(u: \\\\mathbb{R}^D \\\\rightarrow \\\\mathbb{R}\\\\).\\n(c) Transport rays for \\\\(x\\\\) and map \\\\(y = T(x)\\\\).\\n(d) Pushforward samples \\\\(y = T(x) \\\\sim Q\\\\).\\n\\nFigure 3: Our methodology to construct benchmark pairs \\\\((P, Q)\\\\). An example with \\\\((D, N) = (2, 4)\\\\).\\n\\nWe pick a distribution \\\\(P\\\\) and a MinFunnel function \\\\(u: \\\\mathbb{R}^D \\\\rightarrow \\\\mathbb{R}\\\\) \\\\((11)\\\\). To sample \\\\(y \\\\sim Q\\\\), we get \\\\(x \\\\sim P\\\\), compute its transport ray and move \\\\(x\\\\) along the ray with the \\\\(u\\\\)-ray monotone map \\\\(T\\\\) \\\\((12)\\\\).\\n\\nBelong to transport rays. For example, for the function \\\\(u(x) = \\\\frac{1}{2}x\\\\) there are no transport rays at all.\\n\\nIn Figures 3c and 9, we provide examples of transport rays for 1-Lipschitz functions \\\\(u\\\\) in \\\\(D = 2\\\\).\\n\\nFor a 1-Lipschitz function \\\\(u: \\\\mathbb{R}^D \\\\rightarrow \\\\mathbb{R}\\\\), we say that a transport plan \\\\(\\\\pi \\\\in \\\\Pi(P, Q)\\\\) is \\\\(u\\\\)-ray monotone (decreasing) if \\\\(u(x) - u(y) = \\\\|x - y\\\\|_2\\\\) holds \\\\(\\\\pi\\\\)-almost surely for \\\\(x, y \\\\in \\\\mathbb{R}^D\\\\). The idea of the definition is that such plans distribute the probability mass of \\\\(x \\\\sim P\\\\) among \\\\(y \\\\in \\\\mathbb{R}^D\\\\) such that \\\\(y = x\\\\) (no mass movement) and/or \\\\(y\\\\) which lie on the same transport ray as \\\\(x\\\\) but below \\\\(x\\\\), i.e., \\\\(u(y) < u(x)\\\\).\\n\\nIf \\\\(x\\\\) is not contained in a transport ray, then \\\\(\\\\pi(y|x) = \\\\delta_x\\\\), i.e., \\\\(\\\\pi\\\\) necessarily does not move \\\\(x\\\\).\\n\\nProposition 1 (Ray monotone transport plans are optimal). Let \\\\(\\\\pi \\\\in \\\\Pi(P, Q)\\\\) be a \\\\(u\\\\)-ray monotone transport plan for a 1-Lipschitz function \\\\(u\\\\). Then it is an optimal plan between \\\\(P, Q\\\\). Besides, \\\\(u\\\\) is an optimal potential, i.e., it attains the maximum in dual formulation \\\\((5)\\\\).\\n\\nFor a distribution \\\\(P\\\\), we say that distribution \\\\(Q\\\\) is a \\\\(u\\\\)-ray-forward of \\\\(P\\\\) if there exists a measurable function \\\\(T: \\\\mathbb{R}^D \\\\rightarrow \\\\mathbb{R}^D\\\\) satisfying \\\\(T \\\\# P = Q\\\\) and \\\\(u(x) - u(T(x)) = \\\\|x - T(x)\\\\|_2\\\\) holds \\\\(P\\\\)-almost surely for all \\\\(x \\\\in \\\\mathbb{R}^D\\\\). We say that such a \\\\(T\\\\) is a \\\\(u\\\\)-ray-monotone transport map from \\\\(P\\\\) to \\\\(Q\\\\). Note that the deterministic plan \\\\(\\\\pi = [\\\\text{id}_{\\\\mathbb{R}^D}, T] \\\\# P\\\\) is \\\\(u\\\\)-ray monotone. We have the following corollary:\\n\\nCorollary 1 (Ray monotone transport maps are optimal). Let \\\\(T\\\\) be a \\\\(u\\\\)-ray monotone transport map from \\\\(P\\\\) to \\\\(Q\\\\). Then the plan \\\\(\\\\pi = [\\\\text{id}_{\\\\mathbb{R}^D}, T] \\\\# P\\\\) is optimal and \\\\(T\\\\) is an OT map from \\\\(P\\\\) to \\\\(Q\\\\).\\n\\nIn M.3.2 below, we derive our recipe to construct benchmark pairs \\\\((P, Q)\\\\) such that \\\\(Q\\\\) is a \\\\(u\\\\)-ray-forward of \\\\(P\\\\) with user-defined \\\\(u\\\\) and analytically known \\\\(T\\\\). In such pairs \\\\(P\\\\) is accessible by samples and is also possible to sample from \\\\(Q\\\\) by pushing \\\\(x \\\\sim P\\\\) forward by \\\\(T\\\\). Since \\\\(T\\\\) is an OT map, the ground truth OT cost is \\\\(W_1(P, Q) = \\\\int \\\\|x - T(x)\\\\|_2^2 dP(x)\\\\). It admits unbiased Monte Carlo estimates from samples \\\\(x \\\\sim P\\\\). Moreover, with mild assumptions \\\\((M.3.3)\\\\), \\\\(\\\\nabla u\\\\) is the unique OT gradient. Our benchmark pairs can be used to test how well OT solvers recover \\\\(W_1\\\\) and the OT gradient.\\n\\n3.2 Method to Construct Benchmark Pairs\\n\\nLet \\\\(u\\\\) be a known 1-Lipschitz function. We aim to find its transport rays and construct a \\\\(u\\\\)-ray-forward map \\\\(T\\\\) and distribution \\\\(Q = T \\\\# P\\\\) (for a given \\\\(P\\\\)) for testing dual OT solvers. First, we describe the parametric class of 1-Lipschitz functions \\\\(u\\\\) which we employ in our benchmark. Second, we explain how to compute the transport rays of \\\\(u\\\\). Finally, we explain how to define \\\\(u\\\\)-ray monotone maps.\\n\\nPart 1. Parameterizing 1-Lipschitz functions. Inspired by the distance representation of optimal potentials \\\\([16]\\\\), as \\\\(u: \\\\mathbb{R}^D \\\\rightarrow \\\\mathbb{R}\\\\), we employ the following 1-Lipschitz MinFunnel functions:\\n\\n\\\\[\\nu(x) \\\\overset{\\\\text{def}}{=} \\\\min_n \\\\{u_n(x)\\\\} = \\\\min_n \\\\{\\\\|x - a_n\\\\|_2 + b_n\\\\},\\\\tag{11}\\\\]\\n\\nwhere \\\\(a_n \\\\in \\\\mathbb{R}^D\\\\) and \\\\(b_n \\\\in \\\\mathbb{R}\\\\) are the parameters. Each funnel \\\\(u_n(x) = \\\\|x - a_n\\\\|_2 + b_n\\\\) has \\\\(\\\\|\\\\nabla u_n(x)\\\\|_2 = 1\\\\) when \\\\(x \\\\neq a_n\\\\). Thus, \\\\(u\\\\) is also 1-Lipschitz as it is their minimum. Note that for \\\\(\\\\mu_L\\\\)-almost every \\\\(x\\\\) it holds that \\\\(u\\\\) is differentiable at \\\\(x\\\\) and \\\\(\\\\|\\\\nabla u(x)\\\\|_2 = 1\\\\).\\n\\nProposition 2 (MinFunnels are universal approximators of 1-Lipschitz functions on compact sets). Let \\\\(S \\\\subset \\\\mathbb{R}^D\\\\) be a compact set and \\\\(f^*: S \\\\rightarrow \\\\mathbb{R}\\\\) be a 1-Lipschitz function. Then for every \\\\(\\\\epsilon > 0\\\\) there exists \\\\(N\\\\) and \\\\(\\\\{a_n, b_n\\\\}_{n=1}^N\\\\) such that function \\\\((11)\\\\) satisfies \\\\(\\\\sup_{x \\\\in S} |u(x) - f^*(x)| \\\\leq \\\\epsilon\\\\).\"}"}
{"id": "VtEEpi-dGlt", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) The pair with $N = 64$, $D = 4$.\\n\\n(b) The pair with $N = 4$, $D = 32$.\\n\\nFigure 4: Visualization of constructed high-dimensional benchmark pairs.\\n\\nIn each pair, we show random samples projected onto 2 principal components of $Q$.\\n\\nThe proposition is included only for completeness of the exposition and is not principal for our construction. We do not aim to approximate a specific $f^*$ by $u$. In practice, we found that simply picking a random MinFunnel $u$ and using it to construct a benchmark pair is reasonable (M 3.3).\\n\\nPart 2. Computing the transport rays. MinFunnels are practically very convenient as the transport ray for a given $x \\\\in \\\\mathbb{R}^D$ can be analytically computed in $O(ND)$ time, see our proposition below.\\n\\nProposition 3 (Transport rays of a MinFunnel).\\n\\nConsider MinFunnel (11) function $u: \\\\mathbb{R}^D \\\\rightarrow \\\\mathbb{R}$ with $n$ distinct centers $a_n$ such that for all $n_1 \\\\neq n_2$ it holds that $\\\\|a_{n_1} - a_{n_2}\\\\|_2 \\\\neq |b_{n_1} - b_{n_2}|$. Let $x \\\\in \\\\mathbb{R}^D$ be a point such that $u$ is differentiable at $x$. Then ray $(x) = [a_m, x + r \\\\cdot v]$, where $m \\\\text{def} = \\\\arg\\\\min_n \\\\{u_n(x)\\\\}$; $v \\\\text{def} = x - a_m \\\\|x - a_m\\\\|_2$; $r \\\\text{def} = \\\\min_n r_n \\\\in \\\\mathbb{R} \\\\cup \\\\{+\\\\infty\\\\}$, and $r_n \\\\text{def} = \\\\frac{1}{2} \\\\|a_n - x\\\\|_2^2 - |u(x) - b_n|^2$ for $n \\\\neq m$ and $r_m \\\\text{def} = +\\\\infty$. Here in the definition of $r$ the $\\\\min$ is taken only over $n$ for which $r_n > 0$ and $r_n \\\\geq |b_n - u_m(x)|$.\\n\\nThe condition on $a_n$, $b_n$ is imposed to avoid inconvenient cases when the center of a funnel lies on some another funnel. In practice, we compute the transport rays for a batch of points $x$ with tensor operations. We provide an example of transport rays of a random MinFunnel $u$ in Figure 9.\\n\\nPart 3. Defining the ray monotone map. Let $P$ be a distribution on $\\\\mathbb{R}^D$ and $u$ be a MinFunnel. We aim to construct a $u$-ray monotone map $T: \\\\mathbb{R}^D \\\\rightarrow \\\\mathbb{R}^D$ and define a distribution $Q = T^\\\\#P$.\\n\\nIf $\\\\nabla u(x)$ does not exist, we define $T(x) = x$ (the set of such points is $\\\\mu$-negligible). Otherwise, since $u$ is a MinFunnel, we have $\\\\|\\\\nabla u(x)\\\\|_2 = 1$. Thus, the $u$-ray forward map $T$ may take any value $T(x) \\\\in [x_0, x_1]$, where $x_0$ is the left (lower) endpoint of ray $(x) = [x_0, x_1]$. According to the analysis in M 3.1, any (measurable) map $T$ defined by this principle is $u$-ray monotone. For such a map $T$ one may put $Q = T^\\\\#P$. As a result, for the pair $(P, Q)$, function $T$ is an OT map (Corollary 1), possibly non-unique. Thus, the ground truth $W_1(P, Q)$ can be estimated from samples $x \\\\sim P$.\\n\\nAs we are also interested in testing how well OT dual solvers recover the OT gradient $\\\\nabla f^*$, we need this gradient to exist and be $P$-unique. From Proposition 1 we know that $f^* = u$ is an optimal potential. However, it is not necessarily unique (up to a constant). For example, in the trivial case $T(x) \\\\equiv x$ and $P = Q$, any 1-Lipschitz $f^*$ is optimal and $\\\\nabla f^*$ is not unique. We show that with mild assumptions on $P$, $T$, $u$, the gradient can be designed to be unique and match $\\\\nabla u(x)$.\\n\\nProposition 4 (Uniqueness of the OT gradient).\\n\\nLet $P$ be absolutely continuous and let $u: \\\\mathbb{R}^D \\\\rightarrow \\\\mathbb{R}$ be 1-Lipschitz. Let $T$ be a $u$-ray-monotone map for which $T(x) \\\\neq x$ holds $P$-almost surely, i.e., it moves every piece of mass of $P$. Define $Q = T^\\\\#P$. Then for every optimal $f^*$ which maximizes (5) the equality $\\\\nabla f^*(x) = \\\\nabla u(x)$ holds $P$-almost surely, i.e., the OT gradient $\\\\nabla u$ is $P$-unique.\\n\\n3.3 Benchmark Pairs\\n\\nWe use our methodology (M 3.2) to construct high-dimensional ($D = 2^2, 2^3, \\\\ldots, 2^7$) benchmark pairs and benchmark pairs on the space of $64 \\\\times 64$ RGB images of celebrity faces ($D = 12288$).\\n\\nFor convenience, we use absolutely continuous $P$ supported on a hypercube $S = [-B, B]^D \\\\subset \\\\mathbb{R}^D$.\\n\\nIn particular, for points $x \\\\in S$, we consider transport rays truncated to $S$, i.e., ray $(x) \\\\cap S$. We\"}"}
{"id": "VtEEpi-dGlt", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) Samples from the pair with \\\\((N, p) = (1, 10)\\\\).\\n\\n(b) Samples from the pair with \\\\((N, p) = (16, 100)\\\\).\\n\\n(c) Samples \\\\(x \\\\sim P(\\\\text{synthetic celebrity faces})\\\\).\\n\\n(d) Samples \\\\(y \\\\sim Q = T \\\\# P(N = 1 \\\\text{ funnel})\\\\).\\n\\n(e) Samples \\\\(y \\\\sim Q = T \\\\# P(N = 16 \\\\text{ funnels})\\\\).\\n\\nFigure 5: Visualization of random samples from our constructed Celeba images benchmark pairs.\\n\\nIn the last three plots, we show distributions projected to the 2 principal components of \\\\(P\\\\).\\n\\nWe construct the \\\\(u\\\\)-ray monotone map \\\\(T: \\\\mathbb{R}^D \\\\rightarrow \\\\mathbb{R}^D\\\\) by the following principle. Let \\\\([x_0, x_1] \\\\subset S\\\\) be the truncated transport ray of \\\\(u\\\\) for \\\\(x \\\\in S\\\\). We pick a parameter \\\\(p > 1\\\\) and define the map \\\\(T\\\\) as follows:\\n\\n\\\\[\\nT(x) = \\\\frac{\\\\|x - x_0\\\\|^2}{\\\\|x_0 - x_1\\\\|^2} x_0 + \\\\frac{1 - \\\\|x - x_0\\\\|^2}{\\\\|x_0 - x_1\\\\|^2} x_1.\\n\\\\]\\n\\nThis is a power function, i.e., if a ray is parametrized as \\\\([0, 1]\\\\), it moves the mass along the ray by \\\\(t \\\\rightarrow t^p\\\\). Since \\\\(P\\\\)-almost all the points in \\\\(S\\\\) belong to (truncated) rays, we have \\\\(T(x) \\\\neq x\\\\) on \\\\(S\\\\).\\n\\nHigh-dimensional benchmark pairs. In dimensions \\\\(D = 2, 2^2, \\\\ldots, 2^7\\\\), we put \\\\(P\\\\) to be the standard uniform distribution on \\\\(S = [-2.5, 2.5]^D\\\\). In each dimension \\\\(D\\\\), we consider \\\\(N = 4, 16, 64, 256\\\\) funnels and pick random parameters \\\\(a_n \\\\sim \\\\text{Uniform}([-2.5, 2.5]^D)\\\\) and \\\\(b_n \\\\sim \\\\mathcal{N}(0, 0.1)\\\\). For this initialization of \\\\(a_n, b_n\\\\), the assumptions of Proposition 3 hold with probability 1. The random seed is hardcoded. We use \\\\(p = 8\\\\). For the case \\\\(D = 2, N = 4\\\\), we visualize the input distribution \\\\(P\\\\), the function \\\\(u\\\\), the constructed map \\\\(T\\\\) and the output distribution \\\\(Q = T \\\\# P\\\\) in Figure 3. We show examples of constructed \\\\((P, Q)\\\\) for higher dimensions in Figure 4.\\n\\nImages benchmark pairs for CIFAR-10 and Celeba. As \\\\(P\\\\) we consider the synthetic distributions of generated \\\\(32 \\\\times 32\\\\) RGB images (\\\\(D = 3072\\\\)) and \\\\(64 \\\\times 64\\\\) RGB images (\\\\(D = 12288\\\\)). To generate these images, we use the WGAN-QC [28] generator model trained on CIFAR-10 [26] and Celeba [30] datasets, respectively. For CIFAR-10, we train WGAN-QC by using its publicly available code.\\n\\nFor Celeba, we pick a readily available pre-trained generator from the related Wasserstein-2 benchmark [23, M4.1]. To make \\\\(P\\\\) absolutely continuous, we add the Gaussian noise with \\\\(\\\\sigma = 0.01\\\\). Then we truncate the distribution to \\\\(S = [-1.1, 1.1]^D\\\\), i.e., we reject samples which are out of \\\\(S\\\\). We construct two benchmark pairs per each dataset (Figures 5, 6) with \\\\((N, p) = (1, 10), (16, 100)\\\\) and \\\\(a_n \\\\sim \\\\text{Uniform}([-1., 1.]^D), b_n \\\\sim \\\\mathcal{N}(0, 0.1)\\\\).\\n\\nIn both high-dimensional and images pairs \\\\((P, Q)\\\\), the input \\\\(P\\\\) is an absolutely continuous distribution supported on a hypercube \\\\(S\\\\). By the design, the distribution \\\\(P\\\\) is accessible by random samples. To sample from \\\\(Q\\\\), we first sample \\\\(x \\\\sim P\\\\), then compute its transport ray (Proposition 3), truncate it to \\\\(S\\\\), and produce \\\\(y = T(x) \\\\sim Q\\\\) by (12). The sampler for \\\\(Q\\\\) outputs only \\\\(y\\\\), i.e., information about \\\\(u, T, x\\\\) is hidden from the user and employed only when estimating the ground truth \\\\(W_1(P, Q)\\\\) or \\\\(\\\\nabla u\\\\).\\n\\n2 https://github.com/harryliew/WGAN-QC\\n\\n3 https://github.com/iamalexkorotin/Wasserstein2Benchmark\"}"}
{"id": "VtEEpi-dGlt", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) Samples from the pair with $(N, p) = (1, 10)$.\\n\\n(b) Samples from the pair with $(N, p) = (16, 100)$.\\n\\n(c) Samples $x \\\\sim P$ (synthetic CIFAR-10 images).\\n\\n(d) Samples $y \\\\sim Q = T \\\\# P$ ($N = 1$ funnel).\\n\\n(e) Samples $y \\\\sim Q = T \\\\# P$ ($N = 16$ funnels).\\n\\nFigure 6: Visualization of random samples from our constructed CIFAR-10 images benchmark pairs.\\n\\nIn the last three plots, we show distributions projected to 2 principal components of $P$.\\n\\nReversed pairs. Doing preliminary tests, we found that for solvers it is more challenging to compute the OT gradient for pair $(Q, P)$ rather than $(P, Q)$. In particular, in our images benchmark, the pair $(Q, P)$ reflects the practical WGAN scenario better. Indeed, in WGANs, the solvers move the generated distribution (bad images, $Q$ in our construction) to the real distribution (good images, $P$).\\n\\nRecall that $Q = T \\\\# P$, where $T$ is a differentiable bijection (12) along the (truncated) transport rays of a MinFunnel $u$. As a result, $Q$ is absolutely continuous and the OT gradient for the reverse $(Q, P)$ is $Q$-unique (Proposition 4). For this pair, the optimal potential is $-u$ and its gradient is $-\\\\nabla u$.\\n\\nTo conclude, in all the experiments, we feed the reverse pair $(P, Q) := (Q, P)$ to OT solvers in view.\\n\\n4 Evaluation of Dual Solvers and Discussion\\n\\nIn this section, we evaluate WGAN dual OT solvers ($M_2$) on our constructed benchmark ($M_3$). Technical details of the implementation are given in Appendix B. The code is written in PyTorch framework and is publicly available together with all the constructed benchmark distributions at https://github.com/justkolesov/Wasserstein1Benchmark.\\n\\nFor $W_1$, we do not use any specific metric but simply report obtained $c_{W_1}$ and the ground truth $W_1$. To quantify the recovered gradient $\\\\hat{\\\\nabla}f$, we use $L_2$ and cosine similarity metrics [23, M.4.2]:\\n\\n$\\\\begin{align*}\\nL_2(\\\\hat{\\\\nabla}f, \\\\nabla f^*) & \\\\overset{\\\\text{def}}{=} \\\\| \\\\hat{\\\\nabla}f - \\\\nabla f^* \\\\|_2 \\\\\\\\\\n\\\\cos(\\\\hat{\\\\nabla}f, \\\\nabla f^*) & \\\\overset{\\\\text{def}}{=} \\\\frac{\\\\langle \\\\hat{\\\\nabla}f, \\\\nabla f^* \\\\rangle_{L_2}}{\\\\|\\\\hat{\\\\nabla}f\\\\|_2 \\\\|\\\\nabla f^*\\\\|_2}\\n\\\\end{align*}$\\n\\n(13)\\n\\nwhere $\\\\langle \\\\nabla f_1(x), \\\\nabla f_2(x) \\\\rangle_{L_2} \\\\overset{\\\\text{def}}{=} \\\\int \\\\langle \\\\nabla f_1(x), \\\\nabla f_2(x) \\\\rangle dP(x)$, $\\\\|\\\\nabla f\\\\|_2 \\\\overset{\\\\text{def}}{=} \\\\langle \\\\nabla f_1, \\\\nabla f_2 \\\\rangle_{L_2}$, and $\\\\nabla f^*$ is the ground truth OT gradient. The $L_2$ metric compares the gradients $\\\\hat{\\\\nabla}f, \\\\nabla f^*$ as elements of the space $L_2(P)$ of quadratically integrable w.r.t. $P$ functions. The cosine compares their directions regardless of the magnitude (Figure 13). To estimate the metrics, we use $2^{13}$ samples $x \\\\sim P$.\\n\\nFor completeness, we add the empirical (batched) OT [9, 10, 36, 37] to evaluation. For batches $X \\\\sim P$, $Y \\\\sim Q$, we use $c_{W_1}(X, Y)$ computed by a discrete solver as an estimate of $W_1(P, Q)$.\\n\\nThe solver can be combined with the automatic differentiation to approximate $\\\\nabla f^*$ on the batch $X$.\\n\\nHigh-dimensional pairs. We test the solvers and report the estimated $W_1$ value and metrics (13) in Tables 9, 10, 11 (Appendix C). We use fully-connected nets as potentials $f_\\\\theta, g_\\\\omega$ and movers $T_\\\\theta, H_\\\\omega$ (in the maximin solvers). In Figure 7, we show the potentials learned by solvers for $D = 2$, $N = 4$. 8\"}"}
{"id": "VtEEpi-dGlt", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Surfaces of potentials $f_\\\\theta$ learned by OT solvers on the pair with $D=2$, $N=N$. For $\\\\lfloor \\\\text{MM:R} \\\\rfloor$ we plot (minus) second potential $-g_\\\\omega$ as the solver does not compute the first one. The ground truth optimal potential is in Figure 3b.\\n\\nAs Table 11 shows, all the solvers randomly over/underestimate $W_1$, typically with notable error. Consequently, none of the solvers can be viewed as precise estimators of the OT cost. Thus, our discussion below primarily focuses on the estimation of the OT gradient needed in WGANs.\\n\\nOriginal $\\\\lfloor \\\\text{WC} \\\\rfloor$ leads to a pathological value surface of the learned potential (Figure 7a), which is far from the ground truth. This agrees with the claims of [17, M3]. Despite this, even in high dimensions the $\\\\cos$ metric is positive (Table 9a), i.e., the recovered gradient correlates with the OT gradient. Popular $\\\\lfloor \\\\text{GP} \\\\rfloor$ notably improves upon $\\\\lfloor \\\\text{WC} \\\\rfloor$ and provides higher $\\\\cos$ values (Table 9b). According to the results, its modification $\\\\lfloor \\\\text{LP} \\\\rfloor$ does not provide any major improvement (Table 9c).\\n\\nSurprisingly, $\\\\lfloor \\\\text{SN} \\\\rfloor$ performs worse than $\\\\lfloor \\\\text{GP} \\\\rfloor$ and is comparable to $\\\\lfloor \\\\text{WC} \\\\rfloor$ in $\\\\cos$ metric (Table 9d). We presume that this happens since the spectral norm negatively affects the expressive power [1]. Even when $D=2$ (Figure 7d), $\\\\lfloor \\\\text{SN} \\\\rfloor$ fails to recover the optimal potential. $\\\\lfloor \\\\text{SO} \\\\rfloor$ replaces spectral norm with orthogonalization and uses GroupSort activations which improve performance. The solver scores higher $\\\\cos$ values (Table 9e) which are comparable to $\\\\lfloor \\\\text{GP} \\\\rfloor$.\\n\\nSolver $\\\\lfloor \\\\text{LS} \\\\rfloor$ recovers biased potential since (7) is the duality formula for regularized OT which yields biased optimal potentials (Table 9f). The bias is huge for large $D$, $N$ when the benchmark pairs are complex. This is analogous to results for the same solver in the Wasserstein-2 ($W_2$) benchmark [23].\\n\\nBatch-based $\\\\lfloor \\\\text{MM:B} \\\\rfloor$ suffers from the bias in high dimensions (Table 9g) due to the overestimation of the value of the inner problem in (8). Note that $\\\\cos$ metric values are even negative in high dimensions. The same is reported in the $W_2$ benchmark [23]. $\\\\lfloor \\\\text{MM-Bv2} \\\\rfloor$ uses a more tricky optimization scheme and yields high $\\\\cos$ values (Table 9h). It captures the direction of the OT gradient but extremely overestimates its magnitude, see high values of $L_2$ in Table 10h.\\n\\nMaximin solvers $\\\\lfloor \\\\text{MM} \\\\rfloor$, $\\\\lfloor \\\\text{MM:R} \\\\rfloor$ perform comparably to $\\\\lfloor \\\\text{GP} \\\\rfloor$, $\\\\lfloor \\\\text{SO} \\\\rfloor$, $\\\\lfloor \\\\text{LP} \\\\rfloor$, see Tables 9i, 9j. However, their training takes longer and sometimes diverges as it solves a saddle point problem. This agrees with the $W_2$ benchmark [23, M4.3]. Using these solvers in GANs is not easy as it yields a challenging min-max-min optimization problem. Importantly, $\\\\lfloor \\\\text{MM:R} \\\\rfloor$ recovers the OT map which can itself be used as a generative model (outside the context of GANs) in computer vision tasks. Here we refer the reader to the recent neural optimal transport (NOT) methods [25, 24, 4, 22, 11, 44, 8].\\n\\nEmpirical $\\\\lfloor \\\\text{DOT} \\\\rfloor$ provides precise estimates of $W_1$ (Table 11k) and the OT gradient (Table 9k) only in small dimensions. In high dimensions, it intolerably overestimates $W_1$; its gradient is almost orthogonal to the ground truth ($\\\\cos \\\\ll 0$). This is due to the exponential (in $D$) sample complexity of DOT [53]. Thus, (unregularized) DOT is an imprecise estimator of $W_1$ or the OT gradient in high $D$.\\n\\nImages pairs. Here we do not consider $\\\\lfloor \\\\text{SO} \\\\rfloor$ as its authors do not provide convolutional architectures with GroupSort and orthonormalization. We use DCGAN [42] as potentials $f_\\\\theta$, $g_\\\\omega$. In $\\\\lfloor \\\\text{MM} \\\\rfloor$ and $\\\\lfloor \\\\text{MM:R} \\\\rfloor$, the movers $T_\\\\theta$, $H_\\\\omega$ are UNets [43]. The evaluation results on Celeba benchmark pairs are given in Tables 6, 7, 8 and on CIFAR-10 benchmark pairs \u2013 in Tables 3, 4, 5 (Appendix C).\"}"}
{"id": "VtEEpi-dGlt", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: The summary of WGAN dual OT solvers\u2019 performance in \\\\( \\\\cos \\\\), \\\\( L^2 \\\\) and \\\\( W^1 \\\\) metrics on our high-dimensional (HD) and images (IMG) benchmark pairs. For details, see Appendix C.\\n\\nSurprisingly, our images benchmark pairs turned to be simpler than some high-dimensional benchmark pairs. Although \\\\((P, Q)\\\\) are absolutely continuous and supported on \\\\([-1, 1] \\\\times [-1, 1] \\\\), their actual probability mass is still concentrated around small low-dimensional sub-manifold of data. We suppose that this is one of the causes for the reasonable performance of most solvers. In particular, we see that even \\\\(\\\\lfloor \\\\text{DOT} \\\\rfloor, \\\\lfloor \\\\text{LS} \\\\rfloor, \\\\lfloor \\\\text{MM:B} \\\\rfloor\\\\) score \\\\(\\\\cos > 0\\\\) in images benchmark pairs, although they struggled to produce good results on high-dimensional pairs.\\n\\nSolvers \\\\(\\\\lfloor \\\\text{GP} \\\\rfloor, \\\\lfloor \\\\text{LP} \\\\rfloor, \\\\lfloor \\\\text{MM:R} \\\\rfloor\\\\) provide very high \\\\(\\\\cos > 0.9\\\\) (Table 6). However, only \\\\(\\\\lfloor \\\\text{MM:R} \\\\rfloor\\\\) provides precise approximation of \\\\(\\\\nabla f^*\\\\) in \\\\(L^2\\\\) norm (Table 7). Interestingly, maximin \\\\(\\\\lfloor \\\\text{MM} \\\\rfloor\\\\) diverges on our images benchmark pairs (tuning the hyper-parameters did not help). Similar to the evaluation in high-dimensional pairs, \\\\(\\\\lfloor \\\\text{WC} \\\\rfloor\\\\) and \\\\(\\\\lfloor \\\\text{SN} \\\\rfloor\\\\) show moderate \\\\(\\\\cos > 0\\\\), but its value is notably smaller than that of the top-performing methods. Solver \\\\(\\\\lfloor \\\\text{MM:Bv2} \\\\rfloor\\\\) demonstrates meaningful estimate of \\\\(W^1\\\\) (Table 8). Nevertheless, its recovered gradient is almost orthogonal to the ground truth (\\\\(\\\\cos \\\\approx 0\\\\)), see Table 6), and the values of \\\\(L^2\\\\) metric are extremely high (Table 7).\\n\\n5 Discussion\\n\\nOur methodology creates pairs of continuous distributions with known ground truth OT cost and gradient, filling the missing gap of benchmarking \\\\(W^1\\\\) dual solvers. This development allows us to evaluate the OT performance of WGAN dual methods. The experimental results are summarized in Table 1. Our evaluation shows that these solvers should not be considered as meaningful estimators of \\\\(W^1\\\\) as they exhibit large error. However, the OT gradient recovered by these solvers shows positive \\\\(\\\\cos\\\\) with ground truth. This suggests that most methods could still be used to minimize \\\\(W^1\\\\) in variational problems, e.g., Wasserstein GANs.\\n\\nComputational complexity. Evaluation of all the OT solvers on our high-dimensional and images benchmark pairs takes less than 50 hours on a single GPU GTX 1080ti (11 GB VRAM).\\n\\nPotential Impact. Our benchmark distributions can be used to evaluate future dual OT solvers in high-dimensional spaces, a crucial step to improve the transparency and replicability of OT and WGAN-related research. We expect our benchmark to become a standard benchmark for \\\\(W^1\\\\) optimal transport as part of the ongoing effort of advancing computational OT.\\n\\nLimitations (benchmark). We rely on MinFunnels as optimal Kantorovich potentials to generate benchmark pairs. Also, we limit our pairs to be absolutely continuous distributions. It is unclear whether our benchmark sufficiently reflects the real-world scenarios in which the WGAN solvers are used. Nevertheless, our methodology is generic and can be used to construct new benchmark pairs.\\n\\nLimitations (evaluation). We evaluate how well the OT solvers compute OT cost and gradient but do not assess their performance in GAN settings. Studying this question is a promising future research avenue which could help to develop new OT-based methods for generative modeling.\\n\\nACKNOWLEDGEMENTS. The work was supported by the Analytical center under the RF Government (subsidy agreement 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021).\"}"}
{"id": "VtEEpi-dGlt", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Cem Anil, James Lucas, and Roger Grosse. Sorting out lipschitz function approximation. In International Conference on Machine Learning, pages 291\u2013301. PMLR, 2019.\\n\\n[2] Martin Arjovsky and L\u00e9on Bottou. Towards principled methods for training generative adversarial networks. arXiv preprint arXiv:1701.04862, 2017.\\n\\n[3] Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein GAN. arXiv preprint arXiv:1701.07875, 2017.\\n\\n[4] Arip Asadulaev, Alexander Korotin, Vage Egiazarian, and Evgeny Burnaev. Neural optimal transport with general cost functionals. arXiv preprint arXiv:2205.15403, 2022.\\n\\n[5] Artem Chernodub and Dimitri Nowicki. Norm-preserving orthogonal permutation linear unit activation functions (oplu). arXiv preprint arXiv:1604.02313, 2016.\\n\\n[6] Grady Daniels, Tyler Maunu, and Paul Hand. Score-based generative neural networks for large-scale optimal transport. Advances in Neural Information Processing Systems, 34, 2021.\\n\\n[7] Lawrence C Evans and Wilfrid Gangbo. Differential equations methods for the Monge-Kantorovich mass transfer problem. Number 653. American Mathematical Soc., 1999.\\n\\n[8] Jiaojiao Fan, Shu Liu, Shaojun Ma, Yongxin Chen, and Haomin Zhou. Scalable computation of monge maps with general costs. arXiv preprint arXiv:2106.03812, 2021.\\n\\n[9] Kilian Fatras, Younes Zine, R\u00e9mi Flamary, R\u00e9mi Gribonval, and Nicolas Courty. Learning with minibatch wasserstein: asymptotic and gradient properties. arXiv preprint arXiv:1910.04091, 2019.\\n\\n[10] Kilian Fatras, Younes Zine, Szymon Majewski, R\u00e9mi Flamary, R\u00e9mi Gribonval, and Nicolas Courty. Minibatch optimal transport distances; analysis and applications. arXiv preprint arXiv:2101.01792, 2021.\\n\\n[11] Milena Gazdieva, Litu Rout, Alexander Korotin, Andrey Kravchenko, Alexander Filippov, and Evgeny Burnaev. An optimal transport perspective on unpaired image super-resolution. arXiv preprint arXiv:2202.01116, 2022.\\n\\n[12] Aude Genevay, Marco Cuturi, Gabriel Peyr\u00e9, and Francis Bach. Stochastic optimization for large-scale optimal transport. In Advances in neural information processing systems, pages 3440\u20133448, 2016.\\n\\n[13] Aude Genevay, Gabriel Peyr\u00e9, and Marco Cuturi. Gan and vae from an optimal transport point of view. arXiv preprint arXiv:1706.01807, 2017.\\n\\n[14] Gene H Golub and Henk A Van der Vorst. Eigenvalue computation in the 20th century. Journal of Computational and Applied Mathematics, 123(1-2):35\u201365, 2000.\\n\\n[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.\\n\\n[16] Luca Granieri. On a distance representation of kantorovich potentials. Applied mathematics letters, 22(4):605\u2013610, 2009.\\n\\n[17] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of Wasserstein GANs. In Advances in Neural Information Processing Systems, pages 5767\u20135777, 2017.\\n\\n[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in neural information processing systems, pages 6626\u20136637, 2017.\\n\\n[19] Antoine Houdard, Arthur Leclaire, Nicolas Papadakis, and Julien Rabin. On the existence of optimal transport gradient for learning generative models. arXiv preprint arXiv:2102.05542, 2021.\"}"}
{"id": "VtEEpi-dGlt", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[108x710][20] Leonid Kantorovitch. On the translocation of masses. Management Science 5(1):1\u20134, 1958.\\n\\n[108x691][21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\n[108x662][22] Alexander Korotin, Vage Egiazarian, Lingxiao Li, and Evgeny Burnaev. Wasserstein iterative networks for barycenter estimation. arXiv preprint arXiv:2201.12245, 2022.\\n\\n[108x632][23] Alexander Korotin, Lingxiao Li, Aude Genevay, Justin M Solomon, Alexander Filippov, and Evgeny Burnaev. Do neural optimal transport solvers work? a continuous wasserstein-2 benchmark. Advances in Neural Information Processing Systems 34:14593\u201314605, 2021.\\n\\n[108x591][24] Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Kernel neural optimal transport. arXiv preprint arXiv:2205.15269, 2022.\\n\\n[108x561][25] Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Neural optimal transport. arXiv preprint arXiv:2201.12220, 2022.\\n\\n[108x532][26] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\\n\\n[108x502][27] Dohyun Kwon, Yeoneung Kim, Guido Mont\u00fafar, and Insoon Yang. Training wasserstein gans without gradient penalties. arXiv preprint arXiv:2110.14150, 2021.\\n\\n[108x472][28] Huidong Liu, Xianfeng Gu, and Dimitris Samaras. Wasserstein GAN with quadratic transport cost. In Proceedings of the IEEE International Conference on Computer Vision, pages 4832\u20134841, 2019.\\n\\n[108x432][29] Huidong Liu, GU Xianfeng, and Dimitris Samaras. A two-step computation of the exact gan wasserstein distance. In International Conference on Machine Learning, pages 3159\u20133168. PMLR, 2018.\\n\\n[108x391][30] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.\\n\\n[108x361][31] Anton Mallasto, Jes Frellsen, Wouter Boomsma, and Aasa Feragen. (q, p)-Wasserstein GANs: Comparing ground metrics for Wasserstein GANs. arXiv preprint arXiv:1902.03642, 2019.\\n\\n[108x332][32] Anton Mallasto, Guido Mont\u00fafar, and Augusto Gerolin. How well do WGANs estimate the Wasserstein metric? arXiv preprint arXiv:1910.03875, 2019.\\n\\n[108x302][33] Paul Milgrom and Ilya Segal. Envelope theorems for arbitrary choice sets. Econometrica 70(2):583\u2013601, 2002.\\n\\n[108x272][34] Tristan Milne and Adrian Nachman. Wasserstein gans with gradient penalty compute congested transport. arXiv preprint arXiv:2109.00528, 2021.\\n\\n[108x242][35] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.\\n\\n[108x213][36] Khai Nguyen, Dang Nguyen, Tung Pham, and Nhat Ho. Improving mini-batch optimal transport via partial transportation. arXiv preprint arXiv:2108.09645, 2021.\\n\\n[108x183][37] Khai Nguyen, Quoc Nguyen, Nhat Ho, Tung Pham, Hung Bui, Dinh Phung, and Trung Le. Bomb-ot: On batch of mini-batches optimal transport. arXiv e-prints, pages arXiv\u20132102, 2021.\\n\\n[108x153][38] Quan Hoang Nhan Dam, Trung Le, Tu Dinh Nguyen, Hung Bui, and Dinh Phung. Three-player Wasserstein GAN via amortised duality. In Proc. of the 28th Int. Joint Conf. on Artificial Intelligence (IJCAI), 2019.\\n\\n[108x113][39] Henning Petzka, Asja Fischer, and Denis Lukovnicov. On the regularization of wasserstein gans. arXiv preprint arXiv:1709.08894, 2017.\\n\\n[108x83][40] Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355\u2013607, 2019.\"}"}
{"id": "VtEEpi-dGlt", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Thomas Pinetz, Daniel Soukup, and Thomas Pock. On the estimation of the Wasserstein distance in generative models. In German Conference on Pattern Recognition, pages 156\u2013170. Springer, 2019.\\n\\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\\n\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234\u2013241. Springer, 2015.\\n\\nLitu Rout, Alexander Korotin, and Evgeny Burnaev. Generative modeling with optimal transport maps. In International Conference on Learning Representations, 2021.\\n\\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In Advances in neural information processing systems, pages 2234\u20132242, 2016.\\n\\nMaziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D Lee. On the convergence and robustness of training GANs with regularized optimal transport. arXiv preprint arXiv:1802.08249, 2018.\\n\\nFilippo Santambrogio. Optimal transport for applied mathematicians. Birk\u00e4user, NY, 55(58-63):94, 2015.\\n\\nVivien Seguy, Bharath Bhushan Damodaran, R\u00e9mi Flamary, Nicolas Courty, Antoine Rolet, and Mathieu Blondel. Large-scale optimal transport and mapping estimation. arXiv preprint arXiv:1711.02283, 2017.\\n\\nJan Stanczuk, Christian Etmann, Lisa Maria Kreusser, and Carola-Bibiane Sch\u00f6nlieb. Wasserstein gans work because they fail (to approximate the wasserstein distance). arXiv preprint arXiv:2103.01678, 2021.\\n\\nAkinori Tanaka. Discriminator optimal transport. Advances in Neural Information Processing Systems, 32, 2019.\\n\\nC\u00e9dric Villani. Topics in optimal transportation. Number 58. American Mathematical Soc., 2003.\\n\\nC\u00e9dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.\\n\\nJonathan Weed, Francis Bach, et al. Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance. Bernoulli, 25(4A):2620\u20132648, 2019.\\n\\nXiang Wei, Boqing Gong, Zixia Liu, Wei Lu, and Liqiang Wang. Improving the improved training of wasserstein gans: A consistency term and its dual effect. arXiv preprint arXiv:1803.01541, 2018.\\n\\nChecklist\\n\\n1. For all authors...\\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n(b) Did you describe the limitations of your work? [Yes]\\n(c) Did you discuss any potential negative societal impacts of your work? [N/A]\\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\"}"}
{"id": "VtEEpi-dGlt", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [Yes] All the assumptions are stated in the main text.\\n   (b) Did you include complete proofs of all theoretical results? [Yes] All the proofs are given in the appendices.\\n\\n3. If you ran experiments...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] The code and the instructions are included in the supplementary material. The datasets and assets that we use are publicly available.\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See the supplementary material (appendices + code).\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No]\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See the Appendix.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes] See the discussion in section M3.3.\\n   (b) Did you mention the license of the assets? [No] We refer to the datasets' and assets' public pages.\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] See the supplementary material\\n   (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [No] For CelebA faces dataset, we refer to the original authors publication.\"}"}
