{"id": "sWOdnSkB0qu", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control\\n\\nNolan Wagener\\n\\nAndrey Kolobov\\n\\nFelipe Vieira Frujeri\\n\\nRicky Loynd\\n\\nChing-An Cheng\\n\\nMatthew Hausknecht\\n\\n1 Institute for Robotics and Intelligent Machines, Georgia Institute of Technology\\n\\n2 Microsoft Research\\n\\nAbstract\\n\\nSimulated humanoids are an appealing research domain due to their physical capabilities. Nonetheless, they are also challenging to control, as a policy must drive an unstable, discontinuous, and high-dimensional physical system. One widely studied approach is to utilize motion capture (MoCap) data to teach the humanoid agent low-level skills (e.g., standing, walking, and running) that can then be re-used to synthesize high-level behaviors. However, even with MoCap data, controlling simulated humanoids remains very hard, as MoCap data offers only kinematic information. Finding physical control inputs to realize the demonstrated motions requires computationally intensive methods like reinforcement learning. Thus, despite the publicly available MoCap data, its utility has been limited to institutions with large-scale compute. In this work, we dramatically lower the barrier for productive research on this topic by training and releasing high-quality agents that can track over three hours of MoCap data for a simulated humanoid in the dm_control physics-based environment. We release MoCapAct (Motion Capture with Actions), a dataset of these expert agents and their rollouts, which contain proprioceptive observations and actions. We demonstrate the utility of MoCapAct by using it to train a single hierarchical policy capable of tracking the entire MoCap dataset within dm_control and show the learned low-level component can be re-used to efficiently learn downstream high-level tasks. Finally, we use MoCapAct to train an autoregressive GPT model and show that it can control a simulated humanoid to perform natural motion completion given a motion prompt. Videos of the results and links to the code and dataset are available at the project website.\\n\\n1 Introduction\\n\\nThe wide range of human physical capabilities makes simulated humanoids a compelling platform for studying motor intelligence. Learning and utilization of motor skills is a prominent research topic in machine learning, with advances ranging from emergence of learned locomotion skills in traversing an obstacle course [Heess et al., 2017] to the picking up and carrying of objects to desired locations [Merel et al., 2020, Peng et al., 2019a] to team coordination in simulated soccer [Liu et al., 2022]. Producing natural and physically plausible human motion animation [Harvey et al., 2020, Kania et al., 2021, Yuan and Kitani, 2020] is an active research topic in the game and movie industries. However, while physical simulation of human capabilities is a useful research domain, it is also very challenging from a control perspective. A controller must contend with an unstable, discontinuous, and high-dimensional system that requires a high degree of coordination to execute a desired motion.\\n\\n\u2217 Correspondence to nolan.wagener@gatech.edu and matthew.hausknecht@gmail.com\\n\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks.\"}"}
{"id": "sWOdnSkB0qu", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: The MoCapAct Dataset includes expert policies that are trained to track individual clips. A dataset of noise-injected rollouts (containing observations and actions) is then collected from each expert. These rollouts can subsequently be used to, for instance, train a multi-clip or GPT policy.\\n\\nTabula rasa learning of complex humanoid behaviors (e.g., navigating through an obstacle field) is extremely difficult for all known learning approaches. In light of this challenge, motion capture (MoCap) data has become an increasingly common aid in humanoid control research [Merel et al., 2017, Peng et al., 2018]. MoCap trajectories contain kinematic information about motion: they are sequences of configurations and poses that the human body assumes throughout the motion in question. This data can alleviate the difficulty of training sophisticated control policies by enabling a simulated humanoid to learn low-level motor skills from MoCap demonstrations. The low-level skills can then be re-used for learning advanced, higher-level motions. Datasets such as CMU MoCap [CMU, 2003], Human3.6M [Ionescu et al., 2013], and LaFAN1 [Harvey et al., 2020] offer hours of recorded human motion, ranging from simple locomotion demonstrations to interactions with other humans and objects.\\n\\nHowever, since MoCap data only offers kinematic information, utilizing it in a physics simulator requires recovering the actions (e.g., joint torques) that induce the sequence of kinematic poses in a given MoCap trajectory (i.e., track the trajectory). While easier than tabula rasa learning of a high-level task, finding an action sequence that makes a humanoid track a MoCap sequence is still non-trivial. For instance, this problem has been tackled with reinforcement learning [Chentanez et al., 2018, Merel et al., 2019b, Peng et al., 2018] and adversarial learning [Merel et al., 2017, Wang et al., 2017]. The computational burden of finding these actions scales with the amount of MoCap data, and training agents to recreate hours of MoCap data requires significant compute.\\n\\nAs a result, despite the broad availability of MoCap datasets, their utility\u2014and their potential for enabling research progress on learning-based humanoid control\u2014has been limited to institutions with large compute budgets. To remove this obstacle and facilitate the use of MoCap data in humanoid control research, we introduce MoCapAct (Motion Capture with Actions, Fig. 1), a dataset of high-quality MoCap-tracking policies for a MuJoCo-based [Todorov et al., 2012] simulated humanoid as well as a collection of rollouts from these expert policies. The policies from MoCapAct can track 3.5 hours of recorded motion from CMU MoCap [CMU, 2003], one of the largest publicly available MoCap datasets. We analyze the expert policies of MoCapAct and, to illustrate MoCapAct's usefulness for learning diverse motions, use the expert rollouts to train a single hierarchical policy which is capable of tracking all of the considered MoCap clips. We then re-use the low-level component of the policy to efficiently learn downstream tasks via reinforcement learning. Finally, we use the dataset for generative motion completion by training a GPT network [Karpathy, 2020] to produce a motion in the MuJoCo simulator given a motion prompt.\"}"}
{"id": "sWOdnSkB0qu", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2 Related Work\\n\\nMoCap Data\\n\\nOf the existing datasets featuring motion capture of humans, the largest and most cited are CMU MoCap [CMU, 2003] and Human3.6M [Ionescu et al., 2013]. These datasets feature tens of hours of human motion capture arranged as a collection of clips recorded at 30-120Hz. They demonstrate a wide range of motions, including locomotion (e.g., walking, running, jumping, and turning), physical activities (e.g., dancing, boxing, and gymnastics), and interactions with other humans and objects.\\n\\nMoCap Tracking via Reinforcement Learning\\n\\nTo make use of MoCap data for downstream tasks, much of prior work first learns individual clip-tracking policies. Peng et al. [2018] and Merel et al. [2019a,b, 2020] use reinforcement learning (RL) to learn the clip-tracking policies, whereas Merel et al. [2017] use adversarial imitation learning. Upon learning the tracking policies, there are a variety of ways to utilize them. Peng et al. [2018] and Merel et al. [2017, 2019a] learn a skill-selecting policy to dynamically choose a clip-tracking policy to achieve new tasks. Merel et al. [2019b, 2020] instead opt for a distillation approach, whereby they collect rollouts from the clip-tracking policies and then train a hierarchical multi-clip policy via supervised learning on the rollouts. The low-level policy is then re-used to aid in learning new high-level tasks.\\n\\nAlternatively, large-scale RL may be used to learn a single policy that covers the MoCap dataset. Hasenclever et al. [2020] use a distributed RL setup for the MuJoCo simulator [Todorov et al., 2012], while Peng et al. [2022] use the GPU-based Isaac simulator [Makoviychuk et al., 2021] to perform RL on a single machine.\\n\\nWhile some prior work has released source code to train individual clip-tracking policies [Peng et al., 2018, Yuan and Kitani, 2020], their included catalog of policies is small, and the resources needed to train per-clip policies scale linearly with the number of MoCap clips. In the process of our work, we found that we needed about 50 years of wall-clock time to train the policies to track our MoCap corpus using a similar approach to Peng et al. [2018].\\n\\nMotion Completion\\n\\nOutside of the constraints of a physics simulator, learning natural completions of MoCap trajectories (i.e., producing a trajectory given a prompt trajectory) is the subject of many research papers [Mourot et al., 2022], typically motivated by the challenging and labor-intensive process of creating realistic animations for video games and films. Prior work [Aksan et al., 2021, Harvey et al., 2020, Kania et al., 2021, Mao et al., 2019, Tevet et al., 2022, Wang et al., 2019] typically trains a model to replicate the kinematic motion found in a MoCap dataset, which is then evaluated according to how well the model can predict or synthesize motions given some initial prompt on held-out trajectories.\\n\\nThe more difficult task of performing motion completion within a physics simulator is not widely studied. Yuan and Kitani [2020] jointly learn a kinematic policy and a tracking policy, where the kinematic policy predicts future kinematic poses given a recent history of observations and the tracking policy outputs a low-level action to track the predicted poses.\\n\\n3 The dm_control Humanoid Environment\\n\\nOur simulated humanoid of interest is the \u201cCMU Humanoid\u201d (Fig. 2) from the dm_control package [Tunyasuvunakool et al., 2020], which contains 56 joints and is designed to be similar to an average human body. The humanoid contains a rich and customizable observation space,\"}"}
{"id": "sWOdnSkB0qu", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"proprioceptive observations like joint positions and velocities, actuator states, and touch sensor measurements to high-dimensional observations like images from an egocentric camera. The action is the desired joint angles of the humanoid, which are then converted to joint torques via some pre-defined PD controllers. The humanoid operates in the MuJoCo simulator [Todorov et al., 2012].\\n\\nThe `dm_control` package contains a variety of tools for the humanoid. The package comes with pre-defined tasks like navigation through an obstacle field [Heess et al., 2017], maze navigation [Merel et al., 2019a], and soccer [Liu et al., 2022], and a user may create custom tasks with the package's API. The `dm_control` package also integrates 3.5 hours of motion sequences from the CMU Motion Capture Dataset [CMU, 2003], including clips of locomotion (standing, walking, turning, running, jumping, etc.), acrobatics, and arm movements. Each clip \\\\( C \\\\) is a reference state sequence \\\\( \\\\hat{s}_C^0, \\\\hat{s}_C^1, \\\\ldots, \\\\hat{s}_C^{T_C-1} \\\\), where \\\\( T_C \\\\) is the clip length and each \\\\( \\\\hat{s}_C^t \\\\) contains kinematic information like joint angles, joint velocities, and humanoid pose.\\n\\nAs discussed in Section 2, training a control policy to work on all of the included clips requires large-scale solutions. For example, Hasenclever et al. [2020] rely on a distributed RL approach that uses about ten billion environment interactions collected by 4000 parallel actor processes running for multiple days. To our knowledge, there are no agents publicly available that can track all the MoCap data within `dm_control`. We address this gap by releasing a dataset of high-quality experts and their rollouts for the \u201cCMU Humanoid\u201d in the `dm_control` package.\\n\\n### 4 MoCapAct Dataset\\n\\nThe MoCapAct dataset (Fig. 1) consists of:\\n\\n- experts each trained to track an individual snippet from the MoCap dataset (Section 4.1) and\\n- HDF5 files containing rollouts from the aforementioned experts (Section 4.2).\\n\\nWe include documentation of the MoCapAct dataset in Appendix A.\\n\\n#### 4.1 Clip Snippet Experts\\n\\nOur expert training scheme largely follows that of Merel et al. [2019a,b] and Peng et al. [2018], which we now summarize.\\n\\n**Training**\\n\\nWe split each clip in the MoCap dataset into 4\u20136 second snippets with 1-second overlaps. With 836 clips in the MoCap dataset, this clip splitting results in 2589 snippets. For each clip snippet \\\\( c \\\\), we train a time-indexed Gaussian policy \\\\( \\\\pi_c(a|s,t) \\\\) to track the snippet. We use the same clip-tracking reward function \\\\( r_c(s,t) \\\\) as Hasenclever et al. [2020], which encourages matching the MoCap clip\u2019s joint angles and velocities, positions of various body parts, and joint orientations. This reward function lies in the interval \\\\([0, 1]\\\\). To speed up training, we use the same early episode termination condition as Hasenclever et al. [2020], which activates if the humanoid deviates too far from the snippet. To help exploration, the initial state of an episode is generated by randomly sampling a time step from the given snippet. The Gaussian policy \\\\( \\\\pi_c \\\\) uses a mean parameterized by a neural network as well as a fixed standard deviation of 0.1 for each action to induce robustness and to prepare for the noisy rollouts (Section 4.2). We use the Stable-Baselines3 [Raffin et al., 2021] implementation of PPO [Schulman et al., 2017] to train the experts. Our training took about 50 years of wall-clock time. We give hyperparameters and training details in Appendix B.1.\\n\\n**Results**\\n\\nTo account for the snippets having different lengths and for the episode initialization scheme used in training, we report our evaluations in a length-normalized fashion.\\n\\nFor a snippet \\\\( c \\\\) (with length \\\\( T_c \\\\)) and some policy \\\\( \\\\pi \\\\), recall that we initialize the humanoid at some randomly chosen time step \\\\( t_0 \\\\) from \\\\( c \\\\) and then generate the trajectory \\\\( \\\\tau \\\\) by rolling out \\\\( \\\\pi \\\\) from \\\\( t_0 \\\\) until either the end of the snippet or early termination. Let \\\\( R(\\\\tau) \\\\) and \\\\( L(\\\\tau) \\\\) denote the accumulated reward and the length of the trajectory \\\\( \\\\tau \\\\), respectively. We define the normalized episode reward and normalized episode length of \\\\( \\\\tau \\\\) as\\n\\n\\\\[\\n\\\\frac{R(\\\\tau)}{T_c - t_0} \\\\quad \\\\text{and} \\\\quad \\\\frac{L(\\\\tau)}{T_c - t_0}\\n\\\\]\\n\\nrespectively. One consequence of this definition is that trajectories that are terminated early in a snippet yield smaller normalized episode rewards and lengths. Next, we\\n\\n\\\\[\\\\text{1} \\\\]\\n\\nWe point out that PPO uses the original unnormalized reward for policy optimization.\"}"}
{"id": "sWOdnSkB0qu", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Snippet expert results on the MoCap snippets within dm_control. We disable the Gaussian noise for \\\\( \\\\pi_c \\\\) when computing these results.\\n\\n|                          | Mean   | Standard deviation | Median | Minimum | Maximum |\\n|--------------------------|--------|--------------------|--------|---------|---------|\\n| Average normalized episode reward | 0.816  | 0.153              | 0.777  | 0.217   | 2.333   |\\n| Average normalized episode length | 0.997  | 0.022              | 1.000  | 0.424   | 2.000   |\\n\\nFigure 3: Clip expert results on the MoCap snippets within dm_control.\\n\\n.define the average normalized episode reward and average normalized episode length of policy \\\\( \\\\pi \\\\) on snippet \\\\( c \\\\) as\\n\\n\\\\[\\n\\\\hat{R}_c(\\\\pi) = \\\\mathbb{E}_{t_0 \\\\sim c} \\\\mathbb{E}_{\\\\tau \\\\sim \\\\pi|t_0} R(\\\\tau)_t \\\\frac{T_c - t_0}{i}\\n\\\\]\\n\\nand\\n\\n\\\\[\\n\\\\hat{L}_c(\\\\pi) = \\\\mathbb{E}_{t_0 \\\\sim c} \\\\mathbb{E}_{\\\\tau \\\\sim \\\\pi|t_0} L(\\\\tau)_t \\\\frac{T_c - t_0}{i}\\n\\\\]\\n\\n, respectively. For example, if \\\\( \\\\pi \\\\) always successfully tracks some MoCap snippet from any \\\\( t_0 \\\\) to the end of the snippet, \\\\( \\\\pi \\\\) has an average normalized episode length of 1 on snippet \\\\( c \\\\).\\n\\nOverall, the clip experts reliably track the overwhelming majority of the MoCap snippets (Table 1 and Fig. 3). Averaged over all the snippets, the experts have a per-joint mean angle error of 0.062 radians.\\n\\nWe find that 80% of the trained experts have an average normalized episode length of at least 0.999.\\n\\nWe also observe there is a bimodal structure to the reward distribution in Fig. 3, which is due to many clips having artifacts like jittery limbs and extremities clipping through the ground. These artifacts limit the extent to which the humanoid can track the clip. Among the handful of experts with very low reward (between 0.2 and 0.5), we find that the corresponding clips are erroneously sped up, making them impossible to track in the simulator.\\n\\nThe experts produce motion that is generally indistinguishable from the MoCap reference (Fig. 4), from simple walking behaviors seen in the top row to highly coordinated motions like the cartwheel in the middle row. On some clips, the expert deviates from the clip because the demonstrated motion\"}"}
{"id": "sWOdnSkB0qu", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"is too highly dynamic, such as the 360-degree jump in the bottom row. Instead, the expert typically learns some other behavior that keeps the episode from terminating early, which in this case is jumping without spinning. We also point out that, in these failure modes, the humanoid still tracks some portions of the reference, such as hand positions and orientations. Yuan and Kitani [2020] rectify similar tracking issues by augmenting the action space with external forces on certain parts of the humanoid body, but we do not explore this avenue since the issue only affects a small number of clips. We encourage the reader to visit the project website to see videos of the clip experts.\\n\\n4.2 Expert Rollouts\\n\\nFollowing Merel et al. [2019b], we roll out the experts on their respective snippets and collect data from the rollouts into a dataset $D$. In order to obtain a broad state coverage from the experts, we repeatedly roll out the stochastic experts (i.e., with Gaussian noise injected into the actions) starting from different initial states. This injected noise helps the dataset cover states that a policy learned by imitating the dataset would visit, therefore mitigating the distribution shift issue for the learned policy [Laskey et al., 2017, Merel et al., 2019b].\\n\\nFor each clip snippet $c$, we denote the corresponding expert policy as $\\\\pi_c(s, t) = \\\\mathcal{N}(a; \\\\mu_c(s, t), 0.1^2)$, where $\\\\mu_c(s, t)$ is the mean of the expert's action distribution. We initialize the humanoid at some point in the snippet (half of the time at the beginning of the snippet and otherwise at some random point in the snippet). We then roll out $\\\\pi_c$ until either the end of the snippet or early termination using the scheme from Section 4.1. At every time step $t$ in the rollout, we log the humanoid state $s_t$, the target reference poses $s_{ref} = (\\\\hat{s}_{c,t} + 1, \\\\ldots, \\\\hat{s}_{c,t} + 5)$ from the next five steps of the MoCap snippet, the expert's sampled action $a_t$, the expert's mean action $\\\\bar{a}_t = \\\\mu_c(s_t, t)$, the observed snippet reward $r_c(s_t, t)$, the estimated value $\\\\hat{V}_{\\\\pi_c}(s_t)$, and the estimated advantage $\\\\hat{A}_{\\\\pi_c}(s_t, a_t)$ into HDF5 files.\\n\\nWe release two versions of the rollout dataset:\\n\\n- a \u201clarge\u201d 600-gigabyte collection at 200 rollouts per snippet with a total of 67 million environment transitions (corresponding to 620 hours in the simulator)\\n- a \u201csmall\u201d 50-gigabyte collection at 20 rollouts per snippet with a total of 5.5 million environment transitions (corresponding to 51 hours in the simulator).\\n\\nIn our application of MoCapAct (Section 5), we use the \u201clarge\u201d version of the dataset. We do observe, though, that the multi-clip policy results (Section 5.1) are similar when using either dataset.\\n\\n5 Applications\\n\\nWe train two policies (Fig. 5) using our dataset:\\n\\n1. A hierarchical policy which can track all the MoCap snippets and be re-used for learning new high-level tasks (Section 5.1).\\n2. An autoregressive GPT model which generates motion from a given prompt (Section 5.2).\"}"}
{"id": "sWOdnSkB0qu", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.1 Multi-Clip Tracking Policy\\n\\nWe first show the MoCapAct dataset can reproduce the results in Merel et al. [2019b] by learning a single policy that tracks the entire MoCap dataset within dm_control. Our policy architecture (Fig. 5a) follows the same encoder-decoder scheme as Merel et al. [2019b], who introduce a \\\"motor intention\\\" $z_t$ which acts as a low-dimensional embedding of the MoCap reference $s_{ref}^t$. The intention $z_t$ is then decoded into an action $a_t$. In other words, the policy $\\\\pi$ is factored into an encoder $\\\\pi_{enc}$ and a decoder $\\\\pi_{dec}$.\\n\\nThe encoder $\\\\pi_{enc}(z_t|s_t, s_{ref}^t, z_{t-1})$ compresses the MoCap reference $s_{ref}^t$ into an intention and may use the current humanoid state $s_t$ and previous intention $z_{t-1}$ in predicting the current intention. Furthermore, the encoder outputs an intention which is stochastic, which models ambiguity in the MoCap reference and allows for the high-level behavior to be specified more coarsely. The decoder $\\\\pi_{dec}(a_t|s_t, z_t)$ translates the sampled intention $z_t$ into an action $a_t$ with the aid of the state $s_t$ as an additional input.\\n\\n5.1.1 Training\\n\\nIn our implementation, the encoder outputs the mean and diagonal covariance of a Gaussian distribution over a 60-dimensional motor intention $z_t$. The decoder outputs the mean of a Gaussian distribution over actions with a standard deviation of $0.1$ for each action. In training, we maximize a variant of the multi-step imitation learning objective from Merel et al. [2019b]:\\n\\n$$\\\\mathbb{E}(s_1:T, s_{ref}^1:T, \\\\bar{a}_1:T, c) \\\\sim D, z_0:T \\\\sim \\\\pi_{enc}\\\\sum_{t=1}^{T} h(w_{c}(s_t, \\\\bar{a}_t)) \\\\log \\\\pi_{dec}(\\\\bar{a}_t|s_t, z_t) - \\\\beta KL(\\\\pi_{enc}(z_t|s_t, s_{ref}^t, z_{t-1}) || p(z_t|z_{t-1}))$$\\n\\nwhere $T$ is the sequence length, $w_{c}$ is a clip-dependent data-weighting function, $p(z_t|z_{t-1})$ is an autoregressive prior, and $\\\\beta$ is a hyperparameter. The weighting function $w_{c}$ allows for some data points to be considered more heavily, which may be useful given the spectrum of expert performance. Letting $\\\\lambda$ be a hyperparameter, we consider the following four weighting schemes:\\n\\n- **Behavioral cloning (BC):** $w_c(s, a) = 1$. This scheme is commonly used in imitation learning and treats every data point equally.\\n- **Clip-weighted regression (CWR):** $w_c(s, a) = \\\\exp(\\\\hat{R}_c(\\\\pi_c)/\\\\lambda)$. This scheme upweights data from snippets where the experts have higher average normalized rewards.\\n- **Advantage-weighted regression (AWR) [Peng et al., 2019b]:** $w_c(s, a) = \\\\exp(\\\\hat{A}_{\\\\pi_c}(s, a)/\\\\lambda)$. This scheme upweights actions that perform better than the expert's average return.\\n- **Reward-weighted regression (RWR) [Peters and Schaal, 2007]:** $w_c(s, a) = \\\\exp(\\\\hat{Q}_{\\\\pi_c}(s, a)/\\\\lambda)$, where $\\\\hat{Q}_{\\\\pi_c}(s, a) = \\\\hat{V}_{\\\\pi_c}(s) + \\\\hat{A}_{\\\\pi_c}(s, a)$. This scheme upweights state-actions which have higher returns, which typically happens with good experts at earlier time steps in the corresponding snippet.\\n\\nThe KL divergence term encourages the decoder to follow a simple random walk. In this case, the prior has the form $p(z_t|z_{t-1}) = N(z_t; \\\\alpha z_{t-1}, \\\\sigma^2 I)$, where $\\\\alpha \\\\in [0, 1]$ is a hyperparameter and $\\\\sigma = \\\\sqrt{1 - \\\\alpha^2}$. This prior in turn encourages the marginals to be a spherical Gaussian, i.e., $p(z_t) = N(z_t; 0, I)$. Furthermore, the regularization introduces a bottleneck [Alemi et al., 2017] that limits the information the intention $z_t$ can provide about the state $s_t$ and MoCap reference $s_{ref}^t$. This forces the encoder to only encode high-level information about the reference (e.g., direction of motion of leg) while excluding fine-grained details (e.g., precise velocity of each joint in leg).\\n\\nIn our experiments, we found that the training takes about three hours on a single-GPU machine. More training details are available in Appendix B.2.\\n\\nResults\\n\\nAll four regression approaches yield broadly good results (Table 2), achieving 80% to 84% of the experts' performance on the MoCap dataset (cf. Table 1). We also see that every weighted regression scheme gives some improvement over the unweighted approach. AWR only gives 1% improvement over BC, likely because the experts are already near-optimal and the dataset lacks sufficient state-action coverage to reliably contain advantageous actions. CWR gives a 3% improvement over BC, which arises from the objective placing more emphasis on data coming from high-reward clips. Finally, RWR gives a 5% improvement over BC, which comes from increased\"}"}
{"id": "sWOdnSkB0qu", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Multi-clip results on the MoCap snippets, showing the mean and standard deviation over three seeds. For evaluation, we disable the Gaussian noise for $\\\\pi_{\\\\text{dec}}$ but keep the stochasticity for $\\\\pi_{\\\\text{enc}}$.\\n\\n| BC | CWR | AWR | RWR |\\n|----|-----|-----|-----|\\n| Avg. normalized episode reward | 0.654 \u00b1 0.005 | 0.671 \u00b1 0.003 | 0.661 \u00b1 0.003 | 0.688 \u00b1 0.002 |\\n| Avg. normalized episode length | 0.855 \u00b1 0.004 | 0.858 \u00b1 0.003 | 0.861 \u00b1 0.001 | 0.868 \u00b1 0.002 |\\n\\nFigure 6: Performance of RWR-trained multi-clip policy.\\n\\nWe re-use the decoder $\\\\pi_{\\\\text{dec}}$ from an RWR-trained multi-clip policy for reinforcement learning to constrain the behaviors of the humanoid and speed up learning. In particular, we study two tasks that require adept locomotion skills:\\n\\n1. A sparse-reward go-to-target task where the agent receives a non-zero reward only if the humanoid is sufficiently close to the target. The target relocates once the humanoid stands on it for a few time steps.\\n2. A velocity control task where shaped rewards encourage the humanoid to go at a given speed in a given direction. The desired speed and direction change randomly every few seconds.\\n\\nWe treat $\\\\pi_{\\\\text{dec}}$ as part of the environment and the motor intention $z$ as the action. We thus learn a new high-level policy $\\\\pi_{\\\\text{task}}(z|s)$ that steers the low-level policy to maximize the task reward. Given the tasks are locomotion-driven, we also consider a more specialized decoder with a 20-dimensional intention which is trained solely on locomotion clips from MoCapAct (called the \u201cLocomotion\u201d subset) to see if further restricting the learned skills offers any more speedup. As a baseline, we also perform RL without a low-level policy.\"}"}
{"id": "sWOdnSkB0qu", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Returns for the transfer tasks, showing the mean and standard deviation over five seeds.\\n\\n| Policy Type          | Go-to-target | Velocity control |\\n|----------------------|--------------|------------------|\\n| General low-level    | 96.3 \u00b1 2.8   | 66.1 \u00b1 32.8      |\\n| Locomotion low-level | 67.5 \u00b1 1.1   | 1074 \u00b1 55       |\\n| No low-level         | 7.00          | 884 \u00b1 81         |\\n\\nWe find that re-using a low-level policy drastically speeds up learning and usually produces higher returns (Table 3 and Fig. 7). For the go-to-target task, the locomotion-based low-level policy induces faster training than the more general low-level policy, though it does converge to lesser performance and on one out of five seeds converges to a very low reward. This performance gap is likely a combination of the lower dimensionality of the locomotion policy restricting the degree of control by the high-level policy and the \\\"Locomotion\\\" subset excluding some useful behaviors, a result also found by Hasenclever et al. [2020]. The baseline without the low-level policy fails to learn the task.\\n\\nFor the velocity control task, the locomotion-based policy induces slightly faster learning than the general policy but again results in lower reward. The baseline without the low-level policy learns the task more slowly, though it does achieve high reward eventually.\\n\\nIn both tasks, we find that including a pretrained low-level policy produces much more realistic gaits. The humanoid efficiently runs from target to target in the go-to-target task and smoothly changes speeds and direction of motion in the velocity control task. On the other hand, the baseline approach produces incredibly unusual motions. In the go-to-target task, the humanoid convulses and contorts itself towards the first target before falling to the ground. In the velocity control task, the humanoid rapidly taps the feet to propel the body at the desired velocity. We encourage the reader to visit the project website to see videos of the RL results.\\n\\n5.2 Motion Completion with GPT\\n\\nWe also train a GPT model [Radford et al., 2019] based on the minGPT implementation [Karpathy, 2020] to generate motion. Starting with a motion prompt (sequence of humanoid observations generated by a clip expert), the GPT policy (Fig. 5b) autoregressively predicts actions from the context of recent humanoid observations. We train the GPT by sampling 32-step sequences (corresponding to 1 second of motion) of humanoid observations $s(t-31):t$ and expert's mean actions $\\\\bar{a}(t-31):t$ from the MoCapAct dataset $D$ and performing supervised learning using the mean squared error loss on the predicted action sequence.\\n\\nTo roll out the policy, we provide the GPT policy with a 32-step prompt from a clip expert and let GPT roll out thereafter. The episode either terminates after 500 steps (about 15 seconds) or if a body part other than the feet touches the ground (e.g., humanoid falling over). On many clip snippets, the GPT model is able to control the humanoid for several seconds past the end of the prompt (Table 4 and Fig. 8a), with similar lengths on the training set and a held-out validation set of prompts. We also observe that on many clips the GPT can control the humanoid for several times longer than the length of the corresponding clip snippet (Table 4 and Fig. 8b).\\n\\nTo visualize the rollouts, we perform principal component analysis (PCA) on action sequences of length 32 applied by GPT and the snippet expert used to generate the motion prompt (Fig. 9). Qualitatively, we find that GPT usually repeats motions demonstrated in locomotion prompts, such as the running motion corresponding to Fig. 9a. Occasionally, GPT will produce a different motion than...\"}"}
{"id": "sWOdnSkB0qu", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Motion completion statistics on the MoCap snippets.\\n\\n| Episode length (seconds) | Mean | Standard deviation | Median | Minimum | Maximum |\\n|--------------------------|------|--------------------|--------|---------|---------|\\n|                          | 5.47 | 3.47               | 4.38   | 0.23    | 15.00   |\\n\\n| Episode length (seconds) | Relative episode length |\\n|--------------------------|-------------------------|\\n|                          | 1.15                    |\\n|                          | 0.94                    |\\n|                          | 0.87                    |\\n|                          | 0.05                    |\\n|                          | 7.63                    |\\n\\n(a) Absolute episode lengths of GPT.\\n\\n(b) Relative episode lengths of GPT.\\n\\nFigure 8: Episode lengths of GPT on MoCap snippets.\\n\\nFigure 9: PCA projections of action sequences of length 32 from experts and GPT.\\n\\nLegend: (a) Locomotion clip where behaviors align. (b) Locomotion clip where behaviors differ. (c) Non-locomotion clip where behaviors differ.\\n\\n6 Discussion\\n\\nWe presented a dataset of high-quality MoCap-tracking policies and their rollouts for the dm_control humanoid environment. From these rollouts, we trained multi-clip tracking policies that can be re-used for new high-level tasks and GPT policies which can generate humanoid motion when given a prompt. We have open sourced our dataset, models, and code under permissive licenses.\\n\\nWe do point out that our models and data are only applicable to the dm_control environment, which uses MuJoCo as the backend simulator. We also point out that all considered clips only occur on flat ground and do not include any human or object interaction. Though this seems to limit the environments and tasks where this dataset is applicable, the dm_control package [Tunyasuvunakool et al., 2020] has tools to change the terrain, add more MoCap clips, and add objects (e.g., balls) to the environment. Indeed, prior work has used custom clips which include extra objects [Merel et al., 2020, Liu et al., 2022]. While the dataset and domain may raise concerns on automation, we believe the considered simulated domain is limited enough to not be of ethical import.\\n\\nThis work significantly lowers the barrier of entry for simulated humanoid control, which promises to be a rich field for studying multi-task learning and motor intelligence. In addition to the showcases presented, we believe this dataset can be used in training other policy architectures like decision and trajectory transformers [Chen et al., 2021, Janner et al., 2021] or in setups like offline reinforcement learning [Fu et al., 2020, Levine et al., 2020] as the dataset allows research groups to bypass the time- and energy-consuming process of learning low-level motor skills from MoCap data.\"}"}
{"id": "sWOdnSkB0qu", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgments and Disclosure of Funding\\n\\nWe thank Leonard Hasenclever for providing helpful information used in DeepMind\u2019s prior work on humanoid control. We also thank Byron Boots for suggesting to use PCA projections for visualization. Finally, we thank the reviewers for their invaluable feedback.\\n\\nThe data used in this project was obtained from mocap.cs.cmu.edu. The database was created with funding from NSF EIA-0196217.\\n\\nReferences\\n\\nE. Aksan, M. Kaufmann, P. Cao, and O. Hilliges. A Spatio-Temporal Transformer for 3D Human Motion Prediction. In 2021 International Conference on 3D Vision (3DV), pages 565\u2013574. IEEE, 2021.\\n\\nA. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy. Deep Variational Information Bottleneck. In International Conference on Learning Representations, 2017.\\n\\nL. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch. Decision Transformer: Reinforcement Learning via Sequence Modeling. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nN. Chentanez, M. M\u00fcller, M. Macklin, V. Makoviychuk, and S. Jeschke. Physics-Based Motion Capture Imitation With Deep Reinforcement Learning. In Proceedings of the 11th Annual International Conference on Motion, Interaction, and Games, pages 1\u201310, 2018.\\n\\nCMU. Carnegie Mellon University Graphics Lab Motion Capture Database. http://mocap.cs.cmu.edu, 2003.\\n\\nJ. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4RL: Datasets for Deep Data-Driven Reinforcement Learning. arXiv preprint arXiv:2004.07219, 2020.\\n\\nF. G. Harvey, M. Yurick, D. Nowrouzezahrai, and C. Pal. Robust Motion In-Betweening. ACM Transactions on Graphics (TOG), 39(4):60\u20131, 2020.\\n\\nL. Hasenclever, F. Pardo, R. Hadsell, N. Heess, and J. Merel. CoMic: Complementary Task Learning & Mimicry for Reusable Skills. In International Conference on Machine Learning, pages 4105\u20134115. PMLR, 2020.\\n\\nN. Heess, D. TB, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez, Z. Wang, S. M. A. Eslami, M. Riedmiller, and D. Silver. Emergence of Locomotion Behaviours in Rich Environments. arXiv preprint arXiv:1707.02286, 2017.\\n\\nC. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu. Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(7):1325\u20131339, 2013.\\n\\nM. Janner, Q. Li, and S. Levine. Offline Reinforcement Learning as One Big Sequence Modeling Problem. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nK. Kania, M. Kowalski, and T. Trzci\u0144ski. TrajeV AE: Controllable Human Motion Generation from Trajectories. arXiv preprint arXiv:2104.00351, 2021.\\n\\nA. Karpathy. minGPT. https://github.com/karpathy/minGPT, 2020.\\n\\nM. Laskey, J. Lee, R. Fox, A. Dragan, and K. Goldberg. DART: Noise Injection for Robust Imitation Learning. In Conference on Robot Learning, pages 143\u2013156. PMLR, 2017.\\n\\nS. Levine, A. Kumar, G. Tucker, and J. Fu. Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems. arXiv preprint arXiv:2005.01643, 2020.\"}"}
{"id": "sWOdnSkB0qu", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "sWOdnSkB0qu", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"G. Tevet, B. Gordon, A. Hertz, A. H. Bermano, and D. Cohen-Or. MotionCLIP: Exposing Human Motion Generation to CLIP Space. In European Conference on Computer Vision. Springer, 2022.\\n\\nE. Todorov, T. Erez, and Y. Tassa. MuJoCo: A Physics Engine for Model-Based Control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033. IEEE, 2012.\\n\\nS. Tunyasuvunakool, A. Muldal, Y. Doron, S. Liu, S. Bohez, J. Merel, T. Erez, T. Lillicrap, N. Heess, and Y. Tassa. dm_control: Software and Tasks for Continuous Control. Software Impacts, 6:100022, 2020.\\n\\nB. Wang, E. Adeli, H.-k. Chiu, D.-A. Huang, and J. C. Niebles. Imitation Learning for Human Pose Prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7124\u20137133, 2019.\\n\\nZ. Wang, J. S. Merel, S. E. Reed, N. de Freitas, G. Wayne, and N. Heess. Robust Imitation of Diverse Behaviors. Advances in Neural Information Processing Systems, 30, 2017.\\n\\nY. Yuan and K. Kitani. Residual Force Control for Agile Human Behavior Imitation and Extended Motion Synthesis. Advances in Neural Information Processing Systems, 33:21763\u201321774, 2020.\"}"}
{"id": "sWOdnSkB0qu", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Checklist\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n   (b) Did you describe the limitations of your work? [Yes]\\n   (c) Did you discuss any potential negative societal impacts of your work? [Yes]\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n   (b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g., for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes]\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes]\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] Appendix B Sections 4.1, 5.1.1, 5.1.2, and 5.2\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Appendix B\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes] Sections 3, 4.1, and 5.2\\n   (b) Did you mention the license of the assets? [Yes] Section 6\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\\n   (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\\n\\nWhile we do use human MoCap data, the data was previously collected and released by the CMU Graphics Lab [CMU, 2003].\"}"}
