{"id": "loJM1acwzf", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: How many chapter authors of this report come from Columbia University?\\n\\nResponse: The report lists the chapter authors on page xii. From Columbia University, the chapter authors are:\\n\\n1. Radley Horton, 2. Patrick Kinney.\\n\\nTherefore, there are two chapter authors from Columbia University.\\n\\nError Type: Incomplete Evidence\\n\\nAnalysis: The model only finds out 2 of 3 chapter authors from Columbia University, leaving out Garcia-Pando shown in the top-left of Page 15.\"}"}
{"id": "loJM1acwzf", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: What are the overlapped apps between page 21 and 62?\\n\\nResponse: The overlapped apps between page 21 and page 62 are:\\n- LinkedIn\\n- SlideShare\\n\\nError Type: Incomplete Evidence\\n\\nAnalysis: The model correctly identifies the two shared apps (LinkedIn and SlideShare) between two pages. But it leaves out another app: Podcast.\"}"}
{"id": "loJM1acwzf", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hallucinated Evidence: Case 1\\n\\nQuestion: How many sampled questions illustrated in this slide?\\n\\nResponse: There are five sample questions illustrated in this slide. They are shown on the following slides:\\n\\n1. Slide 10: \u201cThe following search string will produce which results according to the Venn diagram?\u201d\\n2. Slide 11: \u201cA role you were previously recruiting for just re-opened. Which Talent Pipeline feature should you utilize to source talent already in your pipeline?\u201d\\n3. Slide 12: \u201cBased on the report below, which recruiter should you go to for advice on InMail best practices?\u201d\\n4. Slide 13: \u201cYou recruit for Retail Bankers in high volume. Which efficiency tool(s) will help you constantly uncover new leads?\u201d\\n5. Slide 14: \u201cWhat happens to my certification with the rollout of Next-Gen Recruiter?\u201d\\n\\nError Type: Hallucinated Evidence\\n\\nAnalysis: The model totally fabricates the contents of Slide 14 (Page 14). This slide is not about sampled question.\\n\\nFigure 27: Error example about Hallucinated Evidence\"}"}
{"id": "loJM1acwzf", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hallucinated Evidence: Case 2\\n\\nQuestion: Is there blue color handwritten words in page 30?\\n\\nResponse: Yes, there are blue handwritten words on page 30.\\n\\nError Type: Hallucinated Evidence\\n\\nAnalysis: This document has only 20 pages. There is no \\\"so-called\\\" page 30 in the response.\"}"}
{"id": "loJM1acwzf", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Knowledge Lacking: Case 1\\n\\nQuestion: What is the FY2019 fixed asset turnover ratio for Activision Blizzard?\\n\\nResponse: The fixed asset turnover ratio is calculated by dividing net sales by net fixed assets. The formula is:\\n\\n$$\\\\text{Fixed Asset Turnover Ratio} = \\\\frac{\\\\text{Net Sales}}{\\\\text{Net Fixed Assets}}$$\\n\\nFrom the provided document, we can find the necessary values for the fiscal year 2019:\\n\\n- Net Sales (Net Revenues) for 2019: $6,489 million\\n- Net Fixed Assets (Property and Equipment, net) at the end of 2019: $253 million\\n\\nUsing these values, the fixed asset turnover ratio is:\\n\\n$$\\\\frac{6,489}{253} \\\\approx 25.65$$\\n\\nTherefore, the fixed asset turnover ratio for Activision Blizzard for the fiscal year 2019 is approximately 25.65.\\n\\nError Type: Knowledge Lacking\\n\\nAnalysis: The model uses a wrong formula to calculate the financial metric. To calculate the Fixed Asset Turnover Ratio, the denominator should be the average value of net fixed assets in the past two years, instead of the value of a single year.\"}"}
{"id": "loJM1acwzf", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Knowledge Lacking: Case 2\\n\\nQuestion: How many figures in this document show the old gate of Tsinghua (\\\"Er Xiao Men\\\" in Chinese)?\\n\\nResponse: The document contains two figures showing the old gate of Tsinghua (\\\"Er Xiao Men\\\" in Chinese). The first one is on page 2, and the second one is on page 27.\\n\\nError Type: Knowledge Lacking\\n\\nAnalysis: Though it has similar appearance, the building shown in Page 27 (i.e. the right figure) is not \\\"Er Xiao Men\\\" but \\\"Qing Hua Xue Tang\\\".\"}"}
{"id": "loJM1acwzf", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Reasoning Error: Case 1\\n\\nQuestion: If I drop the green module shown in Figure 6, what is the absolute percentage drop on 100-shot TACREV setting?\\n\\nResponse: The absolute percentage drop on the 100-shot TACREV setting when the green module is dropped can be found in Table 4. For the 100-shot TACREV setting, the F1 score before reranking is 72.0, and after reranking, it is 74.3. The absolute percentage drop is calculated as follows:\\n\\n\\\\[\\n\\\\text{Absolute Percentage Drop} = \\\\left( \\\\frac{\\\\text{F1 score before reranking} - \\\\text{F1 score after reranking}}{\\\\text{F1 score after reranking}} \\\\right) \\\\times 100\\n\\\\]\\n\\n\\\\[\\n= \\\\left( \\\\frac{74.3 - 72.0}{74.3} \\\\right) \\\\times 100\\n\\\\]\\n\\n\\\\[\\n= \\\\left( \\\\frac{2.3}{74.3} \\\\right) \\\\times 100\\n\\\\]\\n\\n\\\\[\\n\\\\approx 3.1\\\\%\\n\\\\]\\n\\nSo, the absolute percentage drop on the 100-shot TACREV setting when the green module is dropped is approximately 3.1%.\\n\\nError Type: Reasoning Error\\n\\nAnalysis: The formula listed in model response is the relative percentage drop, instead of the absolute percentage drop.\"}"}
{"id": "loJM1acwzf", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: How many quizzes are there in units 4, 5, and 6 combined?\\n\\nResponse: There are three quizzes in units 4, 5, and 6 combined:\\n- Unit 4: Quiz 2: Twenty Core Concepts on Unit 3 & 4\\n- Unit 5: No quiz listed\\n- Unit 6: Quiz #3: Comprehension of Twenty Core Concepts from Unit 5 & 6\\n\\nSo, the total number of quizzes in units 4, 5, and 6 is 3.\\n\\nError Type: Reasoning Error\\n\\nAnalysis: The response analyze correctly the quizzes in each unit, but count their number wrongly at the last step.\"}"}
{"id": "loJM1acwzf", "page_num": 41, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: How many distinct button functions are there during a workout?\\n\\nResponse: During a workout, the HONOR Watch GS Pro has the following distinct button functions:\\n\\n1. Press the Up button: Lock, mute, or end workout.\\n2. Press the Down button: Switch screen.\\n3. Press and hold the Up button: Finish the workout.\\n4. Press and hold the Down button: Wake up the voice assistant.\\n5. Swipe up or down on the screen: Switch screen.\\n6. Swipe left or right on the screen: Switch screen.\\n\\nSo, there are 6 distinct button functions during a workout.\\n\\nError Type: Irrelevant Answer\\n\\nAnalysis: The question asks for the button functions. The response, however, answers the button operations and outputs an irrelevant answer.\"}"}
{"id": "loJM1acwzf", "page_num": 42, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What types of discrimination are most commonly reported by women in STEM jobs compared to men in the survey of Pew Research Center, January 2018.\\n\\nIn the Pew Research Center survey titled \u201cWomen and Men in STEM Often at Odds Over Workplace Equity,\u201d published in January 2018, several types of discrimination were reported by women in STEM jobs compared to men, and by women in workplaces with a higher proportion of women. Here are the key findings:\\n\\n1. Gender Discrimination:\\n   - Overall Experience: 50% of women in STEM jobs reported experiencing gender discrimination at work, compared to 19% of men in STEM jobs.\\n   - Specific Types of Discrimination: Women in STEM jobs reported various forms of gender discrimination, including earning less than a man doing the same job (29%), being treated as if they were not competent (29%), experiencing repeated small slights at work (20%), and receiving less support from senior leaders (18%).\\n\\n5. Advanced Degrees:\\n   - Higher Discrimination: 62% of women in STEM jobs with a postgraduate degree reported experiencing gender discrimination, compared to 41% of women with some college or less education.\\n   - Impact on Fair Treatment: Women with advanced degrees were less likely to believe that women are treated fairly in opportunities for promotion and advancement.\\n\\nThese findings highlight the significant gender disparities and challenges that women in STEM fields face, particularly in male-dominated environments and specific job roles like computer-related positions.\"}"}
{"id": "loJM1acwzf", "page_num": 43, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 35: Case Study. Evidence source: table. The evidence pages are zoomed in. The correct\\nfigures,\\n\\n- **Interest Coverage Ratio** for Activision Blizzard in F2019 is approximately 58.32.\\n\\nTherefore, the Interest Coverage Ratio for Activision Blizzard in F2019 is calculated as follows:\\n\\n\\\\[\\n\\\\frac{\\\\text{EBIT}}{\\\\text{Interest Expense}} = \\\\frac{\\\\$28,000,000}{\\\\$4,000,000} = 7\\n\\\\]\\n\\nwhere EBIT is the *Earnings Before Interest and Taxes* and Interest Expense is the interest charges incurred.\\n\\nThis calculation provides insight into the company's ability to cover its interest expenses with its earnings.\\n\\nThe correct Interest Coverage Ratio for Activision Blizzard in F2019 is 58.32.\"}"}
{"id": "loJM1acwzf", "page_num": 44, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. **Vision**: Ensure that it is connected to the same LAN and logged in to the same HUAWEI ID as your phone.\\n\\n2. **Bluetooth devices**: Some Bluetooth devices (such as Bluetooth headsets) can be linked with your phone via Device+ after establishing a Bluetooth connection. These devices can be linked to your phone to enable seamless collaboration and task transfer.\\n\\nOther than phones, the devices introduced for setting Device+ include:\\n\\n1. **Vision**: Devices like smart TVs or displays that can be linked with your phone for a seamless multi-device experience.\\n\\n2. **Bluetooth devices**: Such as Bluetooth headsets, which can be connected to your phone via Device+ for audio streaming or calls. These devices can be controlled and interacted with through your phone, allowing for tasks like transferring Meetime calls, audio, or video content to your Vision device, or managing audio playback through connected Bluetooth devices.\\n\\nIf Device+ is hidden, access Control Panel and go to > Show Device+.\"}"}
{"id": "loJM1acwzf", "page_num": 45, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: At NTU, how many types of Field Sports can students participate in?\\n\\nReference Answer: 4\\n\\nGPT-4 Answer: At Nanyang Technological University (NTU), students can participate in several types of field sports. According to the guidebook, the field sports available include:\\n\\n1. Cricket\\n2. Football\\n3. Rugby\\n4. Touch Football\\n\\nThese sports provide a variety of options for students interested in field sports, catering to different interests and skills.\\n\\nGemini-1 Answer: The provided pamphlet does not list the number of Field Sports students can participate in. Therefore, I cannot answer your question.\\n\\nOCR+GPT-4 Answer: At NTU, students can participate in six types of Field Sports. These are Cricket, Football, Rugby, Touch Football, Mind Games, and Other Sports.\\n\\nInternVL-v Answer: Based on the image, students at NTU can participate in 5 types of Field Sports. These are listed as Football, Netball, Rugby, Volleyball, and Ultimate Frisbee.\\n\\nOCR+Mixtral Answer: Students at NTU can participate in 10 types of Field Sports: Badminton, Cricket, Football, Rugby, Touch Football, Ultimate Frisbee, Golf, Mind Games, International Chess, and Snooker and Pool.\"}"}
{"id": "loJM1acwzf", "page_num": 46, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: How many children are being read to in the photograph of Page 3?\\n\\nReference Answer: 2\\n\\nGPT-4o \u2705\\nAnswer: In the photograph on Page 3, there are two children being read to by an adult.\\n\\nGPT-4 V \u2705\\nAnswer: In the photograph on Page 3, there are two children being read to.\\n\\nGemini-1.5- Pro \u2705\\nAnswer: There are **two** children being read to in the photograph on page 3.\\n\\nOCR + GPT-4 \u274c\\nAnswer: The text provided does not contain any information about a photograph or the number of children being read to in a photograph on Page 3. Therefore, I'm unable to answer the question based on the given text. If you have access to the photograph or additional details, please share them, and I would be happy to help with your query.\\n\\nInternVL- v1.5 \u2705\\nAnswer: In the photograph of Page 3, there are three children being read to by the woman.\\n\\nOCR + Mixtral-8x22B \u274c\\nAnswer: There are three children being read to in the photograph on Page 3.\"}"}
{"id": "loJM1acwzf", "page_num": 47, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nMMLONG BENCH-DOC is the first comprehensive benchmark designed to evaluate the long-context document understanding capabilities of LVLMs. While our benchmark addresses significant gaps in the previous datasets, we acknowledge several limitations.\\n\\nOne primary limitation is the scale of the benchmark. Currently, our benchmark includes a test set comprising 135 documents and 1,082 questions. It is much smaller compared to previous datasets. The complexity and difficulty of annotations limit the scale of our benchmark. As a long-context benchmark, our documents average about 50 pages and 20,000 tokens. And most questions require either complicated reasoning or cross-page comprehension. It takes more than one hour for an expert-level annotator to read through a single document, and then edit existing instances and create new instances on this document. Given the purpose of MMLONG BENCH as an evaluation benchmark, we prioritize annotation quality over quantity. Moreover, the results presented in Sections 3.3 and 3.4 confirm that the scale of our benchmark is sufficient for fine-grained evaluations across different document types, evidence sources, evidence pages, etc. Additionally, we plan to expand our benchmark by adding more documents and questions in future iterations.\\n\\nWe roughly categorize these questions into three types, i.e., single-page, cross-page, and unanswerable questions, based on whether evidence can be found in the documents and the number of evidence pages. However, unlike MMBench [41] or MathVista [56], we provide no further taxonomy to classify some (e.g., 7 or 20) fine-grained, evaluated reasoning or perception capabilities out of two main reasons: (1) Prior (i.e., pre-annotation) taxonomy limits the diversity of the questions. Therefore we provide no predefined classifications in our guideline and encourage the expert-level annotators to freely write questions without constraints. (2) The intrinsic complexity of document understanding presents significant challenges for establishing a posterior (i.e., post-annotation) taxonomy.\\n\\nWhile there exist limitations in our benchmark, MMLONG BENCH-DOC surely represents a significant step forward in this field. We would iteratively maintain and refine this benchmark and hope it could push forward the development of long-context document understanding.\\n\\nSocial Impacts\\n\\nThe development and use of MMLONG BENCH-DOC may have potential societal implications. For instance, biased or inaccurate outputs from benchmarked models could perpetuate harmful stereotypes or reinforce existing social inequalities. Additionally, the ability to process and analyze long documents could potentially be used to surveil or monitor individuals' personal information. Developers and users of MMLONG BENCH-DOC benchmark must be aware of these potential consequences and take steps to ensure responsible development and deployment of AI models.\\n\\nAuthor Statement\\n\\nThe authors state that all of the previous datasets that we collected are licensed under the Creative Commons license (CC-BY) or other open-source licenses. Using this dataset should abide by the policy of OpenAI. Regarding the newly collected documents, we manually check them to ensure their availability for academic use. Should any authors request the removal of their documents, we will promptly comply.\"}"}
{"id": "loJM1acwzf", "page_num": 48, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Checklist\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n   (b) Did you describe the limitations of your work? [Yes] See Appendix D.\\n   (c) Did you discuss any potential negative societal impacts of your work? [Yes] See supplemental material E.\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A] We didn't involve theory in this benchmark.\\n   (b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] https://mayubo2333.github.io/MMLongBench-Doc\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [N/A]\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A]\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 4.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes]\\n   (b) Did you mention the license of the assets? [Yes] See Appendix F.\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] https://mayubo2333.github.io/MMLongBench-Doc\\n   (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] See Appendix A.1 and A.2\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\"}"}
{"id": "loJM1acwzf", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Understanding documents with rich layouts and multi-modal components is a long-standing and practical task. Recent Large Vision-Language Models (LVLMs) have made remarkable strides in various tasks, particularly in single-page document understanding (DU). However, their abilities on long-context DU remain an open problem. This work presents MMLONGBENCH-DOC, a long-context, multi-modal benchmark comprising 1,082 expert-annotated questions. Distinct from previous datasets, it is constructed upon 135 lengthy PDF-formatted documents with an average of 47.5 pages and 21,214 textual tokens. Towards comprehensive evaluation, answers to these questions rely on pieces of evidence from (1) different sources (text, image, chart, table, and layout structure) and (2) various locations (i.e., page number). Moreover, 33.7% of the questions are cross-page questions requiring evidence across multiple pages. 20.6% of the questions are designed to be unanswerable for detecting potential hallucinations. Experiments on 14 LVLMs demonstrate that long-context DU greatly challenges current models. Notably, the best-performing model, GPT-4o, achieves an F1 score of only 44.9%, while the second-best, GPT-4V , scores 30.5%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse performance than their LLM counterparts which are fed with lossy-parsed OCR documents. These results validate the necessity of future research toward more capable long-context LVLMs.\\n\\n1 Introduction\\n\\nDocuments are one of the fundamental forms of information preservation and exchange. In each year, tens of millions of documents are created, read, saved, and dispatched [1]. Beyond unstructured pure-text, documents feature both complicated layout structures and information across distinct modalities such as text, table, chart, image, etc. Accordingly, the automatic understanding of documents (Document Understanding; DU) stands as a long-standing task in urgent and practical needs. Recently, a number of LVLMs, both closed-source ones (GPT-4o [2], Gemini-1.5 [3], Claude-3 [4], etc.) and open-source ones (InternLM-XC2-4KHD [5], InternVL-Chat [6], Otter [7], LLaV A-NeXT [8], CogVLM [9], mPLUG-DocOwl 1.5 [10], TextMonkey [11], etc.) have been developed and presented the great potential to handle documents. Most of them have achieved promising performance on single-page DU datasets like DocVQA [12], ChartQA [13], InfoVQA [14], TAT-DQA [15], etc. However, considerable amounts of documents in the real world are long-context...\"}"}
{"id": "loJM1acwzf", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: According to this document, what is the main color of Tsinghua Campus Bicycle? Give me the color name only.\\n\\nAnswer: Not Answerable\\n\\nQuestion: I want to see a doctor in the campus hospital. After registering at the registration area, what is the next step?\\n\\nAnswer: Go to the medical department you registered at (i.e., internal medicine, surgical medicine, dental medicine)\\n\\nEvidence Pages: Page 22\\n\\nEvidence Sources: Image, Table\"}"}
{"id": "loJM1acwzf", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison between our benchmark and previous DU datasets.\\n\\n| Document | Question type | Answer Evidence | # Pages | # Tokens | Cross-page (%) | Unans. (%) | Doc. Rel. | Source | Avg. Position |\\n|----------|---------------|-----------------|---------|----------|----------------|------------|-----------|--------|---------------|\\n| DocVQA [12] | \u2714 | \u2714 | 1.0 | 151.5 | \u2713 | \u2717 | TXT/L/C/TAB/I | - |\\n| ChartQA [13] | \u2714 | \u2714 | 2 | 236.9 | \u2717 | \u2717 | L/C/TAB/I | C |\\n| InfoVQA [14] | \u2714 | \u2714 | 2 | 288.0 | \u2717 | \u2717 | L/C/TAB/I | - |\\n| TAT-DQA [15] | \u2714 | \u2714 | 1.1 | 577.0 | \u2717 | \u2717 | TXT/TAB | - |\\n| VisualWebBench [21] | \u2714 | \u2714 | 2 | 452.4 | \u2717 | \u2717 | LAY/I | - |\\n| PWC [22] | \u2714 | \u2714 | ~12* | ~7000* | \u2717 | \u2717 | TAB | - |\\n| MP-DocVQA [16] | \u2714 | \u2714 | 8.3 | 2026.6 | \u2717 | \u2717 | TXT/L/C/TAB/I | 6.0 |\\n| DUDE [17] | \u2714 | \u2717 | 5.7 | 1831.5 | \u2713 | (2.1%) | \u2714 | (12.7%) |\\n| SlideVQA [18] | \u2717 | \u2717 | 20.0 | 2030.5 | \u2713 | (13.9%) | \u2717 | \u2714 |\\n| MML ONG BENCH [19] | \u2714 | \u2717 | 47.5 | 21214.1 | \u2713 | (33.0%) | \u2714 | (22.5%) | \u2714 |\\n\\nThe results in Figure 1(c) highlight the challenges that current LVLMs face with long-context DU. The best-performing LVLM, GPT-4, achieves an overall F1 score of only 44.9%, while the second-best LVLM, GPT-4V, scores 30.5%. Moreover, all the remaining LVLMs tested with multi-modal documents performed worse than single-modal LLMs handling lossy, OCR-parsed texts. Specifically, the Gemini-1.5-Pro and Claude-3-Opus present 4.2% and 6.4% absolute decrease when the inputs change from document screenshots to OCR-parsed texts. Regarding open-source models, the best-performing LVLM lags behind the best-performing LLM by 11.7%. These results reveal that long-context DU is a far-from-resolved task for current LVLMs.\"}"}
{"id": "loJM1acwzf", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"in an end-to-end manner. With the rapid advancement of LVLMs, the latter approach has dominated the current DU solutions. As mentioned above, a range of LVLMs demonstrate promising performance on single-page DU datasets. However, as shown in Section 4, even the most advanced LVLMs fall significantly short of achieving satisfactory performance on our benchmark. It reveals that understanding lengthy documents still poses great challenges to current LVLMs.\\n\\nLong-context LVLMs and LLMs. Lengthy documents necessitate the use of LVLMs or LLMs with extended context sizes. Several benchmarks and solutions have been proposed to evaluate and develop long-context LLMs. However, there exists limited related work for long-context LVLMs, leaving this area largely unexplored. Until very recently, contemporary studies assess and/or improve LVLMs' multi-image understanding capabilities. Evaluations on both MMLONGBENCH-DOC and these works indicate that current LVLMs are still not fully equipped to handle long-context DU and many other practical tasks that require extensive contextual comprehension.\\n\\n3 MMLONGBENCH-DOC\\n\\nWe design a three-stage annotation pipeline for the construction of our benchmark. The three stages will be introduced in Section 3.1, Section 3.2, and Section 3.3, respectively. We also provide key statistics of our benchmark in Section 3.4.\\n\\n3.1 Document Collection\\n\\nAs a long-context DU benchmark, the documents shall be of diverse topics and lengthy enough. To this end, we crawl a great amount of documents from various sources. Then we select the lengthy ones from these documents. Specifically, we encompass a diverse array of documents from two approaches. (1) Existing documents from four previous datasets: DUDE [17], SlideVQA [18], ChartQA [13], and FinanceBench [19]. (2) Newly-collected documents from Arxiv [3], ManualsLib [4] and Google Search [5]. Then we (1) filter out the documents with fewer than 15 pages or license restrictions and (2) down-sample documents from DUDE, SlideVQA, and FinanceBench for a more balanced distribution. Detailed descriptions of our selection and processing procedure can be found in Appendix A.1 and Appendix A.2.\\n\\nIn summary, we collect a total of 135 documents. Among them, 76 documents are from existing datasets and incorporate previously annotated questions (represented as triangles). The remaining 59 documents are newly collected and incorporate no existing questions. We manually categorize them into 7 types: Research Report, Financial Report, Academic Paper, Brochure, Guideline, Administration & Industry File, Tutorial / Workshop. We showcase some instances of these documents in Appendix A.3.\\n\\n3.2 Question and Answer Collection\\n\\nTo serve as a high-quality and comprehensive benchmark, the question annotation of our benchmark adheres to the following standards: (1) All questions shall be neither over-easy nor over-difficult. (2) Questions are not repetitively derived from the same page or the same pattern. (3) The distribution of evidence numbers, evidence sources, and evidence locations for the questions shall be balanced. (4) No questions shall be answered correctly without accessing the relevant documents.\\n\\nTen authors serve as expert-level annotators for the question-and-answer collection. All of them are doctors or Ph.D. students proficient in English reading and writing. Before formal annotation, they undergo a training session and pre-annotate three documents for practice. We iteratively review their annotation results and provide personalized feedback until their annotations meet the standards mentioned above. Regarding the formal annotation, we divide 135 documents into 54 batches (each having 2-4 documents) and dispatch these batches to annotators. We then ask the annotators to submit their results in units of batches and set reasonable time intervals for each batch's submission. We...\"}"}
{"id": "loJM1acwzf", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"timely evaluate their annotations after each submission and remind the annotators if their questions in this turn diverge from the standards. It avoids the annotators rushing all assignments in a short time and benefits the annotation quality. We recommend the annotators take 60-90 minutes on each document. Specifically, the annotators shall rapidly read through the whole document in the first 15-30 minutes. For the remaining time, they shall dive deep into specific components to modify existing annotations and/or add new annotations as detailed below.\\n\\nModify Existing Questions.\\n\\nDocuments collected from existing datasets had been annotated with some questions and answers from previous work. However, their crowd-sourcing annotations inevitably make some questions, answers, and other meta information unqualified. Therefore, we edit their annotations before including them as a component of our benchmark.\\n\\nSpecifically, we classify six potential problems in original annotations:\\n\\n- Wrong Answers or Evidence\\n- Repetitive Question\\n- Ambiguous Question\\n- Decontextualization-required Question\\n- Document-relevant Question\\n- Potential Shortcut\\n\\nSee detailed explanations and examples about these problems in Appendix A.4. Given an existing document, the annotators are tasked to evaluate each existing question\u2019s quality according to whether they have one or more above problems and assign a label from {Retain, Revise, Remove} for each question. Then the annotators would revise the Revise questions to meet our quality criteria and remove the Remove questions. Among all 425 original questions from 76 existing documents, 32.2% of them are revised and 46.1% are removed. We finally collect 211 questions in this procedure. The corresponding GUI is shown in Appendix A.7.\\n\\nAdd New Questions.\\n\\nWe newly annotate questions on both existing and newly collected documents to expand the questions in our benchmark. Specifically, we ask annotators to add about 3 questions on existing documents, and 6 questions on newly-collected documents. Given most existing questions (even after editing) are single-page ones and sourced from texts, we put more focus on (1) cross-page and unanswerable questions and (2) questions sourced from tables, charts, and images for newly added questions to balance the distribution. We detail the quantitative requirements in Appendix A.5.\\n\\nAssociated with questions, annotators also provide reference answers and meta-information (i.e., evidence sources, answer format, evidence locations) for all samples. We finalized a collection of 965 samples in this procedure. The corresponding GUI is shown in Appendix A.7.\\n\\n3.3 Quality Control\\n\\nCombining the merits of humans and LVLMs, we adopt a three-round, semi-automatic quality control procedure to improve the annotation quality of our benchmark. We detail each round in the following components and leave the discussion of potential bias in Appendix A.6.\\n\\nDocument-relevant Detection.\\n\\nOur benchmark is designed to evaluate LVLMs\u2019 long-context document understanding abilities. All questions are expected to be unanswerable without access to corresponding documents. To remove low document-relevant questions (i.e., questions not relying on documents), we feed each annotated question WITHOUT documents to GPT-4o. A question will be identified as low document-relevant question if GPT-4o correctly predicts under this case. Ultimately, 94 samples are identified as low document-relevant questions and removed in this round.\\n\\nSelf-reflection.\\n\\nWe draw inspirations from MMBench [41] and leverage LVLMs to reduce the wrongly-annotated samples. Specifically, we feed the remaining questions from the last round WITH their documents to GPT-4o. Samples whose model predictions are inconsistent with the reference answers are sent back to corresponding annotators. The annotators are asked to check each question and identify whether the inconsistency is caused by problematic annotation or not. As a result, 13.8% of the samples are identified as problematic annotations. The annotators revise them accordingly.\\n\\nCross-checking.\\n\\nIn parallel, annotators cross-check the annotated samples from other annotators and determine the inconsistency reasons the same as described above. We calculate Cohen\u2019s kappa value of their identifications as 0.42 (17.5% inconsistent samples), showing a moderate agreement. Regarding the 17.5% inconsistent samples, two primary authors serve as meta-annotators and make final decisions on them (and if necessary, revise accordingly).\\n\\n3.4 Dataset Overview and Analysis\\n\\nThe main statistics of MMLONGBENCH-DOC are presented in Table 2. Overall, our benchmark consists of 1,082 questions. These questions are constructed upon 135 lengthy documents across 7...\"}"}
{"id": "loJM1acwzf", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"documents\\n- Type 7\\n- Average/Medium pages 47.5 / 28\\n- Average/Medium length 21,214.1 / 12,179\\n\\nTotal questions\\n- Single-page question 494 (45.7%)\\n- Cross-page questions 365 (33.7%)\\n- Unanswerable questions 223 (20.6%)\\n- Derived questions 184 (17.0%)\\n- Newly-annotated questions 898 (83.0%)\\n\\n(Evidence source)\\n- Pure-text 305 (35.5%)\\n- Layout 119 (13.9%)\\n- Table 218 (25.4%)\\n- Chart 178 (20.7%)\\n- Image 304 (35.4%)\\n\\n(Answer Format)\\n- String 250 (29.1%)\\n- Integer 299 (34.8%)\\n- Float 159 (18.5%)\\n- List 151 (17.6%)\\n\\nAvg./Max. question length 16.4 / 60\\nAvg./Max. answer length 2.8 / 54\\n\\nTable 2: Dataset Statistics\\nBrochure (11.1%)\\nResearch Report (25.2%)\\nAdministration & Industry File (7.4%)\\nTutorial / Workshop (12.6%)\\nFinancial Report (8.1%)\\nGuidebook (16.3%)\\nAcademic Paper (19.3%)\\n\\nFigure 2: Detailed distribution of documents.\\n\\nTop: Document type.\\nMiddle: Page Number.\\nBottom: Total Number.\\n\\nFigure 3: Detailed distribution of questions & answers.\\nLeft: Absolute position of answer evidences (the page index).\\nMiddle: Relative position (the page index/document page number).\\nRight: Evidence page number of each question. (0: unanswerable question; >2: cross-page question).\\n\\nDocument types, with an average of 47.5 pages and 21,214.1 tokens. Please see detailed distributions of these documents in Figure 2.\\n\\nRegarding the questions, there are 494 single-page questions (1 evidence page), 365 cross-page questions (2+ evidence pages), and 223 unanswerable questions (no evidence page). These three types of questions evaluate the LVLMs\u2019s long-context DU capabilities from complementary aspects: the localization ability, the cross-page comprehension ability, and the hallucination severity, respectively. For single-page and cross-page questions, their answer evidence is scattered among different context sources (i.e., text, layout, table, chart, image) and evenly distributed across different locations of the documents (see Table 2, Figure 3 Left and Middle). Also notably, 28.6% of cross-page questions have more than two evidence pages, which further enhances the challenge of our benchmark.\\n\\n4 Evaluation\\n\\n4.1 Evaluation Protocol\\n\\nWe follow MATHVISTA [56] to conduct a three-step evaluation protocol: response generation, answer extraction, and score calculation. We adopt such a protocol out of three considerations:\\n(1) Current LVLMs are instructed to generate long responses, rather than short-form answers, in conventional settings.\\n(2) The evaluation of long responses, however, remains an open and challenging problem.\\n(3) We focus on the document understanding (not instruction following) abilities of LVLMs.\"}"}
{"id": "loJM1acwzf", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Evaluation of various models on MML-DOC.\\n\\nWe report the generalized accuracy of five types of evidence sources including pure text (TXT), layout (LAY), chart (CHA), table (TAB), and image (IMG). We also present the generalized accuracy of questions categorized by the number of evidence pages: single-page (SIN), cross-page (MUL), and unanswerable (UNA) questions. The best and second-best performance in each section are highlighted.\\n\\n| Model                                      | Evidence Source | Evidence Page | ACC | F1   | Window |\\n|--------------------------------------------|-----------------|---------------|-----|------|--------|\\n| OCR (Tesseract [42]) + Large Language Models (LLMs) | TXT  | LAY  | CHA | TAB | FIG    |\\n| Open-source Models                        |                 |               |     |      |        |\\n| ChatGLM-128k [37]                         | 6B              | 128k          | 23.4| 12.7 | 9.7    |\\n| Mistral-Instruct-v0.2 [43]                | 7B              | 32k           | 19.9| 13.4 | 10.2   |\\n| Mixtral-Instruct-v0.1 [44]                | 8x7B            | 32k           | 24.2| 14.8 | 12.5   |\\n| Mixtral-Instruct-v0.1 [44]                | 8x22B           | 64k           | 34.2| 21.3 | 19.5   |\\n| Proprietary Models                        |                 |               |     |      |        |\\n| QWen-Plus [45]                            |                 |               | 17.4| 15.6 | 7.4    |\\n| DeepSeek-V2 [46]                          |                 |               | 27.8| 19.6 | 8.8    |\\n| Claude-3 Opus [4]                         |                 |               | 30.8| 30.1 | 16.4   |\\n| Gemini-1.5-Pro [3]                        |                 |               | 29.3| 15.9 | 12.5   |\\n| GPT-4-turbo [47]                          |                 |               | 36.5| 21.0 | 20.7   |\\n| GPT-4o [2]                                |                 |               | 41.1| 23.4 | 28.5   |\\n| Large Visual Language Models (LVLMs)       |                 |               |     |      |        |\\n| Open-source, 7-14B Models                 |                 |               |     |      |        |\\n| DeepSeek-VL-Chat [48]                      | 7.3B            | 4k            | 7.2 | 6.5  | 1.6    |\\n| Idefics2 [49]                             | 8B              | 8k            | 9.0 | 10.6 | 4.8    |\\n| MiniCPM-Llama3-V2.5 [50; 51]              | 8B              | 2k            | 11.9| 10.8 | 5.1    |\\n| InternLM-XC2-4KHD [5]                     | 8B              | 16k           | 9.9 | 14.3 | 7.7    |\\n| mPLUG-DocOwl 1.5 [52]                     | 8.1B            | 4k            | 8.2 | 8.4  | 2.0    |\\n| Qwen-VL-Chat [53]                         | 9.6B            | 6k            | 5.5 | 9.0  | 5.4    |\\n| Monkey-Chat [54]                          | 9.8B            | 2k            | 6.8 | 7.2  | 3.6    |\\n| Proprietary Models                        |                 |               |     |      |        |\\n| Claude-3 Opus [4]                         |                 |               | 24.9| 24.7 | 14.8   |\\n| Gemini-1.5-Pro [3]                        |                 |               | 21.0| 17.6 | 6.9    |\\n| GPT-4V(ision) [47]                        |                 |               | 34.4| 28.3 | 28.2   |\\n| GPT-4o [2]                                |                 |               | 46.3| 23.4 | 28.5   |\\n\\nSpecifically, we impose no limitations on response generation stage to encourage LVLMs to answer the questions in a freestyle. Then we propose a unified LLM-based answer extractor (GPT-4o under our setting) to convert their long responses to short-form answers. Finally, we use a rule-based score calculator to evaluate the converted short answers. We report both generalized accuracy and generalized F1 score to balance the answerable (positive) and unanswerable (negative) questions. The used prompt, the high correlation between our automatic answer extractor and human evaluation, and the detailed rules of our score calculation are described in Appendix B.\\n\\n4.2 Experimental Setup\\n\\nWe evaluate 14 LVLMs on MML-ONG-BENCH-DOC, including 4 proprietary LVLMs and 10 open-source LVLMs. To purely evaluate LVLMs' long-context DU abilities, we screenshot each page of the PDF-formatted document with 144 DPI and feed all these PNG-formatted images to LVLMs in an end-to-end approach. Notably, all evaluated open-source LVLMs do not support multi-image inputs or present significant performance drops when fed with excessive images (e.g., more than 10 or 20 images). Therefore, we employ a concatenation strategy that combines all screenshot pages into 1 or 5 images and feeds these concatenated images to open-source LVLMs. Regarding proprietary LVLMs, we adopt the same concatenation strategy and reduce the image number to 20 for Claude-3-Opus to fit its maximum image threshold. For GPT-4o, GPT-4V, and Gemini-1.5-Pro, we directly send all original screenshots to them (i.e., the image number equals the page number).\\n\\nFor comparison, we also use the Tesseract [42] OCR model to recognize and extract texts from the documents and feed the parsed documents to 10 LLMs, including 6 proprietary and 4 open-source.\"}"}
{"id": "loJM1acwzf", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.3 Main Results\\n\\nWe compare the performance of different LVLMs and LLMs in Table 3, reporting their generalized accuracy and F1 scores (shown in the last two columns). Regarding LVLMs, we draw several conclusions as below: (1) The performance demonstrates that long-context DU is still a challenging and unsolved task for current LVLMs. The best-performing LVLM, GPT-4o, merely achieves a 44.9% F1 score. The second best-performing LVLM, GPT-4V, lags behind by over 10% and presents a 31.4% F1 score. All other LVLMs only achieve about 20% or even lower F1 scores. (2) Though far from satisfactory, GPT-4o performs much better than all other models (including GPT-4V). Thus we speculate that the multi-modal pre-training paradigm significantly benefits LVLMs\u2019 cross-modality understanding capabilities. (3) Proprietary LVLMs perform better than open-source LVLMs by a large margin. We attribute it to the difference of acceptable image numbers: open-source LVLMs only support single-image or several-image inputs, while proprietary LVLMs can be fed with at least 20 images or even more. Given that lengthy documents have tens of even hundreds of pages, it is impractical for open-source LVLMs to accurately perceive the information in the documents from the excessively concatenated images. (4) The performances of different models are highly correlated with their acceptable image numbers and maximum image resolutions. Notably, open-source LVLMs that support high-resolution images (i.e., InternLM-XC2-4KHD and InternVL-Chat-v1.5) exhibit superior performance compared to those with lower resolution limits. Surprisingly, LVLMs even demonstrate overall worse performance than LLMs, even LLMs are fed with lossy OCR-parsed documents. Specifically, Gemini-1.5-Pro and Claude-3 Opus have 4.2% and 6.4% absolute F1-score degradations on vision versions. And the best-performing LLM (Mixtral) also surpasses the best-performing LVLM (InternVL-v1.5) by 11.7%. The above results clearly reveal that most current LVLMs are still not proficient in cross-modality, long-context document understandings. It is promising that GPT-4o and GPT-4-turbo achieve better performance when seeing multi-modality PDF documents than parsed text by 14.4% and 5.3% F1-score, respectively. Their performances validate the feasibility, benefit, and necessity of understanding documents in an end-to-end, cross-modality approach. We speculate that the scarce related pre-training corpus (i.e., extremely multi-image or lengthy documents) hinders the long-context DU capabilities of other LVLMs. We will leave related explorations for future work.\\n\\nRegarding the human evaluation, we observe 66.0% F1-score from our annotators and a significant performance gap (exceeding 20% in absolute) between the current LVLMs and humans. This gap highlights the challenges of document understanding for LVLMs and the necessity of our benchmark.\\n\\n4.4 Fine-grained Results.\\n\\nDocument Type.\\n\\nAs illustrated in Figure 4, LVLMs and LLMs exhibit distinct performance patterns across various document types. Our findings include: (1) All evaluated models demonstrate decent performance on industrial documents, which tend to have more standardized formats and less non-textual information. (2) The GPT series and Mixtral (i.e., the SoTA open-source LLM) show relatively balanced performance across different document types. In contrast, other models perform significantly worse in specialized domains such as academic papers and financial reports. (3) When equipped with OCR, LLM-based models like GPT-4 and Mixtral achieve comparable or even superior performance on industrial documents, academic papers, and brochures. Conversely, end-to-end LVLMs outperform OCR+LLMs in areas such as tutorials, research reports, and guidelines. We speculate that comprehending these latter document types requires more extensive multi-modal information, from which LVLMs significantly benefit.\\n\\nEvidence Source.\\n\\nWe categorize questions based on their evidence sources and present fine-grained results in Figure 4 and Table 3. Our observations reveal that only GPT-4o exhibits relatively balanced\"}"}
{"id": "loJM1acwzf", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Fine-grained results on various document types and evidence sources.\\n\\nPerformance across the different sources. Other LVLMs, however, show inferior performance on questions related to charts and/or images compared to those related to text and/or layout. Additionally, LLMs generally demonstrate better or comparable performance to LVLMs on text- and table-related questions but show worse performance on questions involving other elements. This highlights the limitations of OCR (and other PDF parsers) when dealing with charts and images, as well as the gap in OCR capabilities between LVLMs and pure-text LLMs.\\n\\nFigure 5: Relationships between evidence positions and model performances.\\n\\nEvidence Position. We also examine how the evidence locations (i.e., the page indexes where the answer evidence is found) affect model performance. The results shown in Figure 5 reinforce that MML BENCH-DOC poses significant challenges for current models, at least partially due to the extended length of the documents. Almost all models (except InternVL-v1.5) exhibit their best performance on questions derived from the initial pages, while their performance declines progressively as the page index increases. Interestingly, two proprietary models, Gemini-Pro-1.5 and Claude-3-Opus, experience particularly sharp declines in performance.\\n\\nNumber of Evidence Page. We observe a consistent trend that all models achieve higher scores on single-page questions than cross-page questions. It reveals that gathering and reasoning over all necessary information across different pages is not trivial for current LVLMs and LLMs. More interestingly, evaluated LVLMs behave differently on unanswerable questions. GPT-4o and Claude-3 Opus adopt more aggressive strategies and usually tend to provide some answers. It makes their answers more likely helpful, but also increases the risk of hallucination and unfaithfulness (see their scores on unanswerable questions are much lower than answerable questions). On the contrary, Gemini-1.5-Pro, DeepSeek-VL-Chat, and EMU2-Chat are much more cautious and tend to refuse to answer questions about which they are uncertain. It makes their answers safer but less helpful (with large amounts of responses like I don't know).\\n\\n5 Analysis & Discussion\\n5.1 Oracle Setting\\nWe conduct additional experiments to explore to what extent the challenges of MML BENCH-DOC are caused by the long-context lengths of documents. Specifically, we feed 820 answerable questions along with their oracle evidence pages (instead of the whole documents) to three representative LVLMs and show results in Figure 6. On one hand, it indicates that long-context length is a\"}"}
{"id": "loJM1acwzf", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"significantly challenging factor for document understanding. Compared with the oracle-page setting, lengthy documents lead to more than 20% absolute performance degradation on Gemini-1.5-Pro and InternLM-XC2-4KHD. Regarding the single-page questions, the performance difference even achieves up to 30%. On the other hand, the overall performance achieves only about 40% and 30% for Gemini-1.5-Pro and InternLM-XC2-4KHD even under oracle-page setting. And the improvement for GPT-4o is much less (about 10%). It demonstrates that the development of long-context LVLMs can largely facilitate, though still can not fully solve, the long-context DU task.\\n\\n5.2 Error Analysis\\n\\nKnowledge Lacking (3%)\\nHallucinated Evidence (33%)\\nExtractor Error (10%)\\nReasoning Error (5%)\\nPerceptual Error (28%)\\nIrrelevant Answer (11%)\\nIncomplete Evidence (10%)\\n\\nFigure 7: Error distribution\\n\\nWe further conduct error analysis to understand the bottleneck of current LVLMs in a qualitative approach. Specifically, we randomly select 72 error predictions from GPT-4o's responses and manually check their error reasons. These errors are categorized into seven types: Perceptual Error, Irrelevant Answer, Incomplete Evidence, Hallucinated Evidence, Extractor Error, Reasoning Error and Knowledge Lacking. The distribution of these errors is illustrated in Figure 7. It indicates that most errors come from the model's hallucination (i.e., wrong explanations and answers to unanswerable questions) and perceptual errors (mainly in visual contexts). Additionally, GPT-4o sometimes misunderstands the intent of questions and provides irrelevant responses. The errors caused by collecting incomplete evidence (for cross-page questions) are also unignorable. The descriptions and examples of these error types are detailed in Appendix C.1.\\n\\n6 Conclusion\\n\\nIn this work, we present MMLONGBENCH-DOC to evaluate the long-context DU capabilities of LVLMs. Extensive experiments on 14 LVLMs (and 10 LLMs for comparison) reveal that the understanding of lengthy documents poses great challenges to current LVLMs. Even though the performance of GPT-4o proves the benefit of end-to-end, multi-modality perception for DU tasks, most LVLMs struggle on long visual contexts (i.e., extremely multiple images) and show inferior performance compared to OCR+LLM pipelines. We hope that the construction of our benchmark could push forward the development of more powerful LVLMs on lengthy document understanding.\\n\\nAcknowledgements\\n\\nThis study is supported under the RIE2020 Industry Alignment Fund \u2013 Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). This work is also supported by Shanghai Artificial Intelligence Laboratory, the National Key R&D Program of China (2022ZD0160201).\"}"}
{"id": "loJM1acwzf", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Lutz Bornmann and R\u00fcdiger Mutz. Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references. Journal of the Association for Information Science and Technology, 66, 2014.\\n\\n[2] Open AI. Hello gpt-4, 2024.\\n\\n[3] Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024.\\n\\n[4] Anthropic. Introducing the next generation of claude, 2024.\\n\\n[5] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et al. Internlm-Xcomposer2-4KHD: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. ArXiv preprint, abs/2404.06512, 2024.\\n\\n[6] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. ArXiv preprint, abs/2404.16821, 2024.\\n\\n[7] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning, 2023.\\n\\n[8] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. LLaV A-NeXT: Stronger llms supercharge multimodal capabilities in the wild, 2024.\\n\\n[9] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. CogVLM: Visual expert for pretrained language models, 2023.\\n\\n[10] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding, 2024.\\n\\n[11] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai. Textmonkey: An ocr-free large multimodal model for understanding document, 2024.\\n\\n[12] Minesh Mathew, Dimosthenis Karatzas, R. Manmatha, and C. V. Jawahar. Docvqa: A dataset for vqa on document images. 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 2199\u20132208, 2020.\\n\\n[13] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: A benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2263\u20132279, Dublin, Ireland, 2022. Association for Computational Linguistics.\\n\\n[14] Minesh Mathew, Viraj Bagal, Rub\u00e8n P\u00e9rez Tito, Dimosthenis Karatzas, Ernest Valveny, and C.V. Jawahar. Infographicvqa. 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 2582\u20132591, 2021.\\n\\n[15] Fengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang, and Tat-Seng Chua. Towards complex document understanding by discrete reasoning. In Proceedings of the 30th ACM International Conference on Multimedia, pages 4857\u20134866, 2022.\\n\\n[16] Rub\u00e8n Tito, Dimosthenis Karatzas, and Ernest Valveny. Hierarchical multimodal transformers for multi-page docvqa, 2023.\\n\\n[17] Jordy Van Landeghem, Rub\u00e8n P\u00e9rez Tito, \u0141ukasz Borchmann, Michal Pietruszka, Pawel J'oziak, Rafal Powalski, Dawid Jurkiewicz, Micka\u00ebl Coustaty, Bertrand Ackaert, Ernest Valveny, Matthew B. Blaschko, Sien Moens, and Tomasz Stanislawek. Document understanding dataset and evaluation (DUDE). In ICCV, 2023.\\n\\n[18] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. SlideVQA: A dataset for document visual question answering on multiple images. In AAAI, 2023.\\n\\n[19] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. FinanceBench: A new benchmark for financial question answering, 2023.\"}"}
{"id": "loJM1acwzf", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u0141ukasz Borchmann, Michal Pietruszka, Tomasz Stanislawek, Dawid Jurkiewicz, Micha\u0142 Turski, Karolina Szyndler, and Filip Gralinski. Due: End-to-end document understanding benchmark. In NeurIPS Datasets and Benchmarks, 2021.\\n\\nJunpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. VisualWebBench: How far have multimodal llms evolved in web page understanding and grounding?, 2024.\\n\\nMarcin Kardas, Piotr Czapla, Pontus Stenetorp, Sebastian Ruder, Sebastian Riedel, Ross Taylor, and Robert Stojnic. AxCell: Automatic extraction of results from machine learning papers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8580\u20138594, Online, 2020. Association for Computational Linguistics.\\n\\nJon Saad-Falcon, Joe Barrow, Alexa Siu, Ani Nenkova, David Seunghyun Yoon, Ryan A. Rossi, and Franck Dernoncourt. Pdftriage: Question answering over long, structured documents, 2023.\\n\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369\u20132380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.\\n\\nYiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash, editors, KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 1192\u20131200. ACM, 2020.\\n\\nYang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou. LayoutLMv2: Multi-modal pre-training for visually-rich document understanding. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2579\u20132591, Online, 2021. Association for Computational Linguistics.\\n\\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for document ai with unified text and image masking. In Proceedings of the 30th ACM International Conference on Multimedia, MM '22, page 4083\u20134091, New York, NY, USA, 2022. Association for Computing Machinery.\\n\\nGeewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision (ECCV), 2022.\\n\\nKenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: screenshot parsing as pretraining for visual language understanding. In Proceedings of the 40th International Conference on Machine Learning, ICML'23. JMLR.org, 2023.\\n\\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long language sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 12007\u201312021, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics.\\n\\nChenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-eval: Instituting standardized evaluation for long context language models, 2023.\\n\\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: A bilingual, multitask benchmark for long context understanding. ArXiv preprint, abs/2308.14508, 2023.\\n\\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. \u221ebench: Extending long context evaluation beyond 100k tokens, 2024.\\n\\nSzymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Milo\u2019s. Focused transformer: Contrastive training for context scaling. ArXiv preprint, abs/2307.03170, 2023.\\n\\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. ArXiv preprint, abs/2309.12307, 2023.\"}"}
{"id": "loJM1acwzf", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. ArXiv preprint, abs/2309.00071, 2023.\\n\\nYushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. LongAlign: A recipe for long context alignment of large language models. ArXiv preprint, abs/2401.18058, 2024.\\n\\nDingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang. Milebench: Benchmarking mllms in long context. ArXiv preprint, abs/2404.18532, 2024.\\n\\nDongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning, 2024.\\n\\nYujie Lu, Xiujun Li, Tsu-Jui Fu, Miguel Eckstein, and William Yang Wang. From text to pixel: Advancing long-context understanding in mllms, 2024.\\n\\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. MMbench: Is your multi-modal model an all-around player? ArXiv preprint, abs/2307.06281, 2023.\\n\\nRay Smith. An overview of the tesseract ocr engine. In ICDAR, 2007.\\n\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b, 2023.\\n\\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mixtral of experts, 2024.\\n\\nQwen Team. Introducing qwen1.5, 2024.\\n\\nDeepSeek-AI. DeepSeek-V2: A strong, economical, and efficient mixture-of-experts language model, 2024.\\n\\nOpenAI. GPT-4 technical report, 2024.\\n\\nHaoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. DeepSeek-VL: towards real-world vision-language understanding. ArXiv preprint, abs/2403.05525, 2024.\\n\\nHugo Lauren\u00e7on, L\u00e9o Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?, 2024.\\n\\nTianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. RLAIF-V: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. ArXiv preprint, abs/2405.17220, 2024.\\n\\nRuyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, and Gao Huang. LLaV A-UHD: an lmm perceiving any aspect ratio and high-resolution images. ArXiv preprint, abs/2403.11703, 2024.\\n\\nAnwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, et al. mPLUG-DocOwl 1.5: Unified structure learning for ocr-free document understanding. ArXiv preprint, abs/2403.12895, 2024.\\n\\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: A frontier large vision-language model with versatile abilities. ArXiv preprint, abs/2308.12966, 2023.\\n\\nZhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. ArXiv preprint, abs/2311.06607, 2023.\\n\\nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. ArXiv preprint, abs/2312.13286, 2023.\"}"}
{"id": "loJM1acwzf", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024.\\n\\nTomasz Stanislawek, Filip Grali\u0144ski, Anna Wr\u00f3blewska, Dawid Lipi\u0144ski, Agnieszka Kaliska, Paulina Rosalska, Bartosz Topolski, and P. Biecek. Kleister: Key information extraction datasets involving long documents with complex layouts. In IEEE International Conference on Document Analysis and Recognition, 2021.\\n\\nS. Svetlichnaya. DeepForm: Understand structured documents at scale., 2020.\\n\\nGuillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. Funsd: A dataset for form understanding in noisy scanned documents. 2019 International Conference on Document Analysis and Recognition Workshops (ICDARW), 2:1\u20136, 2019.\\n\\nZheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and C. V. Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1516\u20131520, 2019.\\n\\nAniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5376\u20135384, 2017.\\n\\nNitesh Methani, Pritha Ganguly, Mitesh M. Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In The IEEE Winter Conference on Applications of Computer Vision (WACV), March 2020.\\n\\nRyota Tanaka, Kyosuke Nishida, and Sen Yoshida. Visualmrc: Machine reading comprehension on document images. ArXiv, abs/2101.11272, 2021.\\n\\nXingyu Chen, Zihan Zhao, Lu Chen, JiaBao Ji, Danyang Zhang, Ao Luo, Yuxuan Xiong, and Kai Yu. WebSRC: A dataset for web-based structural reading comprehension. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4173\u20134185, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\"}"}
{"id": "loJM1acwzf", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Benchmark Construction Details\\n\\nA.1 Existing Document Collection\\n\\nAlthough previous datasets contain a relatively small proportion of lengthy documents, their absolute quantity should not be disregarded. Therefore, we compile lengthy documents from various datasets to include them as part of the documents in this benchmark. Specifically, we review and consider 21 previous document understanding (DU) datasets, and ultimately select 4 of them for further document selection. The selection reasons are shown in Table 4. All of these four datasets are licensed under the Creative Commons license (CC-BY) or other open-source licenses. Regarding the 4 selected datasets: DUDE [17], SlideVQA [18], ChartQA [13] and FinanceBench [19], we collect a total of 76 documents and detail our collection procedures as below.\\n\\n| Dataset          | Selected | Comment                                      |\\n|------------------|----------|----------------------------------------------|\\n| DUDE [17]        | \u2713        | -                                            |\\n| SlideVQA [18]    | \u2713        | -                                            |\\n| ChartQA [13]     | \u2713        | -                                            |\\n| FinanceBench [19]| \u2713        | -                                            |\\n| DocVQA [12]      | \u2717        | Repetitive with some documents/questions in DUDE; Single-page documents only |\\n| MP-DocVQA [16]   | \u2717        | Repetitive with some documents/questions in DUDE; Single-page questions only |\\n| Kleister Charity [57] | \u2717      | Repetitive with some documents/questions in DUDE; Over-simple |\\n| Kleister NDA [57] | \u2717        | Repetitive with some documents/questions in DUDE; Over-simple |\\n| DeepForm [58]    | \u2717        | Repetitive with some documents/questions in DUDE; Over-simple |\\n| FUNSD [59]       | \u2717        | Repetitive with some documents/questions in DUDE; Over-simple |\\n| Infograohics VQA [14] | \u2717      | Infographs are not long-context documents |\\n| TAT-QA [15]      | \u2717        | Repetitive with some documents/questions in FinanceBench |\\n| PWC [22]         | \u2717        | Repetitive with our self-annotated questions from academic papers |\\n| PaperQA [56]     | \u2717        | Repetitive with our self-annotated questions from academic papers |\\n| TextbookQA [61]  | \u2717        | Low document-relevance; Over-simple |\\n| PlotQA [62]      | \u2717        | Repetitive with our self-annotated questions from academic papers and research reports |\\n| VisualMRC [63]   | \u2717        | Human performance reached; Website screenshots are not long-context documents |\\n| WebSRC [64]      | \u2717        | Human performance reached; Website screenshots are not long-context documents |\\n| VisualWebBench [21] | \u2717   | Human performance reached; Website screenshots are not long-context documents |\\n| PDFTriage [23]   | \u2717        | Not publicly available |\\n\\nDUDE:\\nWe first filter all documents over 15 pages in the validation set of the original dataset, resulting in 87 documents. From these, we randomly sample 23 to include as a component of our benchmark documents.\\n\\nSlideVQA:\\nWe download slide decks in the test set by following the instructions in the original repository. Pursuing lengthy documents, we slightly modified the code to remove the 20-page truncation procedure. Then we randomly select 27 slide decks for our benchmark documents.\\n\\nFinanceBench:\\nWe randomly sample 5 financial reports from the test set.\\n\\nChartQA:\\nDifferent from the above three datasets, ChartQA only contains chart screenshots cropped from documents. We take the following steps to recover these original documents: (1) We use the Tesseract OCR model [42] to recognize the text within the charts. (2) We use these texts as keywords to search for related documents on Google Search. (3) We manually identify these documents and remove all those that are less than 15 pages. From the ChartQA test set, we finalize a collection of 53 research reports from the Pew Research Center. We randomly sample 18 of these documents to include as a component of our benchmark documents.\\n\\n6 https://github.com/nttmdlab-nlp/SlideVQA\"}"}
{"id": "loJM1acwzf", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.2 Newly-annotated Document Collection\\n\\nMost documents collected from previous datasets are Industrial Files, Tutorial & Workshop, Finance Report and Research Report. To diversify our benchmark, we additionally collect 59 documents including Academic Paper, Brochure, and Guideline. We detail the collection procedures as below.\\n\\nAcademic Paper\\n\\nWe collect 24 academic papers from Arxiv. All selected papers are over 15 pages (including references and appendix). To ensure annotation quality, each paper is either written or thoroughly read by at least one of the annotators.\\n\\nGuideline and Brochure\\n\\nWe collect 21 guidelines and 14 brochures from either ManualsLib or Google Search, covering diverse topics such as school, company, institution, products, service etc. Each document is manually reviewed by one corresponding annotator and other primary authors to ensure its availability for academic use.\\n\\nA.3 Document Examples\\n\\nAs stated in Section 2.1, the documents in MML-ONGBENCH-DOC can be categorized into seven types. We show the examples of each type as below.\\n\\nFigure 8: Document example about Administration & Industrial File\\n\\nFigure 9: Document example about Tutorial & Workshop (only show first 50 pages)\\n\\nShould any authors request the removal of their documents, we will promptly comply.\"}"}
{"id": "loJM1acwzf", "page_num": 17, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "loJM1acwzf", "page_num": 18, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "loJM1acwzf", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.4 Existing Question Editing\\n\\nDocuments collected from existing datasets had been annotated with some questions and answers. However, their crowd-sourcing annotations inevitably make some questions, answers, and other meta information unqualified. So we conduct a systematic and manual pipeline to edit their annotations. Specifically, we classify six potential problems in original annotations. The definitions and examples of these problems are shown below.\\n\\n1. Wrong Answer or Evidence Pages:\\nThe reference answers and/or evidence pages in original datasets are wrongly annotated.\\n\\nOriginal Question: Why is the service not safe?\\n\\nOriginal Answer: Medicines were not managed safely.\\n\\nCorrected Answer: 1. Medicines were not managed safely. 2. There were insufficient staff to care for people's needs during the evening and at night. 3. Some risks to people's health and wellbeing were not assessed and action was not taken to reduce the risk. 4. Safeguarding incidents were not investigated or reported appropriately.\\n\\nError type: Wrong Answer or Evidence\\n\\nComment: The original answer only mentions single point of the safety problem and is incomplete: in fact, there are a total of four points.\\n\\nFigure 15: Example of the original annotation with Wrong Answer or Evidence Pages.\"}"}
{"id": "loJM1acwzf", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Repetitive Question: Too many questions with the same types (e.g., key information extraction) occur in a single document (or even on the same page or point).\\n\\nOriginal Question 1: What is the designation of Diane Hanson?  \\nOriginal Answer 1: Superior court of Delaware\\n\\nError type: Repetitive Question\\nComment: The above two questions are created repeatedly upon a single point of the document. We call them repetitive questions and drop any one of them.\\n\\nOriginal Question 2: Who is the superior court of Delaware?  \\nOriginal Answer 2: Diane Hanson\\n\\nFigure 16: Example of the original annotation with Repetitive Question.\"}"}
{"id": "loJM1acwzf", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The question is ambiguous at the document level (e.g., the absence of entity, period, exact section or page, etc.), or too broad to exactly answer.\\n\\nOriginal Question: What is the telephone no?\\n\\nOriginal Answer: 01983 873655\\n\\nError type: Ambiguous Question\\n\\nComment: There are two main entities occurred in this report: (1) The Limes Residential Home, and (2) Care Quality Commission. There is neither explicit statement nor the implicit coreference about any entity in this question, making it ambiguous.\\n\\nRevised Question: What is the telephone no for The Limes Residential Home?\\n\\nRevised Answer: 01983 873655\\n\\nFigure 17: Example of the original annotation with Ambiguous Question.\"}"}
{"id": "loJM1acwzf", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. Potential Shortcut:\\nThe resolution of the question does not rely on two entities (across different pages) but only one of them, i.e., there exists a shortcut for this question.\\n\\nOriginal Question:\\nWhy did the company which Mamoon Hamid is affiliated with invest $1M in the seed for greenhouse?\\n\\nOriginal Answer:\\nStrong conviction in team, market and early customer validation.\\n\\nError Type: Potential Shortcut\\nComment:\\nThe coreference of Adventure Capital circled in white in the left slide, i.e., the company which Mammon Hamid is affiliated with, makes no sense for answering this question. It is because that the words circled in blue in the right slide, i.e., invest $1M in the seed, is a potentially a strong shortcut for answering this question. Though seemingly relying on the information across two pages, it is still likely a single-page question.\\n\\nRevised Question:\\nWhy did greenhouse invest $1M in the seed for greenhouse?\\n\\nRevised Answer:\\nStrong conviction in team, market and early customer validation.\"}"}
{"id": "loJM1acwzf", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. Low Document-relevant Question:\\nThe resolution of the question does not rely on the information from the document. It can be solved by the parametric knowledge in the LVLMs.\\n\\nOriginal Question: Kailali is in which region of Nepal?\\nOriginal Answer: Far-western region\\n\\nError type: Low document-relevant Question\\n\\nComment: This question is originally intended to evaluate the model's understanding ability on this map. However, the development of LVLMs makes this question not relying on the information of this document anymore (can be answered by model's parametric knowledge).\\n\\nRevised Question: What is the color of Kailali in the map of Page 12?\\nRevised Answer: Yellow\\n\\nFigure 19: Example of the original annotation with Low Document-relevant Question.\"}"}
{"id": "loJM1acwzf", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. Decontextulization-required Question:\\n\\nThe understanding of the question is conditioned on a single page or even a single component of the document.\\n\\nOriginal Question:\\n\\nWhat is the percentage value of the first gray bar from the top?\\n\\nOriginal Answer:\\n\\n29\\n\\nError type: Decontextulization-required\\n\\nRevised Question:\\n\\nWhat is the percentage value of west Germany respondents viewing Germany's relationship with the United States as important as its relationship with Russia?\\n\\nRevised Answer:\\n\\n29\\n\\nFigure 20: Example of the original annotation with Decontextulization-required Question.\\n\\nWhen dealing with questions categorized under any of these six problem types, annotators are instructed to either revise or remove them. Typically, repetitive questions and those with potential shortcuts are removed. In contrast, wrongly-annotated or decontextualization-required questions are generally revised. For ambiguous and low document-relevant questions, the course of action depends more on the annotators' discretion.\"}"}
{"id": "loJM1acwzf", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We annotate new questions on both existing and newly-collected documents. To ensure a diverse range of questions, we impose limitations on the question distributions categorized by their types (i.e., single-page, cross-page or unanswerable) and evidence sources (i.e., table, chart, image). To balance existing questions which are mostly single-page and text-based, we place greater emphasis on cross-page, unanswerable, table-related, chart-related, and image-related questions. The detailed standards are as follows:\\n\\n| Document Type                  | Cross-page | Unanswerable | Table | Chart | Image |\\n|--------------------------------|------------|--------------|-------|-------|-------|\\n| Industrial File                | \u22652         | -            | \u22653    | -     | -     |\\n| Workshop & Tutorial            | \u22652         | \u22651           | -     | \u22653    | -     |\\n| Research Report                | \u22653         | \u22651           | \u22652    | -     | -     |\\n| Financial Report               | \u22655         | \u22652           | -     | -     | -     |\\n| Academic Paper                 | \u22653         | \u22651           | -     | -     | \u22653    |\\n| Guidebook                      | \u22653         | \u22651           | -     | -     | \u22654    |\\n| Brochure                       | \u22652         | \u22651           | -     | -     | \u22653    |\\n\\nTable 5: The minimum requirements for the number and distribution of questions, categorized by the evidence page numbers and evidence sources. We have set varying requirements for different document types based on their specific characteristics.\\n\\nA.6 Potential Bias for LVLM-based Quality Checking\\n\\nAs described in Section 3.3, we employ GPT-4o to remove document-agnostic (i.e., can be correctly answer without documents) samples and review potential wrongly-labeled samples. A reasonable speculation raises that our final benchmark can be biased toward GPT-4o's answers, especially when GPT-4o outperforms others by a large margin. We discuss this potential bias as follows.\\n\\nWe check the effect of GPT-4o's involvement in the quality control step-by-step. Specifically, we compare the performance of samples remained after each step across GPT-4o and two other competitive models (GPT-4V and Gemini-1.5-Pro). We show their results in the table below.\\n\\n|                     | GPT-4o (%) | GPT-4V (%) | Gemini-1.5-Pro (%) |\\n|---------------------|------------|------------|--------------------|\\n| No quality control  | 43.1       | 35.2       | 23.3               |\\n| + document-relevance detection | 41.2 | 31.0 | 20.5 |\\n| + document-relevance detection + self-reflection / cross-checking | 42.7 | 31.4 | 20.9 |\\n\\nTable 6: Step-wise performance comparison with and without LVLM-based quality checking\\n\\nThe results illustrate that the potential bias in step 1 (document-relevance detection) actually reduce, rather than increase, the performance gap between GPT4o and other models. It is because that we filter out all samples correctly answered by GPT4o without the access to documents. Under this case, the more significant performance drop of GPT-4V and Gemini-1.5-Pro can only be attributed to their limited document understanding and over-reliance on their internal knowledge. Regarding the step 2 and 3 (self-reflection and cross-checking), we provide inconsistent answers between human annotations and GPT4o's predictions to annotators and ask them to check and revise accordingly. The potential bias of this step does lead to a slight performance bias (1.1% absolute difference at maximum). We believe that such bias is NOT the main cause of GPT-4o's significantly best performance. Without the involvement of GPT-4o in the quality control process, GPT-4o still significantly outperforms GPT-4V by 7.9% (43.1% - 35.2%) and Gemini-1.5-Pro by 19.8% (43.1% - 23.3%). Accordingly, all primary conclusions in our paper still hold.\"}"}
{"id": "loJM1acwzf", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.7 GUI Screenshots\\n\\nWe present the screenshots for editing existing questions and annotating new questions (along with their reference answers and meta-data) in Figure 21 and Figure 22 respectively.\\n\\nFigure 21: GUI screenshot for editing existing questions (along with reference answers and meta-data)\\n\\nFigure 22: GUI screenshot for annotating new questions (along with reference answers and meta-data)\\n\\nA.8 Annotation Cost\\n\\nThis benchmark is annotated by the authors of this paper. Therefore, the data collection does not need compensation. And we count the time cost of our benchmark as below.\\n\\nPre-annotation (about 45h): the development of annotation interface (10h), the writing of annotation guideline (5h), training session (10h), preliminary annotation and personalized feedback (20h).\\n\\nAnnotation (about 150h): It takes about 60-90 minutes for the annotation of each document. And all of the 130 documents take about 150 hours.\\n\\nPost-annotation (about 45h): quality checking (30h), data processing and release preparation (15h).\\n\\nIn summary, our benchmark annotation approximately takes a total of 45+150+45=240 hours (1.36 man months).\"}"}
{"id": "loJM1acwzf", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B Experimental Details\\n\\nB.1 Prompt for Response Generation\\n\\nListing 1: Prompt used for response generation. The document is in PNG format (page screenshots) for LVLMs, and TXT format for LLMs.\\n\\n- Read the above documents and answer this question:\\n\\nB.2 Prompt for Answer Extraction\\n\\nListing 2: Prompt used for answer extraction. Given the question and analysis, you are tasked to extract answers with required formats from the free-form analysis.\\n\\n- Your extracted answers should be one of the following formats: (1) Integer, (2) Float, (3) String and (4) List. If you find the analysis the question can not be answered from the given documents, type \\\"Not answerable\\\". Exception: If the analysis only tells you that it can not read/understand the images or documents, type \\\"Fail to answer\\\".\\n\\n- Please make your response as concise as possible. Also note that your response should be formatted as below:\\n\\n  ```python\\n  Extracted answer: [answer]\\n  Answer format: [answer format]\\n  ```\\n\\n- Please read the following example, then extract the answer from the model response and type it at the end of the prompt.\\n\\n  Question: List the primary questions asked about the services in this report.\\n\\n  Analysis: The primary questions asked about the services in the report for The Limes Residential Home are:\\n  \\n  1. Is the service safe?\\n  2. Is the service effective?\\n  3. Is the service caring?\\n  4. Is the service responsive?\\n  5. Is the service well-led?\\n\\n  Extracted answer: ['Is the service safe?', 'Is the service effective', 'Is the service caring?', 'Is the service responsive?', 'Is the service well-led?']\\n\\n  Answer format: List\\n\\n  Question: How many regulations of the HSCA 2008 are breached in all according to this report?\\n\\n  Analysis: According to the report, the provider breached 10 Health and Social Care Act 2008 (Regulated Activities) Regulations in total. Here are the specifics:\\n\\n  1. Regulation 13: Safeguarding service users from abuse and improper treatment\\n  2. Regulation 12: Safe care and treatment\\n  3. Regulation 18: Staffing\\n  4. Regulation 11: Need for consent\\n  5. Regulation 10: Dignity and respect\\n  6. Regulation 9: Person-centred care\\n  7. Regulation 17: Good governance\\n  8. Regulation 18 (CQC Registration Regulations 2009): Notification of other incidents\\n  9. Regulation 18: Failure to maintain an accurate and up-to-date care plan\\n  10. Regulation 14: Adequate monitoring of the service\\n  11. Regulation 15: Inadequate resource and management of service\\n  12. Regulation 16: Inadequate arrangements for the complaint\\n  13. Regulation 19: Inadequate arrangements for the complaints\\n  14. Regulation 20: Inadequate arrangements for the complaints\\n  15. Regulation 21: Inadequate arrangements for the complaints\\n\\n  Extracted answer: [27]\\n\\n  Answer format: Integer\"}"}
{"id": "loJM1acwzf", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"10. Regulation 11: Failure to implement the Mental Capacity Act 2005 code of practice effectively\\n\\nThese breaches involve issues concerning staffing, safeguarding, medicines management, dignity and respect, consent, care planning, governance, and failure to notify the CQC of incidents.\\n\\nB.3 Rules for Score Calculation\\n\\nWe evaluate the model\u2019s responses by scoring the extracted answers against the reference answers. The scorer is rule-based and employs different strategies according to the format of the reference answer. We detail its rules as below:\\n\\nString:\\n\\nWe firstly use a series of regular expressions to determine whether the answers require exact matching (e.g., telephone numbers, email addresses, website addresses, file names, times, dates, etc.).\\n\\nIf an exact match is needed, we perform a straightforward string comparison and score the answer either 0 or 1. Otherwise, we follow previous work [17] and calculate the ANLS (Average Normalized Levenshtein Similarity) with a pre-defined threshold ($\\\\tau = 0.5$).\\n\\nInteger:\\n\\nWe perform an exact match comparison and score the answer either 0 or 1.\\n\\nFloat:\\n\\nWe view the prediction and reference answers as equal if they fall within a 1% relative tolerance.\\n\\nList:\\n\\nWe adopt a relatively strict rule for scoring answers in list format: predictions that do not have the same number of elements as the reference receive a score of 0. For the remaining predictions, as Eq. 1 indicates, we score each element in order and use the minimum element-wise score as the score for the entire list. The element-wise scoring strategies is determined by the formats of elements (i.e., string, integer or float).\\n\\n$$\\\\text{pred_list}, \\\\text{ref_list} = \\\\text{sorted(pred_list)}, \\\\text{sorted(ref_list)}$$\\n\\n$$\\\\text{Score}(\\\\text{pred_list}, \\\\text{ref_list}) = \\\\min(\\\\left[\\\\text{Score}(\\\\text{pred}, \\\\text{ref}) \\\\text{ for } \\\\text{pred}, \\\\text{ref} \\\\text{ in zip} \\\\left(\\\\text{pred_list}, \\\\text{ref_list}\\\\right)\\\\right])$$ (1)\"}"}
{"id": "loJM1acwzf", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluation detailed in the Appendix B.4 shows that while this scorer is not perfect, it aligns well with human judgment. We will continue refining these rules to cover more corner cases and enhance their accuracy.\\n\\nB.4 Human Evaluation on the Automatic Evaluation Pipeline\\n\\nWe conduct human evaluations to assess the performance of our automatic evaluation pipeline, which includes the answer extractor and the score calculator. Specifically, we randomly select 100 questions and review their responses from two representative LVLMs: GPT-4o and Gemini-1.5-Pro. We manually evaluate the correctness of each response and compare the results between human evaluation and automatic evaluation. The performance, as shown in Table 7, indicates a high correlation between human judgment and our automatic pipeline.\\n\\n| Model      | Inconsistent Evaluation | Ans. Extractor | Scorer | Overall |\\n|------------|-------------------------|----------------|--------|---------|\\n| GPT-4o     | 4                       | 2              | 6      |\\n| Gemini-1.5-Pro | 2                      | 2              | 4      |\\n\\nTable 7: We manually check 100 responses from GPT-4o and Gemini-1.5-Pro, and compare the evaluation results between humans and our automatic pipeline.\\n\\nB.5 Model Hyperparameters\\n\\nThe hyperparameters of used LVLMs and LLMs in Section 3.3 are detailed in Table 8. The temperature is set as 0.0, and the max_new_tokens is set as 1024 for all the models. The 'concatenated_images' parameter determines the maximum number of images that can be combined into a single input for LVLMs. By concatenating multiple images, we can meet the minimum context window requirements. The 'max_pages' parameter specifies the maximum number of images that can be directly input into the LVLMS without concatenation.\\n\\n| Model     | Hyperparameters                |\\n|-----------|--------------------------------|\\n| LLM       |                               |\\n| ChatGLM-128k | max_input_words=60000         |\\n| Mistral-Instruct-v0.2-7B | max_input_words=20000         |\\n| Mixtral-Instruct-v0.1-8x7B | max_input_words=20000         |\\n| Mixtral-Instruct-v0.1-8x22B | max_input_words=40000         |\\n| QWen-Plus | max_input_words=16000         |\\n| DeepSeek-V2 | max_input_words=20000         |\\n| LVLM      |                               |\\n| DeepSeek-VL-Chat | concatenated_images=5          |\\n| Qwen-VL-Chat | concatenated_images=5          |\\n| Idefics2  | concatenated_images=5         |\\n| MiniCPM-Llama3-V2.5 | concatenated_images=2          |\\n| InternLM-XC2-4KHD | concatenated_images=2          |\\n| Monkey-Chat | concatenated_images=1          |\\n| CogVLM2-Llama3-Chat | concatenated_images=1          |\\n| InternVL-Chat-v1.5 | concatenated_images=5          |\\n| EMU2-Chat  | concatenated_images=5         |\\n| LLM & LVLM |                               |\\n| Claude-3 Opus | version=claude-3-opus-20240229, concatenated_images=20 |\\n| Gemini-1.5-Pro | max_pages=120, version=gemini-1.5-pro-latest |\\n| GPT-4-turbo | max_pages=120, version=gpt-4-turbo-2024-04-09 |\\n| GPT-4o     | max_pages=120, version=gpt-4o-2024-05-13 |\"}"}
{"id": "loJM1acwzf", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.1 Error Analysis\\n\\nWe delve into the analysis of error by GPT-4o to further understand its bottlenecks and potentials on long-context document understanding. We manually check 72 incorrect responses and categorized their error reasons into 7 types. Except for the Extraction Error caused by our automatic evaluation pipeline (see Appendix B.4), we detail and showcase another six reasons as below:\\n\\n**Perceptual Error:** GPT-4o sometimes struggles to extract or understand visual information from document screenshots. For instance, it misinterprets the axes and colored circles in the charts shown in Figure 23. Additionally, it inaccurately counts the number of green bars in Figure 24. They demonstrate that even the cutting-edge LVLMs still fall short in fundamental perceptual capabilities.\\n\\n**Incomplete Evidence:** Though GPT-4o has achieved significantly better global searching abilities compared to other models when dealing with lengthy, multi-modal documents, it sometimes still omits certain information. For example, GPT-4o misses one chapter author from Columbia University in the full list (Figure 25). Additionally, it overlooks an app that appears across two pages (Figure 26).\\n\\n**Hallucinated Evidence:** As stated in Section 3.4, GPT-4o adopts more aggressive strategies and tends to provide more false-positive answers. It sometimes even fabricates non-existent evidence in documents to support its incorrect responses. For example, it references a non-existent page in Figure 27, and fabricates the content of a page in Figure 28. The above examples clearly reveal the importance of further research on LVLMs' hallucination and safety.\\n\\n**Knowledge Lacking:** Resolving certain questions requires both information from the documents and the parametric knowledge within LVLMs. We have observed error cases stemming from the absence of specific knowledge. For example, GPT-4o overlooks details about the fixed asset turnover ratio and uses the single-point value instead of the average value to calculate this metric (Figure 29). Additionally, it misidentifies buildings at Tsinghua University in Figure 30.\\n\\n**Reasoning Error:** Though not a primary cause, flawed reasoning based on correctly collected evidence and information from documents can sometimes lead to wrong answers. For example, GPT-4o correctly gathers all data but calculates a relative percentage instead of an absolute percentage in Figure 31. Additionally, as shown in Figure 32, it correctly lists all quizzes but inaccurately counts them in the final step.\\n\\n**Irrelevant Answer:** GPT-4o sometimes misunderstands the intent of questions and provides irrelevant responses. For instance, in Figure 33, GPT-4o answers about button operations when the question asks about button functions. Similarly, in Figure 34, where the question asks for the MOST discrimination type, GPT-4o summarizes all types instead.\"}"}
{"id": "loJM1acwzf", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: Among people who age 30-49, what is the difference of percentage value between Americans and Germans having a positive view of their bilateral relationship?\\n\\nResponse: The difference in the percentage of people aged 30-49 who have a positive view of the U.S.-Germany relationship between Americans and Germans is 9% points. According to data on page 5, 40% of Americans aged 30-49 view the relationship positively, while only 31% of Germans in the same age group share this view.\\n\\nError Type: Perceptual Error\\n\\nAnalysis: The model wrongly identifies the percentages of people aged 30-49 who have a positive view of the U.S.-Germany relationship from the chart. It should be 35% and 72%, rather than 31% and 40%.\"}"}
{"id": "loJM1acwzf", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Perceptual Error: Case 2\\n\\nQuestion:\\n\\nHow many green bars appear in Figure 1?\\n\\nResponse:\\n\\nIn Figure 1, there are four green bars.\\n\\nError Type: Perceptual Error\\n\\nAnalysis:\\nThe model wrongly counts 4 green bars in Figure 1. Obviously, there are 6 green bars.\"}"}
