{"id": "9tVn4f8aJO", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. **Architecture:**\\n\\nFor training multi-modal models, parameters can either be initialized using a pre-trained model and then are fine-tuned/kept frozen, or are initialized randomly and trained in an end-to-end fashion. Based on this choice, we categorize models into two classes - fine-tuned and trained from scratch.\\n\\n4. **Training Data Size:**\\n\\nThe amount of data used for training the models, plays an important role in the performance and generalization of the model. Based on the size of the training data (in our work, the number of image-text or image-image samples), we categorize the models into three categories - Small, Medium, and Large.\\n\\n5. **Number of Parameters:**\\n\\nModel size is an important modeling decision as it affects the performance of the model, cost and efficiency of training, and the inference time. Hence, we also categorize the models based on both the total and trainable number of parameters, and compare the performance across these categories.\\n\\n6. **Diversity in Training Data:**\\n\\nTraining multimodal models on data from different tasks, improves the diversity of the training data and may help the models to perform well on multiple tasks. By categorizing the models based on the diversity of the training data used, we evaluate the effect of using data from diverse tasks.\\n\\n**A.4 Model Details**\\n\\nFor the HEMM benchmark, we currently evaluate the following models. All the models except for Gemini and GPT-4V are open source and we encourage the community to add more models to the benchmark.\\n\\n1. **BLIP-2**\\n\\nBLIP-2 uses pre-trained image encoder and a pre-trained LLM for decoding. A Q-former is used to fuse the input text and the image queries using attention mechanism, and the fused representation is used by the decoder to generate the response. While training, only the parameters of the Q-former are updated using supervised fine-tuning, and the rest of the architecture is kept frozen. In this work we use the blip2_t5 model with pretrain_flant5xxl as the decoder from LA VIS*. The chosen model has 108M and 12.1B trainable and total parameters respectively.\\n\\n**License:**\\n\\nThe model comes with BSD-3 Clause https://github.com/salesforce/LA VIS/blob/main/LICENSE.txt\\n\\n**Access restrictions:**\\n\\nThe model is available to use from the LA VIS repository https://github.com/salesforce/LA VIS\\n\\n2. **INSTRUCT-BLIP**\\n\\nINSTRUCT-BLIP is built on top of the BLIP2 architecture, where the model is first pre-trained similar to BLIP2. In the second phase, the Q-former in the architecture is instruction tuned (rest parameters frozen) to create an instruction following Q-former. For evaluation, we use the blip2_t5_instruct model with flant5xl as the decoder from LA VIS*. The model has 188M trainable parameters and 4B parameters in total. The pre-training data for the first phase is similar to BLIP2 and additional 15M samples from diverse datasets and tasks (e.g., VQA, Reasoning, Captioning, etc.) are used for instruction tuning.\\n\\n**License:**\\n\\nThe model comes with BSD-3 Clause https://github.com/salesforce/LA VIS/blob/main/LICENSE.txt\\n\\n**Access restrictions:**\\n\\nThe model is available to use from the LA VIS repository https://github.com/salesforce/LA VIS\\n\\n3. **MINI-GPT-4**\\n\\nMINI-GPT-4 also has a similar architecture as BLIP2, and uses the same Vision encoder and Q-former. However, the decoding LLM is based on Vicuna. Further, MiniGPT-4 has an additional single projection layer applied to the output of the Q-former. The architecture is instruction tuned with all the parameters except for the projection layer are kept frozen. We evaluate the prerained_minigpt4_7b model from the MiniGPT-4 GitHub repository*. The model has 13B parameters and is fine-tuned using 5M image-text samples.\\n\\n**License:**\\n\\nThe model comes with BSD-3 Clause https://github.com/Vision-CAIR/MiniGPT-4/blob/main/LICENSE.md\\n\\n**Access restrictions:**\\n\\nThe model is available to use from the MiniGPT-4 GitHub repository https://github.com/Vision-CAIR/MiniGPT-4?tab=readme-ov-file\\n\\n*https://github.com/salesforce/LA VIS/tree/main/projects/blip2\\n\\n*https://github.com/salesforce/LA VIS/tree/main/projects/instructblip\\n\\n*https://github.com/Vision-CAIR/MiniGPT-4?tab=readme-ov-file\"}"}
{"id": "9tVn4f8aJO", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. **OPENFLAMINGO** is an open-source reproduction of the Flamingo [3] models. Unlike models that can only take one input image per sample (e.g., BLIP2, MiniGPT-4), OpenFlamingo can handle multiple images by interleaving images and texts. The architecture comprises of pre-trained Vision and Language encoder/decoder, where the layers of the pre-trained LLM are augmented with the vision encoder outputs which allows for cross-modal attention. All the pre-trained components are kept frozen except for the cross-modal attention component. For evaluation, we use the OpenFlamingo-3B-vitl-mpt1b model from the OpenFlamingo Github Repository*. The chosen models have 1.4B trainable parameters and a total of 3.2B parameters. It is trained using 180M image-text samples. \\n\\nLicense: Work is available under MIT License https://github.com/mlfoundations/open_flamingo/blob/main/LICENSE\\n\\nAccess restrictions: The model is available to use from https://github.com/mlfoundations/open_flamingo\\n\\n5. **LLAMA-ADAPTER** is based on the architecture of LLaMA Adapter [125] which augments the text tokens with learnable adaptation prompts. In addition to this, LLaMA Adapter V2 uses early fusion to add visual knowledge to the decoding LLM. The architecture uses both early fusion and late fusion, and while fine-tuning, all the pre-trained components are frozen except for the bias layers of the LLM, Visual Projection Layer and the zero-initialized cross attention module. We evaluate the BIAS-LORA-7B model which uses LLaMA-7B as the decoder*. The model is instruction tuned using 619K samples, and has 14M trainable parameters. \\n\\nLicense: Work is available under GNU General public license https://github.com/OpenGVLab/LLaMA-Adapter/blob/main/LICENSE\\n\\nAccess restrictions: Model is available to use from https://github.com/OpenGVLab/LLaMA-Adapter\\n\\n6. **EMU** is a large multimodal model trained using interleaved video, image and text data, trained in an autoregressive manner to predict the next token in the multimodal sequence. With the ability to produce the next visual token, Emu is also able to generate images and has been evaluated on the Magic Brush dataset in this work. The architecture uses pre-trained encoder and a decoding LLM such as LLaMA. EMU is first pre-trained using interleaved video, image, and text data, and all the parameters are updated during the pre-training. In the second stage, emu is further instruction-tuned. However, in this work we only evaluate the pre-trained version of Emu. We evaluate the Emu-14B model pre-trained using 82M samples. \\n\\nLicense: Work is available under Apache 2.0 license https://github.com/baaivision/Emu/blob/main/LICENSE\\n\\nAccess restrictions: The model is available to use from https://github.com/baaivision/Emu\\n\\n7. **FUYU-8B** is a decoder only architecture where the image patches are linearly projected into the first layer of the transformer architecture. Fuyu's architecture is same as that of Persimmon-8B*, and we use the details of Persimmon-8B to categorise Fuyu into the model categories. Persimmon-8B has 9.3B parameters and is trained from scratch. In our work we evaluate the pre-trained model as the instruction tuned models aren't available and the pre-training data sources and sizes are unknown. We evaluate the Fuyu-8B model available through HuggingFace*. \\n\\nLicense: Work is available under Creative Commons Attribution Non Commercial 4.0 International license https://spdx.org/licenses/CC-BY-NC-4.0\\n\\nAccess restrictions: Model is available to use from huggingface* https://github.com/mlfoundations/open_flamingo https://github.com/OpenGVLab/LLaMA-Adapter/tree/main/llama_adapter_v2_multimodal7b https://www.adept.ai/blog/persimmon-8b https://huggingface.co/adept/fuyu-8b\"}"}
{"id": "9tVn4f8aJO", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. **KOSMOS-2** is based on a causal Transformer Language Model, and has the architecture similar to Kosmos1 [40]. It is trained on the next-token prediction task. In addition to the pre-training data used to train Kosmos1, grounded image-text pairs are added to the dataset to train Kosmos2. Overall, Kosmos2 is trained using interleaved image-text data and later instruction-tuned using both multimodal and language-only instructions. We evaluate the `ydshieh/kosmos-2-patch14-224` model from HuggingFace which has a total of 1.6B parameters. \\n\\nLicense: Work is available under MIT License\\n\\nAccess restrictions: The model is available to use from https://huggingface.co/microsoft/kosmos-2-patch14-224\\n\\n9. **mPLUG-O** uses a vision foundation model to encode input image and uses a visual abstractor model to summarize the input from the encoder. The abstractor output along with the text queries are then passed to a pre-trained language foundation model that generates the response. The model is first pre-trained using supervised fine-tuning of all the parameters except for the language models. In the second phase, the language models is instruction tuned using multimodal and language instructions, with the other parameters frozen. We evaluate the `https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl` model obtained from the mPLUG-Owl Github Repository*. The chosen models has a total of 7.2B parameters. \\n\\nLicense: Work is available under MIT License https://github.com/X-PLUG/mPLUG-Owl/blob/main/LICENSE\\n\\nAccess restrictions: The model is available to use from https://github.com/X-PLUG/mPLUG-Owl\\n\\n10. **GPT-4V** is a multimodal extension to GPT-4 which has been trained on the next word prediction task using image and text data from the internet and licensed data sources and fine-tuned using RLHF[84],[20]. We use 'gpt-4-vision-preview' as a chosen model for our evaluation. As of evaluating the models, 'gpt-4-vision-preview' points to 'gpt-4-1106-vision-preview' in the OpenAI API interface which has been trained up to April 2023. \\n\\nLicense: None\\n\\nAccess restrictions: The model is available via OpenAI's API https://platform.openai.com/docs/guides/vision\\n\\n11. **GEMINI** is a series of multimodal large language models which support interleaved inputs. These models have been trained on multimodal and multilingual data comprising of data from web documents, books, and code, and includes image, audio, and video data. For our evaluation, we use 'gemini-pro-vision' which points to 'gemini-1.0-pro-vision-001' released on February 15, 2024. We also use safety settings such as 'HARM_CATEGORY_DANGEROUS', 'HARM_CATEGORY_HARASSMENT', 'HARM_CATEGORY_HATE_SPEECH', 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'HARM_CATEGORY_DANGEROUS_CONTENT' and set the threshold to 'BLOCK_NONE' provided by the API.\\n\\nLicense: None\\n\\nAccess: Available via API https://ai.google.dev/gemini-api/docs/models/gemini\"}"}
{"id": "9tVn4f8aJO", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11: Evaluation metrics supported in HEMM\\n\\n| Metric       | Task Modalities |\\n|--------------|-----------------|\\n| BLEU         | Text Generation |\\n| ROUGE        | Text Generation |\\n| BertScore    | Text Generation |\\n| BARTScore    | Text Generation |\\n| RefCLIPScore | Image, Text     |\\n| CLIP-I       | Image Generation |\\n| MSE          | Image Generation |\\n\\nB.1 Evaluation metrics\\n\\nWe present our results on BARTScore [122] as models under our evaluation generate noisy free from text, however, we also support other text generation metrics under our evaluation suite listed in the Table 11 below.\\n\\nB.2 Evaluation protocol\\n\\nHEMM supports image generation tasks, models and metrics. However, currently there are only 2 image generation tasks (LNCOCO and MAGIC) and 1 model (EMMA) that supports image generation. Hence, we perform all our evaluation on the remaining 28 text generation tasks and report the results on the image generation tasks in Appendix C.\\n\\nNote: Since HEMM contains models that are unable to process multiple images in the same input, we modify the WINGROUND and IRFL tasks (as per A.1) in order to have a single image-text pair as input for each sample.\\n\\nFor each dataset, we use the same prompts across all models as shown in Section C, for standardization, however, there can be a scenario where these models perform better with other prompts or scenarios and may perform poorly under our scenarios or prompts in our evaluation.\\n\\nFor each dataset, the computed metrics for the models are normalized on a scale of 0 to 1, 0 corresponds to the model achieving the lowest score on that dataset, and 1 corresponds to the performance achieve by exactly generating the ground truth. For BERTScore [126], ROUGE [70], and RefCLIPScore [36] the maximum value is set to 1. BARTScore [122] uses the log of probabilities. Following [16], we calculate the maximum value for each dataset separately as BARTScore(r, r) where r is the ground truth sentence.\\n\\nSince details regarding training type for EMMA and GPT-4V, and modality processing for GPT-4V are not revealed, we do not use the scores from these models while evaluating the performance for the training type and modality processing dimensions. Further, for HATEFUL, OPATH, and EMOTION datasets, GPT-4V did not respond and generated can\u2019t provide assistance and \u201cindeterminate\u201d for many samples. Hence, we exclude the results of GPT-4V on these datasets during evaluation.\\n\\nB.3 Significance tests\\n\\nWhile comparing performance across categories in each dimension, we perform paired t-tests to determine the significance of the results. For datasets, specifically, for each category, we calculate the average performance of each of the 11 models on all the datasets in a category (ci) to create a vector vi \u2208 R11. Next, we performed pairwise t-tests between these vectors to determine the significance of the results. The p-values obtained through the t-tests are presented in Table 13. We find that the difference between the performance on different categories is statistically significant (p-value < 0.05) for real-world use cases, multimodal interaction, external knowledge, and information flow dimensions, which explains that these are particularly difficult dimensions for today\u2019s multimodal models.\"}"}
{"id": "9tVn4f8aJO", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 12: Hyperparameters used for running inference for various models. Temperature for GPT-4V and Beam Size for GEMINI are unknown. We also report the average inference time in seconds for an image-text input. For each models, we take the average of inference times across all the datasets.\\n\\n| Model         | Temperature | Beam Size | Max New Tokens | Inference Time |\\n|---------------|-------------|-----------|----------------|----------------|\\n| BLIP-2        | 1.0         | 5         | 30             | 0.64           |\\n| INSTRUCT-BLIP | 1.0         | 5         | 256            | 0.58           |\\n| MINI-GPT-4    | 1.0         | 3         | 100            | 11.8           |\\n| FUYU-8B       | 1.0         | 1         | 100            | 1.92           |\\n| EMU0.9        | 0.9         | 5         | 100            | 1.43           |\\n| OPENFLAMINGO  | 1.0         | 3         | 50             | 2.35           |\\n| KOSMOS-2      | 1.0         | 1         | 500            | 0.31           |\\n| PLUG-OWL      | 1.0         | 1         | 100            | 0.87           |\\n| LLAMA-ADAPTER | 0.0         | 1         | 100            | 1.30           |\\n| GPT-4V        | -           | -         | 300            | 2.67           |\\n| GEMINI        | 0.4         | -         | 2048           | 4.62           |\\n\\nIn Section B.2, we do not use the scores of GPT-4V and GEMINI for the dimensions where their training/modeling decisions aren't revealed. We find that for all the dimensions, the best-performing category achieves significantly better scores with p-values < 0.05 (Table 14).\\n\\nB.4 Model hyperparameters and inference time\\n\\nIn Table 12, we list the values of important text-generation hyperparameters used to evaluate different models. For each model, we also report the inference time for a single image-text pair averaged across all the datasets.\\n\\nB.5 Human evaluation\\n\\nWe perform human preference-based pair-wise comparison (battles) of model responses across 1000 datapoints and use the following metrics to rank the models.\\n\\n**Average win rate:**\\nSimilar to Chiang et al. [19], for each pair of models, considering only the battles between them, we determine the win rate $w_{ab} = N_a + N_b$, where $N_a$ and $N_b$ are the number of battles won by model $a$ and model $b$ respectively. We then take the average of the win rates across all the models to calculate the average win rate for each model i.e., $\\\\bar{w}_a = \\\\frac{1}{M}\\\\sum_{b=1}^{M} w_{ab}$.\\n\\nThe top 4 models based on the average win rate are GEMINI (0.73), GPT-4V (0.68), INSTRUCT-BLIP (0.60) and BLIP-2 (0.52).\\n\\n**Elo Rating:**\\nUsing the initial rating of each model as 1000, we sequentially process the battles and update the rating of the models as per the below equations.\\n\\n$R_a$ and $R_b$ denote the current ratings of model $a$ and model $b$ in the battle. $S_a = 1$ if model $a$ wins the battle and 0 if it loses. $S_b = 1 - S_a$ and in case of ties, $S_a = S_b = 0$.\\n\\nFor more stable Elo ratings, we use $K = 4$.\\n\\n$E_a = \\\\frac{1}{1 + 10^{(R_b - R_a)/400}}$; $E_b = \\\\frac{1}{1 + 10^{(R_a - R_b)/400}}$.\\n\\n$\\\\hat{R}_a = R_a + K \\\\times (S_a - E_a)$; $\\\\hat{R}_b = R_b + K \\\\times (S_b - E_b)$.\\n\\nThe above update rule is sensitive to battle orders. In order to get more stable and less biased Elo ratings, we run the above computation 1000 times by shuffling the battle order each time, and report the median Elo rating over the 1000 runs for each model.\\n\\nThe 1000 battles were split across 5 authors randomly (200 battles each) for annotation. Using a web interface, the model outputs were presented to the annotators. For each sample, the annotators were instructed to select the output that better answers the query. For cases where both outputs were equally good/bad, or performing the task required domain knowledge (e.g., healthcare datasets), the annotators were instructed to choose the Tie option. For each battle, the models were anonymized for fair comparison.\"}"}
{"id": "9tVn4f8aJO", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 13: Standard deviation and p-values (from paired t-tests) across categories for each dataset dimension. On average, models achieve significantly higher scores on Multimedia and Affect as compared to other use cases. The p-values for Reasoning and Granularity dimensions are higher than 0.05, indicating that there is no category significantly more challenging than the rest.\\n\\n| Dimension       | Category           | Perf (\u2191) | P-value          |\\n|-----------------|--------------------|----------|------------------|\\n| Real-world use case | Multimedia        | 31.30\u00b10.14 | vs Affect: 0.1100 |\\n|                 |                    |          | vs Health: 0.0006 |\\n|                 |                    |          | vs Science: 0.0000 |\\n|                 |                    |          | vs HCI: 0.0002    |\\n| Affect          |                    | 30.35\u00b10.15 | vs Health: 0.0044 |\\n|                 |                    |          | vs Science: 0.0018 |\\n|                 |                    |          | vs HCI: 0.0011    |\\n| Health          |                    | 20.24\u00b10.09 | vs Science: 0.8806 |\\n|                 |                    |          | vs HCI: 0.0961    |\\n| Science         |                    | 19.83\u00b10.13 | vs HCI: 0.2093    |\\n| HCI             |                    | 15.70\u00b10.08 |                  |\\n| Multimodal interaction | Redundancy | 29.04\u00b10.14 | vs Uniqueness: 0.0008 |\\n|                 |                    |          | vs Synergy: 0.0522 |\\n| Uniqueness      |                    | 19.60\u00b10.10 | vs Synergy: 0.0000 |\\n| Synergy         |                    | 33.73\u00b10.15 |                  |\\n| Reasoning       | More Reasoning     | 27.50\u00b10.11 | vs Less Reasoning: 0.6415 |\\n|                 | Less Reasoning     | 26.84\u00b10.13 |                  |\\n| Granularity     | Fine-grained       | 26.52\u00b10.12 | vs Coarse-grained: 0.5887 |\\n|                 | Coarse-grained     | 27.52\u00b10.13 |                  |\\n| Knowledge       | External Knowledge | 23.51\u00b10.10 | vs None: 0.0023   |\\n|                 | None               | 29.62\u00b10.14 |                  |\\n| Information flow | Querying          | 25.88\u00b10.13 | vs Translation: 0.0479 |\\n|                 | Translation        | 18.97\u00b10.07 | vs Fusion: 0.0004 |\\n|                 | Fusion             | 33.77\u00b10.15 |                  |\\n\\nC All Results\\n\\nDue to query limits for GPT-4V and GEMINI, we evaluated the two models only on 100 samples per dataset, and for a fair comparison, we performed our analysis using the outputs of all the models on those 100 samples. In this section, we present the results and analysis on the whole evaluation set using the outputs of all the models except GPT-4V and GEMINI. Further, since our analysis was based on text-generation tasks, we present here the results on the image-generation tasks - MAGICBRUSH and LNCOCO. Specifically, we evaluated EMU (only model in HEMM that can generate images) on both tasks. We find the MSE and the CLIP-I score between the generated and the ground truth image for MAGICBRUSH to be 0.17 and 0.54. For the LNCOCO dataset, the MSE and CLIP-I score are 0.18 and 0.50.\\n\\nNote: due to high inference time of some models (e.g., MAGIC, EMU, OPENTALAMINGO), missing image URLs in the NLVR2 dataset, and compute restrictions for larger evaluation sets like MM-IMDB, VISUALGENOME, and INATURALIST, we use the results from the same 100 samples used for evaluation in Section 4.\\n\\nC.1 Dataset and model comparisons\\n\\nDataset comparisons:\\n\\nOn average, the models achieve the highest scores on IRFL (0.53), WINGROUND (0.42), and NLVR (0.40) datasets. Healthcare, Science, and HCI datasets are the most challenging use cases for the models with the average scores being the lowest for DECMER (0.05),\"}"}
{"id": "9tVn4f8aJO", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 14: Standard deviation and p-values for categories in various modeling dimensions. Models in the best-performing category in each dimension, receive significantly higher scores than the other categories.\\n\\n| Dimension            | Category | Perf (\u2191) | P-value            |\\n|----------------------|----------|----------|--------------------|\\n| Modality Processing  | Interleaved | 22.94 \u00b1 0.10 | vs Separate: 0.0011 |\\n|                      | Separate | 28.58 \u00b1 0.15 |                |\\n| Model Size           | Small    | 23.34 \u00b1 0.14 | vs Medium: 0.7370 vs Large: 0.0004 |\\n|                      | Medium   | 23.87 \u00b1 0.12 | vs Large: 0.0004 |\\n|                      | Large    | 42.33 \u00b1 0.07 |                |\\n| Training Type        | Modular  | 24.92 \u00b1 0.12 | vs End-to-End: 0.0427 |\\n|                      | End-to-End | 21.26 \u00b1 0.13 |                |\\n| Size of Training Data| Small    | 16.80 \u00b1 0.10 | vs Medium: 0.0000 vs Large: 0.0000 |\\n|                      | Medium   | 30.10 \u00b1 0.15 | vs Large: 0.5024 |\\n|                      | Large    | 31.77 \u00b1 0.16 |                |\\n| Diversity of Training Data | Non-diverse | 21.71 \u00b1 0.12 | vs Diverse: 0.0000 |\\n|                      | Diverse  | 30.15 \u00b1 0.14 |                |\\n| Instruction Tuning   | No       | 22.49 \u00b1 0.11 | vs Yes: 0.0004 |\\n|                      | Yes      | 29.71 \u00b1 0.15 |                |\\n\\nPPATH VQA (0.06), INATURALIST (0.06), and ENEIRC (0.08). Meme datasets are also challenging for the models. A low average score (0.12) on MEMECAP shows that the models struggle to understand the visual metaphors and generate suitable captions for the memes.\\n\\nModel comparisons: Overall, INSTRUCT-BLIP and BLIP-2 achieve the highest average scores of 0.38 and 0.37, followed by FUYU-8B (0.29). PENG Lamino and ENU rank lowest on many datasets (receiving a 0 score as per our normalization) and achieve the lowest average scores of 0.05 and 0.11.\\n\\nC.2 Dataset trends\\nIn Table 15, we summarize the average performance of models on various categories in each data dimension. We now closely compare the performance between different categories of individual dimensions.\\n\\nMultimodal Skills 1: Interactions\\nThe average scores on datasets having redundant, unique and synergistic interactions are 0.25, 0.14, and 0.28. The p-values obtained using paired t-test for Redundancy vs Uniqueness, Uniqueness vs Synergy, and Redundancy vs Synergy are 0.01, 0.0008, and 0.22, indicating that average scores on datasets with unique interactions is significantly lower compared to datasets with Redundant and Synergistic interactions. Reasons for lower uniqueness scores can be attributed to the presence of highly challenging datasets such as DECMER, INATURALIST, ENEIRC.\\n\\nMultimodal Skills 2: Granularity\\nThe average scores of the models on datasets with fine-grained (0.23) and coarse-grained alignment (0.22) are not significantly different, indicating that both categories are challenging for the models, with the former containing tasks like GQA, WINGROUND and NLVR and the latter having tasks such as Flickr30K, HatefulMemes, and SciencoQA.\\n\\nMultimodal Skills 3: Reasoning\\nThe average scores achieved by models on tasks requiring less or more reasoning are 0.22 and 0.23 respectively, and we find that the difference is not statistically significant. This indicates that both categories are challenging for the models with the less reasoning category comprising of datasets like ENEIRC and INATURALIST posing challenges related to visual perception and external knowledge. On the other hand, tasks within the more reasoning category such as VCR and MECAP test for compositional and commonsense reasoning.\"}"}
{"id": "9tVn4f8aJO", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 15: Comparisons on different dataset categories.\\n\\n| Category            | Group | Perf (\u2191)   |\\n|---------------------|-------|------------|\\n| Real-world use case | Multimedia | 29.27 \u00b1 0.14 |\\n|                     | Affect | 22.63 \u00b1 0.11 |\\n|                     | Health | 15.51 \u00b1 0.08 |\\n|                     | Science | 14.23 \u00b1 0.08 |\\n|                     | HCI     | 12.49 \u00b1 0.07 |\\n| Multimodal interaction | Redundancy | 24.86 \u00b1 0.13 |\\n|                     | Uniqueness | 13.87 \u00b1 0.06 |\\n|                     | Synergy   | 28.48 \u00b1 0.13 |\\n| Reasoning           | More     | 23.19 \u00b1 0.11 |\\n|                     | Less      | 21.78 \u00b1 0.09 |\\n| Granularity         | Fine-grained | 22.97 \u00b1 0.11 |\\n|                     | Coarse-grained | 21.68 \u00b1 0.10 |\\n| Knowledge           | External | 19.60 \u00b1 0.09 |\\n|                     | None      | 24.21 \u00b1 0.11 |\\n| Information flow    | Querying | 20.15 \u00b1 0.10 |\\n|                     | Translation | 16.72 \u00b1 0.07 |\\n|                     | Fusion    | 29.16 \u00b1 0.14 |\\n\\nTable 16: Comparisons on different modeling decisions. We group models based on the modeling and training decisions, including how they process modalities, their parameter counts, model architecture, training data size and diversity, and the presence of instruction tuning. Performance is measured via the mean BARTscore across all 30 tested multimodal datasets.\\n\\n| Category            | Group | Perf (\u2191)   |\\n|---------------------|-------|------------|\\n| Modality processing | Interleaved | 16.92 \u00b1 0.09 |\\n|                     | Separate | 26.48 \u00b1 0.15 |\\n| Model size          | Small | 21.51 \u00b1 0.13 |\\n|                     | Medium | 22.59 \u00b1 0.12 |\\n| Training decisions  | Modular | 23.18 \u00b1 0.13 |\\n|                     | End-to-end | 20.93 \u00b1 0.13 |\\n| Size of training data | Small | 16.08 \u00b1 0.11 |\\n|                     | Medium | 27.60 \u00b1 0.15 |\\n|                     | Large   | 20.72 \u00b1 0.15 |\\n| Diversity of training data | Non-diverse | 19.92 \u00b1 0.12 |\\n|                     | Diverse | 24.09 \u00b1 0.13 |\\n| Instruction tuning  | No      | 21.00 \u00b1 0.12 |\\n|                     | Yes     | 23.22 \u00b1 0.14 |\\n\\nMultimodal Skills 4: External Knowledge\\n\\nAverage performance of models on tasks requiring external knowledge (0.20) is significantly lower than tasks not requiring knowledge (0.24). For example, on average, models perform better on NLV, FER-2013 and WINGROUND that do not require external knowledge as compared to tasks like INATURALIST and SLAKE which require external knowledge to identify appropriate species or organs in the image.\\n\\nMultimodal Skills 5: Information flow\\n\\nModels achieve significantly lower average score on translation datasets (0.17) as compared to querying (0.20) and fusion (0.29) datasets. Lower scores on translation dataset is due to the presence of highly challenging datasets such as DECIMER which requires domain knowledge of molecules to generate the correct textual sequence.\\n\\nC.3 Modeling trends\\n\\nModel scale: Since we do not consider GPT-4V and GEMINI for analysis in this section, there are no models in the large category. Amongst small and medium models, we find no significant difference (p-value = 0.45) between the average performance of models from the two categories with small and medium models receiving 0.21 and 0.23 average scores respectively.\\n\\nPretraining data scale: On average, models with medium pretraining data achieve the highest score (0.28) as compared to the models pretrained with small (0.16) or large (0.21) scale data. Although the average score of models trained with large pretraining data is lower as compared to models trained with medium pretraining data, we find that the former models perform better on tasks such as IRFL, WINGROUND, MECAP, and DECIMER which require complex reasoning and external knowledge.\\n\\nDiversity of pretraining data: Models trained with diverse pretraining data (0.24) perform better than models trained only on image-captioning datasets (0.20). The p-value for the paired t-test is 0.01 indicating that the difference is significant. On average, we find that models pretrained with diverse data achieve better scores on knowledge-intensive tasks such as INATURALIST and OK-VQA with improvements in average scores up to 0.21.\"}"}
{"id": "9tVn4f8aJO", "page_num": 41, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: Model outputs on samples from ENRICO, VQARAD, NATURALIST, and SCIENCEQA. In (a), all the models struggle to reason about the use of the zip code field in the UI, which will be used to search the TV provider. Example (b) underscores the complexity faced by models in interpreting medical images, particularly evident in their inability to recognize the absence of a kidney in the radiology image. As shown in (c), the highly fine-grained NATURALIST dataset is very challenging and none of the models can determine the species of the insect. In (d), all models provide incorrect responses when tasked with identifying the colony's name, illustrating the challenges posed by tasks requiring external knowledge.\\n\\nInstruction tuning vs supervised fine-tuning: Instruction-tuned models achieve a higher average score (0.23) as compared to models with only supervised fine-tuning (0.21). We observe the highest improvements in translation tasks such as DECMER, FLICKR30K, and SCREEN2WORDS. We also observe that instruction-tuned models receive a higher average score as compared to supervised fine-tuned models (improvement of 0.12).\\n\\nModality processing: Models that process the modalities separately perform significantly better than the models that operate on interleaved inputs. The average scores for the former and latter models are 0.17 and 0.26 respectively (p-value $\\\\approx 0$). We find high improvements of 0.26, 0.24, 0.22, and 0.2 in the average scores for the datasets SCIENCEQA, NY CARTOON, MM-IMDB, and UCMERCED LAND USE.\\n\\nTraining type: We do not find a significant difference between the models that are fine-tuned in a single phase end-to-end manner (0.21) as compared to the models where only specific modules are fine-tuned in a single phase (0.23).\\n\\nC.4 Summary of takeaway messages\\nFinally, we summarize the main findings regarding the performance and evaluation of multimodal foundation models that can be important directions for future work:\\n\\n1. Challenging datasets: Health, HCI, and Science are all relatively difficult use cases for today\u2019s multimodal foundation models, which are statistically significantly harder than Multimedia and Affective Computing use cases. In particular, images of scientific diagrams, satellite images, medical images, memes, and rich social interactions pose challenges. It is therefore important to evaluate multimodal models on a diverse range of input modalities and output tasks to get a better measure of generalization performance.\\n\\n2. Multimodal interactions: Models perform better on redundant interactions but struggle when visual information is not directly referenced by text (i.e., uniqueness or synergy). Future benchmarks should contain richer multimodal interactions beyond redundancy, such as in analyzing sarcasm, humor, memes, science, environment, and education. These can serve as better test beds for multimodal models and enable their applications towards real-world multimodal interactions.\\n\\n3. Reasoning, fine-grained, and knowledge: We need better datasets that test for complex reasoning and fine-grained alignment - current ones do not pose enough challenges to today\u2019s models, with no significant performance differences with or without reasoning and fine-grained alignment.\"}"}
{"id": "9tVn4f8aJO", "page_num": 42, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We do find that tasks requiring external knowledge are significantly harder than no knowledge; bridging this gap can be a promising direction for multimodal research.\\n\\n4. Model and data size: Perhaps unsurprisingly, larger scales of data and models improve the average score across the board, with significant improvements of up to 75% as compared to medium-sized models. Training on diverse data sources also improves over models that only pretrain on images and captions. The tasks that show the most improvement are INATURALIST and MEMPAP which are knowledge-intensive and require complex reasoning.\\n\\n5. Model training: Instruction-tuned models performed better than those with only supervised fine-tuning. Cross-modal translation (image-to-text) tasks show the most improvements (e.g., DECMER, MEMPAP, and SCREEN2WORDS). However, some instruction-tuned models still struggle to follow the instructions (e.g., generating a caption when asked to classify an image, or generating long responses when asked to answer in a few words). Instruction tuning using larger datasets with diverse instructions can help alleviate this problem.\"}"}
{"id": "9tVn4f8aJO", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HEMM: Holistic Evaluation of Multimodal Foundation Models\\n\\nPaul Pu Liang,\u2217 Akshay Goindani,\u2217 Talha Chafekar, Leena Mathur, Haofei Yu, Ruslan Salakhutdinov, Louis-Philippe Morency\\n\\nMachine Learning Department and Language Technologies Institute\\nCarnegie Mellon University\\n\\nhttps://github.com/pliang279/HEMM\\n\\nAbstract\\n\\nMultimodal foundation models that can holistically process text alongside images, video, audio, and other sensory modalities are increasingly used in a variety of real-world applications. However, it is challenging to characterize and study progress in multimodal foundation models, given the range of possible modeling decisions, tasks, and domains. In this paper, we introduce Holistic Evaluation of Multimodal Models (HEMM ) to systematically evaluate the capabilities of multimodal foundation models across a set of 3 dimensions: basic skills, information flow, and real-world use cases.\\n\\nBasic multimodal skills are internal abilities required to solve problems, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and the ability to handle external knowledge.\\n\\nInformation flow studies how multimodal content changes during a task through querying, translation, editing, and fusion.\\n\\nUse cases span domain-specific challenges introduced in real-world multimedia, affective computing, natural sciences, healthcare, and human-computer interaction applications. Through comprehensive experiments across the 30 tasks in HEMM , we (1) identify key dataset dimensions (e.g., basic skills, information flows, and use cases) that pose challenges to today's models, and (2) distill performance trends regarding how different modeling dimensions (e.g., scale, pre-training data, multimodal alignment, pre-training, and instruction tuning objectives) influence performance. Our conclusions regarding challenging multimodal interactions, use cases, and tasks requiring reasoning and external knowledge, the benefits of data and model scale, and the impacts of instruction tuning yield actionable insights for future work in multimodal foundation models.\\n\\n1 Introduction\\n\\nBuilding upon rapid progress in large-scale language and vision pretraining [24, 69, 106], the new generation of multimodal foundation models is increasing adept at learning interactions between modalities [83], enables both static prediction and dynamic interaction [55], and even shows emergent properties never seen before in pretraining corpora [60]. Previous standards for benchmarking multimodal models based on collections of modality and task-specific datasets [8, 57, 29, 66] are increasingly insufficient in light of these general capabilities. In order to study fundamental questions regarding why multimodal foundation models exhibit certain behaviors, when they perform well in the real world, and which modeling paradigms are most effective, there is a need for a holistic evaluation scheme beyond individual datasets or contexts.\\n\\nTo address this need, we contribute Holistic Evaluation of Multimodal Models (HEMM ), visualized in Figure 1. HEMM , as an evaluation framework, goes beyond conventional lists of datasets to emphasize holistic benchmarking at three levels. The first level benchmarks basic multimodal skills: fundamental internal abilities required to address multimodal problems, such as interactions between modalities.\"}"}
{"id": "9tVn4f8aJO", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: HEMM is an evaluation framework that characterizes multimodal models along several dimensions (size, architecture, pretraining objective, fine-tuning objective, training data) and emphasizes holistic benchmarking of these models at three disentangled levels: basic skills, information flow, and use cases.\\n\\nRedundant, unique, and synergistic features [26, 68], alignment of fine-grained and coarse-grained information [104], reasoning across compositional features [115], and integration of external knowledge [90]. The second level benchmarks information flow: how multimodal information transforms during tasks such as querying [98], translation [109], editing [108], and fusion [60]. The third level benchmarks multimodal use cases: how models perform in real-world challenges across domains, including multimedia, affective computing, natural sciences, healthcare, and human-computer interaction (HCI). Together, these three levels taxonomize a wide spectrum of 30 image-text datasets, enabling HEMM to serve as a holistic framework to evaluate multimodal models.\\n\\nTo aid in HEMM evaluation, we also present a new categorization of models spanning key modeling decisions, such as model size and modality processing (e.g., interleaved inputs), and training decisions, such as pretraining and fine-tuning objectives. We (1) identify key dataset dimensions (e.g., basic skills, information flows, and use cases) that pose challenges to today\u2019s models, and (2) distill performance trends regarding how different modeling and training decisions (e.g., scale, pre-training data, multimodal alignment, pre-training, and instruction tuning objectives) influence downstream task performance. Our analysis yields tangible directions for future work, including challenging multimodal skills, tasks, and use cases, impacts of diversity and scale, and guidelines on modeling architectures and training objectives. HEMM is publicly available at anon, and encourages community involvement in its expansion of datasets, annotations, models, and evaluation metrics.\\n\\n2 Key Benchmarking Principles and Datasets in HEMM\\n\\nHEMM includes 30 datasets summarized in Table 1. These datasets require different multimodal skills to solve, display different types of multimodal information flow, and belong to different real-world use cases with domain-specific challenges.\\n\\n2.1 Basic multimodal skills\\n\\nMultimodal skills are internal abilities required to solve multimodal tasks, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and using external knowledge. Multimodal interactions study how modality information is integrated for a multimodal task [69, 77, 52, 9], which can be redundant: shared between modalities, such as smiling while telling a humorous joke [43, 89], unique: present in only one of the modalities [35, 54], and synergistic: emergence of new information from both modalities, such as conveying sarcasm through conflicting verbal and nonverbal cues [15, 68]. Datasets with high referential information between modalities test for redundancy, such as in VQA, and translation on NOCAPS. Tasks with uniqueness or synergy include understanding movie posters (MM-IMDB), memes (MEMEAP), figurative language (IRFL), facial expressions (FER-2013), and cartoons (NEWYORKERARTOON).\"}"}
{"id": "9tVn4f8aJO", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: HEMM includes a comprehensive suite of datasets to benchmark multimodal foundation models. We categorize each dataset based on the basic multimodal skills needed to solve them \u2013 the type of multimodal interaction, granularity of multimodal alignment, level of reasoning, and need for external knowledge, how information flows between modalities, and the real-world use cases they impact.\\n\\n| Dataset # | Samples | Interactions | Fine-grained | Reasoning | Knowledge | Info. Flow | Use case       |\\n|-----------|---------|--------------|--------------|-----------|-----------|------------|----------------|\\n| VQA       | 614K    | Redundancy   | Yes          | Less      | No        | No         | Querying Multimedia |\\n| VISUALGENOME | 1.7M    | Redundancy   | Yes          | Less      | No        | No         | Querying Multimedia |\\n| VCR       | 290K    | Redundancy   | Yes          | Less      | No        | No         | Fusion Multimedia |\\n| OK-VQA    | 14K     | Redundancy   | Yes          | Less      | Yes       | No         | Fusion Multimedia |\\n| GQA       | 22M     | Redundancy   | Yes          | Less      | No        | No         | Querying Multimedia |\\n| NOAPS     | 15K     | Redundancy   | No           | Less      | No        | No         | Translation Multimedia |\\n| FLICKR    | 30K     | Redundancy   | No           | Less      | No        | No         | Translation Multimedia |\\n| WINOGROUND | 1.6K    | Redundancy   | Yes          | Less      | No        | No         | Querying Multimedia |\\n| NLVR      | 92K     | Redundancy   | Yes          | Less      | No        | No         | Querying Multimedia |\\n| NLVR2     | 107K    | Redundancy   | No           | Less      | No        | No         | Querying Multimedia |\\n| IRFL      | 3.9K    | Synergy      | No           | More      | No        | No         | Fusion Multimedia |\\n| MM-IMDB   | 25K     | Synergy      | No           | Less      | No        | No         | Fusion Multimedia |\\n| MAGICBRUSH | 10K     | Synergy      | Yes          | Less      | No        | No         | Editing Multimedia |\\n| LNCOCO    | 8.5K    | Uniqueness   | Yes          | Less      | Yes       | Translation Multimedia |\\n| CARTOON   | 364     | Synergy      | No           | More      | Yes       | Fusion Affect |\\n| NATURALIST | 675K    | Synergy      | No           | More      | Yes       | Fusion Affect |\\n| MEmAP     | 560     | Synergy      | No           | More      | Yes       | Fusion Affect |\\n| EMOTION   | 10K     | Synergy      | No           | More      | Yes       | Fusion Affect |\\n| FER-2013  | 30K     | Uniqueness   | No           | Less      | No        | Querying Affect |\\n| SCIENCEQA | 21K     | Synergy      | No           | Less      | Yes       | Fusion Science |\\n| RESISC45  | 31K     | Uniqueness   | No           | Less      | No        | Querying Science |\\n| UCMERCED LAND USE | 2K | Uniqueness | No | Less | No | Querying Science |\\n| INATURALIST | 675K | Uniqueness | Yes | Less | Yes | Querying Science |\\n| DECIMER  | 5K      | Uniqueness   | No           | More      | Yes       | Translation Science |\\n| PAVTHQVA | 33K     | Redundancy   | Yes          | Less      | Yes       | Querying Healthcare |\\n| VQARAD   | 3.5K    | Redundancy   | Yes          | More      | Yes       | Querying Healthcare |\\n| PENPATH  | 218K    | Redundancy   | Yes          | More      | Yes       | Querying Healthcare |\\n| S LAKE   | 13K     | Redundancy   | Yes          | More      | Yes       | Querying Healthcare |\\n| ENRICO   | 1.4K    | Uniqueness   | No           | Less      | No        | Querying HCI |\\n| SCREEN2WORDS | 112K | Uniqueness | No | Less | No | Translation HCI |\\n\\nGranularity of multimodal alignment involves identifying alignment across elements in different modalities. For example, answering a question might require a model to perform fine-grained alignment to reference one specific object out of many possible objects in an image. Tasks that explicitly test for fine-grained alignment include localized reasoning on VISUALGENOME, WINOGROUND, while tasks that emphasize coarse-grained alignment (e.g., making a prediction relevant to a whole image) include interpreting cartoon images [37], movie posters [5], and memes [46, 43].\\n\\nReasoning and external knowledge involve the combination of local pieces of information to form increasingly rich and complex multimodal representations. For example, being able to perform multi-hop inference from Wikipedia text and images [76] or solving science questions given visual diagrams and executing multiple logical steps [75]. Tasks like WINOGROUND explicitly test for reasoning and tasks like OK-VQA are designed to assess external knowledge.\\n\\n2.2 Multimodal information flow\\nMultimodal information flow studies how information transforms across tasks, including cross-modal translation, editing, querying, and fusion.\\n\\nCross-modal translation exploits shared information by mapping data in one modality to another. Examples include translating from text to image for image generation (e.g., LNCOCO) and translating from image to text for image captioning (e.g., NOAPS, SCREEN2WORDS).\\n\\nCross-modal editing involves semantically editing data in one modality according to another modality (e.g., given an image, following a natural language instruction to \u201cchange the background from day to night\u201d). The model takes in the original image (with potentially more reference images), along with a task description specifying the edit, and outputs the edited image. We use the MAGICBRUSH dataset to test cross-modal editing.\\n\\nCross-modal querying involves a model\u2019s ability to answer natural language questions that query specific information about an input. The model takes in the original image, a text description, the\"}"}
{"id": "9tVn4f8aJO", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model   | #Param | Data Size | Data Diversity | Training Type | INST | Modality Proc |\\n|---------|--------|-----------|----------------|---------------|------|---------------|\\n| OSIMOS  | 8.5    | 1.6B      | Yes            | End-to-end    | Yes  | interleaved   |\\n| PEN     | 3.2B   | 180M      | No             | Modular Fine-tune | No   | interleaved   |\\n| INSTRUCT| 4.0B   | 244M      | Yes            | Modular Fine-tune | Yes  | separate      |\\n| LAMADA  | 7.0B   | 567K      | No             | Modular Fine-tune | Yes  | separate      |\\n| PLUG-WL | 7.2B   | -         | Yes            | Modular Fine-tune | Yes  | separate      |\\n| FUYU    | 10.0B  | -         | Yes            | End-to-end    | No   | interleaved   |\\n| BLIP-2  | 12.1B  | 244M      | No             | Modular Fine-tune | No   | separate      |\\n| MINGPT-4| 13.0B  | 5M        | Yes            | End-to-end    | No   | interleaved   |\\n| NY-YORKER | 14.0B | 82M      | Yes            |                  | No   | interleaved   |\\n\\nMultimodal fusion aims to learn interactions to combine information from different modalities, such as classifying diseases given x-ray images and medical tests, or detecting humor from cartoon images and captions. Multimodal fusion takes in the image, text, and a description of the task, and then outputs a prediction, which can include affective states like humor in NEW YORKER CARTOON, hate speech detection in HATEFUL MEME, or in science problems (SCIENCE QA).\\n\\n2.3 Real-world Use Cases\\n\\nEach use case is drawn from a real-world application with their own specific challenges. Multimedia includes efficient search, retrieval, indexing, and generation of digital content. Multimedia tasks in HEMM include question answering about images and videos (VQA, VCR), multimedia captioning (FLICKR 30K, NOCPS), compositional visual reasoning (WINGROUND, NLVR), understanding cartoons, movie posters (MM-IMDB), memes (MEME, MEME and MEME and MEME), and figurative language (IRFL), and editing images (MAGIC RUSH).\\n\\nAffective computing aims to perceive human affective states (emotions, sentiment, personalities, humor, sarcasm, social interactions) [86], and is important for building emotionally and socially-intelligent AI [56, 78] and human-AI interaction [55]. HEMM includes NEW YORKER CARTOON (cartoon images and captions), HATEFUL MEME (hateful content in memes), FER-2013 for facial expressions, MEME CAP for meme captioning, and MEME for emotions in memes.\\n\\nNatural sciences aims to deepen our knowledge of physical, chemical, biological, and environmental sciences. These can involve satellite images, chemical bonds, land and agriculture use, wildlife, and specific scientific terminology [101]. Tasks in HEMM include SCIENCE QA testing different science topics and RESISC 45 for land scene classification.\\n\\nHealthcare involves integrating multimodal signals such as lab tests, imaging reports, and doctor-patient interactions to help doctors interpret high-dimensional data and assist them in diagnosis [48, 51]. We include processing text reports and medical images in the form of PATHVQA for pathology, VQARAD for radiology images, and VQARAD for medical visual question answering.\\n\\nHCI involves user design, usability, user experience, and other challenges related to humans interacting with computers [81]. HCI tasks can involve visual information such as screen layouts, user actions, and feedback mechanisms. HCI tasks in HEMM include ENRICO for classifying mobile UI designs and SCREEN2WORDS for UI screen content summarization.\\n\\n3 Key Modeling Principles and Models in HEMM\\n\\nTable 2 summarizes the 11 models we evaluate in HEMM, which span different numbers of parameters, model architectures, training datasets, pretraining objectives, and fine-tuning objectives.\\n\\n3.1 Modeling decisions\\n\\nModel parameters\\n\\nParameters can vary greatly across different multimodal models, from 100M params to approximately 1000B params. We consider models with total number of parameters less than 1000B as they are more efficient and scale well with the number of parameters.\"}"}
{"id": "9tVn4f8aJO", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Performance on different dataset dimensions, as measured via the mean BARTscore on each dataset across all 11 tested multimodal models.\\n\\n| Dimension Category | Perf (\u2191) |\\n|--------------------|----------|\\n| Real-world use case |          |\\n| Multimedia         |          |\\n| Affect             | 30.35    |\\n| Health             | 20.24    |\\n| Science            | 19.83    |\\n| HCI                | 15.70    |\\n| Multimodal interaction |     |\\n| Redundancy         | 29.04    |\\n| Uniqueness         | 19.60    |\\n| Synergy            | 33.73    |\\n| Reasoning          |          |\\n| More Reasoning     | 27.50    |\\n| Less Reasoning     | 26.84    |\\n| Granularity        |          |\\n| Fine-grained       | 26.52    |\\n| Coarse-grained     | 27.52    |\\n| Knowledge          |          |\\n| External           | 23.51    |\\n| None               | 29.62    |\\n| Information flow   |          |\\n| Querying           | 25.88    |\\n| Translation        | 18.97    |\\n| Fusion             | 33.77    |\\n\\nTable 4: Performance on different modeling decisions, as measured via the mean BARTscore for each model across all 30 tested multimodal datasets.\\n\\n| Dimension Category | Perf (\u2191) |\\n|--------------------|----------|\\n| Modality processing |        |\\n| Interleaved        | 22.94    |\\n| Separate           | 28.58    |\\n| Model size         |          |\\n| Small              | 23.34    |\\n| Medium             | 23.87    |\\n| Large              | 42.33    |\\n| Training decisions |          |\\n| Training type      |          |\\n| Modular            | 24.92    |\\n| End-to-end         | 21.26    |\\n| Size of training data |      |\\n| Small              | 16.80    |\\n| Medium             | 30.10    |\\n| Large              | 31.77    |\\n| Diversity of training data |    |\\n| Non-diverse        | 21.71    |\\n| Diverse            | 30.15    |\\n| Instruction tuning |          |\\n| No                 | 22.49    |\\n| Yes                | 29.71    |\\n\\nthan or equal to 4B (e.g., INSTRUCT-BLIP) as small, whereas those having more than 4B parameters (e.g., FUYU-8B) are considered medium. GPT-4V and GEMINI are considered large.\\n\\nModality processing: Some multimodal models (e.g., FUYU-8B) support interleaved inputs like \\\"<dog_img> This is a very cute dog.<cat_img> This is a very cute cat.\\\", unlike models that only support separate image and text queries (e.g., BLIP-2, MINI-GPT-4).\\n\\n3.2 Training Characteristics\\n\\nTraining type: End-to-end training involves fine-tuning unimodal encoders, pretrained LLMs, and a multimodal model jointly, as seen in ELM, FUYU-8B, etc. Another category operates by freezing unimodal encoders and LLM, and then training only a mapping that aligns frozen image features with frozen LLM features. These trainable mappings include Q-former [22] (used in INSTRUCT-BLIP), linear layers [128, 92] (used in MINI-GPT-4), and attention blocks used in OPEF LAMINGO.\\n\\nSize of pre-training data: We consider the total size of pre-training data used for training, including instruction and supervised data. ELM has small data scale, with less than 100M training data points. FUYU-8B has medium data-scale, with more than 100M training data points. While GPT-4V and GEMINI do not release data sizes, we estimate their size to be much larger than other models and therefore are considered to have large data scale.\\n\\nDiversity of pre-training data: We consider the diversity of multimodal tasks used for training, including visual QA, visual conversations, and interleaved images and text. INSTRUCT-BLIP and ELM are pre-trained on diverse data, in contrast to LLAMA DAPTER, OPEF LAMINGO, etc., which only use image captioning data for training.\\n\\nInstruction tuning: By transforming supervised tasks into an 'instruction' format, instruction tuning has been shown to benefit performance and improve the controllability of LLMs. MINI-GPT-4 and INSTRUCT-BLIP include an instruction tuning stage, while models like BLIP-2 do not.\\n\\n4 Experiments\\n\\nIn this section, we discuss extensive experiments conducted to holistically evaluate the performance of multimodal foundation models based on HEMM.\"}"}
{"id": "9tVn4f8aJO", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Responses of GPT-4V and GEMINI on samples from the science category. These failure cases show that the models lack domain knowledge and are unable to correctly translate the images of molecules to the SMILES notations (a). Example (b) shows that the models struggle on tasks requiring complex reasoning, failing to comprehend the relation between the force and the size of the magnets. In (c), all models except GPT-4V are unable to capture the fine-grained details and misclassify the image as an airport instead of a runway.\\n\\n4.1 Experimental setup\\n\\nIndividual metrics\\n\\nFor all text generation tasks, we use the established natural language generation evaluation metric BARTScore [122], which was found to have the highest correlation with human judgement [122]. We compute BARTScore(r, c), where r is the reference and c is the candidate. It can be interpreted as the probability of generating the candidate sentence from the reference. For example, a model might caption an image with the following generated candidate: A row of violins hanging on a wall. The reference (ground truth) of A painting of 5 cello's with a green background would be used to compute the BARTScore with respect to c.\\n\\nAggregating metrics\\n\\nTo aggregate scores across multiple tasks or models, we normalize scores using min-max scaling. Following Chang et al. [16], min represents the score of the worst multi-modal model and max represents the identity score BARTScore(r, r), where r is the ground truth. Subsequently, these normalized scores in the 0 to 1 range can be interpreted as a percentage of model performance relative to the ground truth.\\n\\nComputation\\n\\nSince GPT-4V and GEMINI have query limits, we evaluate their performance on 100 random samples for each dataset (2800 total data points). For a fair comparison with other models, we present the results and findings below based on the performance of those 100 samples per dataset. In Appendix C we present the results of the other models on the full evaluation sets. We evaluate all the models on a single NVIDIA A100 80GB GPU with the inference time for a single image-text pair ranging from 0.1 seconds to 63.7 seconds. We report the average inference times for the models across all datasets and include additional details on the evaluation protocol in Appendix B.\\n\\n4.2 Main results\\n\\nWe summarize our main results here and include full details in Appendix C. We first explain performance trends across the datasets in HEMM, before explaining performance differences across different multimodal foundation models and their design decisions.\\n\\n4.2.1 Performance across dataset dimensions\\n\\nOverall comparisons\\n\\nWe summarize overall trends in Figure 3 and Table 3. On average, models perform better on multimedia datasets, with IRFL (0.58), NLR (0.50), and WN (0.49) showing the highest scores. The lowest scores are for Healthcare, HCI, and Science use cases, such as on DEIMER (0.07), INATURALIST (0.08), ENE (0.12), PATHVQA (0.15), and MEMECAP (0.32). For predicting molecular structures on DEIMER, models are not able to generate correct chemical notations (in Simplified Molecular Input Line Entry System notation) and instead only generate names of individual atoms or compounds (see Figure 2). Other challenging datasets include INATURALIST due to fine-grained visual differences between 5000 species of plants and animals, and healthcare datasets that require intricate analysis of pathology images to identify organs, tissues, and anomalies (see Figure 8). Datasets related to memes were also challenging (0.32 and 0.38 on MEMECAP [43] and MEMOTION [89]), requiring knowledge about current events, pop culture, and metaphors beyond literal meanings.\"}"}
{"id": "9tVn4f8aJO", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Tasks requiring commonsense and compositional reasoning are challenging. In (a), GPT-4V and GEMINI are unable to employ social commonsense to analyze the relationships between the two people. Example (b) demonstrates the models' difficulty in composing information from both modalities, leading to their failure to comprehend the scenario where a tree smashed into the car (not a car smashed into the tree). In (c), all models except GPT-4V fail to grasp the visual metaphors and the juxtaposition of the two scenarios.\\n\\nFigure 3: Average scores are higher for multimedia datasets as compared to other use cases, and lowest for healthcare, HCI, and science. The models struggle on INATURALIST, DECIMER, ENRICO, PATHVQA, and MECAP which require external knowledge, fine-grained alignment, and complex reasoning.\\n\\nMultimodal skills 1: Interactions\\nThe average scores for redundant, unique, and synergistic interactions are 0.29, 0.20, and 0.33. One reason for lower uniqueness scores is the presence of highly challenging visual datasets like DECIMER and ENRICO. On average, the easiest tasks in redundancy are NLVR (0.50) and WINGROUND (0.49). The hardest datasets in uniqueness are INATURALIST (0.08) and DECIMER (0.07), and in synergy are MECAP (0.14) and EMOTION (0.21).\\n\\nMultimodal skills 2: Granularity\\nWe do not find that fine-grained datasets are significantly harder than those with coarse-grained alignment. Tasks requiring fine-grained alignment between image and text like GQA and WINGROUND achieve a score of 0.26, while those only needing coarse-grained alignment (e.g., ENRICO, SCIENCE QA) are still quite challenging (score: 0.27).\\n\\nMultimodal skills 3: Reasoning\\nWe do not find a significant difference between the performance of the models on tasks requiring more (average score = 0.275) or less reasoning (average score = 0.268). The most challenging datasets requiring less reasoning include INATURALIST (0.08) and ENRICO (0.12) due to challenges in fine-grained visual perception and external knowledge, while there are also several challenging datasets requiring more complex reasoning like VCR (0.34) and MECAP (0.14), where the models encounter difficulties with samples requiring commonsense and compositional reasoning (See Figure 4 for examples).\\n\\nMultimodal skills 4: External knowledge\\nThe average performance on tasks requiring external knowledge is 0.23, compared to 0.30 for those not requiring external knowledge. For example, INSTRUCT-BLIP performs well on WINGROUND and VCR that do not require external knowledge but struggles more on knowledge-intensive tasks e.g., INATURALIST, which requires knowledge.\"}"}
{"id": "9tVn4f8aJO", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"about characteristics of a vast number of species, and SLAKE, where medical knowledge is needed to identify the abnormalities in organs.\\n\\nMultimodal Skills 5: Information flow\\nTranslation has the lowest average score amongst all types of information flow (0.19), whereas the average scores on querying and fusion are 0.26 and 0.33 respectively. The low performance on translation is due to the presence of challenging datasets like DECMER and SCREENWORDS requiring mapping images of chemicals and screenshots into text.\\n\\nAlthough the average score for fusion is high, the performance on some datasets is still quite low, such as INSTRUCT-BLIP achieving a score of only 0.04 on MECAP and 0.15 on MM-IMDB.\\n\\n4.2.2 Performance across modeling dimensions\\nWe now compare different modeling decisions and training objectives in Table 4.\\n\\nOverall comparisons across models: GEMINI [97] (0.44), INSTRUCT-BLIP [22] (0.41), BLIP-2 [62] (0.41), and GPT-4V [1] (0.40) achieve the best average performance across all tasks. The low scores of GPT-4V as compared to GEMINI and INSTRUCT-BLIP are due to its generation of keywords like \u201cIndeterminate\u201d, \u201cUncertain\u201d, and \u201cUnknown\u201d on datasets like VQA and GQA, perhaps due to its alignment process. Further, on some datasets related to Memes (e.g., HATEFULMemes) and Health (e.g., SLAKE), GPT-4V refrains from answering the questions and instead generates a response saying \u201cCannot assist with the request\u201d. OPENFALMINGO [6] (0.06), EMU [95] (0.11) have the lowest average scores. From their generations, we find that these models struggle to follow the instructions for challenging datasets like DECMER and ENRICO, and generate hallucinated responses. Moreover, with relatively easier datasets such as FLICKR30K, the captions produced by EMU and OPENFALMINGO tend to fixate on specific objects rather than providing a comprehensive description of the scene, often leading to instances of hallucination related to these objects. As a result, these models rank lowest on many datasets, receiving a normalized score of 0.\\n\\nFigure 5: On average, large models are better than small and medium models (p-values < 0.001). INSTRUCT-BLIP and BLIP-2 are outliers - despite having fewer params, they achieve relatively high performance, even close to GPT-4V and GEMINI.\\n\\nModel scale\\nWe find that the performance of larger models (both total and trainable parameters) is significantly better than the models with a medium or small number of parameters (Figure 5). When grouped based on the total number of parameters, the average scores achieved by large, medium, and small models are 0.42, 0.24, and 0.23 respectively. The difference between the performance of large and medium models is significant (p-value for paired t-Test < 0.001). In particular, large models showed the most improvement on MM-IMDB, MECAP, and HATEFULMemes datasets, which fall into the category of tasks requiring synergistic interactions. On average, the large models perform the best on synergistic tasks with a score of 0.53 compared to 0.30 for medium and 0.23 for small models. For instance, on the MM-IMDB dataset, we observed significant gains in performance when increasing model size: from 0.15 for INSTRUCT-BLIP (small) to 0.36 for BLIP-2 (medium) and 0.48 for GEMINI (large).\\n\\nPretraining data scale\\nAverage scores of the models in large and medium data size categories are 0.31 and 0.30 respectively, whereas models with small pretraining data achieve a significantly lower score of 0.17. We also find that for all datasets, the average score of models with medium pretraining data is higher than the models with small pretraining data. For instance, on the WINOGROUND dataset which requires fine-grained alignment between the modalities, the maximum scores achieved by the models with medium and small pretraining data are 0.45 and 0.80. We also find a significant gap between the maximum scores achieved by the models in the medium (maximum score - 0.18) and small (maximum score - 0.70) categories on the NLVR2 dataset for visual reasoning.\\n\\nDiversity of pre-training data\\nOn average, models trained on diverse datasets perform better (score: 0.30) than models trained only on image captioning datasets (score: 0.21). Diverse training...\"}"}
{"id": "9tVn4f8aJO", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Data allows the models to share learned knowledge and generalize across different tasks. For example, models pretrained with diverse datasets perform significantly better on the knowledge-intensive INATURALIST task, such as BLIP-2 (non-diverse) scoring 0.08 and GEMINI scoring 0.24. For the MECAP dataset which requires external knowledge and complex reasoning, we observe that BLIP-2 (non-diverse) scores 0.06 and PLUG-O WL (diverse) scores 0.21.\\n\\nInstruction tuning vs supervised fine-tuning\\n\\nOn average, instruction-tuned models (average score of 0.30) performed better than the models trained using only supervised fine-tuning (average score of 0.22). The top 3 tasks with the largest performance gap between instruction-tuned and non-instruction-tuned models are DECMER, MECAP, and SCREEN2WORDS, with improvements of 0.15, 0.09, and 0.09 respectively. We also observe that translation tasks (image-to-text) (e.g., FLICKR30K, NOCAPS) benefit from instruction tuning, where the models generate more accurate and detailed captions after human instruction.\\n\\n4.3 Human evaluation\\n\\nTo assess how well HEMM aligns with human preferences, we performed human preference-based evaluation, following Chiang et al. [19], where annotators are shown the outputs of two different models for the same inputs and choose the better output or a tie option. Across 1000 pairwise comparisons by 5 annotators, the pairwise rankings are used to calculate each model's average win rate and Elo rating (see Appendix B.5 for calculation details).\\n\\nTable 5: Average win rate and Elo Rating of 11 models calculated based on the human evaluation of 1000 pairwise comparisons of model responses. Elo rating is reported as the median over 1000 runs with shuffled battle sequences and an initial rating of 1000 for each model. Top 4 and bottom 2 models identified by Elo Rating are consistent with those found by Average BARTScore.\\n\\n| Model       | Avg. Win Rate | Elo Rating |\\n|-------------|---------------|------------|\\n| GEMINI      | 0.73          | 1074       |\\n| GPT-4V      | 0.68          | 1057       |\\n| BLIP-2      | 0.52          | 1033       |\\n| INSTRUCT-BLIP | 0.60       | 1032       |\\n| PLUG-O WL  | 0.45          | 1010       |\\n| LLAMA-DAPTER | 0.45          | 1008       |\\n| FUYU-8B    | 0.42          | 992        |\\n| MINI-GPT-4 | 0.38          | 990        |\\n| KOSMOS-2   | 0.39          | 968        |\\n| EMU        | 0.20          | 924        |\\n| OPENN F LAMINGO | 0.17     | 911        |\\n\\nThe models ranked by Elo ratings are GEMINI (1074), GPT-4V (1057), BLIP-2 (1033), and INSTRUCT-BLIP (1032) (see Table 5). The top 4 models based on the Elo Rating are the same as the top 4 models ranked by BARTScore. Elo Rating of GPT-4V is better than BLIP-2 and INSTRUCT-BLIP. However, the average BARTScore for GPT-4V (0.40) is lower than INSTRUCT-BLIP (0.42) and BLIP-2 (0.41). We also find Elo Rating of bottom two models to be consistent with BARTScore rankings - EMU (0.11) and OPENN F LAMINGO (0.06).\\n\\n5 Related Work\\n\\nMultimodal machine learning brings unique challenges for ML research due to the heterogeneity between modalities and the interconnections found between them [69]. It has inspired many theoretical studies in data heterogeneity and interactions [25], as well as diverse applications in multimedia [44, 14, 88], affective computing [86], robotics [47], finance [39], HCI [25, 82], education [12] and healthcare [80, 110].\\n\\nEvaluation frameworks for multimodal models have significantly shaped the multimodal research landscape, through holistic [57, 66] and domain-specific benchmarks [31, 28]. Recent benchmarks have focused on testing the capabilities of multimodal foundation models, such as MME [29], MMBench [73], LVLM-ehub [111], SEED-Bench [59], Touchstone [7], Mm-vet [121], ReForm-Eval [65], VisIT-Bench [11], FLAVA [45]. Other benchmarks focus on evaluating hallucination [21] and applications in medicine [113] and autonomous driving [107]. These benchmarks contain many tasks, but without the systematic taxonomy and comprehensiveness that HEMM provides.\\n\\nMultimodal foundation models are promising foundations for the future of AI, with impressive reasoning [75], interactive dialogue [49], and few-shot generalization abilities [100]. These models can be pre-trained (typically with image-text self-supervised learning) and fine-tuned for downstream tasks [63, 74, 91, 67], or based on adapting language models with vision to enable text generation conditioned on images [61, 105]. Cross-modal transformer architectures have emerged as a popular backbone due to their suitability for both language and image data [17, 99]. Additionally, composable...\"}"}
{"id": "9tVn4f8aJO", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"models [108] and mixtures of experts [120] can be used to further generate combinations of output modalities. Adapting language models for multimodality is another promising approach where frozen models are aligned on both vision and language to generate text from multimodal inputs [128, 62, 118, 109]. These approaches typically use parameter-efficient modules like LLaMA-Adapter V2 [30] and MAGMA [27] for efficient finetuning. Vision-language instruction tuning has also emerged as a useful technique, as it allows the models to better follow human instructions [112, 128]. Our goal is to make HEMM the most comprehensive benchmark to study the current and future generation of multimodal foundation models, and for the community to continuously contribute to its expansion.\\n\\n6 Conclusion\\n\\nHolistic Evaluation of Multimodal Models (HEMM) is a framework for benchmarking multimodal foundation models. Through a new taxonomy of multimodal skills, information flow, and real-world use cases, HEMM enables comprehensive analysis of multimodal models. HEMM is publicly available, will be regularly updated, and encourages community involvement in its expansion.\\n\\nLimitations and social impact\\n\\nThe evaluation of multimodal models is done only on a subset of all possible skills, information, and use cases in the world. Future work can improve the categorization of datasets into skills, information, and use cases, and discover new dimensions that pose challenges to multimodal models. Such evaluation is critical to ensure that models are sufficiently robust when deployed in real-world scenarios, to prevent unexpected and unintended consequences. Future work should also add new metrics to HEMM measuring real-world societal concerns such as fairness, robustness, social biases, privacy, and efficiency of multimodal models.\\n\\nAcknowledgements\\n\\nThis material is based upon work partially supported by National Science Foundation awards 1722822 and 1750439, National Institutes of Health awards R01MH125740, R01MH132225, R01MH096951 and R21MH130767, and Meta. PPL is supported in part by a Siebel Scholarship and a Waibel Presidential Fellowship. RS is supported in part by ONR grant N000142312368 and DARPA FA8750-23-M-015. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors, and no official endorsement should be inferred. We are also grateful to NVIDIA's GPU support.\\n\\nReferences\\n\\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\\n[2] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In Proceedings of the IEEE International Conference on Computer Vision, pages 8948\u20138957, 2019.\\n[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35:23716\u201323736, 2022.\\n[4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425\u20132433, 2015.\\n[5] John Arevalo, Thamar Solorio, Manuel Montes-y G\u00f3mez, and Fabio A Gonz\u00e1lez. Gated multimodal units for information fusion. arXiv preprint arXiv:1702.01992, 2017.\\n[6] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023.\\n[7] Shuai Bai, Shusheng Yang, Jinze Bai, Peng Wang, Xingxuan Zhang, Junyang Lin, Xinggang Wang, Chang Zhou, and Jingren Zhou. Touchstone: Evaluating vision-language models by language models. arXiv preprint arXiv:2308.16890, 2023.\"}"}
{"id": "9tVn4f8aJO", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Eslam Mohamed Bakr, Pengzhan Sun, Xiaogian Shen, Faizan Farooq Khan, Li Erran Li, and Mohamed Elhoseiny. Hrs-bench: Holistic, reliable and scalable benchmark for text-to-image models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20041\u201320053, 2023.\\n\\nJohn Bateman. Text and image: A critical introduction to the visual/verbal divide. Routledge, 2014.\\n\\nRohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sa\u011fnak Ta\u015fl\u0131lar. Introducing our multimodal models, 2023. URL https://www.adept.ai/blog/fuyu-8b.\\n\\nYonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and Ludwig Schimdt. Visit-bench: A benchmark for vision-language instruction following inspired by real-world use. arXiv preprint arXiv:2308.06595, 2023.\\n\\nPaulo Blikstein and Marcelo Worsley. Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks. Journal of Learning Analytics, 3(2):220\u2013238, 2016.\\n\\nHenning Otto Brinkhaus, Achim Zielesny, Christoph Steinbeck, and Kohulan Rajan. Decimer\u2014hand-drawn molecule images dataset. Journal of Cheminformatics, 14(1):1\u20134, 2022.\\n\\nShyamal Buch, Crist\u00f3bal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, and Juan Carlos Niebles. Revisiting the \\\"video\\\" in video-language understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2917\u20132927, 2022.\\n\\nYitao Cai, Huiyu Cai, and Xiaojun Wan. Multi-modal sarcasm detection in Twitter with hierarchical fusion model. In Anna Korhonen, David Traum, and Llu\u00eds M\u00e0rquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2506\u20132515, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1239. URL https://aclanthology.org/P19-1239.\\n\\nYingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. Webqa: Multihop and multimodal qa. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16495\u201316504, 2022.\\n\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104\u2013120. Springer, 2020.\\n\\nGong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 105(10):1865\u20131883, 2017.\\n\\nWei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024.\\n\\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.\\n\\nChenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. Holistic analysis of hallucination in gpt-4vision: Bias and interference challenges. arXiv preprint arXiv:2311.03287, 2023.\\n\\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning, June 2023. URL http://arxiv.org/abs/2305.06500. arXiv:2305.06500 [cs].\\n\\nBiplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: A mobile app dataset for building data-driven design applications. In Proceedings of the 30th annual ACM symposium on user interface software and technology, pages 845\u2013854, 2017.\\n\\nYifan Du, Zikang Liu, Junyi Li, and Wayne Xin Zhao. A survey of vision-language pre-trained models. arXiv preprint arXiv:2202.10936, 2022.\\n\\nBruno Dumas, Denis Lalanne, and Sharon Oviatt. Multimodal interfaces: A survey of principles, models and frameworks. In Human machine interaction: Research results of the mmi program, pages 3\u201326. Springer, 2009.\"}"}
{"id": "9tVn4f8aJO", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[26] Bruno Dumas, Jonathan Pirau, and Denis Lalanne. Modelling fusion of modalities in multimodal interactive systems with mmmm. In Proceedings of the 19th ACM International Conference on Multimodal Interaction, pages 288\u2013296, 2017.\\n\\n[27] Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, and Anette Frank. Magma\u2013multimodal augmentation of generative models through adapter-based finetuning. arXiv preprint arXiv:2112.05253, 2021.\\n\\n[28] Francis Ferraro, Nasrin Mostafazadeh, Ting-Hao Huang, Lucy Vanderwende, Jacob Devlin, Michel Galley, and Margaret Mitchell. A survey of current datasets for vision and language research. In Llu\u00eds M\u00e0rquez, Chris Callison-Burch, and Jian Su, editors, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 207\u2013213, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1021. URL https://aclanthology.org/D15-1021.\\n\\n[29] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023.\\n\\n[30] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023.\\n\\n[31] Dimitris Gkoumas, Qiuchi Li, Christina Lioma, Yijun Yu, and Dawei Song. What makes the difference? an empirical comparison of fusion strategies for multimodal language analysis. Information Fusion, 66:184\u2013197, 2021.\\n\\n[32] Ian J Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron Courville, Mehdi Mirza, Ben Hamner, Will Cukierski, Yichuan Tang, David Thaler, Dong-Hyun Lee, et al. Challenges in representation learning: A report on three machine learning contests. In Neural Information Processing: 20th International Conference, ICONIP 2013, Daegu, Korea, November 3-7, 2013. Proceedings, Part III 20, pages 117\u2013124. Springer, 2013.\\n\\n[33] Hessel Haagsma, Johan Bos, and Malvina Nissim. Magpie: A large corpus of potentially idiomatic expressions. In 12th Language Resources and Evaluation Conference: LREC 2020, pages 279\u2013287. European Language Resources Association (ELRA), 2020.\\n\\n[34] F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis), 5(4):1\u201319, 2015.\\n\\n[35] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286, 2020.\\n\\n[36] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.\\n\\n[37] Jack Hessel, Ana Marasovi\u00b4c, Jena D Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, and Yejin Choi. Do androids laugh at electric sheep? humor\u201d understanding\u201d benchmarks from the new yorker caption contest. arXiv preprint arXiv:2209.06293, 2022.\\n\\n[38] Micah Hodosh, Peter Young, and Julia Hockenmaier. Framing image description as a ranking task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research, 47:853\u2013899, 2013.\\n\\n[39] Markus A H\u00f6llerer, Dennis Jancsary, and Maria Grafstr\u00f6m. 'a picture is worth a thousand words': Multimodal sensemaking of the global financial crisis. Organization Studies, 39(5-6):617\u2013644, 2018.\\n\\n[40] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning perception with language models. Advances in Neural Information Processing Systems, 36, 2024.\\n\\n[41] Zhi Huang, Federico Bianchi, Mert Yuksekgonul, Thomas Montine, and James Zou. Leveraging medical twitter to build a visual\u2013language foundation model for pathology ai. bioRxiv, pages 2023\u201303, 2023.\\n\\n[42] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700\u20136709, 2019.\\n\\n[43] EunJeong Hwang and Vered Shwartz. Memecap: A dataset for captioning and interpreting memes. arXiv preprint arXiv:2305.13703, 2023.\"}"}
{"id": "9tVn4f8aJO", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi Xie. Prompting visual-language models for efficient video understanding. In European Conference on Computer Vision, pages 105\u2013124. Springer, 2022.\\n\\nDouwe Kiela. Grounding, meaning and foundation models: Adventures in multimodal machine learning. In Proceedings of the 30th ACM International Conference on Multimedia, pages 5\u20135, 2022.\\n\\nDouwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. Advances in neural information processing systems, 33:2611\u20132624, 2020.\\n\\nElsa A Kirchner, Stephen H Fairclough, and Frank Kirchner. Embedded multimodal interfaces in robotics: applications, future trends, and societal implications. In The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions-Volume 3, pages 523\u2013576. 2019.\\n\\nAdrienne Kline, Hanyin Wang, Yikuan Li, Saya Dennis, Meghan Hutch, Zhenxing Xu, Fei Wang, Feixiong Cheng, and Yuan Luo. Multimodal machine learning in precision health: A scoping review. npj Digital Medicine, 5(1):171, 2022.\\n\\nJing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal inputs and outputs. 2023.\\n\\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32\u201373, 2017.\\n\\nFelix Krones, Umar Marikkar, Guy Parsons, Adam Szmul, and Adam Mahdi. Review of multimodal machine learning approaches in healthcare. arXiv preprint arXiv:2402.02460, 2024.\\n\\nJulia Kruk, Jonah Lubin, Karan Sikka, Xiao Lin, Dan Jurafsky, and Ajay Divakaran. Integrating text and image: Determining multimodal document intent in Instagram posts. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4622\u20134632, 2019.\\n\\nJason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. A dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):1\u201310, 2018.\\n\\nMina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin Paranjape, Ines Gerard-Ursin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong, et al. Evaluating human-language model interaction. arXiv preprint arXiv:2212.09746, 2022.\\n\\nSangmin Lee, Bolin Lai, Fiona Ryan, Bikram Boote, and James M Rehg. Modeling multimodal social interactions: New challenges and baselines with densely aligned representations. arXiv preprint arXiv:2403.02090, 2024.\\n\\nTony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Benita Teufel, Marco Bellagente, et al. Holistic evaluation of text-to-image models. arXiv preprint arXiv:2311.04287, 2023.\\n\\nLuis A Leiva, Asutosh Hota, and Antti Oulasvirta. Enrico: A high-quality dataset for topic modeling of mobile UI designs. Proc. MobileHCI extended abstracts, 2020.\\n\\nBohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023.\\n\\nChunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020, 1, 2023.\\n\\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888\u201312900. PMLR, 2022.\\n\\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\"}"}
{"id": "9tVn4f8aJO", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.\\n\\nYang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language instructions to mobile UI action sequences. arXiv preprint arXiv:2005.03776, 2020.\\n\\nZejun Li, Ye Wang, Mengfei Du, Qingwen Liu, Binhao Wu, Jiwen Zhang, Chengxing Zhou, Zhihao Fan, Jie Fu, Jingjing Chen, et al. Reform-eval: Evaluating large vision language models via unified reformulation of task-oriented benchmarks. arXiv preprint arXiv:2310.02569, 2023.\\n\\nPaul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Yufan Chen, Peter Wu, Michelle A Lee, Yuke Zhu, et al. Multibench: Multiscale benchmarks for multimodal representation learning. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.\\n\\nPaul Pu Liang, Yiwei Lyu, Xiang Fan, Jeffrey Tsaw, Yudong Liu, Shentong Mo, Dani Yogatama, Louis-Philippe Morency, and Ruslan Salakhutdinov. High-modality multimodal transformer: Quantifying modality & interaction heterogeneity for high-modality representation learning. arXiv preprint arXiv:2203.01311, 2022.\\n\\nPaul Pu Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard Chen, Zihao Deng, Faisal Mahmood, Ruslan Salakhutdinov, and Louis-Philippe Morency. Quantifying & modeling multimodal interactions: An information decomposition framework. In Advances in Neural Information Processing Systems, 2023.\\n\\nPaul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Foundations & trends in multimodal machine learning: Principles, challenges, and open questions. ACM Computing Surveys, 2023.\\n\\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74\u201381, 2004.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.\\n\\nBo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 1650\u20131654. IEEE, 2021.\\n\\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023.\\n\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019.\\n\\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022.\\n\\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\nEmily E Marsh and Marilyn Domas White. A taxonomy of relationships between images and text. Journal of documentation, 2003.\\n\\nLeena Mathur, Paul Pu Liang, and Louis-Philippe Morency. Advancing social intelligence in AI agents: Technical challenges and open questions. arXiv preprint arXiv:2404.11023, 2024.\\n\\nGeorge A Miller. Wordnet: a lexical database for English. Communications of the ACM, 38(11):39\u201341, 1995.\\n\\nChristian Moro, Jessica Smith, and Zane Stromberga. Multimodal learning in health sciences and medicine: Merging technologies to enhance student learning and communication. Biomedical Visualisation: Volume 5, pages 71\u201378, 2019.\"}"}
{"id": "9tVn4f8aJO", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Brad A. Myers. A brief history of human-computer interaction technology. *INTERACTIONS*, 5(2):44\u201354, 1998.\\n\\nZeljko Obrenovic and Dusan Starcevic. Modeling multimodal human-computer interaction. *Computer*, 37(9):65\u201372, 2004.\\n\\nChristian Otto, Matthias Springstein, Avishek Anand, and Ralph Ewerth. Understanding, categorizing and predicting semantic image-text relations. In *Proceedings of the 2019 on International Conference on Multimedia Retrieval*, pages 168\u2013176, 2019.\\n\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. *Advances in neural information processing systems*, 35:27730\u201327744, 2022.\\n\\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. *arXiv preprint arXiv:2306.14824*, 2023.\\n\\nRosalind W. Picard. *Affective computing*. MIT press, 2000.\\n\\nJordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In *Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part V* 16, pages 647\u2013664. Springer, 2020.\\n\\nVignesh Ramanathan, Percy Liang, and Li Fei-Fei. Video event understanding using natural language descriptions. In *Proceedings of the IEEE international conference on computer vision*, pages 905\u2013912, 2013.\\n\\nChhavi Sharma, William Paka, Scott, Deepesh Bhageria, Amitava Das, Soujanya Poria, Tanmoy Chakraborty, and Bj\u00f6rn Gamb\u00e4ck. Task Report: Memotion Analysis 1.0 @SemEval 2020: The Visuo-Lingual Metaphor! In *Proceedings of the 14th International Workshop on Semantic Evaluation (SemEval-2020)*, Barcelona, Spain, Sep 2020. Association for Computational Linguistics.\\n\\nSheng Shen, Chunyuan Li, Xiaowei Hu, Yujia Xie, Jianwei Yang, Pengchuan Zhang, Zhe Gan, Lijuan Wang, Lu Yuan, Ce Liu, et al. K-lite: Learning transferable visual models with external knowledge. *Advances in Neural Information Processing Systems*, 35:15558\u201315573, 2022.\\n\\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic representations. *arXiv preprint arXiv:1908.08530*, 2019.\\n\\nYixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. *arXiv preprint arXiv:2305.16355*, 2023.\\n\\nAlane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. A corpus of natural language for visual reasoning. In *Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)*, pages 217\u2013223, 2017.\\n\\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for reasoning about natural language grounded in photographs. *arXiv preprint arXiv:1811.00491*, 2018.\\n\\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. *arXiv preprint arXiv:2307.05222*, 2023.\\n\\nZineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. *arXiv preprint arXiv:2305.11846*, 2023.\\n\\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. *arXiv preprint arXiv:2312.11805*, 2023.\\n\\nTristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 5238\u20135248, 2022.\\n\\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov. Multimodal transformer for unaligned multimodal language sequences. In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, pages 6558\u20136569, 2019.\"}"}
{"id": "9tVn4f8aJO", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34:200\u2013212, 2021.\\n\\nDevis Tuia, Benjamin Kellenberger, Sara Beery, Blair R Costelloe, Silvia Zuffi, Benjamin Risse, Alexander Mathis, Mackenzie W Mathis, Frank Van Langevelde, Tilo Burghardt, et al. Perspectives in machine learning for wildlife conservation. Nature communications, 13(1):1\u201315, 2022.\\n\\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset-supplementary material. Reptilia, 32(400):1\u20133, 2017.\\n\\nBryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. Screen2words: Automatic mobile ui summarization with multimodal learning. In The 34th Annual ACM Symposium on User Interface Software and Technology, pages 498\u2013510, 2021.\\n\\nFei Wang, Liang Ding, Jun Rao, Ye Liu, Li Shen, and Changxing Ding. Can linguistic knowledge improve multimodal alignment in vision-language pretraining? arXiv preprint arXiv:2308.12898, 2023.\\n\\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022.\\n\\nWenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022.\\n\\nLicheng Wen, Xuemeng Yang, Daocheng Fu, Xiaofeng Wang, Pinlong Cai, Xin Li, Tao Ma, Yingxuan Li, Linran Xu, Dengke Shang, Zheng Zhu, Shaoyan Sun, Yeqi Bai, Xinyu Cai, Min Dou, Shuanglu Hu, and Botian Shi. On the road with gpt-4v(ision): Early explorations of visual-language model on autonomous driving, 2023.\\n\\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.\\n\\nShengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023.\\n\\nKeyang Xu, Mike Lam, Jingzhi Pang, Xin Gao, Charlotte Band, Piyush Mathur, Frank Papay, Ashish K Khanna, Jacek B Cywinski, Kamal Maheshwari, et al. Multimodal machine learning for automated icd coding. In Machine learning for healthcare conference, pages 197\u2013215. PMLR, 2019.\\n\\nPeng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. arXiv preprint arXiv:2306.09265, 2023.\\n\\nZhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. arXiv preprint arXiv:2212.10773, 2022.\\n\\nZhiling Yan, Kai Zhang, Rong Zhou, Lifang He, Xiang Li, and Lichao Sun. Multimodal chatgpt for medical applications: an experimental study of gpt-4v. arXiv preprint arXiv:2310.19061, 2023.\\n\\nYi Yang and Shawn Newsam. Bag-of-visual-words and spatial extensions for land-use classification. In Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems, pages 270\u2013279, 2010.\\n\\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.\\n\\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.\\n\\nRon Yosef, Yonatan Bitton, and Dafna Shahaf. Irfl: Image recognition of figurative language. arXiv preprint arXiv:2303.15445, 2023.\"}"}
{"id": "9tVn4f8aJO", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023.\\n\\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. TACL, 2:67\u201378, 2014.\\n\\nHaofei Yu, Paul Pu Liang, Ruslan Salakhutdinov, and Louis-Philippe Morency. Mixture of multimodal interaction experts. In UniReps: the First Workshop on Unifying Representations in Neural Models, 2023.\\n\\nWeihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023.\\n\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34:27263\u201327277, 2021.\\n\\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\\n\\nKai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: A manually annotated dataset for instruction-guided image editing. In Advances in Neural Information Processing Systems, 2023.\\n\\nRenrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023.\\n\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019.\\n\\nBolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127:302\u2013321, 2019.\\n\\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\"}"}
{"id": "9tVn4f8aJO", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Checklist\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n   (b) Did you describe the limitations of your work? [Yes] We have included limitations in Section 6.\\n   (c) Did you discuss any potential negative societal impacts of your work? [Yes] We have included potential negative societal impacts in Section 6.\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A] We do not present any theoretical results in our work.\\n   (b) Did you include complete proofs of all theoretical results? [N/A] We do not present any theoretical results in our work, hence there are no proofs.\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] We have included code in the supplemental material.\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] We provide the experimental details in Section 4.1 and in Appendix B.\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] We report results with mean and standard deviation from running multiple times in the appendix. Using multiple runs we also compute statistical significance for all dataset and model performance comparisons, all the results in the main paper are only highlighted if they are statistically significant according to p-value.\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] We provide details about compute and the type of resources used in Section 4.1.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes] We cite all existing models, datasets, and work we used in the references.\\n   (b) Did you mention the license of the assets? [Yes] The license of all the assets used in our work has been mentioned in Appendix A.1 and A.4.\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We have included full links to the dataset, models, and code in the supplementary material.\\n   (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] We provide access information of the datasets in Appendix A.1. To the best of our knowledge, all of these datasets are collected with consent from participating users, especially in the healthcare domain where user data is sensitive. Best practices for de-identification of user data were followed by these datasets. The dataset for facial expression recognition (FER-2013) contains human faces collected through Google image search queries, so user consent was not directly obtained, but the authors of FER-2013 have ensured that their dataset follows fair use guidelines and there is no personally identifiable information released.\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] Yes, we have included all dataset details in Appendix A.1 including if the individual datasets in HEMM contain personally identifiable information.\"}"}
{"id": "9tVn4f8aJO", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. If you used crowdsourcing or conducted research with human subjects...\\n\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] We provide the instructions regarding annotations in Section 4.3 and in Appendix A.2.\\n\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [Yes] Based on direct communication with our institution's IRB office, this line of research is exempt from IRB, and the information obtained during our study is recorded in such a manner that the identity of the human subjects cannot readily be ascertained, directly or through identifiers linked to the subjects. There is no potential risk to participants and we do not collect any identifiable information from annotators.\\n\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes] We include participant details in Appendix A.2.\"}"}
{"id": "9tVn4f8aJO", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix A: Details on HEMM Benchmark Tasks\\n\\nA.1 Individual dataset details\\n\\nIn this section, we provide the details of the tasks and datasets chosen for the HEMM benchmark: we describe the split used to evaluate the models, any preprocessing applied to the samples, and their access restrictions and licenses.\\n\\n1. **VQA**\\n   - **Dataset**: Consists of samples of an image and a corresponding free-form, open-ended question. To answer the questions, the models need to perform fine-grained recognition of objects and activities. Some of the samples require commonsense reasoning to correctly answer the questions. Most of the samples in the dataset have \u201cyes\u201d or \u201cno\u201d answers.\\n   - **Split**: We evaluate on the real images validation set which comprises a total of 244,302 questions.\\n   - **Prompt used**: You are given an image and a question. Answer the question in a single word.\\n   - **Question**: [Blank]\\n   - **Access restrictions**: The dataset is available to download from [https://visualqa.org/vqa_v1_download.html](https://visualqa.org/vqa_v1_download.html)\\n   - **Licenses**: The images in the dataset come with a CC BY 4.0 DEED license [https://creativecommons.org/licenses/by/4.0/deed.en](https://creativecommons.org/licenses/by/4.0/deed.en)\\n   - **Ethical considerations**: No personally identifiable information or offensive content present in the dataset.\\n\\n2. **NOCAPS**\\n   - **Dataset**: Large-scale image captioning dataset. Training data for this dataset consists of Image-Caption pairs from COCO dataset [71] as well as images and labels from Open Images. Many objects seen in the test set have very few associated captions from the training set making it a robust benchmark for image captioning.\\n   - **Split**: Evaluation is performed on the validation set which consists of 4500 images.\\n   - **Prompt used**: You are given an image. This image might contain a lot of objects. You have to generate a caption for the image but the caption should just be a single sentence. Please do not generate more than one sentences. **Caption**: [Blank]\\n   - **Access restrictions**: The dataset is available to download from [https://nocaps.org/download](https://nocaps.org/download)\\n   - **Licenses**: Images belonging to the dataset are available under CC BY 2.0 [https://creativecommons.org/licenses/by/2.0/deed.en](https://creativecommons.org/licenses/by/2.0/deed.en) license which permits to redistribute the data.\\n   - **Ethical considerations**: No personally identifiable information or offensive content present in the dataset.\\n\\n3. **DECEMER**\\n   - **Dataset**: Hand-drawn molecule image dataset consisting of chemical structure as the images and their SMILES representation as the strings. This SMILES representation stands for \u2018Simplified Molecular Input Line Entry System\u2019, which depicts the three-dimensional structure of the chemical into a string of symbols. In order to solve this task, the model should have an understanding of structure of the chemical and how these structures are depicted in the given format.\\n   - **Split**: The dataset consists of 5088 images over which evaluation has been performed.\\n   - **Prompt used**: Simplified molecular-input line-entry system (SMILES) notation of the given molecule: [Blank]\\n   - **Access restrictions**: The dataset is available to download from [https://zenodo.org/records/7617107](https://zenodo.org/records/7617107)\\n   - **Licenses**: The dataset is available under Creative Commons Attribution 4.0 International License [https://creativecommons.org/licenses/by/4.0/deed.en](https://creativecommons.org/licenses/by/4.0/deed.en), which permits use and sharing of data.\"}"}
{"id": "9tVn4f8aJO", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"EMOTION dataset was introduced in the 'Memotion Analysis' challenge. This task consisted of three different tasks: sentiment classification, humor classification, and the scale of semantic classes. In our evaluation, we focus on the scale of humor class which consists of 'funny', 'very funny', 'not funny', and 'hilarious'. Images in this dataset consists of memes from the internet, which have been annotated by humans for their class labels.\\n\\n**Splits:**\\nA total of 6992 images were used.\\n\\n**Prompt used:**\\nQuestion: Given the Meme and the following caption, Caption:<caption>.\\nHow funny is the meme? Choose from the following comma separated options: funny, very funny, not funny, hilarious.\\n\\n**Access restrictions:**\\nThe dataset is available to download from [https://www.kaggle.com/datasets/williamscott701/memotion-dataset-7k](https://www.kaggle.com/datasets/williamscott701/memotion-dataset-7k)\\n\\n**Licenses:**\\nThe dataset is available under Creative Commons Attribution 4.0 International License [https://creativecommons.org/licenses/by/4.0/deed.en](https://creativecommons.org/licenses/by/4.0/deed.en) which allows sharing of data.\\n\\nEthical considerations:\\nNo personally identifiable information is present in the data.\\nOffensive content is present in the dataset in some meme images.\\n\\nSCIENCE QA consists of multiple choice questions from different science topics consisting of natural science, social science, and language science. The model has to choose an answer from the given set of options for a question, by making sense of lecture and explanation which are optional for a question. Some questions do not consist of an image, however, we evaluate only on questions that have an image in the data point.\\n\\n**Split:**\\nA total of 4.24k questions from the test set.\\n\\n**Prompt used:**\\nYou are given a question and few choices. There is context provided with the image which will help you to understand the image. To answer the question, you have been given lecture notes. You can use these lecture notes, image, and context to answer the question. There are some choices given to you which are comma-separated. You have to select which choice best answers the question. Generate choice as it is from the given choices.\\n\\n**Access restrictions:**\\nThe dataset is available to download from [https://huggingface.co/datasets/derek-thomas/ScienceQA](https://huggingface.co/datasets/derek-thomas/ScienceQA)\\n\\n**Licenses:**\\nDataset is distributed under the CC BY-NC-SA (Attribution-NonCommercial-ShareAlike) [https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en) which allows sharing data.\\n\\nEthical considerations:\\nNo personally identifiable information or offensive content is present in the dataset.\\n\\nSLAKE is a medical visual question-answering dataset that consists of image and question-answer pairs. Annotations have been done by experienced physicians and a medical knowledge base for medical visual question answering. The dataset consists of Yes/No type of questions as well as questions which could be answered with a single word.\\n\\n**Split:**\\nWe use the test set of this dataset which consists of 2070 questions.\\n\\n**Prompt used:**\\nAnswer the question in a single word, Question:<question>\\n\\n**Access restrictions:**\\nThe dataset is available to download from [https://huggingface.co/datasets/BoKelvin/SLAKE](https://huggingface.co/datasets/BoKelvin/SLAKE)\\n\\n**Licenses:**\\nImages under this dataset are available in CC-BY-SA 4.0 license [https://creativecommons.org/licenses/by-sa/4.0/deed.en](https://creativecommons.org/licenses/by-sa/4.0/deed.en) which allows sharing data.\\n\\nEthical considerations:\\nNo personally identifiable information or offensive content is present in the dataset.\"}"}
{"id": "9tVn4f8aJO", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. **Visual Genome** dataset is a visual question-answering dataset that grounds visual concepts to language. Visual Genome provides a formal representation of an image, as relationships between objects in the image are depicted with the help of a scene graph. WordNet [79] is used to canonicalize objects, attributes, and relationships in each image.\\n\\n**Split:**\\nWe use the splits available from https://homes.cs.washington.edu/ranjay/visualgenome/data/dataset/question_answers.json.zip\\n\\n**Prompt used:**\\nYou are given an image and a question. Answer the question in a single word only. Question: <question>\\n\\n**Access restrictions:**\\nThe dataset is available to download from https://homes.cs.washington.edu/ranjay/visualgenome/api.html\\n\\n**Licenses:**\\nThe data is available under Creative Commons Attribution 4.0 International License https://creativecommons.org/licenses/by/4.0/deed.en\\n\\n**Ethical considerations:**\\nNo personally identifiable information or offensive content is present in the dataset.\\n\\n8. **PathVQA** is a visual QA dataset based on pathology images. PathVQA consists of images taken from pathology textbooks and online digital libraries, with question-answer pairs generated from captions using a question generation pipeline. Each pathology image is coupled with a question-answer pair.\\n\\n**Split:**\\nThe test set consists of 6,012 questions.\\n\\n**Prompt used:**\\nYou are given a radiology image and a question. Answer the question in a single word. Question: <question>\\n\\n**Access restrictions:**\\nThe data is available to download from https://github.com/UCSD-AI4H/PathVQA\\n\\n**Licenses:**\\nNo licenses are available for this dataset.\\n\\n**Ethical considerations:**\\nNo personally identifiable information or offensive content is present in the dataset.\\n\\n9. **UCM ERCED LAND USE** is another dataset for land use classification which has 21 classes. Images from the USGS National Map Urban Area Imagery were extracted manually, which involves various urban areas around the country. We include all the possible classes in the prompt so the model can choose from them.\\n\\n**Split:**\\nWe evaluate on the validation split present in https://www.kaggle.com/datasets/apollo2506/landuse-scene-classification.\\n\\n**Prompt used:**\\nImage is given to you. Classify if the image belongs to one of the following classes: mediumresidential, buildings, tenniscourt, denseresidential, baseballdiamond, intersection, harbor, parkinglot, river, overpass, mobilehomepark, runway, forest, beach, freeway, airplane, storagetanks, chaparral, golfcourse, sparseresidential, agricultural. Choose a class from the above classes.\\n\\n**Access restrictions:**\\nThe dataset is available to download from http://weegee.vision.ucmerced.edu/datasets/landuse.html or https://www.kaggle.com/datasets/apollo2506/landuse-scene-classification\\n\\n**Licenses:**\\nNo licenses are available for this dataset.\\n\\n**Ethical considerations:**\\nNo personally identifiable information or offensive content is present in the dataset.\\n\\n10. **ENRICO** is a topic modeling dataset for mobile UI screens. It is an enhanced version of RICO dataset [23] where samples were ranked as a good or bad design example by two human annotators. UI classes in the dataset consist of interfaces such as calculator, camera, chat, news, profile, etc from which the model has to choose for a particular image.\\n\\n**Split:**\\nWe evaluate on the dataset provided in http://userinterfaces.aalto.fi/enrico/resources/screenshots.zip\"}"}
{"id": "9tVn4f8aJO", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"11. MM-IMD is a genre prediction dataset that consists of an image of the poster of the movie along with the plot. Each movie can belong to multiple genres. This dataset was built with MovieLens 20M dataset [34] which consists of movie ratings. Using this, information such as genre, plot, year, and additional metadata were collected. For our evaluation, only poster image and plot is used for genre prediction.\\n\\nSplit: We evaluate on the test split.\\n\\n12. VQARAD is a visual question-answering dataset over radiology images. Images are taken from MedPix* an open radiology database. The dataset is constructed manually by clinical annotators consisting of medical students and senior radiologists. Ground truth answers for the questions are related to counting, color, abnormality, and presence of condition among others.\\n\\nSplit: We evaluate on the test set present in https://huggingface.co/datasets/flaviagiammarino/vqa-rad/viewer/default/test which consists of 451 questions.\\n\\n13. FLICKR 30K is an image captioning dataset collected from Flickr* which extends [38] dataset with similar dataset collection and annotation guidelines.\\n\\nSplit: We evaluate the dataset on the test split.\\n\\n* https://medpix.nlm.nih.gov/home\\n* https://www.flickr.com/\"}"}
{"id": "9tVn4f8aJO", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Picture of\\nAccess restrictions:\\nThe dataset is available to download from https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset\\nLicenses:\\nThe dataset is available under CC0: Public Domain License https://creativecommons.org/publicdomain/zero/1.0/deed.en\\nEthical considerations:\\nNo personally identifiable information or offensive content is present in the dataset.\\n\\n14. FER-2013 is a classic dataset for facial expression recognition, where each image has to be classified into 7 labels. Images for this dataset were obtained from Google images, by searching them using Google Search API. OpenCV was used to get bounding boxes for faces in each of the images.\\n\\nSplit:\\nWe evaluate on the test dataset present in https://www.kaggle.com/datasets/msambare/fer2013\\n\\nPrompt used:\\nGiven the photo of a face, determine the face expression, choose from the following choices: angry, disgust, fear, happy, neutral, sad, surprise. Answer in a single word.\\n\\nAccess restrictions:\\nThe dataset is available to download from https://www.kaggle.com/datasets/msambare/fer2013\\nLicenses:\\nNo license is provided with the dataset.\\nEthical considerations:\\nThis dataset contains human faces collected through Google image search queries but does not contain any identifying information about user identities and backgrounds. No offensive content is present in the dataset.\\n\\n15. NY CARTOON is collected from the weekly New Yorker magazine cartoon captioning contest, where readers are tasked to give a humorous caption for a cartoon image and the funniest captions are selected based on public votes. The dataset is formulated based on taking in the image and caption to predict how funny the pair is based on the normalized number of votes. Given an image and its caption, we ask the model if the caption is humorous or not. Each image has multiple caption choices with votes for the caption being not funny, somewhat funny, funny. We select the funniest caption to have a ground truth answer as 'yes' when prompted for evaluation. The next four funniest captions are selected to have ground truth answers as 'no' when prompted for evaluation.\\n\\nSplit:\\nWe use the data available on https://github.com/nextml/caption-contest-data\\n\\nPrompt used:\\nYou are given a cartoon image and a caption. start the answer with yes if the caption is funny or No if the caption is not funny. Caption:\\n\\nAccess restrictions:\\nThe dataset is available to download from https://github.com/nextml/caption-contest-data\\nLicenses:\\nNo license is provided with the dataset.\\nEthical considerations:\\nNo personally identifiable information or offensive content is present in the dataset.\\n\\n16. OK-VQA is a visual question-answering task that requires outside knowledge and reasoning to answer questions. Images for this dataset are taken from the COCO dataset and MTurk is used for labeling questions. A specific instruction is given to the workers to label questions that require knowledge outside the image. In this dataset, questions are of open-ended type.\\n\\nSplit:\\nWe use the test set available here https://okvqa.allenai.org/download.html\\n\\nPrompt used:\\nYou are given an image and a question. Answer the question in a single word.\\n\\nQuestion:\"}"}
{"id": "9tVn4f8aJO", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"17. MAGIC BRUSH is an instruction-based image editing dataset consisting of manually annotated images consisting of single-turn and multi-turn instruction-guided editing. Images are sampled from MS COCO [71] dataset and are annotated using DALL-E 2* with the help of crowdworkers from Amazon Mechanical Turk (AMT)*. For our evaluation, we follow a single-turn instruction editing.\\n\\nSplit:\\nWe evaluate on the test set available from https://osu-nlp-group.github.io/MagicBrush/\\n\\nPrompt used:\\nEdit the given image based on the provided instruction. Instruction: <instruction>\\n\\nAccess restrictions:\\nThe dataset is available to download from https://osu-nlp-group.github.io/MagicBrush/\\n\\nLicenses:\\nThe dataset comes under CC BY 4.0 license https://creativecommons.org/licenses/by/4.0/deed.en\\n\\nEthical considerations:\\nNo personally identifiable information or offensive content is present in the dataset.\\n\\n18. MEME CAP is a meme captioning dataset, whose images have been taken from the subreddit r/memes*. The captions for these images are generated in a two-round process by human annotators using Amazon Mechanical Turk. For our evaluation process, we provide the model with the image description and title of the meme and ask what the meme is trying to convey.\\n\\nSplit:\\nWe evaluate on the test set from https://github.com/eujhwang/meme-cap/tree/main\\n\\nPrompt used:\\nThis is a meme with the title <title>. The image description is <image_description>. What is the meme poster trying to convey? Answer:\\n\\nAccess restrictions:\\nThe dataset can be downloaded from https://github.com/eujhwang/meme-cap/tree/main\\n\\nLicenses:\\nNo license is available for the dataset.\\n\\nEthical considerations:\\nNo personally identifiable information is present. However, offensive content may be present in the images due to the dataset containing meme data.\\n\\n19. HATEFUL MEMES was a challenge hosted by Meta to classify if a meme image along with its text caption describes hateful intentions. Images were obtained from Getty images* annotated by a third-party annotation platform. Here, an image and text are provided to the model to ask if the image promotes hateful sentiments.\\n\\nSplits:\\nWe use the 'dev' split from https://www.kaggle.com/datasets/parthplc/facebook-hateful-meme-dataset/data\\n\\nPrompt used:\\nYou are given an image. In the image, the text phrase that you will be given and the image are innocuous when considered by themselves. The semantic content of the meme becomes mean only when the text phrase and image are considered together. Text phrase: <text_phrase>\\n\\nYou have to judge if the combination of image and text is hateful or not. Always begin your answer with either 'yes' or 'no' with 'yes' indicating that the meme is hateful and 'no' if it is not hateful. Answer:\\n\\n*https://openai.com/dall-e-2\\n*https://www.mturk.com/\\n*https://www.reddit.com/r/memes/\\n*https://www.gettyimages.in/\"}"}
{"id": "9tVn4f8aJO", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"20. **INATURALIST** is an image classification dataset for 5000 wildlife species of plants and animals. Images and labels are sourced from the iNaturalist website.* We evaluate the models by asking them to identify the species present in the given image. We do not provide it with possible classes as the dataset spans over a set of 5000 species.\\n\\n**Split:**\\nWe evaluate the model on the validation split provided in the 2021 edition of the dataset.\\n\\n**Prompt used:**\\nThe scientific species name of the species present in the image is: \\n\\n---\\n\\n21. **NLVR** consists of image-text pairs for visual reasoning. Images are created by generating objects and their properties randomly. These images are then given to a crowd worker to describe the image in a sentence.\\n\\n**Split:**\\nData is evaluated on the dev split from [https://github.com/lil-lab/nlvr](https://github.com/lil-lab/nlvr).\\n\\n**Prompt used:**\\nGiven this image along with a question about the image, please answer the question with only the word 'true' or 'false'. Question: <question>\\n\\n---\\n\\n22. **NLVR2** extends NLVR to real-world photographs, and captions for these photographs. Images are retrieved using search queries from the ILSVRC2014 ImageNet challenge.* Crowdworkers are used to write the captions for the images. For this dataset, each data point has two images and a sentence that talks about the images. We concatenate the two images so that we pass a single image in the model.\\n\\n**Split:**\\nEvaluation is performed on the dev split from [https://github.com/lil-lab/nlvr/tree/master/nlvr2](https://github.com/lil-lab/nlvr/tree/master/nlvr2).\\n\\n**Prompt used:**\\nYou are given an image and a related text, use the image as context and reply with true or false only. Text: <text>\\n\\n---\\n\\n* [https://www.inaturalist.org/](https://www.inaturalist.org/)\\n* [https://www.image-net.org/challenges/LSVRC/2014/](https://www.image-net.org/challenges/LSVRC/2014/)\"}"}
{"id": "9tVn4f8aJO", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VCR tests commonsense reasoning skills in question answering over images. Still images are extracted from movie clips, and annotations are crowdsourced using Amazon Mechanical Turk where each worker is provided an image along with detailed video captions to collect questions, answers, and rationales for an image.\\n\\nSplit: 'val' split is used from https://visualcommonsense.com/download/\\n\\nPrompt used:\\n\\nQuestion:\\n\\nChoose from the below choices:\\n\\nAccess restrictions:\\nThe dataset is available to download from https://visualcommonsense.com/download/\\n\\nLicenses:\\nThe dataset is provided in license as https://visualcommonsense.com/license/\\n\\nEthical considerations:\\nNo personally identifiable information or offensive content is present in the dataset.\\n\\nWINOGROUND is a dataset for visual linguistic compositional reasoning involving images from Getty Images and annotations given by four expert annotators. The original task consists of matching images and captions for a pair of two images and captions. We transform this task by creating a total of four data points for each pair by pairing each caption, with each image which leads to two correct and two wrong pairs per data point. We then ask the model to see if the caption matches the pair or not.\\n\\nSplit: Test set from https://huggingface.co/datasets/facebook/winoground is used.\\n\\nPrompt used:\\n\\nYou are given an image and a text. Answer yes if the text matches the image and no if the text does not match the image.\\n\\nText:\\n\\nAnswer:\\n\\nAccess restrictions:\\nThe dataset is downloaded from https://huggingface.co/datasets/facebook/winoground\\n\\nLicenses:\\nAuthors of the dataset have Getty image license https://www.gettyimages.in/eula.\\n\\nEthical considerations:\\nNo personally identifiable information or offensive content is present in the dataset.\\n\\nRESISC45 is a land use dataset that involves land scene classification of images over 45 classes. The images for this dataset have been taken from Google Earth by experts in remote sensing image interpretation. We add all 45 classes to the prompt and let the model choose the class from the prompt itself.\\n\\nSplit: We use the dataset from https://www.kaggle.com/datasets/happyyang/nwpu-data-set\\n\\nPrompt used:\\n\\nImage is given to you. Classify if the image belongs to one of the following classes: 'basketball_court', 'overpass', 'ground_track_field', 'church', 'chaparral', 'forest', 'parking_lot', 'golf_course', 'baseball_diamond', 'meadow', 'beach', 'sparse_residential', 'desert', 'terrace', 'palace', 'bridge', 'commercial_area', 'stadium', 'runway', 'lake', 'railway', 'tennis_court', 'ship', 'intersection', 'river', 'freeway', 'airplane', 'industrial_area', 'mountain', 'storage_tank', 'cloud', 'roundabout', 'wetland', 'mobile_home_park', 'island', 'harbor', 'railway_station', 'medium_residential', 'sea_ice', 'thermal_power_station', 'snowberg', 'circular_farmland', 'airport', 'dense_residential', 'rectangular_farmland'. Choose a class from the above classes.\\n\\nAccess restrictions:\\nThe dataset is available to downloaded from https://www.kaggle.com/datasets/happyyang/nwpu-data-set\\n\\nLicenses:\\nNo license is provided with the dataset.\\n\\nEthical considerations:\\nNo personally identifiable information or offensive content is present in the dataset.\\n\\nGQA builds up on Visual Genome scene graph structures for reasoning questions. It consists of real-world reasoning, scene understanding, and compositional question answering. Questions are generated using a robust engine which makes sure that the questions are...\"}"}
{"id": "9tVn4f8aJO", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"27. **OPENPATH** is a dataset created from Twitter and other public sources. Each image has a natural language description, and the dataset is sourced from tweets across 32 hashtag sub-specialty categories in pathology.\\n\\n**Split:** We use the test split for evaluation.\\n\\n**Prompt used:** Choose from the below choices, Given image is a hematoxylin and eosin image of: cancer-associated stroma, adipose tissue, debris, lymphocytes, mucus, background, normal colon mucosa, colorectal adenocarcinoma epithelium, smooth muscle\\n\\n**Access restrictions:** The dataset is available to download from huggingface datasets [link](https://huggingface.co/datasets/akshayg08/OpenPath).\\n\\n**Licenses:** The dataset is available under CC BY-NC 4.0 license. [link](https://creativecommons.org/licenses/by-nc/4.0/).\\n\\n**Ethical considerations:** No personally identifiable information or offensive content is present in the dataset.\\n\\n28. **IRFL** is an image-text dataset for figurative language. The dataset consists of three broad categories: idioms, similes, and metaphors. Metaphors and similes were collected from online lists whereas idioms were collected from MAGPIE corpus [33]. Since the MAGPIE corpus did not contain definitions for idioms, definitions were crawled from online dictionaries to search for figurative images. Google images were used for searching the images for idioms using these definitions. For similes and metaphors, annotators were used for definitions, and images were searched on the internet. For our evaluation, we use simile categorization. For each data point, one simile and four images are given. We modify this task to evaluate one image at a time, so a pair of an image and similes are passed to the model to see if they match or not.\\n\\n**Split:** We use the Simile understanding task for evaluation.\\n\\n**Prompt used:** You are given a simile and a picture along with the simile. You have to say if the simile matches the given picture. Answer the following question in a single word with a yes or no. Simile: <simile>\\n\\n**Access restrictions:** Dataset is available for download from [link](https://github.com/irfl-dataset/IRFL).\\n\\n**Licenses:** No license is provided with the dataset.\\n\\n**Ethical considerations:** No personally identifiable information or offensive content is present in the dataset.\\n\\n29. **SCREEN2WORDS** is a mobile UI summarization dataset consisting of images from Rico-SCA [64] dataset. A total of 85 annotators were used to describe the image.\\n\\n**Split:** We use the test split from [link](https://github.com/google-research-datasets/screen2words/tree/main).\\n\\n**Prompt used:** You are given a phone UI screen. Describe the screen in one sentence.\"}"}
{"id": "9tVn4f8aJO", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Multimodal skills are the basic building blocks central to solving problems, spanning information integrated across modalities at different granularities, different ways modalities might interact to create new information, reasoning, and external knowledge.\\n\\nFigure 7: Multimodal information flow studies how the content changes across the two modalities for the task, such as through cross-modal translation, editing, querying, and fusion.\\n\\nLicenses:\\nNo license is provided with the dataset.\\n\\nEthical considerations:\\nNo personally identifiable information or offensive content is present in the dataset.\\n\\nLocalized Narratives (COCO subset) (LNCOCO) was built on images from COCO[71], Flickr30k[119], and ADE20K[127] datasets by annotating these datasets with localized information. We use this dataset for the task of image generation.\\n\\nSplit:\\nWe use the COCO subset from the Localized Narratives Dataset [87] containing 8,573 samples. The ground truth images are used from the MSCOCO (17) validation set.\\n\\nPrompt used:\\nGenerate an Image based on the provided caption. Caption:\\n\\nAccess restrictions:\\nThe dataset is available to download from https://google.github.io/localized-narratives/\\n\\nLicenses:\\nThe dataset is available under CC BY-NC 4.0 license. https://creativecommons.org/licenses/by-nc/4.0/\\n\\nEthical considerations:\\nNo personally identifiable information or offensive content is present in the dataset.\\n\\nA.2 Dataset Categorization\\nFor categorizing the datasets, we follow a three-stage approach with the majority of the categorizations done using human annotators versed in machine learning, followed by using multimodal large language models to alleviate any annotator disagreement issues, and performing a final check by the authors of this work who are experts in multimodal machine learning.\\n\\nA.2.1 Categorization stage 1: Human annotation of dimensions\\nIn the first stage of the annotation process, we sample five data points from each dataset, for a total of 145 data points spread out across 10 sets. Each set was evaluated by two annotators each. Annotators for this task were from the machine learning research community. For each data point, we provide the image, prompt, and the ground truth answer followed by five questions which the annotator has to answer. These questions span across various dimensions which we consider for datasets, which are the following: 1) Does answering this question require you to use external knowledge? [Options: Yes, No] 2) Does answering this question require you to use reasoning? [Options: Less Reasoning, Neutral Reasoning, More Reasoning] 3) Which information flow does the data use? [Options: Querying, Translation, Fusion, Editing] 4) Does the data use fine-grained interactions? [Options: Yes, No] 5) What type of interactions does the data have? [Options: Redundancy, Synergy, Uniqueness]. We calculate inter-annotator agreement for the annotators and present them in Table 6.\"}"}
{"id": "9tVn4f8aJO", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: Inter-Annotator agreement scores for stage 1 annotation.\\n\\n| Set | Knowledge | Info. Flow | Interactions | Fine-grained Reasoning |\\n|-----|------------|------------|--------------|------------------------|\\n| 1   | 0.242      | 0.407      | 0.156        | 0.375                  |\\n| 2   | 0.364      | 0.115      | 0.102        | 0.286                  |\\n| 3   | 0.250      | 0.640      | 0.019        | 0.571                  |\\n| 4   | 0.708      | 0.299      | 0.286        | -0.024                 |\\n| 5   | 0.500      | 0.190      | 0.166        | 0.143                  |\\n| 6   | 0.192      | 0.045      | -0.037       | 0.017                  |\\n| 7   | 0.473      | 0.171      | 0.204        | -0.153                 |\\n| 8   | 0.439      | 0.469      | 0.067        | -0.365                 |\\n| 9   | 0.032      | 0.419      | 0.464        | -0.029                 |\\n| 10  | 0.472      | 0.417      | 0.097        | 0.286                  |\\n\\nTable 7: Categorization after aggregating human annotations.\\n\\n| Dataset | Knowledge | Reasoning | Info. Flow | Fine-grained Interactions |\\n|---------|-----------|-----------|------------|---------------------------|\\n| N_LVR  | No Less   | Querying  | No Uniqueness |                           |\\n| N_LVR  | No Less   | Querying  | Yes Uniqueness |                           |\\n| NY     | C_ARTOON | Yes More  | Fusion | No Synergy                 |\\n| MM_IMD | B         | No Less   | Fusion | No Synergy                 |\\n| M_EMOTION | No Less   | Fusion | No Redundancy |                           |\\n| M_EMEE | C_AP      | No More   | Fusion | No Synergy                 |\\n| MAGIC  | B_RUSH    | No Less   | Editing | Yes Synergy                 |\\n| IRFL   | No Less   | Fusion | No Synergy |                           |\\n| HATEFUL | M_EMES   | Yes Less  | Fusion | No Synergy                 |\\n| INATURALIST | Yes Less | Querying | No Uniqueness |                           |\\n| FLICKR | 30K       | No Less   | Translation | No Uniqueness             |\\n| GQA    | No Less   | Querying  | Yes Redundancy |                           |\\n| NRICO  | Yes Less  | Querying  | No Uniqueness |                           |\\n| FER-2013 | No Less | Querying  | No Uniqueness |                           |\\n| D_ECMER | Yes Less  | Translation | Yes Uniqueness |                           |\\n| WINGROUND | No Less | Querying  | Yes Redundancy |                           |\\n| VQA    | Yes More  | Querying  | Yes Uniqueness |                           |\\n| VISUALG | Yes Less  | Querying  | Yes Uniqueness |                           |\\n| VCR    | Yes More  | Fusion | Yes Redundancy |                           |\\n| UCMERICED LAND USE | Yes Less | Querying | No Uniqueness |                           |\\n| LAKE   | Yes Less  | Querying  | Yes Uniqueness |                           |\\n| SCREEN | 2WORDS    | No Less   | Translation | No Uniqueness             |\\n| SCIENCE | QA       | Yes Less  | Querying | Yes Synergy                 |\\n| RESISC | 45        | Yes Less  | Querying | No Uniqueness             |\\n| O_PEN  | Yes Less  | Querying  | No Uniqueness |                           |\\n| PTVQA  | Yes Less  | Querying  | Yes Uniqueness |                           |\\n| NOCAPS | No Less   | Translation | Yes Uniqueness |                           |\\n| LNCOCO | Yes Less  | Querying  | Yes Uniqueness |                           |\\n\\nAs per the annotations, we aggregate the annotations for each dataset across each dimension and calculate the maximum occurrence of annotation across all dimensions to categorize the datasets presented in Table 7. We also consider \u2018Neutral Reasoning\u2019 and \u2018Less Reasoning\u2019 to be the same category and label them as \u2018Less Reasoning\u2019 before aggregating over the annotations. However, we see that the inter-annotator scores have low agreement, and some annotations go against the definitions above in the section 2. Hence, we carry out an additional round of the annotation process using GPT-4V, and explain the process below.\\n\\nA.2.2 Categorization stage 2: Automatic annotation with human verification\\n\\nAfter the first stage was done, we found that most of the annotations were reliable but there were some cases where annotators misunderstood the definitions and tasks which led to low agreement values. For the second stage of the annotation process, we query GPT-4V for categorization of datapoints into dimensions to supplement the human annotations we obtained in the first stage. For each dataset, we consider three samples from each dataset for a total of 87 data points for categorization spread out across six sets. For each data point, we ask the model the same questions as asked to the human annotators above and obtain the categorization across the dimensions. For some questions, the model...\"}"}
{"id": "9tVn4f8aJO", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 8: Categorization after aggregating GPT-4V annotations. Cases where '-' is present are due to the model refusing to answer the question citing enough information is not provided, so we do not consider the output for categorization. Aggregation is done similarly to stage 1 of the annotation process and the categories are provided in Table 8. For each set, we ask two annotators to label the annotation by GPT-4V as either correct or wrong, depending on the categorization provided by the model. The inter-annotator agreement scores are provided in Table 9. We see improvements over the previous annotation process in some dimensions and datasets, however, cases where annotations do not match the definition persist. Also, GPT-4V does not give output for a few cases due to which aggregation is not possible. Hence, we carry out the third stage of the annotation process to get a more refined categorization.\\n\\n| Dataset          | Knowledge | Reasoning | Info. Flow | Fine-grained | Interactions |\\n|------------------|-----------|-----------|------------|--------------|--------------|\\n| NVR              | No        | Less      | Fusion     | Yes          | Synergy      |\\n| No               | No        | More      | Querying   | Yes          | -            |\\n| CANTON           | Yes       | Less      | Fusion     | No           | Synergy      |\\n| MM-IMD           | No        | Less      | Fusion     | No           | Synergy      |\\n| EMOTION          | Yes       | More      | Editing    | No           | Synergy      |\\n| MM-EME           | Yes       | More      | Fusion     | No           | Synergy      |\\n| MM-AGIC          | Yes       | More      | Translation| No           | Synergy      |\\n| MM-EME           | Yes       | More      | Fusion     | No           | Synergy      |\\n| MM-EME           | Yes       | More      | Querying   | No           | -            |\\n| GQA              | No        | Less      | Querying   | Yes          | Uniqueness   |\\n| ENSICO           | No        | More      | -          | No           | Synergy      |\\n| FER-2013         | No        | Less      | Querying   | -            | -            |\\n| DERMIMER         | Yes       | More      | Translation| No           | Uniqueness   |\\n| WINOGROUND       | No        | Less      | Fusion     | No           | Redundancy   |\\n| VQARD            | Yes       | More      | Querying   | No           | Uniqueness   |\\n| VQA              | No        | Less      | Querying   | Yes          | Synergy      |\\n| VISUALGENOME     | Yes       | More      | Querying   | No           | -            |\\n| VCR              | Yes       | More      | Fusion     | No           | Redundancy   |\\n| UCM-ERCED LAND USE| Yes     | More      | Querying   | No           | Synergy      |\\n| LAKE             | Yes       | More      | Querying   | Yes          | Uniqueness   |\\n| SCIENCE          | Yes       | More      | Fusion     | No           | -            |\\n| RESISC           | No        | More      | Querying   | -            | -            |\\n| PATH             | Yes       | More      | Querying   | -            | -            |\\n| PATHVQA          | Yes       | Less      | Fusion     | Yes          | -            |\\n| NOAPS            | Yes       | Less      | Translation| No           | Uniqueness   |\\n| OK-VQA           | Yes       | Less      | Querying   | Yes          | Synergy      |\\n| LNCOCO           | Yes       | Less      | Translation| Yes          | Uniqueness   |\\n\\nTable 9: Inter-annotator agreement scores for stage 2 annotations.\\n\\n| Set | Knowledge | Reasoning | Info. Flow | Interactions | Fine-grained |\\n|-----|-----------|-----------|------------|--------------|--------------|\\n| 1   | 0.667     | 0.420     | 0.868      | 0.705        | 0.000        |\\n| 2   | 0.631     | 0.797     | 0.363      | 1.000        | 0.450        |\\n| 3   | -0.097    | 1.000     | 0.732      | 0.732        | 0.444        |\\n| 4   | 0.588     | 0.658     | 0.851      | 0.571        | 0.417        |\\n| 5   | 0.444     | 1.000     | 0.842      | 0.722        | 0.587        |\\n| 6   | 0.317     | 1.000     | 0.222      | 0.837        | 0.000        |\\n\\nA.2.3 Categorization stage 3: Final check by experts\\n\\nIn the third stage of the annotation process, the authors of the project manually go through the annotations from both stages to check for errors and obtain the final categorization of datasets. We present the categorization in Table 10 with the source for each categorization in the table. (1) indicates that the category has been agreed upon both by human annotators and GPT-4V, (2) indicates that GPT-4V better categorizes the dataset for the dimension and hence the annotation from GPT-4V has been chosen, (3) indicates that human annotations better categorize the dataset for the dimension, (4) indicates that GPT-4V better categorizes the dataset for the dimension and hence the annotation from GPT-4V has been chosen, (5) indicates that human annotations better categorize the dataset for the dimension.\"}"}
{"id": "9tVn4f8aJO", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 10: Final dataset categorization.\\n\\n| Dataset | Knowledge | Reasoning | Info. Flow | Fine-grained Interactions |\\n|---------|-----------|-----------|------------|---------------------------|\\n| LVR     | No (1)    | Less (1)  | Querying (3) | No (4) Redundancy (4)  |\\n| LVR     | No (1)    | Less (3)  | Querying (1) | Yes (1) Redundancy (4)  |\\n| CARTOON | Yes (1)   | More (3)  | Fusion (1)  | No (1) Synergy (1)      |\\n| MM-ID   | No (1)    | Less (1)  | Fusion (1)  | No (1) Synergy (1)      |\\n| EMOTION | Yes (1)   | More (4)  | Fusion (1)  | No (1) Synergy (2)      |\\n| EMCEAP  | Yes (2)   | More (1)  | Fusion (1)  | No (1) Synergy (3)      |\\n| MAGIC   | No (3)    | Less (1)  | Editing (1) | Yes (3) Synergy (1)     |\\n| BRUSH   | No (3)    | More (4)  | Fusion (1)  | No (1) Synergy (3)      |\\n| HATFUL  | Yes (1)   | More (2)  | Fusion (1)  | No (1) Synergy (1)      |\\n| NATURAL | Yes (1)   | Less (1)  | Querying (1) | Yes (4) Uniqueness (3) |\\n| FLICKR30K | No (1)   | Less (1)  | Translation (1) | No (3) Uniqueness (3) |\\n| GQA     | No (1)    | Less (1)  | Querying (1) | Yes (1) Redundancy (3)  |\\n| ERTICO  | No (2)    | Less (1)  | Querying (3) | No (1) Uniqueness (3)  |\\n| FER-2013| No (1)    | Less (1)  | Querying (1) | No (3) Uniqueness (3)  |\\n| ECIMER  | Yes (1)   | More (2)  | Translation (1) | No (2) Uniqueness (1)  |\\n| INGROUND| No (1)    | Less (1)  | Querying (3) | Yes (4) Redundancy (1)  |\\n| VQARAD  | Yes (1)   | More (4)  | Querying (1) | Yes (4) Redundancy (4)  |\\n| VQA     | No (1)    | Less (1)  | Querying (1) | Yes (1) Redundancy (4)  |\\n| VISUALGENOME | No (1) | Less (1)  | Querying (1) | Yes (1) Redundancy (4)  |\\n| VCR     | No (4)    | Less (2)  | Fusion (1)  | Yes (3) Redundancy (1)  |\\n| UCM     | No (4)    | Less (1)  | Querying (1) | No (1) Uniqueness (3)  |\\n| LAKE    | Yes (1)   | More (4)  | Querying (1) | Yes (4) Redundancy (4)  |\\n| SCREEN 2W | No (1)   | Less (1)  | Translation (3) | No (1) Uniqueness (3) |\\n| SCIENCEQA | Yes (3)  | Less (1)  | Fusion (4)  | No (2) Synergy (1)      |\\n| RESISC45| No (2)    | Less (1)  | Querying (1) | No (3) Uniqueness (1)  |\\n| PENATH  | Yes (1)   | More (4)  | Querying (1) | Yes (4) Redundancy (4)  |\\n| VATHQA  | Yes (1)   | Less (1)  | Querying (3) | Yes (4) Redundancy (4)  |\\n| GCAPS   | No (3)    | Less (1)  | Translation (1) | No (2) Uniqueness (1) |\\n| OK-VQA  | Yes (1)   | Less (1)  | Querying (1) | Yes (1) Redundancy (4)  |\\n| LNCOCO  | Yes (1)   | Less (1)  | Translation (1) | Yes (1) Uniqueness (1) |\\n\\n(4) indicates that authors of this work have categorized the dataset for the dimension. As we can see from Table 10, the majority of categories are agreed upon both by human annotators and GPT-4V, indicating reliability. There are only a few with (4), indicating that authors had to provide the final categorization due to dimensions that were hard to understand by non-experts in multimodal learning and by GPT-4V.\\n\\nA.2.4 Details on annotation and participants\\n\\nThe annotations in stages 1 (human annotation) and 2 (automatic inference with human verification) are all university students with some knowledge of machine learning. There were 10 sets of annotations each evaluated by two annotators for a total of 20 annotators. All participation in user studies was voluntary and done for pay at a level consistent with research participation at our university (15 dollars an hour). The annotations in stage 3 (final check) are done by 5 experts in the multimodal machine learning community for a final verification in case of misunderstandings in the first two stages.\\n\\nA.3 Modeling categorizations and details\\n\\nWe also evaluate the performance of the models based on various modeling decisions. To achieve this, we categorize the models into various classes based on the following properties:\\n\\n1. Interleaved modality training:\\n   In the multi-modal setting, models are broadly trained/fine-tuned either by separately processing individual modalities using modality-specific encoders followed by fusion, or by interleaving the raw modalities first and then processing the interleaved input together.\\n\\n2. Instruction Tuning:\\n   Generative multimodal models can be trained/fine-tuned using objectives such as image-text matching, image-grounded text generation [62], etc., to generate relevant outputs. However, recently such generative models are also instruction-tuned in order to generate outputs that resemble human responses. Therefore, we also categorize the models based on whether instruction tuning is employed or not.\"}"}
