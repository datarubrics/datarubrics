{"id": "dUFf0pgkC7", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This paper introduces HHD-Ethiopic, a new OCR dataset for historical handwritten Ethiopic script, characterized by a unique syllabic writing system, low resource availability, and complex orthographic diacritics. The dataset consists of roughly 80,000 annotated text-line images from 1700 pages of 18th to 20th century documents, including a training set with text-line images from the 19th to 20th century and two test sets. One is distributed similarly to the training set with nearly 6,000 text-line images, and the other contains only images from the 18th century manuscripts, with around 16,000 images. The former test set allows us to check baseline performance in the classical IID setting (Independently and Identically Distributed), while the latter addresses a more realistic setting in which the test set is drawn from a different distribution than the training set (Out-Of-Distribution or OOD). Multiple annotators labeled all text-line images for the HHD-Ethiopic dataset, and an expert supervisor double-checked them. We assessed human-level recognition performance and compared it with state-of-the-art (SOTA) OCR models using the Character Error Rate (CER) and Normalized Edit Distance (NED) metrics. Our results show that the model performed comparably to human-level recognition on the 18th century test set and outperformed humans on the IID test set. However, the unique challenges posed by the Ethiopic script, such as detecting complex diacritics, still present difficulties for the models. Our baseline evaluation and HHD-Ethiopic dataset will stimulate further research on tailored OCR techniques for the Ethiopic script. The HHD-Ethiopic dataset and the code are publicly available at https://github.com/bdu-birhanu/HHD-Ethiopic.\"}"}
{"id": "dUFf0pgkC7", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Sample historical handwritten document image from HHD-Ethiopic dataset: two-column 19th-century manuscript (left), one-column 20th-century manuscript (middle), two-column 18th-century manuscript (right).\\n\\nFigure 2: The first two top rows of Fidel-Gebeta (the row-column matrix structure of Ethiopic characters): The first column shows the consonants, while the following columns (1-6) illustrate syllabic variations (obtained by adding diacritics or modifying parts of the consonant, circled in color). These modifications result in complex and distinct characters having similar shape, which makes them challenging for machine learning models (see Appendix B).\\n\\nDocuments, particularly those written in Ethiopic scripts, due to a shortage of suitable datasets for training machine learning models and the unique complexities of orthography [8, 34]. Typical historical handwritten Ethiopic manuscripts from different centuries are displayed in Figure 1. The Ethiopic script, also known as the Abugida, Ge'ez, or Amharic script, is one of the oldest writing systems in the world, with a history dating back to the 4th century AD [22]. It is used to write several languages in Ethiopia and Eritrea, including Amharic, Tigrinya, and Ge'ez. The script has a unique syllabic writing system and is written from left to right. It contains about 317 graphemes, including 231 basic characters arranged in a 33 consonants by 7 vowels matrix, one special (1x7) character, 50 labialized characters, 9 punctuation marks, and 20 numerals. The script's complexity is increased by the presence of diacritical marks, which are used to indicate vowel length, tone, and other phonological features [2, 32, 30] (see Appendix B). The first two consonant Ethiopic characters and their corresponding vowels formation is shown in Figure 2.\\n\\nThe Ethiopian National Archive and Library Agency (ENALA) has collected numerous non-transcribed historical Ethiopic manuscripts from various sources, covering different periods starting from the 12th century [49]. These documents are manually cataloged and some are digitized and stored as scanned copies. They contain valuable information about Ethiopian cultural heritage and have been registered in UNESCO's Memory of the World program [9, 35]. The manuscripts are mainly written in Ge'ez and Amharic languages, which share the same syllabic writing system.\\n\\nTo address the scarcity of suitable datasets for machine learning tasks in historical handwritten Ethiopic text-image recognition, we aim to prepare a new dataset that can advance research on the Ethiopic script and facilitate access to knowledge from these historical documents by various communities, including paleographers, historians, librarians, and researchers.\\n\\nThe main contributions of this paper are stated as follows.\\n\\n\u2022 We introduce the first sizable dataset for historical handwritten Ethiopic text-image recognition, named HHD-Ethiopic.\"}"}
{"id": "dUFf0pgkC7", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We evaluate an independent human-level performance from multiple participants in historical handwritten text-image recognition, providing a baseline for comparison with machine learning models.\\n\\nWe evaluate several state-of-the-art Transformer, attention, and Connectionist Temporal Classification (CTC)-based methods.\\n\\nWe compare the recognition performance of the machine learning model with human-level performance in predicting the sequence of Ethiopic characters in text-line images, supported by examples.\\n\\nThe rest of the paper is organized as follows: Section 2 reviews the relevant methods and related works. Settings of human-level recognition performance and OCR models are described in section 3. Section 4 presents results obtained from the experiment and comparative analysis between the model and human-level recognition performance. Finally, in Section 5, we conclude and suggest directions for future works.\\n\\n2 Related work\\n\\nIn this section, we briefly review related work in optical character recognition and highlight challenges we are facing in OCR of historical Ethiopic manuscripts.\\n\\n2.1 Optical character recognition\\n\\nMachine Learning techniques have been extensively applied to the problem of optical character recognition, see [11, 48, 51, 12, 50] for a review. This has been facilitated by the public availability of a multitude of datasets for various document image analysis tasks, in a variety of scripts: Among these, we can mention IAM-HistDB[19], DIDA[24], IMPACT[37], GRPOLY-DB[20], DIV A-HisDB[44], ICDAR-2017 Dataset[39], SCUT-CAB[13] and HJDataset[40] as examples of historical and handwritten datasets. There are other datasets that can be used for printed and scene text-image recognition, including the ADOCR database[8], OmniPrint datasets[45], UHTelPCC[23], COCO dataset[47], and TextCaps[43], in addition to the historical and handwritten datasets mentioned previously.\\n\\nNowadays, segmentation-free OCR approaches[3, 36, 51] based on CTC[7, 15, 11, 31, 48, 41] attention mechanisms[27, 38, 42, 50], and transformer-based models[5, 18, 26, 33] have become a popular choice among researchers and are widely used for text-image recognition (in both well-known and low-resourced scripts), as opposed to the traditional segmentation-based OCR approaches.\\n\\nResearchers have reported remarkable recognition performance using these approaches for a wide range of scripts, encompassing everything from historical to modern[5, 28], and from handwritten to machine-printed[9]. Consequently, several OCR applications have been developed that perform exceptionally well for high-resource and well-known scripts. However, many of these applications have not been as assessed for their ability to recognize text in historical handwritten manuscripts and missing these potential benefits, especially in the case of Ethiopic manuscripts. In the following sections, we briefly discuss the features of historical Ethiopic manuscripts and the challenges of text-image recognition in ancient Ethiopic manuscripts.\\n\\n2.2 Features of historical Ethiopic manuscripts\\n\\nThere are various collections of ancient Ethiopic manuscripts in museums and libraries in Ethiopia and other countries. For example, the ENALA collection contains 859 manuscripts, the Institutes of Ethiopian Studies has 1500 manuscripts[35, 1], and the collections in Rome (Biblioteca Apostolica Vaticana), Paris (Biblioth\u00e8que nationale de France), and London (British Library) contain a total of 2700 manuscripts[35]. These manuscripts were typically written on a material called Brana, which could vary in quality depending on the intended purpose or function of the book[29, 35]. Black and red were the most commonly used inks, with black reserved for the main text and red reserved...\"}"}
{"id": "dUFf0pgkC7", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Examples of historical Ethiopic Manuscripts: (a) Two-column writing in liturgical books with decorated heading, (b) Two-column writing in liturgical books without decoration, (c) Three-column writing in the Synaxarion, (d) One column for Psalms and prayer books. The Ethiopic script is written and read in the same direction as English, from left to right and top to bottom.\\n\\nThe manuscript layout can also vary and include formats such as three columns in the Synaxarion, one column for Psalms and prayer books, and two columns in liturgical books [6, 35]. The materials used for writing, including the pen and ink and the writing style, can also vary depending on the time period and region in which the manuscripts were produced. The use of punctuation marks is also very irregular (see Appendix B, Figure 10 for an extended discussion).\\n\\nHistorical documents, such as Ethiopic manuscripts, often have artifacts like color bleed-through, paper degradation, and stains, making them more challenging to work with than contemporary, well-printed documents [17]. Some major challenges in recognizing historical Ethiopic manuscripts include: (i) the complexity of character sets and writing system, which consists of over 317 distinct but similar-looking indigenous characters (see Figure 2 and details are given in Appendix B); (ii) variations in writing styles, including handwriting and punctuation, which can vary greatly among individuals and over time, affecting model accuracy; and (iii) a shortage of labeled data for training machine learning algorithms for Ethiopic script recognition.\\n\\nTherefore, in this paper, we aim to tackle the challenges in recognizing the Ethiopic script by creating a new dataset called HHD-Ethiopic which is composed of manuscripts dating from the 18th to 20th centuries. We also conduct experimental evaluations to showcase the usefulness of the HHD-Ethiopic dataset for historical handwritten Ethiopic script recognition and compare the performance of both human and machine predictions.\\n\\n3 Dataset and baseline methods\\n\\nIn this section, we provide an overview of our work, focusing on two key aspects: the detailed characteristics of our new dataset (subsection 3.1) and the benchmark methods employed. Our dataset, comprehensively outlined, includes essential details such as size, composition, data collection, and annotation process. It serves as a valuable resource for evaluating historical handwritten Ethiopic OCR. Additionally, we present the benchmark methods, including human-level recognition performance and baseline OCR models (subsection 3.2).\\n\\n3.1 HHD-Ethiopic dataset\\n\\nThe HHD-Ethiopic dataset consists of 79,684 text-line images with their corresponding ground-truth texts that are extracted from 1,746 pages of Ethiopic manuscripts dating from 18th to 20th centuries. The dataset includes 306 unique characters (including one blank token), with the shortest text comprising two characters and the longest containing 46 characters.\"}"}
{"id": "dUFf0pgkC7", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Summary of the training and test text-line images\\n\\n| Type of data | Pub date of manuscript | #text-line images | Remark |\\n|--------------|------------------------|-------------------|--------|\\n| Training set | 90% of (A+B+C)         | 57,374 real       |        |\\n| Test set I   | 10% of (A+B+C)         | 6,375 real        |        |\\n| Test set II  | 100% of (D)            | 15,935 real       |        |\\n\\nA = Unknown pub date, B = 20th century, C = 19th century, D = 18th century manuscript.\\n\\nFigure 4: Sample historical handwritten Ethiopic text-line images from HHD-Ethiopic\\n\\nThe training set includes text-line images from recent manuscripts, primarily from the 19th and 20th centuries. We created two test sets: the first one consists of 6,375 images that are randomly selected using a sklearn train/test split to cols 5, from a distribution similar to the training set, specifically from 19th and 20th century books. The second one, with 15,935 images, is drawn from a different distribution and made of 18th century manuscripts (see Table 1 for the splitting processes and size of the each set). The goal of the first test set is to evaluate the baseline performance in the IID (Independently and Identically Distributed) setting, while the second test aims to assess the model's performance in a more realistic scenario, where the test set is OOD (Out-Of-Distribution) and different from the training set.\\n\\nTo perform preprocessing and layout analysis tasks, such as text-line segmentation, we utilized the OCRopus 6 framework. For text-line image annotation, we developed a simple tool with a graphical user interface, which displays an image of a text-line and provides a text box for typing and editing the corresponding ground-truth text. Additionally, we employed this tool to collect predicted text during the evaluation of human-level performance.\\n\\nA team of 14 people participated in creating the HHD-Ethiopic datasets, with 12 individuals tasked with labeling and the remaining two individuals responsible for reviewing and ensuring the accuracy of the alignment between the ground-truth text and text-line images, making any necessary corrections as needed. To ensure the accuracy of the annotations, participants were provided with access to reference materials for the text-lines, and all of them were familiar with the characters in the Ethiopic script. Table 1 and Figure 4 provide a summary of the dataset and show sample text-line images of the HHD-Ethiopic dataset, respectively (see Appendix C.3 for an extended discussion).\\n\\n5 https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\\n6 https://github.com/ocropus/ocropy\"}"}
{"id": "dUFf0pgkC7", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 Settings for human-level performance and baseline models\\n\\nTo establish a baseline for evaluating the performance of models on the HHD-Ethiopic OCR dataset, we propose two approaches: (i) **human-level performance** and (ii) **sequence-to-sequence models**.\\n\\nThe human-level performance serves as a benchmark for evaluating and comparing the recognition performance of machine learning models on historical handwritten Ethiopic scripts and provides insights for error analysis.\\n\\nTo calculate the human-level recognition performance, independent annotators were hired and divided into two groups. It is important to note that these individuals are different from those mentioned in Section 3.1. The first group transcribed text-line images from the first test set, which consisted of 6375 randomly selected images from the training set. The second group transcribed the second test set of 15935 images from the 18th century. Each text-line image was predicted by multiple people (i.e., nine for Test-set-I and four for Test-set-II). The annotators were already familiar with the Ethiopic script, and they were explicitly instructed to carry out the task without using any references. The predicted texts by each annotator, along with comprehensive details of the data collection and annotation process, is documented as metadata for future reference.\\n\\nThe second reference point involves various state-of-the-art OCR models, which includes CTC, attention and transformer-based methods. The CTC-based models employ a combination of Convolutional Neural Networks (CNN) and Bidiirectional Long Short-Term Memory (Bi-LSTM) as an encoder and CTC as a decoder, and are trained end-to-end with and without an attention mechanism (see Appendix C for an extended discussion).\\n\\nIn addition, for the attention-based baseline, we employ ASTER [42], and for the transformer-based baselines, we utilize the ABINet [18] and TrOCR [26]. Moreover, we use Bayesian optimization (see e.g., [4, 16] for a review) to optimize the hyperparameters of the CTC-based models. Optimizing hyperparameters involves finding an optimal setting for the model hyperparameters that could result in the best generalization performance, without using test data. Considering the trade-offs between model performance and computational cost, we use a small subset of the training set to optimize the hyperparameters of models (see, e.g., [10] for a review), and then train the model on the full training set using the optimal hyperparameter settings.\\n\\nWe used the Character Error Rate (CER) [7, 21] and Normalized Edit Distance (NED) [14] as our evaluation metric for both the OCR models and human-level recognitions (see appendix C, equation 3 and 4 for extended discussion).\\n\\n4 Experimental results\\n\\nOur objective is to perform a fair comparison between human and machine performance on historical handwritten Ethiopic scripts recognition task. This comparison is intended to showcase the utility and value of our new HHD-Ethiopic dataset, evaluate human recognition capabilities, and highlight any advancements made by baseline OCR methods.\\n\\n4.1 Human-level performance\\n\\nAs previously discussed in Section 3.1, the ground-truth text was annotated by multiple people and double-checked by supervisors who were familiar with Ethiopic scripts. For this phase, new annotators who were also familiar with Ethiopic characters were selected and instructed not to use any reference materials. The reviewer of both the training and test sets was permitted to use reference materials. However, in contrast to the training set, the test sets were reviewed by an expert in historical Ethiopic documents.\\n\\nTo measure the human-level recognition performance, multiple annotators were asked to predict the text in the images and then their character recognition rates were recorded. The best annotator on Test-set-I scored a CER of 25.39% and an NED of 23.78% on Test-set-I, and a CER of 33.20% and an NED of 30.73% on Test-set-II. In contrast, the average human-level recognition performance was a CER of 30.46% and an NED of 26.32% on Test-set-I, and a CER of 35.63% and an NED of 38.59% on Test-set-II. We used the best human-level recognition performance as a baseline for comparison.\"}"}
{"id": "dUFf0pgkC7", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: The human-level recognition performance in CER and Normalized Edit Distance (NED) for different test sets.\\n\\n| Type-of-test | Data | Year-of-Pub | Annotator-ID | CER  | NED  |\\n|--------------|------|-------------|--------------|------|------|\\n| IID          |      | 19th & 20th| Annot-I      | 29.02| 27.67|\\n|              |      |             | Annot-II     | 27.87| 25.89|\\n|              |      |             | Annot-III    | 29.93| 28.16|\\n|              |      |             | Annot-IV     | 29.16| 27.80|\\n| OOD          |      | 18th        | Annot-V      | 26.56| 24.56|\\n|              |      |             | Annot-VI     | 25.39| 23.78|\\n|              |      |             | Annot-VII    | 29.26| 28.08|\\n|              |      |             | Annot-VIII   | 25.95| 24.78|\\n|              |      |             | Annot-IX     | 51.03| 25.46|\\n|              |      |             | Annot-X      | 33.20| 30.77|\\n|              |      |             | Annot-XI     | 54.33| 52.20|\\n|              |      | 18th        | Annot-XIII   | 39.96| 35.90|\\n|              |      |             | Annot-XIV    | 45.06| 39.89|\\n\\nComparison with SOTA machine learning models for performance throughout this paper. Table 2 shows the human-level recognition performance on both test sets, based on assessments from nine annotators on Test-set-I and four on Test-set-II.\\n\\n4.2 Baseline OCR models\\n\\nThis section presents the results obtained from the experimental setups detailed in Section 3. Firstly, we present the results of the CTC-based OCR models previously proposed for Amharic script recognition [7, 9], followed by the results of other state-of-the-art models [15, 18, 26, 41, 42] validated in Latin and/or Chinese scripts.\\n\\nThe experiments conducted using the CTC-based models previously proposed for Amharic script were categorized into four groups:\\n\\n\u2022 **HPopt-Plain-CTC**: plain-CTC (optimized hyper-parameters)\\n\u2022 **Plain-CTC**: Plain-CTC\\n\u2022 **HPopt-Attn-CTC**: Attention-CTC (optimized hyper-parameters)\\n\u2022 **Attn-CTC**: Attention-CTC\\n\\nIn all the CTC-based setups, to minimize computational costs during training, we resized all the text-line images to 48 by 368 pixels. We used 10% of the text-line images randomly drawn from the training set for validation. As previously discussed, in Section 3, we have two test sets: (i) Test-set-I, which includes 6375 text-line images randomly selected from 19th, 20th century manuscripts and other manuscripts with unknown publication dates, and (ii) Test-set-II, a text-line images that are drawn from a different distribution other than the training set, which includes 15935 text-line images from 18th century Ethiopic manuscripts only. The HPopt-Attn-CTC baseline model achieved the best CER of 16.41% and 28.65% on Test-set-I and Test-set-II, respectively (see Table 3 for details).\\n\\nThe results depicted in Figure 5 demonstrate that the CTC-based OCR models outperform human-level performance on Test-set-I in all configurations. However, only the HPopt-Attn-CTC model can surpass human-level performance, while the other configurations achieve comparable or worse results compared to human recognition on Test-set-II. Test-set-I was randomly selected from the training set, while Test-set-II consisted of 18th century manuscripts and represented out-of-distribution data. This disparity in performance is to be expected, as machine learning models typically perform better on samples that are independently and identically distributed rather than those in an out-of-distribution setting. The repeat experiments aimed to capture the variability in the performance of the models due to random weight initialization and sample order.\"}"}
{"id": "dUFf0pgkC7", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Box Plot comparison of variance in the recognition performance of CTC-based models and human level performance from ten experiments with varying random weight initialization and training sample orders on Test-set-I (IID) (left) and Test-set-II (OOD) (right). The results demonstrate that HPopt-attn-CTC outperforms all other CTC-based methods and surpassing human-level recognition on both test sets. The second group of models, being complex, was run through individual experiments. Instead of utilizing a Box Plot, a learning curve is provided (see Appendix C.2 for an extended discussion).\\n\\nHPopt-plain-CTC exhibits consistent variability across the 10 experiments due to the benefits of hyper-parameter optimization and a simplified architecture without attention mechanisms. The systematic fine-tuning of hyper-parameters, coupled with a simpler model architecture, resulted in stable and predictable performance throughout the experiments. In contrast, HPopt-attn-CTC achieved the lowest error despite some variability in certain trials, demonstrating its robustness across ten trials (see Table 3). The optimized hyperparameter configuration significantly improved recognition accuracy compared to non-optimized settings on both test sets, highlighting the importance of hyperparameter tuning for superior performance beyond relying solely on prior knowledge or trial-and-error approaches.\\n\\nThe second category of baseline OCR models assessed using our HHD-Ethiopic dataset comprises state-of-the-art models, including CRNN [41], ASTER [42], ABI-Net [18], SVTR [15], and TrOCR [26]. Considering our available computing resources, except for the TrOCR model, which was trained with fewer iterations, all other models were trained for 25 epochs. The learning curve, which illustrates the recognition performance using a CER metric on the IID and OOD test sets, is presented in Appendix Figure 12. In this group, the SVTR and ABINet models achieved the highest performance, with both models showing nearly equivalent results within a 1% difference during evaluation. As shown in Table 3, compared to the CTC-based models, the attention and transformer-based models exhibit larger number of parameter (see Appendix C for an extended discussion).\\n\\nBased on Figure 6 and our experimental observations, we observed distinct error patterns between humans and models: both exhibit substitution errors, but the model tends to make a higher number of insertions and deletions. This highlights the imperfection of the baseline OCR models in terms of sequence alignments. Furthermore, our study found that the evaluated baseline OCR models were highly effective, surpassing human-level recognition performance on Test-set-I. However, only a few models achieved better recognition performance on Test-set-II. Compared to other methods, the HPopt-Attn-CTC model has achieved the best recognition accuracy on both datasets. The baseline models evaluated in this study comprise CTC-based models previously proposed for the Amharic script, alongside five state-of-the-art attention and transformer-based models validated using English and Chinese scripts. These models could serve as references for evaluating the effectiveness of advanced models in recognizing historical handwritten Ethiopic scripts. Each of the CTC-based models previously proposed for Amharic script underwent ten experiments. In contrast, the other models, though trained for only single experiments and fewer epochs, achieved comparable outcomes. In addition, among the CTC-based models, the optimized hyperparameters model demonstrates...\"}"}
{"id": "dUFf0pgkC7", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: A summary of baseline models and their recognition performance on Test-set-I (IID, 6k) and Test-set-II (OOD, 16k) using CER and NED. The table includes model parameters measured in millions (M) and presents the lower and upper quartiles, denoted as $[\\\\text{val}^-, \\\\text{val}^+]$, obtained from multiple experiments.\\n\\n| Methods          | Model-Parms | Type-of-test | data  | [val$^-$, val$^+$] | CER  | NED  |\\n|------------------|-------------|--------------|-------|-------------------|------|------|\\n| Plain-CTC        | 2.5M        | IID          |       | $[21.05, 25.80]$  | 20.88| 19.09|\\n|                  |             | OOD          |       | $[35.15, 40.38]$  | 33.56| 31.90|\\n| Attn-CTC         | 1.9M        | IID          |       | $[21.05, 26.01]$  | 19.42| 21.01|\\n|                  |             | OOD          |       | $[35.00, 37.94]$  | 33.07| 32.92|\\n| HPopt-Plain-CTC  | 4.5M        | IID          |       | $[21.02, 21.73]$  | 19.42| 17.77|\\n|                  |             | OOD          |       | $[34.32, 34.98]$  | 32.01| 29.02|\\n| HPopt-Attn-CTC  | 2.2M        | IID          |       | $[17.55, 22.56]$  | 16.41| 16.06|\\n|                  |             | OOD          |       | $[30.79, 34.88]$  | 28.65| 27.37|\\n| TrOCR            | 333.9M      | IID          |       |                   | 35.00| 33.00|\\n|                  |             | OOD          |       |                   | 45.00| 43.87|\\n| CRNN             | 8.3M        | IID          |       |                   | 21.04| 21.01|\\n|                  |             | OOD          |       |                   | 29.86| 29.29|\\n| ASTER            | 27M         | IID          |       |                   | 24.43| 20.88|\\n|                  |             | OOD          |       |                   | 35.13| 30.75|\\n| SVTR             | 6M          | IID          |       |                   | 19.78| 17.98|\\n|                  |             | OOD          |       |                   | 30.82| 28.00|\\n| ABINet           | 23M         | IID          |       |                   | 21.49| 18.11|\\n|                  |             | OOD          |       |                   | 32.76| 28.84|\\n\\n- denotes no lower/upper quartiles due to model complexity; single experiment with ASTER, CRNN, SVTR, ABINet, and TrOCR models}\\n\\nFigure 6: Sample human-machine recognition errors per text-line image from the Test-set-I. Deleted characters are marked in red, while substituted and inserted characters are marked by green and yellow boxes, respectively. The inner ED denotes the Edit distance between the ground-truth and model prediction, while the outer ED denotes ground-truth to human prediction Edit distance.\\n\\nSuperior performance, benefiting from fine-tuning and reduced overfitting. The reported results and dataset serve as a benchmark for future research in machine learning, historical document image analysis, and recognition, while the analysis of human-level recognition performance enhances our understanding of the dataset.\\n\\n5 Conclusion\\n\\nIn this paper, we presented a novel dataset for text-image recognition research in the field of machine learning and historical handwritten Ethiopic scripts. The dataset comprises 79,684 text-line images obtained from manuscripts ranging from the 18th to 20th centuries and includes two test sets for evaluating OCR systems in both the IID (Independent and Identically Distributed) and OOD (Out-of-Distribution) domains.\"}"}
{"id": "dUFf0pgkC7", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of-Distribution) settings. We provided human-level performance and baseline results using CTC, attention and transformer based models to aid in the evaluation of OCR systems. To the best of our knowledge, this is the first study to offer a sizable historical dataset with human-level performance in this domain.\\n\\nIn addition to the human-level performance, we demonstrated the use of our dataset in addressing the problem of text-image recognition. We evaluated it using previously proposed models for Amharic script and state-of-the-art models validated with Latin and Chinese scripts. We evaluated their performance using the Character Error Rate (CER) and Normalized Edit Distance (NED).\\n\\nOur experiments demonstrate that both the trained SOTA methods and the smaller networks yield comparable results. Notably, the SOTA models produce equivalent outcomes even with fewer and smaller iterations, but larger parameter size. The smaller networks requires multiple experiments, making them suitable for low-resource computing infrastructure while still achieving comparable results.\\n\\nThe dataset and source code can be accessed at https://github.com/bdu-birhanu/HHD-Ethiopic, serving as a benchmark for machine learning and historical handwritten Ethiopic OCR research in low-resource settings. One limitation of our work is the scarcity of rare characters within the dataset. To tackle this limitation, we generate synthetic text-line images for the less frequent characters. However, our models have not been trained extensively using a larger synthetic dataset due to constraints on computational resources. To address this, future work includes expanding the dataset, and incorporating language models and contextual information for improved recognition. Additionally, we aim to refine the baseline models and conduct further experiments to enable a more systematic and conclusive evaluation of the different methods.\"}"}
{"id": "dUFf0pgkC7", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgments and Disclosure of Funding\\n\\nThis work was partially supported by ChaLearn, the ANR (L\u2019agence Nationale de la Recherche) under the Chair for AI Democratization, project grant number ANR-19-CHIA-0022 and ICT4D research center of Bahir Dar Institute of Technology. We are also grateful to the the Ethiopian National Archive and Library Agency (ENALA) staffs who provided valuable assistance with data collection, and allowing us access to necessary documents, as well as to Tariku Adane, Gizaw Wakjira, and Lemma Kassaye for their help with data collection approval and validation.\\n\\nReferences\\n\\n[1] C. Bosc-Tiess\u00e9 A. Wion and M.-L. Derat. \u201cinventory of libraries and catalogues of ethiopian manuscripts\u201d. http://www.menestrel.fr/?-Inventaire-des-bibliotheques-et-des-catalogues-des-manuscrits-ethiopiens-&lang=en&art=en, (last visited: 01.01.2023).\\n\\n[2] Azeb Amha. On loans and additions to the fid\u00e4l (ethiopic) writing system. In The Idea of Writing, pages 179\u2013196. Brill, 2009.\\n\\n[3] Jeonghun Baek, Geewook Kim, Junyeop Lee, Sungrae Park, Dongyoon Han, Sangdoo Yun, Seong Joon Oh, and Hwalsuk Lee. What is wrong with scene text recognition model comparisons? dataset and model analysis. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4715\u20134723, 2019.\\n\\n[4] Prasanna Balaprakash, Michael Salim, Thomas D Uram, Venkat Vishwanath, and Stefan M Wild. Deep-hyper: Asynchronous hyperparameter search for deep neural networks. In 2018 IEEE 25th international conference on high performance computing (HiPC), pages 42\u201351. IEEE, 2018.\\n\\n[5] Killian Barrere, Yann Soullard, Aur\u00e9lie Lemaitre, and Bertrand Co\u00fcasnon. Transformers for historical handwritten text recognition. In 16th International Conference on Document Analysis and Recognition (ICDAR), 2021.\\n\\n[6] Alessandro Bausi. La tradizione scrittoria etiopica. Segno e testo, 6:507\u2013557, 2008.\\n\\n[7] Birhanu Belay, Tewodros Habtegebrial, Million Meshesha, Marcus Liwicki, Gebeyehu Belay, and Didier Stricker. Amharic ocr: An end-to-end learning. Applied Sciences, 10(3):1117, 2020.\\n\\n[8] Birhanu Hailu Belay, Tewodros Habtegebrial, Marcus Liwicki, Gebeyehu Belay, and Didier Stricker. Amharic text image recognition: Database, algorithm, and analysis. In 2019 International conference on document analysis and recognition (ICDAR), pages 1268\u20131273. IEEE, 2019.\\n\\n[9] Birhanu Hailu Belay, Tewodros Habtegebrial, Marcus Liwicki, Gebeyehu Belay, and Didier Stricker. A blended attention-ctc network architecture for amharic text-image recognition. In ICPRAM, pages 435\u2013441, 2021.\\n\\n[10] L\u00e9on Bottou. Stochastic gradient descent tricks. Neural Networks: Tricks of the Trade: Second Edition, pages 421\u2013436, 2012.\\n\\n[11] Thomas M Breuel. High performance text recognition using a hybrid convolutional-lstm implementation. In 2017 14th IAPR international conference on document analysis and recognition (ICDAR), volume 1, pages 11\u201316. IEEE, 2017.\\n\\n[12] Xiaoxue Chen, Lianwen Jin, Yuanzhi Zhu, Canjie Luo, and Tianwei Wang. Text recognition in the wild: A survey. ACM Computing Surveys (CSUR), 54(2):1\u201335, 2021.\\n\\n[13] Hiuyi Cheng, Cheng Jian, Sihang Wu, and Lianwen Jin. Scut-cab: A new benchmark dataset of ancient chinese books with complex layouts for document layout analysis. In International Conference on Frontiers in Handwriting Recognition (ICFHR), pages 436\u2013451. Springer, 2022.\\n\\n[14] Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet Ng, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao Zhang, Junyu Han, Errui Ding, et al. Icdar2019 robust reading challenge on arbitrary-shaped text-rrc-art. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1571\u20131576. IEEE, 2019.\\n\\n[15] Yongkun Du, Zhineng Chen, Caiyan Jia, Xiaoting Yin, Tianlun Zheng, Chenxia Li, Yuning Du, and Yu-Gang Jiang. Svtr: Scene text recognition with a single visual model. In Lud De Raedt, editor, Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 884\u2013890. International Joint Conferences on Artificial Intelligence Organization, 7 2022. Main Track.\"}"}
{"id": "dUFf0pgkC7", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"351 Romain Egele, Joceran Gouneau, Venkatram Vishwanath, Isabelle Guyon, and Prasanna Balaprakash. \\nAsynchronous distributed bayesian optimization at hpc scale. arXiv preprint arXiv:2207.00479, 2022.\\n\\n352 Floriana Esposito. Symbolic machine learning methods for historical document processing. In Proceedings of the 2013 ACM symposium on Document engineering, pages 1\u20132, 2013.\\n\\n355 Shancheng Fang, Hongtao Xie, Yuxin Wang, Zhendong Mao, and Yongdong Zhang. Read like humans: Autonomous, bidirectional and iterative language modeling for scene text recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\\n\\n359 Andreas Fischer. Iam-histdb a dataset of handwritten historical documents. Handwritten historical document analysis, recognition, and retrieval-state of the art and future trends, 2020.\\n\\n363 Basilis Gatos, Nikolaos Stamatopoulos, Georgios Louloudis, Giorgos Sfikas, George Retsinas, Vassilis Papavassiliou, Fotini Sunistira, and Vassilis Katsouros. Grpoly-db: An old greek polytonic document image database. In 2015 13th international conference on document analysis and recognition (ICDAR), pages 646\u2013650. IEEE, 2015.\\n\\n370 Alex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, and J\u00fcrgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd international conference on Machine learning (ICML), pages 369\u2013376, 2006.\\n\\n374 James Jeffrey. The 4th century art that died out across the world and the ethiopian scribes trying to preserve it. https://www.globalissues.org/news/2014/05/08/18652, 2004.\\n\\n376 Rakesh Kummari and Chakravarthy Bhagvati. Uhtelpcc: a dataset for telugu printed character recognition. In International Conference on Recent Trends in Image Processing and Pattern Recognition, pages 24\u201336. Springer, 2018.\\n\\n378 Huseyin Kusetogullari, Amir Yavariabdi, Johan Hall, and Niklas Lavesson. Digitnet: a deep handwritten digit detection and recognition method using a new historical handwritten digit dataset. Big Data Research, 23:100182, 2021.\\n\\n381 Ladislav Lenc, Ji\u0159\u00ed Mart\u00ednek, Pavel Kr\u00e1l, Anguelos Nicolao, and Vincent Christlein. Hdpa: historical document processing and analysis framework. Evolving Systems, 12:177\u2013190, 2021.\\n\\n382 Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, and Furu Wei. Trocr: Transformer-based optical character recognition with pre-trained models. In AAAI 2023, February 2023.\\n\\n388 Qingxiang Lin, Canjie Luo, Lianwen Jin, and Songxuan Lai. Stan: A sequential transformation attention-based network for scene text recognition. Pattern Recognition, 111:107692, 2021.\\n\\n392 Nam Tuan Ly, Cuong Tuan Nguyen, and Masaki Nakagawa. An attention-based end-to-end model for multiple text lines recognition in Japanese historical documents. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 629\u2013634. IEEE, 2019.\\n\\n394 John Mellors and Anne Parsons. Ethiopian Bookmaking: bookmaking in rural Ethiopia in the twenty-first century. New Cross Books, 2002.\\n\\n398 Million Meshesha and CV Jawahar. Indigenous scripts of african languages. Indilinga African Journal of Indigenous Knowledge Systems, 6(2):132\u2013142, 2007.\\n\\n399 Ronaldo Messina and Jerome Louradour. Segmentation-free handwritten chinese text recognition with lstm-rnn. In 2015 13th International conference on document analysis and recognition (ICDAR), pages 171\u2013175. IEEE, 2015.\\n\\n402 Ronny Meyer. the ethiopic script: linguistic features and socio-cultural connotations. Oslo Studies in Language, 8(1), 2016.\\n\\n404 Aly Mostafa, Omar Mohamed, Ali Ashraf, Ahmed Elbehery, Salma Jamal, Ghada Khoriba, and Amr S Ghoneim. Ocformer: A transformer-based model for arabic handwritten text recognition. In 2021 International Mobile, Intelligent, and Ubiquitous Computing Conference (MIUCC), pages 182\u2013186. IEEE, 2021.\\n\\n406 Konstantina Nikolaidou, Mathias Seuret, Hamam Mokayed, and Marcus Liwicki. A survey of historical document image datasets. IJDAR 25, page 305\u2013338, 2022.\"}"}
{"id": "dUFf0pgkC7", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[36] Umapada Pal, Nabin Sharma, Tetsushi Wakabayashi, and Fumitaka Kimura. Off-line handwritten character recognition of devnagari script. In Ninth International Conference on Document Analysis and Recognition (ICDAR 2007), volume 1, pages 496\u2013500. IEEE, 2007.\\n\\n[37] Christos Papadopoulos, Stefan Pletschacher, Christian Clausner, and Apostolos Antonacopoulos. The impact dataset of historical document images. In Proceedings of the 2Nd international workshop on historical document imaging and processing, pages 123\u2013130, 2013.\\n\\n[38] Zhi Qiao, Yu Zhou, Dongbao Yang, Yucan Zhou, and Weiping Wang. Seed: Semantics enhanced encoder-decoder framework for scene text recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13528\u201313537, 2020.\\n\\n[39] Joan Andreu Sanchez, Ver\u00f3nica Romero, Alejandro H Toselli, Mauricio Villegas, and Enrique Vidal. Icdar2017 competition on handwritten text recognition on the read dataset. In 2017 14th IAPR international conference on document analysis and recognition (ICDAR), volume 1, pages 1383\u20131388. IEEE, 2017.\\n\\n[40] Zejiang Shen, Kaixuan Zhang, and Melissa Dell. A large dataset of historical japanese documents with complex layouts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 548\u2013549, 2020.\\n\\n[41] Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition. IEEE transactions on pattern analysis and machine intelligence, 39(11):2298\u20132304, 2016.\\n\\n[42] Baoguang Shi, Mingkun Yang, Xinggang Wang, Pengyuan Lyu, Cong Yao, and Xiang Bai. Aster: An attentional scene text recognizer with flexible rectification. IEEE transactions on pattern analysis and machine intelligence, 41(9):2035\u20132048, 2018.\\n\\n[43] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In European conference on computer vision, pages 742\u2013758. Springer, 2020.\\n\\n[44] Foteini Simistira, Mathias Seuret, Nicole Eichenberger, Angelika Garz, Marcus Liwicki, and Rolf Ingold. Diva-hisdb: A precisely annotated large dataset of challenging medieval manuscripts. In 2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR), pages 471\u2013476. IEEE, 2016.\\n\\n[45] Haozhe Sun, Wei-Wei Tu, and Isabelle M Guyon. Omniprint: A configurable printed character synthesizer. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. https://openreview.net/forum?id=R07XwJPmgpl.\\n\\n[46] Nimol Thuon, Jun Du, and Jianshu Zhang. Improving isolated glyph classification task for palm leaf manuscripts. In International Conference on Frontiers in Handwriting Recognition (ICFHR), pages 65\u201379. Springer, 2022.\\n\\n[47] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text: Dataset and benchmark for text detection and recognition in natural images. arXiv preprint arXiv:1601.07140, 2016.\\n\\n[48] Christoph Wick, Jochen Z\u00f6llner, and Tobias Gr\u00fcning. Rescoring sequence-to-sequence models for text line recognition with ctc-prefixes. In International Workshop on Document Analysis Systems (DAS), pages 260\u2013274. Springer, 2022.\\n\\n[49] Ana\u00efs Wion. The national archives and library of ethiopia: six years of ethio-french cooperation (2001-2006). In The National Archives and Library of Ethiopia: six years of Ethio-French cooperation (2001-2006), pages 20\u2013p, 2006.\\n\\n[50] Jianshu Zhang, Jun Du, and Lirong Dai. A gru-based encoder-decoder approach with attention for online handwritten mathematical expression recognition. In 2017 14th IAPR International conference on document analysis and recognition (ICDAR), volume 1, pages 902\u2013907. IEEE, 2017.\\n\\n[51] Zhuoyao Zhong, Lianwen Jin, and Zecheng Xie. High performance offline handwritten chinese character recognition using googlenet and directional feature maps. In 2015 13th International Conference on Document Analysis and Recognition (ICDAR), pages 846\u2013850. IEEE, 2015.\"}"}
{"id": "dUFf0pgkC7", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. For all authors...\\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n(b) Did you describe the limitations of your work? [Yes] See the last paragraph of the conclusion section.\\n(c) Did you discuss any potential negative societal impacts of your work? [N/A]\\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n(b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] we have made all the code, data, and instructions needed to reproduce the main experimental results available under an open-source license. They can be accessed and downloaded from https://github.com/bdu-birhanu/HHD-Ethiopic.\\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] we provide these details in section 3.1 and 3.2 in addition, hyperparameter and model configuration is given in Appendix C.\\n(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] but we opted to use Boxplot to visualize results which provides a comprehensive summary of data distribution, allowing quick assessment of central tendency, spread, and outliers for a better understanding of overall result variability and shown in figure 5.\\n(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] see Appendix C.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n(a) If your work uses existing assets, did you cite the creators? [Yes]\\n(b) Did you mention the license of the assets? [Yes]\\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\\n(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes]\\n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] The instructions for the annotation task were simple and included guidance on how to use the annotation tool.\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes] we paid money as a reward for the annotations per a text-line image.\"}"}
