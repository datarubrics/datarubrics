{"id": "OTjTKFk7gb", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"AuctionNet: A Novel Benchmark for Decision-Making in Large-Scale Games\\n\\nKefan Su 1,2 \u2217, Yusen Huo 2, Zhilin Zhang 2, Shuai Dou 2, Chuan Yu 2, Jian Xu 2 \u2020, Zongqing Lu 1 \u2020, Bo Zheng 2\\n\\n1 School of Computer Science, Peking University\\n2 Alibaba Group\\n1 {sukefan,zongqing.lu}@pku.edu.cn\\n2 {huoyusen.huoyusen,zhangzhilin.pt,doushuai.ds,yuchuan.yc,xiyu.xj,bozheng}@alibaba-inc.com\\n\\nAbstract\\nDecision-making in large-scale games is an essential research area in artificial intelligence (AI) with significant real-world impact. However, the limited access to realistic large-scale game environments has hindered research progress in this area. In this paper, we present AuctionNet, a benchmark for bid decision-making in large-scale ad auctions derived from a real-world online advertising platform. AuctionNet is composed of three parts: an ad auction environment, a pre-generated dataset based on the environment, and performance evaluations of several baseline bid decision-making algorithms. More specifically, the environment effectively replicates the integrity and complexity of real-world ad auctions through the interaction of several modules: the ad opportunity generation module employs deep generative networks to bridge the gap between simulated and real-world data while mitigating the risk of sensitive data exposure; the bidding module implements diverse auto-bidding agents trained with different decision-making algorithms; and the auction module is anchored in the classic Generalized Second Price (GSP) auction but also allows for customization of auction mechanisms as needed. To facilitate research and provide insights into the environment, we have also pre-generated a substantial dataset based on the environment. The dataset contains 10 million ad opportunities, 48 diverse auto-bidding agents, and over 500 million auction records. Performance evaluations of baseline algorithms such as linear programming, reinforcement learning, and generative models for bid decision-making are also presented as a part of AuctionNet. AuctionNet has powered the NeurIPS 2024 Auto-Bidding in Large-Scale Auctions competition, providing competition environments for over 1,500 teams. We believe that AuctionNet is applicable not only to research on bid decision-making in ad auctions but also to the general area of decision-making in large-scale games. Code: https://github.com/alimama-tech/AuctionNet.\\n\\n1 Introduction\\nDecision-making in large-scale games is a fundamental area of research in artificial intelligence. Agents in a large-scale game need to make strategic decisions to fulfill their objectives under certain constraints in a competitive environment. The research advances in this area have a profound impact on a broad range of real-world applications [13, 34, 35, 37]. Online advertising, with a market size of\\n\\n\u2217 This work is done during internship at Alibaba Group.\\n\u2020 Corresponding author.\\n3 Alibaba Group retains full ownership rights to this benchmark.\"}"}
{"id": "OTjTKFk7gb", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"arriving ad opportunities can have multi-slot impressions. Each ad opportunity can have multi-slot impressions. For each advertiser\u2019s unique objective, the auto-bidding agent makes bid decisions for continuously arriving ad opportunities and competes against each other in the ad auction. Then, each agent may win some impressions, which may be exposed to users and potentially result in conversions. Finally, the agents\u2019 performance will be reported to advertisers.\\n\\nFigure 1: Overview of typical large-scale online advertising platform. Numbers 1 through 5 illustrate how an auto-bidding agent helps an advertiser optimize performance. For each advertiser\u2019s unique objective, the auto-bidding agent makes bid decisions for continuously arriving ad opportunities, and competes against each other in the ad auction. Then, each agent may win some impressions, which may be exposed to users and potentially result in conversions. Finally, the agents\u2019 performance will be reported to advertisers.\\n\\nMore than $600 billion in 2023, is perhaps one of the most representative applications that calls for sophisticated decision-making solutions in large-scale games. More specifically, as shown in Figure 1, a significant part of online advertising is based on real-time bidding (RTB), a process in which advertising inventory is bought and sold in real-time ad auctions. The auto-bidding agents strategically bid for impressions on behalf of the advertisers across a large number of continuously arriving ad opportunities to maximize performance, subject to certain constraints such as return-on-investment (ROI) [28].\\n\\nBid decision-making in large-scale ad auctions is a concrete example of decision-making in large-scale games. However, researchers usually only have limited access to realistic large-scale ad auction environments, hindering research progress in this area. Although a few existing works provide certain environments, there remains a considerable gap between these environments and the real-world environments. For instance, AuctionGym [18] overlooks changes in advertiser budgets across multiple auction rounds, while AdCraft [11] models competing bidders by sampling from a parameterized distribution, an approach that falls short of fully capturing the essence of the multi-agent dynamics inherent to this problem.\\n\\nIn this paper, we present AuctionNet, a benchmark for bid decision-making in large-scale ad auctions derived from a real-world online advertising platform. AuctionNet is composed of three parts: an ad auction environment, a pre-generated dataset based on the environment, and performance evaluations of a couple of baseline bid decision-making algorithms. More specifically, the environment effectively replicates the integrity and complexity of real-world ad auctions with the interaction of several modules: the ad opportunity generation module employs deep generative networks to bridge the gap between simulated and real-world data while mitigating the risk of sensitive data exposure; the bidding module implements diverse auto-bidding agents trained with different decision-making algorithms; and the auction module is anchored in the classic and popular Generalized Second Price (GSP) [9, 23, 7] auction but also allows customization of auction mechanisms as needed. To facilitate research and provide insights into the game environment, we also pre-generated a substantial dataset based on the environment. The dataset contains 10 million ad opportunities, 48 diverse auto-bidding agents, and over 500 million auction records. Performance evaluations of baseline algorithms such as linear programming, reinforcement learning, and generative models for bid decision-making are also presented as a part of AuctionNet.\\n\\nWe believe that AuctionNet is applicable not only to research on bid decision-making algorithms in ad auctions but also to the general area of decision-making in large-scale games. It can also benefit...\"}"}
{"id": "OTjTKFk7gb", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"researchers in a broader range of areas such as reinforcement learning, generative models, operational\\nresearch, and mechanism design.\\n\\n2 The Decision-Making Problem Concerned\\n\\nIn this paper, we are concerned with the auto-bidding problem in ad auctions. We use a Partially\\nObservable Stochastic Game (POSG) \\\\[14\\\\] to formulate the problem. A POSG \\\\( M \\\\) can be represented\\nas a tuple \\\\( M = \\\\{ S, A, P, r, \\\\gamma, Z, O, I, T \\\\} \\\\), where \\\\( I = \\\\{ 1, 2, \\\\ldots, n \\\\} \\\\) is the set of all the agents,\\n\\\\( T \\\\) is the horizon, i.e., the number of time steps in one episode, \\\\( S \\\\) is the state space and \\\\( A \\\\) is the action\\nspace, \\\\( P(\\\\cdot|s, a) : S \\\\times A \\\\rightarrow \\\\Delta(S) \\\\) is the transition probability, \\\\( \\\\gamma \\\\) is the discount factor,\\n\\\\( Z \\\\) is the observation space, \\\\( O(s, i) : S \\\\times I \\\\rightarrow Z \\\\) is the mapping from state to observation for each agent\\n\\\\( i \\\\), \\\\( r = r_1 \\\\times r_2 \\\\times \\\\cdots \\\\times r_n \\\\) is the joint reward function of all the agents, and \\\\( r_i(s, a) : S \\\\times A \\\\rightarrow R \\\\) is the\\nindividual reward function for each agent \\\\( i \\\\), where \\\\( a = (a_1, a_2, \\\\ldots, a_n) \\\\in A = A_1 \\\\times A_2 \\\\times \\\\cdots \\\\times A_n \\\\) is the joint action of all the agents.\\n\\nSpecifically, the interaction in one time step is as follows: The state \\\\( s = (\\\\omega, u, q, v) \\\\) consists of\\nbudgets \\\\( \\\\omega \\\\), ad opportunity features \\\\( u \\\\), advertiser features \\\\( q \\\\) such as industry category, corresponding\\nvalue matrix \\\\( v = \\\\{ v_{ij} \\\\} \\\\), where \\\\( v_{ij} \\\\) is the value of ad opportunity \\\\( j \\\\) for agent \\\\( i \\\\). Agent \\\\( i \\\\)\u2019s observation \\\\( o_i = (\\\\omega_i, u_i, q_i, v_i) \\\\in Z \\\\) contains only part of the information in state\\n\\\\( s \\\\), i.e., agent \\\\( i \\\\) may not know the budgets of other agents. A convention in the auto-bidding area \\\\[ 3\\\\] proves that the\\noptimal bid is proportional to the ad opportunity value. Following this convention, the action of\\nagent \\\\( i \\\\) is a coefficient \\\\( \\\\alpha_i \\\\), and the bids of agent \\\\( i \\\\) for all the ad opportunities of this time step are\\n\\\\( b_i = (b_{i1}, b_{i2}, \\\\ldots, b_{im}) = (\\\\alpha_i v_{i1}, \\\\alpha_i v_{i2}, \\\\ldots, \\\\alpha_i v_{im}) \\\\), where \\\\( m \\\\) is the number of ad opportunities\\nwithin this time step. Given the bids of all the agents, determined by the auction mechanism, agent\\n\\\\( i \\\\) will receive the auction result \\\\( x_i = (x_{i1}, x_{i2}, \\\\ldots, x_{im}) \\\\), where \\\\( x_{ij} = 1 \\\\) if and only if agent \\\\( i \\\\) wins\\nopportunity \\\\( j \\\\). Agents will only receive rewards and incur costs from the winning impressions,\\ni.e., reward \\\\( r_i(s, a) = \\\\sum_{j=1}^{m} x_{ij} v_{ij} \\\\) and budget for the next time step \\\\( \\\\omega' = \\\\omega - \\\\sum_{j=1}^{m} x_{ij} c_{ij} \\\\), where\\n\\\\( c_{ij} \\\\) is the cost of impression \\\\( j \\\\) for agent \\\\( i \\\\).\\n\\nTaking a typical auto-bidding scenario as an example, given the definition above, the optimization\\nobjective from the perspective of agent \\\\( i \\\\) is as follows:\\n\\n\\\\[\\n\\\\max_{\\\\{\\\\alpha_t\\\\}} \\\\sum_{t=1}^{T} x_{ti} v_{ti}, \\\\quad v_{ti}, \\\\quad c_t, \\\\quad \\\\omega_i, \\\\quad \\\\langle \\\\cdot \\\\rangle\\n\\\\]\\n\\nwhere \\\\( x_{ti} = (x_{ti1}, x_{ti2}, \\\\ldots, x_{tim}) \\\\), \\\\( v_{ti} = (v_{ti1}, v_{ti2}, \\\\ldots, v_{tim}) \\\\), \\\\( c_t = (c_{t1}, c_{t2}, \\\\ldots, c_{tm}) \\\\), \\\\( \\\\omega_i \\\\) is the budget\\nof agent \\\\( i \\\\), and \\\\( \\\\langle \\\\cdot \\\\rangle \\\\) denotes the inner product. As for the implementation, we know from our problem\\nformulation that \\\\( r_i(s_{t}, a_{t}) = \\\\langle x_{ti}, v_{ti} \\\\rangle \\\\), so the objective in the optimization formulation is the same\\nas \\\\( \\\\sum_{t=1}^{T} r_i(s_{t}, a_{t}) \\\\). For more complex scenarios, we can add the CPA constraint to ensure effective\\nutilization of the budget. More details on these CPA-constrained problems are included in Appendix\\nE. The decision-making formulation above can be easily extended to various real-world scenarios.\\n\\n3 Ad Auction Environment\\n\\nTo comprehensively demonstrate large-scale games from real-world online advertising platforms,\\nwe have developed an ad auction environment. To standardize the auto-bidding process, we divide\\nad opportunities within a period into \\\\( T \\\\) decision time steps. Given the objective, the auto-bidding\\nagent sequentially bids at each step, using the results from step \\\\( t \\\\) and prior historical information\\nto refine its strategy for step \\\\( t+1 \\\\). This design philosophy enables agents to continuously optimize\\ntheir bidding strategies in order to adapt to the changing environment. Within each step, all ad\\nopportunities are executed independently and in parallel. At the end of the period, the environment\\nprovides the final performance for the agent.\\n\\nThe environment effectively replicates the integrity and complexity of real-world ad auctions through\\nthe interaction of several modules: the ad opportunity generation module, the bidding module, and\\nthe auction module. To better simulate large-scale auctions in reality, a substantial number of ad\\nopportunities are fed into the environment and configured with dozens of bidding agents. These ad\\nopportunities are generated using deep generative networks to reduce the gap between the simulation\\n3\"}"}
{"id": "OTjTKFk7gb", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Overview of the pipeline of the ad opportunity generation network. The generation process consists of two stages. In the first stage, ad opportunity features are generated through a latent diffusion model. In the second stage, the value prediction for the generated ad opportunity features is performed, incorporating both the time feature and the advertiser feature. Moreover, the volume of ad opportunities fluctuates over time, mirroring that of real-world online advertising platforms.\\n\\n3.1 The Ad Opportunity Generation Module\\n\\nThe target of the ad opportunity generation module is to generate diverse ad opportunities similar to real online advertising data with deep generative networks, as shown in Figure 2. We aimed to adopt the diffusion model to generate ad opportunity but encountered difficulties with the denoising operation, which can yield unreasonable outputs. Therefore, we followed the approach of the Latent Diffusion Model (LDM) [25] to generate ad opportunity. LDM adds noise and performs denoising in the latent space using a diffusion model, and then generates data from the latent space with an encoder and decoder. Specifically, LDM maps the ad opportunity feature \\\\( u \\\\) to a latent vector \\\\( y \\\\) with the encoder and reconstructs this feature with the decoder during training. For generation, LDM samples a random latent vector from a normal distribution and then generates an ad opportunity feature based on this vector. Let \\\\( U \\\\subseteq \\\\mathbb{R}^d \\\\) be the space of ad opportunity feature data \\\\((u_1, u_2, \\\\cdots, u_K)\\\\), where \\\\( d \\\\) is the dimension of the original data and \\\\( K \\\\) is the number of ad opportunities. Let \\\\( Y \\\\subseteq \\\\mathbb{R}^{d'} \\\\) be the latent space \\\\((d' < d)\\\\). The encoder and decoder are represented as \\\\( g_\\\\phi \\\\) and \\\\( h_\\\\psi \\\\), respectively, where \\\\( \\\\phi \\\\) and \\\\( \\\\psi \\\\) are the parameters. The function of the encoder \\\\( g_\\\\phi \\\\) is to obtain a latent representation of the original data as \\\\( g_\\\\phi(u_k) = (\\\\mu_k, \\\\sigma_k) \\\\), where \\\\( y_k \\\\sim N(\\\\mu_k, \\\\sigma_k^2) \\\\) and \\\\( y_k \\\\in Y \\\\) is the latent representation.\\n\\nIn practice, the reparameterization trick [20] is applied to ensure that this operation is differentiable during backpropagation. Given the latent representation \\\\( y_k \\\\), the decoder is responsible for reconstructing the original data from \\\\( y_k \\\\), i.e., \\\\( h_\\\\psi(y_k) = \\\\tilde{u}_k \\\\in U \\\\). In addition to the reconstruction, the latent distribution \\\\( N(\\\\mu_k, \\\\sigma_k^2) \\\\) is expected to approximate the standard Gaussian distribution \\\\( N(0, 1) \\\\). Therefore, we have the following loss function for the encoder and decoder:\\n\\n\\\\[\\nL_{\\\\text{recons}} = \\\\frac{1}{K} \\\\sum_{k=1}^{K} \\\\| u_k - h_\\\\psi(y_k) \\\\|_2^2,\\n\\\\]\\n\\n\\\\[\\nL_{\\\\text{reg}} = \\\\frac{1}{K} \\\\sum_{k=1}^{K} D_{\\\\text{KL}}[N(\\\\mu_k, \\\\sigma_k^2) \\\\| N(0, 1)],\\n\\\\]\\n\\nwhere \\\\( L_{\\\\text{recons}} \\\\) is the reconstruction loss and \\\\( L_{\\\\text{reg}} \\\\) is the regularization loss for the latent distribution. Different from the original idea of VAE [20], where the latent variable \\\\( y \\\\in Y \\\\) is sampled from \\\\( N(0, 1) \\\\) in the generation process, LDM uses a diffusion model in the latent space to generate the latent variable. In general, the idea behind the diffusion model is to add Gaussian noise to the original data to obtain variables that follow \\\\( N(0, 1) \\\\) and to denoise from \\\\( N(0, 1) \\\\) for generation. Given a...\"}"}
{"id": "OTjTKFk7gb", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"latent variable $y$, we denote its noisy version after $p$ iterations as $y_p$. The diffusion model includes a network to predict noise $\\\\epsilon_\\\\theta(y_p, p)$, and the loss function can be represented as\\n\\n$$L_{DM} = \\\\frac{1}{K} \\\\sum_{k=1}^{K} \\\\| \\\\epsilon_k - \\\\epsilon_\\\\theta(y_k, p_k) \\\\|_2^2,$$\\n\\nwhere $\\\\epsilon_k \\\\sim N(0, 1)$, $y_k$ is the latent embedding of $u_k$, and $p_k$ is uniformly sampled from the set $\\\\{1, 2, \\\\ldots, p_{\\\\text{max}}\\\\}$. The network $\\\\epsilon_\\\\theta(y_p, p)$ is the only learnable component in the diffusion model, which enables the process of adding noise and denoising through basic operations.\\n\\nAs for the generation process, a latent variable $\\\\bar{y}$ is sampled from $N(0, 1)$, and $\\\\tilde{y}$ is obtained through $p_{\\\\text{max}}$ denoising steps from $\\\\bar{y}$ using the noise prediction network $\\\\epsilon_\\\\theta$. Finally, the decoder generates an ad opportunity feature based on $\\\\tilde{y}$ as $\\\\tilde{u} = h_\\\\psi(\\\\tilde{y})$.\\n\\nGiven an ad opportunity feature $u_k$, we also need to determine the value of this ad opportunity combined with the category information of the corresponding advertiser $q_k$ and the time information $u_{\\\\text{time}}$, where $q_k$ is the advertiser information in the real-world data associated with $u_k$. We use Multi-head Attention (MHA) [31] as the network architecture for information integration. Let $v_\\\\xi$ represent the value prediction module, and $v_\\\\xi(u_k, q_k, u_{\\\\text{time}})$ denote the predicted value of the ad opportunity feature $u_k$ for a specific advertiser at a specific time step. The loss of the value prediction model is shown below:\\n\\n$$L_{\\\\text{pred}} = \\\\frac{1}{K} \\\\sum_{k=1}^{K} v_k - v_\\\\xi(u_k, q_k, u_{\\\\text{time}})^2,$$\\n\\nwhere $v_k$ is the true value of the ad opportunity in the record associated with $u_k$.\\n\\n### 3.2 The Bidding Module\\n\\nThe bidding module replicates the dynamic competition between advertisers, each of whom has distinct advertising objectives and utilizes a separate auto-bidding agent, while remaining unaware of their competitors' strategies. Researchers can control a subset of the agents in the environment, while other agents remain uncontrollable, thereby better reflecting the complex and dynamic game in real-world online advertising.\\n\\nSeveral algorithms in the auto-bidding area have been implemented as baselines, including the PID Controller [36], Online LP [15], IQL [21], Behavior Cloning [30], and Decision Transformer [8]. This facilitates researchers who are interested in quickly starting up and evaluating these baselines in a unified environment.\\n\\n### 3.3 The Auction Module\\n\\nThe task of the auction module is to determine the winner and the winning price given all the bids from agents for ad opportunities. The costs for agents will vary depending on the different auction rules. The most commonly discussed auction rule is the Generalized Second-Price (GSP) Auction, which stipulates that the winner pays a cost slightly higher than the second-highest bid rather than the highest bid. The auction module internally supports several popular auction rules, including GSP, for the convenience of researchers. Additionally, researchers can design specific auction rules tailored to their purposes using the interface of the auction module.\\n\\nAdditionally, the property of multiple slots has been implemented in the environment. Multiple slots arise from applications in the industry, meaning that a single ad opportunity may have multiple ad slots for display. A slot with a higher exposure rate is more valuable to advertisers. Suppose the number of slots is $l$, then the auction module will allocate $l$ slots to the top $l$ bidders, and these bidders will receive different values according to the varying exposure rates of the slots. In summary, the multiple slots feature increases the complexity of the optimal bidding strategy, as the exposure rate serves as a discount factor for both cost and value.\\n\\n### 3.4 API\\n\\nThe code of the environment is implemented in Python. The environment API is similar to OpenAI Gym [5], so the construction and interactions of the environment may be familiar to related researchers. We included an example code as follows:\"}"}
{"id": "OTjTKFk7gb", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: The 3D PCA results of 100K generated data and 100K real-world data.\\n\\n```python\\nfrom AuctionNet import Controller\\n\\n# Load player agent\\nbidding_controller = Controller(player_agent=player_agent)\\n\\n# Init other competing agents\\nagents = bidding_controller.agents\\n\\n# Init auction module\\nenvs = bidding_controller.biddingEnv\\n\\n# Generate ad opportunities\\nad_opportunities = bidding_controller.adOpportunityGenerator.generate()\\n\\n# Init the budget and reward of each agent\\nrewards = np.zeros(shape=(len(agents)))\\ncosts = np.zeros(shape=(num_agents))\\n\\nfor episode in range(num_episode):\\n    for tick_index in range(num_tick):\\n        # load ad opportunities\\n        tick_ad_opportunities = ad_opportunities[episode][tick_index]\\n\\n        # Collect bids from each agent\\n        bids = []\\n        for agent in agents:\\n            bids.append(agent.bidding())\\n\\n        # Simulate bidding process\\n        auction_res = envs.simulate_ad_bidding(tick_ad_opportunities, bids)\\n\\n        # Aggregate bidding results\\n        rewards += auction_res[\"reward\"]\\ncosts += auction_res[\"cost\"]\\n```\\n\\n4 Pre-Generated Dataset Based on the Environment\\n\\nIn this section, we first verify whether the ad opportunity generation module can generate ad opportunity features similar to those in real-world data. Next, we briefly introduce and analyze the dataset generated from the AuctionNet environment.\\n\\n4.1 Verification of the Ad Opportunity Generation Module\\n\\nIn order to better demonstrate that the generated data can reflect the properties of real-world data, the effectiveness of the ad opportunity generation module itself was verified. The ad opportunity\"}"}
{"id": "OTjTKFk7gb", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"generation module comprises two components: a feature generation model and a value prediction model. Experiments were conducted to verify the effectiveness of these models. We randomly sample 100K real-world online advertising data points to compare with 100K generated data points. The details of the generated data can be found in Appendix D. First, we perform PCA [19] to visualize the similarity between the real-world and generated data. The 3D PCA results are illustrated in Figure 3. For better presentation, we use six different views in the 3D space. We observe that the generated data overlap with the original data in the 3D space. Moreover, the generated data points form four main separate clusters in the 3D space, similar to the real-world data points. These visualization results demonstrate that the generated data generally resemble the real-world data.\\n\\nTo further compare these two datasets, we study the value distributions of identity information and consumption behavior information in both datasets. The empirical results are included in Figure 4 and Figure 5. The feature vector contains over 20 fields, as described in Appendix D, so we only select a subset of these fields for our experiments. Regarding identity information, the generated value distributions are similar to the real-world value distributions overall, although biases exist for certain terms, such as 'level 7' for the Taobao VIP Level. Distributions with more categories are more challenging to match, while the gender distributions are nearly identical in both datasets. For consumption behavior information, we observe that the distributions in the selected fields share a strong resemblance and exhibit long-tail characteristics. A long-tail distribution indicates that most users do not engage in frequent consumption, and users with a high volume of consumption behavior are rare. This phenomenon aligns with our experience in online advertising.\\n\\nWe investigate whether the generated data can capture the connections between different fields. Based on the observation that users with higher VIP levels typically exhibit a higher volume of consumption behavior, we examine the connection between the Taobao VIP level and consumption behavior. We select four consumption behavior fields. The mean values of these fields across different VIP levels are shown in Figure 6. We find that the overall monotonically increasing trend is captured by the generated data, although biases exist in the specific values. Moreover, the drop in values from 'level 7' to 'level 8' is also captured by the generated data in three out of the four fields, except for the consumption amount. The rarity of 'level 8' data points may be the reason why the generative model is unable to distinguish different trends for different fields.\\n\\nIn real-world online advertising, the metrics for bidding strategy evaluation are Click-Through Rate (CTR) and Conversion Rate (CVR). Bidding strategies make decisions based on the predicted CTR (pCTR) and predicted CVR (pCVR), which are the estimated values of CTR and CVR, respectively. For simplicity, in this environment, we assume that the estimations are accurate and define the value as \\n\\n$$\\\\text{value} = pCTR \\\\cdot pCVR$$\\n\\nOur value prediction model learns to predict pCTR and pCVR and subsequently calculates the value. We predict the pCTR, pCVR, and value for 100K real-world data points and compare these predictions with the real-world ground truth. We hope that the value prediction model can capture the value variation over changes in category and time. The means of predicted pCTR, pCVR, and values across different categories and time steps,\"}"}
{"id": "OTjTKFk7gb", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: The mean values of consumption behavior information including the number of cart items, the number of collected items, the consumption amounts, and the number of visited categories in different VIP levels in 100K generated data and 100K real-world data.\\n\\nFigure 7: The means of the predicted pCTR, pCVR, and value in different categories and time steps compared with the ground truth. The shaded areas are related to the standard deviation.\\n\\nThe empirical results show that, in general, the variation trends in predictions over changes in category and time are similar to the ground truth. To present the results more intuitively, we provide additional quantitative results. We compare the mean squared error (MSE) between the generated and original distributions with the standard deviation of the original distribution. The quantitative results are shown in Table 1. It can be observed that the MSEs are all smaller than the original standard deviations (original_stds), indicating that our prediction model can capture the patterns of value variation and is accurate.\\n\\n4.2 Pre-Generated Dataset\\n\\nTable 1: The comparison of the MSE between the generated and original distribution with the standard deviation of the original distribution.\\n\\n|                  | original_std | MSE   | pCVR_category | MSE  | pCTR_category | MSE  | value_category | MSE  |\\n|------------------|--------------|-------|---------------|------|---------------|------|----------------|------|\\n| pCVR_category    | 0.0685       | 0.0341| 0.0341        | 0.0685| 0.0341        | 0.0341| 0.0685        | 0.0341|\\n| pCTR_category    | 0.0517       | 0.0280| 0.0280        | 0.0517| 0.0280        | 0.0280| 0.0517        | 0.0280|\\n| value_category   | 0.00573      | 0.00496| 0.00496      | 0.00573| 0.00496      | 0.00496| 0.00573    | 0.00496|\\n\\nThe dataset is derived from game data generated within the environment, where numerous auto-bidding agents compete against each other. We have pre-generated large-scale game data to assist researchers in gaining deeper insights into the auction ecosystem. This data can be used to model the environment and to train the auto-bidding agents effectively.\\n\\nThe dataset contains 10 million ad opportunities, including 21 advertising episodes. Each episode contains more than 500,000 ad opportunities, divided into 48 steps. Each opportunity includes the top 48 agents with the highest bids. The dataset comprises over 500 million records, totaling 80 GB in size. Each record includes information such as the predicted value, bid, auction, and impression results, among other details. The specific data format and data samples of the dataset are included in Appendix C.\\n\\nWe have conducted an analysis of the AuctionNet Dataset to provide some insights. We first investigate the variation of impression values over time within a single day. We selected five categories from the AuctionNet Dataset and denote them as Category 1, Category 2, and so on. As shown in Figure 8, the impression values of different categories exhibit distinct patterns of variation. Given the budget constraint, agents should consider the variation in impression values over time to bid for appropriate impressions at the optimal times. Furthermore, we examine the relations between the values of different categories. The relations between Category 1 and other categories are illustrated in Figure 9.\\n\\nReal-world data show that 48 agents can ensure competitive pressure for auto-bidding agent training.\"}"}
{"id": "OTjTKFk7gb", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Category | Time | Density |\\n|----------|------|---------|\\n| Category 1 | 0.06 | 0.08 |\\n| Category 2 | 0.6 | 0.8 |\\n| Category 3 | 0.4 | 0.2 |\\n| Category 4 | 0.0 | 0.0 |\\n| Category 5 | 0.06 | 0.08 |\\n\\nFigure 8: The joint value distribution between different categories and time in the dataset.\\n\\nThe impression values of Category 1 and Category 3 are positively correlated, indicating that the corresponding advertisers are competitors for similar ad opportunities. Therefore, considering the preferences of other agents may be beneficial for developing better bidding strategies. The full datasheet of the dataset is included in Appendix B.\\n\\n5 Performance Evaluations of Baseline Algorithms\\n\\nIn this section, we evaluate the performance of baseline algorithms, such as linear programming, reinforcement learning, and generative models. It is important to note that we used the original algorithms from the papers and did not perform any special optimization on the methods specifically for the auto-bidding tasks. We provide a brief introduction to these baselines. The idea of the PID Controller is straightforward: it uses three parameters, $\\\\lambda_P$, $\\\\lambda_I$, and $\\\\lambda_D$, for Proportional Control, Integral Control, and Derivative Control, respectively. In this baseline, the PID Controller is employed to control the cost or bids of agents. Online LP utilizes linear programming for the auto-bidding problem. At each time step, Online LP solves a knapsack problem using a greedy algorithm. IQL is an offline RL algorithm. The core idea behind IQL is to evaluate the offline Q-function only on actions that appeared in the offline data, thereby avoiding overestimation in out-of-distribution data. Behavior Cloning (BC) is a supervised learning algorithm that uses expert trajectories. The agent's policy is learned by predicting the expert's actions in the state of given trajectories. Decision Transformer (DT) leverages the capabilities of the Transformer model for sequential decision-making. DT treats the trajectories in a MDP as sequences and predicts actions based on previous transitions. More generative models such as AIGB will also be integrated into baseline algorithms in the future. To better illustrate the performances, we add a heuristic method, Abid, to the experiments. Abid means the agent will give a fixed bid rate for all impressions. Its performance can be seen as a reference in comparison. More details of the evaluation can be found in Appendix A.\\n\\nThe empirical results are included in Figure 10. For better illustration, we normalize the performances of all baselines by the mean episode reward of the heuristic baseline Abid. Therefore, the mean relative performance of Abid is $1.0$ in the basic task. Online LP achieves the best performance, possibly because it is relatively robust and does not require special adaptation for auto-bidding tasks to achieve good results. Although methods like IQL and BC perform not as well as Online LP, we observe that proposing optimized solution can significantly optimize the performance, proving that such methods have great potential for optimization. In addition, the drop in rewards observed for all baselines during the target CPA task is due to the CPA penalty for exceeding constraints in (4).\\n\\n6 Applications\\n\\nAuctionNet has powered the NeurIPS 2024 competition \\\"Auto-bidding in Large-Scale Auctions\\\". The competition addressed the critical issue of making high-frequency bid decision-making in uncertain and competitive environments and attracted more than 1,500 teams.\"}"}
{"id": "OTjTKFk7gb", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Simulation environments have been widely applied in decision-making research and have successfully promoted the development of related studies [6, 24, 32, 27, 29]. However, simulation environments for real-world online advertising platforms are relatively scarce in the bid decision-making field. AuctionGym [18] models the bidding problem as a contextual bandit problem [2], where the advertiser decides the bidding value given the information of the ad opportunity as context. The contextual bandit has only one time step per episode, meaning that AuctionGym does not consider budget constraints in auto-bidding. Moreover, AuctionGym describes the auto-bidding problem from a single-agent perspective and ignores the influence of other agents. AdCraft [11] is a simulation environment for the bidding problem in Search Engine Marketing (SEM). Although AdCraft explicitly models the influences of other agents, these agents' policies are sampled from parameterized distributions, which cannot fully reflect the multi-agent nature of this problem. Despite the points discussed above, these existing simulation environments lack data-driven methods for modeling real-world online advertising platforms.\\n\\n8 Conclusion and Limitations\\n\\nWe present AuctionNet, a benchmark for bid decision-making in large-scale ad auctions derived from a real-world online advertising platform. AuctionNet consists of three components: an ad auction environment augmented with verified deep generative networks, a pre-generated dataset based on this environment, and performance evaluations of several baseline bid decision-making algorithms. The AuctionNet not only provides researchers with the opportunity to study auto-bidding algorithms in large-scale auctions, but also helps researchers and practitioners in game theory, reinforcement learning, generative models, operations optimization, and other fields to solve a wide range of decision-making research problems. Regarding limitations, while the generated data in the AuctionNet environment and the real-world data are similar in general, there are biases in some details, and the performance of the generative model can be improved.\\n\\n9 Acknowledgments\\n\\nThis work was supported in parts by NSFC under grants 62450001 and 62476008 and Alibaba Group through Alibaba Innovative Research Program. The authors would like to thank the anonymous reviewers for their valuable comments and advice.\"}"}
{"id": "OTjTKFk7gb", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] NeurIPS 2024 Competition: Auto-Bidding in Large-Scale Auctions. https://tianchi.aliyun.com/specials/promotion/neurips2024_alimama#/?lang=en_us.\\n\\n[2] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In International conference on machine learning, pages 127\u2013135. PMLR, 2013.\\n\\n[3] Santiago R Balseiro, Omar Besbes, and Gabriel Y Weintraub. Repeated auctions with budgets in ad exchanges: Approximations and design. Management Science, 61(4):864\u2013884, 2015.\\n\\n[4] S. Bennett. Development of the pid controller. IEEE Control Systems Magazine, 13(6):58\u201362, 1993.\\n\\n[5] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.\\n\\n[6] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.\\n\\n[7] Ioannis Caragiannis, Christos Kaklamanis, Panagiotis Kanellopoulos, and Maria Kyropoulou. On the efficiency of equilibria in generalized second price auctions. In Proceedings of the 12th ACM conference on Electronic commerce, pages 81\u201390, 2011.\\n\\n[8] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084\u201315097, 2021.\\n\\n[9] Benjamin Edelman, Michael Ostrovsky, and Michael Schwarz. Internet advertising and the generalized second-price auction: Selling billions of dollars worth of keywords. American economic review, 97(1):242\u2013259, 2007.\\n\\n[10] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 III, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64(12):86\u201392, 2021.\\n\\n[11] Maziar Gomrokchi, Owen Levin, Jeffrey Roach, and Jonah White. Adcraft: An advanced reinforcement learning benchmark environment for search engine marketing optimization. arXiv preprint arXiv:2306.11971, 2023.\\n\\n[12] Jiayan Guo, Yusen Huo, Zhilin Zhang, Tianyu Wang, Chuan Yu, Jian Xu, Bo Zheng, and Yan Zhang. Aigb: Generative auto-bidding via conditional diffusion modeling. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 5038\u20135049, 2024.\\n\\n[13] Ben Hambly, Renyuan Xu, and Huining Yang. Recent advances in reinforcement learning in finance. Mathematical Finance, 33(3):437\u2013503, 2023.\\n\\n[14] Eric A Hansen, Daniel S Bernstein, and Shlomo Zilberstein. Dynamic programming for partially observable stochastic games. In AAAI, volume 4, pages 709\u2013715, 2004.\\n\\n[15] Xiaotian Hao, Zhaoqing Peng, Yi Ma, Guan Wang, Junqi Jin, Jianye Hao, Shan Chen, Rongquan Bai, Mingzhou Xie, Miao Xu, et al. Dynamic knapsack optimization towards efficient multi-channel sequential advertising. In International Conference on Machine Learning, pages 4060\u20134070. PMLR, 2020.\\n\\n[16] Yue He, Xiujun Chen, Di Wu, Junwei Pan, Qing Tan, Chuan Yu, Jian Xu, and Xiaoqiang Zhu. A unified solution to constrained bidding in online display advertising. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 2993\u20133001, 2021.\\n\\n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.\"}"}
{"id": "OTjTKFk7gb", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[108x710][119] Olivier Jeunen, Sean Murphy, and Ben Allison. Off-policy learning-to-bid with auctiongym. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 4219\u20134228, 2023.\\n\\n[119] Ian T Jolliffe and Jorge Cadima. Principal component analysis: a review and recent developments. Philosophical transactions of the royal society A: Mathematical, Physical and Engineering Sciences, 374(2065):20150202, 2016.\\n\\n[119] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\\n\\n[119] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. In Deep RL Workshop NeurIPS 2021, 2021.\\n\\n[119] Haoming Li, Yusen Huo, Shuai Dou, Zhenzhe Zheng, Zhilin Zhang, Chuan Yu, Jian Xu, and Fan Wu. Trajectory-wise iterative reinforcement learning framework for auto-bidding. In Proceedings of the ACM on Web Conference 2024, pages 4193\u20134203, 2024.\\n\\n[119] Brendan Lucier, Renato Paes Leme, and \u00c9va Tardos. On revenue in the generalized second price auction. In Proceedings of the 21st international conference on World Wide Web, pages 361\u2013370, 2012.\\n\\n[119] C Berner OpenAI, Greg Brockman, Brooke Chan, Vicki Cheung, P Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\\n\\n[119] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.\\n\\n[119] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234\u2013241. Springer, 2015.\\n\\n[119] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nan tas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.\\n\\n[119] Yumin Su, Min Xiang, Yifei Chen, Yanbiao Li, Tian Qin, Hongyi Zhang, Yasong Li, and Xiaobing Liu. Spending programmed bidding: Privacy-friendly bid optimization with roi constraint in online advertising. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 5731\u20135740, 2024.\\n\\n[119] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2012.\\n\\n[119] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization, 2018.\\n\\n[119] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\n[119] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Jun-young Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350\u2013354, 2019.\\n\\n[119] Di Wu, Xiujun Chen, Xun Yang, Hao Wang, Qing Tan, Xiaoxun Zhang, Jian Xu, and Kun Gai. Budget constrained bidding by model-free reinforcement learning in display advertising. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 1443\u20131451, 2018.\"}"}
{"id": "OTjTKFk7gb", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yuxin Wu, Tianyang Zhao, Haoyuan Yan, Min Liu, and Nian Liu. Hierarchical hybrid multi-agent deep reinforcement learning for peer-to-peer energy trading among multiple heterogeneous microgrids. IEEE Transactions on Smart Grid, 2023.\\n\\nWei Zhang, Yanjun Han, Zhengyuan Zhou, Aaron Flores, and Tsachy Weissman. Leveraging the hints: Adaptive bidding in repeated first-price auctions. Advances in Neural Information Processing Systems, 35:21329\u201321341, 2022.\\n\\nWeinan Zhang, Yifei Rong, Jun Wang, Tianchi Zhu, and Xiaofan Wang. Feedback control of real-time display advertising. In Proceedings of the Ninth ACM International Conference on Web Search and Data Mining, pages 407\u2013416, 2016.\\n\\nYiheng Zhu, Yang Zhan, Xuankun Huang, Yuwei Chen, Jiangwen Wei, Wei Feng, Yinzhi Zhou, Haoyuan Hu, Jieping Ye, et al. Ofcourse: A multi-agent reinforcement learning environment for order fulfillment. Advances in Neural Information Processing Systems, 36, 2024.\"}"}
{"id": "OTjTKFk7gb", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Evaluation Details\\n\\nThere are 48 agents of 7 types in our experiments and each type corresponds to one algorithm. We test 7 rounds where we permute the order of agents in each round. Therefore, agents will represent different advertisers with different budgets in different rounds. We choose the best agent as the representative of an algorithm if there are multiple agents of this algorithm. We use the average performances of the 7 rounds as the final performance of all the algorithms. We provide the model file of these agents and the evaluation code for reproduction.\\n\\nB Datasheet for AuctionNet\\n\\nWe present a datasheet for the AuctionNet Dataset.\\n\\nB.1 Motivation\\n\\nFor what purpose was the dataset created?\\n\\nIn general, learning from interactions with the real-world online advertising platforms is difficult and expensive, so offline RL algorithms are more popular in auto-bidding. Therefore, we build the Auction Dataset to facilitate offline training of users. Moreover, the Auction Dataset will also be provided to the participants of the competition we will hold in the future.\\n\\nWho created the dataset?\\n\\nThe dataset was created by the authors of this paper. The dataset was not created on the behalf of any entity.\\n\\nWho funded the creation of the dataset?\\n\\nAlibaba Group funds the creation of the AuctionNet Dataset.\\n\\nB.2 Composition\\n\\nWhat do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?\\n\\nThe AuctionNet Dataset contains trajectories of diverse agents competing with each other. Please refer to Appendix C and Section 4.2 for more details.\\n\\nIs there a label or target associated with each instance?\\n\\nThe AuctionNet Dataset contains offline trajectories where the actions or bids of agents can be seen as labels for the time step.\\n\\nIs any information missing from individual instances?\\n\\nNot to our knowledge.\\n\\nAre there recommended data splits (e.g., training, development/validation, testing)?\\n\\nNo.\\n\\nAre there any errors, sources of noise, or redundancies in the dataset?\\n\\nThe AuctionNet Dataset contains trajectories of diverse agents, some of these agents may not perform well. However, the tasks in the environment are still difficult for some algorithms and we think keeping agents diverse in the AuctionNet Dataset is beneficial.\\n\\nDo/did we do any data cleaning on the dataset?\\n\\nWe did not. All data is presented exactly as collected.\\n\\nB.3 Collection Process\\n\\nHow was the data associated with each instance acquired?\\n\\nThe AuctionNet Dataset is collected from the interactions of baseline agents in the environment.\"}"}
{"id": "OTjTKFk7gb", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Who was involved in the data collection process and how were they compensated?\\n\\nThe data collection process is done by the authors and not involve with any crowdsource.\\n\\nOver what timeframe was the data collected?\\n\\nThe AuctionNet Dataset was collected between March 2024 and May 2024.\\n\\nB.4 Uses\\n\\nHas the dataset been used for any tasks already?\\n\\nNo.\\n\\nIs there a repository that links to any or all papers or systems that use the dataset?\\n\\nNo.\\n\\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?\\n\\nWe do not believe so since the AuctionNet Dataset consists of data generated by the interactions of baseline agents.\\n\\nB.5 Distribution\\n\\nWill the dataset be distributed to third parties?\\n\\nYes, but the AuctionNet Dataset and environment are involved with a large competition we will hold in NeurIPS 2024, so we will not distribute them until the end of the competition considering competition fairness. However, we will open-source the AuctionNet Dataset as soon as possible.\\n\\nHow will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?\\n\\nThe AuctionNet Dataset will be distributed by a Github link after the end of the competition we will hold. The AuctionNet Dataset doesn't have a digital object identifier now.\\n\\nAll data is under the MIT license.\\n\\nHave any third parties imposed IP-based or other restrictions on the data associated with the instances?\\n\\nNo.\\n\\nDo any export controls or other regulatory restrictions apply to the dataset or to individual instances?\\n\\nNo.\\n\\nB.6 Maintenance\\n\\nWho will be supporting/hosting/maintaining the dataset?\\n\\nThe authors of this paper will provide needed maintenance to the datasets.\\n\\nHow can the owner/curator/manager of the dataset be contacted (e.g., email address)?\\n\\nPlease email us at huoyusen.huoyusen@alibaba-inc.com.\\n\\nIs there an erratum?\\n\\nThere is not and we believe generated features, predicted values, and trajectories in our datasets do not involve an erratum.\\n\\nWill the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?\\n\\nYes, but as we won\u2019t add extra data points, the update will be minimal.\"}"}
{"id": "OTjTKFk7gb", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The specific data format of the AuctionNet Dataset is as follows:\\n\\n- **c1. deliveryPeriodIndex**: The index of the current delivery period.\\n- **c2. advertiserIndex**: The unique identifier of the advertiser.\\n- **c3. advertiserCategoryIndex**: The index of the advertiser's category.\\n- **c4. budget**: The advertiser's budget for a period.\\n- **c5. CPAConstraint**: The CPA constraint of the advertiser.\\n- **c6. timeStepIndex**: The index of the current decision time step.\\n- **c7. remainingBudget**: The advertiser's remaining budget before the current step.\\n- **c8. pvIndex**: The index of the ad opportunity.\\n- **c9. pValue**: The conversion probability when the ad is exposed to the user.\\n- **c10. pValueSigma**: The variance of predicted probability.\\n- **c11. bid**: The agent's bid of the ad opportunity.\\n- **c12. xi**: The winning status of the agent of the ad opportunity.\\n- **c13. adSlot**: The won ad slot.\\n- **c14. cost**: The cost needs to be paid if the ad is exposed to the user.\\n- **c15. isExposed**: The indicator signifying whether the ad in the slot was displayed to the user.\\n- **c16. conversionAction**: The indicator signifying whether the conversion action has occurred.\\n- **c17. leastWinningCost**: The minimum cost to win the ad opportunity.\\n- **c18. isEnd**: The completion status of the advertising period.\\n\\nTable 2 presents an ad opportunity involving the top five advertisers. The top three advertisers, numbered 31, 22, and 15, won the ad opportunity with the highest bids and were allocated to ad slots 1, 2, and 3, respectively. During this impression, slots 1 and 2 were exposed to the user, while slot 3 remained unexposed. Consequently, ads in slots 1 and 2 need to pay 0.2702 and 0.2154, respectively. Additionally, the user engaged in a conversion action with the ad in slot 2.\\n\\nTable 2: Bidding, auction, and impression processes for each advertiser during the same opportunity.\\n\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |\\n|   |   |   |   |   |   |   |   |   |   |   |   |   |"}
{"id": "OTjTKFk7gb", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 3: An advertiser's bidding process across time steps.\\n\\n| c1 | c2 | c3 | c4 | c5 | c6 | c7 | c8 | c9 | c10 | c11 | c12 | c13 | c14 | c15 | c16 | c17 | c18 |\\n|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|\\n| 3  | 48 | 6  | 7500.00 | 40.00 | 1 | 7500.00 | 1 | 0.0032157 | 0.0003567 | 0.1345 | 0 | 0 | 0 | 0 | 0.1628 | 0 |\\n| 3  | 48 | 6  | 7500.00 | 40.00 | 1 | 7500.00 | 2 | 0.0146256 | 0.0021352 | 0.5852 | 0 | 0 | 0 | 0 | 0.6421 | 0 |\\n| 3  | 48 | 6  | 7500.00 | 40.00 | 1 | 7500.00 | 3 | 0.0054324 | 0.0007631 | 0.1924 | 1 | 1 | 0.1673 | 1 | 0.1454 | 0 |\\n| 3  | 48 | 6  | 7500.00 | 40.00 | 1 | 7500.00 | 4 | 0.0073145 | 0.0006529 | 0.2786 | 0 | 0 | 0 | 0 | 0.2862 | 0 |\\n| 3  | 48 | 6  | 7500.00 | 40.00 | 2 | 7341.25 | 20901 | 0.0076453 | 0.0006579 | 0.2856 | 0 | 0 | 0 | 0 | 0.3245 | 0 |\\n| 3  | 48 | 6  | 7500.00 | 40.00 | 2 | 7341.25 | 20902 | 0.0139234 | 0.0012358 | 0.5629 | 1 | 2 | 0.6782 | 0 | 0 | 0 |\\n| 3  | 48 | 6  | 7500.00 | 40.00 | 2 | 7341.25 | 20903 | 0.0077212 | 0.0006579 | 0.3045 | 0 | 0 | 0 | 0 | 0.3122 | 0 |\\n| 3  | 48 | 6  | 7500.00 | 40.00 | 2 | 7341.25 | 20904 | 0.0021341 | 0.0001873 | 0.0926 | 0 | 0 | 0 | 0 | 0.1151 | 0 |\\n| 3  | 48 | 6  | 7500.00 | 40.00 | 43 | 0.00 | 895201 | 0.0065274 | 0.0005689 | 0.0000 | 0 | 0 | 0 | 0 | 0.1243 | 1 |\\n| 3  | 48 | 6  | 7500.00 | 40.00 | 43 | 0.00 | 895202 | 0.0032125 | 0.0002986 | 0.0000 | 0 | 0 | 0 | 0 | 0.2986 | 1 |\\n| 3  | 48 | 6  | 7500.00 | 40.00 | 43 | 0.00 | 895203 | 0.0112986 | 0.0013253 | 0.0000 | 0 | 0 | 0 | 0 | 0.0932 | 1 |\\n| 3  | 48 | 6  | 7500.00 | 40.00 | 43 | 0.00 | 895204 | 0.0051678 | 0.0006782 | 0.0000 | 0 | 0 | 0 | 0 | 0.1687 | 1 |\\n\\n- **(c2). idGender**: Represents the gender of the user. The meanings of values: 0 for unknown, 1 for Female, and 2 for Male. Data format: one-hot vector, dimension [9, 12].\\n\\n- **(c3). isForeign**: Represents whether the user is foreign. The meanings of values: 0 for unknown, 1 for No, and 2 for Yes. Data format: one-hot vector, dimension [12, 15].\\n\\n- **(c4). cityLevel**: Represents the level of the city where the user is living. The meanings of values: 0 for unknown, 1 to 6 for different city development levels in descending order. Data format: one-hot vector, dimension [15, 22].\\n\\n- **(c5). isCap**: Represents whether the city the user living in is the capital. The meanings of values: 0 for No, and 1 for Yes. Data format: one-hot vector, dimension [22, 24].\\n\\n- **(c6). buyerStarName**: Represents the rating of the user as a buyer. The meanings of values: 0 for unknown, 1 to 5 for stars respectively, 6 to 10 for diamonds respectively, 11 to 15 for crowns respectively, 16 to 20 for golden crowns respectively, 21 for credit score \u2264 3, and 22 for credit score = 0. In general, the order of these values' ratings is 22 < 21 < 2 < ... < 20. Data format: one-hot vector, dimension [24, 47].\\n\\n- **(c7). tmLevel**: Represents the VIP level of the user in Tmall. The meanings of values: 0 for unknown or no VIP level, 1 to 5 for VIP levels 1 to 5 respectively. Data format: one-hot vector, dimension [47, 53].\\n\\n- **(c8). vipLevelName**: Represents the VIP level of the user in Taobao. The meanings of values: 0 for unknown or no VIP level, 1 to 8 for VIP levels 1 to 8 respectively. Data format: one-hot vector, dimension [53, 62].\\n\\n- **(c9). phonePriceLevelPrefer**: Represents the preferred phone price interval of the user. The meanings of values: 0 for unknown, 1 for 0 to 650 CNY, 2 for 1100 to 1700 CNY, 3 for 1700 to 1900 CNY, 4 for 1900 to 2600 CNY, 5 for 2600 to 3500 CNY, 6 for 3500 to 5000 CNY, 7 for 5000 to 8000 CNY, 8 for 650 to 1100 CNY, 9 for higher than 8000 CNY. Data format: one-hot vector, dimension [62, 72].\\n\\n- **(c10). zipCode**: Represents the zip code of the address where the user is living. The meanings of values: The zip code contains 6 digits and each digit is a number from 0 to 9. We encode each digit with an one-hot vector and concatenate them together. Data format: one-hot vector, dimension [72, 132].\\n\\n- **(c11). idBirthyear**: Represents the birthyear of the user. Data format: integer, dimension [132, 133].\\n\\n- **(c12). nationId**: Represents the nation of the user. The meanings of values: 1 for China. (The real-world training data contains almost no data points from other countries. So does our generated data.) Data format: integer, dimension [133, 134].\\n\\n- **(c13). payOrdAmt**: Represents the order amounts of the user in the last one month, one year, three months and six months. Data format: float numbers, dimension [134, 138].\\n\\n- **(c14). payOrdCnt**: Represents the user's number of orders in the last one month, one year, three months and six months. Data format: integers, dimension [138, 142].\\n\\n- **(c15). payOrdDays**: Represents the number of days when the user placed orders in the last one month, one year, three months and six months. Data format: integers, dimension [142, 146].\\n\\n- **(c16). payOrdItmCnt**: Represents the number of item types the user bought in the last one month, one year, three months and six months. Data format: integers, dimension [146, 150].\"}"}
{"id": "OTjTKFk7gb", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(c17). payOrdItmQty: Represents the number of items the user bought in the last one month, one year, three months and six months. Data format: integers, dimension [150, 154].\\n\\n(c18). pvAndIpv: Represents the PV and IPV value of the user bought in the last month. Data format: float numbers, dimension [154, 156].\\n\\n(c19). vstSlrCnt: Represents the number of sellers the user visited in the last month. Data format: integers, dimension [156, 157].\\n\\n(c20). vstCateCnt: Represents the number of categories the user visited in the last month. Data format: integers, dimension [157, 158].\\n\\n(c21). vstItmCnt: Represents the number of items the user visited in the last month. Data format: integers, dimension [158, 159].\\n\\n(c22). vstItmCnt: Represents the number of items the user visited in the last month. Data format: integers, dimension [158, 159].\\n\\n(c23). vstDays: Represents the number of days the user visited items in the last month. Data format: integers, dimension [159, 160].\\n\\n(c24). stayTimeLen: Represents the number of seconds the user spent on visiting items in the last month. Data format: integers, dimension [160, 161].\\n\\n(c25). cartItmCnt: Represents the number of items the user added to the cart in the last one week, two weeks, one month, three months, and six months. Data format: integers, dimension [161, 166].\\n\\n(c26). cltSlrCnt: Represents the number of sellers the user collected in the last one week, two weeks, one month, six months, and one year. Data format: integers, dimension {166, 168, 170, 172, 174}.\\n\\n(c27). cltItmCnt: Represents the number of items the user collected in the last one week, two weeks, one month, six months, and one year. Data format: integers, dimension {167, 169, 171, 173, 175}.\\n\\nStructure of the value vector. The value vector has 60 dimensions corresponding to 59 categories involved in our environment and one conserved category for the undefined or unknown category. The corresponding relations between the dimension indexes and categories are listed in Table 4.\\n\\nE. Tasks\\n\\nThough AuctionNet provides a general framework for auto-bidding problem studies, we choose two typical scenarios in auto-bidding as tasks in AuctionNet for easier understanding.\\n\\nE.1 Basic Task\\n\\nOur basic task is based on the scenario Budget Constrained Bidding (BCB) [33], where agents maximize their obtained values within the constraint on the budget. The optimization formulation of BCB from agent $i$'s perspective is as follows:\\n\\n$$\\\\text{maximize} \\\\langle x_{t}, v_{t} \\\\rangle$$\\n\\nsubject to $x_{t} \\\\leq \\\\omega_{i}$, $\\\\forall t$.\\n\\nwhere $x_{t} = (x_{1}, x_{2}, \\\\cdots, x_{m})$ is the auction result of all ad opportunities for agent $i$ in time step $t$, $v_{t} = (v_{1}, v_{2}, \\\\cdots, v_{m})$ is the value for agent $i$, $c_{t} = (c_{1}, c_{2}, \\\\cdots, c_{m})$ is the cost in time step $t$, $b_{i}$ is the budget for agent $i$, and $\\\\langle \\\\cdot \\\\rangle$ is the inner product.\\n\\nAs for the implementation, we know from our problem formulation that $r_{i}(s, a_{t}) = \\\\langle x_{t}, v_{t} \\\\rangle$, so the objective in the optimization formulation is the same as the objective $\\\\sum_{t=1}^{T} r_{i}(s_{t}, a_{t})$ in the RL formulation. The budget constraint is guaranteed by ignoring the bids exceeding agents' budgets in the environment. Therefore, BCB corresponds to the default setting of AuctionNet.\"}"}
{"id": "OTjTKFk7gb", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Corresponding relations for categories.\\n\\n| ID | Category                      |\\n|----|-------------------------------|\\n| 1  | Snacks                        |\\n| 2  | Personal Care                 |\\n| 3  | Electric Vehicles             |\\n| 4  | Tmall Underwear               |\\n| 5  | Smart Toys & Games            |\\n| 6  | Tea                           |\\n| 7  | Household Cleaning            |\\n| 8  | Chilled Food                  |\\n| 9  | Tmall Women's Clothing        |\\n| 10 | Enterprise Services           |\\n| 11 | Dairy Products                |\\n| 12 | Fragrances and Aromatherapy   |\\n| 13 | Life Services                 |\\n| 14 | Household Appliances          |\\n| 15 | Taobao Men's Clothing         |\\n| 16 | Tmall Home Decor              |\\n| 17 | Taobao Home & Living          |\\n| 18 | Taobao Home Decor             |\\n| 19 | Instant Drinks                |\\n| 20 | Alcohol                       |\\n| 21 | Taobao Women's Clothing       |\\n| 22 | Auto Aftermarket              |\\n| 23 | Fruits and Vegetables         |\\n| 24 | Flowers and Gardening         |\\n| 25 | Office & School Supplies      |\\n| 26 | Computers                     |\\n| 27 | Tmall Watches & Glasses       |\\n| 28 | Computer Accessories          |\\n| 29 | Aquatic Products, Meat, Poultry & Eggs |\\n| 30 | Cosmetics                     |\\n| 0  | Other                         |\\n\\nE.2 Target CPA Task\\n\\nWe propose Target CPA Task based on the real-world scenario Target CPA (Cost Per Action) with some simplifications for understanding. The CPA of agent $i$ is defined as $\\\\text{cpa}_i = \\\\frac{\\\\langle x_{ti}, c_{ti} \\\\rangle}{\\\\langle x_{ti}, v_{ti} \\\\rangle}$, which can be seen as the cost taken by agent $i$ for unit value. A low CPA means the budgets are consumed to obtain values effectively. Based on the basic task, Target CPA Task adds one more constraint on CPA that $\\\\text{cpa}_i$ should be lower than the desired CPA $d_i$. The formulation is as follows:\\n\\n$$\\\\max \\\\{ \\\\alpha_t \\\\} \\\\quad \\\\text{subject to} \\\\quad \\\\langle x_{ti}, v_{ti} \\\\rangle, \\\\langle x_{ti}, c_{ti} \\\\rangle \\\\leq \\\\omega_i, \\\\text{cpa}_i \\\\leq d_i.$$\\n\\n(3)\\n\\nGiven that CPA can only be calculated at the end of one episode, the environment will only provide a sparse reward in Target CPA Task, which is different from the basic task. Unlike the budget constraint which cannot be violated in the environment, we allow agents to violate the CPA constraint, but we will penalize those agents for violations on their obtained values based on their CPA $\\\\text{cpa}_i$. The sparse reward formulation in Target CPA Task is as follows:\\n\\n$$r_{CSB_i} = p(\\\\text{cpa}_i; d_i) \\\\quad \\\\text{subject to} \\\\quad \\\\langle x_{ti}, v_{ti} \\\\rangle, \\\\langle x_{ti}, c_{ti} \\\\rangle.$$\\n\\n(4)\\n\\nhttps://support.google.com/google-ads/answer/6268632\"}"}
{"id": "OTjTKFk7gb", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $p(c_{pa}^i; d^i) = \\\\min d^i c_{pa}^i \\\\beta$, $\\\\beta > 0$ is typically set to 3. Therefore, Target CPA Task can be implemented with modifications to the reward function in the basic task.\\n\\n### Baseline Algorithms\\n\\n**PID Controller.**\\n\\nPID Controller is a traditional algorithm in the control field with a long history. It is simple but effective in many scenarios. Recently, PID Controller has also been adopted in online advertising. The idea of PID Controller is straightforward: PID Controller takes three parameters $\\\\lambda_P$, $\\\\lambda_I$, and $\\\\lambda_D$ for Proportional Control, Integral Control, and Derivative Control, respectively. We use the PID Controller to control the cost or bids of agents in this baseline.\\n\\n**Online LP.**\\n\\nThe optimization formulation (2) is a typical Linear Programming (LP) problem. Moreover, the variable $x_{tij} \\\\in \\\\{0, 1\\\\}$ is binary, so the problem in each time step can be converted to a dynamic knapsack problem. Online LP solves this dynamic knapsack problem using a greedy algorithm.\\n\\n**IQL.**\\n\\nImplicit Q-learning (IQL) is an offline RL algorithm. The idea of IQL is evaluating offline Q-function only on the actions that appeared in the offline data, to avoid the overestimation in the out-of-distribution data. In practice, IQL utilizes expectile regression to realize the offline Q-learning on in-distribution data.\\n\\n**Behavior Cloning.**\\n\\nBehavior Cloning (BC) is a supervised learning algorithm given expert trajectories. The agent's policy learns by predicting the expert's action in the state of given trajectories. BC is a baseline for verifying the effectiveness of RL algorithms.\\n\\n**Decision Transformer.**\\n\\nDecision Transformer (DT) utilizes the ability of Transformer for sequential decision-making. DT views the trajectories in MDP as a sequence and predicts actions given previous transitions.\\n\\n### Implementation and Modules\\n\\nThe environment of AuctionNet consists of three main modules: the ad opportunity generation module, the auction module, and the bidding module. The general process of one time step in AuctionNet can be concluded as follows:\\n\\n1) The ad opportunity generation module generates features $u = (u_1, u_2, \\\\ldots, u_m)$ and values $v = \\\\{v_{ij}\\\\}$ of $m$ ad opportunities for $n$ agents, where the number of ad opportunities $m$ is sampled from an internal distribution within AuctionNet. This internal distribution is obtained from real-world online advertising statistics.\\n\\n2) Agents bid for all the ad opportunities considering the predicted values provided by the environment and the historical auction logs.\\n\\n3) The auction module determines the winner of each auction, rewards, and costs by the auction mechanism.\\n\\n4) Agents receive rewards, costs, and new auction logs. The budgets of all the agents are updated according to auction results. In the next time step, all the processes above will be repeated.\"}"}
{"id": "OTjTKFk7gb", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"G.1 Ad Opportunity Generation Module\\n\\nThe target of the ad opportunity generation module is to generate diverse ad opportunity features similar to real online advertising data. The core of this module is the generative model. The objective of the generative model in AuctionNet is to generate data resembling real advertising delivery data. Useful information in the real advertising delivery data can be divided into four parts: features of ad opportunities (users' information), features of advertisers, time when the ad opportunity arises, and the values of the ad opportunities. In our model, we simplify the feature of advertisers to be the advertisers' industry categories. We focus on the generation of ad opportunity features and take the categories and time as conditions. The generative model consists of two components: the generative model for ad opportunity features and the prediction model for the values.\\n\\nFeature Generation.\\n\\nThe ad opportunity feature contains two parts of information: the basic identity information of users and the consumption records such as the consumption amount. The identity information is discrete and the consumption records are continuous in general, which are processed with different measures. Diffusion [17] model is the most popular generative model recently which obtains SOTA performances in image generation with a simplified training process. We would like to adopt the diffusion model to generate the ad opportunity feature but struggle with the denoising operation which can result in unreasonable outputs such as a negative consumption amount. So we follow the idea of the Latent Diffusion Model (LDM)[25] to generate ad opportunity features. LDM adds noises and denoises in the latent space with a diffusion model and generates data from the latent space with an encoder and decoder. More details can be found in Appendix H.1.\\n\\nValue Prediction.\\n\\nThe value prediction model needs to handle three types of information: ad opportunity features, the industry category information of advertisers, and time information. We simplify the category and time information as discrete values. Therefore, we aim to integrate the category and time information into the ad opportunity features for better value prediction. Besides, we hope this integration can partly reflect the variation pattern of the impression values related to advertisers' features and time. Multi-head attention (MHA), as a popular network architecture and the critical part of Transformer [31], can capture the relations among a sequence, thus we hope to utilize MHA for better integration. We combine cross-attention and self-attention to integrate the three types of information. We also follow the idea of position embedding in the Transformer to process the time information. More details are included in Appendix H.2.\\n\\nFor the consideration of interaction efficiency in AuctionNet, the environment utilizes a dataset consisting of generated features and corresponding predicted values. More details of the dataset will be discussed in Section 4.1. Though the ad opportunity generation module is trained with real online advertising data, an important question is whether the generated data can reflect the properties of real data. Therefore, we have done several related experiments and the empirical results will also be discussed in Section 4.1.\\n\\nG.2 Auction Module\\n\\nThe task of the auction module is to determine the winner and the winning price given all bids of agents for the ad ad opportunities. The costs of agents will change given different auction rules. The most commonly discussed auction rule is the Generalized Second-Price (GSP) Auction which means the winner should pay a cost slightly higher than the second-highest bid instead of the highest bid. The auction module internally supports several popular auction rules including GSP for the convenience of researchers. Besides, researchers can also design a specific auction rule related to their purposes with the interface of the auction module.\\n\\nAdditionally, the property of multiple slots has been implemented in our simulation platform. Multiple slots emerge from the application in the industry, which means one ad opportunity has multiple ad slots for ad displays. The ad slots are ranked by their exposure rates. A higher exposure rate slot is more valuable for advertisers. Suppose the number of slots is $l$, then the auction module will attribute $l$ slots to the top $l$ bidders and these bidders will receive different values according to different exposure rates of slots. In the environment, $l$ is set to 3. Let $slot_{ij}$ represent the slot of ad opportunity $j$ won by agent $i$ and $e_{ij} \\\\in [0, 1]$ represent the exposure rate of $slot_{ij}$, then the optimization formulation of BCB with multiple slots is as follows:\"}"}
{"id": "OTjTKFk7gb", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[\\n\\\\maximize \\\\left\\\\{ \\\\alpha_t \\\\right\\\\}\\n\\\\]\\n\\\\[T \\\\times t = 1 \\\\]\\n\\\\[m \\\\times X_j = 1 \\\\]\\n\\\\[e_{tij} x_{tij} v_{tij} s_t. \\\\]\\n\\\\[T \\\\times t = 1 \\\\]\\n\\\\[m \\\\times X_j = 1 \\\\]\\n\\\\[e_{tij} x_{tij} c_{tij} \\\\leq \\\\omega_i, \\\\]\\n\\n(5)\\n\\nIn summary, the multiple slots property increases the complexity of the optimal bidding strategy, since the exposure rate is a discount factor for both the cost and values. For instance, a strategy using a lower budget to bid for slots with relatively lower ranks may be better than the strategy that always chases the highest value slot. We believe supporting multiple slots in AuctionNet will be beneficial to reducing the gap between related research and the real-world online advertising platforms.\\n\\nG.3 Bidding Module\\n\\nThe bidding module is responsible for processing the multi-agent interactions between advertisers. This module implements the budget constraint and models the auto-bidding problem with sequence decision-making. Therefore, AuctionNet supports the mainstream paradigms including Budget Constrained Bidding (BCB) \\\\[33\\\\] and Multiple Constraints Bidding (MCB) \\\\[16\\\\] in the auto-bidding field. This will help researchers validate and gain insights from existing algorithms.\\n\\nIn the bidding module, we explicitly model the multi-agent setting. Researchers can implement multi-agent algorithms to achieve competition or cooperation among different agents. The varying bidding strategies of other agents can better reflect the complex and dynamic auction environment in real-world online advertising platforms. Besides, researchers can only control a part of the agents in AuctionNet while others is uncontrollable. This scenario is closer to the real advertising platform. The multi-agent setting of AuctionNet can adapt to different research objectives.\\n\\nThere are several different metrics for different business goals of advertisers in online advertising platforms such as Return-on-Investment (ROI) and Return-On-Ad-Spend (ROAS). AuctionNet has several built-in metrics covering the popular metrics used by the major advertising platform. Researchers can adopt these metrics conveniently to evaluate the performances of their auto-bidding strategies. Besides, researchers can define customized metrics according to their research objectives. Additionally, several popular algorithms in the auto-bidding field have been implemented as baselines in AuctionNet, including PID Controller \\\\[36\\\\], Online LP \\\\[15\\\\], IQL \\\\[21\\\\], Behavior Cloning \\\\[30\\\\], and Decision Transformer \\\\[8\\\\]. This can facilitate the interested researchers to quickly start up and evaluate these baselines in a unified environment.\\n\\nH Details of Deep Generative Networks\\n\\nH.1 Ad Opportunity Generation\\n\\nThe ad opportunity feature contains two parts of information: one is the basic identity information of users including gender, age, address and so on; another is the consumption records such as the consumption amount and the number of orders in the last three months. The identity information consists of discrete fields and each field has several candidates. The consumption records are continuous in general. Therefore, we process these two types of information with different measures.\\n\\nDiffusion \\\\[17\\\\] model is the most popular generative model recently which obtains SOTA performances in image generation with a simplified training process. The principle of the diffusion model is adding Gaussian noises to original data in training and denoising from Gaussian noises in the generation process. We would like to adopt the diffusion model to generate the ad opportunity feature but struggle with the denoising operation which can result in unreasonable outputs such as a negative consumption amount. So we follow the idea of the Latent Diffusion Model (LDM) \\\\[25\\\\]. LDM has a latent space to encode the original data. LDM combines the idea of diffusion model with VAE \\\\[20\\\\]. LDM adds noises and denoises in the latent space with a diffusion model and generates data from the latent space with an encoder and decoder.\\n\\nSpecifically, let \\\\( U \\\\subset \\\\mathbb{R}^d \\\\) be the space of ad opportunity feature data \\\\( (u_1, u_2, \\\\cdots, u_K) \\\\) where \\\\( d \\\\) is the dimension of original data and \\\\( K \\\\) is the volume of the ad opportunity. Let \\\\( Y \\\\subset \\\\mathbb{R}^d' \\\\) be the latent space.\"}"}
{"id": "OTjTKFk7gb", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The encoder and decoder are represented as $g\\\\phi$ and $h\\\\psi$ respectively, where $\\\\phi$ and $\\\\psi$ are the parameters. The function of the encoder $g\\\\phi$ is obtaining a latent representation of original data as follows:\\n\\n$$g\\\\phi(u_k) = (\\\\mu_k, \\\\sigma_k^2), \\\\quad y_k \\\\sim N(\\\\mu_k, \\\\sigma_k^2),$$\\n\\nwhere $y_k \\\\in Y$ is the latent representation. In practice, the reparameterize trick \\\\cite{20} is applied to make sure this operation is differentiable in the backpropagation. Given the latent representation $y_k$, the decoder is responsible for reconstructing the original data from $y_k$, i.e., $h\\\\psi(y_k) = \\\\tilde{u}_k \\\\in U$. Besides the reconstruction, the latent distribution $N(\\\\mu_k, \\\\sigma_k^2)$ is expected to be close to the standard Gaussian distribution $N(0, 1)$. Therefore, we have the following loss function for the encoder and decoder:\\n\\n$$L_{\\\\text{recons}} = \\\\frac{1}{K} \\\\sum_{k=1}^{K} \\\\|u_k - h\\\\psi(y_k)\\\\|_2^2,$$\\n\\n$$L_{\\\\text{reg}} = \\\\frac{1}{K} \\\\sum_{k=1}^{K} D_{\\\\text{KL}} N(\\\\mu_k, \\\\sigma_k^2) \\\\|N(0, 1),$$\\n\\nwhere $L_{\\\\text{recons}}$ is the reconstruction loss and $L_{\\\\text{reg}}$ is the regularization loss for the latent distribution.\\n\\nDifferent from the original idea of VAE, where the latent variable $y \\\\in Y$ is sampled from $N(0, 1)$ in the generation process, LDM uses a diffusion model in the latent space to generate the latent variable. In general, the idea of the diffusion model is adding Gaussian noises to the original data to obtain variables in $N(0, 1)$ and denoising from $N(0, 1)$ for generation. Given a latent variable $y$, we denote the noisy version of $y$ after $p$ iterations as $y_p$. The diffusion model has a network to predict noise $\\\\epsilon_\\\\theta(y_p, p)$ and the loss function can be represented as\\n\\n$$L_{\\\\text{LDM}} = \\\\frac{1}{K} \\\\sum_{k=1}^{K} \\\\|\\\\epsilon - \\\\epsilon_\\\\theta(y_k, p_k)\\\\|_2^2,$$\\n\\nwhere $\\\\epsilon \\\\sim N(0, 1)$, $y_k$ is the latent embedding of $u_k$ and $p_k$ is uniformly sampled from the set $\\\\{1, 2, \\\\ldots, p_{\\\\text{max}}\\\\}$. $\\\\epsilon_\\\\theta(y_p, p)$ is the only learnable network in the diffusion model, with which the process of adding noises and denoising can be completed by the basic operations.\\n\\nAs for the generation process, a latent variable $\\\\bar{y}$ is sampled from $N(0, 1)$ and $\\\\tilde{y}$ is obtained by $p_{\\\\text{max}}$ denoising steps from $\\\\tilde{y}$ given the noise prediction network $\\\\epsilon_\\\\theta$. Finally, the decoder generates an ad opportunity feature based on $\\\\tilde{y}$ as $\\\\tilde{x} = h\\\\psi(\\\\tilde{y})$.\\n\\nH.2 Value Prediction\\n\\nThe value prediction model needs to handle three types of information: ad opportunity features, category information and time information. The category information corresponds to the industry categories of advertisers and the time information corresponds to the time when the ad opportunity arrived. In our model, the category and time information are simplified as discrete values. Therefore, we aim to integrate the category and time information into the ad opportunity features for better value prediction. Besides, we hope this integration can partly reflect the variation pattern of the impression values related to advertisers' features and time.\\n\\nMulti-head attention (MHA) \\\\cite{31} is a popular network architecture and the critical part of Transformer \\\\cite{31}. MHA can capture the relations among a sequence, thus we hope to utilize MHA for better integration. The formulation of the attention network is straightforward as\\n\\n$$\\\\text{Attention}(Q, K, V) = \\\\text{softmax}(\\\\frac{QK^T}{\\\\sqrt{d}}) \\\\cdot V.$$\\n\\nMulti-head attention can be viewed as applying the attention network in different representation subspaces as follows:\\n\\n$$\\\\text{MultiHead}(Q, K, V) = \\\\text{Concat}(\\\\text{head}_1, \\\\text{head}_2, \\\\ldots, \\\\text{head}_h)W_O,$$\\n\\nwhere $\\\\text{head}_i = \\\\text{Attention}(QW_{Qi}, KW_{Ki}, VW_{Vi})$.\\n\\n$W_Q, W_K, W_V$ are the parameters for the projection networks of head $i$ and $W_O$ is the output network parameters of the MHA model.\\n\\nWe combine cross-attention and self-attention to integrate the three types of information. Suppose $u_k$, $u_{\\\\text{time}}_k$ and $q_k$ are the ad opportunity feature, time information and category information in a single\"}"}
{"id": "OTjTKFk7gb", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"record respectively, then we will process the information as follows:\\n\\n\\\\[ Q^{(1)} = \\\\tau^{(1)} Q(u_{time}^k), \\\\quad K^{(1)} = \\\\tau^{(1)} K(u_{time}^k), \\\\quad V^{(1)} = \\\\tau^{(1)} V(u_{time}^k), \\\\]\\n\\n\\\\[ z^k = \\\\text{MultiHead}(Q^{(1)}, K^{(1)}, V^{(1)}) \\\\]\\n\\n\\\\[ Q^{(2)} = \\\\tau^{(2)} Q(z^k), \\\\quad K^{(2)} = \\\\tau^{(2)} K(z^k), \\\\quad V^{(2)} = \\\\tau^{(2)} V(z^k), \\\\]\\n\\n\\\\[ z^k = \\\\text{MultiHead}(Q^{(2)}, K^{(2)}, V^{(2)}) \\\\]\\n\\n\\\\[ Q^{(3)} = \\\\tau^{(3)} Q(z^k), \\\\quad K^{(3)} = \\\\tau^{(3)} K(z^k), \\\\quad V^{(3)} = \\\\tau^{(3)} V(z^k), \\\\]\\n\\n\\\\[ z^k = \\\\text{MultiHead}(Q^{(3)}, K^{(3)}, V^{(3)}) \\\\]\\n\\n\\\\[ Q^{(4)} = \\\\tau^{(4)} Q(z^k), \\\\quad K^{(4)} = \\\\tau^{(4)} K(z^k), \\\\quad V^{(4)} = \\\\tau^{(4)} V(z^k), \\\\]\\n\\n\\\\[ z^k = \\\\text{MultiHead}(Q^{(4)}, K^{(4)}, V^{(4)}) \\\\]\\n\\nwhere \\\\( \\\\tau^{(1)}, \\\\tau^{(2)}, \\\\tau^{(3)}, \\\\tau^{(4)} \\\\) are the projection function for the multi-head attention network.\\n\\nThe variation of ad opportunity values has some temporal patterns in the real world. Therefore, we follow the position encoding idea in Transformer [31] and Diffusion Model [17] to process the time information. Let \\\\( \\\\text{PE} : \\\\mathbb{N} \\\\rightarrow \\\\mathbb{R}^d \\\\) represent the position encoding function, then\\n\\n\\\\[ \\\\text{PE}_2^s(t) = \\\\sin \\\\left( \\\\frac{10000}{2^s} t \\\\right), \\\\]\\n\\n\\\\[ \\\\text{PE}^s_{2+1}(t) = \\\\cos \\\\left( \\\\frac{10000}{2^s} t \\\\right), \\\\]\\n\\nwhere \\\\( t \\\\) is the discrete time and \\\\( s \\\\) corresponds to the dimension in the embedding \\\\( \\\\text{PE}(t) \\\\).\\n\\nLet \\\\( e^k = \\\\text{PE}(u_{time}^k) \\\\), then the value prediction is conducted by\\n\\n\\\\[ \\\\hat{v}^k = U_\\\\xi(z^k, e^k), \\\\]\\n\\nwhere \\\\( U_\\\\xi(z^k, e^k) \\\\) is the prediction network with a similar architecture to the U-Net [26] used by the Diffusion Model [17].\\n\\nThe loss of the value prediction model is shown below:\\n\\n\\\\[ L_{\\\\text{pred}} = \\\\frac{1}{N} \\\\sum_{k=1}^{N} \\\\| v^k - \\\\hat{v}^k \\\\|_2^2, \\\\]\\n\\nwhere \\\\( v^k \\\\) is the true value of the ad opportunity in the record of \\\\( u_k \\\\).\"}"}
{"id": "OTjTKFk7gb", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Checklist\\n\\n1. For all authors...\\n\\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? [Yes] See Section 1.\\n\\n(b) Did you describe the limitations of your work? [Yes] See Section 8.\\n\\n(c) Did you discuss any potential negative societal impacts of your work? [N/A] We believe our benchmark does not involve any potential negative societal impacts.\\n\\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n\\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n\\n(b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n\\n(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Appendix B.5.\\n\\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Appendix A.\\n\\n(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] See Section ??.\\n\\n(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix A.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n\\n(a) If your work uses existing assets, did you cite the creators? [N/A]\\n\\n(b) Did you mention the license of the assets? [Yes] See Appendix B.5.\\n\\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes] See Appendix B.5.\\n\\n(d) Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? [Yes] See Appendix B.5.\\n\\n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See Appendix B.3.\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\\n\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\"}"}
