{"id": "nQZHEunntbJ", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"AutoWS-Bench-101: Benchmarking Automated Weak Supervision with 100 Labels\\n\\nNicholas Roberts\u2217, Xintong Li\u2217, Tzu-Heng Huang, Dyah Adila, Spencer Schoenberg, Cheng-Yu Liu, Lauren Pick, Haotian Ma, Aws Albarghouthi\u2020, Frederic Sala\\n\\nUniversity of Wisconsin\u2013Madison\\n{nick11roberts, aws, fredsala}@cs.wisc.edu\\n{xli2224, thuang273, adila, spencer.schoenberg, cliu547, lpick2, hma232}@wisc.edu\\n\\nAbstract\\n\\nWeak supervision (WS) is a powerful method to build labeled datasets for training supervised models in the face of little-to-no labeled data. It replaces hand-labeling data with aggregating multiple noisy-but-cheap label estimates expressed by labeling functions (LFs). While it has been used successfully in many domains, weak supervision's application scope is limited by the difficulty of constructing labeling functions for domains with complex or high-dimensional features. To address this, a handful of methods have proposed automating the LF design process using a small set of ground truth labels. In this work, we introduce AutoWS-Bench-101: a framework for evaluating automated WS (AutoWS) techniques in challenging WS settings\u2014a set of diverse application domains on which it has been previously difficult or impossible to apply traditional WS techniques. While AutoWS is a promising direction toward expanding the application-scope of WS, the emergence of powerful methods such as zero-shot foundation models reveals the need to understand how AutoWS techniques compare or cooperate with modern zero-shot or few-shot learners. This informs the central question of AutoWS-Bench-101: given an initial set of 100 labels for each task, we ask whether a practitioner should use an AutoWS method to generate additional labels or use some simpler baseline, such as zero-shot predictions from a foundation model or supervised learning.\\n\\nWe observe that in many settings, it is necessary for AutoWS methods to incorporate signal from foundation models if they are to outperform simple few-shot baselines, and AutoWS-Bench-101 promotes future research in this direction. We conclude with a thorough ablation study of AutoWS methods.\\n\\n1 Introduction\\n\\nThe success of modern deep learning has been driven by large labeled datasets. Unfortunately, obtaining labeled data at scale is expensive and slow. Weak supervision (WS) [1, 2, 3, 4] is one of the most successful solutions to the problem of generating labeled datasets. The idea behind WS is simple: replace hand-labeling with acquiring and aggregating noisy-but-cheap label estimates from external sources\u2014also called labeling functions (LFs)\u2014to produce denoised estimates of true labels. WS is deployed widely and has had a significant impact. For example, in the medical domain, it has been used to re-create radiography datasets in a few hours that saved physician-months in work.\"}"}
{"id": "nQZHEunntbJ", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"It has led to the creation of the largest dataset of aortic valve malformations developed atop the large-scale UK Biobank. In addition, WS has enjoyed wide industrial adoption. It is used in production by core teams within Google, Apple, and many more.\\n\\nDespite its massive success and widespread adoption, weak supervision can be challenging to apply. While much cheaper than manually labeling millions of examples, LF generation is sometimes costly. Obtaining high-quality LFs often requires training domain experts and painstaking iteration. Furthermore, creative choices must be made when applying WS to new modalities. For example, LFs that apply to text may not lift to image segmentation or recommendation tasks.\\n\\nAutomated WS (AutoWS) methods are a best-of-both-worlds solution. These techniques generate LFs automatically by training models on a small initial set of ground truth labels. At first glance, AutoWS techniques resemble few-shot learners. However, they are much more, forming omnivorous frameworks capable of incorporating and combining signals from many sources, including powerful few- and zero-shot models. Indeed, recent evidence suggests that WS can exploit signal from powerful foundation models like CLIP, GPT-3, and T0++.\\n\\nAutoWS techniques have tremendous potential. For example, they can fuse multiple sources of signal, including foundation models, to maximize performance, inherit from WS the ability to denoise labels and to generalize beyond their constituent LFs, enable applying WS to domains and modalities where hand-crafting LFs is difficult or impossible. Missing, however, is a thorough comparison of AutoWS techniques or an in-depth understanding of which components lead to good performance. For example, intuitively these methods rely on high-quality feature representations, but it is not clear how performance scales with the choice of representation. It is crucial to understand how these methods compare to\u2014and can be combined with\u2014existing few- or zero-shot models. Motivated to understand these mechanics under a diverse data settings previously underexplored by WS, we propose AutoWS-Bench-101.\\n\\nAutoWS-Bench-101 is a framework and benchmark for experimenting with, understanding, and evaluating AutoWS methods. It enables plug-and-play combinations of feature representations, LF candidate generation approaches, and LF selection procedures, conveniently built on top of the WRENCH framework. It can be used with existing AutoWS methods like Snuba, interactive WS, and GOGGLES, or extensions of such techniques. A pipeline is shown in Figure 1.\\n\\nAutoWS-Bench-101 evaluation uses a diverse set of datasets on which WS has previously seen little application. Our benchmark reveals the following key insights about AutoWS:\\n\\n1. Foundation models are only helpful to AutoWS for in-distribution or related tasks,\\n2. LFs that output multiple classes might be better than class-specialized LFs,\\n3. Foundation model usage can hurt coverage, the fraction of points labeled by AutoWS.\\n\\nOnce AutoWS methods are better understood, these methods have the promise of drastically increasing the application scope of WS. Our benchmark takes a crucial first step toward this understanding.\\n\\nWe publicly release all experiment code and datasets for AutoWS-Bench-101 on GitHub. \\n\\nhttps://github.com/Sala-Group/AutoWS-Bench-101\"}"}
{"id": "nQZHEunntbJ", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2 Background\\n\\nWe provide background on weak supervision (WS), automated WS (AutoWS), along with pre-trained and foundation models. Finally, we formalize the background on AutoWS by providing a mathematical formulation of the problem setting.\\n\\nWeak Supervision\\n\\nWS methods generate labels for unlabeled data by combining multiple sources of imperfect label estimates [1, 2, 4, 11]. These easily obtained and applied sources are called labeling functions (LFs). They can be created from heuristic rules produced by an expert, knowledge-base queries, pre-trained model outputs, or by any other means cheaper than obtaining ground-truth labels [21, 22, 23, 24, 25]. LFs provide a noisy estimate of the label, or otherwise abstain. If all the LFs abstain on a point, WS methods cannot label it; the fraction of points labeled by WS is the coverage.\\n\\nMuch of the WS literature focuses on the label model, the tool used to aggregate LF outputs to produce de-noised label estimates. New variants with different properties were introduced in [1, 3, 4, 11]. The recent benchmark WRENCH [20] focuses on evaluating such models. In contrast, we are focused on understanding the mechanics of AutoWS independent of the choice of label model.\\n\\nAutomated Weak Supervision\\n\\nIn many cases, the process of designing LFs is expensive or challenging. Several automated weak supervision (AutoWS) methods have been developed to automatically create LFs from some small initial set of labeled examples. Snuba [12] generates a large set of weak learners trained on combinations of features and iteratively selects LFs based on various performance criteria. Interactive Weak Supervision (IWS) [13] also selects LFs from a pool of candidates by interactively learning from human feedback. While not explicitly an automated procedure, IWS is easily modified to not require a human in the loop. Finally, GOGGLES [14] was introduced to address the challenge of automating the LF creation process, as well as the challenge of using WS for image data. GOGGLES computes pairwise affinity scores between feature representations of examples obtained by, for example, a pretrained VGG16 network, and learns a hierarchical generative model on the resulting affinity matrices. Additionally, several AutoWS methods have been developed specifically for the NLP domain. Some methods such as Darwin [26], PRBoost [27], and Nemo [28], like IWS, query annotators for feedback to assess the quality of synthesized rules. Other frameworks can be more complex and even more tailored to the NLP domain, such as ARI, which integrates inferred rules into pretrained NLP models [29].\\n\\nPre-trained and Foundation Models\\n\\nPowerful pre-trained models are increasingly useful in modern machine learning. A standard approach to sidestep acquiring huge labeled datasets is to fine-tune a pre-trained model. For image data, this might be a ResNet [30] trained on ImageNet. Self-supervised models [31, 32] can be used to acquire high-quality representations enabling users to train a simpler downstream model using fewer labeled points. Foundation models [33], large-scale models trained on vast amounts of data, can be used as few- or zero-shot predictors. These approaches can be thought of as either competitors of AutoWS\u2014if used as alternatives\u2014or as AutoWS ingredients. For example, WS can be performed on top of foundation model embeddings, or use them to extend LF coverage [15], or use prompts as sources [16]. Understanding how pre-trained or foundation models compete with or contribute to AutoWS is a key element of this work.\\n\\nProblem formulation\\n\\nWe provide a mathematical formulation of the AutoWS problem setting here. First, we begin with an unlabeled training set of \\\\( n \\\\) examples \\\\( X_{train} = [x_i]_{n=1}^{n} \\\\) with \\\\( x_i \\\\in X \\\\) and a labeled validation set of \\\\( m \\\\) examples \\\\( X_{val} = [x_j]_{n+m=1}^{n+m} \\\\) with \\\\( x_j \\\\in X \\\\), \\\\( Y_{val} = [y_j]_{n+m=1}^{n+m} \\\\) with \\\\( y_j \\\\in Y \\\\). Next, we embed examples in \\\\( X \\\\) into \\\\( \\\\mathbb{R}^d \\\\) using a feature extractor, which we denote as \\\\( \\\\phi: X \\\\rightarrow \\\\mathbb{R}^d \\\\). Additionally, we denote the hypothesis class of synthetic LFs as \\\\( H \\\\) where \\\\( \\\\forall h \\\\in H \\\\), we have \\\\( h: \\\\mathbb{R}^d \\\\rightarrow Y \\\\cup \\\\{-1\\\\} \\\\). We will denote an AutoWS method as a pair of algorithms, the first of which generates LFs and is parameterized by a feature extractor: \\\\( A_{\\\\phi WS} = (LF_{\\\\phi WS}, LM_{WS}) \\\\) where \\\\( LF_{\\\\phi WS}: X_{n+m} \\\\times Y_m \\\\rightarrow P(H) \\\\) and the second is the label model, which infers a weak label from one or more LF predictions: \\\\( LM_{WS}: Y \\\\rightarrow Y \\\\). We generate LFs using \\\\( \\\\Lambda = LF_{\\\\phi WS}(X_{train}, X_{val}, Y_{val}) \\\\) and obtain LF predictions \\\\( \\\\forall h_k \\\\in \\\\Lambda \\\\) as \\\\( \\\\lambda_{k_i} = h_k(\\\\phi(x_i)) \\\\). We can furthermore obtain additional label predictions for each example from other sources, such as zero-shot predictions from foundation models or by using hand-designed LFs. We will denote the set of these additional predictions as...\"}"}
{"id": "nQZHEunntbJ", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"$\\\\lambda_i = \\\\{\\\\tilde{\\\\lambda}_l \\\\}_l \\\\geq 0, \\\\forall i \\\\in [n + m].$ Finally, we obtain weak labels for the training set using the label model:\\n\\n$$\\\\hat{y}_i = \\\\text{LM}_{WS}(\\\\{\\\\lambda_k\\\\}_k | \\\\Lambda | k = 1 \\\\cup \\\\tilde{\\\\lambda}_i), \\\\forall i \\\\in [n].$$\\n\\nIn this work, we evaluate choices of $\\\\phi$ and $A_{WS}$. \\n\\n### 3 Benchmark Goals and Methodology\\n\\nWe describe the goals of our proposed benchmark and our experimental methodology.\\n\\n#### Goals\\n\\nThe goal of this work is to understand the effects of the key components of AutoWS\u2014LFs, selection methods, and representations\u2014on a diverse collection of datasets. We are especially interested in settings far from the ambit of conventional weak supervision\u2014most often used in text applications. We also seek to understand how AutoWS compares to leading zero-/few-shot methods when used as either an alternative or an ingredient.\\n\\n#### Methodology\\n\\nFor ease-of-use, our benchmark is implemented as an extension of the recently introduced Weak supeRvision bENCHmark (WRENCH) [20]. We note that WRENCH and AutoWS-Bench-101 have fundamentally different goals: WRENCH evaluates label models, not AutoWS techniques. An additional difference is that WRENCH uses fixed pre-computed LFs and only includes a handful of image-based datasets without access to the original source images or videos.\\n\\nWe consider three well-known methods for generating LFs: Snuba, IWS, and GOGGLES [12, 13, 14], operating on a handful of baseline features: raw pixels, PCA with 100 principal components, the logits layer of a pre-trained ResNet-18 [30], and zero-shot logits from CLIP [17]. The AutoWS-Bench-101 pipeline is shown in Figure 1. Below, we explain our choices in more detail.\\n\\n### 3.1 Datasets\\n\\nIn order to evaluate AutoWS techniques on a range of challenging WS settings, we include a diverse set of 10 classification datasets. Image classification datasets are a salient example of the types of datasets that fit our goal, as these have proven to be a challenging domain for weak supervision. We include two such datasets, MNIST and CIFAR-10 [34, 35], in AutoWS-Bench-101. We wish to avoid AutoWS methods overfitting to the image domain through the exploitation of feature representations specifically tailored to these tasks. For this reason we include two image-derived datasets that move beyond the scope of natural image classification. Spherical MNIST [36] consists of MNIST digits stereographically projected onto the sphere, rotated, and mapped back to the plane. Permuted MNIST consists of MNIST digits whose pixels are permuted by a fixed bit-reversal permutation applied to the rows and columns. We additionally include three text datasets that are also used in WRENCH [20]: YouTube [37], Yelp [38, 39], and IMDb [40, 39]. Finally, we include three tasks in more diverse application settings beyond image, image derived, or text datasets. These are the electrocardiogram (ECG) dataset of [41], a turbulence classification variant of the Navier-Stokes PDE data used in [42], and EMBER, a tabular malware detection dataset [43]. Further details of the latter three datasets are provided below.\\n\\n#### ECG\\n\\nThe ECG, or electrocardiogram dataset is commonly used to detect heart disease. It contains 60 second or shorter ECG recordings posed as a four-class classification problem: normal, disease, other, or noisy. This dataset was derived from the 2017 PhysioNet Challenge [41]. We use a fixed size of sliding window to process signals into a 1,000 ms time-series dataset.\\n\\n#### Navier-Stokes\\n\\nThe Navier-Stokes equation is a partial differential equation that describes viscous fluid flows. A property of interest to the computational fluid dynamics community is the turbulence of these flows. In the case of the Navier-Stokes equation, this is governed by a viscosity parameter $\\\\eta$. Navier-Stokes solutions are generated by [42]. We adapt the Navier-Stokes dataset at $\\\\eta = 10^{-3}$ (non-turbulent) and $\\\\eta = 10^{-5}$ (turbulent) to be a binary turbulence classification task.\\n\\n#### EMBER\\n\\nThe EMBER dataset, or Elastic Malware Benchmark for Empowering Researchers, introduced by [43], is a tabular malware detection dataset extracted from malicious and benign Windows portable executable files. EMBER dataset is a binary classification task, and it includes 4...\"}"}
{"id": "nQZHEunntbJ", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"several groups of feature, such as parsed features, format-agnostic features, and string information to detect malware. We use the open source implementation to extract features.\\n\\n3.2 Implementation Details\\n\\nWe adapt the open source implementations of Snuba, IWS, and GOGGLES to be compatible with WRENCH [12, 13, 14, 20]. As AutoWS methods require a small set of labeled examples, we use 100 labeled examples in all experiments unless otherwise stated. We include a few-shot learning baseline\u2014a logistic regression classifier trained on the same 100 labeled examples. Additionally, we include a semi-supervised baseline\u2014label propagation [44]. Finally, we include zero-shot CLIP as another baseline. In the final evaluation of each method, we report output label model accuracy (see Table 1) and coverage (see Appendix).\\n\\nSnuba and Multiclass Snuba\\n\\nUnless otherwise stated, we use the Snuba implementation and hyperparameters [12]. This implementation only supports binary classification\u2014however, five of our datasets are multiclass. We adapt Snuba to multiclass settings in two ways. First, we run Snuba\u2019s LF synthesis procedure for each class as a one-vs-all problem. This results in unipolar LFs\u2014a given LF will either vote for its assigned class or abstain. We also implemented a multipolar version of Snuba where LFs are themselves multiclass. By default, the search space of LFs is fixed to be decision stumps, logistic regression, or nearest neighbor classifiers. We found that allowing Snuba to consider both decision stumps and logistic regression classifiers for any given LF led to a slight improvement in performance. All of our experiments consider both of these. Each LF abstains according to a confidence threshold that is inferred by Snuba during the LF synthesis procedure. Finally, as in the original Snuba implementation, we aggregate the outputs of each LF using the label model.\\n\\nInteractive Weak Supervision\\n\\nWe adapt the original implementation of IWS to our setting. IWS operates on an initial pool of binary LFs. For simplicity, we use Snuba\u2019s LF synthesis procedure to generate a large set of LFs that IWS can select from. Note that since we use Snuba\u2019s LF synthesis procedure, each LF has its own inferred abstention threshold. Specifically, we generate LFs with cardinality $D = 1$, the number of features that each LF can consider. Similarly, unless otherwise stated, we consider both decision stumps and logistic regression classifiers. Like Snuba, the original IWS implementation only supports binary classification. Since our benchmark is over 10-class problems, we run the IWS LF selection procedure once per class to produce unipolar LFs for each class before passing the outputs of these to the label model.\\n\\nGOGGLES\\n\\nGOGGLES computes pairwise affinity scores on a set of unlabeled examples which are used in a hierarchical generative model with to infer labels. To construct the affinity matrix, the default implementation uses the max-pooling layers of the pre-trained VGG-16 network to extract various levels of semantic representations. In our experiments, we replace these VGG-16 representations with our baseline feature representations. Pairwise affinity scores are computed by cosine similarity between embeddings of each instance. After training a generative model to create clusters, GOGGLES uses a small amount of labeled datapoints to map clusters to classes. Note that GOGGLES does not abstain.\\n\\nCLIP\\n\\nWe used standard CLIP [17] zero-shot prediction as a baseline, comparing image embeddings (except for the tabular dataset EMBER) with text embeddings of strings related to the class label. The complete list of these strings is provided in the Appendix.\\n\\nBERT embeddings for text datasets\\n\\nOur pipeline requires the features of each dataset to be compatible with all (or most) of the feature representation techniques that we evaluate. In order to make the text datasets that we evaluate work with our pipeline, we first extract feature vectors for our text datasets using BERT before applying other feature representations (except in the case of CLIP, which operates on text directly). For YouTube, Yelp, and IMDb, raw features refers to these BERT features, PCA embeddings refers to the PCA of BERT features, and ResNet-18 embeddings refers to BERT features passed through a pretrained ResNet-18 model. In the case of CLIP, we directly use the CLIP text encoder for both the input text and the candidate labels to produce a logits vector.\"}"}
{"id": "nQZHEunntbJ", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Test accuracies of AutoWS methods across features and datasets using 100 labels.\\n\\n| Feature Representation | MNIST | CIFAR-10S | PHEMERT | PERMUTED | ECG | NAVIER | YOUTUBE | YELP | IMD | B | MNIST | STOKES | GOGGLES | +R | AW FEATURES | 0.3778 | 0.1697 | 0.0897 | 0.3778 | 0.2710 | 0.5284 | 0.5283 | 0.5880 | 0.4732 | 0.5516 |\\n|-----------------------|-------|-----------|---------|----------|-----|--------|---------|------|-----|---|-------|---------|----------|    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           | PCA     | +R       | -18 | CLIP   |         |      |     |   |       |         | GOGGLES  |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n|                       |       |           |         |          |     |        |         |      |     |   |       |         |          |    |             |       |       |       |       |       |       |       |       |       |       |\\n"}
{"id": "nQZHEunntbJ", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2 Comparison of feature representations and AutoWS methods\\n\\nWhen do foundation models help AutoWS?\\n\\nFirst, we study the performance of different feature representation methods across datasets and AutoWS methods. Full results are presented in Table 1.\\n\\nWe compare feature representation methods using performance profiles by defining an environment to be a dataset and AutoWS method pair. Specifically, we set $S = E$ and $P = D \\\\times W$. We then plot performance profile curves for each feature representation method as shown in Figure 2 (bottom row).\\n\\nWe find that CLIP performs quite well on all of the image tasks that we evaluate, whereas ResNet-18 features and raw features were less performant compared to other methods. We postulate that this is because these tasks are somewhat similar to CLIP's training distribution. On diverse tasks, however, CLIP performs much worse\u2014due to its incompatibilities with certain data modalities, such as time series (ECG) and embeddings of tabular data (EMBER), and when using IWS on Navier-Stokes, where the input dimensionality (2) of is too small to generate a large number of LFs. We hypothesize that this is because Navier-Stokes is farther from CLIP's training distribution. Surprisingly, ResNet-18 (which was pretrained on ImageNet) appears to perform quite well on diverse tasks; it achieves the best performance on ECG, and Navier-Stokes under using few-shot baseline and the best performance on EMBER when using Snuba with multipolar LFs. We hypothesize that, while aimed at handling diverse downstream image tasks, ImageNet contains enough semantic information beyond images that it can handle, to at least a rudimentary degree, diverse tasks as well. This observation is in the same vein as prior work which shows that training on (seemingly unrelated) non-linguistic data improves downstream performance on natural language tasks [46]. On text tasks, we find that CLIP underperforms, but we postulate that this is mainly due to the lack of image input in these settings. On the other hand, we find that raw features (i.e., text features extracted using BERT) outperforms other methods. Perhaps unsurprisingly, we find that our few-shot baseline (logistic regression) using raw BERT features outperforms all other methods on text tasks, as this corresponds to standard BERT fine-tuning, which furthermore suggests that this method should be combined with AutoWS methods.\\n\\nOur key finding is that CLIP helps AutoWS on in-distribution image tasks or image-derived tasks. CLIP does not help on diverse out-of-distribution tasks or on tasks where only text input is available, in which raw BERT features help.\\n\\nHow does AutoWS compare to few-shot learning?\\n\\nNext, we compare AutoWS to few-shot learning. Full results are in Table 1. For our performance profiles we set $S = W$ and $P = D \\\\times E$. The curves are shown in Figure 2 (top row). We find that few-shot learning typically ourperforms any other method on image tasks, attaining strong average performance. However, we note that in terms of absolute performance, IWS and Snuba perform the best on CIFAR-10 and Spherical MNIST (both when using CLIP), though they are less robust to changes in the underlying feature representation.\"}"}
{"id": "nQZHEunntbJ", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Accuracy on image tasks\\n\\n2. Accuracy on diverse tasks\\n\\n3. Accuracy on text tasks\\n\\n4. Coverage on image tasks\\n\\n5. Coverage on diverse tasks\\n\\n6. Coverage on text tasks\\n\\nFigure 3: Using multipolar LFs typically improves the performance of Snuba (top). The difference is more pronounced on diverse tasks (top center). Multipolar LFs can result in lower coverage among image tasks (bottom left) which are notably all 10-class classification tasks. Conversely, multipolar LFs result in perfect coverage for the diverse tasks, which have fewer classes (bottom center).\\n\\nFigure 2 (top center) shows that Snuba (both unipolar and multipolar) perform similarly to few-shot learning on diverse tasks. On Navier-Stokes, multipolar Snuba nearly matches the 100% accuracy of the few-shot baseline and on EMBER, multipolar Snuba outperforms few-shot learning using ResNet-18 features. In terms of absolute performance on all applicable datasets, at least two of the AutoWS methods also outperform raw zero-shot CLIP predictions. We again note that for all text tasks, few-shot learning using raw BERT features (i.e., fine-tuning BERT) outperforms all other methods. Our key finding is that few-shot learning is a strong baseline with good average performance across tasks, which suggests that it should be combined with AutoWS methods.\\n\\n4.3 Evaluation of scientific questions using AutoWS-Bench-101\\n\\nAre multiclass LFs better than class-specialized LFs?\\n\\nNext, we directly compare Snuba with unipolar LFs to Snuba with multipolar LFs. To our knowledge, a study of the value of unipolar vs. multipolar LFs has not been previously conducted in the WS literature. The top row of Figure 3 shows that multipolar Snuba can attain slightly better average performance than unipolar Snuba across all feature representations on image tasks, though in terms of absolute numbers in Table 1, unipolar Snuba significantly outperforms multipolar Snuba when using CLIP embeddings (typically the best feature representation for both methods). On diverse tasks, multipolar Snuba outperforms or nearly matches unipolar Snuba, across all feature representations. We furthermore find that on diverse tasks, multipolar Snuba attains perfect coverage in all cases, whereas unipolar Snuba often does not.\\n\\nWe note that on these diverse tasks, ECG has 4 classes the other two, Navier-Stokes and EMBER, are binary. We find that on text tasks, multipolar Snuba tends to outperform unipolar Snuba subject to using the best feature representations for each. We also note that multipolar Snuba attains greater than or equal coverage to Snuba on the text tasks (which are all binary) in all but one instance\u2014YouTube with CLIP embeddings. Surprisingly, we find that multipolar Snuba's high coverage does not translate to image tasks, where the number of classes is higher, and unipolar LFs tend to result in much higher coverage on average. We conclude that multipolar LFs tend to perform better on average but result in lower coverage when the number of classes increases.\\n\\nDo the best methods still cover the label distribution?\\n\\nA critical aspect for a WS method to be practically useful is that it must be able to produce labels for all of the classes in the label distribution with the same frequency at which they occur in the underlying data distribution. We thus perform an analysis of the coverage of methods in our pipeline subject to using different AutoWS methods and feature representations. We first consider the effect of using different AutoWS methods on coverage. We detail all of the coverage results across IWS, Snuba, and multipolar Snuba in the Appendix (note that GOGGLES by definition always produces 100% coverage). We present performance profile results of our coverage analysis in Figure 4. As before, we find that unipolar Snuba still attains the best coverage across 10-class image tasks, while multipolar Snuba attains the best coverage across diverse tasks and text tasks (with the only exception being YouTube with CLIP embeddings). In the bottom row of Figure 4, we show that methods using CLIP attain, on average, comparatively low coverage across all tasks. On diverse tasks, this can be attributed to the fact that CLIP is only...\"}"}
{"id": "nQZHEunntbJ", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: For image tasks (left)\u2014MNIST, CIFAR-10, Spherical MNIST, Permuted MNIST\u2014Snuba attains high coverage, and CLIP usage tends to worsen coverage. For non-image tasks (right)\u2014ECG, Navier-Stokes, Ember\u2014Multipolar Snuba always attains full coverage and CLIP is either non-applicable or attains poor coverage.\\n\\nConversely, ResNet-18 attains the best coverage on image tasks, and coverage is roughly similar between ResNet-18, PCA, and raw features on diverse and text tasks. From this analysis, we conclude that CLIP usage can hurt coverage.\\n\\nFigure 5: (Top left) Tuning Snuba's LF selection and abstention criteria do not lead to an improvement on MNIST. Results are reported as averages, minima, and maxima over 3 random seeds. We evaluate Snuba on MNIST using PCA and 500 feature combination samples with cardinality $D = 2$. (Bottom left) Increasing the number of labeled examples does not improve the performance of Snuba. (Right) Increasing cardinality can lead to some improvement with Snuba.\\n\\n4.4 Ablation study of Snuba\\n\\nWe conclude our analyses with an ablation study of Snuba, the best performing AutoWS method that we evaluated. We consider the effects of varying the cardinality (the number of features that each LF can consider), the LF selection criterion used by Snuba (F1 score), and the effect of increasing the initial number of labels beyond the original 100.\\n\\nDoes increasing cardinality help Snuba?\\n\\nSnuba's cardinality parameter dictates how many components of the feature representation each LF can consider during its LF synthesis procedure. We evaluate the aggregate effect on performance of varying this parameter using performance profiles, as shown in Figure 5 (left). In this case, we set $S$ to be Snuba with cardinalities $D = 1, 2, 4, 8$. We consider only CLIP embeddings and all datasets on which CLIP is applicable: MNIST, CIFAR-10, Spherical MNIST, Permuted MNIST, and Navier-Stokes. This limits the maximum cardinality that we can consider to 10, which allows us to explore the full space of possible improvements for Snuba along the axis of cardinality. In these settings, increasing cardinality improves performance up to a...\"}"}
{"id": "nQZHEunntbJ", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Beyond $D = 4$, we observe that Snuba ceases to improve much. From this, we conclude that increasing cardinality can lead to performance improvements up to a point.\\n\\nIs F1 the right metric for MNIST when using Snuba? Given the poor performance of Snuba on MNIST, we evaluate Snuba's default choice to use F1 scores for LF selection and for determining abstain thresholds\u2014micro F1 and weighted F1, respectively. We do so by first defining a search space over all applicable classification metrics available in scikit-learn [47]. These include Snuba's default choices, accuracy, balanced accuracy, precision, recall, Cohen's kappa coefficient, Jaccard index, Matthews correlation coefficient, and using the micro-averaged F1 score for both LF selection and to determine abstain thresholds. We perform a random search over convex combinations of these 9 metrics by sampling configurations from a Dirichlet distribution with parameter $\\\\alpha = \\\\frac{1}{9}$. As shown in Figure 5, we find no appreciable improvement over the default configuration. However, the best configuration found by random search has majority weight on micro-averaged F1 score, which only slightly differs from the default configuration in Snuba. These results indicate that Snuba's poor performance on MNIST is not trivially due to a poor choice of metric for LF selection.\\n\\nDo more labels help Snuba? We ask if we can further improve performance on MNIST by increasing the number of labeled examples. In this experiment, we varied the number of labeled examples given to Snuba from 100 to 1000. We set the cardinality to $D = 2$ and use raw pixel features. For efficiency, we randomly subsample 1000 feature combinations so as to avoid exploring the entire space of pairwise feature combinations. We answer the question in the negative: our results in Figure 5 show that increasing the number of labeled examples does not improve the performance of Snuba. This suggests that any performance with Snuba in this setting is likely algorithmic, and not due to a lack of labeled examples.\\n\\n5 Conclusion\\n\\nIn this work, we proposed AutoWS-Bench-101, a benchmarking framework for AutoWS methods and beyond on a diverse set of classification tasks. We found that foundation models can improve the performance of AutoWS methods on in-distribution tasks but they can also hurt coverage, sometimes dramatically, on out-of-distribution tasks. We furthermore found that AutoWS methods using multipolar LFs might outperform unipolar LFs on average, but can result in lower coverage as the number of classes increases. One limitation is that we only considered CLIP\u2014but there are many other applicable methods to be studied in future work. We hope this work takes a first step towards understanding how to build best-of-all-worlds automated weak supervision.\\n\\nAcknowledgments\\n\\nWe are grateful for the support of the NSF (CCF2106707), the American Family Funding Initiative and the Wisconsin Alumni Research Foundation (WARF). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of any of these funding agencies.\\n\\nReferences\\n\\n[1] A. Ratner, C. D. Sa, S. Wu, D. Selsam, and C. R\u00e9, \\\"Data programming: Creating large training sets, quickly,\\\" in Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS'16, (Red Hook, NY, USA), p. 3574\u20133582, Curran Associates Inc., 2016.\\n\\n[2] A. J. Ratner, S. H. Bach, H. R. Ehrenberg, J. A. Fries, S. Wu, and C. R\u00e9, \\\"Snorkel: Rapid training data creation with weak supervision,\\\" Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases, vol. 11 3, pp. 269\u2013282, 2017.\\n\\n[3] A. J. Ratner, B. Hancock, J. Dunnmon, F. Sala, S. Pandey, and C. R\u00e9, \\\"Training complex models with multi-task weak supervision,\\\" in Proceedings of the AAAI Conference on Artificial Intelligence, (Honolulu, Hawaii), 2019.\"}"}
{"id": "nQZHEunntbJ", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D. Y. Fu, M. F. Chen, F. Sala, S. M. Hooper, K. Fatahalian, and C. R\u00e9, \u201cFast and three-rious: Speeding up weak supervision with triplet methods,\u201d in Proceedings of the 37th International Conference on Machine Learning (ICML 2020), 2020.\\n\\nJ. A. Dunnmon, A. J. Ratner, K. Saab, N. Khandwala, M. Markert, H. Sagreiya, R. Goldman, C. Lee-Messer, M. P. Lungren, D. L. Rubin, and C. R\u00e9, \u201cCross-modal data programming enables rapid medical machine learning,\u201d Patterns (N Y), vol. 1, May 2020.\\n\\nJ. A. Fries, P. Varma, V. S. Chen, K. Xiao, H. Tejeda, P. Saha, J. Dunnmon, H. Chubb, S. Maskatia, M. Fiterau, S. Delp, E. Ashley, C. R\u00e9, and J. R. Priest, \u201cWeakly supervised classification of aortic valve malformations using unlabeled cardiac mri sequences,\u201d Nature Communications, vol. 10, no. 1, p. 3111, 2019.\\n\\nC. Sudlow, J. Gallacher, N. Allen, V. Beral, P. Burton, J. Danesh, P. Downey, P. Elliott, J. Green, M. Landray, B. Liu, P. Matthews, G. Ong, J. Pell, A. Silman, A. Young, T. Sprosen, T. Peakman, and R. Collins, \u201cUk biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age,\u201d PLoS Med, vol. 12, p. e1001779, Mar 2015.\\n\\nS. H. Bach, D. Rodriguez, Y. Liu, C. Luo, H. Shao, C. Xia, S. Sen, A. Ratner, B. Hancock, H. Alborzi, et al., \u201cSnorkel drybell: A case study in deploying weak supervision at industrial scale,\u201d in Proceedings of the 2019 International Conference on Management of Data, pp. 362\u2013375, 2019.\\n\\nC. R\u00e9, F. Niu, P. Gudipati, and C. Srisuwananukorn, \u201cOverton: A data system for monitoring and improving machine-learned products,\u201d in Proceedings of the 10th Annual Conference on Innovative Data Systems Research, 2020.\\n\\nS. Hooper, M. Wornow, Y. H. Seah, P. Kellman, H. Xue, F. Sala, C. Langlotz, and C. Re, \u201cCut out the annotator, keep the cutout: better segmentation with weak supervision,\u201d in International Conference on Learning Representations, 2021.\\n\\nC. Shin, W. Li, H. Vishwakarma, N. C. Roberts, and F. Sala, \u201cUniversalizing weak supervision,\u201d in International Conference on Learning Representations (ICLR), 2022.\\n\\nP. Varma and C. R\u00e9, \u201cSnuba: Automating weak supervision to label training data,\u201d Proc. VLDB Endow., vol. 12, p. 223\u2013236, nov 2018.\\n\\nB. Boecking, W. Neiswanger, E. Xing, and A. Dubrawski, \u201cInteractive weak supervision: Learning useful heuristics for data labeling,\u201d in International Conference on Learning Representations (ICLR), 2021.\\n\\nN. Das, S. Chaba, R. Wu, S. Gandhi, D. H. Chau, and X. Chu, \u201cGoggles: Automatic image labeling with affinity coding,\u201d in Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data, SIGMOD '20, (New York, NY, USA), p. 1717\u20131732, Association for Computing Machinery, 2020.\\n\\nM. Chen, D. Fu, D. Adila, M. Zhang, F. Sala, K. Fatahalian, and C. R\u00e9, \u201cShoring up the foundations: Fusing model embeddings and weak supervision,\u201d in Uncertainty in Artificial Intelligence (UAI), 2022.\\n\\nR. Smith, J. A. Fries, B. Hancock, and S. H. Bach, \u201cLanguage models in the loop: Incorporating prompting into weak supervision,\u201d 2022.\\n\\nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., \u201cLearning transferable visual models from natural language supervision,\u201d in International Conference on Machine Learning, pp. 8748\u20138763, PMLR, 2021.\\n\\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, \u201cLanguage models are few-shot learners,\u201d in Advances in Neural Information Processing Systems, 2020.\\n\\nV. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chhablani, N. Nayak, D. Datta, J. Chang, M. T.-J. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fevry, J. A. Fries, R. Teehan, S. Biderman, L. Gao, T. Bers, T. Wolf, and A. M. Rush,\""}
{"id": "nQZHEunntbJ", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multitask prompted training enables zero-shot task generalization, in *International Conference on Learning Representations (ICLR)*, 2022.\\n\\n[20] J. Zhang, Y. Yu, Y. Li, Y. Wang, Y. Yang, M. Yang, and A. Ratner, \u201cWRENCH: A comprehensive benchmark for weak supervision,\u201d in *Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)*, 2021.\\n\\n[21] S. Gupta and C. Manning, \u201cImproved pattern learning for bootstrapped entity extraction,\u201d in *Proceedings of the Eighteenth Conference on Computational Natural Language Learning*, pp. 98\u2013108, 2014.\\n\\n[22] D. R. Karger, S. Oh, and D. Shah, \u201cIterative learning for reliable crowdsourcing systems,\u201d in *Advances in neural information processing systems*, pp. 1953\u20131961, 2011.\\n\\n[23] A. P. Dawid and A. M. Skene, \u201cMaximum likelihood estimation of observer error-rates using the EM algorithm,\u201d *Applied statistics*, pp. 20\u201328, 1979.\\n\\n[24] M. Mintz, S. Bills, R. Snow, and D. Jurafsky, \u201cDistant supervision for relation extraction without labeled data,\u201d in *Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2*, pp. 1003\u20131011, Association for Computational Linguistics, 2009.\\n\\n[25] M. A. Hearst, \u201cAutomatic acquisition of hyponyms from large text corpora,\u201d in *Proceedings of the 14th conference on Computational linguistics-Volume 2*, pp. 539\u2013545, Association for Computational Linguistics, 1992.\\n\\n[26] S. Galhotra, B. Golshan, and W.-C. Tan, \u201cAdaptive rule discovery for labeling text data,\u201d in *Proceedings of the 2021 International Conference on Management of Data*, SIGMOD '21, (New York, NY, USA), p. 2217\u20132225, Association for Computing Machinery, 2021.\\n\\n[27] R. Zhang, Y. Yu, P. Shetty, L. Song, and C. Zhang, \u201cPrompt-based rule discovery and boosting for interactive weakly-supervised learning,\u201d in *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, (Dublin, Ireland), pp. 745\u2013758, Association for Computational Linguistics, May 2022.\\n\\n[28] C.-Y. Hsieh, J. Zhang, and A. Ratner, \u201cNemo: Guiding and contextualizing weak supervision for interactive data programming,\u201d 2022.\\n\\n[29] R. Pryzant, Z. Yang, Y. Xu, C. Zhu, and M. Zeng, \u201cAutomatic rule induction for efficient semi-supervised learning,\u201d 2022.\\n\\n[30] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in *2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pp. 770\u2013778, 2016.\\n\\n[31] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in *Proceedings of the 37th International Conference on Machine Learning (H. D. III and A. Singh, eds.)*, vol. 119 of *Proceedings of Machine Learning Research*, pp. 1597\u20131607, PMLR, 13\u201318 Jul 2020.\\n\\n[32] J.-B. Grill, F. Strub, F. Altch\u00e9, C. Tallec, et al., \u201cBootstrap your own latent a new approach to self-supervised learning,\u201d in *Advances in Neural Information Processing Systems (NeurIPS)*, 2020.\\n\\n[33] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, et al., \u201cOn the opportunities and risks of foundation models.\u201d [https://arxiv.org/abs/2108.07258](https://arxiv.org/abs/2108.07258), 2021.\\n\\n[34] L. Deng, \u201cThe mnist database of handwritten digit images for machine learning research [best of the web],\u201d *IEEE Signal Processing Magazine*, vol. 29, no. 6, pp. 141\u2013142, 2012.\\n\\n[35] A. Krizhevsky, V. Nair, and G. Hinton, \u201cCifar-10 (canadian institute for advanced research),\u201d [36] T. S. Cohen, M. Geiger, J. K\u00f6hler, and M. Welling, \u201cSpherical CNNs,\u201d in *International Conference on Learning Representations (ICLR)*, 2018.\\n\\n[37] T. C. Alberto, J. V. Lochter, and T. A. Almeida, \u201cTubespam: Comment spam filtering on youtube,\u201d in *2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)*, pp. 138\u2013143, 2015.\\n\\n[38] X. Zhang, J. Zhao, and Y. LeCun, \u201cCharacter-level convolutional networks for text classification,\u201d in *Advances in Neural Information Processing Systems* (C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, eds.), vol. 28, Curran Associates, Inc., 2015.\"}"}
{"id": "nQZHEunntbJ", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"W. Ren, Y. Li, H. Su, D. Kartchner, C. Mitchell, and C. Zhang, \u201cDenoising multi-source weak supervision for neural text classification,\u201d in Findings of the Association for Computational Linguistics: EMNLP 2020, (Online), pp. 3739\u20133754, Association for Computational Linguistics, Nov. 2020.\\n\\nA. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts, \u201cLearning word vectors for sentiment analysis,\u201d in Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, (Portland, Oregon, USA), pp. 142\u2013150, Association for Computational Linguistics, June 2011.\\n\\nG. D. Clifford, C. Liu, B. Moody, L.-w. H. Lehman, I. Silva, Q. Li, A. E. Johnson, and R. G. Mark, \u201cAf classification from a short single lead ecg recording: The physionet/computing in cardiology challenge 2017,\u201d in 2017 Computing in Cardiology (CinC), pp. 1\u20134, 2017.\\n\\nZ. Li, N. B. Kovachki, K. Azizzadenesheli, B. liu, K. Bhattacharya, A. Stuart, and A. Anandkumar, \u201cFourier neural operator for parametric partial differential equations,\u201d in International Conference on Learning Representations (ICLR), 2021.\\n\\nH. S. Anderson and P. Roth, \u201cEMBER: an open dataset for training static PE malware machine learning models,\u201d CoRR, vol. abs/1804.04637, 2018.\\n\\nF. Wang and C. Zhang, \u201cLabel propagation through linear neighborhoods,\u201d IEEE Transactions on Knowledge and Data Engineering, vol. 20, no. 1, pp. 55\u201367, 2008.\\n\\nE. D. Dolan and J. J. Mor\u00e9, \u201cBenchmarking optimization software with performance profiles,\u201d Mathematical Programming, vol. 91, no. 2, pp. 201\u2013213, 2002.\\n\\nI. Papadimitriou and D. Jurafsky, \u201cLearning Music Helps You Read: Using transfer to study linguistic structure in language models,\u201d in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), (Online), pp. 6829\u20136839, Association for Computational Linguistics, Nov. 2020.\\n\\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, \u201cScikit-learn: Machine learning in Python,\u201d Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.\\n\\nChecklist\\n1. For all authors...\\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n(b) Did you describe the limitations of your work? [Yes] See Section 5\\n(c) Did you discuss any potential negative societal impacts of your work? [Yes] See Appendix.\\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n(a) Did you state the full set of assumptions of all theoretical results? [N/A] No theoretical results.\\n(b) Did you include complete proofs of all theoretical results? [N/A] No theoretical results.\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Section 1 and codebase.\\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See codebase.\\n(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] We do not report error bars per-se, but our analyses hinge on aggregate performance profiles which are robust to random seed effects.\"}"}
{"id": "nQZHEunntbJ", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [No] Our experiments, for the most part, are comparatively lightweight compared to many deep learning works, as our experiments only required us to perform inference using deep learning models. That said, we used p3.2xlarge instances on AWS for most of our experiments.\\n\\nIf you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n\\n(a) If your work uses existing assets, did you cite the creators? [Yes]\\n(b) Did you mention the license of the assets? [Yes] See codebase and Appendix.\\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes] As a URL to a GitHub repository. See Section 1.\\n(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] See Appendix.\\n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See Appendix.\\n\\nIf you used crowdsourcing or conducted research with human subjects...\\n\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] No crowdsourcing or human subjects.\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] No crowdsourcing or human subjects.\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] No crowdsourcing or human subjects.\"}"}
