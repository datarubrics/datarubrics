{"id": "HvcLKgtbco", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards a Comprehensive Benchmark for High-Level Synthesis Targeted to FPGAs\\n\\nYunsheng Bai, Atefeh Sohrabizadeh, Zongyue Qin, Ziniu Hu, Yizhou Sun, Jason Cong\\n\\nDepartment of Computer Science\\nUniversity of California, Los Angeles\\n{yba,atefehsz,qinzongyue,bull,yzsun,cong}@cs.ucla.edu\\n\\nAbstract\\nHigh-level synthesis (HLS) aims to raise the abstraction layer in hardware design, enabling the design of domain-specific accelerators (DSAs) targeted for field-programmable gate arrays (FPGAs) using C/C++ instead of hardware description languages (HDLs). Compiler directives in the form of pragmas play a crucial role in modifying the microarchitecture within the HLS framework. However, the number of possible microarchitectures grows exponentially with the number of pragmas. Moreover, the evaluation of each candidate design using the HLS tool consumes significant time, ranging from minutes to hours, leading to a slow optimization process. To accelerate this process, machine learning models have been used to predict design quality in milliseconds. However, existing open-source datasets for training such models are limited in terms of design complexity and available optimizations. In this paper, we present HLSYN, a new benchmark that addresses these limitations. It contains more complex programs with a wider range of optimization pragmas, making it a comprehensive dataset for training and evaluating design quality prediction models. The HLSYN benchmark consists of 42 unique programs/kernels, each of which has many different pragma configurations, resulting in over 42,000 labeled designs. We conduct an extensive comparison of state-of-the-art baselines to assess their effectiveness in predicting design quality. As an ongoing project, we anticipate expanding the HLSYN benchmark in terms of both quantity and variety of programs to further support the development of this field.\\n\\n1 Introduction\\nIn recent decades, the demand for specialized computing systems tailored to specific applications has significantly increased. This has led to the emergence of domain-specific accelerators (DSAs) being implemented in either application-specific integrated circuits (ASICs) or field-programmable gate arrays (FPGAs). By leveraging the unique characteristics of specific workloads, the designer can design DSAs to enhance performance and energy efficiency. This becomes particularly valuable when general-purpose processors like CPUs and GPUs cannot meet the performance or efficiency requirements of certain applications due to the end of Dennard scaling [8, 14]. For instance, Google has developed its custom-designed DSA in the form of an ASIC named the Tensor Processing Unit (TPU) [21], which is highly optimized for machine learning workloads, offering remarkably faster performance and improved energy efficiency compared to CPUs and GPUs. In addition, FPGAs offer a cost-effective alternative with reconfigurability, making them increasingly appealing for accelerating applications across various domains, including search engines and numerous datacenter applications [25, 9, 19], machine learning inference acceleration [15, 17, 32, 1, 27], and autonomous vehicles [7], among others.\"}"}
{"id": "HvcLKgtbco", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Nevertheless, the design of DSAs poses distinct difficulties in contrast to general-purpose hardware like CPUs and GPUs [34, 8]. DSAs are commonly developed using hardware description languages (HDLs) at the register-transfer level (RTL), specifically Verilog and VHDL, which are primarily known to circuit designers. To address this challenge, high-level synthesis (HLS) [11, 10] was introduced and is now supported by most EDA (Electronic Design Automation) and FPGA companies [4, 6, 20, 24, 29]. HLS raises the level of design abstraction to C/C++/OpenCL/SystemC, enabling designers to describe the high-level behavioral representation of their designs rather than the low-level data transition in RTL. This abstraction eliminates the need for explicit clock scheduling specifications in the HLS code. Instead, HLS tools analyze the behavior description to schedule operations across different clock cycles, assign operations to available resources, and establish the required control structure. Finally, the HLS tool automatically generates RTL code based on these analyses. It can take several minutes to hours for the HLS tool to generate this RTL code [34]. The RTL can then be passed through logic synthesis and physical design steps, which can consume several hours, to be implemented on the target FPGA. This HLS tool enhances design productivity, shortens design cycles, and allows designers to rapidly explore various design options without the need for manual RTL code writing.\\n\\nDespite the increased level of design abstraction offered by high-level synthesis (HLS) tools, they still require a considerable amount of hardware design expertise to utilize synthesis directives in the form of pragmas. These pragmas play a crucial role in specifying various aspects of the design, such as memory organization, caching strategies, memory buffer partitioning, parallelization and pipelining of computations, etc. As demonstrated by Chi et al. [8], although the performance of a DSA with no performance-optimizing pragmas can be 108\u00d7 slower than a CPU, through proper optimization, it can achieve a remarkable performance improvement, surpassing a CPU by 89\u00d7. However, the optimization process for architecture-specific enhancements is typically limited to hardware programmers and falls beyond the capabilities of the average software programmers. Consequently, there has been a growing focus on automating this optimization process. While some approaches treat the HLS tool as a black box and develop custom-designed heuristics to search through design candidates, a more recent research paradigm leverages machine learning and deep learning techniques. These approaches either learn the behavior of the HLS tool and construct predictive models or employ data-driven exploration methods to search through the solution space. The goal of automating the optimization process is to democratize customized computing and make it more accessible for the average software programmers, allowing them to utilize the tailored hardware acceleration.\\n\\nTo address the need for automating pragma insertion and parameter tuning in high-level synthesis (HLS), machine learning techniques can be used. This approach aims to achieve optimal quality in terms of latency and resource utilization. However, the lack of open-source datasets in this domain and the limitations of existing datasets, which are constrained in terms of design complexity and available optimizations, restrict their practicality. To bridge this gap, this paper introduces HLS YN, the first\"}"}
{"id": "HvcLKgtbco", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This benchmark provides more complex programs and a wider range of optimization pragmas, facilitating advanced research and facilitating in-depth exploration of machine learning techniques in the context of HLS. In our study, we define a design as a piece of C/C++ source code (referred to as a kernel) with associated pragmas. Our primary focus is on predicting the quality of a design using supervised learning by running a model trained on a collection of labeled designs with corresponding quality metrics. However, our dataset can be utilized in various training scenarios, including training agents to efficiently explore the solution space.\\n\\n2 Background\\n\\nThe task of HLS is to predict the quality of the HLS design specified by a program (kernel) with a specific optimization pragma design. Our target implementation platform is FPGA, although similar techniques can be applied to ASIC accelerator designs as well. The quality of a design is defined as a function of its performance, which is measured by its latency in cycle counts, and its area/resource utilization, such as the usage of digital signal processing blocks (DSP), blocks' RAMs (BRAM), flip-flops (FF), and lookup-tables (LUT), which are the fundamental building blocks for implementing digital logic circuits in FPGA designs.\\n\\nIn this work, we specifically consider the optimization pragmas of the Merlin Compiler, an open-source source-to-source optimization tool used for efficient AMD/Xilinx HLS designs. The Merlin Compiler provides three types of optimization pragmas, namely PIPELINE, PARALLEL, and TILE to define the desired microarchitecture.\\n\\nAs illustrated in Figure 1, these pragmas can be applied at the loop level and offer control over the type of pipelining, the parallelization factor, and the amount of data caching. If setting the pragmas properly to non-default parameters for proper parallelizing and pipelining the computation, the resulting accelerator can be 10\u00d7 or even 100\u00d7 faster than a single-core CPU. However, without any pragma insertion, the resulting hardware can be 10\u00d7 slower than a CPU. Figure 2 demonstrates an example where the prediction targets are sensitive to the pragma settings. It is noteworthy that in some other examples, a change in the pragma settings does not lead to any change in the output targets. The machine learning model must learn from existing labeled designs and understand the source code as well as the pragmas in order to accurately forecast the outcome of each design when eventually being executed on an FPGA.\\n\\nTable 1 summarizes the parameter space of these pragmas. For a given program/kernel, any change in the option of any of the pragmas results in a different design with a unique microarchitecture. The \\\"fg\\\" option in pipelining refers to the case where all the inner loops are unrolled (parallelized with separate logic) and each parallel unit is pipelined. The \\\"cg\\\" option, on the other hand, results in coarse-grained processing elements (PEs) that are pipelined together. For example, it can create pipelined load-compute-store units. The PARALLEL and TILE pragma take numeric values that determine the degree of parallelization and loop tiling, respectively.\"}"}
{"id": "HvcLKgtbco", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Target pragmas with their options.\\n\\n|Pragma Name| Parameter Name| Parameter Space| Examples of Pragma Settings|\\n|------------|---------------|----------------|-----------------------------|\\n|PARALLEL    | factor        | integer        | 4, 8                        |\\n|PIPELINE    | mode          | \\\"cg\\\", \\\"fg\\\", off| 'flatten' resulting in the \\\"fg\\\" mode |\\n|TILE        | factor        | integer        | 2, 4                        |\\n\\n3 Related Work\\n\\nIn previous research, optimizing HLS designs has been approached in different ways. One category of methods treats the HLS tool as a black box and utilizes problem-independent heuristics or develops dedicated heuristics to explore the solution space and evaluate the quality of results (QoR) directly using the tool [34, 43, 35]. However, this approach is time-consuming as each evaluation takes several minutes to hours. To mitigate this issue, another category of methods aims to create surrogate models for the HLS tool. Some of these methods construct dependency graphs of the program and employ traditional graph analysis techniques to schedule operations and estimate the QoR accordingly [46, 38], while others develop analytical performance and area models to estimate the QoR [45, 47]. Nevertheless, due to the different heuristics employed by HLS tools in the design process, these models may not accurately predict the QoR [34]. Some methods address this limitation by focusing on designs that can exploit pre-defined microarchitecture templates or follow specific computation patterns [33, 37, 12], but this can limit their generality. Alternatively, a data-driven approach utilizing machine learning and deep learning models has been proposed to enhance prediction accuracy [23, 22]. Graph neural networks (GNNs) have gained attention in this context and demonstrated promising results in enhancing prediction accuracy [30, 5, 40, 36].\\n\\nA fundamental aspect of these approaches is the availability of a large-scale database to effectively train the models. Recent works have focused on gathering such databases [16, 41, 30]. Unfortunately, existing datasets have limitations. The dataset in [41] predominantly consists of synthetic programs that do not utilize any pragmas. DB4HLS [16] targets programs from the MachSuite benchmark [26] but overlooks the inclusion of a key optimization pragma, pipelining. Additionally, DB4HLS views each function in the program as an individual kernel. GNN-DSE [30] targets programs from the Polyhedral benchmark [44], which features more complex kernels for FPGA mapping, in addition to the MachSuite benchmark. This dataset considers a program with all its sub-functions as a kernel, further increasing design complexity. Despite covering a wide range of optimization pragmas for each kernel, the generated dataset is small, with only 9 target kernels and a total of 4,752 data points. To address these limitations, we propose HLSYN, which includes kernels from both the Polyhedral and MachSuite benchmarks. It encompasses a diverse range of optimization pragmas that can pipeline and/or parallelize computation, as well as adjust data caching. Our benchmark is comprised of 42 unique kernels from various domains summarized in Table 2, totaling over 42,000 design points, providing a comprehensive resource for advancing research and facilitating an in-depth exploration of machine learning techniques for HLS.\\n\\n4 The HLSYN Benchmark\\n\\nIn this section, we introduce the datasets in HLSYN. The input data source comes from 42 selected kernels in the MachSuite benchmark [26] and the Polybench benchmark suite [44]. Our selected 42 kernels cover a wide range of applications whose descriptions are shown in Table 2. The benchmark consists of 2 datasets accumulated in the past three years, corresponding to two versions of AMD/Xilinx HLS tools: (1) SDX (V1) [3] and VITIS (V2) [4], with the AMD/Xilinx Alveo U200 as the target FPGA and a working frequency of 250MHz. For each dataset, we select 6 kernels as the held-out testing kernels. They will test the ability of a model to generalize to kernels that it did not see during the training. For the rest of the kernels, we perform a random split with the training, validation, and testing ratio being 70:15:15. Summary statistics of datasets are given in Table 3.\\n\\nFor each dataset kernel in each dataset, we run the two versions of the tools described above to obtain a set of labeled designs. Since the design space size is exponential to the number of pragmas, we rely 1 https://github.com/UCLA-DM/HLSyn 4\"}"}
{"id": "HvcLKgtbco", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: There are 42 kernels in total across SDX (V1) and VITIS (V2) spanning multiple domains such as linear algebra on vectors and matrices, data mining, stencil operations, encryption, dynamic programming, etc. #p denotes the number of pragmas in the kernel. # in v1 and # in v2 denote the number of labeled designs in SDX (V1) and VITIS (V2) respectively.\\n\\n| Kernel | Source | Description | # pragmas | # in v1 | # in v2 |\\n|--------|--------|-------------|-----------|--------|--------|\\n| 2 MM  | POLY  | 2 Matrix Multiplications | 14 | 812  | 861  |\\n| 3 MM  | POLY  | 3 Matrix Multiplications | 21 | 848  | -     |\\n| ADI   | POLY  | Alternating Direction Implicit solver | 13 | 551  | -     |\\n| AES   | MACH  | Advanced Encryption Standard | 3  | 45   | 43    |\\n| ATAX  | POLY  | Matrix Transpose and Vector Multiplication | 5  | 884  | 902  |\\n| ATAX  | MEDIUM| Matrix Transpose and Vector Multiplication | 5  | 362  | 544  |\\n| BICG  | POLY  | BiCG Sub Kernel of BiCGStab Linear Solver | 5  | 512  | 498  |\\n| BICG  | LARGE | BiCG Sub Kernel of BiCGStab Linear Solver | 4  | -    | 456  |\\n| BICG  | MEDIUM| BiCG Sub Kernel of BiCGStab Linear Solver | 5  | 316  | -     |\\n| CORRELATION | POLY  | Correlation Computation | 17 | 1522 | 699  |\\n| COVARIANCE | POLY  | Covariance Computation | 13 | -    | 356  |\\n| DOITGEN | POLY  | Multiresolution Analysis | 6  | 179  | 172  |\\n| DOITGEN | R   | Multiresolution Analysis | 7  | 595  | 230  |\\n| FDTD  | 2D POLY | 2-D Finite Different Time Domain Kernel | 16 | 660  | -     |\\n| FDTD  | L    | 2-D Finite Different Time Domain Kernel | 16 | -    | 240  |\\n| GEMM  | B MACH | Blocked Version of Matrix Multiplication | 9  | 775  | 440  |\\n| GEMM  | NMACH | Matrix Transpose and Vector Multiplication | 7  | 749  | 540  |\\n| GEMM  | PPOLY | Matrix Multiplication | 8  | 1160 | 714  |\\n| GEMM  | P- LPOLY | Matrix Transpose and Vector Multiplication | 8 | -    | 199  |\\n| GEMVER | POLY | Vector Multiplication and Matrix Addition | 13 | 924  | 712  |\\n| GEMVER | MPOLY | Vector Multiplication and Matrix Addition | 13 | 3365 | -     |\\n| GESUMMV | POLY | Scalar, Vector and Matrix Multiplication | 4  | 442  | 371  |\\n| GESUMMV | MPOLY | Scalar, Vector and Matrix Multiplication | 4  | 304  | -     |\\n| HEAT  | 3D POLY | Heat Equation over 3D Data Domain | 11 | 1664 | -     |\\n| JACOBI | 1D POLY | 1-D Jacobi Stencil Computation | 5  | 595  | -     |\\n| JACOBI | 2D POLY | 2-D Jacobi Stencil Computation | 11 | 1862 | -     |\\n| MD MACH | | n-body Molecular Dynamics | 3 | 12 | - |\\n| MVT   | POLY | Matrix-Vector Product and Transpose | 8  | 1175 | 1452 |\\n| MVT   | MMACH | Matrix-Vector Product and Transpose | 8  | 416  | -     |\\n| NW MACH | | Dynamic Programming for Sequence Alignment | 6 | 1347 | 615  |\\n| SEIDEL | 2D POLY | 2-D Seidel Stencil Computation | 7  | 1314 | -     |\\n| SPMV  | CRS MACH | Sparse Mat-Vec Mult. w/ Variable-Len. Neighbor | 3 | 114  | 114  |\\n| SPMV  | ELLPACK MACH | Sparse Mat-Vec Mult. w/ Fixed-size Neighbor | 3 | 114  | 102  |\\n| STENCIL | MACH | A Two-Dimensional Stencil Computation | 7  | 1404 | 1016 |\\n| STENCIL | 3D MACH | A Three-Dimensional Stencil Computation | 5  | 239  | 239  |\\n| SYMM  | POLY | Symmetric Matrix Multiplication | 7  | 153  | 158  |\\n| SYMM  | OPT POLY | Symmetric Matrix Multiplication | 8 | - | 281  |\\n| SYMM  | OPT MPOLY | Symmetric Matrix Multiplication | 8  | 315  | -     |\\n| SYR K KPOLY | | Symmetric Rank-2k Operations | 8 | 433  | 793  |\\n| SYRK  | POLY | Symmetric Rank-k Operations | 8  | 660  | 234  |\\n| TRMM  | POLY | Triangular Matrix Multiplication | 7  | 231  | 968  |\\n| TRMM  | OPT POLY | Triangular Matrix Multiplication | 7  | 964  | 281  |\\n\\nTable 3: Dataset statistics for the input. The meanings of columns are: #K: # kernels, A#K: average # pragmas per kernel, A#T: average # source code tokens per kernel, A#N: average # nodes per kernel's graph, A#E: average # edges per kernel's graph, #nt: # node types, #pt: # pragma node types, nr: numeric attribute range, #it: # instruction type, #ft: # flow types, #bt: # block types, #ept: # edge position types, #eft: # edge position type.\\n\\n| Dataset | #K | A#K | A#T | A#N | A#E | #nt | #pt | nr | #it | #ft | #bt | #ept | #eft |\\n|---------|----|-----|-----|-----|-----|-----|-----|----|-----|-----|-----|-----|------|-----|\\n| SDX (V1) | 37 | 8.0 | 629.9 | 366.7 | 589.6 | 4 | 7 | [0, 494] | 82 | 8 | 56 | 3 | 4 |\\n| VITIS (V2) | 29 | 7.6 | 629.9 | 334.0 | 534.7 | 4 | 7 | [0, 494] | 74 | 8 | 56 | 3 | 4 |\\n\\nOn heuristics provided by AutoDSE [34] to generate the labels for a subset of all possible designs. The labels come in the form of 5 target values: PERF, DSP, BRAM, LUT, and FF. In addition, according to whether the PERF is greater than a threshold value, we classify a design into two categories: valid and invalid. For the valid designs, we perform the regression task of predicting each one of the 5 target values. Table 4 shows the statistics of the prediction targets, i.e., output of a machine learning model.\\n\\nTable 4: Dataset statistics for the output. The meanings of columns are #D for r: # designs for the regression task, PERF: range of the PERF target, DSP: range of the DSP target, BRAM: range of the BRAM target, LUT: range of the LUT target, FF: range of the FF target, #D for c: # designs for the classification task, T:F: True class (Valid) vs False class (Invalid) design ratio.\\n\\n| Dataset | #D for r | PERF | DSP | BRAM | LUT | FF | #D for c | T:F |\\n|---------|---------|------|------|------|-----|----|---------|-----|\\n| SDX (V1) | 9439 | [-6.5, 1.5] | [0.0, 8.4] | [0.0, 3.0] | [0.0, 6.5] | [0.0, 3.2] | 28017 | 9439:18578 |\\n| VITIS (V2) | 5027 | [-3.6, 6.6] | [0.0, 6.6] | [0.0, 0.7] | [0.0, 5.6] | [0.0, 1.9] | 14273 | 5027:9246 |\"}"}
{"id": "HvcLKgtbco", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Visualization of the pragma-augmented PROGRAML graphs of four selected kernels. Node colors indicate the block attribute derived from the assembly code. Edge colors indicate the edge flow.\\n\\nSince our task aims to predict the validity (classification) and quality (5-target regression) of the designs, we provide both the source code and the graph representation derived from PROGRAML [13]. Specifically, we follow [13] to compile a kernel\u2019s source code into assembly code, and then transform the assembly codes into a control data flow graph (CDFG) with call relation, and eventually add pragma nodes to the PROGRAML graph following [30]. The resulting pragma-augmented PROGRAML graphs for four selected kernels are depicted in Figure 3.\\n\\nIt is noteworthy that there are multiple ways to represent the input source programs. As an illustration, we include in this dataset the graph representation used in [30]. Other representations are possible, such as abstract syntax trees. Since we release the source code, it is possible to derive and generate such other representations.\\n\\n5 Experiments on HLS\\n\\nThis section provides several baseline experiments with their results to investigate the performance of various methods on the task of design quality prediction.\\n\\n5.1 Baseline Methods\\n\\nAll the baseline methods share the same encoder-decoder architecture and differ only in the encoder. Specifically, each method encodes a design into a $D$-dimensional vector where $D = 512$ by default and uses a MLP-based decoder to transform the design embedding into the targets.\\n\\nCODE2VEC [2] CODE2VEC is a path-based attention model. It first decomposes the code to a collection of paths in its abstract syntax tree and represent them as a bag of distributed vector representations. Then it uses an attention mechanism to compute a learned weighted average of the path vectors as the overall representation of the code.\\n\\nCODET5-RAND, CODET5-FROZEN, and CODET5 [39] These three methods are based on the CODE2VEC method which performs pre-training on a large amount of source code. We utilize the small version released by the authors to encode the design. CODET5 fine-tunes both the encoder and the decoder, CODET5-FROZEN freezes the encoder and only fine-tunes the decoder, while CODET5-RAND fine-tunes both but replaces the encoder parameters/weights with a random initialization to study the effect of pre-training on our tasks.\\n\\nIn order to handle the long source code as a sequence of code tokens, we set the maximum sub-sequence length (i.e., the maximum number of tokens allowed in a sub-sequence) to be 64, and apply a sliding window of size 64 over the source code to obtain multiple sub-sequences as input to the transformer-based encoder. At the beginning of each sub-sequence, a special starting token is inserted and its embedding is taken as the sub-sequence level embedding, and all the sub-sequence embeddings are aggregated into the final $D$-dimensional embedding for the design.\\n\\nG-CODEBERT [18] and G-CODEBERT-L Similar to CODET5, G-CODEBERT is another pre-trained source code encoder, yet with a larger embedding dimension (768 instead of 512), and is thus\"}"}
{"id": "HvcLKgtbco", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Regression error (RMSE) on SD ($V_1$) and VITIS ($V_2$).\\n\\n| Method            | SD ($V_1$) | VITIS ($V_2$) |\\n|-------------------|------------|---------------|\\n| Trans Ind Ind Adapt |            |               |\\n| CODET 2 VEC        | 3.2877     | 4.2186        |\\n| CODET 5-RAND       | 1.7100     | 3.2206        |\\n| CODET 5-FROZEN     | 2.6808     | 2.9447        |\\n| CODET 5            | 0.5515     | 2.8301        |\\n| CODET 5 [39]       | 0.5541     | 2.6639        |\\n| CODET 5 [39] GNN   | 0.5651     | 2.7668        |\\n| CODET 5-GNN-DSE    | 0.8641     | 3.1366        |\\n| CODET 5-GNN-DSE-2L | 0.8020     | 2.6587        |\\n| CODET 5-GNN-GSE    | 0.4648     | 2.6620        |\\n| CODET 5-GNN-GSE-L  | 0.5502     | 2.8521        |\\n| G-CODEBERT         | 0.5541     | 2.6639        |\\n| G-CODEBERT-L       | 0.5651     | 2.7668        |\\n| GNN-DSE            | 0.8641     | 3.1366        |\\n| GNN-DSE-2L         | 0.8020     | 2.6587        |\\n\\npresumably more expressive.\\n\\nG-CODEBERT-L uses a larger maximum sub-sequence length, 128 instead of 64, which would capture a longer dependency between source code tokens, and may yield better results.\\n\\nGNN-DSE employs 8 layers whereas GNN-DSE-2L only utilizes 2.\\n\\nThese two methods concatenate the design embeddings produced by a source code transformer and a graph neural network-based encoder, i.e., the input embedding into the decoder is $2 \\\\times D$ instead of $D$.\\n\\n5.2 Evaluation Protocol\\n\\n**Metrics**\\n\\nThere are two tasks for our dataset: regression and classification. The goal of the regression task is to predict the five targets: PERF, DSP, BRAM, LUT, and FF. We use rooted mean square error (RMSE) to evaluate each method. And the goal of the classification task is to predict whether a design is valid or not, i.e., whether the downstream RTL and physical synthesis are likely to complete or not. We use the classification accuracy as the evaluation metric.\\n\\nIn addition, recall that we have two versions of the dataset (SD ($V_1$) and VITIS ($V_2$)). Each (version, task) combination receives a separate evaluation. For each evaluation, there are two evaluating settings: transductive and inductive testing. Specifically, for each (version, task) combination: (1) We select six kernels as the held-out testing kernels. These kernels are never seen during training and are used for the inductive testing; (2) For the rest of the kernels, we merge all the labeled designs, and randomly split them into training, validation, and transductive testing designs with the 70:15:15 ratio; (3) Using the training designs for 1000 epochs for the regression task, and 200 epochs for the classification task, we train each baseline method. We employ the validation set to determine the best epoch to use for testing; (4) We test the trained model on the transductive testing set. It is called transductive (\u201cTrans\u201d) since this testing set contains designs from kernels that are seen during training; (5) We test the trained model on the held-out six kernels. Specifically, we (5.1) select the 30% designs from each held-out kernel as the testing designs, (5.2) then repeat the following procedure 5 times. For each kernel, from the rest of the 70% remaining designs, 10 designs are sampled and are utilized to adapt the trained model for 10 epochs. Then, the adapted model is tested on the 30% designs for that held-out kernel. We call such a setting inductive because the model is tested on six kernels that are not visible in the training stage in a zero-shot (\u201cInd\u201d) or few-shot learning setting (\u201cInd Adapt\u201d).\\n\\n5.3 Results and Analysis\\n\\nThe overall regression and classification results are exhibited in Tables 5 and 6. Tables 7, 8, 9, and 10 reveal the breakdown results over individual held-out kernels.\\n\\nObservation 1: There is no consistent winner among the baselines. For the regression task, G-CODEBERT and G-CODEBERT-L achieve the lowest error when adapted to the held-out kernels, whereas, for the classification task, GNN-based methods perform better. Such a phenomenon calls for a hybrid model utilizing both the source code and the assembly code graph, e.g., the concatenation models [CODET 5, GNN-GSE] and [G-CODEBERT, GNN-GSE]. However, a simple concatenation of the\"}"}
{"id": "HvcLKgtbco", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: Classification accuracy on SD\\n\\n| Method | SD (V1) | VITIS (V2) |\\n|--------|---------|------------|\\n| Trans Ind | Ind Adapt | Trans Ind | Ind Adapt |\\n| CODE 2 | VEC | 0.7576 | 0.5662 | 0.6617 | 0.7060 |\\n| CODET 5-RAND | | 0.5444 | 0.6337 |\\n| CODET 5-FROZEN | | 0.7515 | 0.6098 | 0.7486 | 0.7334 | 0.4161 | 0.6061 |\\n| CODET 5 | | 0.9501 | 0.6394 | 0.7447 | 0.9045 | 0.4781 | 0.6734 |\\n| CODEBERT | | 0.9536 | 0.6478 | 0.7610 | 0.9233 |\\n| CODEBERT-L | | 0.9204 | 0.5730 | 0.7701 | 0.8970 | 0.5342 | 0.7180 |\\n| GNN-DSE | | 0.9422 | 0.6529 | 0.7623 | 0.9045 | 0.4781 | 0.6734 |\\n| GNN-DSE-2L | | 0.8912 | 0.6085 | 0.7421 | 0.9126 | 0.5303 |\\n| CODET 5, GNN-GSE | | 0.9434 | 0.6141 | 0.7385 | 0.9195 | 0.5053 | 0.7002 |\\n| GCODEBERT, GNN-GSE | | 0.9212 | 0.6174 | 0.7446 | 0.9126 | 0.5001 | 0.7160 |\\n\\nTable 7: Regression result breakdown on SD (V1) on individual test kernels.\\n\\n| Method | DOITGEN-R | FDTD-D | GEMM-N | JACOBI-D | STENCIL-D | TRMM-OPT |\\n|--------|------------|--------|--------|----------|-----------|----------|\\n| CODE 2 | VEC | 2.5711\u00b10.08 | 3.9973\u00b10.19 | 5.6191\u00b10.08 | 2.5710\u00b10.11 | 2.9029\u00b10.04 | 2.8322\u00b10.30 |\\n| CODET 5-RAND | | 1.2123\u00b10.02 | 3.3038\u00b10.10 | 3.8933\u00b10.11 | 1.8584\u00b10.12 | 0.9961\u00b10.14 | 2.1133\u00b10.09 |\\n| CODET 5-FROZEN | | 1.3197\u00b10.00 | 3.2819\u00b10.06 | 4.3073\u00b10.03 | 1.7756\u00b10.04 | 0.6807\u00b10.03 | 2.0248\u00b10.09 |\\n| CODET 5 | | 0.9990\u00b10.27 | 2.9818\u00b10.05 | 3.6910\u00b10.41 | 1.3470\u00b10.12 | 0.3695\u00b10.06 | 1.5336\u00b10.14 |\\n| CODEBERT | | 0.8301\u00b10.17 | 2.5938\u00b10.15 | 3.3672\u00b10.45 | 1.1581\u00b10.06 | 0.4830\u00b10.16 | 0.8990\u00b10.19 |\\n| CODEBERT-L | | 0.5376\u00b10.08 | 2.8015\u00b10.18 | 3.1310\u00b10.52 | 1.0214\u00b10.12 | 0.3636\u00b10.07 | 0.9384\u00b10.10 |\\n| GNN-DSE | | 0.5278\u00b10.03 | 2.6449\u00b10.10 | 2.8907\u00b10.51 | 1.3855\u00b10.10 | 0.6918\u00b10.23 | 0.9673\u00b10.05 |\\n| GNN-DSE-2L | | 0.9113\u00b10.16 | 2.6677\u00b10.11 | 2.7881\u00b10.28 | 1.5230\u00b10.07 | 0.6984\u00b10.09 | 1.1499\u00b10.05 |\\n| CODET 5, GNN-GSE | | 0.9637\u00b10.20 | 2.7998\u00b10.30 | 2.9908\u00b10.16 | 1.2991\u00b10.14 | 0.7373\u00b10.14 | 1.0056\u00b10.04 |\\n| GCODEBERT, GNN-GSE | | 1.0409\u00b10.21 | 2.7988\u00b10.17 | 3.2942\u00b10.23 | 1.0845\u00b10.12 | 0.3072\u00b10.06 | 0.7891\u00b10.24 |\\n\\nTable 8: Regression result breakdown on VITIS (V2) on individual test kernels.\\n\\n| Method | COVARIANCE | FDTD-D | L GEMM-N | P-L SYMM TRMM-OPT |\\n|--------|------------|--------|----------|------------------|\\n| CODE 2 | VEC | 2.4977\u00b10.09 | 3.1259\u00b10.18 | 4.5526\u00b10.19 | 3.7389\u00b10.03 | 1.4209\u00b10.11 | 2.6379\u00b10.21 |\\n| CODET 5-RAND | | 1.6388\u00b10.13 | 2.4587\u00b10.22 | 2.8477\u00b10.15 | 3.0722\u00b10.04 | 0.4969\u00b10.03 | 1.2295\u00b10.09 |\\n| CODET 5-FROZEN | | 1.2137\u00b10.02 | 2.4494\u00b10.22 | 2.6576\u00b10.10 | 3.1420\u00b10.02 | 0.3854\u00b10.02 | 1.2199\u00b10.02 |\\n| CODET 5 | | 1.3594\u00b10.15 | 2.2958\u00b10.07 | 2.8436\u00b10.37 | 2.7803\u00b10.07 | 0.4092\u00b10.09 | 0.8427\u00b10.14 |\\n| CODEBERT | | 1.0652\u00b10.06 | 1.9245\u00b10.09 | 2.3772\u00b10.31 | 2.3913\u00b10.04 | 0.4019\u00b10.08 | 0.9452\u00b10.09 |\\n| CODEBERT-L | | 0.9107\u00b10.08 | 1.7740\u00b10.11 | 2.6635\u00b10.33 | 2.5029\u00b10.09 | 0.3384\u00b10.03 | 1.1469\u00b10.11 |\\n| GNN-DSE | | 1.1496\u00b10.04 | 1.8897\u00b10.04 | 2.3157\u00b10.23 | 2.7828\u00b10.18 | 0.4110\u00b10.02 | 0.7790\u00b10.06 |\\n| GNN-DSE-2L | | 1.1457\u00b10.02 | 1.9521\u00b10.09 | 2.0248\u00b10.26 | 2.9691\u00b10.10 | 0.4067\u00b10.04 | 0.9146\u00b10.03 |\\n| CODET 5, GNN-GSE | | 1.0988\u00b10.07 | 1.8532\u00b10.10 | 2.2430\u00b10.23 | 2.5489\u00b10.04 | 0.3766\u00b10.06 | 1.0440\u00b10.09 |\\n| GCODEBERT, GNN-GSE | | 1.1182\u00b10.07 | 1.9088\u00b10.10 | 2.5352\u00b10.25 | 2.7655\u00b10.08 | 0.3842\u00b10.05 | 0.8986\u00b10.13 |\\n\\nTable 9: Classification result breakdown on SD (V1) on individual test kernels.\\n\\n| Method | DOITGEN-R | FDTD-D | GEMM-N | JACOBI-D | STENCIL-D | TRMM-OPT |\\n|--------|------------|--------|--------|----------|-----------|----------|\\n| CODE 2 | VEC | 0.6449\u00b10.07 | 0.6192\u00b10.01 | 0.4598\u00b10.03 | 0.8122\u00b10.05 | 0.6085\u00b10.03 | 0.8256\u00b10.04 |\\n| CODET 5-RAND | | 0.6663\u00b10.05 | 0.5283\u00b10.08 | 0.6366\u00b10.08 | 0.8384\u00b10.03 | 0.5887\u00b10.11 | 0.9509\u00b10.00 |\\n| CODET 5-FROZEN | | 0.7472\u00b10.00 | 0.6414\u00b10.00 | 0.6286\u00b10.17 | 0.8889\u00b10.00 | 0.6338\u00b10.00 | 0.9516\u00b10.00 |\\n| CODET 5 | | 0.7236\u00b10.04 | 0.6455\u00b10.02 | 0.5804\u00b10.04 | 0.8681\u00b10.01 | 0.7042\u00b10.05 | 0.9467\u00b10.01 |\\n| CODEBERT | | 0.6607\u00b10.06 | 0.6455\u00b10.03 | 0.6714\u00b10.03 | 0.8509\u00b10.02 | 0.8000\u00b10.04 | 0.9377\u00b10.02 |\\n| CODEBERT-L | | 0.7236\u00b10.08 | 0.6556\u00b10.03 | 0.5741\u00b10.07 | 0.8792\u00b10.01 | 0.8394\u00b10.05 | 0.9488\u00b10.01 |\\n| GNN-DSE | | 0.6506\u00b10.06 | 0.6889\u00b10.02 | 0.6402\u00b10.03 | 0.8652\u00b10.03 | 0.7803\u00b10.03 | 0.9488\u00b10.00 |\\n| GNN-DSE-2L | | 0.6674\u00b10.05 | 0.6778\u00b10.03 | 0.6545\u00b10.02 | 0.8534\u00b10.03 | 0.6648\u00b10.04 | 0.9349\u00b10.03 |\\n| CODET 5, GNN-GSE | | 0.7022\u00b10.07 | 0.6364\u00b10.02 | 0.6438\u00b10.03 | 0.8419\u00b10.03 | 0.6620\u00b10.04 | 0.9446\u00b10.01 |\\n| GCODEBERT, GNN-GSE | | 0.7124\u00b10.06 | 0.7091\u00b10.04 | 0.6393\u00b10.04 | 0.8616\u00b10.04 | 0.6028\u00b10.09 | 0.9426\u00b10.02 |\\n\\nTable 10: Classification result breakdown on VITIS (V2) on individual test kernels.\\n\\n| Method | COVARIANCE | FDTD-D | L GEMM-N | P-L SYMM TRMM-OPT |\\n|--------|------------|--------|----------|------------------|\\n| CODE 2 | VEC | 0.5830\u00b10.05 | 0.5750\u00b10.02 | 0.5099\u00b10.02 | 0.6678\u00b10.05 | 0.7191\u00b10.05 | 0.7476\u00b10.03 |\\n| CODET 5-RAND | | 0.6453\u00b10.03 | 0.6444\u00b10.04 | 0.6185\u00b10.03 | 0.6678\u00b10.04 | 0.6936\u00b10.05 | 0.5048\u00b10.25 |\\n| CODET 5-FROZEN | | 0.6698\u00b10.00 | 0.4778\u00b10.11 | 0.4951\u00b10.04 | 0.6610\u00b10.00 | 0.5234\u00b10.11 | 0.8095\u00b10.00 |\\n| CODET 5 | | 0.6698\u00b10.04 | 0.5750\u00b10.06 | 0.5728\u00b10.03 | 0.6983\u00b10.05 | 0.6766\u00b10.04 | 0.8476\u00b10.03 |\\n| CODEBERT | | 0.5585\u00b10.07 | 0.6111\u00b10.07 | 0.5840\u00b10.03 | 0.8373\u00b10.04 | 0.9021\u00b10.05 | 0.7214\u00b10.07 |\\n| CODEBERT-L | | 0.5887\u00b10.05 | 0.6583\u00b10.08 | 0.6778\u00b10.03 | 0.8203\u00b10.07 | 0.7702\u00b10.07 | 0.7929\u00b10.02 |\\n| GNN-DSE | | 0.6509\u00b10.01 | 0.7000\u00b10.07 | 0.6840\u00b10.06 | 0.7559\u00b10.05 | 0.8170\u00b10.08 | 0.8381\u00b10.03 |\\n| GNN-DSE-2L"}
{"id": "HvcLKgtbco", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Observation 1: Design embeddings does not consistently yield better performance. Particularly, [CODET 5, GNN-GSE] and [G-CODEBERT, GNN-GSE] achieve the lowest regression error under the transductive setting but fall short when adapted to new kernels. Such results imply that future efforts can be made on studying the generalization abilities of machine learning models on our HLS YN benchmark.\\n\\nObservation 2: In general, pre-training helps with the performance of source code transformer models. This can be seen by comparing CODET 5-RAND and CODET 5, where the former starts training from scratch while the latter loads a pre-trained model released by [39]. This should not come as a surprise, because pre-training has been shown to demonstrate success in natural language processing. Our experimental results verify the effectiveness of pre-training on the HLS YN benchmark. One implication is that one can design better pre-training methods on source code related to electronic design automation, or even design pre-training for GNNs operating on assembly-level graphs.\\n\\nObservation 3: More GNN message passing layers does not necessarily improve the performance of GNN models. In many cases, the 2-layer version, GNN-DSE-2L, performs even better than the 8-layer version, GNN-DSE. This may be attributed to the fact that an attention-based global readout function is used to aggregate node embeddings into a graph-level embedding representing the entire design, and thus each node does not necessarily need to reach far-away nodes in the local message passing stage.\\n\\nObservation 4: Generalization to new kernels is difficult, and the performance after adaptation differs across kernels. For example, G-CODEBERT-L achieves the overall lowest error (\u201cInd Adapt\u201d) on SDX (V1) on the regression task, but Table 7 demonstrates that G-CODEBERT-L does not always yield the lowest error on each of the six held-out kernels. For example, [G-CODEBERT, GNN-GSE] performs the best on STENCIL-3D and TRMM-OPT, but poorly on DOITGEN-R, and thus the average regression error over the six held-out kernels is higher than G-CODEBERT-L. This calls for a more in-depth study of the discrepancy between kernels and a model that is capable of generalizing to a diverse set of kernels. In general, adaptation is necessary, since across all the experiments, without any adaptation (\u201cInd\u201d), directly applying the trained model to new unseen kernels leads to poor regression error and classification accuracy.\\n\\n6 Conclusion and Future Work\\n\\nThis work introduces the task of design quality prediction in the forms of regression and classification tasks and presents the HLS YN benchmark to evaluate state-of-the-art program representation learning methods. Although there is no method that consistently outperforms all the other methods, we notice several trends and identify promising directions toward a more accurate prediction model design. As program representation learning is a continuously growing research domain, we plan to maintain the benchmark to test new methods. For example, our recent work [31] uses hierarchical graphs for program representations to predict design performance. Our HLS YN benchmark is a growing project. We expect to include more kernels and labeled designs running newer versions of HLS tools and establish a leaderboard to encourage participation. In addition, the current benchmark does not consider the design space exploration (DSE) stage, which will be added as the project develops. In fact, the regression task aims to be eventually integrated into the DSE process, which traverses the design space in order to find the optimal pragma setting.\\n\\n7 Acknowledgement\\n\\nThis work was partially supported by NSF 2211557, NSF 1937599, NSF 2119643, NSF 2303037, NASA, SRC JUMP 2.0 Center, Okawa Foundation, Amazon Research, Cisco, Picsart, Snapchat, and CDSC industrial partners (https://cdsc.ucla.edu/partners/). We would also like to thank AMD/Xilinx for HACC equipment donation and Marci Baun for editing the paper.\\n\\nReferences\\n\\n[1] Sergi Abadal, Akshay Jain, Robert Guirado, Jorge L\u00f3pez-Alonso, and Eduard Alarc\u00f3n. Computing graph neural networks: A survey from algorithms to accelerators. ACM Computing Surveys (CSUR), https://github.com/UCLA-DM/HLSyn#leaderboard\"}"}
{"id": "HvcLKgtbco", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[2] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. code2vec: Learning distributed representations of code. Proceedings of the ACM on Programming Languages, 3(POPL):1\u201329, 2019.\\n\\n[3] AMD/Xilinx Vivado HLS. https://docs.xilinx.com/v/u/2018.3-English/ug902-vivado-high-level-synthesis.\\n\\n[4] AMD/Xilinx Vivado HLS. https://docs.xilinx.com/v/u/2020.2-English/ug1416-vitis-documentation.\\n\\n[5] Yunsheng Bai, Atefeh Sohrabizadeh, Yizhou Sun, and Jason Cong. Improving GNN-based accelerator design automation with meta learning. In Proceedings of the 59th ACM/IEEE Design Automation Conference, pages 1347\u20131350, 2022.\\n\\n[6] Cadence Stratus High-Level Synthesis. https://www.cadence.com/en_US/home/tools/digital-design-and-signoff/synthesis/stratus-high-level-synthesis.html.\\n\\n[7] David Castells-Rufas, Vinh Ngo, Juan Borrego-Carazo, Marc Codina, Carles Sanchez, Debora Gil, and Jordi Carrabina. A survey of FPGA-based vision systems for autonomous cars. IEEE Access, 10:132525\u2013132563, 2022.\\n\\n[8] Yuze Chi, Weikang Qiao, Atefeh Sohrabizadeh, Jie Wang, and Jason Cong. Democratizing domain-specific computing. Communications of the ACM, 66(1):74\u201385, 2022.\\n\\n[9] Eric Chung, Jeremy Fowers, Kalin Ovtcharov, Michael Papamichael, Adrian Caulfield, Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, Michael Haselman, et al. Serving DNNs in real time at datacenter scale with project brainwave. IEEE Micro, 38(2):8\u201320, 2018.\\n\\n[10] Jason Cong, Jason Lau, Gai Liu, Stephen Neuendorffer, Peichen Pan, Kees Vissers, and Zhiru Zhang. FPGA HLS today: successes, challenges, and opportunities. ACM Transactions on Reconfigurable Technology and Systems (TRETS), 15(4):1\u201342, 2022.\\n\\n[11] Jason Cong, Bin Liu, Stephen Neuendorffer, Juanjo Noguera, Kees Vissers, and Zhiru Zhang. High-level synthesis for FPGAs: From prototyping to deployment. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 30(4):473\u2013491, 2011.\\n\\n[12] Jason Cong, Peng Wei, Cody Hao Yu, and Peng Zhang. Automated accelerator generation and optimization with composable, parallel and pipeline architecture. In DAC, 2018.\\n\\n[13] Chris Cummins, Zacharias V Fisches, Tal Ben-Nun, Torsten Hoefler, Michael FP O'Boyle, and Hugh Leather. Programl: A graph-based program representation for data flow analysis and compiler optimizations. In International Conference on Machine Learning, pages 2244\u20132253. PMLR, 2021.\\n\\n[14] William J Dally, Yatish Turakhia, and Song Han. Domain-specific hardware accelerators. Communications of the ACM, 63(7):48\u201357, 2020.\\n\\n[15] Javier Duarte, Philip Harris, Scott Hauck, Burt Holzman, Shih-Chieh Hsu, Sergo Jindariani, Suffian Khan, Benjamin Kreis, Brian Lee, Mia Liu, et al. FPGA-accelerated machine learning inference as a service for particle physics computing. Computing and Software for Big Science, 3:1\u201315, 2019.\\n\\n[16] Lorenzo Ferretti, Jihye Kwon, Giovanni Ansaloni, Giuseppe Di Guglielmo, Luca Carloni, and Laura Pozzi. DB4HLS: a database of high-level synthesis design space explorations. IEEE Embedded Systems Letters, 13(4):194\u2013197, 2021.\\n\\n[17] Jeremy Fowers, Kalin Ovtcharov, Michael Papamichael, Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, Michael Haselman, Logan Adams, Mahdi Ghandi, et al. A configurable cloud-scale DNN processor for real-time AI. In 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA), pages 1\u201314. IEEE, 2018.\\n\\n[18] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. Graphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366, 2020.\\n\\n[19] Muhuan Huang, Di Wu, Cody Hao Yu, Zhenman Fang, Matteo Interlandi, Tyson Condie, and Jason Cong. Programming and runtime support to blaze FPGA accelerator deployment at datacenter scale. In Proceedings of the Seventh ACM Symposium on Cloud Computing, pages 456\u2013469, 2016.\"}"}
{"id": "HvcLKgtbco", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[108x710][21] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual international symposium on computer architecture, pages 1\u201312, 2017.\\n\\n[22] David Koeplinger, Raghu Prabhakar, Yaqi Zhang, Christina Delimitrou, Christos Kozyrakis, and Kunle Olukotun. Automatic generation of efficient accelerators for reconfigurable hardware. In ISCA, pages 115\u2013127, 2016.\\n\\n[23] Hung-Yi Liu and Luca P Carloni. On learning-based methods for design-space exploration with high-level synthesis. In DAC, pages 1\u20137, 2013.\\n\\n[24] NEC CyberWorkBench. https://www.nec.com/en/global/prod/cwb/index.html.\\n\\n[25] Andrew Putnam, Adrian Caulfield, Eric Chung, Derek Chiou, Kypros Constantinides, John Demme, Hadi Esmaeilzadeh, Jeremy Fowers, Jan Gray, Michael Haselman, Scott Hauck, Stephen Heil, Amir Hormati, Joo-Young Kim, Sitaram Lanka, Eric Peterson, Aaron Smith, Jason Thong, Phillip Yi Xiao, Doug Burger, Jim Larus, Gopi Prashanth Gopal, and Simon Pope. A reconfigurable fabric for accelerating large-scale datacenter services. In Proceeding of the 41st Annual International Symposium on Computer Architecture (ISCA), pages 13\u201324. IEEE Press, June 2014. Selected as an IEEE Micro TopPick.\\n\\n[26] Brandon Reagen, Robert Adolf, Yakun Sophia Shao, Gu-Yeon Wei, and David Brooks. Machsuite: Benchmarks for accelerator design and customized architectures. In IISWC, 2014.\\n\\n[27] Ahmad Shawahna, Sadiq M Sait, and Aiman El-Maleh. FPGA-based accelerators of deep learning networks for learning and classification: A review. IEEE Access, 7:7823\u20137859, 2018.\\n\\n[28] Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjin Wang, and Yu Sun. Masked label prediction: Unified message passing model for semi-supervised classification. IJCAI, 2021.\\n\\n[29] Siemens Catapult High-Level Synthesis. https://eda.sw.siemens.com/en-US/ic/ic-design/high-level-synthesis-and-verification-platform/.\\n\\n[30] Atefeh Sohrabizadeh, Yunsheng Bai, Yizhou Sun, and Jason Cong. Automated accelerator optimization aided by graph neural networks. In Proceedings of the 59th ACM/IEEE Design Automation Conference, pages 55\u201360, 2022.\\n\\n[31] Atefeh Sohrabizadeh, Yunsheng Bai, Yizhou Sun, and Jason Cong. Robust GNN-based representation learning for HLS. In The International Conference on Computer-Aided Design, 2023.\\n\\n[32] Atefeh Sohrabizadeh, Yuze Chi, and Jason Cong. StreamGCN: Accelerating graph convolutional networks with streaming processing. In 2022 IEEE Custom Integrated Circuits Conference (CICC), pages 1\u20138. IEEE, 2022.\\n\\n[33] Atefeh Sohrabizadeh, Jie Wang, and Jason Cong. End-to-end optimization of deep learning applications. In Proceedings of the 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, pages 133\u2013139, 2020.\\n\\n[34] Atefeh Sohrabizadeh, Cody Hao Yu, Min Gao, and Jason Cong. AutoDSE: Enabling software programmers to design efficient FPGA accelerators. ACM Transactions on Design Automation of Electronic Systems (TODAES), 27(4):1\u201327, 2022.\\n\\n[35] Qi Sun, Tinghuan Chen, Siting Liu, Jin Miao, Jianli Chen, Hao Yu, and Bei Yu. Correlated multi-objective multi-fidelity optimization for HLS directives design. In IEEE/ACM Proceedings Design, Automation and Test in Europe (DATE), pages 01\u201305, 2021.\\n\\n[36] Ecenur Ustun, Chenhui Deng, Debjit Pal, Zhijing Li, and Zhiru Zhang. Accurate operation delay prediction for FPGA HLS using graph neural networks. In Proceedings of the 39th International Conference on Computer-Aided Design, pages 1\u20139, 2020.\\n\\n[37] Jie Wang, Licheng Guo, and Jason Cong. AutoSA: A polyhedral compiler for high-performance systolic arrays on FPGA. In Proceedings of the 2021 ACM/SIGDA international symposium on Field-programmable gate arrays, 2021.\\n\\n[38] Shuo Wang, Yun Liang, and Wei Zhang. Flexcl: An analytical performance model for OpenCL workloads on flexible FPGAs. In DAC, pages 1\u20136, 2017.\"}"}
{"id": "HvcLKgtbco", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. EMNLP, 2021.\\n\\nNan Wu, Yuan Xie, and Cong Hao. Ironman-pro: Multi-objective design space exploration in HLS via reinforcement learning and graph neural network based modeling. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2022.\\n\\nNan Wu, Hang Yang, Yuan Xie, Pan Li, and Cong Hao. High-level synthesis performance prediction using GNNs: Benchmarking, modeling, and advancing. In Proceedings of the 59th ACM/IEEE Design Automation Conference, pages 49\u201354, 2022.\\n\\nKeyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. ICML, 2018.\\n\\nCody Hao Yu, Peng Wei, Max Grossman, Peng Zhang, Vivek Sarker, and Jason Cong. S2FA: an accelerator automation framework for heterogeneous computing in datacenters. In DAC, pages 1\u20136, 2018.\\n\\nTomofumi Yuki and Louis-No\u00ebl Pouchet. Polybench/c.\\n\\nJieru Zhao, Liang Feng, Sharad Sinha, Wei Zhang, Yun Liang, and Bingsheng He. COMBA: A comprehensive model-based analysis framework for high level synthesis of real applications. In ICCAD, pages 430\u2013437, 2017.\\n\\nGuanwen Zhong, Alok Prakash, Yun Liang, Tulika Mitra, and Smail Niar. Lin-analyzer: a high-level performance analysis tool for FPGA-based accelerators. In DAC, pages 1\u20136, 2016.\\n\\nGuanwen Zhong, Vanchinathan Venkataramani, Yun Liang, Tulika Mitra, and Smail Niar. Design space exploration of multiple loops on FPGAs using high level synthesis. In ICCD, pages 456\u2013463, 2014.\"}"}
{"id": "HvcLKgtbco", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: There are 42 kernels in total across SDX (V1) and VITIS (V2) spanning multiple domains such as linear algebra on vectors and matrices, data mining, stencil operations, encryption, dynamic programming, etc. #p denotes the number of pragmas in the kernel. # in v1 and # in v2 denote the number of labeled designs in SDX (V1) and VITIS (V2) respectively.\\n\\n| Kernel | Source | Description | # pragmas | # in v1 | # in v2 |\\n|--------|--------|-------------|-----------|---------|---------|\\n| MM_POLYBENCH | 2 Matrix Multiplications | 14 | 812 | 861 |\\n| MM_POLYBENCH | 3 Matrix Multiplications | 21 | | |\\n| ADIPOLYBENCH | Alternating Direction Implicit solver | 13 | 551 | |\\n| AESMACH | Advanced Encryption Standard | 3 | 45 | 43 |\\n| ATAXPOLYBENCH | Matrix Transpose and Vector Multiplication | 5 | 884 | 902 |\\n| ATAXMEDIUMPOLYBENCH | Matrix Transpose and Vector Multiplication | 5 | 362 | 544 |\\n| BICGBENCH | BiCG Sub Kernel of BiCGStab Linear Solver | 5 | 512 | 498 |\\n| BICGLARGEPOLYBENCH | BiCG Sub Kernel of BiCGStab Linear Solver | 4 | | 456 |\\n| BICGMEDIUMPOLYBENCH | BiCG Sub Kernel of BiCGStab Linear Solver | 5 | 316 | |\\n| CORRELATIONPOLYBENCH | Correlation Computation | 17 | 1522 | 699 |\\n| COVARIANCEPOLYBENCH | Covariance Computation | 13 | | 356 |\\n| DOITGENPOLYBENCH | Multiresolution Analysis | 6 | 179 | 172 |\\n| DOITGENRPOLYBENCH | Multiresolution Analysis | 7 | 595 | 230 |\\n| FDTD-2D | 2-D Finite Different Time Domain Kernel | 16 | 660 | |\\n| FDTD-2DLPOLYBENCH | 2-D Finite Different Time Domain Kernel | 16 | | 240 |\\n| GEMM-BMACH | Blocked Version of Matrix Multiplication | 9 | 775 | 440 |\\n| GEMM-NMACH | Matrix Transpose and Vector Multiplication | 7 | 749 | 540 |\\n| GEMMPOLYBENCH | Matrix Multiplication | 8 | 1160 | 714 |\\n| GEMMPOLYBENCH | Matrix Transpose and Vector Multiplication | 8 | | 199 |\\n| GEMVERPOLYBENCH | Vector Multiplication and Matrix Addition | 13 | 924 | 712 |\\n| GEMVERMPOLYBENCH | Vector Multiplication and Matrix Addition | 13 | 3365 | |\\n| GESUMMVMPOLYBENCH | Scalar, Vector and Matrix Multiplication | 4 | 442 | 371 |\\n| GESUMMVMPOLYBENCH | Scalar, Vector and Matrix Multiplication | 4 | 304 | |\\n| HEATMEDIUM | Heat Equation over 3D Data Domain | 11 | 1664 | |\\n| JACOBI1D | 1-D Jacobi Stencil Computation | 5 | 595 | |\\n| JACOBI2D | 2-D Jacobi Stencil Computation | 11 | 1862 | |\\n| JACOBI3D | 2-D Seidel Stencil Computation | 7 | 1314 | |\\n| MDMACH | n-body Molecular Dynamics | 3 | 12 | |\\n| MVTMPOLYBENCH | Matrix-Vector Product and Transpose | 8 | 1175 | 1452 |\\n| MVTMPOLYBENCH | Matrix-Vector Product and Transpose | 8 | 416 | |\\n| NWMPOLYBENCH | Dynamic Programming for Sequence Alignment | 6 | 1347 | 615 |\\n| SEIDELMEDIUM | 2-D Seidel Stencil Computation | 7 | 1314 | |\\n| SPMVCRSMACH | Sparse Mat-Vec Mult. w/ Variable-Len. Neighbor | 3 | 114 | 114 |\\n| SPMVELLPACKMACH | Sparse Mat-Vec Mult. w/ Fixed-size Neighbor | 3 | 114 | 102 |\\n| STENCMACH | A Two-Dimensional Stencil Computation | 7 | 1404 | 1016 |\\n| STENCMEDIUM | A Three-Dimensional Stencil Computation | 5 | 239 | 239 |\\n| SYMMPOLYBENCH | Symmetric Matrix Multiplication | 7 | 153 | 158 |\\n| SYMMOPTPOLYBENCH | Symmetric Matrix Multiplication | 8 | | 281 |\\n| SYMMOPTMPOLYBENCH | Symmetric Matrix Multiplication | 8 | 315 | |\\n| SYRKPOLYBENCH | Symmetric Rank-k Operations | 8 | 660 | 234 |\\n| TRMMPOLYBENCH | Triangular Matrix Multiplication | 7 | 231 | 968 |\\n| TRMOPTPOLYBENCH | Triangular Matrix Multiplication | 7 | 964 | 281 |\\n\\nTable 3: Dataset statistics for the input. The meanings of columns are:\\n\\n- #K: # kernels,\\n- #K: average # pragmas per kernel,\\n- #K: average # source code tokens per kernel,\\n- #K: average # nodes per kernel's graph,\\n- #K: average # edges per kernel's graph,\\n- #nt: # node types,\\n- #pt: # pragma node types,\\n- nr: numeric attribute range,\\n- #it: # instruction type,\\n- #ft: # flow types,\\n- #bt: # block types,\\n- #ept: # edge position types,\\n- #eft: # edge position type.\\n\\n| Dataset | #K | #K | #K | #K | #nt | #pt | nr | #it | #ft | #bt | #ept | #eft |\\n|---------|----|----|----|----|-----|-----|----|-----|-----|-----|-----|-----|\\n| SDX (V1) | 37 | 8.0 | 629.9 | 366.7 | 589.6 | 4 | 7 | [0, 494] | 82 | 8 | 56 |\\n| VITIS (V2) | 29 | 7.6 | 629.9 | 334.0 | 534.7 | 4 | 7 | [0, 494] | 74 | 8 | 56 |\\n\\non heuristics provided by AutoDSE [34] to generate the labels for a subset of all possible designs. The labels come in the form of 5 target values: PERF, DSP, BRAM, LUT, and FF. In addition, according to whether the PERF is greater than a threshold value, we classify a design into two categories: valid and invalid. For the valid designs, we perform the regression task of predicting each one of the 5 target values. Table 4 shows the statistics of the prediction targets, i.e., output of a machine learning model.\\n\\nTable 4: Dataset statistics for the output. The meanings of columns are\\n\\n- #D for r: # designs for the regression task,\\n- #D for c: # designs for the classification task,\\n- PERF: range of the PERF target,\\n- DSP: range of the DSP target,\\n- BRAM: range of the BRAM target,\\n- LUT: range of the LUT target,\\n- FF: range of the FF target,\\n- T:F: True class (Valid) vs False class (Invalid) design ratio.\\n\\n| Dataset | #D for r | PERF | DSP | BRAM | LUT | FF | #D for c | T:F |\\n|---------|---------|------|-----|------|-----|----|----------|-----|\\n| SDX (V1) | 9439 | [-6.5, 1.5] | [0.0, 8.4] | [0.0, 3.0] | [0.0, 6.5] | [0.0, 3.2] | 28017 | 9439:18578 |\\n| VITIS (V2) | 5027 | [-3.6, 6.6] | [0.0, 6.6] | [0.0, 0.7] | [0.0, 5.6] | [0.0, 1.9] | 14273 | 5027:9246 |\"}"}
{"id": "HvcLKgtbco", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) Graph of 2M consist-ing of 354 nodes and 566 edges.  \\n(b) Graph of CORRELA-tion consisting of 662 nodes and 1071 edges.  \\n(c) Graph of MVT consisting of 206 nodes and 328 edges.  \\n(d) Graph of NW consisting of 439 nodes and 704 edges.  \\n\\nFigure 3: Visualization of the pragma-augmented PROGRA ML graphs of four selected kernels. Node colors indicate the block attribute derived from the assembly code. Edge colors indicate the edge flow.\\n\\nSince our task aims to predict the validity (classification) and quality (5-target regression) of the designs, we provide both the source code and the graph representation derived from PROGRA ML [13]. Specifically, we follow [13] to compile a kernel's source code into assembly code, and then transform the assembly codes into a control data flow graph (CDFG) with call relation, and eventually add pragma nodes to the PROGRA ML graph following [30]. The resulting pragma-augmented PROGRA ML graphs for four selected kernels are depicted in Figure 3.\\n\\nIt is noteworthy that there are multiple ways to represent the input source programs. As an illustration, we include in this dataset the graph representation used in [30]. Other representations are possible, such as abstract syntax trees. Since we release the source code, it is possible to derive and generate such other representations.\\n\\n5 Experiments on HLS\\n\\nThis section provides several baseline experiments with their results to investigate the performance of various methods on the task of design quality prediction.\\n\\n5.1 Baseline Methods\\n\\nAll the baseline methods share the same encoder-decoder architecture and differ only in the encoder. Specifically, each method encodes a design into a $D$-dimensional vector where $D = 512$ by default and uses a MLP-based decoder to transform the design embedding into the targets.\\n\\n**CODE 2VEC** [2] is a path-based attention model. It first decomposes the code to a collection of paths in its abstract syntax tree and represent them as a bag of distributed vector representations. Then it uses an attention mechanism to compute a learned weighted average of the path vectors as the overall representation of the code.\\n\\n**CODET 5-RAND**, **CODET 5-FROZEN**, and **CODET 5 [39]**. These three methods are based on the CODE 2VEC method which performs pre-training on a large amount of source code. We utilize the small version released by the authors to encode the design. **CODET 5 fine-tunes both the encoder and the decoder, CODET 5-FROZEN freezes the encoder and only fine-tunes the decoder, while CODET 5-RAND fine-tunes both but replaces the encoder parameters/weights with a random initialization to study the effect of pre-training on our tasks.\\n\\nIn order to handle the long source code as a sequence of code tokens, we set the maximum sub-sequence length (i.e., the maximum number of tokens allowed in a sub-sequence) to be 64, and apply a sliding window of size 64 over the source code to obtain multiple sub-sequences as input to the transformer-based encoder. At the beginning of each sub-sequence, a special starting token is inserted and its embedding is taken as the sub-sequence level embedding, and all the sub-sequence embeddings are aggregated into the final $D$-dimensional embedding for the design.\\n\\n**G-CODEBERT** [18] and **G-CODEBERT-L** Similar to CODET 5, G-CODEBERT is another pre-trained source code encoder, yet with a larger embedding dimension (768 instead of 512), and is thus\"}"}
{"id": "HvcLKgtbco", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Regression error (RMSE) on SD X (V1) and VITIS (V2).\\n\\n| Method          | SD X (V1) | VITIS (V2) |\\n|-----------------|-----------|------------|\\n| CODE 2 VEC      | 3.2877    | 4.2186     |\\n| CODET 5-RAND    | 1.7100    | 3.2206     |\\n| CODET 5-FROZEN  | 2.6808    | 2.9447     |\\n| CODET 5         | 0.5515    | 2.8301     |\\n| CODET 5 [39]    | 0.5515    | 2.8301     |\\n| CODET 5 [39]    | 0.5541    | 2.6639     |\\n| CODET 5 [39]    | 0.5651    | 2.7668     |\\n| CODET 5         | 0.4648    | 2.6620     |\\n| CODET 5         | 0.5502    | 2.8521     |\\n| CODET 5         | 0.5515    | 2.8301     |\\n\\npresumably more expressive. G - CODEBERT-L uses a larger maximum sub-sequence length, 128 instead of 64, which would capture a longer dependency between source code tokens, and may yield better results.\\n\\nGNN-DSE employs 8 layers whereas GNN-DSE-2L only utilizes 2.\\n\\nThese two methods concatenate the design embeddings produced by a source code transformer and a graph neural network-based encoder, i.e., the input embedding into the decoder is $2 \\\\times D$ instead of $D$.\\n\\n5.2 Evaluation Protocol\\n\\nMetrics\\n\\nThere are two tasks for our dataset: regression and classification. The goal of the regression task is to predict the five targets: PERF, DSP, BRAM, LUT, and FF. We use rooted mean square error (RMSE) to evaluate each method. And the goal of the classification task is to predict whether a design is valid or not, i.e. whether the downstream RTL and physical synthesis are likely to complete or not. We use the classification accuracy as the evaluation metric.\\n\\nIn addition, recall that we have two versions of the dataset (SD X (V1) and VITIS (V2)). Each (version, task) combination receives a separate evaluation. For each evaluation, there are two evaluating settings: transductive and inductive testing. Specifically, for each (version, task) combination: (1) We select six kernels as the held-out testing kernels. These kernels are never seen during training and are used for the inductive testing; (2) For the rest of the kernels, we merge all the labeled designs, and randomly split them into training, validation, and transductive testing designs with the 70:15:15 ratio; (3) Using the training designs for 1000 epochs for the regression task, and 200 epochs for the classification task, we train each baseline method. We employ the validation set to determine the best epoch to use for testing; (4) We test the trained model on the transductive testing set. It is called transductive (\\\"Trans\\\") since this testing set contains designs from kernels that are seen during training; (5) We test the trained model on the held-out six kernels. Specifically, we (5.1) select the 30% designs from each held-out kernel as the testing designs, (5.2) then repeat the following procedure 5 times. For each kernel, from the rest of the 70% remaining designs, 10 designs are sampled and are utilized to adapt the trained model for 10 epochs. Then, the adapted model is tested on the 30% designs for that held-out kernel. We call such a setting inductive because the model is tested on six kernels that are not visible in the training stage in a zero-shot (\\\"Ind\\\") or few-shot learning setting (\\\"Ind Adapt\\\").\\n\\n5.3 Results and Analysis\\n\\nThe overall regression and classification results are exhibited in Tables 5 and 6. Tables 7, 8, 9, and 10 reveal the breakdown results over individual held-out kernels.\\n\\nObservation 1: There is no consistent winner among the baselines. For the regression task, G - CODEBERT and G - CODEBERT-L achieve the lowest error when adapted to the held-out kernels, whereas, for the classification task, GNN-based methods perform better. Such a phenomenon calls for a hybrid model utilizing both the source code and the assembly code graph, e.g., the concatenation models [CODET 5, GNN-GSE] and [G - CODEBERT, GNN-GSE]. However, a simple concatenation of the\"}"}
{"id": "HvcLKgtbco", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: Classification accuracy on SDX (V\\\\textsubscript{1}) and V\\\\textsubscript{ITIS} (V\\\\textsubscript{2}).\\n\\n| Method        | SDX (V\\\\textsubscript{1}) | V\\\\textsubscript{ITIS} (V\\\\textsubscript{2}) |\\n|---------------|---------------------------|------------------------------------------|\\n| Trans Ind Ind Adapt | 0.7576 0.5662 0.6617 0.7060 | 0.5444 0.6337 |\\n| CODET 5-RAND   | 0.8524 0.5257 0.7015 0.7924 0.4851 0.6291 |\\n| CODET 5-FROZEN | 0.7515 0.6098 0.7486 0.7334 0.4161 0.6061 |\\n| CODET 5        | 0.9501 0.6394 0.7447 0.9045 0.4781 0.6734 |\\n| CODE\\\\textsubscript{B}    | 0.9536 0.6478 0.7610 0.9233 0.5415 0.7024 |\\n| CODE\\\\textsubscript{B} -L | 0.9204 0.5730 0.7701 0.8970 0.5342 0.7180 |\\n| GNN DSE        | 0.9422 0.6529 0.7623 0.9045 0.4781 0.6734 |\\n| GNN DSE -2L    | 0.8912 0.6085 0.7421 0.9126 0.5303 0.7632 |\\n| CODET 5, GNN GSE | 0.9434 0.6141 0.7385 0.9195 0.5053 0.7002 |\\n| G - CODE\\\\textsubscript{B}, GNN GSE | 0.9212 0.6174 0.7446 0.9126 0.5001 0.7160 |\\n\\nTable 7: Regression result breakdown on SDX (V\\\\textsubscript{1}) on individual test kernels.\\n\\n| Method        | DOITGEN - R | FDTD -2 | D | GEMM - N | JACOBI -2 | D | STENCIL -3 | D | TRMM - OPT |\\n|---------------|-------------|---------|---|----------|-----------|---|------------|---|-----------|\\n| CODEV\\\\textsubscript{C} | 2.5711\u00b10.08 3.9973\u00b10.19 5.6191\u00b10.08 2.5710\u00b10.11 2.9029\u00b10.04 2.8322\u00b10.30 |\\n| CODET 5-RAND   | 1.2123\u00b10.02 3.3038\u00b10.10 3.8933\u00b10.11 1.8584\u00b10.12 0.9961\u00b10.14 2.1133\u00b10.09 |\\n| CODET 5-FROZEN | 1.3197\u00b10.00 3.2819\u00b10.06 4.3073\u00b10.03 1.7756\u00b10.04 0.6807\u00b10.03 2.0248\u00b10.09 |\\n| CODET 5        | 0.9990\u00b10.27 2.9818\u00b10.05 3.6910\u00b10.41 1.3470\u00b10.12 0.3695\u00b10.06 1.5336\u00b10.14 |\\n| \\\\textsubscript{B}     | 0.8301\u00b10.17 2.5938\u00b10.15 3.3672\u00b10.45 1.1581\u00b10.06 0.4830\u00b10.16 0.8990\u00b10.19 |\\n| \\\\textsubscript{B} -L | 0.5376\u00b10.08 2.8015\u00b10.18 3.1310\u00b10.52 1.0214\u00b10.12 0.3636\u00b10.07 0.9384\u00b10.10 |\\n| GNN DSE        | 0.5278\u00b10.03 2.6449\u00b10.10 2.8907\u00b10.51 1.3855\u00b10.10 0.6918\u00b10.23 0.9673\u00b10.05 |\\n| GNN DSE -2L    | 0.9113\u00b10.16 2.6677\u00b10.11 2.7881\u00b10.28 1.5230\u00b10.07 0.6984\u00b10.09 1.1499\u00b10.05 |\\n| CODET 5, GNN GSE | 0.9637\u00b10.20 2.7998\u00b10.30 2.9908\u00b10.16 1.2991\u00b10.14 0.7373\u00b10.14 1.0056\u00b10.04 |\\n| G - CODE\\\\textsubscript{B}, GNN GSE | 1.0409\u00b10.21 2.7988\u00b10.17 3.2942\u00b10.23 1.0845\u00b10.12 0.3072\u00b10.06 0.7891\u00b10.24 |\\n\\nTable 8: Regression result breakdown on V\\\\textsubscript{ITIS} (V\\\\textsubscript{2}) on individual test kernels.\\n\\n| Method        | COVARIANCE | FDTD -2 | D | L GEMM | N GEMM | P - L SYMM TRMM | OPT |\\n|---------------|------------|---------|---|--------|--------|----------------|-----|\\n| CODE\\\\textsubscript{C} | 2.4977\u00b10.09 3.1259\u00b10.18 4.5526\u00b10.19 3.7389\u00b10.03 1.4209\u00b10.11 2.6379\u00b10.21 |\\n| CODET 5-RAND   | 1.6388\u00b10.13 2.4587\u00b10.22 2.8477\u00b10.15 3.0722\u00b10.04 0.4969\u00b10.03 1.2295\u00b10.09 |\\n| CODET 5-FROZEN | 1.2137\u00b10.02 2.4494\u00b10.22 2.6576\u00b10.10 3.1420\u00b10.02 0.3854\u00b10.02 1.2199\u00b10.02 |\\n| CODET 5        | 1.3594\u00b10.15 2.2958\u00b10.07 2.8436\u00b10.37 2.7803\u00b10.07 0.4092\u00b10.09 0.8427\u00b10.14 |\\n| \\\\textsubscript{B}     | 1.0652\u00b10.06 1.9245\u00b10.09 2.3772\u00b10.31 2.3913\u00b10.04 0.4019\u00b10.08 0.9452\u00b10.09 |\\n| \\\\textsubscript{B} -L | 0.9107\u00b10.08 1.7740\u00b10.11 2.6635\u00b10.33 2.5029\u00b10.09 0.3384\u00b10.03 1.1469\u00b10.11 |\\n| GNN DSE        | 1.1496\u00b10.04 1.8897\u00b10.04 2.3157\u00b10.23 2.7828\u00b10.18 0.4110\u00b10.02 0.7790\u00b10.06 |\\n| GNN DSE -2L    | 1.1457\u00b10.02 1.9521\u00b10.09 2.0248\u00b10.26 2.9691\u00b10.10 0.4067\u00b10.04 0.9146\u00b10.03 |\\n| CODET 5, GNN GSE | 1.0988\u00b10.07 1.8532\u00b10.10 2.2430\u00b10.23 2.5489\u00b10.04 0.3766\u00b10.06 1.0440\u00b10.09 |\\n| G - CODE\\\\textsubscript{B}, GNN GSE | 1.1182\u00b10.07 1.9088\u00b10.10 2.5352\u00b10.25 2.7655\u00b10.08 0.3842\u00b10.05 0.8986\u00b10.13 |\\n\\nTable 9: Classification result breakdown on SDX (V\\\\textsubscript{1}) on individual test kernels.\\n\\n| Method        | DOITGEN - R | FDTD -2 | D | GEMM | JACOBI -2 | D | STENCIL -3 | D | TRMM - OPT |\\n|---------------|-------------|---------|---|------|-----------|---|------------|---|-----------|\\n| CODE\\\\textsubscript{C} | 0.6449\u00b10.07 0.6192\u00b10.01 0.4598\u00b10.03 0.8122\u00b10.05 0.6085\u00b10.03 0.8256\u00b10.04 |\\n| CODET 5-RAND   | 0.6663\u00b10.05 0.5283\u00b10.08 0.6366\u00b10.08 0.8384\u00b10.03 0.5887\u00b10.11 0.9509\u00b10.00 |\\n| CODET 5-FROZEN | 0.7472\u00b10.00 0.6414\u00b10.00 0.6286\u00b10.17 0.8889\u00b10.00 0.6338\u00b10.00 0.9516\u00b10.00 |\\n| CODET 5        | 0.7236\u00b10.04 0.6455\u00b10.02 0.5804\u00b10.04 0.8681\u00b10.01 0.7042\u00b10.05 0.9467\u00b10.01 |\\n| \\\\textsubscript{B}     | 0.6607\u00b10.06 0.6455\u00b10.03 0.6714\u00b10.03 0.8509\u00b10.02 0.8000\u00b10.04 0.9377\u00b10.02 |\\n| \\\\textsubscript{B} -L | 0.7236\u00b10.08 0.6556\u00b10.03 0.5741\u00b10.07 0.8792\u00b10.01 0.8394\u00b10.05 0.9488\u00b10.01 |\\n| GNN DSE        | 0.6506\u00b10.06 0.6889\u00b10.02 0.6402\u00b10.03 0.8652\u00b10.03 0.7803\u00b10.03 0.9488\u00b10.00 |\\n| GNN DSE -2L    | 0.6674\u00b10.05 0.6778\u00b10.03 0.6545\u00b10.02 0.8534\u00b10.03 0.6648\u00b10.04 0.9349\u00b10.03 |\\n| CODET 5, GNN GSE | 0.7022\u00b10.07 0.6364\u00b10.02 0.6438\u00b10.03 0.8419\u00b10.03 0.6620\u00b10.04 0.9446\u00b10.01 |\\n| G - CODE\\\\textsubscript{B}, GNN GSE | 0.7124\u00b10.06 0.7091\u00b10.04 0.6393\u00b10.04 0.8616\u00b10.04 0.6028\u00b10.09 0.9426\u00b10.02 |\\n\\nTable 10: Classification result breakdown on V\\\\textsubscript{ITIS} (V\\\\textsubscript{2}) on individual test kernels.\"}"}
{"id": "HvcLKgtbco", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards a Comprehensive Benchmark for High-Level Synthesis Targeted to FPGAs\\n\\nYunsheng Bai, Atefeh Sohrabizadeh, Zongyue Qin, Ziniu Hu, Yizhou Sun, Jason Cong\\n\\nDepartment of Computer Science\\nUniversity of California, Los Angeles\\n\\n{yba,atefehsz,qinzongyue,bull,yzsun,cong}@cs.ucla.edu\\n\\nAbstract\\n\\nHigh-level synthesis (HLS) aims to raise the abstraction layer in hardware design, enabling the design of domain-specific accelerators (DSAs) targeted for field-programmable gate arrays (FPGAs) using C/C++ instead of hardware description languages (HDLs). Compiler directives in the form of pragmas play a crucial role in modifying the microarchitecture within the HLS framework. However, the number of possible microarchitectures grows exponentially with the number of pragmas. Moreover, the evaluation of each candidate design using the HLS tool consumes significant time, ranging from minutes to hours, leading to a slow optimization process. To accelerate this process, machine learning models have been used to predict design quality in milliseconds. However, existing open-source datasets for training such models are limited in terms of design complexity and available optimizations. In this paper, we present HLSYN, a new benchmark that addresses these limitations. It contains more complex programs with a wider range of optimization pragmas, making it a comprehensive dataset for training and evaluating design quality prediction models. The HLSYN benchmark consists of 42 unique programs/kernels, each of which has many different pragma configurations, resulting in over 42,000 labeled designs. We conduct an extensive comparison of state-of-the-art baselines to assess their effectiveness in predicting design quality. As an ongoing project, we anticipate expanding the HLSYN benchmark in terms of both quantity and variety of programs to further support the development of this field.\\n\\n1 Introduction\\n\\nIn recent decades, the demand for specialized computing systems tailored to specific applications has significantly increased. This has led to the emergence of domain-specific accelerators (DSAs) being implemented in either application-specific integrated circuits (ASICs) or field-programmable gate arrays (FPGAs). By leveraging the unique characteristics of specific workloads, the designer can design DSAs to enhance performance and energy efficiency. This becomes particularly valuable when general-purpose processors like CPUs and GPUs cannot meet the performance or efficiency requirements of certain applications due to the end of Dennard scaling [8, 14]. For instance, Google has developed its custom-designed DSA in the form of an ASIC named the Tensor Processing Unit (TPU) [21], which is highly optimized for machine learning workloads, offering remarkably faster performance and improved energy efficiency compared to CPUs and GPUs. In addition, FPGAs offer a cost-effective alternative with reconfigurability, making them increasingly appealing for accelerating applications across various domains, including search engines and numerous datacenter applications [25, 9, 19], machine learning inference acceleration [15, 17, 32, 1, 27], and autonomous vehicles [7], among others.\"}"}
{"id": "HvcLKgtbco", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FPGA Implementation for the Source Code\\n\\nHLS followed by RTL and Physical Synthesis\\n\\nPragmas\\n\\nSource Code with Pragmas\\n\\nFigure 1: High-level synthesis (HLS) transforms the source code of the kernel written in C into a lower-level programming language and eventually implements the design on the target FPGA as shown with the chip die photo.\\n\\nNevertheless, the design of DSAs poses distinct difficulties in contrast to general-purpose hardware like CPUs and GPUs [34, 8]. DSAs are commonly developed using hardware description languages (HDLs) at the register-transfer level (RTL), specifically Verilog and VHDL, which are primarily known to circuit designers. To address this challenge, high-level synthesis (HLS) [11, 10] was introduced and is now supported by most EDA (Electronic Design Automation) and FPGA companies [4, 6, 20, 24, 29]. HLS raises the level of design abstraction to C/C++/OpenCL/SystemC, enabling designers to describe the high-level behavioral representation of their designs rather than the low-level data transition in RTL. This abstraction eliminates the need for explicit clock scheduling specifications in the HLS code. Instead, HLS tools analyze the behavior description to schedule operations across different clock cycles, assign operations to available resources, and establish the required control structure. Finally, the HLS tool automatically generates RTL code based on these analyses. It can take several minutes to hours for the HLS tool to generate this RTL code [34]. The RTL can then be passed through logic synthesis and physical design steps, which can consume several hours, to be implemented on the target FPGA. This HLS tool enhances design productivity, shortens design cycles, and allows designers to rapidly explore various design options without the need for manual RTL code writing.\\n\\nDespite the increased level of design abstraction offered by high-level synthesis (HLS) tools, they still require a considerable amount of hardware design expertise to utilize synthesis directives in the form of pragmas. These pragmas play a crucial role in specifying various aspects of the design, such as memory organization, caching strategies, memory buffer partitioning, parallelization and pipelining of computations, etc. As demonstrated by Chi et al. [8], although the performance of a DSA with no performance-optimizing pragmas can be $108 \\\\times$ slower than a CPU, through proper optimization, it can achieve a remarkable performance improvement, surpassing a CPU by $89 \\\\times$. However, the optimization process for architecture-specific enhancements is typically limited to hardware programmers and falls beyond the capabilities of the average software programmers. Consequently, there has been a growing focus on automating this optimization process. While some approaches treat the HLS tool as a black box and develop custom-designed heuristics to search through design candidates, a more recent research paradigm leverages machine learning and deep learning techniques. These approaches either learn the behavior of the HLS tool and construct predictive models or employ data-driven exploration methods to search through the solution space. The goal of automating the optimization process is to democratize customized computing and make it more accessible for the average software programmers, allowing them to utilize the tailored hardware acceleration.\\n\\nTo address the need for automating pragma insertion and parameter tuning in high-level synthesis (HLS), machine learning techniques can be used. This approach aims to achieve optimal quality in terms of latency and resource utilization. However, the lack of open-source datasets in this domain and the limitations of existing datasets, which are constrained in terms of design complexity and available optimizations, restrict their practicality. To bridge this gap, this paper introduces HLSYN, the first...\"}"}
{"id": "HvcLKgtbco", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"comprehensive benchmark for HLS designs targeted to FPGAs for performance optimization. This benchmark provides more complex programs and a wider range of optimization pragmas, facilitating advanced research and facilitating in-depth exploration of machine learning techniques in the context of HLS. In our study, we define a design as a piece of C/C++ source code (referred to as a kernel) with associated pragmas. Our primary focus is on predicting the quality of a design using supervised learning by running a model trained on a collection of labeled designs with corresponding quality metrics. However, our dataset can be utilized in various training scenarios, including training agents to efficiently explore the solution space.\\n\\n2 Background\\n\\nDesign 1\\nDesign 2\\n\\nFigure 2: Two example designs selected from the 2\\n\\nDesign 1\\nDesign 2\\n\\nFigure 1, the kernel contains 14 pragmas, and each row has the format \u201c<pragma name>: <pragma setting>\u201d. A small change in one of the pragma parameters leads to changes in the FPGA HLS results, i.e., the five prediction targets for the regression task.\\n\\nThe task of HLS YN is to predict the quality of the HLS design specified by a program (kernel) with a specific optimization pragma design. Our target implementation platform is FPGA, although similar techniques can be applied to ASIC accelerator designs as well. The quality of a design is defined as a function of its performance, which is measured by its latency in cycle counts, and its area/resource utilization, such as the usage of digital signal processing blocks (DSP), blocks' RAMs (BRAM), flip-flops (FF), and lookup-tables (LUT), which are the fundamental building blocks for implementing digital logic circuits in FPGA designs.\\n\\nIn this work, we specifically consider the optimization pragmas of the Merlin Compiler, an open-source source-to-source optimization tool used for efficient AMD/Xilinx HLS designs. The Merlin Compiler provides three types of optimization pragmas, namely PIPELINE, PARALLEL, and TILE to define the desired microarchitecture [34]. As illustrated in Figure 1, these pragmas can be applied at the loop level and offer control over the type of pipelining, the parallelization factor, and the amount of data caching. If setting the pragmas properly to non-default parameters for proper parallelizing and pipelining the computation, the resulting accelerator can be $10 \\\\times$ or even $100 \\\\times$ faster than a single-core CPU. However, without any pragma insertion, the resulting hardware can be $10 \\\\times$ slower than a CPU. Figure 2 demonstrates an example where the prediction targets are sensitive to the pragma settings. It is noteworthy that in some other examples, a change in the pragma settings does not lead to any change in the output targets. The machine learning model must learn from existing labeled designs and understand the source code as well as the pragmas in order to accurately forecast the outcome of each design when eventually being executed on an FPGA.\\n\\nTable 1 summarizes the parameter space of these pragmas. For a given program/kernel, any change in the option of any of the pragmas results in a different design with a unique microarchitecture. The \u201cfg\u201d option in pipelining refers to the case where all the inner loops are unrolled (parallelized with separate logic) and each parallel unit is pipelined. The \u201ccg\u201d option, on the other hand, results in coarse-grained processing elements (PEs) that are pipelined together. For example, it can create pipelined load-compute-store units. The PARALLEL and TILE pragma take numeric values that determine the degree of parallelization and loop tiling, respectively.\"}"}
{"id": "HvcLKgtbco", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Target pragmas with their options.\\n\\n|Pragma Name| Parameter Name| Parameter Space| Examples of Pragma Settings|\\n|-----------|---------------|----------------|-----------------------------|\\n|PARALLEL   | factor        | integer        | 4, 8                        |\\n|PIPELINE   | mode          | \u201ccg\u201d, \u201cfg\u201d, off| \u2018flatten\u2019 resulting in the \u201cfg\u201d mode |\\n|TILE       | factor        | integer        | 2, 4                        |\\n\\n3 Related Work\\n\\nIn previous research, optimizing HLS designs has been approached in different ways. One category of methods treats the HLS tool as a black box and utilizes problem-independent heuristics or develops dedicated heuristics to explore the solution space and evaluate the quality of results (QoR) directly using the tool [34, 43, 35]. However, this approach is time-consuming as each evaluation takes several minutes to hours. To mitigate this issue, another category of methods aims to create surrogate models for the HLS tool. Some of these methods construct dependency graphs of the program and employ traditional graph analysis techniques to schedule operations and estimate the QoR accordingly [46, 38], while others develop analytical performance and area models to estimate the QoR [45, 47]. Nevertheless, due to the different heuristics employed by HLS tools in the design process, these models may not accurately predict the QoR [34]. Some methods address this limitation by focusing on designs that can exploit pre-defined microarchitecture templates or follow specific computation patterns [33, 37, 12], but this can limit their generality. Alternatively, a data-driven approach utilizing machine learning and deep learning models has been proposed to enhance prediction accuracy [23, 22]. Graph neural networks (GNNs) have gained attention in this context and demonstrated promising results in enhancing prediction accuracy [30, 5, 40, 36].\\n\\nA fundamental aspect of these approaches is the availability of a large-scale database to effectively train the models. Recent works have focused on gathering such databases [16, 41, 30]. Unfortunately, existing datasets have limitations. The dataset in [41] predominantly consists of synthetic programs that do not utilize any pragmas. DB4HLS [16] targets programs from the MachSuite benchmark [26] but overlooks the inclusion of a key optimization pragma, pipelining. Additionally, DB4HLS views each function in the program as an individual kernel. GNN-DSE [30] targets programs from the Polyhedral benchmark [44], which features more complex kernels for FPGA mapping, in addition to the MachSuite benchmark. This dataset considers a program with all its sub-functions as a kernel, further increasing design complexity. Despite covering a wide range of optimization pragmas for each kernel, the generated dataset is small, with only 9 target kernels and a total of 4,752 data points. To address these limitations, we propose HLSYN, which includes kernels from both the Polyhedral and MachSuite benchmarks. It encompasses a diverse range of optimization pragmas that can pipeline and/or parallelize computation, as well as adjust data caching. Our benchmark is comprised of 42 unique kernels from various domains summarized in Table 2, totaling over 42,000 design points, providing a comprehensive resource for advancing research and facilitating an in-depth exploration of machine learning techniques for HLS.\\n\\n4 The HLSYN Benchmark\\n\\nIn this section, we introduce the datasets in HLSYN. The input data source comes from 42 selected kernels in the MachSuite benchmark [26] and the PolyBench benchmark suite [44]. Our selected 42 kernels cover a wide range of applications whose descriptions are shown in Table 2. The benchmark consists of 2 datasets accumulated in the past three years, corresponding to two versions of AMD/Xilinx HLS tools: (1) SDX (V1) [3] and VITIS (V2) [4], with the AMD/Xilinx Alveo U200 as the target FPGA and a working frequency of 250MHz. For each dataset, we select 6 kernels as the held-out testing kernels. They will test the ability of a model to generalize to kernels that it did not see during the training. For the rest of the kernels, we perform a random split with the training, validation, and testing ratio being 70:15:15. Summary statistics of datasets are given in Table 3.\\n\\nFor each dataset kernel in each dataset, we run the two versions of the tools described above to obtain a set of labeled designs. Since the design space size is exponential to the number of pragmas, we rely...\"}"}
{"id": "HvcLKgtbco", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"design embeddings does not consistently yield better performance. Particularly, CODET5, GNN-GSE and G-CODEBERT, GNN-GSE achieve the lowest regression error under the transductive setting but fall short when adapted to new kernels. Such results imply that future efforts can be made on studying the generalization abilities of machine learning models on our HLS benchmark.\\n\\nObservation 2: In general, pre-training helps with the performance of source code transformer models. This can be seen by comparing CODET5-RAND and CODET5, where the former starts training from scratch while the latter loads a pre-trained model released by [39]. This should not come as a surprise, because pre-training has been shown to demonstrate success in natural language processing. Our experimental results verify the effectiveness of pre-training on the HLS benchmark. One implication is that one can design better pre-training methods on source code related to electronic design automation, or even design pre-training for GNNs operating on assembly-level graphs.\\n\\nObservation 3: More GNN message passing layers does not necessarily improve the performance of GNN models. In many cases, the 2-layer version, GNN-DSE-2L, performs even better than the 8-layer version, GNN-DSE. This may be attributed to the fact that an attention-based global readout function is used to aggregate node embeddings into a graph-level embedding representing the entire design, and thus each node does not necessarily need to reach far-away nodes in the local message passing stage.\\n\\nObservation 4: Generalization to new kernels is difficult, and the performance after adaptation differs across kernels. For example, G-CODEBERT-L achieves the overall lowest error (\u201cInd Adapt\u201d) on SDX(V1) on the regression task, but Table 7 demonstrates that G-CODEBERT-L does not always yield the lowest error on each of the six held-out kernels. For example, G-CODEBERT, GNN-GSE performs the best on STENCIL-3D and TRMM-OPT, but poorly on DOITGEN-R, and thus the average regression error over the six held-out kernels is higher than G-CODEBERT-L. This calls for a more in-depth study of the discrepancy between kernels and a model that is capable of generalizing to a diverse set of kernels. In general, adaptation is necessary, since across all the experiments, without any adaptation (\\\"Ind\\\"), directly applying the trained model to new unseen kernels leads to poor regression error and classification accuracy.\\n\\n6 Conclusion and Future Work\\n\\nThis work introduces the task of design quality prediction in the forms of regression and classification tasks and presents the HLS benchmark to evaluate state-of-the-art program representation learning methods. Although there is no method that consistently outperforms all the other methods, we notice several trends and identify promising directions toward a more accurate prediction model design. As program representation learning is a continuously growing research domain, we plan to maintain the benchmark to test new methods. For example, our recent work [31] uses hierarchical graphs for program representations to predict design performance. Our HLS benchmark is a growing project. We expect to include more kernels and labeled designs running newer versions of HLS tools and establish a leaderboard to encourage participation. In addition, the current benchmark does not consider the design space exploration (DSE) stage, which will be added as the project develops. In fact, the regression task aims to be eventually integrated into the DSE process, which traverses the design space in order to find the optimal pragma setting.\\n\\n7 Acknowledgement\\n\\nThis work was partially supported by NSF 2211557, NSF 1937599, NSF 2119643, NSF 2303037, NASA, SRC JUMP 2.0 Center, Okawa Foundation, Amazon Research, Cisco, Picsart, Snapchat, and CDSC industrial partners (https://cdsc.ucla.edu/partners/). We would also like to thank AMD/Xilinx for HACC equipment donation and Marci Baun for editing the paper.\\n\\nReferences\\n\\n[1] Sergi Abadal, Akshay Jain, Robert Guirado, Jorge L\u00f3pez-Alonso, and Eduard Alarc\u00f3n. Computing graph neural networks: A survey from algorithms to accelerators. ACM Computing Surveys (CSUR), https://github.com/UCLA-DM/HLSyn#leaderboard\"}"}
{"id": "HvcLKgtbco", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[2] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. code2vec: Learning distributed representations of code. Proceedings of the ACM on Programming Languages, 3(POPL):1\u201329, 2019.\\n\\n[3] AMD/Xilinx Vivado HLS. https://docs.xilinx.com/v/u/2018.3-English/ug902-vivado-high-level-synthesis.\\n\\n[4] AMD/Xilinx Vivado HLS. https://docs.xilinx.com/v/u/2020.2-English/ug1416-vitis-documentation.\\n\\n[5] Yunsheng Bai, Atefeh Sohrabizadeh, Yizhou Sun, and Jason Cong. Improving GNN-based accelerator design automation with meta learning. In Proceedings of the 59th ACM/IEEE Design Automation Conference, pages 1347\u20131350, 2022.\\n\\n[6] Cadence Stratus High-Level Synthesis. https://www.cadence.com/en_US/home/tools/digital-design-and-signoff/synthesis/stratus-high-level-synthesis.html.\\n\\n[7] David Castells-Rufas, Vinh Ngo, Juan Borrego-Carazo, Marc Codina, Carles Sanchez, Debora Gil, and Jordi Carrabina. A survey of FPGA-based vision systems for autonomous cars. IEEE Access, 10:132525\u2013132563, 2022.\\n\\n[8] Yuze Chi, Weikang Qiao, Atefeh Sohrabizadeh, Jie Wang, and Jason Cong. Democratizing domain-specific computing. Communications of the ACM, 66(1):74\u201385, 2022.\\n\\n[9] Eric Chung, Jeremy Fowers, Kalin Ovtcharov, Michael Papamichael, Adrian Caulfield, Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, Michael Haselman, et al. Serving DNNs in real time at datacenter scale with project brainwave. IEEE Micro, 38(2):8\u201320, 2018.\\n\\n[10] Jason Cong, Jason Lau, Gai Liu, Stephen Neuendorffer, Peichen Pan, Kees Vissers, and Zhiru Zhang. FPGA HLS today: successes, challenges, and opportunities. ACM Transactions on Reconfigurable Technology and Systems (TRETS), 15(4):1\u201342, 2022.\\n\\n[11] Jason Cong, Bin Liu, Stephen Neuendorffer, Juanjo Noguera, Kees Vissers, and Zhiru Zhang. High-level synthesis for FPGAs: From prototyping to deployment. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 30(4):473\u2013491, 2011.\\n\\n[12] Jason Cong, Peng Wei, Cody Hao Yu, and Peng Zhang. Automated accelerator generation and optimization with composable, parallel and pipeline architecture. In DAC, 2018.\\n\\n[13] Chris Cummins, Zacharias V Fisches, Tal Ben-Nun, Torsten Hoefler, Michael FP O'Boyle, and Hugh Leather. Programl: A graph-based program representation for data flow analysis and compiler optimizations. In International Conference on Machine Learning, pages 2244\u20132253. PMLR, 2021.\\n\\n[14] William J Dally, Yatish Turakhia, and Song Han. Domain-specific hardware accelerators. Communications of the ACM, 63(7):48\u201357, 2020.\\n\\n[15] Javier Duarte, Philip Harris, Scott Hauck, Burt Holzman, Shih-Chieh Hsu, Sergo Jindariani, Suffian Khan, Benjamin Kreis, Brian Lee, Mia Liu, et al. FPGA-accelerated machine learning inference as a service for particle physics computing. Computing and Software for Big Science, 3:1\u201315, 2019.\\n\\n[16] Lorenzo Ferretti, Jihye Kwon, Giovanni Ansaloni, Giuseppe Di Guglielmo, Luca Carloni, and Laura Pozzi. DB4HLS: a database of high-level synthesis design space explorations. IEEE Embedded Systems Letters, 13(4):194\u2013197, 2021.\\n\\n[17] Jeremy Fowers, Kalin Ovtcharov, Michael Papamichael, Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, Michael Haselman, Logan Adams, Mahdi Ghandi, et al. A configurable cloud-scale DNN processor for real-time AI. In 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA), pages 1\u201314. IEEE, 2018.\\n\\n[18] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. Graphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366, 2020.\\n\\n[19] Muhuan Huang, Di Wu, Cody Hao Yu, Zhenman Fang, Matteo Interlandi, Tyson Condie, and Jason Cong. Programming and runtime support to blaze FPGA accelerator deployment at datacenter scale. In Proceedings of the Seventh ACM Symposium on Cloud Computing, pages 456\u2013469, 2016.\"}"}
{"id": "HvcLKgtbco", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[21] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual international symposium on computer architecture, pages 1\u201312, 2017.\\n\\n[22] David Koeplinger, Raghu Prabhakar, Yaqi Zhang, Christina Delimitrou, Christos Kozyrakis, and Kunle Olukotun. Automatic generation of efficient accelerators for reconfigurable hardware. In ISCA, pages 115\u2013127, 2016.\\n\\n[23] Hung-Yi Liu and Luca P Carloni. On learning-based methods for design-space exploration with high-level synthesis. In DAC, pages 1\u20137, 2013.\\n\\n[24] NEC CyberWorkBench. https://www.nec.com/en/global/prod/cwb/index.html.\\n\\n[25] Andrew Putnam, Adrian Caulfield, Eric Chung, Derek Chiou, Kypros Constantinides, John Demme, Hadi Esmaeilzadeh, Jeremy Fowers, Jan Gray, Michael Haselman, Scott Hauck, Stephen Heil, Amir Hormati, Joo-Young Kim, Sitaram Lanka, Eric Peterson, Aaron Smith, Jason Thong, Phillip Yi Xiao, Doug Burger, Jim Larus, Gopi Prashanth Gopal, and Simon Pope. A reconfigurable fabric for accelerating large-scale datacenter services. In Proceedings of the 41st Annual International Symposium on Computer Architecture (ISCA), pages 13\u201324. IEEE Press, June 2014. Selected as an IEEE Micro TopPick.\\n\\n[26] Brandon Reagen, Robert Adolf, Yakun Sophia Shao, Gu-Yeon Wei, and David Brooks. Machsuite: Benchmarks for accelerator design and customized architectures. In IISWC, 2014.\\n\\n[27] Ahmad Shawahna, Sadiq M Sait, and Aiman El-Maleh. FPGA-based accelerators of deep learning networks for learning and classification: A review. IEEE Access, 7:7823\u20137859, 2018.\\n\\n[28] Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjin Wang, and Yu Sun. Masked label prediction: Unified message passing model for semi-supervised classification. IJCAI, 2021.\\n\\n[29] Siemens Catapult High-Level Synthesis. https://eda.sw.siemens.com/en-US/ic/ic-design/high-level-synthesis-and-verification-platform/.\\n\\n[30] Atefeh Sohrabizadeh, Yunsheng Bai, Yizhou Sun, and Jason Cong. Automated accelerator optimization aided by graph neural networks. In Proceedings of the 59th ACM/IEEE Design Automation Conference, pages 55\u201360, 2022.\\n\\n[31] Atefeh Sohrabizadeh, Yunsheng Bai, Yizhou Sun, and Jason Cong. Robust GNN-based representation learning for HLS. In The International Conference on Computer-Aided Design, 2023.\\n\\n[32] Atefeh Sohrabizadeh, Yuze Chi, and Jason Cong. StreamGCN: Accelerating graph convolutional networks with streaming processing. In 2022 IEEE Custom Integrated Circuits Conference (CICC), pages 1\u20138. IEEE, 2022.\\n\\n[33] Atefeh Sohrabizadeh, Jie Wang, and Jason Cong. End-to-end optimization of deep learning applications. In Proceedings of the 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, pages 133\u2013139, 2020.\\n\\n[34] Atefeh Sohrabizadeh, Cody Hao Yu, Min Gao, and Jason Cong. AutoDSE: Enabling software programmers to design efficient FPGA accelerators. ACM Transactions on Design Automation of Electronic Systems (TODAES), 27(4):1\u201327, 2022.\\n\\n[35] Qi Sun, Tinghuan Chen, Siting Liu, Jin Miao, Jianli Chen, Hao Yu, and Bei Yu. Correlated multi-objective multi-fidelity optimization for HLS directives design. In IEEE/ACM Proceedings Design, Automation and Test in Europe (DATE), pages 01\u201305, 2021.\\n\\n[36] Ecenur Ustun, Chenhui Deng, Debjit Pal, Zhijing Li, and Zhiru Zhang. Accurate operation delay prediction for FPGA HLS using graph neural networks. In Proceedings of the 39th International Conference on Computer-Aided Design, pages 1\u20139, 2020.\\n\\n[37] Jie Wang, Licheng Guo, and Jason Cong. AutoSA: A polyhedral compiler for high-performance systolic arrays on FPGA. In Proceedings of the 2021 ACM/SIGDA international symposium on Field-programmable gate arrays, 2021.\\n\\n[38] Shuo Wang, Yun Liang, and Wei Zhang. Flexcl: An analytical performance model for OpenCL workloads on flexible FPGAs. In DAC, pages 1\u20136, 2017.\"}"}
{"id": "HvcLKgtbco", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. EMNLP, 2021.\\n\\nNan Wu, Yuan Xie, and Cong Hao. Ironman-pro: Multi-objective design space exploration in HLS via reinforcement learning and graph neural network based modeling. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2022.\\n\\nNan Wu, Hang Yang, Yuan Xie, Pan Li, and Cong Hao. High-level synthesis performance prediction using GNNs: Benchmarking, modeling, and advancing. In Proceedings of the 59th ACM/IEEE Design Automation Conference, pages 49\u201354, 2022.\\n\\nKeyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. ICML, 2018.\\n\\nCody Hao Yu, Peng Wei, Max Grossman, Peng Zhang, Vivek Sarker, and Jason Cong. S2FA: an accelerator automation framework for heterogeneous computing in datacenters. In DAC, pages 1\u20136, 2018.\\n\\nTomofumi Yuki and Louis-No\u00ebl Pouchet. Polybench/c.\\n\\nJieru Zhao, Liang Feng, Sharad Sinha, Wei Zhang, Yun Liang, and Bingsheng He. COMBA: A comprehensive model-based analysis framework for high level synthesis of real applications. In ICCAD, pages 430\u2013437, 2017.\\n\\nGuanwen Zhong, Alok Prakash, Yun Liang, Tulika Mitra, and Smail Niar. Lin-analyzer: a high-level performance analysis tool for FPGA-based accelerators. In DAC, pages 1\u20136, 2016.\\n\\nGuanwen Zhong, Vanchinathan Venkataramani, Yun Liang, Tulika Mitra, and Smail Niar. Design space exploration of multiple loops on FPGAs using high level synthesis. In ICCD, pages 456\u2013463, 2014.\"}"}
