{"id": "FhqzyGoTSH", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks\\n\\nTejas Srinivasan\\nTing-Yun Chang\\nLeticia Pinto Alva\\nGeorgios Chochlakis\\nMohammad Rostami\\nJesse Thomason\\n\\n1 University of Southern California\\n2 USC Information Sciences Institute\\nd{tejas.srinivasan,tingyun,pintoalv,chochlak,rostamim,jessetho}@usc.edu\\n\\nAbstract\\n\\nCurrent state-of-the-art vision-and-language models are evaluated on tasks either individually or in a multi-task setting, overlooking the challenges of continually learning (CL) tasks as they arrive. Existing CL benchmarks have facilitated research on task adaptation and mitigating \\\"catastrophic forgetting\\\", but are limited to vision-only and language-only tasks. We present CLiMB, a benchmark to study the challenge of learning multimodal tasks in a CL setting, and to systematically evaluate how upstream continual learning can rapidly generalize to new multimodal and unimodal tasks. CLiMB includes implementations of several CL algorithms and a modified Vision-Language Transformer (ViLT) model that can be deployed on both multimodal and unimodal tasks. We find that common CL methods can help mitigate forgetting during multimodal task learning, but do not enable cross-task knowledge transfer. We envision that CLiMB will facilitate research on a new class of CL algorithms for this challenging multimodal setting.\\n\\n1 Introduction\\n\\nLarge-scale pre-trained models, including crossmodal vision-and-language models, are generally fine-tuned on each downstream task individually, requiring fine-tuning and storing new models for each task. By contrast, multi-task learning requires fixing a set of tasks, but such training is incapable of dynamically learning new tasks. Although continual learning (CL) algorithms have explored cross-task knowledge transfer, existing methods primarily consider unimodal tasks in artificial settings [Jin et al., 2021, Lin et al., 2021]. Multimodal pre-training can encode useful and transferable features for diverse tasks, but learning from a sequence of different multimodal tasks and the subsequent forgetting effects [Kirkpatrick et al., 2017] have not yet been studied.\\n\\nAdditionally, it is assumed that these deployed models will encounter tasks containing all modalities seen during training time. This assumption means learning separate models for language-only, vision-only, and vision-language tasks, as opposed to a single \\\"generalist\\\" model that can handle all modalities or subsets of them [Reed et al., 2022]. Yet, existing work suggests that knowledge grounded in multiple modalities can benefit unimodal tasks [Desai and Johnson, 2021, Jin et al., 2022]. Currently, the research community lacks a suitable benchmark to systematically study how models can continually learn vision-and-language tasks while being transferable to unimodal tasks.\\n\\nIn this paper, we introduce the Continual Learning in Multimodality Benchmark (CLiMB), to facilitate the study of CL in vision-and-language tasks with deployment to multi- and unimodal tasks. We formulate a learning problem wherein a model is first trained on sequentially arriving vision-and-language tasks, referred to as upstream continual learning, and then transferred downstream to low-shot multimodal and unimodal tasks. CLiMB initially includes four vision-and-language\\n\\n1 The code for our benchmark is available at https://github.com/GLAMOR-USC/CLiMB\\n\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks.\"}"}
{"id": "FhqzyGoTSH", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: CLiMB evaluates candidate CL models and learning algorithms in two phases. For Phase I, **Upstream Continual Learning**, a pre-trained multimodal model is trained on a sequence of vision-and-language tasks, and evaluated after each task on its degree of Forgetting of past tasks and Knowledge Transfer to the next task. For Phase II, after each multimodal task the model is evaluated for its Downstream Low-Shot Transfer capability on both multimodal and unimodal tasks.\\n\\nExperiments using CLiMB find that existing CL algorithms can mitigate forgetting, but not transfer knowledge across tasks, revealing a need for new research into continual learning strategies for vision-language tasks. Further, current CL algorithms and multimodal models are not well suited for low-shot adaptation to multimodal or unimodal tasks. We hope CLiMB will provide the basis for developing models and learning algorithms for multimodal continual learning.\\n\\n2 Background and Related Work\\n\\nContinual, or lifelong, learning is an essential ability to develop autonomous agents that can learn in a cumulative way [Chen and Liu, 2018]. In CL, a model is trained on sequentially arriving tasks and evaluated both on its ability to learn future tasks as well as retain performance on past learned tasks [Kirkpatrick et al., 2017]. A necessity for developing CL algorithms is benchmarks that collate suitable sequential tasks. There are two primary approaches to create such CL benchmarks.\\n\\nThe first approach is to split existing tasks into non-overlapping sub-tasks that are sequentially presented for continual learning. For example, one can divide tasks along input categories [Greco et al., 2019] or output classes [Vinyals et al., 2016, Kirkpatrick et al., 2017] into disjoint sets. Mimicking real world distribution shift, timestamps can group data instances according to the order of their creation [Lin et al., 2021].\\n\\nCLiMB goes beyond such artificial, single-task-based CL and instead aggregates several diverse tasks. Similarly, unimodal benchmarks such as Visual Domain Decathlon [Rebuffi et al., 2017] and Natural Language Decathlon [McCann et al., 2018] aggregate 10 image classification and 10 language tasks, respectively. The CLIF-26 benchmark [Jin et al., 2021] is built for CL on the GLUE [Wang et al., 2019] language tasks. CLiMB goes beyond these unimodal benchmarks by evaluating on sequences of multimodal, vision-and-language tasks and testing downstream transfer to unimodal tasks.\"}"}
{"id": "FhqzyGoTSH", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: The initial tasks in CLiMB include various forms of vision and language inputs, and each task is framed as a classification problem. Multimodal vision-and-language tasks serve as upstream training for both multimodal and unimodal downstream, low-shot tasks.\\n\\n3 CLiMB: The Continual Learning in Multimodality Benchmark\\n\\nCLiMB tests the ability of models and learning algorithms to adapt to a sequentially arriving stream of vision-language tasks, as well as rapidly transfer to new multimodal and unimodal tasks (Table 1).\\n\\n3.1 CLiMB Learning and Evaluation\\n\\nLearning and evaluation in CLiMB proceeds in two phases: upstream continual learning and downstream low-shot transfer (Figure 1). Table 2 summarizes our CL evaluation metrics. We denote a task with modality $M \\\\in \\\\{V, L, VL\\\\}$ as $T_{iM}$ and the number of such tasks as $K_M$.\\n\\nUpstream Continual Learning of Multimodal Tasks\\n\\nA candidate model $M$ encounters a sequence of $K_{VL}$ vision-language tasks, $T_1^{VL} ... T_{i-1}^{VL} T_i^{VL}$. $M$ can be initialized with a pre-trained encoder. We allow parameter additions to the base model on a per-task basis. In this work we add task-specific classification layers for each new task on top of the base encoder model. The model $M$ is sequentially trained on the training split of each task $T_{iM}$ with candidate CL algorithm $A$. For task $T_i^{VL}$, the model is not presented with any inputs from $T_1^{VL} ... T_{i-1}^{VL}$. However, the CL algorithm $A$ may allocate memory to access previous training examples.\\n\\nWe evaluate two primary model properties in the upstream phase: upstream knowledge transfer from past learned tasks to new tasks, and withstanding forgetting of previously-seen tasks. The upstream knowledge transfer $T_{UK}(i)$ on task $T_i^{VL}$ is the relative gain in score from learning the previous tasks $T_1^{VL} ... T_{i-1}^{VL}$. Forgetting $T_{F}(j \\\\leftarrow i)$ of previously-seen task $T_j^{VL}$ is the relative performance degradation in that task after learning subsequent tasks $T_{j+1}^{VL} ... T_i^{VL}$ (Table 2).\\n\\nDownstream Transfer to Low-Shot Tasks\\n\\nWe evaluate the low-shot adaptation ability of the model $M$ after learning each upstream vision-language task. After training on the $i$th upstream task $T_i^{VL}$, we evaluate low-shot transfer to remaining multimodal tasks $T_{i+1}^{VL} ... T_{K_{VL}}^{VL}$, as well as unimodal tasks $T_1^{VL} ... T_{K_{VL}}^{V}$ and $T_1^{VL} ... T_{K_{VL}}^{L}$. Specifically, for every task in each modality, a low-shot instance of task $T_{iM}$, denoted as $T_{LS}(i)^{M}$, is created. The low-shot transfer ability to this task is evaluated by fine-tuning upstream encoder checkpoints on task $T_{LS}(i)^{M}$. We compute the low-shot transfer $T_{M_{LS}}(i)^{A}$ as the relative improvement of the CL encoder's performance on the low-shot task $T_{LS}(i)^{M}$, denoted as $S_{LS}(i)^{A}$, over the pre-trained encoder's performance on the same low-shot task, $S_{LS}(i)^{PT}$.\\n\\n2 https://visualqa.org/evaluation.html\"}"}
{"id": "FhqzyGoTSH", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Model and algorithm evaluation metrics in the upstream and downstream phases. For the $i$th task, we compute a model's $\\\\delta_i = S_i - S_i^{R}$, the model's task score $S_i$ minus the score $S_i^{R}$ of random selection. Our evaluation protocol computes each metric as a relative change in the $\\\\delta_i$ of a CL algorithm $A$ over a baseline setting $B$ to enable across-task comparisons. Each evaluation metric is calculated as $\\\\frac{\\\\delta_A - \\\\delta_B}{\\\\delta_B} = \\\\frac{S_A - S_B}{S_B - S_R}$, and is presented as a percentage.\\n\\n3.2 CLiMB Multimodal and Unimodal Tasks\\nCLiMB initially includes four multimodal upstream vision-language tasks, five language-only tasks, and four vision-only tasks (Table 1). We frame each as a classification task.\\n\\nVision-Language Tasks\\nCLiMB includes VQAv2 [Goyal et al., 2017], NLVR2 [Suhr et al., 2019], SNLI-VE [Xie et al., 2019] and VCR [Zellers et al., 2019a]. Solving these challenging tasks requires different kinds of knowledge in the multimodal model: question answering, visual and commonsense reasoning, entailment understanding.\\n\\nLanguage-Only Tasks\\nCLiMB includes IMDb [Maas et al., 2011], SST-2 [Socher et al., 2013], HellaSwag [Zellers et al., 2019b], CommonsenseQA [Talmor et al., 2019], and PIQA [Bisk et al., 2020]. We hypothesize that visually-grounded knowledge from upstream tasks may benefit tasks such as IMDb and SST-2, which are sourced from movie reviews, as well as HellaSwag, sourced from video captions, and PIQA, sourced from physically-grounded instructions from images and videos. Further, commonsense knowledge and reasoning skills obtained from VCR and NLVR2 may benefit tasks like HellaSwag, CommonsenseQA, and PIQA.\\n\\nVision-Only Tasks\\nCLiMB includes ImageNet-1000 [Russakovsky et al., 2015], iNaturalist2019 [Van Horn et al., 2018], Places365 [Mahajan et al., 2018], and MS-COCO object detection (formulated as a multi-label classification task). Since VQAv2 images are sourced from MS-COCO [Lin et al., 2014], we hypothesize VQAv2 upstream learning may aid in the COCO object detection task.\\n\\n4 Modeling and Experiments\\nUsing CLiMB, we study the performance of several commonly used CL algorithms on multimodal tasks. We use fixed upstream task order (VQAv2 \u2192 NLVR2 \u2192 SNLI-VE \u2192 VCR).\\n\\n4.1 Vision-Language Encoder: ViLT\\nWe use a pre-trained Vision-Language Transformer (ViLT) [Kim et al., 2021] as a backbone encoder. Unlike other pre-trained vision-language models [Lu et al., 2019, Chen et al., 2020] that build upon region-level features extracted from Faster R-CNN [Ren et al., 2015], ViLT directly operates on image patches without using any convolutional layers. In ViLT, text tokens and image patches are concatenated into an input sequence and passed through a Transformer, which learns the vision-language alignment with self-attention across both modalities (Figure 2).\"}"}
{"id": "FhqzyGoTSH", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The ViLT model [Kim et al., 2021] operates on vision and language inputs (left). We adapt these inputs for language-only tasks by providing the average MS-COCO image as in-domain visual input, and vision-only tasks by providing vacuous language input \u201cThis is an image\u201d (right).\\n\\n4.2 Upstream Experiments: Algorithms and Task Ordering\\nCLiMB includes several CL algorithm implementations. Sequential Fine-tuning (SeqFT) fine-tunes the full encoder and task-specific layers for each task in order. This baseline algorithm is an extension of the single-task fine-tuning paradigm to the CL setting. We also experiment with a Frozen Encoder baseline that trains only the task-specific layers. Fine-tuning all parameters may cause forgetting since the encoder parameters are overwritten, while fine-tuning only the task-specific layer prevents knowledge transfer since the shared encoder parameters are fixed. In Frozen Bottom-K, we freeze the bottom K layers of the encoder and fine-tune the rest, balancing these solutions (we set K = 9).\\n\\nCLiMB also includes two CL algorithms that fine-tune all parameters but are designed to mitigate forgetting. Experience Replay (ER) [Chaudhry et al., 2019] caches a small percentage of training examples in a memory buffer after training on each task, and periodically performs a \u201creplay\u201d training step using cached examples to refresh the model. Elastic Weight Consolidation (EWC) [Kirkpatrick et al., 2017] is a regularization method that adds an auxiliary L2 loss between weights in the current model and previous checkpoints to slow change in important encoder parameters.\\n\\nFinally, CLiMB includes Adapters [Houlsby et al., 2019], which add a small number of task-specific parameters, called Adapter modules, within layers of the pre-trained encoder. During training, the encoder\u2019s original parameters are kept frozen and only the Adapter modules are trained. We use a new set of Adapter modules each time we train on a new task, which leaves the previous tasks\u2019 modules untouched and prevents forgetting, but also does not facilitate cross-task knowledge transfer.\\n\\n4.3 Downstream Low-Shot Experiments\\nWe consider low-shot multimodal and unimodal tasks. We first define low-shot settings for different task types, then explain how we apply the multimodal ViLT model to unimodal settings.\\n\\nLow-Shot Task Settings\\nWe study \u201clow-shot\u201d training paradigms where only a fraction of full training data is available. For the multimodal classification tasks, NLVR2 and SNLI-VE, we use 2048 examples per class, whereas for the multiple choice VCR task, we use 5% of the training data. Among unimodal tasks, for vanilla classification tasks (IMDb, SST2, ImageNet-1000, iNaturalist, and Places365), we consider training with N = {16, 32} examples per class. For multiple choice classification tasks (PIQA, CommonsenseQA, HellaSwag), we use N = {1024, 4096} since these tasks are considerably more challenging. For the multi-label COCO object detection task, we consider M = {5%, 10%} of the original training data.\"}"}
{"id": "FhqzyGoTSH", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Upstream Knowledge Transfer\\n\\n\\nt\\nt\\n\\nUnimodal Tasks in ViLT\\n\\nTo apply ViLT to vision-only tasks, we use the phrase \\\"This is an image\\\" as the language input paired with the input image from the vision task. For language-only tasks, however, we need to address several challenges to effectively apply ViLT.\\n\\nFirst, we find that averaging all MS-COCO training images into a single, in-distribution image paired with text inputs produces better results with ViLT than not concatenating any image tokens at all to the Transformer input sequence.\\n\\nSecond, ViLT only allows a maximum of 40 language tokens in the input, which is enough for captions but insufficient for most language tasks. To deal with this challenge, we first downsample the vacuous image to reduce its token length from 144 to 16. Next, we extend the available language tokens by creating copies of pre-trained ViLT's language positional embeddings, $E \\\\in \\\\mathbb{R}^{40 \\\\times d}$, and concatenating these copies to get extended positional embeddings, $\\\\hat{E} \\\\in \\\\mathbb{R}^{L \\\\times d}$, where $L$ is the maximum sequence length of each task and $d$ is the embedding dimension.\\n\\nFinally, ViLT's language pre-training is on image captions that do not represent more general language use. We additionally experiment with a V AuLT [Chochlakis et al., 2022] model that extracts language representations from a pre-trained, frozen BERT [Devlin et al., 2019] that serve as input embeddings for ViLT. Please refer to the supplementary materials for more experiments and details.\\n\\n5 Results\\n\\nWe present Knowledge Transfer and Forgetting capabilities of different CL algorithms, experiments with multiple upstream task orders, and Low-Shot Transfer to downstream tasks.\\n\\n5.1 Upstream Learning Results\\n\\nWe find that common CL algorithms do not facilitate positive knowledge transfer in the vision-and-language setting of CLiMB, and in fact often hurt future task learning. Some are able to effectively mitigate forgetting, but none perform as well as directly fine-tuning on a candidate task. By examining the effects of task order, we conclude that the VCR task hurts further upstream task learning.\\n\\nUpstream Knowledge Transfer\\n\\nIn Table 3, we compare the upstream knowledge transfer exhibited by the different algorithms described in Section 4.2. Freezing the entire encoder severely underperforms the direct fine-tuning baseline for each task. Among other methods, all perform similarly to directly fine-tuning on the first task, with approximately zero knowledge transfer. However, for all methods other than Adapters, more continual learning hurts the model's ability to learn new tasks, as evidenced by the increasingly negative upstream transfer for later tasks. This effect may be due to loss of pre-training knowledge which is useful for task adaptation. This property of models to learn new tasks poorly in a continual learning setting is also called intransigence [Chaudhry et al., 2018]. Adapters, which do not train shared encoder parameters, do not exhibit this negative knowledge transfer, and show comparable performance to full model fine-tuning despite having very few learnable parameters.\"}"}
{"id": "FhqzyGoTSH", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Forgetting of Previous Tasks (%)\\n\\nTask 1 Task 2 Task 3 Task 4\\nUpstream Learning Task\\n20 15 10 5 0\\nUpstream Forward Transfer (%)\\nVQAv2 NLVR2 SNLI-VE VCR\\nTask 2 Task 3 Task 4\\nUpstream Learning Task\\n40 50 60 70 80 90\\nAverage Forgetting of Previous Tasks (%)\\n\\nFigure 3: (a) Forgetting $T_F(j \\\\leftarrow i)$ (%) of the previous $i - 1$ tasks for each algorithm. Each subplot denotes model performance on one of the previous tasks. While all algorithms that fine-tune shared parameters exhibit Forgetting, ER best preserves past task performance.\\n\\n(b) Effect of task order on upstream Knowledge Transfer (left) and Forgetting (right) for three different orders. Lines represent performance conditioned on a particular task order. After experiencing the VCR task, models exhibit lower Knowledge Transfer and higher Forgetting.\\n\\nMultimodal Low-Shot Transfer (%)\\n\\nVCR 20 10 0\\nSNLI-VE\\nTask 2: VQA Task 3: NLVR2 Task 4: SNLI-VE\\nUpstream Learning Task\\n20 10 0\\nNLVR2\\nSeqFT\\nFrozen-B9\\nER\\nEWC\\nMultimodal Low-Shot Transfer (%)\\n\\nFigure 4: Low-shot transfer, $T_{VL_{LS}}(j)$, for multimodal tasks $j = \\\\{i + 1, \\\\ldots, K_{VL}\\\\}$ after training on upstream task $T_i$. All CL algorithms exhibit negative Low-shot transfer on all multimodal tasks.\\n\\nForgetting Figure 3a shows how each algorithm affects forgetting of previous tasks. Sequential Fine-tuning forgets previous tasks to a large extent, Frozen Bottom-9 shows slight improvement, and freezing the encoder prevents forgetting entirely. Experience Replay does a better job at retaining task performance, while EWC shows only a slight improvement.\\n\\nAdapters enable a model to learn upstream tasks in the multimodal CL setting while not forgetting tasks already learned, adding only 3-4% parameters per task. Interestingly, forgetting is more severe after training models on the VCR task, demonstrating that the order of encountering tasks affects continual learning.\\n\\nEffect of Upstream Task Order Figure 3b shows the upstream knowledge transfer and forgetting for ViLT using Sequential Fine-tuning on three different task orders. While the upstream transfer is similar for the first two tasks in each task ordering, training on VCR negatively affects both knowledge transfer to future tasks and forgetting of past tasks. This effect may be due to a shift in the visual domain of VCR: input images have colored boxes drawn on them to represent grounded objects in the question, following previous work [Zellers et al., 2021, Hessel et al., 2022].\\n\\n5.2 Downstream Low-Shot Transfer Results In downstream transfer, we fine-tune the entire model irrespective of the upstream CL algorithm. We find that upstream learning with current CL algorithms does not help the ViLT encoder generalize to multimodal and unimodal tasks in low-shot settings.\\n\\nVision-Language Tasks Figure 4 presents the low-shot transfer $T_{VL_{LS}}(j)$ for all future tasks $j > i$ after training on upstream task $T_i$. We observe that low-shot transfer is always negative, implying that upstream continual learning always hurts the model's ability to learn new tasks in low-shot settings. Since upstream learning hurts model adaptation on new multimodal tasks with full training data (Table 3), it is expected that this effect will also be reflected in the low-shot regime.\"}"}
{"id": "FhqzyGoTSH", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Low-Shot Transfer (%) comparison between different CL algorithms on downstream vision-only tasks (left: ImageNet; right: COCO). Findings on iNaturalist 2019 and Places365 are similar to ImageNet (see Supp). Generally, current CL algorithms hurt low-shot transfer compared to direct fine-tuning, with Frozen Bottom-9 being the least harmful.\\n\\nVision-Only Tasks\\n\\nFigure 5 presents low-shot transfer on vision downstream tasks, using checkpoints from different upstream CL algorithms. Fine-tuning ViLT without CL performs well on vision-only tasks, achieving 65% top-1 accuracy on ImageNet-1000 with only 16 shots per class (see Supp). This performance suggests that ViLT already contains rich visual representations, making it sample efficient when transferred to vision-only tasks.\\n\\nSecond, CL actually hurts the transferability to downstream vision tasks. Among CL algorithms, Sequential Fine-tuning is the most harmful one, while freezing the bottom 9 layers causes the least degradation, almost matching direct fine tuning. This finding is consistent with previous work suggesting that bottom layers in deep models learn more general and transferable representations [Yosinski et al., 2014, Lee et al., 2019].\\n\\nNotably, upstream VQA and SNLI-VE checkpoints have a less negative effect on downstream COCO performance compared to NLVR2 and VCR. Because images from NLVR2 and VCR are more dissimilar to MS-COCO than the image sources of VQA and SNLI-VE, we hypothesize that large data distribution shifts between upstream and downstream tasks hurts transfer.\\n\\nLanguage-Only Tasks\\n\\nIn Figure 6, we compare the performance of two pre-trained encoders, ViLT and V AuLT, on low-shot language tasks, and the effects of upstream multimodal CL on low-shot transfer when applied to both encoders.\\n\\nWe observe that model adaptation to language tasks is challenging. The ViLT model frequently performs only marginally better than the random baseline, regardless of the upstream algorithm. Using V AuLT as the encoder achieves notably higher accuracy compared to ViLT on all tasks, indicating that strong language priors are key to low-shot language adaptation.\\n\\nAll upstream CL tasks improve V AuLT's transferability to SST-2 except for VCR. For both SST-2 and IMDb, there are significant drops after learning VCR in the upstream phase with ViLT and V AuLT, following vision-only results showing VCR is farther out of distribution than other upstream tasks. However, we do not observe similar trends on the three multiple-choice tasks, where CL generally hurts. We believe that current multimodal tasks do not learn complex language reasoning skills, hurting model transferability to language-only reasoning tasks.\\n\\n6 Conclusions\\n\\nWe propose the Continual Learning in Multimodality Benchmark (CLiMB) to study CL in multimodal tasks with deployment to multi- and unimodal tasks. Our experiments find that existing CL strategies do not generalize well to sequences of multimodal tasks or enable effective low-shot adaptation to downstream multi- or unimodal tasks. We hope CLiMB will allow systematic evaluation of new models and algorithms for multimodal continual learning. CLiMB is designed to be an extensible community tool for studying tasks, model architectures, and CL algorithms.\"}"}
{"id": "FhqzyGoTSH", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Comparisons between different encoders and continual learning algorithms on downstream language-only tasks (left: SST-2; right: CommonsenseQA). Note that the proposed Low-Shot Transfer metric is computed relative to the pre-trained encoder, making scores for different encoders (in this case, ViLT and V AuLT) incomparable. Hence, we plot the absolute accuracy with shaded standard deviation. V AuLT strictly improves absolute accuracy over ViLT in direct fine-tuning and CL settings. See Supp for comparable IMDb, HellaSwag, and PIQA results.\\n\\nLimitations\\n\\nTask-Specificity\\n\\nThe current CLiMB design allows for task-specific parameters and for model awareness of the task, but multi-task language modelling has seen impressive results from reframing all tasks as sequence-to-sequence problems that remove task-specific parameters [Raffel et al., 2020]. In future iterations of CLiMB, we intend to explore this task-agnostic paradigm, building further on the promising Adapters methods by learning a library of adapters that are dynamically selected based on input vision and language on a per-instance basis. Additionally, the task formulations in CLiMB are mostly classification, but sequence-based vision-and-language tasks could allow the study of embodied navigation [Anderson et al., 2018] and task completion [Shridhar et al., 2020], and may be feasible in a more task-agnostic CLiMB framework.\\n\\nAdditional CL Metrics\\n\\nWe have defined a set of metrics and methodologies for the challenge of multimodal continual learning, but these metrics are only an initial starting point. We design CLiMB to be flexible so that researchers can add metrics that they find valuable to measure, such as intransigence [Chaudhry et al., 2018].\\n\\nEthical Considerations\\n\\nThe initial CLiMB release is limited to English-only text, eliding the challenges of multi-lingual language tasks. Further, images in currently included datasets are sourced from social media, movie clips and web searches, thus excluding certain image domains, including those taken for accessibility needs such as descriptions for people with blindness [Gurari et al., 2018]. Such biases in a benchmark, inherited from the multi- and unimodal datasets selected, serve the needs of English-speaking, able-bodied folks as a \u201cdefault.\u201d\\n\\nFuture Work\\n\\nThe initial findings from CLiMB reveal several promising opportunities and lines of research. Adapters\\n\\nPrimarily, we find that Adapters are effective at mitigating catastrophic forgetting, while achieving comparable performance to full model fine-tuning. However, our current Adapter experiments introduce an independent set of parameters for each multimodal task, which does not facilitate sharing of task knowledge between tasks. Within unimodal multi-task and continual learning, Hypernetworks [Mahabadi et al., 2021] and compositional Adapter modules [Zhang et al., 2022] have been shown to facilitate knowledge transfer by generating Adapter parameters from shared task information. We plan to investigate how these methods generalize to multimodal CL, where shared information across tasks in either one or both modalities can influence generation of Adapter module parameters for new tasks.\"}"}
{"id": "FhqzyGoTSH", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Distribution Shifts with Multiple Modalities\\n\\nSecond, the stark performance degradation of the CL model after training on VCR, and the subsequent poor downstream few-shot transfer, invites study of how domain shifts in both vision and language inputs can affect upstream learning and forgetting, and can be mitigated.\\n\\nSequence-to-Sequence Tasks\\n\\nFinally, as we noted in our Limitations, currently CLiMB only supports classification tasks. However, recently several \u201cgeneralist\u201d models have been developed, such as UnifiedIO [Lu et al., 2022] and FLAVA [Singh et al., 2022], that can solve a large variety of multimodal and unimodal tasks by formulating all tasks as a Sequence-to-Sequence problem. We plan to extend CLiMB to support such all-purpose Sequence-to-Sequence models.\\n\\nAcknowledgments and Disclosure of Funding\\n\\nThis work was supported by the Laboratory for Analytic Sciences (LAS), National Security & Special Research Initiatives, and in part by DARPA under contract HR001121C0168.\\n\\nReferences\\n\\nPeter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. PIQA: Reasoning about physical commonsense in natural language. In AAAI Conference on Artificial Intelligence, 2020.\\n\\nArslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In European Conference on Computer Vision (ECCV), 2018.\\n\\nArslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc'Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv, 2019.\\n\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Learning universal image-text representations. In European Conference on Computer Vision (ECCV), 2020.\\n\\nZhiyuan Chen and Bing Liu. Lifelong machine learning. Synthesis Lectures on Artificial Intelligence and Machine Learning, 12(3):1\u2013207, 2018.\\n\\nGeorgios Chochlakis, Tejas Srinivasan, Jesse Thomason, and Shrikanth Narayanan. Vault: Augmenting the vision-and-language transformer with the propagation of deep language representations. arXiv preprint arXiv:2208.09021, 2022.\\n\\nKaran Desai and Justin Johnson. VirTex: Learning visual representations from textual annotations. In Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics (NAACL), 2019.\\n\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.\\n\\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Computer Vision and Pattern Recognition (CVPR), 2017.\"}"}
{"id": "FhqzyGoTSH", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Claudio Greco, Barbara Plank, Raquel Fern\u00e1ndez, and Raffaella Bernardi. Psycholinguistics meets continual learning: Measuring catastrophic forgetting in visual question answering. In Association for Computational Linguistics (ACL), 2019.\\n\\nDanna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\nJack Hessel, Jena D Hwang, Jae Sung Park, Rowan Zellers, Chandra Bhagavatula, Anna Rohrbach, Kate Saenko, and Yejin Choi. The abduction of sherlock holmes: A dataset for visual abductive reasoning. arXiv, 2022.\\n\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning (ICML), 2019.\\n\\nWoojeong Jin, Dong-Ho Lee, Chenguang Zhu, Jay Pujara, and Xiang Ren. Leveraging visual knowledge in language tasks: An empirical study on intermediate pre-training for cross-modal knowledge transfer. In Association for Computational Linguistics (ACL), 2022.\\n\\nXisen Jin, Bill Yuchen Lin, Mohammad Rostami, and Xiang Ren. Learn continually, generalize rapidly: Lifelong knowledge accumulation for few-shot learning. In Findings of Empirical Methods in Natural Language Processing (Findings of EMNLP), 2021.\\n\\nWonjae Kim, Bokyung Son, and Ildoo Kim. ViLT: Vision-and-language transformer without convolution or region supervision. International Conference on Machine Learning (ICML), 2021.\\n\\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521\u20133526, 2017.\\n\\nJaejun Lee, Raphael Tang, and Jimmy Lin. What would elsa do? freezing layers during transformer fine-tuning. arXiv, 2019.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In European Conference on Computer Vision (ECCV), 2014.\\n\\nZhiqiu Lin, Jia Shi, Deepak Pathak, and Deva Ramanan. The CLEAR benchmark: Continual learning on real-world imagery. In Neural Information Processing Systems (NeurIPS), 2021.\\n\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Neural Information Processing Systems (NeurIPS), 2019.\\n\\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-ai: A unified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916, 2022.\\n\\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Association for Computational Linguistics (ACL), 2011.\\n\\nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. In Association for Computational Linguistics (ACL), 2021.\\n\\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining. In European Conference on Computer Vision (ECCV), 2018.\"}"}
{"id": "FhqzyGoTSH", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv, 2018.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research (JMLR), 21(140):1\u201367, 2020.\\n\\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. In Neural Information Processing Systems (NeurIPS), 2017.\\n\\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv, 2022.\\n\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Neural Information Processing Systems (NeurIPS), 28:91\u201399, 2015.\\n\\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015.\\n\\nMohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED: A benchmark for interpreting grounded instructions for everyday tasks. In Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\nAmanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Empirical Methods in Natural Language Processing (EMNLP), 2013.\\n\\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic representations. In International Conference on Learning Representations, 2019.\\n\\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for reasoning about natural language grounded in photographs. In Association for Computational Linguistics (ACL), 2019.\\n\\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In North American Chapter of the Association for Computational Linguistics (NAACL), 2019.\\n\\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The iNaturalist species classification and detection dataset. In Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. Neural Information Processing Systems (NeurIPS), 2016.\\n\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. International Conference on Learning Representations (ICLR), 2019.\\n\\nNing Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual entailment: A novel task for fine-grained image understanding. arXiv, 2019.\\n\\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? Neural Information Processing Systems (NeurIPS), 2014.\"}"}
{"id": "FhqzyGoTSH", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\\n\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In Association for Computational Linguistics (ACL), 2019.\\n\\nRowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. In Neural Information Processing Systems (NeurIPS), 2021.\\n\\nYanzhe Zhang, Xuezhi Wang, and Diyi Yang. Continual sequence generation with adaptive compositional modules. In Association for Computational Linguistics (ACL), 2022.\"}"}
{"id": "FhqzyGoTSH", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Checklist\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n   (b) Did you describe the limitations of your work? [Yes] See Section 7\\n   (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section 7\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n   (b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Our code and instructions can be found at https://github.com/GLAMOR-USC/CLiMB (mentioned on Page 1).\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Supplementary material.\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] Yes, for language-only tasks (See Figure 6), where we run experiments with three different random seeds. For other tasks, we do a single run with fixed the random seed, due to training time cost.\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Supplementary material.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes]\\n   (b) Did you mention the license of the assets? [Yes] Our benchmark license is mentioned in the project's GitHub repository (linked above). Licenses of datasets included (but not distributed) are mentioned in the Supplementary material.\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [N/A]\\n   (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\"}"}
