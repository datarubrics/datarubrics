{"id": "6Hl7XoPNAVX", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ambiguous Images With Human Judgments for Robust Visual Event Classification\\n\\nKate Sanders Reno Kriz Anqi Liu Benjamin Van Durme\\nJohns Hopkins University\\nHuman Language Technology Center of Excellence\\n{ksande25, rkriz1, aliu74, vandurme}@jhu.edu\\n\\nAbstract\\nContemporary vision benchmarks predominantly consider tasks on which humans can achieve near-perfect performance. However, humans are frequently presented with visual data that they cannot classify with 100% certainty, and models trained on standard vision benchmarks achieve low performance when evaluated on this data. To address this issue, we introduce a procedure for creating datasets of ambiguous images and use it to produce SQUID-E (\u201cSquidy\u201d), a collection of noisy images extracted from videos. All images are annotated with ground truth values and a test set is annotated with human uncertainty judgments. We use this dataset to characterize human uncertainty in vision tasks and evaluate existing visual event classification models. Experimental results suggest that existing vision models are not sufficiently equipped to provide meaningful outputs for ambiguous images and that datasets of this nature can be used to assess and improve such models through model training and direct evaluation of model calibration. These findings motivate large-scale ambiguous dataset creation and further research focusing on noisy visual data.\\n\\n1 Introduction\\nWhen making decisions, the human brain uses perceptual uncertainty judgments to account for missing visual information and other noise [22, 2, 26]. For instance, when humans enter a new environment, they must quickly gauge what events are taking place in it using limited sensory input [53, 80, 79]. However, this practice is not reflected in most vision models. Robustness to out-of-domain or otherwise noisy data has been an area of focus within the computer vision community in recent years [24, 4] with various studies showing model limitations in this regard. However, little work has been done on classifying images that humans also struggle to classify accurately. This lack of emphasis on ambiguous data can cause poor model performance on noisy images that require human uncertainty quantification.\\n\\nDue to the temporal nature of events and their semantic complexity, visual event classification invites significant data-driven ambiguity. A common task in visual event classification is situation recognition, where a model must identify the verb and corresponding semantic roles (e.g. subject, object, place, reason, etc.) depicted in an image to characterize the event taking place [78]. In a typical framework, a situation recognition model first classifies the verb depicted in the image using an action recognition network and then, given that predicted verb, recurrently identifies semantic roles associated with it shown in the image [52]. This paradigm lends itself particularly poorly to ambiguous data, as the output of the model depends heavily on correctly identifying the verb depicted using only raw pixels as input. If even humans cannot identify the verb taking place with certainty,\\n\\n1 Dataset and code are available at https://katesanders9.github.io/ambiguous-images\\n\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks.\"}"}
{"id": "6Hl7XoPNAVX", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Images from SQUID-E and their corresponding human judgment scores. Each judgment score shows the mean human-elicited likelihood of the image depicting the event listed on the left in that row (wedding, birthday party, or medical procedure).\\n\\nthen it follows that these models will rarely output meaningful information even when they accurately recognize important event-centric attributes in the data.\\n\\nIgnoring ambiguity in data can lead to significant consequences in downstream applications. For example, autonomous agents that collaborate with humans in tasks like manufacturing must accurately assess this kind of ambiguous event-based data (such as visual input showing what its human partner is doing) to make behavioral decisions [76] and ensure user safety [6, 41]. This requires an ability to produce reliable outputs under perceptual data-driven uncertainty. In some scenarios, a model's calibration scores may additionally need to resemble judgments made by humans to imitate human behavior and allow for better model interpretation [67].\\n\\nIn this paper, we consider the question of how to construct a dataset of noisy images for uncertainty-aware situation recognition. We propose a method for collecting ambiguous images from internet videos and assigning them human judgment scores. This data collection process mitigates the reporting bias found in existing image datasets to model the distribution of visual input experienced by humans. Using this method, we present the first dataset of intentionally ambiguous event-centric images: SQUID-E, or the Scenes with Quantitative Uncertainty Information Dataset for Events. To our knowledge this is also the first event-centric image dataset that uses quantitative human uncertainty judgments as labels. We show in experiments that these images and labels can be used to train robust classification models, assess model accuracy on different distributions of ambiguous data, additionally directly assess model calibration techniques. Sample images and human judgments from SQUID-E are shown in Figure 1.\\n\\nIn summary, we make the following contributions:\\n\\n1. We introduce a novel method for generating visual uncertainty datasets consisting of noisy images scraped from videos and corresponding human uncertainty judgments.\\n\\n2. We use this process to construct SQUID-E which consists of 12,000 images with ambiguous contexts, corresponding context labels, and 10,800 human uncertainty judgments for a test set of 1,800 of these images.\\n\\n3. We demonstrate the applicability of ambiguous image datasets through experiments: We show that training on ambiguous datasets may result in up to a 9-point accuracy improvement on other ambiguous data, existing situation recognition models do not necessarily produce meaningful outputs for ambiguous data, and human uncertainty scores for noisy data can be used to evaluate model calibration approaches.\\n\\n2 Related Work\\n\\n2.1 Collecting Ambiguous Data in Computer Vision\\n\\nWhile uncertainty in machine learning has been widely studied through lines of work such as model calibration [1], the practice of using collections of human uncertainty judgments in such efforts is...\"}"}
{"id": "6Hl7XoPNAVX", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Previous work on collecting information beyond a single label are largely motivated by the issues of training and evaluating models using the standard \u201cclean\u201d dataset. For example, Beyer et al. [11] explore the issue of model overfitting on standard labeling paradigms such as that used to construct ImageNet [21] by using soft human-annotated label distributions, exploring how even \u201chigh-certainty\u201d data can elicit variance in human responses. Along this line of work, Peterson et al. [51] construct a dataset of CIFAR-10 [36] images labeled with distributions of human judgments. Other work [63, 19, 70] considers frameworks for learning from fuzzy human labels given possibly ambiguous data. Our dataset differs from these papers in that none of the listed datasets include images that are intentionally ambiguous, or depict more than a single object.\\n\\nFurthermore, the human labels used in previous datasets do not include individual human uncertainty judgments. For example, Misra et al. [42] explore the relationship between the explicit contents of an image and the corresponding semantic components that humans label, characterizing the reporting bias of humans, while we solicit quantitative uncertainty judgments pertaining to the image as a whole and its relationship to event classifications instead. Additionally, we provide possible explanations for how humans make these judgments. Another notable research effort is Chen et al.\u2019s work [15] which introduces a dataset of human entailment judgments for the Uncertain Natural Language Inference task in which a model must directly predict these human uncertainty scores. Their annotation process is similar to ours, but is used for text annotation as opposed to images.\\n\\n2.2 Assessing Human Uncertainty in Ambiguous Data\\n\\nWorks in cognitive science provide a natural motivation for the machine learning community to explore the usage of ambiguous data or uncertainty quantification methods when data are collected from humans. Classic works have introduced a variety of notable ideas including theoretical uncertainty taxonomies [12], the impact of implicit bias and context on judgments [66, 64, 46], and strategies humans use to produce judgments [68, 10]. Many papers consider how human uncertainty scores align with probability theory [55, 2, 71]. Ma et al. identify the performance gain achieved by organisms who incorporate uncertainty measures through visual processing, etc. into their decision-making [40, 23, 34, 22] and perceptual organization [81]. Our work differs from these papers in that we approach this concept from a machine learning perspective.\\n\\nWork concerned with ambiguity in data in machine learning frequently quantifies it by assessing the aleatoric, or data-driven uncertainty in systems [33]. In the vision community, aleatoric uncertainty is typically considered in the context of medical image processing, where model calibration is necessary to employ agents in high-stakes applications. A popular method of quantifying aleatoric uncertainty in medical imaging is data augmentation [5, 62], but authors such as Beluch et al. [9] and Reinhold et al. [56] use alternate techniques such as ensembling and dropout network layers. Nado et al. [47] produce a system for benchmarking such methods, but does not consider using human uncertainty scores to assess models. To our knowledge no work exists that considers ambiguity in semantically complex images, such as ones that depict events.\\n\\nVarious work has also considered aleatoric uncertainty estimation from a more theoretical perspective. For example, works have considered how aleatoric uncertainty can be measured by estimating the parameters of a Gaussian distribution by maximizing the log likelihood [65, 49, 37, 33]. Other work considers how outputs produced using this method can be further improved in the case of regression [45, 31, 44, 48] and categorical classification [45, 31, 44, 48]. However, these works usually only consider the aleatoric uncertainty estimation problem in the existing data, but do not directly consider datasets with human uncertainty judgement. In this paper, we consider different sources of uncertainty in the context of human uncertainty judgments. We also investigate how these judgments can be used to train and evaluate models in situation recognition applications.\\n\\n2.3 Situation Recognition and Verb Prediction\\n\\nInitially, event-centric image classification was primarily constrained to simple tasks like pose estimation. Early forays into more sophisticated event classification proposed organizing images through frameworks that draw from linguistic event semantics [28, 58]. This proposal was built on by Yatskar et al. [78] who introduced a FrameNet-based ontology for event classification. Many papers consider novel model architectures and extensions based on this work, some exploring multi-modal extensions [38, 73], and others implementing bounding box grounding [52]. Wei et al. [74] and\"}"}
{"id": "6Hl7XoPNAVX", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Cho et al. [16] introduce new models that depart from the typical two-stage classification pipeline to better model event attribute relationships. Cho et al. [17] incorporate transformers in the original architecture, Sadhu et al. [60] apply the framework to video understanding, and Dehkordi et al. [20] alternatively use a CNN ensembling method. In all of these approaches, it is assumed that the necessary elements to identify the event are clearly depicted in the image, and it is not explored how these models perform when presented with ambiguous data. This is what we explore in this paper.\\n\\nIt is typical for situation recognition models to first predict the verb depicted in a given image and then predict the various semantic roles associated with that verb [52, 17, 16]. Given that the semantic role classification depends on accurate verb classification, it is critical for systems to retrieve reliable information regarding the possible verbs depicted in an input. Therefore, in this paper we focus on the verb prediction modules of these systems. However, these verb prediction modules typically do not consider uncertainty, or image-driven ambiguity. In our work, we consider uncertainty quantification for ambiguous data and focus on techniques that account for aleatoric uncertainty.\\n\\n3 Dataset Construction\\n\\nIn this section we detail our ambiguous dataset construction approach which we used to develop SQUID-E. Our process consists of (1) scraping YouTube for videos of specific events, (2) extracting visually distinct images from these videos, (3) identifying careful annotators to provide human judgments, and (4) executing the full annotation task using images collected in steps (1) and (2) to produce a set of corresponding human uncertainty judgments.\\n\\n3.1 Image Collection\\n\\nConsidering Reporting Bias\\n\\nThe distribution of still images found in publications, internet image libraries, or other corpora is influenced by reporting bias [27]: It may not accurately reflect the distribution of visual data humans perceive in real life because images in these corpora are, typically, intentionally selected to maximize saliency. Therefore, most datasets contain images that are generally easy for humans to classify with high certainty. To produce a dataset of noisier, more ambiguous visual data, we extracted images from videos. While videos still suffer from this reporting bias, the video corpora curation processes typically consider the comprehensive contents of an entire video rather than the individual frames that comprise it, resulting in large collections of noisier images that can still be easily classified based on content.\\n\\nVideo Collection and Image Extraction\\n\\nWe considered 20 event types, covering topics such as various social activities, sports, and natural disasters. We intentionally selected event types that typically take place over relatively long time frames, allowing for a wide variety of images that can belong to these event types. To populate the dataset, we first scraped YouTube for videos that fall into one of these twenty event types using YouTube's search algorithm. Search queries involved the name of the event type and related keywords, and were made in multiple languages for each event type. In SQUID-E, we additionally included a selection of videos from the Extended UCF Crime dataset [50]. Retrieved videos were manually checked and removed if they were not relevant or did not contain sufficiently diverse visual content. This process resulted in a collection of 100 videos per event category. Six frames were extracted from each video using a combination of frame sampling and clustering to produce a collection of visually diverse images from each video. Further image collection details are included in Appendix A.1.\\n\\n3.2 Human Uncertainty Judgment Solicitation\\n\\nAnnotation Setup\\n\\nAnnotations were collected for a set of six event types using Amazon Mechanical Turk. Annotators were provided with an event prompt and six images from the dataset. They were then told to rate their confidence that each image belonged to a video depicting the prompted event type using sliding bars ranging from 0% to 100%. Annotators were provided with three example images of each event type and were shown guidelines explaining how to rate their uncertainty on a numerical scale. Notably, annotators were instructed to only rate an image 0% if the image contained a set of visual attributes that necessarily could not appear together alongside the target event type, and rate an image 100% if the image contained a set of attributes that, together, could only belong alongside the target event type. An image should be rated 50% if the annotator felt there was an...\"}"}
{"id": "6Hl7XoPNAVX", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Example images and corresponding ratings given a target event type. From left to right:\\n\\n1. The visual attributes in this image virtually never coincide with a wedding event.\\n2. The image contains attributes closely related to COVID tests, but does not contain attributes that would necessarily appear in a COVID test setting.\\n3. This image has many attributes that often appear in a wedding, but these attributes also could appear in a related event type.\\n4. Most of the attributes in this image are uniquely characteristic of a birthday party.\\n\\nSelecting Annotators\\n\\nA small pilot was first run to identify high-quality annotators. As discussed in the following section, high disagreement on individual image scores is an intrinsic aspect of the task. Therefore, a purely numerical metric such as mean squared error against a ground truth vector could not be used to identify high-quality annotators. Instead, for each set of images, a rubric identifying possible annotator \u201cpitfalls\u201d was established and used to identify lower-quality annotators who either did not read the instructions or did not apply the instructions correctly when evaluating images. Such pitfalls included heuristics such as rating completely black images above 5%, rating images with text clearly stating the event type below 90% or above 10% (depending on the target event and text), etc. This process produced a set of ten to twenty high-quality annotators per task.\\n\\nTask Variants\\n\\nTo identify whether the other five images on screen influence annotator uncertainty scores for an image, we ran two versions of the annotation task. These two variants present annotators with different sets of images at a time. In Variant A, each of the six images that appeared on screen belonged to the same event type, but were sampled from different videos. In Variant B, given a target event type, three frames on screen belonged to videos depicting that target event type, and the other three frames belonged to videos of the event type most semantically similar to the target event type (e.g., three frames from the \u201cbirthday\u201d event type and three frames from the \u201cwedding\u201d event type, or three frames from the \u201cparade\u201d event type and three frames from the \u201cprotest\u201d event type). The semantic similarity of events was calculated using FrameNet templates. No annotator participated in both task variants after the pilots were completed.\\n\\n4 Ambiguity of Data\\n\\n4.1 Inter-Annotator Agreement\\n\\nWe explicitly aim to quantify the data-driven noise in images through the dataset\u2019s numerical human judgments. However, the set of collected human judgments has its own inter-annotator variance. In SQUID-E, the Spearman correlation among annotators is 0.676 for Variant A, 0.631 for Variant B, and 0.673 across both variants, indicating that there was not a substantial overall difference in annotator behavior between the two versions of the task, although annotators were slightly more confident when annotating for variant B as shown in Figure 3. Alternative metrics for assessing annotation agreement are considered in Appendix C.\\n\\n4.2 Intra-Annotator Agreement\\n\\nWhile most of the analysis in this section focuses on inter-annotator variation, it is also important to consider how a person may annotate the same data differently across multiple samples, and how this intra-annotator variance may influence the annotations of the dataset. To assess this factor, the six annotators who labeled the most images in the original annotation task were given the task again for a random subset of the images they had already annotated. This study was conducted five months after the original data have been labeled.\"}"}
{"id": "6Hl7XoPNAVX", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Histograms illustrating the distribution of mean human uncertainty judgments and standard deviation scores among annotations for each image in task variants A and B. Annotators were slightly more confident in task variant B: For variant A, 16% of images were rated as \u201chigh certainty\u201d, or received a mean certainty score at or above 95%, whereas 19% of images were rated as high certainty when annotated in variant B.\\n\\nafter the original annotations had been collected. Five of the six annotators accepted the task (2 from variant A and 3 from variant B) and annotated 60 images each. The Spearman correlation between an annotators\u2019 original scores and their second set of scores was calculated, and the mean correlation was 0.788. This result suggests that some of the variance in the scores between different annotators is likely due to irreducible variance that would occur even if the two annotators were exactly the same. However, the intra-annotator spearman correlation is still significantly higher than the inter-annotator spearman correlation (0.788 vs. 0.673), indicating that there are additional factors that contribute to different humans rating images differently. We explore these factors below.\\n\\n4.3 Sources of Annotator Disagreement\\n\\nWhy do some images produce human judgments with high variance, while others elicit general agreement? Here, we compile possible explanations that illustrate more general human uncertainty quantification trends. Examples of these categories are provided in Figure 4.\\n\\nBased on an analysis of the human judgments collected for this dataset, the primary sources of inter-annotator variance likely include differences in the following:\\n\\n\u2022 **Visual attention.** A person\u2019s visual attention can affect their perceptual input and uncertainty calculations [14, 13, 57, 43, 42]. Some studies suggest that visual attention may even directly cause conservative bias in perception [54]. We hypothesize that this phenomenon affected the human judgments in our task, since the images in SQUID-E can often be classified as multiple event types depending on where an annotator\u2019s visual attention is focused.\\n\\n\u2022 **Background knowledge.** Many images require an annotator to hold specific knowledge to classify them accurately, and so people may annotate these images differently depending on their personal knowledge bases. Necessary background knowledge is often cultural, or otherwise related to current events or history. This source of disagreement highlights the importance of formulating tasks and datasets such that they include diverse data and are annotated by diverse annotators [39].\\n\\n\u2022 **Uncertainty quantification strategies.** Prior work explores various sources of human probability estimation error and noise, indicating that one notable factor is that the way humans calculate probability is inherently imperfect [68, 32, 25]. These studies detail heuristics and psychological biases that influence human judgments that are not necessarily caused by external factors such as input or contextual knowledge. We hypothesize that this type of internal factor, divorced from visual input and knowledge bases, may affect annotator score discrepancy.\\n\\nIn addition to these sources, it is highly probable that some of the annotation variance can be attributed to annotator carelessness. In other cases, the underlying cause of this variance may be due to a combination of the sources listed above. The complexity of the factors affecting annotator uncertainty scores illustrates the nuance of human uncertainty judgments and indicates that more research may need to be conducted in this field to better understand in what way humans differ in their processing of uncertain sensory input.\"}"}
{"id": "6Hl7XoPNAVX", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Examples of images that may elicit annotator disagreement. Left to right: (1) Visual attention: An image of a birthday party that may be mislabeled due to attention differences, since the key event-based attribute is not centered in the frame. Background knowledge of the source film may also affect values for this image. (2) Background knowledge: An image frame from Princess Ayako's wedding, which was a prominent current event in Japan. Annotators who saw footage from the wedding or are familiar with Japanese wedding traditions will likely have higher certainty scores for this data. (3) Uncertainty quantification: An image from a pulmonary function test that is ambiguous enough that it may receive different uncertainty scores from annotators with different risk tolerances - some annotators may naturally give a lower confidence score despite having the same background knowledge.\\n\\n5 Experiments\\n\\nWe run three experiments using SQUID-E to demonstrate the uses of ambiguous image datasets in training and evaluating models: (1) We show how training models on ambiguous images can improve their accuracy when classifying other ambiguous images by comparing models trained on \\\"high-certainty\\\" data with a model trained on SQUID-E. (2) We illustrate how SQUID-E can be used to assess existing situation models' performance on varying degrees of noisy data by evaluating state-of-the-art models on a subset of SQUID-E and partitioning their performance on SQUID-E's human labels. (3) We explore how SQUID-E can be used to directly evaluate model calibration techniques by comparing model confidence scores to SQUID-E's human labels. Additional details regarding experiments are included in Appendix D. We use mean human annotation scores as human labels for these experiments, which is compared to other aggregation approaches in Appendix D.3.\\n\\n5.1 Training on SQUID-E for Event Classification\\n\\nModels\\n\\nWe compare the accuracy of models trained on standard, \\\"high-certainty\\\" data and models trained on ambiguous images. We train one ResNet-based model using images from SQUID-E (RN+ES), and a set of ResNet-based models using images from standard, high-certainty image datasets (Visual Genome [35], Crowd Activity [72], USED [3], WIDER [77], and UCLA Protest Images [75]): RN+SD is trained on the images with no augmentation techniques, RN+PA is trained with photometric augmentation filters, RN+GA is trained with geometric augmentation filters, RN+NM is trained with noise injection and masking, RN+AU is trained with a combination of the augmentation filters listed above, and RN+AM is trained with the AugMix augmentation method [30].\\n\\nTask\\n\\nWe consider a four-way classification task using birthday party, wedding, parade, and protest images (with their respective event names as labels), since these are the four event types that are both represented in existing high-certainty image datasets and have human labels in SQUID-E. We train each model on these four event types using the same number of images from the two datasets. We run this experiment across 10 seeds and report mean accuracy and standard deviation in Table 1.\\n\\nResults\\n\\nThe results in Table 1 suggest that while data augmentation can improve model results on ambiguous data, it is not necessarily as effective as training on ambiguous data. The results indicate that this is the case even for images with relatively high certainty annotations. However, RN+ES having high accuracy scores for the lower certainty bins seems to indicate that it is poorly calibrated compared to the other approaches. While this may just be poor calibration, we hypothesize that this result is at least partially caused by the small set of possible classification labels in the experiment. In our annotation task, the human annotators had to account for the full range of possible event types that could occur in a video, but in this experiment the models only select between four event types.\"}"}
{"id": "6Hl7XoPNAVX", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Accuracy of verb classifiers on SQUID-E (bins, Avg. Acc) and standard data (SD Acc) when trained on standard data (RN+SD), augmented standard data (RN+PA, RN+GA, RN+MA, RN+AU, RN+AM), and SQUID-E (RN+ES). Results are partitioned into bins based on human-judged ambiguity. Best results for average accuracy are listed in bold. Details regarding data augmentation techniques are located in appendix D. While augmentation methods improve model performance on uncertain data, they do not achieve the accuracy scores of a model trained on ambiguous data.\\n\\n| Model       | 0-20%  | 20-40% | 40-60% | 60-80% | 80-100% | Avg. Acc | SD Acc. |\\n|-------------|--------|--------|--------|--------|---------|----------|---------|\\n| RN+SD       | .34 \u00b1  | .47 \u00b1  | .53 \u00b1  | .69 \u00b1  | .75 \u00b1   | .61 \u00b1    | .93 \u00b1   |\\n| RN+PA       | .27 \u00b1  | .32 \u00b1  | .47 \u00b1  | .65 \u00b1  | .74 \u00b1   | .57 \u00b1    | .90 \u00b1   |\\n| RN+GA       | .38 \u00b1  | .37 \u00b1  | .57 \u00b1  | .63 \u00b1  | .78 \u00b1   | .64 \u00b1    | .89 \u00b1   |\\n| RN+NM       | .28 \u00b1  | .45 \u00b1  | .61 \u00b1  | .69 \u00b1  | .81 \u00b1   | .64 \u00b1    | .91 \u00b1   |\\n| RN+AU       | .35 \u00b1  | .37 \u00b1  | .57 \u00b1  | .61 \u00b1  | .80 \u00b1   | .64 \u00b1    | .87 \u00b1   |\\n| RN+AM       | .31 \u00b1  | .40 \u00b1  | .53 \u00b1  | .69 \u00b1  | .77 \u00b1   | .60 \u00b1    | .93 \u00b1   |\\n| RN+ES       | .51 \u00b1  | .49 \u00b1  | .70 \u00b1  | .67 \u00b1  | .81 \u00b1   | .70 \u00b1    | .71 \u00b1   |\\n\\nTable 2: Accuracy of situation recognition models on SQUID-E. Results are partitioned on human judgments of the data. Results are partitioned into bins based on human-judged ambiguity. Results on the original SWiG dataset are reported under the column titled \u201cSD (Standard Data) Acc.\u201d Accuracy of the top scoring verb as well as the top 10 scoring verbs are reported (listed as \u201cTop 1\u201d and \u201cTop 10\u201d respectively). Best results for average accuracy are listed in bold. Results show how model performance is affected by different levels of uncertainty in the validation data.\\n\\n| Situation Recognition Model | Verb Accuracy (Top 1) | Verb Accuracy (Top 10) |\\n|----------------------------|-----------------------|------------------------|\\n| JSL                        | .00 .07 .17 .22 .52 .35 .40 | -                      |\\n| GSRTR                      | .02 .09 .22 .25 .59 .41    | -                      |\\n| CoFormer                   | .02 .13 .22 .23 .58 .45    | -                      |\\n\\n5.2 Evaluating Verb Prediction Models\\n\\nModels\\n\\nTo assess contemporary situation recognition models on ambiguous data, we evaluate the verb prediction modules of three high-performing models on the SQUID-E test set: JSL (Platt et al. [52]), GSRTR (Cho et al. [17]), and CoFormer (Cho et al. [16]). These models were selected because of their varied architectures and strong performance on existing benchmarks, and because they have publicly available model weights, ensuring accurate comparisons. JSL uses a ResNet-based verb predictor, GSRTR uses a transformer encoder for verb prediction, and CoFormer uses two transformers for verb prediction (a \u201cglance\u201d transformer and a \u201cgaze\u201d transformer). All three models are trained on the SWiG dataset [52] and can classify 504 distinct verbs in images. We specifically assess the models\u2019 verb prediction modules because verb prediction makes up the foundational task of situation recognition and aligns with the course-grained event information we ask annotators to judge in the data collection process. A more detailed explanation is included in appendix D.2.\\n\\nTask\\n\\nWe evaluate each verb classifier on \u201cparade\u201d and \u201cprotest\u201d images in SQUID-E, because these are the two event types in SQUID-E that have human labels and also exist within the ImSitu ontology. We consider the top scoring verb from each model as well as the top 10 scoring verbs. Results are partitioned on the images\u2019 human uncertainty scores, and are reported in Table 2 along with top-1 verb prediction performance on SWiG as reported in the models\u2019 respective papers.\\n\\nResults\\n\\nThis experiment demonstrates how we can use SQUID-E to characterize model performance on noisy data. Using the bin partitions in Table 2, we are able to identify average model performance at different levels of ambiguity. By comparing top-1 accuracy to top-10 accuracy, we are able to identify how much of the accuracy drop between bins is due to fine-tuning compared...\"}"}
{"id": "6Hl7XoPNAVX", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Evaluation of uncertainty quantification methods using accuracy on standard data (SD Acc), accuracy on SQUID-E (SE Acc), MSE against human judgments (HUJ MSE), and expected calibration error (ECE) (calculated using ground truth labels). Best results are listed in bold. (BL - Baseline, MC - Monte-Carlo, LS - Label Smoothing, BM - Belief Matching, FL - Focal Loss, RS - Relaxed Softmax).\\n\\n| Models    | SD Acc \u00b1 SE | SE Acc \u00b1 SE | HUJ MSE \u00b1 SE | ECE \u00b1 SE |\\n|-----------|-------------|-------------|--------------|---------|\\n| BL        | .92 \u00b1 .02   | .69 \u00b1 .02   | .15 \u00b1 .05    | .58 \u00b1 .02 |\\n| MC        | .92 \u00b1 .02   | .69 \u00b1 .02   | .14 \u00b1 .05    | .57 \u00b1 .03 |\\n| LS        | .92 \u00b1 .02   | .69 \u00b1 .02   | .12 \u00b1 .03    | .46 \u00b1 .02 |\\n| BM        | .92 \u00b1 .02   | .69 \u00b1 .02   | .14 \u00b1 .05    | .55 \u00b1 .02 |\\n| FL        | .92 \u00b1 .02   | .68 \u00b1 .02   | .11 \u00b1 .02    | .42 \u00b1 .02 |\\n| RS        | .91 \u00b1 .03   | .68 \u00b1 .03   | .12 \u00b1 .03    | .46 \u00b1 .05 |\\n\\nThese results achieved using the models trained on standard data suggest that model calibration techniques can produce models that align better with human judgments. Similarly, they show that human uncertainty judgments can be used to directly evaluate calibration approaches. The MSE and ECE show positive correlation, indicating that comparing against human judgments aligns with more traditional calibration assessment metrics. Based on these results, work remains to be done to identify the best approaches for aligning model confidence with human judgments for noisy data.\"}"}
{"id": "6Hl7XoPNAVX", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nSQUID-E only includes human judgments for a small portion of its images, and so models cannot be directly trained on these judgments given the high variance within this domain. Furthermore, each annotated image only has 6 human uncertainty judgments, which is not enough samples to capture the distribution of human judgments for a given image. Experiments in Section 5 are consequently run on a small amount of data and may produce different results if run on different event types, etc. We also cannot guarantee that the frame selection algorithm detailed in Section 3.1 produced the optimal set of visually distinct frames. While we attempted to remove all videos that did not involve a particularly wide range of visual data or contained clips used in other videos, it is likely that not all were filtered out, and so there may be some overly-similar images in the dataset.\\n\\nEthical considerations\\n\\nOur video collection queries were made in only 11 languages spoken widely online, and the majority were made in English, which likely produced an uneven distribution of regional and cultural representation within the dataset. Only collecting data via the YouTube platform also skewed the dataset's coverage. Because of these aspects of our data collection process, our dataset does not proportionately represent the experiences of the global population, which can potentially lead to biased models in downstream tasks. Similarly, we solicited our annotations from people located in the U.S. and used three-way redundancy for both tasks, meaning that the human judgments in the dataset are not representative of the general population, are likely skewed toward Western perspectives, and likely include demographic-driven biases.\\n\\nWhile the owners of the videos from which we extracted images were not asked for permission to include their content in the dataset, all used videos have been made publicly available by their creators. If a creator takes their content offline the associated images will automatically be removed from our public dataset loader as well. It should be noted that the videos used to create the dataset were not checked for personally identifiable or offensive content.\\n\\nAlternate approaches\\n\\nThere are multiple interpretations of what makes an image ambiguous, and what sort of ambiguous images are most relevant in the context of computer vision. In this paper, our goal is to minimize reporting bias [27] in image collection to retrieve a distribution of images that closely models natural human visual input. However, focusing on subsets of this distribution may provide data better suited for research in certain applications. For instance, some applications may only require images that depict the event type sufficiently well, and so images with low certainty scores should be removed from the main dataset. For other applications, it might be optimal to additionally remove images with high certainty labels and images with higher variance in their annotations. The remaining dataset, consisting of the images rated near the 50% mark with high agreement from annotators, could be used for efforts such as an in-depth analysis of the decision boundary of a model. These are both exciting directions for future work that consider different aspects of ambiguity and would provide opportunities for interesting analysis on these subsets of the dataset.\\n\\nConclusion\\n\\nIn this paper, we introduce a framework for generating datasets of ambiguous images and human uncertainty judgments and use it to develop SQUID-E (the Scenes with Quantitative Uncertainty Information Dataset for Events) consisting of 12,000 event-based ambiguous images and 10,800 human uncertainty judgments. We explore the characteristics of human uncertainty judgments for ambiguous visual data and show how this dataset can be used to assess the behavior of situation recognition models. Experiment results suggest that there is room for improvement in designing models that produce meaningful outputs when presented with ambiguous data, and that ambiguous datasets with human judgments can be used to train more robust models and to directly evaluate model calibration techniques. These findings motivate the creation of larger-scale ambiguous image datasets to develop more robust models and uncertainty quantification approaches and to further explore the relationship between models and human uncertainty judgments. This work also prompts other future work such as training models to learn individual annotators' uncertainty scoring functions and developing human-centric model calibration methods using human uncertainty judgments.\"}"}
{"id": "6Hl7XoPNAVX", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgments\\n\\nThe authors would like to thank David Etter, Elias Stengel-Eskin, Zhuowan Li, Zhengping Jiang, Jo\u00e3o Sedoc, and the anonymous reviewers.\\n\\nReferences\\n\\n[1] Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, et al. A review of uncertainty quantification in deep learning: Techniques, applications and challenges. *Information Fusion*, 76:243\u2013297, 2021.\\n\\n[2] William T Adler and Wei Ji Ma. Comparing bayesian and non-bayesian accounts of human confidence reports. *PLoS computational biology*, 14(11):e1006572, 2018.\\n\\n[3] Kashif Ahmad, Nicola Conci, Giulia Boato, and Francesco GB De Natale. Used: a large-scale social event detection dataset. In *Proceedings of the 7th International Conference on Multimedia Systems*, pages 1\u20136, 2016.\\n\\n[4] Kyriakos D Apostolidis and George A Papakostas. A survey on adversarial deep learning robustness in medical image analysis. *Electronics*, 10(17):2132, 2021.\\n\\n[5] Murat Seckin Ayhan and Philipp Berens. Test-time data augmentation for estimation of heteroscedastic aleatoric uncertainty in deep neural networks. In *Medical Imaging with Deep Learning*, 2018.\\n\\n[6] Luca Bascetta, Gianni Ferretti, Paolo Rocco, H\u00e5kan Ard\u00f6, Herman Bruyninckx, Eric Demeester, and Enrico Di Lello. Towards safe human-robot interaction in robotic cells: an approach based on visual tracking and intention estimation. In *2011 IEEE/RSJ international conference on intelligent robots and systems*, pages 2971\u20132978. IEEE, 2011.\\n\\n[7] Valerio Basile. It's the end of the gold standard as we know it. on the impact of pre-aggregation on the evaluation of highly subjective tasks. In *2020 AIxIA Discussion Papers Workshop, AIxIA 2020 DP*, volume 2776, pages 31\u201340. CEUR-WS, 2020.\\n\\n[8] Valerio Basile, Michael Fell, Tommaso Fornaciari, Dirk Hovy, Silviu Paun, Barbara Plank, Massimo Poesio, Alexandra Uma, et al. We need to consider disagreement in evaluation. In *1st Workshop on Benchmarking: Past, Present and Future*, pages 15\u201321. Association for Computational Linguistics, 2021.\\n\\n[9] William H Beluch, Tim Genewein, Andreas N\u00fcrnberger, and Jan M K\u00f6hler. The power of ensembles for active learning in image classification. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 9368\u20139377, 2018.\\n\\n[10] Andrea Bertana, Andrey Chetverikov, Ruben S van Bergen, Sam Ling, and Janneke FM Jehee. Dual strategies in human confidence judgments. *Journal of vision*, 21(5):21\u201321, 2021.\\n\\n[11] Lucas Beyer, Olivier J H\u00e9naff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00e4ron van den Oord. Are we done with imagenet? *arXiv preprint arXiv:2006.07159*, 2020.\\n\\n[12] Amy R Bland and Alexandre Schaefer. Different varieties of uncertainty in human decision-making. *Frontiers in neuroscience*, 6:85, 2012.\\n\\n[13] Marisa Carrasco. Visual attention: The past 25 years. *Vision research*, 51(13):1484\u20131525, 2011.\\n\\n[14] Marisa Carrasco, Sam Ling, and Sarah Read. Attention alters appearance. *Nature neuroscience*, 7(3):308\u2013313, 2004.\\n\\n[15] Tongfei Chen, Zhengping Jiang, Adam Poliak, Keisuke Sakaguchi, and Benjamin Van Durme. Uncertain natural language inference. *arXiv preprint arXiv:1909.03042*, 2019.\\n\\n[16] Junhyeong Cho, Youngseok Yoon, and Suha Kwak. Collaborative transformers for grounded situation recognition. *arXiv preprint arXiv:2203.16518*, 2022.\\n\\n[17] Junhyeong Cho, Youngseok Yoon, Hyeonjun Lee, and Suha Kwak. Grounded situation recognition with transformers. *arXiv preprint arXiv:2111.10135*, 2021.\\n\\n[18] Romain Cohendet, Claire-H\u00e9l\u00e8ne Demarty, Ngoc QK Duong, and Martin Engilberge. Videomem: Constructing, analyzing, predicting short-term and long-term video memorability. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 2531\u20132540, 2019.\\n\\n[19] Katherine M Collins, Umang Bhatt, and Adrian Weller. Eliciting and learning with soft labels from every annotator. *arXiv preprint arXiv:2207.00810*, 2022.\\n\\n[20] Hojat Asgarian Dehkordi, Ali Soltani Nezhad, Seyed Sajad Ashrafi, and Shahriar B Shokouhi. Still image action recognition using ensemble learning. In *2021 7th International Conference on Web Research (ICWR)*, pages 125\u2013129. IEEE, 2021.\"}"}
{"id": "6Hl7XoPNAVX", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.\\n\\n[13] Rachel N Denison, William T Adler, Marisa Carrasco, and Wei Ji Ma. Humans incorporate attention-dependent uncertainty into perceptual decisions and confidence. Proceedings of the National Academy of Sciences, 115(43):11090\u201311095, 2018.\\n\\n[14] Deepna Devkar, Anthony A Wright, and Wei Ji Ma. Monkeys and humans take local uncertainty into account when localizing a change. Journal of Vision, 17(11):4\u20134, 2017.\\n\\n[15] Nathan Drenkow, Numair Sani, Ilya Shpitser, and Mathias Unberath. Robustness in deep learning for computer vision: Mind the gap? arXiv preprint arXiv:2112.00639, 2021.\\n\\n[16] Ido Erev, Thomas S Wallsten, and David V Budescu. Simultaneous over-and underconfidence: The role of error in judgment processes. Psychological review, 101(3):519, 1994.\\n\\n[17] Stephen M Fleming and Nathaniel D Daw. Self-evaluation of decision-making: A general bayesian framework for metacognitive computation. Psychological review, 124(1):91, 2017.\\n\\n[18] Jonathan Gordon and Benjamin Van Durme. Reporting bias and knowledge acquisition. In Proceedings of the 2013 Workshop on Automated Knowledge Base Construction, AKBC '13, page 25\u201330, New York, NY, USA, 2013. Association for Computing Machinery.\\n\\n[19] Saurabh Gupta and Jitendra Malik. Visual semantic role labeling. arXiv preprint arXiv:1505.04474, 2015.\\n\\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arxiv 2015. arXiv preprint arXiv:1512.03385, 2015.\\n\\n[21] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. arXiv preprint arXiv:1912.02781, 2019.\\n\\n[22] Taejong Joo, Uijung Chung, and Min-Gwan Seo. Being bayesian about categorical probability. In International Conference on Machine Learning, pages 4950\u20134961. PMLR, 2020.\\n\\n[23] Daniel Kahneman, Olivier Sibony, and Cass R Sunstein. Noise: A flaw in human judgment. Little, Brown, 2021.\\n\\n[24] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? Advances in neural information processing systems, 30, 2017.\\n\\n[25] Shaiyan Keshvari, Ronald van den Berg, and Wei Ji Ma. Probabilistic Computation in Human Perception under Variability in Encoding Precision. PLOS ONE, 7(6):1\u20139, June 2012.\\n\\n[26] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):32\u201373, 2017.\\n\\n[27] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, Toronto, Ontario, 2009.\\n\\n[28] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. arxiv e-prints, page. arXiv preprint arXiv:1612.01474, 5, 2016.\\n\\n[29] Manling Li, Ruochen Xu, Shuohang Wang, Luowei Zhou, Xudong Lin, Chenguang Zhu, Michael Zeng, Heng Ji, and Shih-Fu Chang. Clip-event: Connecting text and images with event structures. arXiv preprint arXiv:2201.05078, 2022.\\n\\n[30] Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, and Desmond Elliott. Visually grounded reasoning across languages and cultures. arXiv preprint arXiv:2109.13238, 2021.\\n\\n[31] Wei Ji Ma and Mehrdad Jazayeri. Neural coding of uncertainty and probability. Annual review of neuroscience, 37:205\u2013220, 2014.\\n\\n[32] Jim Mainprice and Dmitry Berenson. Human-robot collaborative manipulation planning using early prediction of human motion. In 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 299\u2013306. IEEE, 2013.\\n\\n[33] Ishan Misra, C Lawrence Zitnick, Margaret Mitchell, and Ross Girshick. Seeing through the human reporting bias: Visual classifiers from noisy human-centric labels. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2930\u20132939, 2016.\\n\\n[34] Jorge Morales, Guillermo Solovey, Brian Maniscalco, Dobromir Rahnev, Floris P de Lange, and Hakwan Lau. Low attention impairs optimal incorporation of prior knowledge in perceptual decisions. Attention, Perception, & Psychophysics, 77(6):2021\u20132036, 2015.\"}"}
{"id": "6Hl7XoPNAVX", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip Torr, and Puneet Dokania. Calibrating deep neural networks using focal loss. Advances in Neural Information Processing Systems, 33:15288\u201315299, 2020.\\n\\nRafael M\u00fcller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? Advances in neural information processing systems, 32, 2019.\\n\\nThomas Mussweiler and Fritz Strack. Numeric judgments under uncertainty: The role of knowledge in anchoring. Journal of experimental social psychology, 36(5):495\u2013518, 2000.\\n\\nZachary Nado, Neil Band, Mark Collier, Josip Djolonga, Michael Dusenberry, Sebastian Farquhar, Angelos Filos, Marton Havasi, Rodolphe Jenatton, Ghassen Jerfel, Jeremiah Liu, Zelda Mariet, Jeremy Nixon, Shreyas Padhy, Jie Ren, Tim Rudner, Yeming Wen, Florian Wenzel, Kevin Murphy, D. Sculley, Balaji Lakshminarayanan, Jasper Snoek, Yarin Gal, and Dustin Tran. Uncertainty Baselines: Benchmarks for uncertainty & robustness in deep learning. arXiv preprint arXiv:2106.04015, 2021.\\n\\nLukas Neumann, Andrew Zisserman, and Andrea Vedaldi. Relaxed softmax: Efficient confidence auto-calibration for safe pedestrian detection. NIPS 2018 Workshop MLITS, 2018.\\n\\nDavid A Nix and Andreas S Weigend. Estimating the mean and variance of the target probability distribution. In Proceedings of 1994 ieee international conference on neural networks (ICNN'94), volume 1, pages 55\u201360. IEEE, 1994.\\n\\nHalil \u02d9Ibrahim \u00d6zt\u00fcrk and Ahmet Burak Can. Adnet: Temporal anomaly detection in surveillance videos. In International Conference on Pattern Recognition, pages 88\u2013101. Springer, 2021.\\n\\nJoshua C. Peterson, Ruairidh M. Battleday, Thomas L. Griffiths, and Olga Russakovsky. Human uncertainty makes classification more robust. CoRR, abs/1908.07086, 2019.\\n\\nSarah Pratt, Mark Yatskar, Luca Weihs, Ali Farhadi, and Aniruddha Kembhavi. Grounded situation recognition. In European Conference on Computer Vision, pages 314\u2013332. Springer, 2020.\\n\\nGabriel A Radvansky and Jeffrey M Zacks. Event perception. Wiley Interdisciplinary Reviews: Cognitive Science, 2(6):608\u2013620, 2011.\\n\\nDobromir Rahnev, Brian Maniscalco, Tashina Graves, Elliott Huang, Floris P De Lange, and Hakwan Lau. Attention induces conservative subjective biases in visual perception. Nature neuroscience, 14(12):1513\u20131515, 2011.\\n\\nEric Raufaste, Rui da Silva Neves, and Claudette Marin\u00e9. Testing the descriptive validity of possibility theory in human judgments of uncertainty. Artificial Intelligence, 148(1-2):197\u2013218, 2003.\\n\\nJacob C Reinhold, Yufan He, Shizhong Han, Yunqiang Chen, Dashan Gao, Junghoon Lee, Jerry L Prince, and Aaron Carass. Validating uncertainty in medical image translation. In 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), pages 95\u201398. IEEE, 2020.\\n\\nJohn H Reynolds and Leonardo Chelazzi. Attentional modulation of visual processing. Annu. Rev. Neurosci., 27:611\u2013647, 2004.\\n\\nMatteo Ruggero Ronchi and Pietro Perona. Describing common human visual actions in images. arXiv preprint arXiv:1506.02203, 2015.\\n\\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge (2014). arXiv preprint arXiv:1409.0575, 2(3), 2014.\\n\\nArka Sadhu, Tanmay Gupta, Mark Yatskar, Ram Nevatia, and Aniruddha Kembhavi. Visual semantic role labeling for video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5589\u20135600, 2021.\\n\\nKeisuke Sakaguchi and Benjamin Van Durme. Efficient online scalar annotation with bounded support. arXiv preprint arXiv:1806.01170, 2018.\\n\\nAbhishek Singh Sambyal, Narayanan C Krishnan, and Deepti R Bathula. Towards reducing aleatoric uncertainty for medical imaging tasks. arXiv preprint arXiv:2110.11012, 2021.\\n\\nLars Schmarje, Johannes Br\u00fcnger, Monty Santarossa, Simon-Martin Schr\u00f6der, Rainer Kiko, and Reinhard Koch. Fuzzy overclustering: Semi-supervised classification of fuzzy labels with overclustering and inverse cross-entropy. Sensors, 21(19):6661, 2021.\\n\\nPhilipp Schustek and Rub\u00e9n Moreno-Bote. Instance-based generalization for human judgments about uncertainty. PLoS Computational Biology, 14(6):e1006205, 2018.\\n\\nMaximilian Seitzer, Arash Tavakoli, Dimitrije Antic, and Georg Martius. On the pitfalls of heteroscedastic uncertainty estimation with probabilistic neural networks. arXiv preprint arXiv:2203.09168, 2022.\\n\\nMA Sykes, MB Welsh, and SH Begg. Don't drop the anchor: Recognizing and mitigating human factors when making assessment judgments under uncertainty. In SPE Annual Technical Conference and Exhibition. OnePetro, 2011.\"}"}
{"id": "6Hl7XoPNAVX", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jayaraman J Thiagarajan, Prasanna Sattigeri, Deepta Rajan, and Bindya Venkatesh. Calibrating healthcare AI: Towards reliable and interpretable deep predictive models. arXiv preprint arXiv:2004.14480, 2020.\\n\\nAmos Tversky and Daniel Kahneman. Judgment under uncertainty: Heuristics and biases. Science, 185(4157):1124\u20131131, 1974.\\n\\nSirion Vittayakorn and James Hays. Quality assessment for crowdsourced object annotations. In BMVC, pages 1\u201311, 2011.\\n\\nNidhi Vyas, Shreyas Saxena, and Thomas Voice. Learning soft labels via meta learning. arXiv preprint arXiv:2009.09496, 2020.\\n\\nEdgar Y. Walker, R. James Cotton, Wei Ji Ma, and Andreas S. Tolias. A neural basis of probabilistic computation in visual cortex. bioRxiv, 2018.\\n\\nHao Wang, Junchao Liao, Tianheng Cheng, Zewen Gao, Hao Liu, Bo Ren, Xiang Bai, and Wenyu Liu. Knowledge mining with scene text for fine-grained recognition. arXiv preprint arXiv:2203.14215, 2022.\\n\\nShuo Wang, Qiushuo Zheng, Zherong Su, Chongning Na, and Guilin Qi. Meed: A multimodal event extraction dataset. In China Conference on Knowledge Graph and Semantic Computing, pages 288\u2013294. Springer, 2021.\\n\\nMeng Wei, Long Chen, Wei Ji, Xiaoyu Yue, and Tat-Seng Chua. Rethinking the two-stage framework for grounded situation recognition. arXiv preprint arXiv:2112.05375, 2021.\\n\\nDonghyeon Won, Zachary C Steinert-Threlkeld, and Jungseock Joo. Protest activity detection and perceived violence estimation from social media images. In Proceedings of the 25th ACM international conference on Multimedia, pages 786\u2013794, 2017.\\n\\nZanwu Xia, Qujiang Lei, Yang Yang, Hongda Zhang, Yue He, Weijun Wang, and Minghui Huang. Vision-based hand gesture recognition for human-robot collaboration: a survey. In 2019 5th International Conference on Control, Automation and Robotics (ICCAR), pages 198\u2013205. IEEE, 2019.\\n\\nYuanjun Xiong, Kai Zhu, Dahua Lin, and Xiaoou Tang. Recognize complex events from static images by fusing deep channels. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1600\u20131609, 2015.\\n\\nMark Yatskar, Luke Zettlemoyer, and Ali Farhadi. Situation recognition: Visual semantic role labeling for image understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5534\u20135542, 2016.\\n\\nJeffrey M Zacks. Event perception and memory. Annual Review of Psychology, 71:165\u2013191, 2020.\\n\\nJeffrey M Zacks, Nicole K Speer, Khena M Swallow, Todd S Braver, and Jeremy R Reynolds. Event perception: a mind-brain perspective. Psychological bulletin, 133(2):273, 2007.\\n\\nYanli Zhou, Luigi Acerbi, and Wei Ji Ma. The role of sensory uncertainty in simple perceptual organization. bioRxiv, page 350082, 2018.\"}"}
