{"id": "QXTjde8evS", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nWe present DiffInfinite, a hierarchical diffusion model that generates arbitrarily large histological images while preserving long-range correlation structural information. Our approach first generates synthetic segmentation masks, subsequently used as conditions for the high-fidelity generative diffusion process. The proposed sampling method can be scaled up to any desired image size while only requiring...\"}"}
{"id": "QXTjde8evS", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"small patches for fast training. Moreover, it can be parallelized more efficiently than previous large-content generation methods while avoiding tiling artifacts. The training leverages classifier-free guidance to augment a small, sparsely annotated dataset with unlabelled data. Our method alleviates unique challenges in histopathological imaging practice: large-scale information, costly manual annotation, and protective data handling. The biological plausibility of DiffInfinite data is evaluated in a survey by ten experienced pathologists as well as a downstream classification and segmentation task. Samples from the model score strongly on anti-copying metrics which is relevant for the protection of patient data.\\n\\nFigure 1: a) Examples of synthetic and real $2048 \\\\hat{\\\\times} 2048$ images. b) Pairs of $512 \\\\hat{\\\\times} 512$ synthetic tiles (top) with the closest real images found with Inception-v3 near-neighbour (bottom).\\n\\n1 Introduction\\n\\nDeep learning (DL) models are promising auxiliary tools for medical diagnosis [1\u20133]. Applications like segmentation and classification have been refined and pushed to the limit on natural images [4]. However, these models trained on rich datasets still have limited applications in medical data. While segmentation models rely on sharp object contours when applied to natural data, in medical imaging, the model struggles to detect a specific feature because it has a \\\"limited ability to handle objects with missed boundaries\\\" and often \\\"miss tiny and low-contrast objects\\\" [5, 6]. Therefore, task-specific medical applications require their own specialised and fine-grained annotation. Data labelling is arguably one of the most critical bottlenecks in healthcare machine learning (ML) applications.\\n\\nIn histopathology, pathologists examine the histological slide at multiple levels, usually starting with a lower magnification to analyse the tissue architecture and cellular arrangement and gradually proceeding to a higher magnification to examine cell morphology and subcellular features, such as the appearance and number of nucleoli, chromatin density and cytoplasm appearance. Annotating features within gigapixel whole slide images (WSIs) with this level of detail demands effort and time, often leading to sparse, limited annotated data. In addition, due to privacy regulations and ethics [7, 8], having access to medical data can be challenging since it has been shown that it is possible to extract patients' sensitive information [9] from this data.\\n\\nIn histopathology, state-of-the-art ML models require the context of the entire WSIs, with features at different scales, in order to distinguish between different tumor sub-types, grades and stages [10]. Despite the demonstrated effectiveness of diffusion models (DMs) in generating natural images compared to other approaches, they still have rarely been applied in medical imaging. Existing\"}"}
{"id": "QXTjde8evS", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"generative models in histopathology can generate images of relatively small resolution compared to WSIs. To give a few examples, the application of Generative Adversarial Networks (GANs) in cervical dysplasia detection \\\\[11\\\\], glioma classification \\\\[12\\\\], and generating images of breast and colorectal cancer \\\\[13\\\\], generate images with \\\\(256 \\\\times 256\\\\), \\\\(384 \\\\times 384\\\\) and \\\\(224 \\\\times 224\\\\), respectively. In spite of their current limitations in generating images at scales necessary to fully address all medical concerns, the use of synthetic data in medical imaging can provide a valuable solution to the persistent issue of data scarcity \\\\[14\\\\]\u2013\\\\[17\\\\]. Models generally improve after data augmentation and synthetic images are equally informative as real images when added to the training set \\\\[18\\\\],\\\\[19\\\\]. Data augmentation could also help with the underrepresentation in data sets of rare cancer subtypes. By adding synthetic images to the training set, Chen et al. \\\\[20\\\\] demonstrated that their model had better accuracy in detecting chromophobe renal cell carcinoma, which is a rare subtype of renal cell carcinoma. Furthermore, Doleful et al. \\\\[21\\\\] showed how synthetic histological images could be used for educational purposes for pathology residents. Regarding the challenges highlighted before, we present a novel sampling method to generate large histological images with long-range pixel correlation (see Fig. 1), aiming to extend up to the resolution of the WSI.\\n\\nOur contributions are as follows: 1) We introduce DiffInfinite, a hierarchical generative framework that generates arbitrarily large images, paired with their segmentation masks. 2) We introduce a fast outpainting method that can be efficiently parallelized. 3) The quality of DiffInfinite data is evaluated by ten experienced pathologists as well as downstream machine learnings tasks (classification and segmentation) and anti-duplication metrics to assess the leakage of patient data from the training set.\\n\\n2 Related Work\\n\\nLarge-content image generation can be reduced to inpainting/outpainting tasks. Image inpainting is the problem of reconstructing unknown or unwanted areas within an image. A closely related task is image outpainting, which aims to predict visual content beyond the boundaries of an image. In both cases, the newly in- or outpainted image regions have to be visually indistinguishable with respect to the rest of the image. Such image completion approaches can help utilise models trained on smaller patches for the purpose of generating large images, by initially generating the first patch, followed by its extension outward in the desired direction.\\n\\nTraditional approaches\\n\\nTraditional methods for image region completion rely on repurposing known image features, necessitating costly nearest neighbour searches for suitable pixels or patches \\\\[22\\\\]\u2013\\\\[26\\\\]. Such methods often falter with complex or large regions \\\\[24\\\\]. In contrast, DL enables novel, realistic image synthesis for inpainting and outpainting. Some methods like Deep Image Prior \\\\[27\\\\] condition new image areas on the existing image, while others aim to learn natural image priors for realistic generation \\\\[28\\\\],\\\\[29\\\\].\\n\\nGenerative modelling for conditional image synthesis\\n\\nGANs have dominated image-to-image translation tasks like inpainting and outpainting for years \\\\[28\\\\]\u2013\\\\[42\\\\]. Recently, DMs have surpassed GANs in various image generation tasks \\\\[43\\\\]. Palette \\\\[44\\\\] was the first to apply DMs to tasks like inpainting and outpainting. RePaint \\\\[45\\\\] and ControlNet \\\\[46\\\\] demonstrate resampling and masking techniques for conditioning using a pre-trained diffusion model. SinDiffusion \\\\[47\\\\] and DiffCollage \\\\[48\\\\] offer state-of-the-art outpainting solutions using DMs trained with overlapping patches. In parallel to our work, Bond-Taylor and Willcocks \\\\[49\\\\] developed a related approach called 8-Diff which trains on random coordinates, allowing the generation of infinite-resolution images during sampling. However, in contrast to our approach the method does not involve image compression in a latent space.\\n\\nSynthetic data assessment\\n\\nThe authenticity of synthetic data produced by DMs, trained on vast paired labelled datasets \\\\[50\\\\], remains contentious. Ethical implications necessitate distinguishing if generated images are replicas of training data \\\\[51\\\\],\\\\[52\\\\]. The task is complicated due to subjective visual similarities and diverse dataset ambiguities. Various metrics have been proposed for quantifying data replication, including information theory distances from real data \\\\[53\\\\], consistency measurements using downstream models \\\\[54\\\\],\\\\[55\\\\], comparison with inpainted areas \\\\[52\\\\], and detection of \u201cforgotten\u201d examples \\\\[56\\\\].\"}"}
{"id": "QXTjde8evS", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: DiffInfinite generation method. a) Large-scale context mask generation. A diffusion model conditioned on a large-scale conditional prompt (e.g. Adenocarcinoma subtype) generates a low-resolution mask. The mask is upsampled via linear interpolation to the desired image size. b) Diffusion steps on large images. Given a random position, we select a sub-tile with its segmentation mask. A diffusion model generates in parallel the next step conditioned on each conditional label, or prompt, found in the mask. The outputs are masked individually with the corresponding label. The next step is the union of all the sub-patches. c) Tracking time steps pixel-wise. We keep track of the time step of each pixel in the large image. The model evolves only the pixels with the higher time step on each iteration.\\n\\n3 Preliminaries\\n\\nDiffusion Models\\n\\nDMs represent a class of parameterized Markov chains that effectively optimize the lower variational bound associated with the likelihood function of the unknown data distribution. By iteratively adding small amounts of noise until the image signal is destroyed and then learning to reverse this process, DMs can approximate complex distributions much more faithfully than GANs. The increased diversity of samples while preserving sample fidelity comes at the cost of training and sampling speed, with DMs being much slower than GANs. The universally adopted solution to this problem is to encode the images from pixel space into a lower dimensional latent space via a Vector Quantised-Variational AutoEncoder (VQ-V AE), and perform the diffusion process over the latents, before decoding back to pixel space. Pairing this with the Denoising Diffusion Implicit Models (DDIMs) sampling method leads to faster sampling while preserving the DM objective.\"}"}
{"id": "QXTjde8evS", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A convex combination \\\\( \\\\tilde{\\\\varepsilon} \\\\) of \\\\( p_z(t,c)q \\\\) and an unconditional model \\\\( p_z(t),c q \\\\) is used for noise estimation. The parameter \\\\( \\\\theta \\\\) controls the tradeoff between conditioning and diversity, since \\\\( \\\\theta > 0 \\\\) introduces more diversity in the generated data by considering the unconditional model while \\\\( \\\\theta = 0 \\\\) uses only the conditional model.\\n\\n4 Infinite Diffusion\\n\\nThe DiffInfinite approach we present here, is a generative algorithm to generate arbitrarily large images without imposing conditional independence, allowing for long-range correlation structural information. The method overcomes this limitation of DMs for large-content generation by deploying multiple realizations of a DM on smaller patches. In this section, we first define a mathematical description of this hierarchical generation model and then describe the sampling method paired with a masked conditioned generation process.\\n\\n4.1 The Method\\n\\nLet \\\\( X \\\\) be a large-content generating random variable taking values in \\\\( \\\\mathbb{R}^{K,D} \\\\). Using the approach of latent diffusion models [61], the high-dimensional content is first mapped to the latent space \\\\( \\\\mathbb{R}^D \\\\) by \\\\( p_X q = Y \\\\). For simplicity, we assume throughout this work the existence of an ideal encoder-decoder pair \\\\( p, q \\\\) such that \\\\( p p_X qq = X \\\\) is the identity on \\\\( \\\\mathbb{R}^{K,D} \\\\). Assume further, to have a reverse time model \\\\( p_{SM} \\\\) at hand consisting of a sampling method \\\\( SM \\\\) and a learned model \\\\( \\\\tilde{\\\\varepsilon} \\\\) trained on small patches \\\\( Z = Z \\\\) taking values in \\\\( \\\\mathbb{R}^d \\\\). The reverse time model transforms \\\\( z_T = N_p_0, I_d q \\\\) over the time steps \\\\( t = T, T-1, \\\\ldots, 1 \\\\) recursively by \\\\( z_t \\\\approx SM p_z t q \\\\) to an approximate instance of \\\\( Z \\\\). We aim to sample instances from \\\\( Y \\\\) by deploying multiple realizations of the reverse time model \\\\( p_{SM} \\\\). Towards that goal, define the set of projections \\\\( C = \\\\{ \\\\text{proj}_I : \\\\mathbb{R}^D \\\\rightarrow \\\\mathbb{R}^d | I \\\\subseteq N \\\\} \\\\) correspond to \\\\( d \\\\) indices of connected pixels in \\\\( \\\\mathbb{R}^{K,D} \\\\), where \\\\( \\\\text{proj}_P \\\\) models a crop \\\\( \\\\text{proj}_P y \\\\) of \\\\( d \\\\) connected pixels from the latent image \\\\( Y \\\\). Since the model \\\\( \\\\tilde{\\\\varepsilon} \\\\) is trained on images taking values in \\\\( \\\\mathbb{R}^d \\\\) the standing assumption is\\n\\nAssumption 1\\n\\nAny projection \\\\( \\\\text{proj}_P \\\\) maps \\\\( Y \\\\) to the same distribution \\\\( \\\\text{proj}_P y \\\\approx Z \\\\) in \\\\( \\\\mathbb{R}^d \\\\).\\n\\nSince the goal is to approximate an instance of \\\\( Y \\\\), we initialize the sampling method by \\\\( y_T = N_p_0, I_D q \\\\) and proceed in the following way: Given \\\\( y_t \\\\), randomly choose \\\\( \\\\text{proj}_{I_1}, \\\\ldots, \\\\text{proj}_{I_m} \\\\) independent of the state \\\\( y_t \\\\) such that \\\\( \\\\text{proj}_{I_1}, \\\\ldots, \\\\text{proj}_{I_m} \\\\) are non equal crops that cover all latent pixels in \\\\( \\\\mathbb{R}^{K,D} \\\\). To be more precise, for every \\\\( i = 1, \\\\ldots, D_u \\\\) we find at least one \\\\( j = 1, \\\\ldots, m \\\\) with \\\\( i \\\\in I_j \\\\). For every projection \\\\( \\\\text{proj}_{I_1}, \\\\ldots, \\\\text{proj}_{I_m} \\\\) we calculate the crop \\\\( z_{j,t} = \\\\text{proj}_{I_j} y_t \\\\) of the current state \\\\( y_t \\\\) and perform one step of the reverse time model following the sampling scheme\\n\\n\\\\[ z_{j,t-1} = SM p_{z_{j,t}} q \\\\]\\n\\nThis results in overlapping estimates \\\\( z_{1,t-1}, \\\\ldots, z_{m,t-1} \\\\) of the subsequent state \\\\( t-1 \\\\) and we simply assign to every pixel in the latent space the first value computed for this pixel such that\\n\\n\\\\[ r_y t-1 s_i = r z_{j,t-1} s_l, \\\\]\\n\\nwhere \\\\( j = \\\\min_j | i \\\\in I_j \\\\) and \\\\( l \\\\) refers to the entry in \\\\( z_{j,t-1} \\\\) corresponding to \\\\( i \\\\) with \\\\( r \\\\text{proj}_{I_j} y_{t-1} q s_l \\\\). Hence, starting from \\\\( y_T = N_p_0, I_D q \\\\) we sample in the first step from a distribution\\n\\n\\\\[ y_{T-1} = p_{T-1}, \\\\tilde{\\\\varepsilon} p_y | y_T, \\\\text{proj}_{I_1}, \\\\ldots, \\\\text{proj}_{I_m} q. \\\\]\\n\\nUsing Bayes' theorem, this distribution simplifies to\\n\\n\\\\[ p_{T-1}, \\\\tilde{\\\\varepsilon} p_y | y_T, \\\\text{proj}_{I_1}, \\\\ldots, \\\\text{proj}_{I_m} q = p_{T-1}, \\\\tilde{\\\\varepsilon} p_y | y_T q, \\\\]\\n\\nCode available at [https://github.com/marcoaversa/diffinfinite](https://github.com/marcoaversa/diffinfinite)\"}"}
{"id": "QXTjde8evS", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"since we sample the projections independently from $y_T$. Repeating the argument, we sample in every step from a distribution $y_{t \\\\cdot 1} \\\\cdot \\\\mathbb{p}_{t \\\\cdot 1}$, $\\\\mathcal{R}$ instead of sampling from $z_{t \\\\cdot 1} \\\\cdot \\\\mathbb{p}_{t \\\\cdot 1}$, $\\\\mathcal{R}$.\\n\\nHence, we approximate the true latent distribution $Y$ by the approximate distribution with density $p_0, \\\\mathbb{p}_y | y_1, \\\\ldots, y_T q$.\\n\\nIn contrast to [48], our method does not use the assumption of conditional independence and the method can be applied to a wide range of DMs, without an adjustment of the training method. As the authors of [48] point out in their section on limitations, the assumption of conditional independence is not well-suited in cases of a data distribution with long-range dependence. For image generation in the medical context, we aim to circumvent this assumption as we do not want to claim that the density of a given region depends only on one neighboring region. The drawback of dropping the assumption is that we only approximate the reverse time model of the latent image distribution $Y$ indirectly, by multiple realizations of a reverse time model that approximates $Z$.\\n\\n4.2 Semi-supervised Guidance\\n\\nIn order to generate diverse high-fidelity data, DMs require lots of training data. Perhaps, training on a few samples still extracts significant features but it lacks variability, resulting in simple replicas. Here, we show how to enhance synthetic data diversity using classifier-free guidance as a semi-supervised learning method. In the classifier-free guidance [66], a single model is trained conditionally and unconditionally on the same dataset. We adapt the training scheme using two separate datasets. The model is guided by a small and sparsely annotated dataset $q_1$, used for the conditional training step, while extracts features by the large unlabelled dataset $q_0$, used on the unconditional training step (see Alg. 1).\\n\\n4.3 Sampling\\n\\nHigh-level content generation\\n\\nThe outputs of DMs have pixel consistency within the training image size. Outpainting an area with a generative model might lead to unrealistic and odd artifacts due to poor long-range spatial correlations. Here, we show how to predict pixels beyond the image's boundaries by generating a hierarchical mapping of the data. The starting point is the generation of the highest-level representation of the data. In our case, it is the sketch of the cellular arrangement in the WSI (see Figure 2a). Since higher-frequency details are unnecessary at this stage, we can downsample the masks until the clustering pattern is still recognizable. The diffusion model, conditioned on the context prompt (e.g. Adenocarcinoma subtype), learns the segmentation masks which contain the cellular macro-structures information.\\n\\nFigure 3: Comparison of sampling speed for DiffCollage and DiffInfinite, measuring diffusion steps required for image sampling. Demonstrating increased efficiency of DiffInfinite for larger images.\\n\\nRandom patch diffusion\\n\\nGiven a segmentation mask $M$, we can proceed with the large image sampling according to Section 4.1 in the latent space $\\\\mathbb{R}^D$ of $Y$ (see Alg. 2). Since we trained a conditional diffusion model with conditions $c_1, \\\\ldots, c_N$, the learned model takes the form $\\\\mathbb{p}_x q \\\\cdot \\\\mathbb{p}_z | c_1 q, \\\\ldots, \\\\mathbb{p}_z | c_N q$. Given $y_t$, we first sample projections $\\\\text{proj}_{I_1}, \\\\ldots, \\\\text{proj}_{I_m} \\\\mathbb{P}_C$, corresponding to different crops of $d$ connected pixels up to the $m$-th projection with $Y_m j = 1 I_j t, \\\\ldots, D u$ and $Y_m j = 1 I_j zY_m \\\\cdot 1 I_j \\\\cdot j = 1 H$ (see the left hand-side of Figure 2b). Note that $m$ is not fixed, but varies over the sampling steps and is upper bounded by the number of possible crops of $d$ connected pixels. The random selection of\"}"}
{"id": "QXTjde8evS", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"data, we performed evaluations to rule out memorization effects of the generative model. We entailed a proof-of-concept that a model can learn sensible features from the synthetically generated data. A team of pathologists evaluated the images for histological plausibility. The quantitative assessment into account. We extend traditional metrics from the natural image community with new limits. In our approach, we can parallelize the sliding sampling up to the computational resource limit. Here, the randomization eventually leads to a reversed diffusion process of differing time states. We initialize a tensor with random projections of the overall image, in the previous diffusion step for every pixel. Finally, we update all the time states in the sliding window one. Moreover, we are outpainting the image horizontally or on every possible overlap with the neighboring row of Fig. 1. An update in the previous one with 50% of the pixels shared. This introduces a longer pixel correlation across the whole generated image, avoiding artifacts due to tiling. In Figure 19, we show that the number of steps in the whole process of outpainting is drastically reduced with the random patching method with respect to the sliding window one. Moreover, we can generate segmentation masks of arbitrary sizes with the correlation length bounded by the sampling up to the computational resource limit. In Zhang et al. (2021), we are outpainting the image horizontally or on every possible overlap with the neighboring row of Fig. 1. An update in the previous one with 50% of the pixels shared. This introduces a longer pixel correlation across the whole generated image, avoiding artifacts due to tiling. In Figure 19, we show that the number of steps in the whole process of outpainting is drastically reduced with the random patching method with respect to the sliding window one. Moreover, we can generate segmentation masks of arbitrary sizes with the correlation length bounded by the sampling up to the computational resource limit.\\n\\n## Data Assessment\\n\\n### Sampling Method\\n\\nThe sampling method produces an unconditioned outpainting with a random patch as the context prompt. This method can be applied to mask generation, where the only condition is the context prompt. This method can generate segmentation masks of arbitrary sizes with the correlation length bounded by the sampling up to the computational resource limit. In the random patching method, the model can be paralleled only in the sliding sampling method, the model can be paralleled only in the sliding sampling method, the model can be paralleled only in the sliding sampling method, the model can be paralleled only in the sliding sampling method.\\n\\n### Initialization\\n\\nInitialization:\\n\\n- Input: Crop randomly train on labelled or unlabelled data with probability \\\\( \\\\frac{1}{2} \\\\)...\\n- Output: High-level segmentation mask\\n\\n### Algorithm 1\\n\\n1. Crop randomly train on labelled or unlabelled data with probability \\\\( \\\\frac{1}{2} \\\\).\\n2. Repeat until \\\\( t = T \\\\) for all \\\\( m \\\\) do:\\n   - Sample noise, \\\\( z \\\\), \\\\( p \\\\), \\\\( q \\\\) :\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n   - Take gradient descent step:\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step} \\\\)\\n     - \\\\( q = \\\\text{Uniform} \\\\)\\n     - \\\\( z = \\\\text{Sample noise} \\\\)\\n     - \\\\( p = \\\\text{Sample random time step}"}
{"id": "QXTjde8evS", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Metrics to quantitatively evaluate the quality of the generated images. Left: scores for images of size $512 \\\\times 512$. DiffInfinite (a) first generates a mask and secondly an image following Section 4.1. Right: scores for real and generated images of size $2048 \\\\times 2048$ resized to $512 \\\\times 512$. All methods use the same model trained on small patches of size $512 \\\\times 512$. DiffCollage corresponds to the method proposed in [48]. DiffInfinite (b) uses the real masks, while DiffInfinite (c) first generates a mask and secondly the large image. DiffInfinite (b) & (c) refers to the mixture of the generated dataset from DiffInfinite (b) and DiffInfinite (c).\\n\\n| Method               | IP  | IR  | IS  | FID |\\n|----------------------|-----|-----|-----|-----|\\n| Morph-Diffusion      | 0.26| 0.85| 2.1 | 20.1|\\n| NASDM                | --  | 2.7 | 15.7|\\n| DiffInfinite (a)     | 0.94| 0.70| 2.7 | 26.7|\\n| DiffInfinite (b)     | 0.95| 0.48|     |     |\\n| DiffInfinite (c)     | 0.98| 0.44|     |     |\\n| DiffInfinite (b) & (c)| 0.98| 0.33|     |     |\\n\\n5.1 Traditional Fidelity\\nWe evaluate the fidelity of synthetic $512 \\\\times 512$ images by calculating Improved Precision (IP) and Improved Recall (IR) metrics between 10240 real and synthetic images [67]. The IP evaluates synthetic data quality, while the IR measures data coverage. Despite their unsuitability for histological data [68, 69], Frechet-Inception Distance (FID) and Inception Score (IS) [70, 71] are reported for comparison with [72] and Shrivastava and Fletcher [73]. The metrics' explanations and formulas can be found in Appendix C.\\n\\nIn Table 1 (left), we report an IP of 0.94 and an IR of 0.70, indicating good quality and coverage of the generated samples. However, we note that these metrics are only somewhat comparable due to the different types of images generated by MorphDiffusion [72] and NASDM [73]. For the large images of size $2048 \\\\times 2048$, we rely solely on the IP and IR for quantitative evaluation due to the limited number of 200 generated large images. As shown in Figure 3(a) of [67], FID is unsuitable for evaluating such a small sample size, while IP and IR are more reliable. In Table 1 (right), we find that generating images first results in slightly higher IR, while generating the mask first achieves an IP of 0.98. For the sake of completeness we also report the scores then combining the two datasets.\\n\\nTo compare our method to DiffCollage we generate 200 images using [48]. DiffInfinite performs better than DiffCollage wrt. to IP and IR. The drop of IR to 0.22 might be a result of the tiling artifacts observable in the LHS of Figure 11.\\n\\n5.2 Domain Experts Assessment\\nTo assess the histological plausibility of our generated images, we conducted a survey with a cohort of ten experienced pathologists, averaging 8.7 years of professional tenure. The pathologists were tasked with differentiating between our synthetized images and real image patches extracted from whole slide images. We included both small patches ($512 \\\\times 512$ px) as commonly used for downstream tasks as well as large patches ($2048 \\\\times 2048$ px). Including large patches enabled us to additionally evaluate the modelled long-range correlations in terms of transitions between tissue types as well as growth patterns which are usually not observable on the smaller patch sizes but essential in histopathology. In total the survey contained 60 images, in equal parts synthetic and real images as well as small and large patches. The overall ability of pathologists to discern between real and synthetic images was modest, with an accuracy of 63%, and an average reported confidence level of 2.22 on a 1-7 Likert scale. While we observed high inter-rater variance, there was no clear correlation between experience and accuracy ($r(8) = .069, p=.850$), nor between confidence level and accuracy ($r(8) = .446, p=.197$). Furthermore there was no significant correlation between the participants' completion time of the survey and the number of correct responses ($r(8) = -.08, p=.826$).\\n\\nSurprisingly, we found a similar performance for both, real and synthetic images. This indicates that, while clinical practice is mostly based on visual assessment, it is not a common task for pathologists to be restricted to parts of the whole slide image only. More detailed visualizations of the individual scores can be found in Appendix B. Besides this satisfactory result, we additionally wanted to...\"}"}
{"id": "QXTjde8evS", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Zero-shot evaluation results of the downstream tasks, encompassing both classification and segmentation scenarios. We employed three distinct models for each scenario: The first, \u201cTrained Real,\u201d was trained using real data (in-house IH1), which also served as the training set for DiffInfinite. The second, \u201cTrained Synthetic,\u201d was trained using samples generated from DiffInfinite, and the third, \u201cTrained Augmented,\u201d utilized a combination of real and synthetic data. Our evaluation extends across separate lung cohorts (internal datasets IH2 and IH3) and additional indications (external datasets NCT, CRC, PCam), with varying degrees of data drift introduced.\\n\\n| Drift Components | IH1 | IH2 | IH3 | NCT-100K | CRC-7K | PCam-327K |\\n|------------------|-----|-----|-----|-----------|--------|-----------|\\n| Patient Change   |     |     |     |           |        |           |\\n| Different Center |     |     |     |           |        |           |\\n| Indication Change|     |     |     |           |        |           |\\n| Lower Resolution |     |     |     |           |        |           |\\n\\n(a) Classification results\\n\\n| Model                  | IH1          | IH2          | IH3          | NCT-100K    | CRC-7K      | PCam-327K   |\\n|------------------------|--------------|--------------|--------------|-------------|-------------|-------------|\\n| Trained Real           | 0.846 \u00b1 0.005| 0.743 \u00b1 0.005| 0.697 \u00b1 0.021| 0.598 \u00b1 0.049| 0.822 \u00b1 0.034| 0.628 \u00b1 0.035|\\n| Trained Synthetic      | 0.747 \u00b1 0.025| 0.753 \u00b1 0.005| 0.699 \u00b1 0.002| 0.796 \u00b1 0.023| 0.753 \u00b1 0.038| 0.628 \u00b1 0.012|\\n| Trained Augmented      | 0.852 \u00b1 0.007| 0.732 \u00b1 0.027| 0.637 \u00b1 0.025| 0.847 \u00b1 0.044| 0.811 \u00b1 0.057| 0.641 \u00b1 0.035|\\n\\n(b) Segmentation results\\n\\n| Model                  | IH1          | IH2          | IH3          | NCT-100K    | CRC-7K      | PCam-327K   |\\n|------------------------|--------------|--------------|--------------|-------------|-------------|-------------|\\n| Trained Real           | 0.614 \u00b1 0.009| 0.471 \u00b1 0.039| 0.710 \u00b1 0.021| 0.628 \u00b1 0.035| 0.641 \u00b1 0.035|             |\\n\\n5.3 Synthetic Data for Downstream Tasks\\n\\nA major interest in the availability of high quality labeled synthetic images is their use in downstream digital pathology applications. In this area, two primary challenges are the binary classification of images into cancerous or healthy tissues and the segmentation of distinct tissue areas in the tumor microenvironment. The unique ability of our technique to generate images of different cancer subtypes through the context prompt as well as the ability to create new segmentation masks and their corresponding H&E images specifically addresses these two challenges. Notably, expert annotations are costly and time consuming to acquire thus emphasizing the benefits of being able to train on purely synthetic datasets or augmenting annotated data in the low data regime. To showcase these two usecases we performed a series of experiments in both classification and segmentation settings. For all experiments, we trained a baseline classifier on a relatively small number of expert annotations (IH1 (#patches = 3726)) \u2014 the same that were used to train DiffInfinite \u2014 and additionally trained one model purely on synthetic data (IH1-S, #patches = 9974), and one model on the real data augmented with the synthetic images. To generate target labels for the classification experiments, we simplified the segmentation challenge by categorizing patches with at least 0.05% of pixels labeled as \u2018Carcinoma\u2019 in the segmentation masks as \u2018Carcinoma\u2019. All other patches were labeled \u2018Non-Carcinoma\u2019. We evaluated all three classification models on several out-of-distribution datasets. We utilized two proprietary datasets (from the same cancer type with similar attributes but from distinct patient groups: IH1 (# patients=13, # patches=704) and IH3 (# patients=2, # patches=2817). Moreover, we assessed the models using two public datasets (NCK-CRC [74] and PatchCamelyon [75]), both representing tissue from different organs with distinct morphologies. Our findings, summarized in Table 2a, suggest that a classifier\u2019s out-of-distribution performance, trained with limited sample size and morphological diversity, can vary significantly (ranging from 0.628 to 0.857 balanced accuracy). This variability cannot be attributed solely to morphology but may also be influenced by other factors.\"}"}
{"id": "QXTjde8evS", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"factors such as resolution and variations in scanning and staining techniques. Training exclusively with a larger set of synthetic images can enhance performance on some datasets (specifically IH2 and IH3), underscoring the advantages of leveraging the full training data in a semi-supervised manner within the generative model. Incorporating synthetic data as an augmentation to real data not only prevents the classifier's performance decline, as seen on NCT-CRC and Patchcamelyon, on similar datasets but also bolsters its efficiency on more distinct ones. For the more challenging segmentation task we again trained three segmentation models to differentiate between carcinoma, stroma, necrosis, and a miscellaneous class that included all other tissue types, such as artifacts. The baseline performance of the real data model on a distinct group of lung patients (dataset IH2) of a $F_1$ score of $0.614 \\\\pm 0.009$ (across three random seeds) highlights the difficulty of generalizing out of distribution in this task. While the purely synthetic model was not able to fully recover the baseline performance ($0.471 \\\\pm 0.039$), augmenting the small annotated dataset with synthetic data enhanced predictive performance to an $F_1$ score of $0.710 \\\\pm 0.021$. This boost of 10 percentage points in performance demonstrates that the synthetic data provide new, relevant information to the downstream task. In summary, our findings demonstrate the feasibility of meeting or surpassing baseline performance levels for both tasks using either entirely synthetic data or within an augmented context. Nevertheless, the advantages of employing synthetic data in downstream tasks continue to pose a challenge, not only within the medical image domain but also across various other domains.\\n\\n5.4 Considerations on Memorization\\n\\nIn medicine the adherence to privacy regulations is a sensitive requirement. While it is generally not possible for domain experts to infer patient identities from the image content of a histological tile or slide alone [79], developers and users of generative models are well advised to understand the risk of correspondence between the training data and the synthesized data. To this end, we evaluate the training and synthesized data against two memorization measures. The authenticity score $A_{Pr}$ by [54] aims to measure the rate by which a model generates new samples (higher score means more innovative samples). Similarly, [80] aims to estimate the degree of data copying $C_T$ from the training data by the generative model. $A_{C_T}$ implies data copying, while $A_{C_T} \\\\leq 0$ implies an underfitting of the model. The closer to 0 the better. See Appendix C for a precise closed form of the measures and Table 5 for the full quantitative results, indicating that the DiffInfinite model is not prone to data copying across all resolutions and variations considered here. The $A_{range}$ between 0.86 and 0.98, signifying a high rate of authenticity. While other papers unfortunately do not report such detailed memorization statistics for their models, the results by [54] suggest that a score $\\\\geq 0.8$ is not trivial to achieve. None of the models under consideration in [54] (VAE, DCGAN, WGAN-GP, ADS-GAN) achieve more than 0.82 in $A$ on simpler data (MNIST). This interpretation is strengthened by the results of a $C_T \\\\leq 0$ which indicates that the model might even be underfitting and is not in a data copying regime. Qualitative results on the nearest neighbour search between training and synthetic data in Figure 1 further corroborate these quantitative results.\\n\\n6 Conclusions\\n\\nDiffInfinite offers a novel sampling method to generate large images in digital pathology. Due to the high-level mask generation followed by the low-level image generation, synthetic images contain long-range correlations while maintaining high-quality details. Since the model trains and samples on small patches, it can be efficiently parallelized. We demonstrated that the classifier-free guidance can be extended to a semi-supervised learning method, expanding the labelled data feature space with unlabelled data. The biological plausibility of the synthetic images was assessed in a survey by 10 domain experts. Despite their training, most participants found it challenging to differentiate between real and synthetic data, reporting an average low confidence in their decisions. We found that samples from DiffInfinite can help in certain downstream machine learning tasks, on both in- as well as out-of-distribution datasets. Finally, authenticity metrics validate DiffInfinite's capacity to generate novel data points with little similarity to the training data which is beneficial for the privacy-preserving use of generative models in medicine.\"}"}
{"id": "QXTjde8evS", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nWe would like to acknowledge our team of pathologists who provided valuable feedback in and outside of the conducted survey - special thank you to Frank Dubois, Niklas Prenissl, Cleopatra Schreiber, Vitaly Garg, Alexander Arnold, Sonia Villegas, Rosemarie Krupar and Simon Schallenberg. Furthermore, we would like to thank Marvin Sextro for his support in the analyses. This work was supported by the Federal Ministry of Education and Research (BMBF) as grants [SyReal (01IS21069B)]. RM-S is grateful for EPSRC support through grants EP/T00097X/1, EP/R018634/1 and EP/T021020/1, and DI for EP/R513222/1. MA is funded by Dotphoton, QuantIC and a UofG Ph.D. scholarship.\\n\\nReferences\\n\\n[1] Ravi Aggarwal, Viknesh Sounderajah, Guy Martin, Daniel SW Ting, Alan Karthikesalingam, Dominic King, Hutan Ashrafian, and Ara Darzi. Diagnostic accuracy of deep learning in medical imaging: a systematic review and meta-analysis. NPJ digital medicine, 4(1):65, 2021.\\n\\n[2] Antonio Parziale, Monica Agrawal, Shengpu Tang, Kristen Severson, Luis Oala, Adarsh Subbaswamy, Sayantan Kumar, Elora Schoerverth, Stefan Hegselmann, Helen Zhou, Ghada Zamzmi, Purity Mugambi, Elena Sizikova, Girmaw Abebe Tadesse, Yuyin Zhou, Taylor Killian, Haoran Zhang, Fahad Kamran, Andrea Hobby, Mars Huang, Ahmed Alaa, Harvineet Singh, Irene Y. Chen, and Shalmali Joshi. Machine learning for health (ml4h) 2022. In Antonio Parziale, Monica Agrawal, Shalmali Joshi, Irene Y. Chen, Shengpu Tang, Luis Oala, and Adarsh Subbaswamy, editors, Proceedings of the 2nd Machine Learning for Health symposium, volume 193 of Proceedings of Machine Learning Research, pages 1\u201311. PMLR, 28 Nov 2022.\\n\\n[3] Maximilian Springenberg, Annika Frommholz, Markus Wenzel, Eva Weicken, Jackie Ma, and Nils Strodthoff. From modern cnns to vision transformers: Assessing the performance, robustness, and classification strategies of deep learning models in histopathology. Medical Image Analysis, 87:102809, 2023. ISSN 1361-8415.\\n\\n[4] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt: Segmenting everything in context. arXiv preprint arXiv:2304.03284, 2023.\\n\\n[5] Miriam H\u00e4gele, Philipp Seegerer, Sebastian Lapuschkin, Michael Bockmayr, Wojciech Samek, Frederick Klauschen, Klaus-Robert M\u00fcller, and Alexander Binder. Resolving challenges in deep learning-based analyses of histopathological images using explanation methods. Scientific reports, 10(1):1\u201312, 2020.\\n\\n[6] Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023.\\n\\n[7] Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R Roth, Shadi Albarqouni, Spyridon Bakas, Mathieu N Galtier, Bennett A Landman, Klaus Maier-Hein, et al. The future of digital health with federated learning. NPJ digital medicine, 3(1):119, 2020.\\n\\n[8] Luis Oala, Jana Fehr, Luca Gilli, Pradeep Balachandran, Alixandro Werneck Leite, Saul Calderon-Ramirez, Danny Xie Li, Gabriel Nobis, Erick Alejandro Mu\u00f1oz Alvarado, Giovanna Jaramillo-Gutierrez, Christian Matek, Arun Shroff, Ferath Kherif, Bruno Sanguinetti, and Thomas Wiegand. Ml4h auditing: From paper to practice. In Emily Alsentzer, Matthew B. A. McDermott, Fabian Falck, Suproteem K. Sarkar, Subhrajit Roy, and Stephanie L. Hyland, editors, Proceedings of the Machine Learning for Health NeurIPS Workshop, volume 136 of Proceedings of Machine Learning Research, pages 280\u2013317. PMLR, 11 Dec 2020.\\n\\n[9] Christopher G Schwarz, Walter K Kremers, Terry M Therneau, Richard R Sharp, Jeffrey L Gunter, Prashanthi Vemuri, Arvin Arani, Anthony J Spychalla, Kejal Kantarci, David S Knopman, et al. Identification of anonymous mri research participants with face-recognition software. New England Journal of Medicine, 381(17):1684\u20131686, 2019.\"}"}
{"id": "QXTjde8evS", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Richard J Chen, Chengkuan Chen, Yicong Li, Tiffany Y Chen, Andrew D Trister, Rahul G Krishnan, and Faisal Mahmood. Scaling vision transformers to gigapixel images via hierarchical self-supervised learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16144\u201316155, 2022.\\n\\nYuan Xue, Jiarong Ye, Qianying Zhou, L Rodney Long, Sameer Antani, Zhiyun Xue, Carl Cornwell, Richard Zaino, Keith C Cheng, and Xiaolei Huang. Selective synthetic augmentation with histogan for improved histopathology image classification. Medical image analysis, 67:101816, 2021.\\n\\nLe Hou, Ayush Agarwal, Dimitris Samaras, Tahsin M Kurc, Rajarsi R Gupta, and Joel H Saltz. Unsupervised histopathology image synthesis. arXiv preprint arXiv:1712.05021, 2017.\\n\\nAdalberto Claudio Quiros, Roderick Murray-Smith, and Ke Yuan. PathologyGAN: learning deep representations of cancer tissue. Journal of Machine Learning for Biomedical Imaging, 4:1\u201348, 2021.\\n\\nAhmad B Qasim, Ivan Ezhov, Suprosanna Shit, Oliver Schoppe, Johannes C Paetzold, Anjany Sekuboyina, Florian Kofler, Jana Lipkova, Hongwei Li, and Bjoern Menze. Red-gan: Attack-ting class imbalance via conditioned generation. yet another medical imaging perspective. In Medical Imaging with Deep Learning, pages 655\u2013668. PMLR, 2020.\\n\\nPierre Chambon, Christian Bluethgen, Jean-Benoit Delbrouck, Rogier Van der Sluijs, Ma\u0142gorzata Pollarinc, Juan Manuel Zambrano Chaves, Tanishq Mathew Abraham, Shivanshu Purohit, Curtis P Langlotz, and Akshay Chaudhari. Roentgen: Vision-language foundation model for chest x-ray generation. arXiv preprint arXiv:2211.12737, 2022.\\n\\nPierre Chambon, Christian Bluethgen, Curtis P Langlotz, and Akshay Chaudhari. Adapting pretrained vision-language foundational models to medical imaging domains. arXiv preprint arXiv:2210.04133, 2022.\\n\\nLuis Oala, Marco Aversa, Gabriel Nobis, Kurt Willis, Yoan Neuenschwander, Mich\u00e8le Buck, Christian Matek, Jerome Extermann, Enrico Pomarico, Wojciech Samek, Roderick Murray-Smith, Christoph Clausen, and Bruno Sanguinetti. Data models for dataset drift controls in machine learning with optical images. Transactions on Machine Learning Research, 2023.\\n\\nAdrian B Levine, Jason Peng, David Farnell, Mitchell Nursey, Yiping Wang, Julia R Naso, Hezhen Ren, Hossein Farahani, Colin Chen, Derek Chiu, Aline Talhouk, Brandon Sheffield, Maziar Riazy, Philip P Ip, Carlos Parra-Herran, Anne Mills, Naveena Singh, Basile Tessier-Cloutier, Taylor Salisbury, Jonathan Lee, Tim Salcudean, Steven JM Jones, David G Huntsman, C Blake Gilks, Stephen Yip, and Ali Bashashati. Synthesis of diagnostic quality cancer pathology images by generative adversarial networks. The Journal of Pathology, 252(2):178\u2013188, 2020.\\n\\nVirginia Fernandez, Walter Hugo Lopez Pinaya, Pedro Borges, Petru-Daniel Tudosiu, Mark S Graham, Tom Vercauteren, and M Jorge Cardoso. Can segmentation models be trained with fully synthetically generated data? In Simulation and Synthesis in Medical Imaging: 7th International Workshop, SASHIMI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18, 2022, Proceedings, pages 79\u201390. Springer, 2022.\\n\\nRichard Chen, Ming Lu, Tiffany Chen, Drew Williamson, and Faisal Mahmood. Synthetic data in machine learning for medicine and healthcare. Nature Biomedical Engineering, 5:1\u20135, 06 2021.\\n\\nJames M Dolezal, Rachelle Wolk, Hanna M Hieromnimon, Frederick M Howard, Andrew Srisuwananukorn, Dmitry Karpeyev, Siddhi Ramesh, Sara Kochanny, Jung Woo Kwon, Meghana Agni, et al. Deep learning generates synthetic cancer histology for explainability and education. NPJ Precision Oncology, 7(1):49, 2023.\\n\\nAlexei A Efros and Thomas K Leung. Texture synthesis by non-parametric sampling. In Proceedings of the seventh IEEE international conference on computer vision, volume 2, pages 1033\u20131038. IEEE, 1999.\"}"}
{"id": "QXTjde8evS", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Li-Yi Wei and Marc Levoy. Fast texture synthesis using tree-structured vector quantization. In Proceedings of the 27th annual conference on Computer graphics and interactive techniques, pages 479\u2013488, 2000.\\n\\nMarcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and Coloma Ballester. Image inpainting. In Proceedings of the 27th annual conference on Computer graphics and interactive techniques, pages 417\u2013424, 2000.\\n\\nZongben Xu and Jian Sun. Image inpainting by patch propagation using patch sparsity. IEEE transactions on image processing, 19(5):1153\u20131165, 2010.\\n\\nLin Liang, Ce Liu, Ying-Qing Xu, Baining Guo, and Heung-Yeung Shum. Real-time texture synthesis by patch-based sampling. ACM Transactions on Graphics (ToG), 20(3):127\u2013150, 2001.\\n\\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9446\u20139454, 2018.\\n\\nAvisek Lahiri, Arnav Kumar Jain, Sanskar Agrawal, Pabitra Mitra, and Prabir Kumar Biswas. Prior guided gan based semantic inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13696\u201313705, 2020.\\n\\nLiang Liao, Jing Xiao, Zheng Wang, Chia-Wen Lin, and Shin\u2019ichi Satoh. Guidance and evaluation: Semantic-aware image inpainting for mixed scenes. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXVII 16, pages 683\u2013700. Springer, 2020.\\n\\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014.\\n\\nDeepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2536\u20132544, 2016.\\n\\nJiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image inpainting with contextual attention. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5505\u20135514, 2018.\\n\\nJiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Free-form image inpainting with gated convolution. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4471\u20134480, 2019.\\n\\nYi Wang, Ying-Cong Chen, Xin Tao, and Jiaya Jia. Vcnet: A robust approach to blind image inpainting. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXV 16, pages 752\u2013768. Springer, 2020.\\n\\nShengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I Chang, and Yan Xu. Large scale image completion via co-modulated generative adversarial networks. arXiv preprint arXiv:2103.10428, 2021.\\n\\nRoman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. arXiv preprint arXiv:2109.07161, 2021.\\n\\nMark Sabini and Gili Rusak. Painting outside the box: Image outpainting with gans. arXiv preprint arXiv:1808.08483, 2018.\\n\\nBasile Van Hoorick. Image outpainting and harmonization using generative adversarial networks. arXiv preprint arXiv:1912.10960, 2019.\"}"}
{"id": "QXTjde8evS", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chieh Hubert Lin, Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, and Ming-Hsuan Yang. InfinityGAN: Towards infinite-pixel image synthesis. In International Conference on Learning Representations, 2022.\\n\\nYen-Chi Cheng, Chieh Hubert Lin, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, and Ming-Hsuan Yang. Inout: diverse image outpainting via gan inversion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11431\u201311440, 2022.\\n\\nYaxiong Wang, Yunchao Wei, Xueming Qian, Li Zhu, and Yi Yang. Sketch-guided scenery image outpainting. IEEE Transactions on Image Processing, 30:2643\u20132655, 2021.\\n\\nYaxiong Wang, Yunchao Wei, Xueming Qian, Li Zhu, and Yi Yang. Rego: Reference-guided outpainting for scenery image. arXiv preprint arXiv:2106.10601, 2021.\\n\\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780\u20138794, 2021.\\n\\nChitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, pages 1\u201310, 2022.\\n\\nAndreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11461\u201311471, 2022.\\n\\nLvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023.\\n\\nWeilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, and Houqiang Li. Sindiffusion: Learning a diffusion model from a single natural image. arXiv preprint arXiv:2211.12445, 2022.\\n\\nQinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, and Ming-Yu Liu. Diffcollage: Parallel generation of large content with diffusion models. arXiv preprint arXiv:2303.17076, 2023.\\n\\nSam Bond-Taylor and Chris G. Willcocks. 8-diff: Infinite resolution diffusion with subsampled mollified states, 2023.\\n\\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\\n\\nGowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6048\u20136058, 2023.\\n\\nNicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. arXiv preprint arXiv:2301.13188, 2023.\\n\\nNikhil Vyas, Sham Kakade, and Boaz Barak. Provable copyright protection for generative models. arXiv preprint arXiv:2302.10870, 2023.\\n\\nAhmed Alaa, Boris Van Breugel, Evgeny S Saveliev, and Mihaela van der Schaar. How faithful is your synthetic data? sample-level metrics for evaluating and auditing generative models. In International Conference on Machine Learning, pages 290\u2013306. PMLR, 2022.\\n\\nLuis Oala. Metrological machine learning (2ML). 1 edition, 2023. URL https://metrological.ml.\"}"}
{"id": "QXTjde8evS", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "QXTjde8evS", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Aman Shrivastava and P Thomas Fletcher. Nasdm: Nuclei-aware semantic histopathology image generation using diffusion models. arXiv preprint arXiv:2303.11477, 2023.\\n\\nJakob Nikolas Kather, Niels Halama, and Alexander Marx. 100,000 histological images of human colorectal cancer and healthy tissue, April 2018. URL https://doi.org/10.5281/zenodo.1214456.\\n\\nBastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant cnns for digital pathology. In Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part II 11, pages 210\u2013218. Springer, 2018.\\n\\nShenghuan Sun, Gregory M Goldgof, Atul Butte, and Ahmed M Alaa. Aligning synthetic medical images with clinical knowledge using human feedback. arXiv preprint arXiv:2306.12438, 2023.\\n\\nXian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. Self-alignment with instruction backtranslation. arXiv preprint arXiv:2308.06259, 2023.\\n\\nIlia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. The curse of recursion: Training on generated data makes models forget. arXiv preprint arXiv:2305.17493, 2023.\\n\\nPetr Holub, Heimo M\u00fcller, Tom\u00e1\u0161 B\u011bl, Luca Pireddu, Markus Plass, Fabian Prasser, Irene Schlunder, Kurt Zatloukal, Rudolf Nenutil, and Tom\u00e1\u0161 Br\u00e1zdil. Privacy risks of whole-slide image sharing in digital pathology. Nature Communications, 14(1):2577, 2023.\\n\\nCasey Meehan, Kamalika Chaudhuri, and Sanjoy Dasgupta. A non-parametric test to detect data-copying in generative models. CoRR, abs/2004.05675, 2020.\\n\\nMarco Jiralerspong, Avishek Joey Bose, Ian Gemp, Chongli Qin, Yoram Bachrach, and Ga\u00ebthier Gidel. Feature likelihood score: Evaluating generalization of generative models using samples, 2023.\\n\\nNicolas Pielawski and Carolina Wahlby. Introducing hann windows for reducing edge-effects in patch-based image segmentation. PloS one, 15(3):e0229839, 2020.\"}"}
