{"id": "dI4wzAE6uV", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We follow instructions provided by Datasheet for Datasets to answer the important questions considering this dataset.\\n\\nA.1 Motivation\\n\\nFor what purpose was the dataset created?\\n\\nThe advancement of Large Language Models (LLMs) has raised concerns regarding whether state-of-the-art LLMs, such as ChatGPT and Codex, can replace human effort in real-world text-to-SQL tasks involving large database values. That is because their exceptional performance on previous academic tasks like PIDER impresses researchers. However, we observe that current cross-domain text-to-SQL benchmarks only focus on the database schema, which lacks full attention to values, resulting in a gap between academic and real-world applications. To address this issue, we introduce BIRD, the largest cross-domain text-to-SQL benchmark highlighting extensive and realistic databases for community development. Additionally, we hope to observe the performance gap between LLMs and humans. Our experimental results indicate that, as of now, LLMs are still unable to replace human effort. As far as we know, BIRD is the first text-to-SQL benchmark to collect human performance.\\n\\nWho created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\\n\\nOur research team involves Star Lab at The University of Hong Kong, Alibaba DAMO Academy ConAI Team, the Department of Computer Science at the University of Illinois Urbana-Champaign, the Department of EECS at Massachusetts Institute of Technology, the School of Data Science at The Chinese University of Hong Kong (Shenzhen), and Database Group of Tsinghua University.\\n\\nWho funded the creation of the dataset?\\n\\nThis dataset is fully funded by the Alibaba DAMO Academy ConAI team. We spent 97,654 USD for presenting this data. The budget includes 10% for recruiting competent research interns, 80% for developing the benchmark, and 10% for refining and implementing the benchmark.\\n\\nA.2 Composition\\n\\nWhat do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?\\n\\nBIRD contains natural language questions, external knowledge evidence sentences, processed large databases, database description files (csv), and SQL queries.\\n\\nHow many instances are there in total (of each type, if appropriate)?\\n\\nBIRD contains 12,751 natural language questions, 12,751 external knowledge evidence sentences, 95 processed large databases, 95 folders of database description CSV files, and 12,751 ground truth SQL queries.\\n\\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?\\n\\nIn BIRD, we divide it into three sets: training, development, and testing. Training and development sets are public while testing data set is hidden for the fair evaluation of all text-to-SQL challengers. This could witness the real development of text-to-SQLs in the LLM era.\\n\\nIs there a label or target associated with each instance?\\n\\nIn BIRD, we provide two labels for each question instance: SQLs (the target of input) and external knowledge evidence (expert annotated evidence for each expected SQL).\\n\\nIs any information missing from individual instances?\\n\\nNo.\\n\\nAre relationships between individual instances made explicit (e.g., users' movie ratings, social network links)?\\n\\nNo.\\n\\nAre there recommended data splits?\\n\\nOur data consists of 9,428 instances for the training set, 1,534 instances for the development set, and 1,789 instances for the concealed test set. The training\"}"}
{"id": "dI4wzAE6uV", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and development sets are derived from public databases, while the test set databases are curated and designed by our specialized team. We do this because some researchers express concerns that the remarkable performance of LLMs in text-to-SQL tasks may not be attributed to an improvement in capabilities, but rather to the exposure of data and database values to the LLMs during the pre-training phase. To address these concerns, we opt to self-design new databases in testing using actual tabular data, thereby ensuring that LLMs do not preview the databases.\\n\\nAre there any errors, sources of noise, or redundancies in the dataset?\\n\\nAs stated in the main content, our double-blind annotation procedure is both expensive and rigorous, ensuring data quality. However, it is virtually impossible for any dataset, especially complex ones, to be entirely free of errors. Our team is committed to enhancing the data even after this paper is accepted, thereby contributing to the text-to-SQL community. In addition, we encourage users to provide feedback and report errors on our data website, allowing us to rectify and enhance the dataset.\\n\\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?\\n\\nYes, all databases in training and development are collected under appropriate licenses. Please see Section 3.2 for more details.\\n\\nDoes the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)?\\n\\nNo.\\n\\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?\\n\\nNo.\\n\\nDoes the dataset identify any subpopulations (e.g., by age, gender)?\\n\\nSome questions mention ages and genders, but they are just used to detect the capability of models on text-to-SQLs. No bias or other opinions are involved.\\n\\nIs it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset?\\n\\nNo. All databases are collected from open-sourced platforms, and any sensitive data has already been processed before.\\n\\nDoes the dataset contain data that might be considered sensitive in any way (e.g., data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)?\\n\\nNo, this is a QA-based text-to-SQL dataset, we don't require models to deliver any opinions on results. And also we don't present any bias or opinions in the dataset.\\n\\nA.3 Collection Process\\n\\nHow was the data associated with each instance acquired?\\n\\nSection 3 and Appendix B.2 introduce this in detail.\\n\\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)?\\n\\nSection 3 and Appendix B.2 introduce this in detail. Our crowdworkers use Alibaba internal labeling software to annotate the data and examine the results.\\n\\nIf the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?\\n\\nNo.\\n\\nWho was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?\\n\\nFour PhD students and two MS students are involved in the creation of database description files. Two independent teams of crowdworkers are recruited to annotate questions and SQLs. The question annotators are composed of 11 English native speakers and SQL annotators are comprised of database engineers and DB students. The total consumption is 97,654 USD.\"}"}
{"id": "dI4wzAE6uV", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Over what timeframe was the data collected? From Sep. 2022 to Mar. 2023.\\n\\nWere any ethical review processes conducted (e.g., by an institutional review board)? Yes, we take such issues very seriously. During the review process, we found that certain questions related to politics or inappropriate language. We have addressed these concerns by modifying the content and providing a serious warning to the annotators responsible for such instances.\\n\\nDid you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)? Section 3 and Appendix B.2 introduce this in detail.\\n\\nWere the individuals in question notified about the data collection? Yes.\\n\\nDid the individuals in question consent to the collection and use of their data? Sure, we recruited them and paid them satisfying salaries.\\n\\nIf consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? No.\\n\\nHas an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? Yes, we did a very comprehensive analysis including error analysis, and efficiency analysis, in the experiments of the paper and Appendix.\\n\\nA.4 Preprocessing/cleaning/labeling\\n\\nWas any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? Yes, we provide the token list for each question and SQLs from NLTK for users.\\n\\nWas the \\\"raw\\\" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? No. Is the software that was used to preprocess/clean/label the data available? Yes, https://www.nltk.org/\\n\\nA.5 Uses\\n\\nHas the dataset been used for any tasks already? No.\\n\\nIs there a repository that links to any or all papers or systems that use the dataset? No.\\n\\nWhat (other) tasks could the dataset be used for? Sure, our databases and analysis-style questions are most valuable, so they could be beneficial to DB-based code generation, data science analysis, etc.\\n\\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? No.\\n\\nAre there tasks for which the dataset should not be used? No.\\n\\nA.6 Distribution\\n\\nWill the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? No.\\n\\nHow will the dataset will be distributed (e.g., tarball on the website, API, GitHub)? All source codings and datasets could be found on our leaderboard website: https://bird-bench.github.io/ And we provide fast download links for the convenience of researchers who want to use our big data. Furthermore, the code repository can be found in https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/bird19\"}"}
{"id": "dI4wzAE6uV", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When will the dataset be distributed?\\nNow.\\n\\nWill the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?\\n\\nGiven the database size of \\\\texttt{IRD} is the largest until now, we are afraid that abusing ample database values may lead to inappropriate commercial use. Therefore, we claim that this dataset should be distributed under CC BY-NC 4.0.\\n\\nHave any third parties imposed IP-based or other restrictions on the data associated with the instances?\\nNo.\\n\\nDo any export controls or other regulatory restrictions apply to the dataset or to individual instances?\\nNo.\\n\\nA.7 Maintenance\\nWho will be supporting/hosting/maintaining the dataset?\\nHKU STAR LAB and Alibaba DAMO Academy\\n\\nHow can the owner/curator/manager of the dataset be contacted (e.g., email address)?\\nContact bird.bench23@gmail.com or the corresponding authors or co-first authors in the author list.\\n\\nIs there an erratum?\\nNo.\\n\\nWill the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?\\nYes, we will keep polishing and optimizing our data periodically.\\n\\nIf the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., was the individuals in question were told that their data would be retained for a fixed period of time and then deleted)?\\nNo.\\n\\nWill older versions of the dataset continue to be supported/hosted/maintained?\\nNo. The most updated version will be more reliable.\\n\\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?\\nYes, but they should contact the authors first.\"}"}
{"id": "dI4wzAE6uV", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix\\n\\nB.1 Text-to-SQL Difficulty\\n\\nIn order to help researchers deeply analyze model performance in various text-to-SQL case levels, we class all examples as simple (30%), moderate (60%), and challenging (10%). Previous work, such as S-PIDER, computed difficulty mainly based on SQL complexity. However, we find that additional factors, such as question comprehension, schema linking, and external knowledge reasoning, also influence model and human performance. Therefore, each SQL annotator is required to evaluate examples based on these factors, and experts conclude the ratings to divide examples into the three aforementioned difficulty levels. This approach offers a more extensive difficulty analysis for text-to-SQL tasks. And the performance of ChatGPT on three different difficulty levels is shown in Table B.1.\\n\\nWe take the approach of human scoring under established rules. A detailed crowdsourcing rule is employed to rate the difficulty when SQL annotators generate SQLs for each question. The process consists of evaluating four dimensions:\\n\\n1. **Question Understanding:** On a discrete scale from 1 to 3, annotators assess the ambiguity and difficulty of comprehending the question\u2019s intent, with 1 being straightforward, 2 being clear but requiring more thought, and 3 being extremely ambiguous.\\n\\n2. **Knowledge Reasoning:** On a discrete scale from 1 to 3, annotators rate the amount of external knowledge required to map the question to SQL, with 1 indicating no knowledge is required, 2 requiring evidence of external knowledge for generating SQLs that is easy to understand, and 3 requiring extensive knowledge and much more thoughts.\\n\\n3. **Data Complexity:** Annotators rate the complexity of schema relations and data size that need analyzing on a discrete scale of 1-3, with 1 being a simple schema and data, 2 being complex schema and values understandable through database description files, and 3 being highly complex and difficult to comprehend values and schema even with description files.\\n\\n4. **SQL Complexity:** Annotators rate the syntactic complexity of the target SQL query on a discrete scale of 1-3, with 1 being a simple SQL without many keywords, 2 being more complicated than 1, and 3 being a highly complex SQL with many functions and operations.\\n\\nEach dimension is considered equally important for text-to-SQL annotations. SQLs are ranked based on these scores, and we present simple, moderate, and challenging difficulties at proportions of 30%, 60%, and 10%, respectively.\\n\\n|                | Simple | Moderate | Challenging | Total |\\n|----------------|--------|----------|-------------|-------|\\n| ChatGPT        | 31.08  | 13.29    | 12.08       | 24.05 |\\n| ChatGPT + KG   | 45.44  | 26.14    | 19.01       | 37.22 |\\n| (VES) ChatGPT  | 36.20  | 15.43    | 14.42       | 27.97 |\\n| (VES) ChatGPT + KG | 54.71 | 28.16    | 22.80       | 43.81 |\\n\\nTable 4: The Execution Accuracy (EX) and Valid Efficiency Score (VES) are presented for both the ChatGPT model and its version with grounding (KG) for external knowledge evidence, taking into consideration development and testing datasets.\\n\\nB.2 Annotation Entrance\\n\\n**Annotation Platform and Compensation.** The data is collected from Alibaba-Appen\u00a7, an internal version. Each Question annotator receives a $0.6 reward for each validated question, while SQL annotators earn $1 per SQL contribution. We also invite text-to-SQL experts and professors to join to check and annotate external knowledge evidence without compensation. There are ~1340 SQLs confirmed per week.\\n\\n\u00a7[https://appen.com/crowd-2/#crowd](https://appen.com/crowd-2/#crowd)\"}"}
{"id": "dI4wzAE6uV", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The three full-time text-to-SQL experts in this project are: (1). A database research scientist who's published over 20 top DB conference papers (e.g., SIGMOD, VLDB). (2). A PhD student with research interests in text-to-SQL, who achieved state-of-the-art results on text-to-SQL open challenges. (3). A DBA engineer with more than 10 years of experience in text-to-SQL applications for both B2B and B2C businesses.\\n\\nWe hire a group of native speakers of English with degrees above the bachelor's level and database-related knowledge to ask a variety of natural language questions regarding the values of databases. To fulfill this objective, we have adopted the following procedure: (1). ER diagrams and database description files are documented to assist the annotators in understanding the databases; (2). we present the annotators with three databases from different domains and require them to generate 10 questions for each database; (3). these questions are then assessed by 3 text-to-SQL experts applying predefined rules. Those questions earning at least two votes are marked as valid. Only annotators capable of generating no less than 8 valid questions per database are preserved. As a result, 11 native speakers contribute questions to BIRD.\\n\\nWe assemble a team of skilled data engineers and database students. The team undergoes rigorous testing through the text-to-SQL evaluation process, which assesses their capability of generating SQL queries for a variety of questions facing different domains of databases. Each annotator is asked to answer 10 questions, and only those who score at least 9 out of 10 will be qualified to annotate SQL queries for BIRD.\\n\\n**B.3 Question Distribution**\\n\\nFigure 8 contains the detailed question types and their examples.\\n\\n**B.4 Experiment Details**\\n\\n**FT-based Models.** T5 is a strong and versatile pre-trained language model (PLM) for text-to-text generation that has achieved state-of-the-art performance in a variety of semantic parsing tasks, including text-to-SQL. We concatenate the question with serialized database schema as input [40, 49, 41]. And SQL can be fetched in an end-to-end fashion by easily fine-tuning. While seq2AST-based methods [43, 5] are also effective in text-to-SQL, actually their grammar rules utilized during decoding are constrained on specific datasets [25]. We implement our codes mainly based on the hugging-face transformers library [247]. We set the max input length as 1024, the generation max length as 512, and the batch size as 32. We also adopt Adafactor as our primary optimizer with a linear decayed learning rate of 5e-5. All experiments are conducted on one NVIDIA Tesla A100 80GB, which is available for most research centers. We set the random seed as 1 for all runs of FT-based models since 1 is an optimal seed proven by previous SOTA models [27, 49].\\n\\n**ICL-based Models.** Codex (code-davinci-002) and ChatGPT (gpt-3.5-turbo) are popular and powerful large-scale pre-trained language models (LLMs) for code generation driven by ICL. They can produce multiple types of codes, including SQL, from human instructions without additional training. We employ programming-based prompts, as described in [39], to collect results by calling the API. Also, we choose the Azure OpenAI API to align the codes with other variants of LLMs. Given that models are not allowed access to unseen databases and ground-truth SQLs in the evaluation set, a zero-shot generation strategy is the most appropriate. Moreover, to investigate the impact of multi-step reasoning of LLMs on BIRD, we implement the Chain-Of-Thought (COT) technique [48] by easily adding the prompt sentence \u201cLet\u2019s think step by step.\u201d before the generation of SQLs [21]. However, we find out the output of ChatGPT is too uncertain with many unexpected explanations, thus we provide a 1-shot pseudo example for ChatGPT to learn the procedure of thinking and output format. The detailed prompt design is shown in Figure 9. In order to minimize the randomness of results, we set the temperature as 0 to ensure reproduction.\\n\\n**Knowledge Fusion.** In the baseline implementation, we naively concatenate the knowledge evidence sentences with questions and database schemas, but we can observe a significant improvement [22].\"}"}
{"id": "dI4wzAE6uV", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\n\\n**Question**\\n\\nHow many gas stations in CZE has Premium gas?\\n\\n```sql\\nSELECT COUNT(GasStationID) FROM gasstations WHERE Country = 'CZE' AND Segment = 'Premium'\\n```\\n\\n**Sub Type**\\n\\nWhat are the titles of the top 5 posts with the highest popularity?\\n\\n```sql\\nSELECT Title FROM posts ORDER BY ViewCount DESC LIMIT 5\\n```\\n\\n**Comparison**\\n\\nHow many color cards with no borders have been ranked higher than 12000 on EDHRec?\\n\\n```sql\\nSELECT COUNT(id) FROM cards WHERE edhrecRank > 12000 AND borderColor = 'borderless'\\n```\\n\\n**Counting**\\n\\nHow many of the members' hometowns are from Maryland state?\\n\\n```sql\\nSELECT COUNT(T2.member_id) FROM zip_code AS T1 INNER JOIN member AS T2 ON T1.zip_code = T2.zip WHERE T1.state = 'Maryland'\\n```\\n\\n**Aggregation**\\n\\nName the ID and age of patient with two or more laboratory examinations which show their hematocrit level exceeded the normal range.\\n\\n```sql\\nSELECT T1.ID, STRFTIME('%Y', CURRENT_TIMESTAMP) - STRFTIME('%Y', T1.Birthday) FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID WHERE T1.ID IN (SELECT ID FROM Laboratory WHERE HCT > 52 GROUP BY ID HAVING COUNT(ID) >= 2)\\n```\\n\\n**Reasoning**\\n\\nWhat is the average height of the superheroes from Marvel Comics?\\n\\n```sql\\nSELECT AVG(T1.height_cm) FROM superhero AS T1 INNER JOIN publisher AS T2 ON T1.publisher_id = T2.id WHERE T2.publisher_name = 'Marvel Comics'\\n```\\n\\n**Domain Knowledge**\\n\\n```sql\\nSELECT T1.ID, STRFTIME('%Y', CURRENT_TIMESTAMP) - STRFTIME('%Y', T1.Birthday) FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID WHERE T1.ID IN (SELECT ID FROM Laboratory WHERE HCT > 52 GROUP BY ID HAVING COUNT(ID) >= 2)\\n```\\n\\n**Numeric Computation**\\n\\nAmong the posts with a score of over 20, what is the percentage of them being owned by an elder user?\\n\\n```sql\\nSELECT CAST(SUM(IIF(T2.Age > 65, 1, 0)) AS REAL) * 100 / count(T1.Id) FROM posts AS T1 INNER JOIN users AS T2 ON T1.OwnerUserId = T2.Id WHERE T1.Score > 20\\n```\\n\\n**Synonym**\\n\\nHow many clients opened their accounts in Jesenik branch were women (female)?\\n\\n```sql\\nSELECT COUNT(T1.client_id) FROM client AS T1 INNER JOIN district AS T2 ON T1.district_id = T2.district_id WHERE T1.gender = 'F' AND T2.A2 = 'Jesenik'\\n```\\n\\n**Value Illustration**\\n\\nAmong the weekly issuance accounts, how many have a loan of under 200000?\\n\\n```sql\\nSELECT COUNT(T1.account_id) FROM loan AS T1 INNER JOIN account AS T2 ON T1.account_id = T2.account_id WHERE T2.frequency = 'POPLATEK TYDNE' AND T1.amount < 200000\\n```\"}"}
{"id": "dI4wzAE6uV", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CREATE TABLE singer(\\n    singer_id         TEXT not null        primary key,\\n    nation       TEXT  not null,\\n    sname       TEXT null,\\n    dname       TEXT null,\\n    cname       TEXT null,\\n    age INTEGER not null,\\n    year INTEGER not null,\\n    birth_year  INTEGER          null,\\n    salary  REAL          null,\\n    city TEXT          null,\\n    phone_number   INTEGER          null,\\n    tax   REAL      null\\n)\\n\\n--\\n-- Using valid SQLite and understanding External Knowledge, answer the following questions for the tables provided above.\\n--\\n1. referring to external knowledge, we need to filter singers by year - birth_year > 27;\\n2. we should find out the singers of step 1 in which nation = 'US',\\n3. use COUNT() to count how many singers.\\nFinally the SQL is:\\nSELECT COUNT(*) FROM singer WHERE year - birth_year > 27;\"}"}
{"id": "dI4wzAE6uV", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ex1.1 Question:\\nList out the age of users who located in Vienna, Austria obtained the badge?\\n\\nChatGPT\\nSQL:\\nSELECT Age FROM users WHERE Location = 'Vienna, Austria'\\nAND Id IN (SELECT UserId FROM badges)\\n\\nOptimized SQL:\\n(time-saving percentage: 99.92%)\\nSELECT u.Age FROM users AS u INNER JOIN badges AS b ON u.Id = b.UserId WHERE u.Location = 'Vienna, Austria'\\n\\nTake Away:\\nBy applying a JOIN operation instead of a subquery with IN can improve efficiency, as the database may execute the JOIN and filtering processes concurrently in just one operation without the need to store the intermediate results to filter primary query.\\n\\nEx1.2 Question:\\nHow many of the members' hometowns are from Maryland state?\\n\\nChatGPT\\nSQL:\\nSELECT COUNT(*) FROM member INNER JOIN zip_code ON member.zip = zip_code.zip_code WHERE zip_code.state = 'Maryland'\\n\\nOptimized SQL:\\n(time-saving percentage: 67.93%)\\nSELECT COUNT(member.member_id) FROM member INNER JOIN zip_code ON member.zip = zip_code.zip_code WHERE zip_code.state = 'Maryland'\\n\\nTake Away:\\nUtilizing the COUNT function on a NOT NULL column, as opposed to COUNT(*), can increase time efficiency. This rewritten SQL enables the database to count NOT NULL values within a single column, rather than compute all rows including those with NULL values. Usually, the primary key column is selected as this NOT NULL column.\\n\\nEx1.3 Question:\\nWho is the owner of the account with the largest loan amount?\\n\\nChatGPT\\nSQL:\\nSELECT c.client_id FROM client c INNER JOIN disp d ON c.client_id = d.client_id INNER JOIN loan l ON d.account_id = l.account_id ORDER BY l.amount DESC LIMIT 1\\n\\nOptimized SQL:\\n(time-saving percentage: 62.39%)\\nSELECT c.client_id FROM client c INNER JOIN disp d ON c.client_id = d.client_id INNER JOIN loan l ON d.account_id = l.account_id WHERE l.amount = (SELECT MAX(amount) FROM loan)\\n\\nTake Away:\\nIn an unindexed environment, employing the MAX function can potentially yield faster results since it avoids the need for sorting, which could run against a large table. Adding indexes to databases can significantly increase the speed of SQL queries because it creates a data structure that enables the database engine to quickly locate rows that match specific criteria instead of scanning the entire table.\\n\\nEx2.1 Question:\\nHow many accounts are there in the district of \\\"Pisek\\\"?\\n\\nChatGPT\\nSQL:\\nSELECT COUNT(*) FROM account a INNER JOIN district d ON a.district_id = d.district_id WHERE d.A2 = 'Pisek'\\n\\nAdded Indexes:\\n(time-saving percentage: 87.27%)\\nCREATE INDEX account_district_id_index ON account (district_id);\\nCREATE UNIQUE INDEX district_district_id_uindex ON district (district_id);\\n\\nTake Away:\\nAdding indexes into a database can significantly increase the speed of SQL queries because it creates a data structure that enables the database engine to quickly locate rows that match specific criteria instead of scanning the entire table.\"}"}
{"id": "dI4wzAE6uV", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: List the names of schools with more than 30 differences in enrollments between K-12 and ages 5-17. Please also give the full street address of the schools.\\n\\nEvidence: \\\\[ \\\\text{Difference in enrollment} = \\\\text{Enrollment (K-12)} - \\\\text{Enrollment (Ages 5-17)} \\\\]\\n\\nGround Truth: \\\\[\\n\\\\text{SELECT T1.School, T1.StreetAbr FROM schools AS T1 INNER JOIN frpm AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.}`Enrollment (K-12)` - T2.}`Enrollment (Ages 5-17)` > 30\\n\\\\]\\n\\nChatGPT SQL: \\\\[\\n\\\\text{SELECT s.School, s.Street, s.City, s.Zip FROM schools s JOIN frpm f ON s.CDSCode = f.CDSCode WHERE f.}`Enrollment (K-12)` - f.}`Enrollment (Ages 5-17)` > 30\\n\\\\]\"}"}
{"id": "dI4wzAE6uV", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A public domain license refers to a legal designation that allows intellectual property, such as creative works or inventions, to be freely used, shared, and built upon by anyone without restrictions. When a work is in the public domain, it is no longer protected by copyright, patent, or trademark laws.\\n\\n- **CC-BY**: Creative Commons Attribution 4.0 International license allows users to share and adapt the dataset so long as they give credit to the creator.\\n\\n- **CC-BY-SA**: Creative Commons Attribution-ShareAlike 4.0 International license allows users to share and adapt the dataset so long as they give credit to the creator and distribute any additions, transformations, or changes to the dataset under this license.\\n\\n- **GPL**: General Public License created by the Free Software Foundation (FSF) and also known as the GNU GPL, allows users to use, study, share, and modify the software under certain terms and conditions.\\n\\n- **CPOL**: Code Project Open License is a software license that is often used for articles, tutorials, and sample code shared on The Code Project website. It is intended to be a more permissive license, allowing developers to use, modify, and distribute the software without many of the restrictions imposed by other licenses like the GPL.\\n\\n- **CC0**: Creative Commons Zero is a public domain dedication tool created by Creative Commons. It allows creators to waive all their copyright and related rights in a work, effectively placing it in the public domain. This means that anyone can freely use, share, modify, and build upon the work without seeking permission or providing attribution to the original creator.\\n\\n**B.11 SQL Function Taxonomy**\\n\\nSQL functions in BIRD mentioned in Table 1 span across multiple categories including:\\n\\n- **Window Functions**: i.e., `OVER()`\\n- **Date Functions**: i.e., `JULIANDAY()`\\n- **Conversion Functions**: i.e., `CAST()`\\n- **Math Functions**: i.e., `ROUND()`\\n- **String Functions**: i.e., `SUBSTR()`\\n\\n**B.12 Keyword Statistic**\\n\\nWe have conducted a comprehensive analysis of the keywords employed in the BIRD dataset and visualize the results in the form of a nice-looking word cloud, which can be found in Figure 12. We further classify keywords into 7 following categories:\\n\\n**Main Body Keywords**\\n\\n- `SELECT`\\n- `FROM`\\n- `WHERE`\\n- `AND`\\n- `OR`\\n- `NOT`\\n- `IN`\\n- `EXISTS`\\n- `IS`\\n- `NULL`\\n- `IIF`\\n- `CASE`\\n- `CASE WHEN`\\n\\n**Join Keywords**\\n\\n- `INNER JOIN`\\n- `LEFT JOIN`\\n- `ON`\\n- `AS`\\n\\n**Clause Keywords**\\n\\n- `BETWEEN`\\n- `LIKE`\\n- `LIMIT`\\n- `ORDER BY`\\n- `ASC`\\n- `DESC`\\n- `GROUP BY`\\n- `HAVING`\\n- `UNION`\\n- `ALL`\\n- `EXCEPT`\\n- `PARTITION BY`\"}"}
{"id": "dI4wzAE6uV", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 12: Keyword cloud presentation for SQLs in BIRD.\\n\\nAggregation Keywords\\n- AVG\\n- COUNT\\n- MAX\\n- MIN\\n- ROUND\\n- SUM\\n\\nScalar Keywords\\n- ABS\\n- LENGTH\\n- STRFTIME\\n- JULIADAY\\n- NOW\\n- CAST\\n- SUBSTR\\n- INSTR\\n\\nComparison Keywords\\n- =\\n- >\\n- <\\n- >=\\n- <=\\n- !=\\n\\nComputing Keywords\\n- -\\n- +\\n- *\\n- /\\n\\nB.13 Study about Text-to-SQL Models\\nThe fundamental principle of a cross-domain text-to-SQL parser involves the construction of an encoder to learn representations of questions and schemas, followed by a decoder to generate SQLs [37]. For example, IRNET [12] designs an encoder consisting of attention-based Bi-LSTM for learning question and schema representations, and a decoder to predict SQLs based on the encoded intermediate representations. RATSQL [43], SDSQL [17], LGESQL [5], and S2SQL [18], Proton [44] enhance the representation learning of natural language questions and database schema via relational graph neural network. R2SQL [16], SCORER [54], and STAR [4] enhance contextual learning for conversational text-to-SQL tasks. Later, sequence-to-sequence pre-trained language models (PLMs) such as T5 [38] become popular in text-to-SQL tasks due to their portability and capability of generation across different datasets. These models achieve impressive results by fine-tuning with minimal effort. Furthermore, RASAT [36] enhances T5's structural information encoding via schema alignment into the encoder, while Graphix [27] equips T5 with multi-hop reasoning to achieve state-of-the-art results on complicated cross-domain text-to-SQL tasks. In recent years, LLMs such as ChatGPT [33], Palm [8], OPT [56], have attracted considerable attention due to their powerful zero-shot reasoning and domain generalization capabilities. ChatGPT can perform exceptionally well on semantic parsing tasks, including text-to-SQL tasks, with minimal input data. In fact, in the BIRD project, ChatGPT even performs more impressively than initially expected.\\n\\nStudy about SQL Efficiency\\nEfficient execution of SQL queries on big databases has been a significant topic in both academia and industries. Many techniques are proposed to improve SQL query efficiency, by index selection [22], SQL optimization [26, 61], etc. SQL optimization is a common method for enhancing the efficiency of SQL queries. Several SQL optimization algorithms [28, 30, 47], such as rule-based optimization and cost-based optimization, are proven effective. Rule-based optimization employs a set of principles to transform the SQL query into a form that can be executed more efficiently. On the other hand, cost-based optimization estimates the execution cost of various query plans and selects the one with the lowest cost by analyzing the statistic distribution of database values. Similar to the NLP community, there are also recent works utilizing artificial intelligence for query optimization such as [61]. Index prediction is another important technique for improving SQL execution efficiency. Researchers propose many algorithms of index prediction [60] based on various optimization criteria, such as minimizing SQL execution time, and maximizing index utilization. In this work, we provide VES to measure the efficiency of text-to-SQL generators to encourage them to generate accurate and fast SQLs for users.\"}"}
{"id": "dI4wzAE6uV", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Can LLM Already Serve as A Database Interface?\\n\\nA Big Bench for Large-Scale Database Grounded Text-to-SQLs\\n\\nJinyang Li\\\\(^1\\\\), \\\\(^\\\\dagger\\\\), Binyuan Hui\\\\(^2\\\\), Ge Qu\\\\(^1\\\\), Jiaxi Yang\\\\(^2\\\\), Binhua Li\\\\(^2\\\\), Bowen Li\\\\(^6\\\\), Bailin Wang\\\\(^5\\\\), Bowen Qin\\\\(^2\\\\), Ruiying Geng\\\\(^2\\\\), Nan Huo\\\\(^1\\\\), Xuanhe Zhou\\\\(^3\\\\), Chenhao Ma\\\\(^6\\\\), Guoliang Li\\\\(^3\\\\), Kevin C.C. Chang\\\\(^7\\\\), Fei Huang\\\\(^2\\\\), Reynold Cheng\\\\(^1\\\\), Yongbin Li\\\\(^2\\\\)\\n\\n\\\\(^1\\\\)The University of Hong Kong\\n\\\\(^2\\\\)DAMO Academy, Alibaba Group\\n\\\\(^3\\\\)Tsinghua University\\n\\\\(^4\\\\)Shanghai AI Laboratory\\n\\\\(^5\\\\)MIT CSAIL\\n\\\\(^6\\\\)The Chinese University of Hong Kong, Shenzhen\\n\\\\(^7\\\\)University of Illinois at Urbana-Champaign\\n\\n\\\\{jl0725,quge\\\\}@connect.hku.hk, ckcheng@cs.hku.hk, binyuan.hby@alibaba-inc.com\\n\\nAbstract\\n\\nText-to-SQL parsing, which aims at converting natural language questions into executable SQLs, has gained increasing attention in recent years. In particular, GPT-4 and Claude-2 have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database values leaving the gap between academic study and real-world applications. To mitigate this gap, we present Bird, a Big Bench for large-scale Database grounded in text-to-SQL tasks, containing 12,751 text-to-SQL pairs and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty and noisy database values, external knowledge grounding between NL questions and database values, and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. The experimental results demonstrate the significance of database values in generating accurate text-to-SQLs for big databases. Furthermore, even the most effective text-to-SQL models, i.e. GPT-4, only achieve 54.89% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand. We also provide an efficiency analysis to offer insights into generating text-to-efficient-SQLs that are beneficial to industries. We believe that Bird will contribute to advancing real-world applications of text-to-SQL research. The leaderboard and source code are available: https://bird-bench.github.io/.\\n\\n1 Introduction\\n\\nText-to-SQL parsing [55, 50, 51, 3, 52, 37], which focuses on converting natural language questions into SQL queries, has attracted significant research interests from both academia and industry. This attention stems from its potential to empower non-expert data analysts in automatically extracting desired information from ubiquitous relational databases using natural language. Recent advances in neural models, including those based on large language models (LLMs), have led to an impressive performance on existing benchmarks such as Spider [53] and WikiSQL [58]. For instance, the execution accuracy of the top-performing model in the Spider leaderboard has increased from 53.5%...\"}"}
{"id": "dI4wzAE6uV", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is the winning rate of... account.frequency = 'POPLATEK TYDNE';\\n\\nExternal Knowledge Reasoning\\n\\nLarge and Realistic Database Values\\n\\nSQL Execution Efficiency\\n\\nWhat is the average salary of the worst performing managers?\\n\\nHow\\n\\nSELECT\\n\\n= 'Owner'\\n\\nAVG\\n\\naccount.\\n\\nWe evaluate the performance of state-of-the-art text-to-SQL parsers using two popular methodologies:\\n\\n- Valid Efficiency Score (VES) to evaluate the efficiency of generated SQLs.\\n- To the best of our knowledge, B\\ndatabases for a hidden test set. Given these databases, we rely on crowdsourcing to collect natural\\nplatforms (Kaggle, Relation.vit). To further avoid data leakage, we curated 15 additional relational\\ndevelopment, we collected and modified 80 open-source relational databases from real analysis\\n\\n95 big databases with a total size of\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n"}
{"id": "dI4wzAE6uV", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: An Overview of the BIRD Annotation Workflow in (a). This figure depicts a four-step procedure. (1) The workflow begins with specialists assembling and producing databases and description files. (2) Experts then teach and evaluate crowdsourcing people, keeping only those who pass the evaluation. (3) Question annotators create a corpus of questions using databases and their corresponding description files. (4) SQL annotators produce SQL files, equipped with databases, descriptions, and questions. (b) and (c) also depict the Double-blind annotation procedure and an example of database descriptions.\\n\\nwhere \\\\( C \\\\) and \\\\( T \\\\) are columns and tables respectively. When dealing with complex database values, such as BIRD, it is crucial to incorporate external knowledge evidence, denoted as \\\\( K \\\\), to improve the models' understanding of database values. Finally, the text-to-SQL could be formulated as:\\n\\n\\\\[\\nY = f(Q, D, K | \\\\theta), (1)\\n\\\\]\\n\\nwhere the function \\\\( f(\\\\cdot | \\\\theta) \\\\) can represent a model or neural network with the parameter \\\\( \\\\theta \\\\).\\n\\n3 Dataset Construction\\n\\n3.1 Annotation Entrance\\n\\nTo deliver a high-quality benchmark, we administer thorough exams to all applicants and only hire those who pass these rigorous tests. Further information is available in the Appendix B.2.\\n\\n3.2 Database Source\\n\\nIt is difficult to collect databases with complex schemas and sufficient value due to privacy protection. Earlier works \\\\cite{45, 53} choose to self-design database schemas and value production. Nonetheless, the value distribution and schemas may differ from real-world scenarios in this way. In our work, we obtain and process databases from three different sources to enrich real-world attributes. 32% of our databases are sourced from Kaggle*, a platform renowned for holding data science competitions with difficult, noisy values and schemas. Another 48% come from CTU Prague Relational Learning Repository\u2020, an open platform for machine learning research with multi-relational data. The remaining 20% are built by acquiring open tables, synthesizing and standardizing schemas, and generating database constraints. All of these databases contain real and large value distributions and are easily accessible with the appropriate licenses. Finally, we present 95 databases consisting of 69, 11, and 15 databases for training, development, and testing respectively. Our databases cover 37 professional domains, including blockchain, sports, health care, politics, etc. We anticipate that it will be a significant resource for researchers to explore domain generalization in semantic parsing tasks with large database values.\\n\\n* [https://www.kaggle.com/](https://www.kaggle.com/)\\n\u2020 [https://relational.fit.cvut.cz/](https://relational.fit.cvut.cz/)\"}"}
{"id": "dI4wzAE6uV", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.3 Question Annotation\\n\\nDatabase Description File.\\n\\nThe Database Description File is a crucial resource designed to aid annotators in comprehending database values, thereby allowing them to ask insightful questions. It offers two primary pieces of information regarding the database:\\n\\n1. **Full schema names:** Database table and column names are frequently abbreviated, which are difficult to understand.\\n2. **Value description:** This aspect is particularly useful when phrases or tokens in a question do not directly match values in the database.\\n\\nExternal Knowledge Evidence.\\n\\nIn our study of professional data analysis, we find that external knowledge evidence is required to map the natural language instructions into counterpart database values. Therefore, we collect and classify such evidence into four categories:\\n\\n1. **Numeric Reasoning Knowledge:** This category refers to the mathematical computation required for certain SQL operations. In our benchmark, we present 8 basic math operations, including 4 complex operations as MINUS, ADDITION, DIVISION, MULTIPLY. BIRD also contains compositional operations over basic ones, such as percentages, formulas, etc.\\n2. **Domain Knowledge:** This category consists of domain-specific knowledge that is utilized to generate SQL operations [10, 57]. For instance, a business analyst in the banking business requires knowledge of financial indicators such as return on investment and net income in order to generate effective SQL queries.\\n3. **Synonym Knowledge:** This category includes words or expressions that have the same or similar meanings regardless of how they are phrased differently [11].\\n4. **Value Illustration:** This category refers to detailed descriptions of database values, including value types, value categories, and the mapping combinations of columns and values that correspond to entities, for example: \u201ccenter\u201d can be represented by \u201cpos = C\u201d in the database professional_basketball.\\n\\n3.4 SQL Annotation\\n\\nDouble-Blind Annotation.\\n\\nAs shown in Figure 2 (b), we employ a double-blind approach [42] for SQL annotation. This approach involves two independent SQL annotators who generate SQLs for the same question without discussion. The annotated SQLs are executed in databases, and those yielding identical results are gathered. Otherwise, the SQLs are checked with experts until a consensus is reached. Double-blind procedures can dramatically reduce the SQL annotation error rate, as there is a small probability for two skillful annotators to generate the same incorrect results when databases have large values. The more semantic-equivalent and efficient SQL selected by experts for each question is picked as ground truth SQL in BIRD, and the external knowledge evidence sentences are recorded for each SQL if utilized.\\n\\nExamination.\\n\\nExperts evaluate each text-to-SQL pair to ensure the highest quality of data. The evaluation process includes two dimensions: SQL validness, and text-knowledge-SQL alignment. Firstly, the SQL validness will be confirmed that each SQL is executable and can return a valid result from the database. The \u201cvalid result\u201d refers to the set of results that is not \u201cNULL\u201d. If the executed result set is \u201cNULL\u201d, experts will make slight changes to the conditions of the questions until the associated SQLs can provide a valid result set. Secondly, text-knowledge-SQL alignment is involved to ensure that each SQL can be generated with the given texts and knowledge evidence. If the evidence is insufficient to generate the SQL or contains errors, experts will be in charge of correcting them.\\n\\n4 Data Statistics\\n\\nOverall Statistics\\n\\nTable 1 presents an overview comparison between BIRD and other cross-domain text-to-SQL benchmarks. As the statistics demonstrate, BIRD is a large-scale cross-domain benchmark, covering complex SQL functions, knowledge reasoning, and efficiency evaluation.\\n\\nQuestion Statistics\\n\\nDatabase values bring more challenges in text-to-SQLs. In order to underscore this, we classify questions into two macro-categories: Fundamental Type and Reasoning Type, and each contains 4-5 micro-categories in detail. The Fundamental Type of questions refers to those that can be answered without database value comprehension. It contains Match-based (83.9%), Ranking (20.3%), Comparison (16.7%), Counting (30.4%), Aggregation (15.7%).\"}"}
{"id": "dI4wzAE6uV", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Domain Distribution**\\n\\n| Database                          | Size  | Na.  | Size  | Na.  | Size  |\\n|----------------------------------|-------|------|-------|------|-------|\\n| Pets                             | 26,521| 569  | 10,181| 186  | 2K    |\\n| Books                            | 80,654| 1,590| 8,779 | 170  | 57    |\\n| Health                           | 4,472 | 101  | 727   | 14.1 | 2K    |\\n| Movies                           | 13,837| 322  | 2,944 | 60.3 | 100K  |\\n| Games                            | 7,939 | 185  | 1,974 | 37.3 | 1K    |\\n| Music                            | 4,178 | 100  | 1,105 | 31.9 | 5K    |\\n| Food                             | 4,178 | 100  | 1,105 | 31.9 | 5K    |\\n| Sports                           | 3,364 | 89   | 578   | 11.9 | 5K    |\\n| Transportation                   | 2,869 | 72   | 474   | 9.8  | 5K    |\\n| University                       | 2,869 | 72   | 474   | 9.8  | 5K    |\\n| Geography                        | 2,348 | 58   | 319   | 6.6  | 5K    |\\n| Cartoons                         | 1,590 | 39   | 319   | 6.6  | 5K    |\\n| IT                               | 1,590 | 39   | 319   | 6.6  | 5K    |\\n| Movies                           | 1,590 | 39   | 319   | 6.6  | 5K    |\\n| Books                            | 1,590 | 39   | 319   | 6.6  | 5K    |\\n| Superstore                       | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192   | 3.9  | 5K    |\\n| Restaurants                      | 1,288 | 32   | 192  "}
{"id": "dI4wzAE6uV", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: A comparative statistical analysis of SQL queries in the BIRD dataset and other cross-domain text-to-SQL benchmarks.\\n\\nExecution Accuracy (EX)\\n\\nEX is defined as the proportion of examples in the evaluation set for which the executed results of both the predicted and ground-truth SQLs are identical, relative to the overall number of SQLs [37]. Considering the result set as $V_n$ executed by the $n$th ground-truth SQL $Y_n$, and the result set $\\\\hat{V}_n$ executed by the predicted SQL $\\\\hat{Y}_n$, EX can be computed by:\\n\\n$$EX = \\\\frac{\\\\sum_{n=1}^{N} 1(V_n, \\\\hat{V}_n)}{N},$$\\n\\n(2)\\n\\nwhere $1(\\\\cdot)$ is an indicator function, which can be represented as:\\n\\n$$1(V, \\\\hat{V}) = \\\\begin{cases} 1, & V = \\\\hat{V} \\\\\\\\ 0, & V \\\\neq \\\\hat{V} \\\\end{cases}$$\\n\\n(3)\\n\\nValid Efficiency Score (VES)\\n\\nVES is designed to measure the efficiency of valid SQLs generated by models. It is worth noting that the term \\\"valid SQLs\\\" refers to predicted SQL queries whose result sets align with those of the ground-truth SQLs. Any SQL queries that fail to fetch the correct values will be declared invalid since they are totally useless if they cannot fulfill the user requests, regardless of their efficiency. In this case, the VES metric considers both the efficiency and accuracy of execution results, providing a comprehensive evaluation of a model's performance. Formally, the VES can be expressed as:\\n\\n$$VES = \\\\sum_{n=1}^{N} 1(V_n, \\\\hat{V}_n) \\\\cdot R(Y_n, \\\\hat{Y}_n)\\\\cdot E(Y_n, \\\\hat{Y}_n),$$\\n\\n(4)\\n\\nwhere $R(\\\\cdot)$ denotes the relative execution efficiency of predicted SQL in comparison to ground-truth SQL, allowing for machine status-related uncertainty. $E(\\\\cdot)$ is a function to measure the absolute execution efficiency for each SQL in a given environment.\u2021 Furthermore, we incorporate the square root function to minimize random instances that are abnormally faster or slower than the ground-truth SQLs. Here, efficiency can refer to running time, throughput, memory cost, or merged metrics. In BIRD, we consider the running time mainly at this time. Appendix B.8 provides a detailed description of the VES.\\n\\n6 Experiments\\n\\n6.1 Baseline Models\\n\\nWe present the performance of two types of baseline models in BIRD. The first type of model is based on fine-tuning (FT) techniques, which outputs SQL by tuning all parameters of language models to learn the annotated train set. On the other hand, the second type of model based on\\n\\n\u2021In BIRD evaluation, we run 100 times for each SQL in the same CPU and evaluate average results after dropping the outliers.\"}"}
{"id": "dI4wzAE6uV", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: A bar chart provides a clear visualization of the performance of advanced models on BIRD.\\n\\nTable 2: The Execution Accuracy (EX) of advanced text-to-SQL models in BIRD. The human performance is also provided.\\n\\n| Models Development | Data          | Testing Data       |\\n|--------------------|---------------|--------------------|\\n|                    | w/o knowledge| w/ knowledge       |\\n| FT-based           |               |                    |\\n| T5-Base            | 6.32          | 11.54 (+5.22)      |\\n|                    | 7.06          | 12.89 (+5.83)      |\\n| T5-Large           | 9.71          | 19.75 (+10.04)     |\\n|                    | 10.38         | 20.94 (+10.56)     |\\n| T5-3B              | 10.37         | 23.34 (+12.97)     |\\n|                    | 11.17         | 24.05 (+12.88)     |\\n| ICL-based          |               |                    |\\n| Palm-2             | 18.77         | 27.38 (+8.61)      |\\n|                    | 24.71         | 33.04 (+8.33)      |\\n| Codex              | 25.42         | 34.35 (+8.93)      |\\n|                    | 24.86         | 36.47 (+11.61)     |\\n| ChatGPT            | 24.05         | 37.22 (+13.17)     |\\n|                    | 26.77         | 39.30 (+12.53)     |\\n| ChatGPT + COT      | 25.88         | 36.64 (+10.76)     |\\n|                    | 28.95         | 40.08 (+11.24)     |\\n| Claude-2           | 28.29         | 42.70 (+14.41)     |\\n|                    | 34.60         | 49.02 (+14.42)     |\\n| GPT-4              | 30.90         | 46.35 (+15.45)     |\\n|                    | 34.88         | 54.89 (+20.01)     |\\n| GPT-4 + DIN-SQL    | -             | 55.90 (-)          |\\n| Human Performance  | -             | 72.37 (+20.59)     |\\n\\nIn-context learning (ICL), can generate results without additional training. In FT models, we select T5 family [38] as the main baseline models. For ICL-based models, we provide zero-shot results of Codex (code-davinci-002), ChatGPT (gpt-3.5-turbo), GPT-4 (gpt-4-32k), Claude-2 (claude-2.0), Palm-2 (text-bison-001). Additionally, we also implement a state-of-the-art (SOTA) model of SPIDER, DIN-SQL [35], to evaluate the challenges proposed by the BIRD dataset.\\n\\nTable 2, Table 3 and Figure 5 present the overall results of advanced language models on BIRD.\\n\\n6.2 Execution Accuracy Analysis\\n\\nTable 2 and Figure 5 presents stratified performances of various models in BIRD. GPT-4 surpasses all baseline language models. Claude-2 closely follows, demonstrating outstanding abilities in semantic parsing and knowledge reasoning. Further, the incorporation of a dedicated reasoning prompt by [35], enables DIN-SQL + GPT-4 to achieve a new state-of-the-art result on BIRD. It contains value sampling, few-shot demonstrations, and self-correction. Despite considerable advancements in Language Model Learning (LLMs) and prompt intelligence, the performance of these models lags obviously behind human capabilities. Not only does this gap highlight the complex nature of BIRD, but it also presents opportunities for uncovering more capable models or advanced reasoning prompt methods applicable to real-world text-to-SQL scenarios.\"}"}
{"id": "dI4wzAE6uV", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Models Development Data Testing Data |\\n|-------------------------------------|\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\\n|                                     |\"}"}
{"id": "dI4wzAE6uV", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: The fine-grained categorical evaluation of advanced large language models on BIRD.\\n\\nbe reached at 87.3% by configuring indexes within the database. The detailed efficiency analysis is presented in Appendix B.5.\\n\\n6.5 Knowledge Evidence Analysis\\n\\nWe implement each baseline model for both two scenarios. The first is NOT to provide the ground truth external knowledge evidence sentence (w/o knowledge) for each sample. The other testing bed is to provide such evidence (w/ knowledge) and make text-to-SQL models do knowledge grounding by themselves. As we discuss in Section 3.3, expert annotations on external knowledge evidence sentences are employed to enhance the model's comprehension of database values. After being easily fed with the external knowledge evidence about the database values, all models have a clear improvement across the different difficulty levels as shown in Table 2 and Table 4. This indicates that external knowledge evidence in BIRD is effective and instructive for models to better understand the database values. Also it illustrates that the database values are very important to text-to-SQL models when facing more real databases. Besides, ICL-based approaches have a better self-knowledge grounding capability and pre-trained SQL knowledge than FT smaller models with less than 5B parameters. Equipped with COT, ChatGPT can perform better, since multi-step reasoning is beneficial when the knowledge and data are low-resource. Despite this, we observe a decline or limited improvements in performance for ChatGPT + external knowledge evidence for COT version. We hypothesize that the internal multi-step knowledge reasoning of LLMs is not compatible with the way of external knowledge (evidence) in this situation. Therefore, the development of methods that effectively combine the strong multi-step self-reasoning capabilities of LLMs with external knowledge reasoning coherently presents a promising future direction [29].\\n\\n6.6 More Analysis\\n\\nFine-grained Category Analysis. Figure 7 provides a detailed comparison of various dimensions of sub-capabilities of advanced LLMs on BIRD. The results indicate that GPT-4 exhibits superior performance against ChatGPT and Claude-2 in all areas. Nevertheless, there is a notable disparity in the performance of ranking and numerical computing (math) among all the models. This limitation may suggest the inadequacy of contemporary LLMs for deep data science tasks because such tasks always incorporate mathematical computations and rankings within the context of vague user queries. Conversely, these models demonstrate relatively better performance in domain knowledge, synonym detection, and value illustration, which can be attributed to their adequate linguistic training and reasoning capabilities during the pretraining phases.\\n\\nHuman Performance. In order to activate the efforts of text-to-SQL studies to achieve an application-level performance in real-world scenarios, we provide human performance in BIRD. Table 2, Table 3 shows that there's still a huge gap between even SOTA text-to-SQL models and human performance. The thorough introduction of procedures is in Appendix B.9.\\n\\nError Analysis. ChatGPT is currently the most prevalent and cost-efficient LLM. Therefore, the performance of ChatGPT is concentrated in this error analysis. The detailed analysis is in Appendix B.6. We observe 500 randomly sampled error cases, providing an in-depth assessment in the following...\"}"}
{"id": "dI4wzAE6uV", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Wrong Schema Linking (41.6%) pertains to the scenario where ChatGPT can accurately comprehend the structure of the database but erroneously associates it with inappropriate columns and tables. This demonstrates that the task of schema linking, even in intricate and practical situations, continues to be a significant obstacle for models.\\n\\nMisunderstanding Database Content (40.8%) occurs when ChatGPT either fails to recall the correct database structure (e.g., rtype doesn't belong to the satscores table) or generates fake schema items (e.g., lap_records is not appearing in the formula_1 database and many values are predicted incorrectly) especially when the database is very large. In this case, how to make ChatGPT really understand database structure and values is still a pain point topic in LLMs.\\n\\nMisunderstanding Knowledge Evidence (17.6%) refers to cases in which the model does not accurately interpret human-annotated evidence. An instance is that ChatGPT directly copies the formula \\\\( \\\\text{DIVIDE(\\\\text{SUM(spent)}, \\\\text{COUNT(spent)})} \\\\). This finding demonstrates that ChatGPT exhibits a lack of robustness in response to unfamiliar prompts or knowledge, causing it to directly replicate formulas without considering SQL syntax.\\n\\nWe also observe that ChatGPT occasionally employs incorrect keywords (e.g., misusing the MySQL Year() function instead of an SQLite function STRFTIME()) or exhibits decoding errors.\\n\\nRelated Work\\n\\nHigh-quality datasets are crucial for advancing various natural language processing tasks, including text-to-SQL. Early single-domain text-to-SQL datasets like GeoQuery [55], ATIS [9], and Restaurant [20] targeted specific information retrieval tasks, while more recent datasets such as WikiSQL [58] and SPIDER [53] propose cross-domain dataset to require domain generalization. However, most cross-domain text-to-SQL datasets still emphasize database schema rather than values, diverging from real-world scenarios. KaggleDBQA [24] addressed this by constructing 272 text-to-SQL pairs from eight databases on Kaggle, while other datasets like EHRSQL [25], SEDE [13], and MIMICSQL [46] collected diverse, large-value databases with more professional SQL queries.\\n\\nDespite these advancements, these datasets remain single-domain focused. Recent work has explored knowledge-intensive text-to-SQL benchmarks [10, 57], aiding experts in real-world analysis through knowledge grounding. BIRD is the first large-scale benchmark to incorporate these real-world features, emphasizing database values.\\n\\nLimitation and Future work\\n\\nDespite the high quality of SQL annotation produced by double-blind annotation, the procedure is resource-intensive. Future research could explore a human-computer interaction (HCI) based approach, incorporating advanced AI systems such as GPT-4 for taking parts of annotation duties, to maintain data quality while reducing human effort. In addition, SQLite was chosen as the primary SQL codebase for previous text-to-SQL benchmarks and this study since it's friendly to users. While it presents difficulties in fetching Query Execution Plans (QEP) for precise efficiency computation and adapting to different SQL syntaxes. Future work will include PostgreSQL and MySQL versions of BIRD to resolve these limitations and provide a more robust research environment for both NLP and DB experts.\\n\\nConclusion\\n\\nIn this paper, we introduce BIRD, a large-scale cross-domain, text-to-SQL benchmark with a particular focus on large database values. BIRD mitigates the gap between text-to-SQL research and real-world applications by exploring three additional challenges: 1) handling large and dirty database values, 2) external knowledge evidence, and 3) optimizing SQL execution efficiency. Our experimental results demonstrate that BIRD presents a more daunting challenge compared to existing benchmarks since even the most popular and powerful LLM, ChatGPT, falls significantly short of human performance. This leaves plenty of room for improvement and innovation in the text-to-SQL tasks. Moreover, our thorough efficiency and error analyses provide valuable insights and directions for future research, paving the way for the development of more advanced and practical text-to-SQL solutions in real-world scenarios.\"}"}
{"id": "dI4wzAE6uV", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgement\\n\\nWe thank all constructive comments from anonymous reviewers. Reynold Cheng, Jinyang Li, Ge Qu and Nan Huo were supported by the Hong Kong Jockey Club Charities Trust (Project 260920140) and the University of Hong Kong (Project 104006830). Chenhao Ma was supported by NSFC under Grant 62302421, Basic and Applied Basic Research Fund in Guangdong Province under Grant 2023A1515011280, Shenzhen Science and Technology Program ZDSYS20211021111415025. Jinyang Li and Ge Qu were supported by HKU Presidential PhD Scholar Programme. Ge Qu was also funded by Hong Kong PhD Fellowship Scheme. This work was supported by Alibaba Group through Alibaba Research Intern Program.\\n\\nReferences\\n\\n[1] Peter Alsberg. Space and time savings through large data base compression and dynamic restructuring. *Proceedings of the IEEE*, 63:1114\u20131122, 1975.\\n\\n[2] Anthropic. Introducing Claude. 2023. URL https://www.anthropic.com/index/introducing-claude.\\n\\n[3] Ruichu Cai, Boyan Xu, Zhenjie Zhang, Xiaoyan Yang, Zijian Li, and Zhihao Liang. An encoder-decoder framework translating natural language to database queries. In *Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)*, page 3977\u20133983, 2018.\\n\\n[4] Zefeng Cai, Xiangyu Li, Binyuan Hui, Min Yang, Bowen Li, Binhua Li, Zheng Cao, Weijie Li, Fei Huang, Luo Si, and Yongbin Li. STAR: SQL guided pre-training for context-dependent text-to-SQL parsing. In *Findings of the Association for Computational Linguistics: EMNLP 2022*, pages 1235\u20131247, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\\n\\n[5] Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, and Kai Yu. LGESQL: Line graph enhanced text-to-SQL model with mixed local and non-local relations. In *Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, pages 2541\u20132555, Online, August 2021. Association for Computational Linguistics.\\n\\n[6] Shuaichen Chang, Jun Wang, Mingwen Dong, Lin Pan, Henghui Zhu, Alexander Hanbo Li, Wuwei Lan, Sheng Zhang, Jiarong Jiang, Joseph Lilien, Steve Ash, William Yang Wang, Zhiguo Wang, Vittorio Castelli, Patrick Ng, and Bing Xiang. Dr.spider: A diagnostic evaluation benchmark towards text-to-SQL robustness. In *The Eleventh International Conference on Learning Representations*, 2023.\\n\\n[7] Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang Wang. FinQA: A dataset of numerical reasoning over financial data. In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages 3697\u20133711, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\\n\\n[8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. *ArXiv*, abs/2204.02311, 2022.\"}"}
{"id": "dI4wzAE6uV", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deborah A. Dahl, Madeleine Bates, Michael Brown, William Fisher, Kate Hunicke-Smith, David Pallett, Christine Pao, Alexander Rudnicky, and Elizabeth Shriberg. Expanding the scope of the ATIS task: The ATIS-3 corpus. In Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994, 1994.\\n\\nLongxu Dou, Yan Gao, Xuqi Liu, Mingyang Pan, Dingzirui Wang, Wanxiang Che, Dechen Zhan, Min-Yen Kan, and Jian-Guang Lou. Towards knowledge-intensive text-to-SQL semantic parsing with formulaic knowledge. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5240\u20135253, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\\n\\nYujian Gan, Xinyun Chen, Qiuping Huang, Matthew Purver, John R. Woodward, Jinxia Xie, and Pengsheng Huang. Towards robustness of text-to-sql models against synonym substitution. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 2505\u20132515. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.195.\\n\\nJiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-Guang Lou, Ting Liu, and Dongmei Zhang. Towards complex text-to-SQL in cross-domain database with intermediate representation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4524\u20134535, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1444.\\n\\nMoshe Hazoom, Vibhor Malik, and Ben Bogin. Text-to-SQL in the wild: A naturally-occurring dataset based on stack exchange data. In Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021), pages 77\u201387, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.nlp4prog-1.9.\\n\\nJoseph M. Hellerstein. Quantitative data cleaning for large databases. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, page 1197\u20131200, 2008.\\n\\nJie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. Are large pre-trained language models leaking your personal information? In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2038\u20132047, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\\n\\nBinyuan Hui, Ruiying Geng, Qiyu Ren, Binhua Li, Yongbin Li, Jian Sun, Fei Huang, Luo Si, Pengfei Zhu, and Xiaodan Zhu. Dynamic hybrid relation exploration network for cross-domain context-dependent semantic parsing. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 13116\u201313124. AAAI Press, 2021.\\n\\nBinyuan Hui, Xiang Shi, Ruiying Geng, Binhua Li, Yongbin Li, Jian Sun, and Xiaodan Zhu. Improving text-to-sql with schema dependency learning. In arXiv:2103.04399, 2021.\\n\\nBinyuan Hui, Ruiying Geng, Lihan Wang, Bowen Qin, Yanyang Li, Bowen Li, Jian Sun, and Yongbin Li. S^2SQL: Injecting syntax to question-schema interaction graph encoder for text-to-SQL parsers. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1254\u20131262, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.99.\\n\\nIhab F. Ilyas and Xu Chu. Trends in cleaning relational data: Consistency and deduplication. Foundations and Trends in Databases, 5:281\u2013393, 2015.\\n\\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, and Luke Zettlemoyer. Learning a neural semantic parser from user feedback. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 963\u2013973, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1089.\"}"}
{"id": "dI4wzAE6uV", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, volume 35, pages 22199\u201322213, 2022.\\n\\nJan Kossmann, Stefan Halfpap, Marcel Jankrift, and Rainer Schlosser. Magic mirror in my hand, which is the best in the land? an experimental evaluation of index selection algorithms. Proceedings of the VLDB Endowment, 13(12):2382\u20132395, 2020.\\n\\nPrerna S. Kulkarni and Jagdish W. Bakal. Survey on data cleaning. In 2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI), page 2361\u20132366, 2014.\\n\\nChia-Hsuan Lee, Oleksandr Polozov, and Matthew Richardson. KaggleDBQA: Realistic evaluation of text-to-SQL parsers. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2261\u20132273, Online, August 2021. Association for Computational Linguistics.\\n\\nGyubok Lee, Hyeonji Hwang, Seongsu Bae, Yeonsu Kwon, Woncheol Shin, Seongjun Yang, Minjoon Seo, Jong-Yeup Kim, and Edward Choi. Ehrsql: A practical text-to-sql benchmark for electronic health records. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 15589\u201315601. Curran Associates, Inc., 2022.\\n\\nDandan Li, Lu Han, and Yi Ding. Sql query optimization methods of relational database system. In 2010 Second International Conference on Computer Engineering and Applications, volume 1, pages 557\u2013560. IEEE, 2010.\\n\\nJinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing. ArXiv, abs/2301.07507, 2023.\\n\\nTanzim Mahmud, KM Azharul Hasan, Mahtab Ahmed, and Thwoi Hla Ching Chak. A rule based approach for nlp based query processing. In 2015 2nd International Conference on Electrical Information and Communication Technologies (EICT), pages 78\u201382. IEEE, 2015.\\n\\nGr\u00e9goire Mialon, Roberto Dess\u00ec, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Rozi\u00e8re, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. Augmented language models: a survey. ArXiv, abs/2302.07842, 2023.\\n\\nVamsi Krishna Myalapalli and ASN Chakravarthy. Revamping sql queries for cost based optimization. In 2016 International Conference on Circuits, Controls, Communications and Computing (I4C), pages 1\u20136. IEEE, 2016.\\n\\nPaulo H. Oliveira, Daniel dos Santos Kaster, Caetano Traina, and Ihab F. Ilyas. Batchwise probabilistic incremental data cleaning. ArXiv, abs/2011.04730, 2020.\\n\\nOpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\\n\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022.\\n\\nHamid Pirahesh, Joseph M. Hellerstein, and Waqar Hasan. Extensible/rule based query rewrite optimization in starburst. In Proceedings of the ACM SIGMOD International Conference on Management of Data, pages 39\u201348, 1992.\\n\\nMohammadreza Pourreza and Davood Rafiei. DIN-SQL: decomposed in-context learning of text-to-sql with self-correction. CoRR, abs/2304.11015, 2023. doi: 10.48550/arXiv.2304.11015.\"}"}
{"id": "dI4wzAE6uV", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Yu Cheng, Chenghu Zhou, Xinbing Wang, Quanshi Zhang, and Zhouhan Lin. RASAT: Integrating relational structures into pretrained Seq2Seq model for text-to-SQL. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3215\u20133229, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\\n\\nBowen Qin, Binyuan Hui, Lihan Wang, Min Yang, Jinyang Li, Binhua Li, Ruiying Geng, Rongyu Cao, Jian Sun, Luo Si, Fei Huang, and Yongbin Li. A survey on text-to-sql parsing: Concepts, methods, and future directions. In arXiv:2208.13629, 2022.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020.\\n\\nNitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau. Evaluating the text-to-sql capabilities of large language models. ArXiv, abs/2204.00498, 2022.\\n\\nTorsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. PICARD: Parsing incrementally for constrained auto-regressive decoding from language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9895\u20139901, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\\n\\nPeter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. Compositional generalization and natural language variation: Can a semantic parsing approach handle both? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 922\u2013938, Online, August 2021. Association for Computational Linguistics.\\n\\nAlon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, and Jonathan Berant. Commonsenseqa 2.0: Exposing the limits of ai through gamification. In J. Vanschoren and S. Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1. Curran, 2021.\\n\\nBailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. RAT-SQL: Relation-aware schema encoding and linking for text-to-SQL parsers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7567\u20137578, Online, July 2020. Association for Computational Linguistics.\\n\\nLihan Wang, Bowen Qin, Binyuan Hui, Bowen Li, Min Yang, Bailin Wang, Binhua Li, Jian Sun, Fei Huang, Luo Si, and Yongbin Li. Proton: Probing schema linking information from pre-trained language models for text-to-sql parsing. In Aidong Zhang and Huzefa Rangwala, editors, KDD '22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022, pages 1889\u20131898. ACM, 2022. doi: 10.1145/3534678.3539305.\\n\\nLijie Wang, Ao Zhang, Kun Wu, Ke Sun, Zhenghua Li, Hua Wu, Min Zhang, and Haifeng Wang. DuSQL: A large-scale and pragmatic Chinese text-to-SQL dataset. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6923\u20136935, Online, November 2020. Association for Computational Linguistics.\\n\\nPing Wang, Tian Shi, and Chandan K. Reddy. Text-to-sql generation for question answering on electronic medical records. Proceedings of The Web Conference 2020, 2020.\\n\\nZhaoguo Wang, Zhou Zhou, Yicun Yang, Haoran Ding, Gansen Hu, Ding Ding, Chuzhe Tang, Haibo Chen, and Jinyang Li. Wetune: Automatic discovery and verification of query rewrite rules. In Proceedings of the 2022 International Conference on Management of Data, pages 94\u2013107, 2022.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022.\"}"}
{"id": "dI4wzAE6uV", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. UnifiedSKG: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 602\u2013631, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\\n\\nXiaojun Xu, Chang Liu, and Dawn Song. Sqlnet: Generating structured queries from natural language without reinforcement learning. ArXiv preprint, 2017.\\n\\nNavid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and Thomas Dillig. Sqlizer: query synthesis from natural language. Proceedings of the ACM on Programming Languages, 1(OOPSLA):1\u201326, 2017.\\n\\nTao Yu, Zifan Li, Zilin Zhang, Rui Zhang, and Dragomir Radev. TypeSQL: Knowledge-based type-aware neural text-to-SQL generation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), page 588\u2013594, 2018.\\n\\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, page 3911\u20133921, 2018.\\n\\nTao Yu, Rui Zhang, Alex Polozov, Christopher Meek, and Ahmed Hassan Awadallah. Score: Pre-training for context representation in conversational semantic parsing. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\\n\\nJohn M. Zelle and Raymond J. Mooney. Learning to parse database queries using inductive logic programming. In Proceedings of the Fourteenth National Conference on Artificial Intelligence and Ninth Conference on Innovative Applications of Artificial Intelligence, pages 1050\u20131055, 1996.\\n\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models. ArXiv, abs/2205.01068, 2022.\\n\\nChen Zhao, Yu Su, Adam Pauls, and Emmanouil Antonios Platanios. Bridging the generalization gap in text-to-SQL parsing with schema expansion. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5568\u20135578, Dublin, Ireland, May 2022. Association for Computational Linguistics.\\n\\nVictor Zhong, Caiming Xiong, and Richard Socher. Seq2SQL: Generating structured queries from natural language using reinforcement learning. In CoRR abs/1709.00103, 2017.\\n\\nVictor Zhong, Mike Lewis, Sida I. Wang, and Luke Zettlemoyer. Grounded adaptation for zero-shot executable semantic parsing. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 6869\u20136882. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.558.\\n\\nRong Zhou. Research on key performance index prediction of distributed database based on machine learning algorithm. In Proceedings of the 2nd International Conference on Cognitive Based Information Processing and Applications (CIPA 2022) Volume 2, pages 563\u2013567. Springer, 2023.\\n\\nXuanhe Zhou, Guoliang Li, Chengliang Chai, and Jianhua Feng. A learned query rewrite system using monte carlo tree search. Proceedings of the VLDB Endowment, 15(1):46\u201358, 2021. doi: 10.14778/3485450.3485456.\"}"}
{"id": "dI4wzAE6uV", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Xuanhe Zhou, Chengliang Chai, Guoliang Li, and Ji Sun. Database meets artificial intelligence: A survey. IEEE Transactions on Knowledge and Data Engineering, 34(3):1096\u20131116, 2022. doi: 10.1109/TKDE.2020.2994641.\"}"}
