{"id": "vfzXDRTcF4", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.2 Visual Understanding Annotation\\n\\nWe employ GPT-3.5 to generate the answer to downstream tasks according to the given prompt. We follow this format to query GPT-3.5:\\n\\nYou are a visual art designer. Here is a prompt for a text-to-image generation model: [PROMPT]. You will be required to do the following 3 tasks based on the prompt you receive. Please be faithful to the prompt given to you and do not hallucinate. Directly answer the questions without any redundant sentences in the format of Python dict. The first task is to separate the prompt into 'Style', 'Content', 'Atmosphere', and 'Other' categories. 'Style' words describe the whole image style. 'Content' words describe the image content. 'Atmosphere' words describe the emotional and psychological elements associated with the image, including the mood and feeling conveyed by the scene. If some words are hard to be sorted into 'Style', 'Content', or 'Atmosphere', put them in the 'Other' category. You should try to limit putting words into the 'Other' category as much as possible. The second task is to provide the caption according to the content of the prompt. Only consider the 'Content' words separated in the first task and ignore the 'Style' and 'Atmosphere' words. Be faithful to the 'Content' prompt and do not add any redundant information. The caption should be in a tone that a visual AI assistant is describing the image. The caption should be a single complete sentence. The Third task is to design a set of multiple-choice questions based on the style and content that are separated in the first task. The answers should be in a tone that a visual AI assistant is seeing the image and answering the question. Ask diverse questions and give corresponding answers and also provide wrong options. Only include questions that have definite answers that satisfy the following conditions: 1) one can see the content in the image that the question asks about and can answer confidently, 2) one can determine confidently from the image that wrong options are not in the image. Do not ask any questions that cannot be answered confidently. The answer should not be 'Unknown'. Please include complex questions that are relevant to the content in the image, for example, asking about background knowledge of the objects in the image, asking to discuss events happening in the image, etc. Never ask about uncertain details. Never ask questions you cannot determine from the given prompt. Provide detailed answers when answering complex questions. For each question, there should be 4 options. The options should be relevant but only one is correct. Return a single json file with the following format: [FORMAT]. Strictly follow the provided format please. Directly return the python dict ONLY! Do not say the polite words. Make sure your answer can be parsed by json directly.\\n\\nB.3 Style Clustering\\n\\nSimilarly, we ask the GPT-3.5 to cluster the style prompts into several categories. The prompt we use to query GPT-3.5 is:\\n\\nHere are some style-relative prompts: [PROMPTS]. Please help me build a hierarchal tree, to summarise these prompts into several categories like photography styles, camera parameters, colour grading, lighting, film looks, mood, artist style, etc. And each category may have fine-grained sub-categories. Please return a Python dict in the format like: [FORMAT]. You should design the keyword to make sure it summarizes its following list. One prompt can belong to more than one category. Directly return the python dict ONLY! Do not say the polite words. Make sure your answer can be parsed by json directly.\"}"}
{"id": "vfzXDRTcF4", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C Additional Experiments\\n\\nC.1 Question Answering Score (QAS)\\n\\nThe grammar structure of the prompts is quite different from the caption. The image caption is usually a complete sentence, while some prompts might simply be some broken phases instead. Prior metrics treat the prompts as complete sentences, which do not always hold and bring noise into the evaluation. Therefore, we propose a Question Answering Score (QAS) for the evaluation of the prompt inversion task, which is computed by feeding the predicted prompt to a large language model (LLM) and calculating the accuracy of the LLM answers to the relevant questions provided in the annotation.\\n\\nSpecifically, we make use of Vicuna [54] as the LLM. Given a generated image $I$, a prompt inversion model predicts the prompt $\\hat{p}$. And as the annotation, there are $N$ style-relevant questions $q_s$ and answers $a_s$ concerning the style elements in the ground-truth prompts, as well as $M$ content-relevant questions $q_c$ and answers $a_c$ concerning the content. In the following, we do not distinguish between symbols for style and content, but in implementation, we treat them separately to calculate QAS$^s$ and QAS$^c$. We construct a prompt $P$ with $\\hat{p}$, $q$, and $a$ in the following format:\\n\\nHere is a prompt: $\\\\hat{p}$. Assume you see the image generated from the given prompt. Answer the question: $q$. Choose from: $a$. Directly answer A or B or C or D without any further illustration. Your answer should be one single letter.\\n\\nBy feeding the constructed prompt $P$ into the LLM we obtain the predicted result: $\\\\hat{a} = L(P)$.\\n\\nWe calculate the average accuracy separately among the $N$ style questions and $M$ content question for this image, and obtain the final QAS by computing the average for all the $K$ images:\\n\\n$$QAS^s = \\\\frac{1}{K} \\\\sum_{k=1}^{K} \\\\sum_{i=1}^{N} I(\\\\hat{a} = a)$$\\n\\n$$QAS^c = \\\\frac{1}{K} \\\\sum_{k=1}^{K} \\\\sum_{i=1}^{M} I(\\\\hat{a} = a)$$\\n\\nIn this way, we convert the \u201cprompt similarity\u201d problem to compute the accuracy of the question-answering task, which is more interpretable. We show QAS results in the last two columns in Table 8.\\n\\n| Mode     | Models          | Test  | BLEU-4 | METEOR | ROUGE-L | CIDEr | Similarity | QAS$^s$ | QAS$^c$ |\\n|----------|-----------------|-------|--------|--------|---------|-------|------------|---------|---------|\\n| ZeroShot | BLIP-2          | OPT   | 0.29   | 2.85   | 7.06    | 6.46  | 0.36       | 12.42%  | 18.55%  |\\n|          | BLIP-2          | FlanT5| 0.40   | 2.95   | 7.69    | 8.86  | 0.37       | 13.79%  | 18.58%  |\\n|          | MiniGPT-4       | [38]  | 1.71   | 6.51   | 13.13   | 11.40 | 0.43       | 17.12%  | 26.79%  |\\n|          | Uni-Perceiver v2| [30]  | 0.37   | 2.73   | 9.88    | 15.45 | 0.34       | 12.43%  | 18.49%  |\\n| Finetune | Uni-Perceiver v2|       | 4.68   | 8.56   | 16.98   | 34.01 | 0.51       | 19.71%  | 24.84%  |\"}"}
{"id": "vfzXDRTcF4", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Comparison between images generated by Stable Diffusion v1.4 and images in JourneyDB regarding HPS. Images in the same row are generated with the same prompt.\\n\\nC.2 Analysis of Image Quality\\n\\nAs shown in Fig. 6, Images in JourneyDB (credit to Midjourney [55]) exhibit better visual quality than those generated by Stable Diffusion, quantified by Human Preference Score (HPS) [56].\"}"}
{"id": "vfzXDRTcF4", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Statistics of JourneyDB.\\n\\nWe provide 4 million generated image-prompt pairs, 1 million captions and over 8 million VQA annotations.\\n\\n| Dataset                  | Labeled Image | Labeled Prompt | Style QA | Content QA |\\n|--------------------------|---------------|---------------|----------|------------|\\n| VQ-Diffusion [6]         | 1,888         | 1,869         | 3,067    | 3,346      |\\n| VQGAN + CLIP [57]        | 1,888         | 1,869         | 3,067    | 3,346      |\\n| GLIDE [58]               | 1,898         | 1,878         | 3,101    | 3,349      |\\n| CogView2 [59]            | 1,914         | 1,896         | 3,135    | 3,387      |\\n| Latent Diffusion [14]    | 1,942         | 1,942         | 3,159    | 3,438      |\\n| Versatile Diffusion [60] | 1,953         | 1,935         | 3,179    | 3,427      |\\n| Stable Diffusion v1.4 [14]| 2,028     | 2,010         | 3,301    | 3,621      |\\n| DeepFloyd-XL [61]        | 2,052         | 2,028         | 3,334    | 3,655      |\\n| Epic Diffusion [62]      | 2,066         | 2,047         | 3,366    | 3,685      |\\n| DALL\u00b7E mini [63]         | 2,097         | 2,075         | 3,415    | 3,739      |\\n| Dreamlike Photoreal 2.0 [64] | 2,100     | 2,080         | 3,393    | 3,737      |\\n| Stable Diffusion v2.0 [14]| 2,104     | 2,084         | 3,405    | 3,756      |\\n| Deliberate [65]          | 2,122         | 2,101         | 3,447    | 3,781      |\\n| LAFITE [66]              | 2,124         | 2,105         | 3,439    | 3,804      |\\n| Realistic Vision [67]    | 2,144         | 2,119         | 3,475    | 3,832      |\\n| FuseDream [68]           | 2,176         | 2,157         | 3,536    | 3,893      |\\n| SDXL Refiner 0.9 [8]     | 2,184         | 2,161         | 3,540    | 3,915      |\\n| MajicMix Realistic [69]  | 2,189         | 2,167         | 3,568    | 3,910      |\\n| ChilloutMix [70]         | 2,207         | 2,185         | 3,571    | 3,923      |\\n| DALL\u00b7E 2 [7]             | 2,220         | 2,197         | 3,593    | 3,967      |\\n| Openjourney [71]         | 2,237         | 2,214         | 3,630    | 3,992      |\\n| SDXL Base 0.9 [8]        | 2,270         | 2,246         | 3,686    | 4,062      |\\n| Total                    | 45,803        | 45,365        | 74,407   | 81,565     |\\n\\nTable 10: Evaluation results of Prompt Inversion and Image Captioning on the extension test set of JourneyDB.\\n\\n| Models                  | Prompt Inversion | Image Caption | VQA |\\n|-------------------------|------------------|---------------|-----|\\n| BLIP-2 OPT [4]          | 3.46 8.06 24.82 51.81 | 3.99 9.00 26.25 55.64 |       |\\n| BLIP-2 FlanT5 [7]       | 5.15 9.41 26.06 54.57 | 4.56 9.77 27.57 59.28 | 68.38% 66.57% |\\n\\nD Cross-model Test Set\\n\\nAs listed in Table 9, we additionally introduce another 22 text-to-image generative models into JourneyDB, such as VQ-Diffusion [6] DALL\u00b7E 2 [7] StableDiffusion-XL [8], etc., which significantly improves the diversity of JourneyDB, making it a comprehensive benchmark for evaluating the comprehension of generated images. For each generative model, we originally generated 3,200 images, and a group of 60 annotators helped clean up the pairs without consistency to obtain the final cross-model test set containing 45,803 images in total.\\n\\nWe evaluate the BLIP models on the new dataset on the image caption and prompt inversion tasks. Results are shown in Table 10.\\n\\nThis additional text-image dataset, with manually cleaned text prompt, image captions, and VQA annotations, serves as a divergent and comprehensive benchmark for the evaluation of the visual understanding model for generated images.\"}"}
{"id": "vfzXDRTcF4", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"JourneyDB: A Benchmark for Generative Image Understanding\\n\\nKeqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, Limin Wang, Hongsheng Li\\n\\n1 Multimedia Laboratory, The Chinese University of Hong Kong\\n2 The University of Hong Kong\\n3 Shanghai Artificial Intelligence Laboratory\\n4 Centre for Perceptual and Interactive Intelligence\\n\\nAbstract\\n\\nWhile recent advancements in vision-language models have had a transformative impact on multi-modal comprehension, the extent to which these models possess the ability to comprehend generated images remains uncertain. Synthetic images, in comparison to real data, encompass a higher level of diversity in terms of both content and style, thereby presenting significant challenges for the models to fully grasp. In light of this challenge, we introduce a comprehensive dataset, referred to as JourneyDB, that caters to the domain of generative images within the context of multi-modal visual understanding. Our meticulously curated dataset comprises 4 million distinct and high-quality generated images, each paired with the corresponding text prompts that were employed in their creation. Furthermore, we additionally introduce an external subset with results of another 22 text-to-image generative models, which makes JourneyDB a comprehensive benchmark for evaluating the comprehension of generated images. On our dataset, we have devised four benchmarks to assess the performance of generated image comprehension in relation to both content and style interpretation. These benchmarks encompass prompt inversion, style retrieval, image captioning, and visual question answering. Lastly, we evaluate the performance of state-of-the-art multi-modal models when applied to the JourneyDB dataset, providing a comprehensive analysis of their strengths and limitations in comprehending generated content. We anticipate that the proposed dataset and benchmarks will facilitate further research in the field of generative content understanding. The dataset is publicly available at https://journeydb.github.io.\\n\\n1 Introduction\\n\\nIn recent times, notable progress has been achieved in the domain of Artificial Intelligence Generative Content (AIGC), particularly in the advancement of diffusion models [1] that have significantly enhanced the quality of generative content. As a consequence, AIGC platforms such as DALLE, Stability AI, Runway, and Midjourney have gained considerable popularity, enabling users to generate exceptionally high-quality images using text prompts composed in natural language. These text prompts encompass both content and style descriptions provided by users, playing a pivotal role in image generation (see Figure 1 for an illustrative prompt). Unlike descriptions acquired from captioning real images, text prompts for image generation tend to exhibit a high level of detail and specificity, surpassing mere portrayal of salient content. The primary objective behind the creation\\n\\n\u2217 Equal Contribution\\n\u2020 Project Lead\\n\u2021 Corresponding Authors\"}"}
{"id": "vfzXDRTcF4", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A group of brown maltese poodle dogs are singing Karaoke. \\n\\nQ1: What are the dogs doing? D. Singing Karaoke\\nQ2: What type of light is used in this image? B. Spotlight\\n\\nFigure 1: Data Collection Procedure.\\nTo collect enough generated images, we investigate the Midjourney channel on Discord to collect the available pictures. Then we employ the GPT-3.5 to annotate the downstream tasks, including 1) separating the prompt into \u201cStyle\u201d and \u201cContent\u201d, 2) generating the caption according to the content words obtained from task 1, 3) generating \u201cStyle-relevant questions\u201d and \u201cContent-relevant questions\u201d, providing 4 options for each question, together with the answer. Please refer to Section 3 for more details.\\n\\nof these prompts lies in visual generation, resulting in intricate descriptions that encompass diverse stylistic facets such as lighting, camera angle, artistic style, medium, and more. Moreover, the generated content originates from the users\u2019 imagination, often depicting scenes and compositions that are entirely fictional and devoid of real-world existence.\\n\\nConsidering the aforementioned characteristics, we contend that both the elaborate textual prompts and the generated images themselves serve as valuable sources of information that can be incorporated into existing visual understanding benchmarks. On one hand, the detailed text prompts offer a more comprehensive interpretation of the visual scene, enabling us to perceive the scene and comprehend its underlying style. On the other hand, the abundance of novel object compositions in the generated images provides insights into a realm unrestricted by conventional sense biases, facilitating exploration beyond the constraints of traditional visual representations.\\n\\nFoundation models have achieved unparalleled capabilities across various visual understanding tasks, owing to large-scale pre-training on datasets, such as CLIP [2], Flamingo [3], and BLIP-2 [4]. However, it is essential to acknowledge that current foundation models are primarily pre-trained on real data, giving rise to concerns regarding their generalization ability and effectiveness in handling the distinctive characteristics associated with generative content. These models may not fully capture the nuanced aspects of generative content and might encounter difficulties in comprehending and generating high-quality images based on complex text prompts.\\n\\nIn view of this challenge, our research initiative seeks to address this gap by curating a dataset comprising a substantial number of 4 million meticulously generated images accompanied by corresponding text prompts. This dataset serves as the fundamental basis for a benchmark consisting of four distinct tasks, which collectively facilitate a comprehensive evaluation of generative content understanding.\\n\\nThe initial task, referred to as prompt inversion, involves identifying the text prompts employed by the user to generate the given images. This task serves to decipher the original prompt or description, assessing the model\u2019s ability to comprehend both the content and style of the generated images. The second task involves style retrieval, wherein the model is tasked with identifying and retrieving similar generative images based on their stylistic attributes. This task evaluates the model\u2019s proficiency in discerning subtle stylistic nuances within generative images. The third task centres around image captioning, requiring the model to generate descriptive captions that accurately represent the content of the generative image. This task evaluates the model\u2019s capability to effectively comprehend and express the visual elements of the generated content using natural language. The fourth and final task is visual question answering (VQA), in which the model is expected to provide accurate answers to questions related to the generative image. This task evaluates the model\u2019s ability to comprehend the visual and stylistic content and deliver relevant responses based on the provided questions.\\n\\nWe collected a total of 4,692,751 pairs of image-text prompts, which were subsequently divided into a training set comprising 4,453,193 pairs, a validation set comprising 234,156 pairs, and a test set.\"}"}
{"id": "vfzXDRTcF4", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: A comparison between JourneyDB and other commonly-used Text-Image multi-modal datasets.\\n\\n| Dataset            | Total Image Num | H | M | Label | Source | Image Caption | VQA Prompt | Inversion | Style Retrieval |\\n|--------------------|-----------------|---|---|-------|--------|---------------|------------|-----------|-----------------|\\n| Flickr Caption     | 32k             | \u2713 |   | \u2713     | H      | \u2713            |            |           |                 |\\n| COCO Caption       | 164k            | \u2713 |   | \u2713     | H      | \u2713            |            |           |                 |\\n| VQA v2             | 204k            | \u2713 |   | \u2713     | H      | \u2713            |            |           |                 |\\n| A-OKVQA            | 24k             | \u2713 |   | \u2713     | H      | \u2713            |            |           |                 |\\n| LAION-COCO         | 600M            |   | M | \u2713     | M      | \u2713            |            |           |                 |\\n| DiffusionDB        | 14M             |   | M | \u2713     | M      | \u2713            |            |           |                 |\\n| Ours               | 4M              | \u2713 | \u2713 | \u2713     | H + M  | \u2713            | \u2713          | \u2713         | \u2713               |\\n\\nThe dataset comprises 5,402 pairs. We also include 45,803 images from 22 other text-to-image models provided by HPD v2 [5], including VQ-Diffusion [6], DALL\u00b7E 2 [7], StableDiffusion-XL [8], etc., to build the external set for cross dataset evaluation. Given that the generative model is not flawless, some discrepancies in the text prompts may be present. Consequently, for the test set, we carried out human verification, where annotators were tasked with removing word descriptions that do not align with the corresponding images. To create annotations for tasks 2, 3, and 4, we utilized GPT-3.5 to convert text prompts into annotations specific to each task.\\n\\nTo comprehensively evaluate the performance of current state-of-the-art multi-modal models, we conducted extensive assessments using our benchmark dataset. Furthermore, we performed in-depth analyses to gain insights into the strengths and limitations of these models when applied to generative content. Overall, we observed that the state-of-the-art models do not perform as effectively as they do on real datasets, and fine-tuning on the proposed dataset significantly enhances their performance.\\n\\nIn conclusion, our contribution encompasses three key aspects: 1) To the best of our knowledge, we are the first to draw attention to the visual understanding of generated images. 2) We propose JourneyDB, a large-scale benchmark that serves as both a training and evaluation resource for this emerging field. 3) We conducted an extensive evaluation of state-of-the-art visual understanding models using the proposed dataset, revealing their relatively limited performance on generative content. We hope that our endeavours will contribute to further advancements in the field of generative content understanding.\\n\\n2 Related Works\\n\\n2.1 Image-Text Datasets\\n\\nWe present a summary of existing image-text datasets in Table 1. The Flickr Caption dataset [9] consists of 32,000 images obtained from the Flickr [15] platform, accompanied by five reference sentences provided by human annotators. The COCO Caption dataset [10] comprises 164 thousand images, with five independent human-generated captions provided for each image for training and validation, resulting in over 1.5 million captions. These datasets play a crucial role in fostering the development of the Image-Caption Task. The Visual Question Answering (VQA) v2.0 [11] dataset, which is the second version of the VQA dataset [16], contains open-ended questions about images that require an understanding of vision, language, and commonsense knowledge to answer.\\n\\nA-OKVQA [12], an augmented successor of OK-VQA [17], encompasses a diverse set of 24 thousand questions that demand a broad foundation of common and world knowledge for accurate responses. These datasets involve human employees in the annotation process, ensuring consistently high-quality annotations. However, manual annotation by human annotators is a time-consuming and costly endeavour, thereby limiting the scalability of the datasets. LAION-COCO [13] is another large-scale dataset containing 600 million image-caption pairs, where GPT3.5 is employed to generate more detailed captions. Although these datasets may contain noise due to the cleaning or generation process using pre-trained neural network models, they have demonstrated their utility in training multi-modal models. However, it is important to note that these datasets primarily focus on real images and cater to a specific task. A comparable dataset to the present study is DiffusionDB [18], a large-scale text-to-image prompt dataset comprising 14 million images generated using Stable Diffusion. However, the image quality from Stable Diffusion is plausible, and no further annotations...\"}"}
{"id": "vfzXDRTcF4", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this paper, we collect data from Midjourney and provide annotations generated by GPT3.5 to support four downstream tasks.\\n\\n2.2 Text-to-Image Generative Models\\n\\nText-to-image generative models [19, 20, 14, 7, 21] aim at generating images according to text conditions, apart from traditional generative models [22, 23, 24, 25], which map random noise to images. Text-to-image generative models have experienced rapid development in recent years, empowering users to create image content through natural language specifications. This field has seen significant progress since Mansimov et al. [26, 27] demonstrated that Deep Recurrent Attention Writer (DRAW) can generate images conditioned on text. Since then, several generative architectures and modeling approaches have been applied for text-to-image generation, including autoregressive models [19], GANs [20], and diffusion models [14, 7, 21]. Among these, diffusion models have shown better computational efficiency and the ability to produce higher-quality samples compared to autoregressive models [7]. These diffusion models have reached a level of maturity where they can generate high-quality images suitable for industrial deployment. Notably, Midjourney provides state-of-the-art text-to-image generation service using diffusion models [5]. A vast number of artificial images are generated each day at unprecedented speed. As perception and generation tasks are double sides of the same coin, the achievements in the generative models open new probability for the perception studies. In this context, our dataset aims to organize and consolidate recent progress in text-to-image generative models while laying the foundations for further research in perception studies.\\n\\n2.3 Multi-modal Foundation Models and Datasets\\n\\nAided by data from diverse sources, multi-modal foundation models are capable of understanding and connecting data across multiple modalities, such as image, text, audio and so on. As prioneering vision-language models, CLIP [2] and ALIGN [28] adopt contrastive learning paradigms and are pre-trained by millions of web-collected image-text pairs, which showcases promising visual zero-shot capabilities. Flamingo [3] and BLIP-2 [4] further align pre-trained vision backbones with language models with intermediate networks and billions of data pairs, exhibiting superior results on vision-language tasks. OFA [29], Uni-Perceivers [30, 31, 32], and Unified-IO [33] also introduce unified training architectures for different modalities with competitive performance to uni-modal methods. Recently, inspired by the powerful GPT-4 [34], many efforts have been devoted to multi-modal instruction-following models, such as LLaMA-Adapter [35, 36], LLaV A [37] and MiniGPT-4 [38]. Given the textual prompts with image conditions, these models fine-tune a frozen LLaMA [39] to respond to multi-modality instructions, the training data of which is either existing image-caption data [10] or GPT-annotated pairs [37]. Despite the popularity of multi-modal models, it is still rarely explored for their generalization capacity on generated vision-language data, considering the difference between the real-world pre-training data and generative content. In this paper, we propose a large-scale synthetic dataset, JourneyDB, along with customized benchmarks to fully validate the extension efficacy current multi-modal models.\\n\\n2.4 Training with Generated Data\\n\\nIt is worth noting that the annotations generated by GPT demonstrate a lower level of noise than expected, validating the effectiveness of these models. Notably, LLaV A [37] introduces a novel instruction-tuning dataset that leverages the capabilities of both GPT3.5 and GPT4. Their experiments reveal a remarkable relative score increase of 295.8%, elevating the score from 21.5 to 85.1, thus emphasizing the effectiveness of their generated data. LaCLIP [40] integrates text augmentations by employing text rewriting techniques with GPT3.5 and Bard. By rewriting the textual descriptions within existing image caption datasets, they achieve a notable improvement of 36.08%, raising the score from 15.8 to 21.5. StableRep [41] unveils the remarkable potential of using exclusively synthetic data generated from text-to-image models to train highly effective visual representations, surpassing the performance of models trained solely on real image datasets. In a similar vein, VideoChat [42] constructs a dataset by sequentially feeding dense captions to GPT3.5 in temporal order. Despite the inherent challenges in comprehending individual frames with GPT3.5, their successful mastery of understanding the entire video demonstrates the effectiveness of their approach. The generated annotations not only validate the effectiveness of GPT models but also significantly contribute to...\"}"}
{"id": "vfzXDRTcF4", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Statistics of JourneyDB.\\n\\nWe provide 4 million generated image-prompt pairs, 1 million captions and over 8 million VQA annotations.\\n\\n| Dataset       | Image Prompt | Labeled Image | Labeled Prompt | Style QA | Content QA |\\n|---------------|--------------|---------------|----------------|----------|------------|\\n| Training Set  | 4,453,193    | 1,643,375     | 4,189,737      | 1,385,317| 7,056,394  |\\n| Validation Set| 234,156      | 82,093        | 234,156        | 82,093   | 311,569    |\\n| Testing Set   | 5,402        | 5,171         | 5,402          | 5,171    | 10,040     |\\n| External Set  | 45,803       | 45,365        | 45,803         | 45,365   | 74,407     |\\n| Total         | 4,738,554    | 1,776,004     | 4,475,098      | 1,517,946| 7,452,410  |\\n\\nadvancing the understanding of images. Therefore, based on our demonstrated results, we firmly believe that our JourneyDB can serve as a valuable tool to enhance numerous image-related tasks.\\n\\n3 Dataset\\n\\nIn this section, we present the methodology employed for dataset collection and annotation, along with relevant statistical insights to gain a deeper understanding of the dataset.\\n\\n3.1 Data Collection\\n\\nThe data collection procedure is presented in Figure 1. In order to obtain a sufficient number of generated images, we investigated the Midjourney channel [43] on the Discord platform [44] to access the available pictures. Within the public Discord channel named \\\"Midjourney,\\\" users submit text prompts to the channel, and the Midjourney bot responds with the corresponding generated images. Users then select the preferred image for upscaling, and Midjourney provides the corresponding upscaled images. The chat history contains numerous publicly accessible prompt-image pairs. To collect the data, we utilized DiscordChatExporter [45], a widely used Discord crawler, to download the publicly available images along with their corresponding prompts. In this version of the dataset, we only retained images that were generated solely based on text prompts, filtering out any images conditioned on given images. Additionally, we removed Midjourney-specific arguments, such as \\\"-v 4\\\", to enhance the generalizability of the prompts and ensure their comprehensibility for existing large language models.\\n\\nMoreover, to improve the diversity of JourneyDB, we additionally introduce another text-to-image generative models into JourneyDB, such as VQ-Diffusion [6], DALL\u00b7E 2 [7], StableDiffusion-XL [8], etc., which makes our data a comprehensive benchmark for evaluating the comprehension of generated images. For each generative model, we originally generated 3,200 images, and a group of 60 annotators helped clean up the pairs without consistency to obtain the final cross-model test set containing 45,803 images in total. Please find more details of this part in the appendix D.\\n\\n3.2 Data Annotation\\n\\nWe provide ample annotations for multiple visual understanding tasks. The dataset is compared with existing methods in Table 1, demonstrating its versatility in supporting four downstream tasks.\\n\\nAnnotation for Visual Understanding.\\n\\nIn this section, GPT-3.5 is employed to annotate the downstream tasks. Specifically, a set of Midjourney prompts and explicit instructions are provided to GPT-3.5. The objectives are as follows: 1) segmenting the prompt into \\\"Style\\\", \\\"Content\\\", \\\"Atmosphere\\\", and \\\"Others\\\", 2) generating captions based on the content words identified in task 1, 3) generating \\\"Style-relevant questions\\\" and \\\"Content-relevant questions,\\\" accompanied by four answer choices for each question. The detailed instructions provided to GPT-3.5 can be found in the Supplementary Materials.\\n\\nClustering of Styles.\\n\\nNumerous prompts related to style are highly intricate for style retrieval. Taking inspiration from existing prompt engineering platforms [4], we propose a hierarchical clustering approach.\"}"}
{"id": "vfzXDRTcF4", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"approach for organizing styles, which simplifies style retrieval and facilitates user reference. Since traditional word embedding and clustering methods struggle to handle sophisticated style words, we leverage GPT-3.5 for this task. Specifically, we divide the prompts into smaller patches, each comprising 200 prompts, and instruct GPT-3.5 to cluster the words. Subsequently, we manually merge the categories from different patches to obtain the final \u201cstyle tree\u201d. The distribution of the clustered style space is visualized in Figure 2.\\n\\nFiltering for Image-Prompt Consistency. Due to the limitations of Text-to-Image generative models, inconsistencies may arise between the prompt and the generated image. To ensure the quality of the test set, we engaged 40 annotators to identify inconsistent prompt words in the test set. Specifically, given a pair of text prompts and the corresponding generated image, the annotators are instructed to verify if each word is depicted in the image. Words annotated as \u201cNot Appear\u201d are removed to obtain the clean prompts.\\n\\n3.3 Data Statistics\\n\\nGeneral Statistics\\nIn this iteration, a total of 4,692,751 images were collected, all with a resolution exceeding 1024 \u00d7 1024, accompanied by corresponding text prompts. Among them, 1,730,639 prompts were found to be independent. Furthermore, 1,472,581 instances were annotated using the GPT-3.5 model, following the procedure outlined in Figure 1. Additionally, 5,402 images were filtered out due to inconsistencies between the images and prompts, as determined by the Image-Prompt consistency check. Moreover, 45,803 images from 22 other text-to-image models provided by HPD v2 [5] are introduced to build the external set for cross dataset evaluation. In addition, a clustering process was conducted to summarize the 70,521 fine-grained styles into 334 style categories, displaying a long-tail distribution pattern, as illustrated in Figure 2.\\n\\nDataset Split\\nDetailed statistics for each subset of the dataset are provided in Table 2. The entire dataset was randomly divided, with approximately a 20 : 1 ratio, to create the training and validation sets. The training set comprises 4,189,737 images and 1,385,317 prompts, while the validation set consists of 234,156 images and 82,093 prompts. Additionally, a separate testing set was sampled for manual filtering, consisting of 5,402 images and 5,171 prompts.\\n\\n4 Benchmarks\\n4.1 Prompt Inversion\\nThe prompt, which determines both the content and style of a generated image, contains crucial and comprehensive information regarding the image. When presented with an appealing generated image, individuals are eager to discern the prompt employed for its creation. By accurately identifying the prompts, they can further enhance the corresponding image, such as modifying its content or generating images with a similar style.\"}"}
{"id": "vfzXDRTcF4", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Evaluation results of Prompt Inversion on JourneyDB. We list results on the validation set in the upper half, results on the test set in the lower. For all metrics, the higher, the better.\\n\\n| Models      | Validation | Test | COCO Caption |\\n|-------------|------------|------|--------------|\\n| BLIP-2      | 0.18       | 0.29 | 0.82         |\\n| OPT         | 2.39       | 2.85 | 5.43         |\\n| BLIP-2      | 0.27       | 0.40 | 0.54         |\\n| FlanT5     | 7.19       | 7.69 | 19.94        |\\n| MiniGPT-4  | 5.50       | 6.51 | 6.88         |\\n| Uni-Perceiver v2 | 0.23 | 0.37 | 0.41         |\\n| Uni-Perceiver v2 FT | 20.6 | 4.68 | 8.20        |\\n\\nTable 4: Evaluation results of Image Captioning on JourneyDB. We list the zero-shot results in the upper half, and the fine-tuned results in the lower. For all metrics, the higher, the better. FT denotes \u201cFine-Tune.\u201d\\n\\n| Models          | Validation | Test | COCO Caption |\\n|-----------------|------------|------|--------------|\\n| BLIP-2 OPT      | 0.82       | 2.35 | 0.82         |\\n| BLIP-2 FlanT5   | 0.54       | 2.07 | 0.54         |\\n| Flamingo9B     | 0.94       | 1.39 | 0.94         |\\n| MiniGPT-4      | 2.28       | 2.79 | 2.28         |\\n| Uni-Perceiver v2 | 0.41 | 0.94 | 0.41         |\\n| Uni-Perceiver v2 FT | 8.20 | 3.23 | 8.20        |\\n\\nHowever, predicting the prompts of an image is a challenging task. Existing visual understanding models, such as image-caption models, often fall short in providing a detailed description of the image's main elements, such as the subject, while neglecting other indispensable details like the viewpoint, illumination, or art style.\\n\\nPrompt inversion aims to address this gap, involving the process of taking a single image and predicting the corresponding prompts associated with it. We anticipate that the proposed dataset would further facilitate the development of prompt inversion through the in-depth analysis of the prompts.\\n\\nTo evaluate the effectiveness of prompt inversion, we extend the metrics commonly utilized in image captioning, including Bleu, METEOR, ROUGE, and CIDEr. Additionally, we adopt the approach employed in a related Kaggle competition [46] to calculate the Cosine Similarity of the sentence-transformers features [47].\\n\\nFurthermore, in the supplementary materials, we propose a Question Answering Score (QAS) for evaluating the prompt inversion results. In this paper, we establish a benchmark for the zero-shot prompt inversion task by leveraging state-of-the-art multi-modal models, namely BLIP-2 OPT [4], BLIP-2 FlanT5XL [4], Flamingo9B [3], MiniGPT-4 [38], and Uni-Perceiver v2 [30]. To ensure optimal performance in this novel task, we customize different prompts for each model.\\n\\nWe evaluate these models on the test set of our dataset, and the results are presented in Table 3. During the experiment, we observed that the existing models struggle to capture the intricate details and style-related information of the input image, leading to lower performance compared to conventional datasets.\\n\\nTo verify the effectiveness of our dataset, we fine-tuned Uni-Perceiver v2 for 20 epochs and noted a significant improvement in the prompt inversion task. It is important to note that we followed the training methodology outlined in [30] without tuning the hyperparameters or utilizing data augmentations. This demonstrates that our JourneyDB can complement existing image-text datasets for training prompt inversion models. Nevertheless, it is evident that there is still a substantial way to go in developing a robust and effective prompt inversion model.\\n\\n4.2 Image Caption\\n\\nImage captioning tasks require multi-modal models to generate textual descriptions for the visual content of an image. In comparison to existing image captioning benchmarks such as COCO Caption [10], JourneyDB encompasses both detailed descriptions and high-level summarizations of images, thereby assessing the model's proficiency in fine-grained recognition and holistic understanding.\"}"}
{"id": "vfzXDRTcF4", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Inside a mother ship from the distant future, a big portal is activated, revealing intricate details and neon lights, all captured in cinematic style with 8k resolution and soft and bold lights.\\n\\nFigure 3: Samples from the validation set of JourneyDB captioning. The examples show that existing multi-modal models failed to recognize some key concepts from the AI-generated images.\\n\\nTable 5: Style Retrieval Results. The metric used there is the Recall.\\n\\n| Method     | Validation | Test |\\n|------------|------------|------|\\n| CLIP-ViT-L/14 [2] | 0.65 | 0.47 |\\n|             | 41.72      | 41.33 |\\n\\nWe evaluate various existing multi-modal models on the image captioning sub-task of JourneyDB. The results are presented in Table 4, indicating the challenges faced by multi-modal models trained on natural images in providing accurate descriptions for AI-generated content. The quantitative performance is notably poorer (significantly worse than COCO Caption results) due to two primary factors: GPT-3.5 tends to generate verbose descriptions for the images in JourneyDB, resulting in lengthy ground-truth captions. This discrepancy between lengthy ground-truth captions and shorter predicted captions undermines the quantitative performance. When describing AI-generated images, the focus may differ in terms of concepts such as emotions, human/object attributes, etc., compared to natural images. However, existing image captioning models have not adequately addressed these concepts.\\n\\nWe provide qualitative examples in Fig 3. Existing multi-modal approaches fail to describe key concepts present in the AI-generated content (e.g., Fig 3(b) depicts kids in astronaut suits, Fig 3(d) shows a sad potato). Moreover, some models may hallucinate contents that do not exist in the images (e.g., Open-Flamingo hallucinates objects and text in Fig 3(a, c)).\\n\\n4.3 Style Retrieval\\n\\nWe inhabit a captivating world enveloped in a multitude of vibrant colours and ever-shifting illumination. Artists, in turn, develop their distinct styles to convey their unique perspectives of the world. Elements such as weather conditions, moods, and atmospheres all contribute to the style portrayed in an image, resulting in a complex \u201cstyle system.\u201d As detailed in Section 3.2, we have compiled a comprehensive collection of over 150,000 style words to describe the style-related attributes of images.\"}"}
{"id": "vfzXDRTcF4", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Evaluation results of the content-relevant and style-relevant zero-shot Multiple-Choice Visual Question Answering on JourneyDB. The evaluation metric here is accuracy.\\n\\n| Method   | Validation Test | Content | Style | Content | Style |\\n|----------|-----------------|---------|-------|---------|-------|\\n| Flamingo9B [3] | 32.1% | 31.9% | 35.6% | 41.4% |\\n| MiniGPT-4 [38] | 28.2% | 26.6% | 31.1% | 29.3% |\\n| BLIP-2 | 65.8% | 54.9% | 69.7% | 57.4% |\\n\\nQ1: What type of camera angle is used in this image?\\nA: Close-up, B: Medium, C: Top-down, D: Side-view.\\nBlip-2: A\\nGround Truth: C\\n\\nQ2: What is the dominant visual element in the image?\\nA: Lines, B: Shapes, C: Colors, D: Textures.\\nBlip2: A\\nGround Truth: B\\n\\nQ4: Which animation studio is most associated with the style of this image?\\nA: Disney, B: Dreamworks, C: Pixar, D: Studio Ghibli.\\nBlip2: D\\nGround Truth: C\\n\\nQ3: What type of lighting is used in the image?\\nA: Soft lighting, B: Directional lighting, C: Volumetric lighting, D: Ambient lighting.\\nBlip2: B\\nGround Truth: C\\n\\nQ5: Who are present at the wedding?\\nA: Just the couple, B: A large crowd, C: Only family members, D: No one.\\nBlip-2: A\\nGround Truth: B\\n\\nQ6: What creature is shown in the image?\\nA: A large spider with the head of a fly, B: A giant scorpion with head of butterfly, C: A massive giant crab with the head of a bee, D: A colossal ant with the head of a wasp.\\nBlip2: A\\nGround Truth: C\\n\\nQ8: How is the tree related to the piano in the image?\\nA: The tree is behind the piano, B: The tree is growing out of the piano, C: The tree is on top of the piano, D: The tree and piano are not related.\\nBlip2: D\\nGround Truth: B\\n\\nQ7: What is the main subject of the image?\\nA: A baby cat, B: A musical instrument, C: A group of kittens, D: A fire pit.\\nBlip2: B\\nGround Truth: A\\n\\nFigure 4: Failure cases of BLIP-2 [4] for Multiple-Choice Visual Question Answering.\\n\\nGiven the vast expanse of the style space, identifying the style of a given image poses a significant challenge, even for human observers. Consequently, there is a need for style retrieval techniques to aid in comprehending the style exhibited in an image.\\n\\nDirectly retrieving a style prompt from an extensive pool of candidates is a complex and time-consuming task. Therefore, we employ clustering techniques to group the style prompts into 344 categories, including camera parameters, lighting, artist style, colour schemes, and more, as outlined in Section 3.2. By doing so, we effectively narrow down the search space for style prompt retrieval within each category. To establish a benchmark, we employ CLIP [2] for zero-shot style retrieval evaluation. We extract the features of both the images and the style prompts, subsequently calculating the inner product between the image feature and all candidate style prompts. The results are presented in Table 5. Notably, we observe that conducting retrieval in the overall style prompt space yields significantly low recall. Conversely, the model performs substantially better when performing retrieval in the sub-space of each category.\"}"}
{"id": "vfzXDRTcF4", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.4 Visual Question Answering (VQA)\\n\\nJourneyDB comprises a collection of images encompassing abundant and diverse prompts. These prompts not only encompass stylistic attributes but also describe the visual contents of the generated images. To assess the model\u2019s proficiency in comprehending both style and content of generative data, we establish two tasks: multiple-choice visual question answering (MC-VQA)\\\\[48, 49, 12\\\\]. In the MC-VQA tasks, we utilize GPT-3.5 to generate \u201cStyle-relevant questions\u201d and \u201cContent-relevant questions\u201d along with three distracting options for each question, in addition to the correct answer.\\n\\nThe evaluation metric employed is accuracy, where the model selects its answer from the four options based on the given question and the image. A-OKVQA\\\\[ 12\\\\] highlights that MC-VQA overcomes several inherent challenges of direct answer evaluation, such as ambiguity, which is prevalent in open-ended VQA [16, 17]. The versatility of language expression implies that MC-VQA, by directly matching the selected option with the correct answer, provides a lucid and objective evaluation approach. This characteristic proves advantageous, especially considering the extensive spectrum of answers in our benchmark, encompassing a wide range of descriptions for diverse styles and contents.\\n\\nTo assess the performance of current multimodal models in the zero-shot visual question answering task within our benchmark, we adopt a methodology inspired by recent studies [50, 51, 52, 53]. In this approach, we provide the model with a question and its corresponding candidate answers enumerated with symbols (\u201cA\u201d, \u201cB\u201d, \u201cC\u201d, \u201cD\u201d). By assigning the highest probability to a predicted token (\u201cA\u201d, \u201cB\u201d, etc.), the model selects the associated answer choice as its response.\\n\\nThe evaluation outcomes for the zero-shot multiple-choice visual question answering tasks, specifically the content-relevant and style-relevant tasks, are presented in Table 6. It is evident that the performance of existing multimodal models falls significantly short of satisfactory levels in both the content-relevant and style-relevant MC-VQA tasks. BLIP-2 [4] outperforms Flamingo9B [3] and MiniGPT-4 [38], yet its accuracy remains below 70\\\\%\\\\[440\\\\]. These results highlight the substantial challenges that generative data poses to existing models in comprehending the visual contents and stylistic attributes. Generative data often represents scenes and object compositions that are absent in reality, thereby posing difficulties for multimodal models pre-trained on real images to interpret the visual elements of generated images when answering content-relevant questions. For instance, as illustrated in the second row and fourth column of Figure 4, the model fails to recognize the relationship between the piano and the tree in the image and predicts the option \u201cD: the tree and piano are not related.\u201d This failure arises due to the rarity of scenes depicting a tree growing out of a piano in the real world. In comparison to answering content-relevant questions, the performance of all models generally deteriorates when addressing style-relevant questions. The generation of multiple-choice questions from text prompts encompassing diverse stylistic aspects, such as camera angle, lighting, and artistic styles, enables a comprehensive evaluation of a model\u2019s capacity to identify the stylistic attributes of generative data within JourneyDB. However, previous multimodal models are pre-trained using descriptions derived from captioning real images, thereby lacking exposure to the broad range of stylistic variations prevalent in generative data. Consequently, these models encounter difficulties in recognizing and distinguishing the various styles manifested in generative data. As illustrated in Figure 4, BLIP-2 provides incorrect answers to the style-relevant questions in the first row pertaining to camera angle, visual elements, lighting type, and animation style depicted in the images of JourneyDB.\\n\\n5 Conclusion\\n\\nWe introduce JourneyDB, an extensive benchmark comprising four distinct downstream tasks, aiming to foster advancements in the comprehension of generative content. By providing a platform that facilitates the development of visual understanding in relation to generated images, researchers and practitioners are empowered to drive progress in this field.\\n\\n6 Acknowledgement\\n\\nThanks Mengwei R. for her insightful feedback. Thanks Mingjie Z. for his assistance with data cura-\"}"}
{"id": "vfzXDRTcF4", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\\n\\n[2] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\\n\\n[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.\\n\\n[4] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\\n\\n[5] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023.\\n\\n[6] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10696\u201310706, 2022.\\n\\n[7] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. ArXiv, abs/2204.06125, 2022.\\n\\n[8] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.\\n\\n[9] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014.\\n\\n[10] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.\\n\\n[11] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\\n\\n[12] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part VIII, pages 146\u2013162. Springer, 2022.\\n\\n[13] Christoph Schuhmann, Andreas K\u00f6pf, Richard Vencu, Theo Coombes, and Romain Beaumont. Laion coco: 600m synthetic captions from laion2b-en, 2022.\\n\\n[14] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10674\u201310685, 2021.\\n\\n[15] Flickr. https://www.flickr.com.\\n\\n[16] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425\u20132433, 2015.\\n\\n[17] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195\u20133204, 2019.\\n\\n[18] Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. DiffusionDB: A large-scale prompt gallery dataset for text-to-image generative models. arXiv:2210.14896, 2022.\"}"}
{"id": "vfzXDRTcF4", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[10] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in Neural Information Processing Systems, 34:19822\u201319835, 2021.\\n\\n[20] Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, and Edward Raff. Vqgan-clip: Open domain image generation and editing with natural language guidance. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXVII, pages 88\u2013105. Springer, 2022.\\n\\n[21] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kam-yar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. ArXiv, abs/2205.11487, 2022.\\n\\n[22] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139\u2013144, 2020.\\n\\n[23] Jingtan Piao, Keqiang Sun, Quan Wang, Kwan-Yee Lin, and Hongsheng Li. Inverting generative adversarial renderer for face reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15619\u201315628, 2021.\\n\\n[24] Keqiang Sun, Shangzhe Wu, Zhaoyang Huang, Ning Zhang, Quan Wang, and Hongsheng Li. Controllable 3d face synthesis with conditional generative occupancy fields. In Advances in Neural Information Processing Systems, 2022.\\n\\n[25] Keqiang Sun, Shangzhe Wu, Ning Zhang, Zhaoyang Huang, Quan Wang, and Hongsheng Li. Cgof++: Controllable 3d face synthesis with conditional generative occupancy fields. arXiv preprint arXiv:2211.13251, 2022.\\n\\n[26] Elman Mansimov, Emilio Parisotto, Jimmy Ba, and Ruslan Salakhutdinov. Generating images from captions with attention. CoRR, abs/1511.02793, 2015.\\n\\n[27] Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw: A recurrent neural network for image generation. ArXiv, abs/1502.04623, 2015.\\n\\n[28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR, 2021.\\n\\n[29] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. OFA: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In International Conference on Machine Learning, pages 23318\u201323340. PMLR, 2022.\\n\\n[30] Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision and vision-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2691\u20132700, 2023.\\n\\n[31] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng Li, Xiaohua Wang, and Jifeng Dai. Uni-perceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16804\u201316815, 2022.\\n\\n[32] Jinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, and Jifeng Dai. Uni-perceiver-moe: Learning sparse generalist models with conditional moes. In Advances in Neural Information Processing Systems, 2022.\\n\\n[33] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-IO: A unified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916, 2022.\\n\\n[34] OpenAI. GPT-4 technical report. ArXiv, abs/2303.08774, 2023.\\n\\n[35] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023.\"}"}
{"id": "vfzXDRTcF4", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023.\\n\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.\\n\\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\\n\\nLijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving clip training with language rewrites. In NeurIPS, 2023.\\n\\nYonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and Dilip Krishnan. Stablerep: Synthetic images from text-to-image models make strong visual representation learners. arXiv preprint arXiv:2306.00984, 2023.\\n\\nKunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023.\\n\\nJoin the midjourney discord server! https://discord.com/invite/midjourney.\\n\\nDiscord | your place to talk and hang out. https://discord.com/.\\n\\nDiscordchatexporter. https://github.com/tyrrrz/discordchatexporter.\\n\\nWill Cukierski Ashley Chow, inversion. Stable diffusion - image to prompts, 2023.\\n\\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019.\\n\\nYuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4995\u20135004, 2016.\\n\\nJiaying Lu, Xin Ye, Yi Ren, and Yezhou Yang. Good, better, best: Textual distractors generation for multiple-choice visual question answering via reinforcement learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4921\u20134930, 2022.\\n\\nJoshua Robinson, Christopher Michael Rytting, and David Wingate. Leveraging large language models for multiple choice question answering. arXiv preprint arXiv:2210.12353, 2022.\\n\\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\\n\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\\n\\nValentin Li\u00e9vin, Christoffer Egeberg Hother, and Ole Winther. Can large language models reason about medical questions? arXiv preprint arXiv:2207.08143, 2022.\\n\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\\n\\nMidjourney. https://www.midjourney.com/.\\n\\nXiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Better aligning text-to-image models with human preference, 2023.\"}"}
{"id": "vfzXDRTcF4", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873\u201312883, 2021.\\n\\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\\n\\nMing Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via hierarchical transformers. Advances in Neural Information Processing Systems, 35:16890\u201316902, 2022.\\n\\nXingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text, images and variations all in one diffusion model. arXiv preprint arXiv:2211.08332, 2022.\\n\\nTim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization. arXiv preprint arXiv:2110.02861, 2021.\\n\\nEpicdiffusion. https://civitai.com/models/3855/epic-diffusion.\\n\\nDalle mini. https://github.com/borisdayma/dalle-mini.\\n\\nDreamlike photoreal 2.0. https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0.\\n\\nDeliberate. https://civitai.com/models/4823/deliberate.\\n\\nY Zhou, R Zhang, C Chen, C Li, C Tensmeyer, T Yu, J Gu, J Xu, and T Sun. Lafite: Towards language-free training for text-to-image generation. arxiv 2021. arXiv preprint arXiv:2111.13792, 2021.\\n\\nRealistic vision. https://civitai.com/models/4201/realistic-vision-v20.\\n\\nXingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su, and Qiang Liu. Fusedream: Training-free text-to-image generation with improved clip+ gan space optimization. arXiv preprint arXiv:2112.01573, 2021.\\n\\nMajicmix realistic. https://huggingface.co/sinkinai/majicmix-realistic-v5.\\n\\nChilloutmix. https://huggingface.co/swl-models/chilloutmix.\\n\\nOpenjourney. https://openjourney.art.\"}"}
{"id": "vfzXDRTcF4", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendices\\n\\nA Data samples\\n\\nIn this section, we randomly sample instances from the dataset to provide a visualization for users. Shown in the Figure 5 are the uncurated samples of JourneyDB. The 5 columns from left to right are images generated by Midjourney, the corresponding prompts, the captions, style-relative and content-relative VQA annotation. We mark the content-relative words with green color, and the style-relative words with orange color.\\n\\nB Details of Data Annotation\\n\\nIn this section, we introduce the details of data annotation, including image-prompt consistency filtering, downstream visual understanding annotations, and style clustering.\\n\\nB.1 Image-Prompt consistency filtering\\n\\nTo build a clean test set, we hire a team of 40 professional annotators to mark the prompt word not presented in the corresponding image. Specifically, given a picture and a corresponding piece of text, the annotators are required to find words or phrases in the text that do not match the picture.\\n\\nDefinition of \u201cmismatch\u201d: 1) Content that exists in the text but is missing (not reflected) in the picture. For example, if the text is \u201capples and bananas are placed on a table\u201d, and only apples are placed on the table in the picture, then mark \u201cand bananas\u201d. 2) It exists in the text, but the content in the picture is inconsistent with the text. For example, if the text is \u201ca red apple is placed on a table\u201d, but the picture is a green apple, then mark \u201cred\u201d. We show the examples we use in the annotation document in Table 7.\\n\\n| Num Images | Text Analysis |\\n|------------|--------------|\\n| 1          | face portraits of two mechanical cyborg warriors facing off, in the style of a UFC poster, cinematic blue orange lighting, volumetric lighting, smoke, hyper detailed armor, hard surface, neon light tubes, realistic, intricate details, symetrical | All elements are fully represented in the picture. Thus this pair passes review without annotation. |\\n| 2          | a cute 1950s style alien with glowing green eyes waving | All elements are well represented in the picture. Thus this pair passes review without annotation. |\\n| 3          | Pok\u00e9mon picnic, detailed, realistic | \u201cPok\u00e9mon\u201d are not clearly represented in the picture, so mark it. \u201cpicnic\u201d, \u201cdetailed\u201d, and \u201crealistic\u201d are all reflected and not marked. |\\n| 4          | sketch, a blue glowing gem with two pieces of rope tied on each side, fantasy, Lord of The Rings, watercolor | \u201cwith two pieces of rope tied on each side\u201d it does not conform to the effect shown in the picture, mark it. \u201cLord of The Rings\u201d does not manifest. \u201csketch\u201d, \u201ca blue glowing gem\u201d and \u201cfantasy, watercolor\u201d are all reflected and not marked. |\"}"}
{"id": "vfzXDRTcF4", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prompts\\nImages\\nCaption\\nVQA (Style)\\n\\nAmanda Seyfried portrays a forest witch in this image, dressed in a gown made of green moss, standing before a swamp forest. She gazes directly at the camera, capturing a portrait view that accentuates her captivating presence.\\n\\nQ1: Which artists' style inspired the visual design of this image?\\nA: Mignola, B: Diterlizzi, C: Vess, D: None of the above.\\n\\nQ2: What type of view does the image provide?\\nA: Landscape view, B: Bird's-eye view, C: Portrait view, D: Close-up view.\\n\\nAn overgrown rococo carousel with many colorful animals in a wonderland dream nightmare, presented in a photorealistic style.\\n\\nQ1: What style is depicted in the image?\\nA: Impressionism, B: Realism, C: Quantum, D: Surrealism.\\n\\nA mesmerizing image capturing the ethereal beauty of quantum particles in a state of superposition, their wave functions delicately intertwined in a mysterious and transcendent dance.\\n\\nQ1: What style is the carousel depicted in?\\nA: Minimalist, B: Rococo, C: Gothic, D: Baroque.\\n\\nQ2: What type of animals are on the carousel?\\nA: Birds, B: Fish, C: Lions, D: Horses.\\n\\nA plate of Chinese and Thai food is elegantly presented on a gaiwan porcelain adorned with dragon ornaments.\\n\\nQ1: What type of restaurant is shown in the image?\\nA: Italian trattoria, B: Luxury oriental, C: Mexican taqueria, D: French bistro.\\n\\nQ2: What is the material of the plate used?\\nA: Porcelain, B: Stoneware, C: Glass, D: Melamine.\\n\\nPrompts Images Caption VQA (Style)\\n\\nblack pharaoh panther god, crystal purple filigree, insanely detailed and intricate, hypermaximalist, elegant, ornate, hyper realistic, super detailed, 8K\\n\\nA hypermaximalist, elegant, and ornate 8K image featuring a stunningly detailed depiction of a black pharaoh panther god surrounded by crystal purple filigree.\\n\\nQ1: What is the level of detail in the image?\\nA: Minimal, B: Moderate, C: High, D: Extreme.\\n\\nQ2: How would you describe the style of the image?\\nA: Minimalistic, B: Surreal, C: Hyperrealistic, D: Abstract.\\n\\nFilm still of rabbit sitting at the counter of an art-deco loungebar, drinking whisky from a tumbler glass, in the style of \\\"Blade Runner (1982), velvety, soft lights, long shot, high quality photo, sharp, look at that detail -- v 4 -- quality 2 -- ar 1:1\\n\\nA rabbit sits at the counter of an art-deco loungebar, drinking whisky from a tumbler glass.\\n\\nQ1: In which film's style is this image depicted?\\nA: Blade Runner (1982), B: The Great Gatsby (2013), C: Metropolis (1927), D: Inception (2010).\\n\\nQ1: What is the main subject inside the computer?\\nA: Robot, B: Astronaut, C: Flower, D: Rocket.\\n\\nAn astronaut's hand is reaching out from inside a 2D retro computer towards the viewer, hand open, in a flat illustration style with vector art, featuring thick lines, bright colors, and a simple design outlined in black.\\n\\nQ1: What type of illustration style is used in the image?\\nA: Watercolor, B: 3D rendering, C: Vector art, D: Impressionism.\"}"}
