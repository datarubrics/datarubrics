{"id": "qmvtDIfbmS", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.3.2 Composition\\n\\nWhat do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?\\n\\nThe instances in the dataset represent two main types of data. The first type includes elements for constructing the game environment of scripted murder mystery games, such as overall scripts, character scripts, and graphic and textual clues. The second type includes data for evaluating agents, comprising multiple-choice questions and open-ended questions.\\n\\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?\\n\\nOur script dataset is a sample from an existing collection of real-world scripted murder mystery game data, while the evaluation dataset consists of new annotations created by us.\\n\\nWhat data does each instance consist of?\\n\\nIn the first part of the script dataset, each instance consists of background script information, character script information, and graphic and textual clues within the script. In the second part, the evaluation dataset, each multiple-choice question instance includes a set of images, a long text passage, a question with options, and the correct answer. Each open-ended question instance includes a question, and the correct answer.\\n\\nIs there a label or target associated with each instance?\\n\\nYes, there is a label or target associated with each instance. In the script dataset, each instance includes roles, clues, and context information, which can be considered as targets or labels depending on the task. In the evaluation dataset, the multiple-choice questions and open-ended questions have correct answers that serve as the targets or labels for each instance.\\n\\nIs any information missing from individual instances?\\n\\nNo, there is no information missing from individual instances. Each instance in both the script dataset and the evaluation dataset is complete with all necessary information.\\n\\nAre relationships between individual instances made explicit (e.g., users' movie ratings, social network links)?\\n\\nNo, the relationships between individual instances are not explicitly defined. Each instance in the script and evaluation datasets is treated independently without direct links to other instances.\\n\\nAre there recommended data splits (e.g., training, development/validation, testing)?\\n\\nNo, there are no recommended data splits. Users of the dataset can define their own splits based on their specific needs and goals.\\n\\nAre there any errors, sources of noise, or redundancies in the dataset?\\n\\nYes, there are potential sources of noise and redundancies in the dataset. Since the script dataset is sourced from existing open-source scripted murder mystery games, some scripts may contain inconsistencies or ambiguities that can introduce noise despite our best efforts to minimize them.\\n\\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?\\n\\nThe dataset is not entirely self-contained as it includes scripts sourced from real-world scripted murder mystery games. While these scripts are included in the dataset, they originate from publicly available resources. Despite this, all evaluation data and annotations are self-contained within the dataset and do not rely on external resources.\\n\\nDoes the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor\u2013patient confidentiality, data that includes the content of individuals' non-public communications)?\\n\\nNo, the dataset does not contain any data that might be considered confidential. The scripts are sourced from publicly available scripted murder mystery games, and all evaluation data and annotations were created specifically for this dataset.\\n\\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?\\n\\nThe dataset does not intentionally contain any data that might be offensive, insulting, threatening, or anxiety-inducing. However, given the nature of scripted murder mystery games, some scripts may include themes of crime, violence, or other mature content that could be sensitive to some users. We have made efforts to review and filter the content to minimize potential issues. Additionally, if users identify any content they find problematic, we encourage them to report it to us, and we will take steps to replace or remove such content as necessary.\\n\\nDoes the dataset identify any subpopulations (e.g., by age, gender)?\\n\\nNo\"}"}
{"id": "qmvtDIfbmS", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset?\\n\\nNo, it is not possible to identify individuals, either directly or indirectly, from the dataset. The dataset consists of fictional characters and scenarios from scripted murder mystery games and does not include any real personal information or data that could be used to identify natural persons.\\n\\nDoes the dataset contain data that might be considered sensitive in any way (e.g., data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)?\\n\\nNo. The dataset focuses solely on fictional characters and scenarios from scripted murder mystery games, without including any real or sensitive personal information.\\n\\nA.3.3 Collection Process\\n\\nHow was the data associated with each instance acquired?\\n\\nThe data associated with each instance was acquired through two main methods. The script dataset was sourced from a collection of publicly available, real-world scripted murder mystery game scripts. The evaluation dataset, on the other hand, was created by our team, who carefully annotated and developed multiple-choice and open-ended questions based on the content of the scripts.\\n\\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)?\\n\\nThe data was collected using a combination of manual curation and various software tools. For the game scripts, we downloaded them from publicly available online resources. We then used OCR (Optical Character Recognition) technology to process and extract the script content. The images within the scripts were processed using image cropping techniques to obtain the clue information. The evaluation dataset was created by our team. We manually annotated the questions and correct answers based on the script content. For generating distractor options for the multiple-choice questions, we used GPT-4, and our team further refined these options to ensure quality and relevance. The entire process involved meticulous manual work complemented by advanced software tools to ensure accuracy and consistency.\\n\\nIf the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?\\n\\nThe dataset is a sample from a larger set, and the sampling strategy was deterministic. We enlisted the expertise of seasoned murder mystery game experts to ensure quality and applicability. The scripts were sourced from industry-recognized creative teams and platforms, with selection criteria focusing on:\\n\\n\u2460 Scientific Integrity: Excluding scripts with supernatural phenomena to ensure realistic resolutions.\\n\u2461 Content Complexity: Choosing scripts with high reasoning complexity to test deductive capabilities.\\n\u2462 Logical Coherence: Ensuring logical soundness with balanced evidence and clues.\\n\\nWho was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?\\n\\nIn this study, five seasoned experts from the murder mystery game domain were invited to select and extract suitable scripts. Each expert screened the script library and chose the top 50 scripts that best met our criteria, receiving a compensation of $100 each. The author personally reviewed all selected scripts to ensure quality and thematic alignment. For the question annotation phase, we employed ten experienced murder mystery game experts. They were compensated at $0.50 per question for general annotations and $2 per intricate reasoning chain. Additionally, three experts were hired to review the annotations at $0.20 per question to ensure accuracy and reliability.\\n\\nOver what timeframe was the data collected?\\n\\nThe data was collected over a period of three months, from March 2024 to May 2024. During this time, the script selection, annotation, and review processes were conducted to ensure the quality and applicability of the dataset.\\n\\nWere any ethical review processes conducted (e.g., by an institutional review board)?\\n\\nNo\\n\\nDid you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?\\n\\nWe did not collect the data directly from individuals. The script data was obtained from publicly available sources on websites, curated and selected by seasoned murder mystery game experts. The evaluation data, including annotations and questions, was created by our team of experts based on the sourced scripts.\"}"}
{"id": "qmvtDIfbmS", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Were the individuals in question notified about the data collection? N/A\\n\\nDid the individuals in question consent to the collection and use of their data? N/A\\n\\nIf consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? N/A\\n\\nHas an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? No\\n\\nA.3.4 Preprocessing/cleaning/labeling\\n\\nWas any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? Yes, extensive preprocessing, cleaning, and labeling of the data were performed. For the script dataset, we used OCR technology to extract text from images and processed the images for clarity. We also performed manual cleaning to remove any inconsistencies and ensure logical coherence. For the evaluation dataset, we annotated questions and correct answers, and generated distractor options using GPT-4, followed by manual refinement.\\n\\nWas the \\\"raw\\\" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? Yes, the raw data was saved alongside the preprocessed, cleaned, and labeled data to support unanticipated future uses and to ensure transparency and reproducibility.\\n\\nIs the software that was used to preprocess/clean/label the data available? No\\n\\nA.3.5 Uses\\n\\nHas the dataset been used for any tasks already? No, this dataset has not been utilized for any tasks before the baseline experiments conducted in this paper.\\n\\nIs there a repository that links to any or all papers or systems that use the dataset? No\\n\\nWhat (other) tasks could the dataset be used for? The dataset can be used to test multimodal agents\u2019 abilities to perceive, reason, and make decisions in dynamic, incomplete information environments. It aims to assess how well agents can complete tasks in a manner akin to human behavior, addressing the significant challenge of developing a theory of mind to navigate complex scenarios.\\n\\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? We will continue to maintain the dataset and attempt to expand its scale to achieve a more comprehensive evaluation.\\n\\nAre there tasks for which the dataset should not be used? Yes, the dataset should not be used for tasks that require large-scale training data due to its limited size.\\n\\nA.3.6 Distribution\\n\\nWill the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? The dataset is open-source, and we will also provide the scripts we used. However, it is important to note that we do not claim any rights over the scripts.\\n\\nHow will the dataset be distributed (e.g., tarball on website, API, GitHub)? The dataset will be distributed via a GitHub repository.\\n\\nWhen will the dataset be distributed? The dataset will be made open-source after a final review by our team.\\n\\nWill the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? No\\n\\nHave any third parties imposed IP-based or other restrictions on the data associated with the instances? No\\n\\nDo any export controls or other regulatory restrictions apply to the dataset or to individual instances? No\\n\\n19\"}"}
{"id": "qmvtDIfbmS", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.3.7 Maintenance\\n\\nWho will be supporting/hosting/maintaining the dataset?\\nAll the authors.\\n\\nHow can the owner/curator/manager of the dataset be contacted (e.g., email address)?\\nContact by email at any time.\\n\\nIs there an erratum?\\nNo\\n\\nWill the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?\\nIt may be updated, and if necessary, we will propose modifications on our GitHub.\\n\\nIf the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)?\\nNo\\n\\nWill older versions of the dataset continue to be supported/hosted/maintained?\\nNo\\n\\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?\\nYes, if others wish to extend, augment, build on, or contribute to the dataset, they can do so by submitting pull requests or opening issues on our GitHub repository. We encourage community contributions and aim to review and integrate them in a timely manner to enhance the dataset.\\n\\nA.4 Additional Details on the Evaluation Benchmark\\n\\nA.4.1 Prompts\\n\\nTo enable LMAs to perform within our WhodunitBench, we introduce a series of structured prompts. The categories of prompt templates we use are detailed in the table. The specific content for each prompt type is presented in Figures 11, 12, 13 and 14. The symbol within the table links directly to the corresponding detailed contents.\\n\\n| Category | Name       | Symbol | Description                                      |\\n|----------|------------|--------|--------------------------------------------------|\\n| System   | Rules      | I_e    | Describes the rules and procedures of the Game   |\\n|          | Script     | I_S    | Provide the agent with its role details          |\\n|          | Live-info  | I_d    | Real-time information about the current game, such as dialog information |\\n| Action   | Introduction| I_i    | The action that involves asking the agent to introduce itself |\\n|          | Discussion | I_D    | The action that prompts the agent to discuss and choose an action |\\n|          | Reasoning  | I_r    | The action that directs the agent to identify the murderer and their motive |\\n|          | Voting     | I_v    | The action that directs the agent to vote the murderer |\\n| Evaluation|           | RP     | Prompts used to evaluate the naturalness of agent role-playing |\\n|          |            | SPC    | Prompts used to evaluate the degree of agent role immersion |\\n|          |            | CMD    | Prompts used to score the agent's final reasoning on the motive and method of the crime |\\n\\nA.4.2 Additional Examples of Dialogue Content\\n\\nFigures 15, 16 and 17 display examples of dialogue content generated by LMAs at various stages of the game.\\n\\nA.4.3 Human Performance\\n\\nHuman performance serves as an upper bound for our benchmark. To obtain more rigorous and robust results, we plan to include a wider range of participants with diverse skill levels in future evaluations. And we have developed an interface that allows human participants to directly engage with different LMAs within a murder mystery game scenario. This setup not only offers participants a tangible sense of the differences between LMAs but also furnishes data that facilitates an in-depth analysis of human and agent behavior patterns, decision-making processes, and the efficacy of human-agent collaboration. These insights are invaluable for the continued development of intelligent systems.\"}"}
{"id": "qmvtDIfbmS", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.5 Author Statement\\n\\nThe scripts used in this study were collected from publicly available online websites. All scripts were gathered within the scope of public accessibility, ensuring compliance with relevant data usage and privacy policies. We acknowledge that all intellectual property rights of the collected scripts belong to the original authors or platforms, and we thank them for creating and sharing these resources. These resources are used solely for academic research, and we pledge not to use this data for any purposes unrelated to research. The annotated data is marked by our team, and we own the copyright.\"}"}
{"id": "qmvtDIfbmS", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 6: Detailed role scripts of Figure 5.\"}"}
{"id": "qmvtDIfbmS", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Additional examples of murder mystery game scripts utilized in our dataset.\"}"}
{"id": "qmvtDIfbmS", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: Additional examples of murder mystery game scripts utilized in our dataset.\"}"}
{"id": "qmvtDIfbmS", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Additional examples of multi-step reasoning QA and corresponding reasoning chains.\"}"}
{"id": "qmvtDIfbmS", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: Additional examples of multi-step reasoning QA and corresponding reasoning chains.\"}"}
{"id": "qmvtDIfbmS", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11: System prompts for game rule introduction.\"}"}
{"id": "qmvtDIfbmS", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 12: System prompts for role scripts and live information introduction.\"}"}
{"id": "qmvtDIfbmS", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13: Action prompts for guiding LMAs in self-introduction, discussion, reasoning and voting.\"}"}
{"id": "qmvtDIfbmS", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 14: Evaluation prompts for assessing RP, CMD and SPC metrics.\"}"}
{"id": "qmvtDIfbmS", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 15: Examples of dialogue content generated by LMAs in the self-introduction stage.\"}"}
{"id": "qmvtDIfbmS", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 16: Examples of dialogue content generated by LMAs for sharing clues in the discussion stage.\"}"}
{"id": "qmvtDIfbmS", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 17: Examples of dialogue content generated by LMAs for battle in the discussion stage.\"}"}
{"id": "qmvtDIfbmS", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WhodunitBench: Evaluating Large Multimodal Agents via Murder Mystery Games\\n\\nJunlin Xie, Ruifei Zhang, Zhihong Chen, Xiang Wan, Guanbin Li\\n\\nThe Chinese University of Hong Kong, Shenzhen\\nShenzhen Research Institute of Big Data\\nSun Yat-sen University\\nPeng Cheng Laboratory\\nGuangDong Province Key Laboratory of Information Security Technology\\n\\n{junlinxie,ruifeizhang,zhihongchen}@link.cuhk.edu.cn\\nwanxiang@sribd.cn, liguanbin@mail.sysu.edu.cn\\n\\nAbstract\\n\\nRecently, large language models (LLMs) have achieved superior performance, empowering the development of large multimodal agents (LMAs). An LMA expected to perform practical tasks must possess a range of capabilities, including multimodal perception, interaction, reasoning, and decision-making skills. However, existing benchmarks are limited in assessing compositional skills and actions.\"}"}
{"id": "qmvtDIfbmS", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"demanded by practical scenarios, where they primarily focused on single tasks and static scenarios. To bridge this gap, we introduce WhodunitBench, a benchmark rooted from murder mystery games, where players are required to utilize the aforementioned skills to achieve their objective (i.e., identifying the 'murderer' or hiding themselves), providing a simulated dynamic environment for evaluating LMAs. Specifically, WhodunitBench includes two evaluation modes. The first mode, the arena-style evaluation, is constructed from 50 meticulously curated scripts featuring clear reasoning clues and distinct murderers; The second mode, the chain of evaluation, consists of over 3000 curated multiple-choice questions and open-ended questions, aiming to assess every facet of the murder mystery games for LMAs. Experiments show that although current LMAs show promising performance in basic perceptual tasks, they are insufficiently equipped for complex multi-agent collaboration and multi-step reasoning tasks. Furthermore, the full application of the theory of mind to complete games in a manner akin to human behavior remains a significant challenge. We hope this work can illuminate the path forward, providing a solid foundation for the future development of LMAs.\\n\\nOur WhodunitBench is open-source and accessible at: https://github.com/jun0wanan/WhodunitBench-Murder_Mystery_Games.\\n\\n1 Introduction\\n\\nLarge multimodal agents (LMAs) [29, 21, 27, 11] are systems capable of perceiving its environment and making decisions based on these perceptions to achieve specific goals within the multimodal context driven by large language models (LLMs). LMAs are anticipated to handle diverse and challenging tasks that demand a broad range of capabilities, including low-level multimodal perception, high-level cognition (e.g., multi-step reasoning), role-playing for interactive engagement and deliberative decision-making.\\n\\nGiven these diverse capabilities, the evaluation of LMAs varies widely across research domains. Some studies prioritize the agents' competency in executing complex internet-based tasks [8, 10, 16, 30], while others focus on assessing their reasoning and decision-making abilities [14, 15, 26, 6]. Additionally, a significant body of research explores these agents' capacities for long-term planning and execution [35, 28, 33]. However, it is challenging to design such an evaluation benchmark to evaluate the various capabilities of LMAs within the same environment. We categorize the capabilities into the following four classes:\\n\\n- **Multi-modal Perception** is the most basic ability for LMAs, which requires LMAs to perceive information from the multimodal environment (e.g., vision and language).\\n- **Interaction** requires LMAs, whether through role-playing or direct engagement, to communicate with the environment or other agents to gather essential information for task completion.\\n- **Reasoning** requires LMAs to combine their internal knowledge with newly gathered information to perform long-chain, multi-step reasoning.\\n- **Decision Making and Goal Achievement** requires LMAs to establish clear goals and make independent decisions in response to environmental changes. This autonomous decision-making is crucial for effectively navigating and completing tasks in dynamic settings.\\n\\nInterestingly, murder mystery games [5, 12, 3], a genre of party games, offer a unique and covert opportunity to evaluate LMAs across the aforementioned dimensions.\\n\\nAs illustrated in Figure 1, a murder mystery game unfolds in a virtual world crafted by multiple players, with the goal being to identify the 'murderer' through the following procedure:\\n\\n- **Initialization Phase**: Players need to perceive multimodal information, including extensive script text and various types of image clues, while role-playing their assigned characters to present this information.\\n- **Discussion Phase**: Players need to role-play their own roles to interact with the environments or other players to get more clues. During this process, they need to perform decision making to assess the authenticity of information and to gather more details to supplement and refine the tasks required for each role.\\n\\n*https://en.wikipedia.org/wiki/Murder_mystery_game*\"}"}
{"id": "qmvtDIfbmS", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Detailed comparative analysis of our benchmark with others across multiple dimensions. Specifically, \u201cIncomplete Information\u201d refers to cases where an agent\u2019s information includes only a portion of what is required for reasoning, with the remaining information needing to be acquired through effective interactions. Meanwhile, \u201cOnline Competition\u201d denotes direct, real-time, head-to-head matches between agents in a dynamic environment.\\n\\n| Benchmarks       | Multi-Modal | Multi-step Reasoning | Role-play Reasoning |\\n|------------------|-------------|----------------------|---------------------|\\n| DDD [26]         | Incomplete Information | Isolated Evaluation |                    |\\n| AVALONBENCH [15] | Incomplete Information | Online Competition |                    |\\n| GAIA [16]        | Isolated Evaluation   |                      |                    |\\n| VisualWebArena [13] | Isolated Evaluation |                      |                    |\\n| WorldQA [34]     | Complete Information  | Isolated Evaluation |                    |\\n| MCOT [7]         | Complete Information  | Isolated Evaluation |                    |\\n| Rolellm [23]     | Isolated Evaluation   |                      |                    |\\n| SOTOPIA [36]     | Isolated Evaluation   |                      |                    |\\n| WhodunitBench (ours) | Incomplete Information | Online Competition & Chain of Evaluation |                    |\\n\\n- **Reasoning Phase**: Players need to reason over the information collated from the previous two phrases, always involving complex multi-step multi-modal reasoning.\\n- **Voting Phase**: Ultimately, through a voting process involving all players, the \u2018murderer\u2019 is determined. This can evaluate if the players achieve their goals (i.e., identifying the \u2018murderer\u2019 or hiding themselves).\\n\\nTherefore, based on this, we introduce a comprehensive benchmark via murder mystery games designed to evaluate LMAs (named WhodunitBench) in this paper. Table 1 presents the primary characteristics of our benchmark in comparison to others. Specifically, we propose two evaluation modes: (1) Arena-style Evaluation, which simulates real gameplay by having agents act as players in one-on-one online competitions, uses their win rate as the primary evaluative metric. (2) Chain of Evaluation, which provides a comprehensive analysis of agent performance by designing and annotating over 3,000 multiple-choice questions and brief answer sentences. In this evaluation, each metric is designed to align with the game environment while comprehensively supplementing previous assessments.\\n\\nWe select five representative LMAs, including Yi-Vision [32], Qwen-VL-Plus [4], Gemini-pro-vision [19], Claude-Opus [2] and GPT-4V [1] and conduct extensive experiments on our WhodunitBench. Experimental findings reveal that even the advanced GPT-4V [1], which attains the highest win rate in the online arena, still encounters challenges in successfully completing this game. Hallucinations [17], failure to truly understand the script, and difficulty in immersing into roles are its primary error manifestations. Our Chain of Evaluation (CoE) offers more insights for researchers, highlighting that while LMAs typically perform well in basic perception, they struggle with complex multi-modal reasoning and effective interaction within role-playing scenarios.\\n\\nUltimately, the contributions of our paper are three-fold:\\n\\n- **We propose to use murder mystery games as the environments to assess a variety of abilities of LMAs. To this end, we design a benchmark, called WhodunitBench, consisting of two modes: an online battle arena and a chain of evaluation.**\\n- **We curate evaluation samples in two modes: 50 scripted scenarios using win rate for direct confrontation between LMAs, and over 3,000 multiple-choice and open-ended questions to quantify specific capabilities, complementing the win rate assessment with detailed skill evaluations.**\\n- **Experiments conducted on WhodunitBench demonstrate that existing state-of-the-art LMAs struggle in dynamic scenarios and complex composition tasks. Against our naively designed agent, these agents achieve a maximum win rate of only 25%, and their scores for role-play interaction barely exceed 20 points.**\\n\\n### 2 Related Work\\nEvaluating Agent. As LLMs become increasingly prevalent, the development of intelligent agents and the benchmarks for evaluating them continue to evolve [16, 8, 10, 15]. Previous benchmarks have primarily focused on simple yet tedious web-based tasks [16, 30, 8] aimed at developing agents...\"}"}
{"id": "qmvtDIfbmS", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"capable of managing repetitive aspects of human online activities. Besides, environments such as \\\"Werewolf\\\" \\\\[15, 22\\\\] are used to assess agents' strategic and decision-making skills, while other benchmarks \\\\[28, 35\\\\] evaluate long-term strategy and adaptability in specialized scenarios. In contrast, our proposed benchmark evaluates agents in realistic scenarios, where they must simultaneously employ multiple skills rather than focusing on a single ability in a controlled lab environment. More importantly, these skills closely mirror those humans rely on when completing tasks in the real world. This includes the perception and understanding of multimodal content, gathering additional information through interactions with the surrounding environment or other individuals, and finally, integrating this information with prior knowledge to carry out multi-step analysis, reasoning, and decision-making under incomplete information to accomplish their tasks.\\n\\nEvaluating LMAs on Gaming Platforms.\\n\\nGames \\\\[11, 9\\\\], with their simple rules, clear standards, controllable difficulty, and limited scope for action or observation, are increasingly being used as benchmarks for evaluation agents. In addition to the Werewolf-style text games previously mentioned, studies have also explored using games like \\\"Red Dead Redemption II\\\" \\\\[18\\\\] and various open-world environments \\\\[31, 24\\\\] to evaluate the capabilities of LMAs. Employing these games for testing generally demands significant resources and time. Some researchers also have suggested employing murder mystery games as a more efficient alternative for testing \\\\[ 26\\\\]. They primarily assessed text-based agents using relatively straightforward evaluation methods, focusing on multiple-choice questions. In contrast, our evaluation system not only offers two distinct assessment methods but also integrates a range of question types in the second method, particularly emphasizing multi-step multi-modal long-chain reasoning questions. This comprehensive evaluation system fully leverages the scripted murder mystery platform to test agents' abilities in dynamic, information-incomplete environments, closely mirroring human performance.\\n\\n3 WhodunitBench: Construction\\n\\nIn this section, we describe the construction of WhodunitBench, which features an online competitive arena that simulates a realistic gameplay experience, as well as the CoE framework designed to assess LMAs' capabilities through a sequence of \\\"Perception - Role-playing Interaction - Cognition\\\" aligned with the respective stages of gameplay.\\n\\n3.1 Constructing Arena\\n\\nData Collection:\\nThe construction of different games relies on diverse scripts, making the selection and collection of these scripts particularly crucial. We enlisted the expertise of seasoned murder mystery game experts to ensure the quality and applicability of the selected scripts. These scripts were sourced primarily from industry-recognized creative teams and platforms. We established clear selection criteria focused on three key aspects:\\n\\n\u2022 Scientific Integrity: We have systematically excluded scripts incorporating metaphysical elements, particularly temporal displacement and consciousness transference. This methodological approach ensures that murder mysteries within these scripts remain grounded in empirical logic and scientific principles, thus maximizing operational viability and narrative credibility.\\n\\n\u2022 Content Complexity: We chose scripts with a higher degree of reasoning complexity to thoroughly test the deductive capabilities of LMAs.\\n\\n\u2022 Logical Coherence: We ensured all scripts were logically sound, with evidence and clues distributed in a balanced and reasonable manner.\\n\\nData Quality Control:\\nWe conducted a systematic review and optimization of the 50 real scripts collected. Initially, we ensured that the extracted script sections were complete, and we verified the fluidity and grammatical correctness of the text. Subsequently, we confirmed the completeness and integrity of the visual and textual clues within the scripts. Lastly, we examined the consistency of the timeline and the sequence of events, ensuring the logical coherence and rational progression of the plot. Following the comprehensive selection and rigorous review processes delineated above, we curated and refined a total of 50 scripts that conformed to our stringent criteria. The distribution overview of the number of roles in the script is shown in Figure 2 (b). These scripts were utilized to construct the online competitive arena.\"}"}
{"id": "qmvtDIfbmS", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Statistics of the proposed dataset: (a) Distribution of perception QA; (b) Distribution of the number of roles in the scripts; (c) Distribution of reasoning steps for cognition assessments.\\n\\nFigure 3: Data generation and annotation: (a) Examples of annotated ground truth reasoning chains; (b) Multiple-choice question generation process; (c) Interactive statement annotation process.\\n\\n3.2 Constructing Chain of Evaluation (CoE) Dataset\\n\\n3.2.1 Perception Question Types: The evaluation of perception consists of multiple-choice questions categorized into three types: (1) Text-rich Image Questions (TRI-QA): These questions primarily involve text-based clues presented as images within the game, particularly those containing only text. (2) Media-rich Image Questions (MRI-QA): These questions primarily concern image clues within the game that contain both rich textual and visual elements. (3) Long Script Questions (LS-QA): These questions pertain to the textual content embedded within the game's script and role's script.\\n\\nQuestion Statistics: There are a total of 1,911 multi-choice questions for perception assessment, categorized into 283 long script questions, 1,103 text-rich image questions, and 525 media-rich image questions. The distribution is illustrated in Figure 2(a).\\n\\n3.2.2 Role-play Interaction Question Types: To evaluate the role-play interaction capability of LMAs, we primarily annotated two types of data to serve as the ground truth for our assessment: (1) The first category comprises a collection of statements containing key clues in each script. (2) The second category consists of the core roles within each script.\"}"}
{"id": "qmvtDIfbmS", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As shown in Figure 3(c), we compile all key clues necessary to solve each murder mystery in the script, encompassing both direct and indirect clues. These clues serve as ground truth statements for evaluating the effectiveness of LMAs in their role-play and for advancing the game's progression during the discussion phase. Additionally, we identify the core roles in each script, whose personal scripts contain critical clues for identifying the murderer. For instance, the clue \\\"Character 2 was standing outside shooting\\\" appears solely in Character 1's script, marking Character 1 as a core role.\\n\\n3.2.3 Cognition Question Types\\n\\nThe design of cognitive evaluation questions primarily consists of two types: (1) Multi-choice questions for multi-step reasoning assessments. (2) Open-ended questions to evaluate the accuracy of LMAs' analysis of murderer's motives and methods.\\n\\nEach script is accompanied by a truth manual that contains all clues essential for task completion. Annotators reference this manual, refining details to establish the final ground truth statements concerning the murderer's motive and methodology. The annotation process of multi-choice questions is divided into two stages, as shown in Figure 3(a) and (b):\\n\\n\u2460 Construction of Reasoning Chains: we construct each reasoning step required to unravel the murder mystery within the script, leveraging the information provided in the truth manual and supplementing it with essential details. For example, to deduce that \\\"The victim's fatal wound was not caused by a knife\\\", we first identify key clues given directly in the game, such as images showing the victim's internal organs mirrored compared to a normal person. This information leads us to a 1-hop indirect but crucial clue: the victim's heart is on the right side. Further combining this with expert knowledge and textual clues about a knife wound on his left chest, we infer a 2-hop indirect clue: the knife wound was not fatal. Using this approach, we continuously pinpoint direct clues and deduce indirect ones, eventually linking them into a complete reasoning chain.\\n\\n\u2461 Constructing multiple-choice questions: after building the reasoning chains, annotators use the content of these chain nodes to formulate tiered reasoning questions with correct answers. GPT-4 then creates distractor options based on these correct answers, matching their length to enhance confusion.\\n\\nWe annotated 1,308 reasoning multiple-choice questions. The distribution across different levels is illustrated in Figure 2(c).\\n\\n3.2.4 Data Quality Control\\n\\nTo improve our dataset's quality, we engaged three experts to perform data reviews based on specific standards. Any questions not meeting these standards will be refined. The standards are as follows: (1) If there are substantial informational gaps between nodes in the reasoning chain, intermediate steps will be added to maintain logical consistency; (2) For incorrect options generated by GPT-4, if they are too simplistic or clearly implausible within the problem's context, experts will manually revise them.\\n\\n4 WhodunitBench: Arena-style Evaluation\\n\\nWhodunitBench provides an online arena where LMAs compete in pairwise, faction-based matches, with win rates serving as the primary measure of success. Additionally, we record each multimodal agent's performance data in the arena, including their dialogue outputs and chosen actions.\\n\\n4.1 Settings\\n\\nAgent Settings\\n\\nWe design two settings: one where a naive agent is defined as the murderer and each LMA competes against this naive agent, and another where selected LMAs engage in pairwise competitions. In the game, a multimodal agent in the non-murderer suspect faction, which comprises various roles, will adopt different roles to compete against the agent representing the murderer in the murderer faction. For LMAs, we selected five multimodal agents for evaluation: Yi-Vision [32], Qwen-VL-Plus [4], Gemini-pro-vision [19], Claude-Opus [2], GPT-4V [1]. For Naive Agent, we defined a naive agent that retrieves information about itself and provides output based on the search. When questioned by others, it finds and responds with content related to its role; if it finds nothing, it will provide a default response based on the game's rules.\"}"}
{"id": "qmvtDIfbmS", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Comparative benchmarking of LMAs in an online battle arena. Each cell (Row, Col) indicates the win rate of the Row agent against the Col agent. Note that we excluded instances where no clear winner was determined, including cases with API errors or draws in the voting for the murderer.\\n\\n| Agent vs. Agent | Naive Agent | Yi-Vision | Qwen-VL-Plus | Gemini | Claude | GPT-4V |\\n|----------------|-------------|----------|--------------|--------|--------|--------|\\n| Avg. Win Ratio (%) | 12.0% | 10.0% | 14.7% | 11.6% | 7.1% | 11.1% |\\n| Avg. Loss Ratio (%) | 17.7% | 12.3% | 16.3% | 21.4% | 17.7% | 12.7% |\\n\\n4.2 Results\\n\\nWe report the results in Table 2. We have the following observations: (1) The overall win rate remains low. Regardless of the type of multimodal intelligent agent assuming the role of the \\\"Non-Murder,\\\" their win rates hover between 10% and 20%. This underscores the substantial challenges faced by all current advanced LMAs, including the latest iteration, GPT-4V, in achieving the objectives set out in the game. This suggests a significant gap in the performance capabilities of these agents when tasked with complex, goal-oriented tasks in dynamic environments; (2) Stronger models do not necessarily perform better when playing the role of the murderer. For instance, the Gemini model, regardless of its opponent, is most likely to be identified as the murderer. This may be because more capable models, realizing their role as the murderer, tend to over-communicate in an attempt to obscure the truth, which ironically makes them more susceptible to detection by other players. Conversely, less capable models, such as Qwen, might speak less frequently, making them less likely to be convicted in the game.\\n\\n5 WhodunitBench: Chain of Evaluation (CoE)\\n\\nIn this section, we introduce the CoE Evaluation System, specifically designed to assess three core capabilities through a detailed framework of eight evaluation metrics, grounded in the annotated data from the previous section. With these design standards, we can not only systematically analyze and evaluate each agent's performance at various stages of the game but, more importantly, also provide a strong supplement to previous online competition assessments, showcasing each LMA's capabilities in greater detail. Table 3 presents the evaluation metrics for each LMA when playing a non-murder role against the naive agent we designed (acting as the murderer). Since the naive agent lacks certain \\\"intelligence,\\\" the murderer does not interfere, allowing for a clearer demonstration of each LMA's performance across various capabilities.\\n\\n5.1 Assessment Details\\n\\nPerceptual Ability Assessment: To successfully complete the task, the agent must be able to perceive and comprehend a substantial amount of visual and textual information across various stages of the game, particularly during the initialization phase (as illustrated in the figure 1). These information\"}"}
{"id": "qmvtDIfbmS", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Evaluating LMAs in non-murderer factions versus naive agents using the COE dataset revealed distinct outcomes.\\n\\n| Model and Reasoning Perception Role-play Decision-Making Cognition Avg |\\n|-------------------------------------------------|-----------------|-----------------|-----------------|-----------------|\\n|                  | LSU TIU MIU RP SPC ITD MMR CMD                  |\\n|-----------------------------------------------|---------------|---------------|---------------|---------------|\\n| Random           | 25.00 25.00 25.00 - - - 25.00 - -               |\\n| Yi-Vision        | Direct[20] 42.40 28.66 34.99 7.16 2.37 20.61 20.31 16.03 21.57 |\\n| COT              | Direct[25] 32.80 15.36 27.40 7.20 2.79 15.26 25.41 22.47 18.58 |\\n| Qwen-VL-Plus     | Direct[20] 38.40 43.03 39.41 7.15 0.66 17.30 17.68 15.99 22.45 |\\n| Gemini           | Direct[20] 92.00 68.11 57.68 7.45 10.72 25.98 54.32 18.22 41.81 |\\n| Claude           | Direct[20] 90.00 67.39 52.98 8.00 9.51 19.63 55.08 18.96 40.19 |\\n| GPT-4V           | Direct[20] 93.60 79.29 68.09 7.98 9.49 25.95 57.12 25.18 45.84 |\\n\\nare typically referred to as mystery scripts and clues within the game. We have developed three categories of metrics for evaluation: (1) Text-rich image understanding (TIU): This metric assesses agents' proficiency in precisely interpreting and extracting clues from text-rich images, emphasizing their Optical Character Recognition (OCR) capabilities. It primarily utilizes the TRI-QA annotations from Section 3.2.1. (2) Media-rich image understanding (MIU): This metric evaluates how effectively agents integrate textual and visual elements to interpret and understand more complex clues within images, which may include diagrams, maps or residential layouts. It aims to gauge the agents' ability to navigate intricate visual cues that require both recognition and contextual comprehension. And it primarily utilizes the MRI-QA annotations from Section 3.2.1. (3) Long-script understanding (LSU): This metric evaluates agents' ability to process and extract critical information from lengthy texts, specifically the script content within the game, which sometimes exceeds tens of thousands of words in length. It primarily utilizes the LS-QA annotations from Section 3.2.1. Their scoring formula is defined as: \\\\[\\n\\\\text{Score} = \\\\frac{\\\\text{Correct Questions per Category}}{\\\\text{Total Questions per Category}}.\\n\\\\]\\n\\nStrategic Decision-Making and Role-playing Assessment: To evaluate the role-playing and interactive communication abilities of LMAs, we recorded their dialogues and performances from the online competition and assessed them using two metrics: (1) RP (Role-Playing) Index: This metric assesses the naturalness of the recorded agent dialogues with other roles. It is scored on a ten-point scale, with several criteria designed for GPT-4 to use in scoring. (2) SPC (Scenario Progression Capability) Index: This metric evaluates whether the agent's dialogue contributes to task completion (e.g., identifying the murderer's motive), rather than discussing irrelevant or off-topic content. The score is calculated using annotated ground truth statements from Section 3.2.2: \\\\[\\n\\\\text{Score} = \\\\frac{\\\\text{Number of correct statements}}{\\\\text{Total number of statements in the script}} \\\\times 100.\\n\\\\]\\n\\nAdditionally, we assess the decision-making ability of the agent during the discussion phase. Points will be awarded if the agent chooses to question previously identified key roles from Section 3.2.2 and deems this decision valuable. The calculation method is as follows: \\\\[\\n\\\\text{Score} = \\\\frac{\\\\text{Number of key characters successfully questioned}}{\\\\text{Total number of key characters in the game}} \\\\times 100.\\n\\\\]\\n\\nComprehensive Cognition Assessment: To accurately identify the murder in the murder mystery, the agent must integrate various clues to perform complex reasoning across different levels. This evaluation primarily focuses on assessing this capability. As detailed in Table 3, the assessment utilizes two metrics: MMR (Multi-modal Multi-step Reasoning) and CMD (Case Murder Detail), each with its distinct evaluation method. The MMR metric is primarily evaluated through four multiple-choice questions labeled in Section 3.2.3, scored similarly to perception questions. The CMD metric requires the agent to present its conclusions about the murderer's method and motive in\"}"}
{"id": "qmvtDIfbmS", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"GuiGui(character name), did you notice anything unusual about the victim at that time?\\n\\nWhich song do you like the most?\\n\\nAs a member of the non-murderer faction, I have to find it.\\n\\nI believe the butler is the murderer, but none of the characters present should be the murderer.\\n\\nI couldn't possibly be the murderer; the janitor at the company can vouch for me, providing an alibi that I wasn't present at the scene.\\n\\nI'm an art teacher. At that time, I was taking the children to the police, so I couldn't possibly be the murderer.\\n\\nThe shooter could not be very tall or was kneeling/shooting from a lower position. Someone either knelt to shoot or aimed deliberately to mislead investigation.\\n\\nTraveler's white robe suggests he covers his face, implying secrecy and possible guilt.\\n\\n\\\"I was previously devoted to the royal family and served as a knight of the highest order\\\"\\n\\n\\\"Carmen appeared unusually nervous and isolated, distinctly worried after the duel\\\"\\n\\nCarmen has significant reasons for fear or concern that may be connected to the inner workings or secrets of the Gypsy group.\\n\\n\\\"Milrio and DonJose both discussed Carmen's nervous conditions and isolation\\\"\\n\\nCarmen possibly discovered a threatening secret or plan within the Gypsy community that contributed to her fear.\\n\\nThe motive for the murder could be to silence Carmen to keep this secret from being disclosed, implicating someone from within her close circle with the motive to preserve this secrecy!\"}"}
{"id": "qmvtDIfbmS", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations and Potential Societal Impact\\n\\nOur benchmark, WhodunitBench, features two modes: an online arena and a chain evaluation, designed to assess LMAs in realistic scenarios. This setup mirrors human behavior by requiring LMAs to combine multiple abilities at once, rather than isolating skills in controlled experiments. However, potential concerns and limitations remain regarding the evaluation methodology, metric design, and current data collection practices.\\n\\nCombination of Social Skills and Reasoning Abilities. We find that the current evaluation intertwines interaction and reasoning, making the results less interpretable. In the data annotation process, we labeled critical clues that necessitate interaction to be uncovered. To separate interaction from reasoning, these key clues can be directly provided to agents in the \u201cno-murderer\u201d faction, enabling an analysis of each aspect\u2019s individual impact on scoring. Although this approach has been attempted, more effective solutions may exist for addressing this issue.\\n\\nThe Kind of Reasoning Abilities. Murder mystery games primarily assess core reasoning skills, such as logical deduction, visual-text detail verification, timeline reasoning and hypothesis testing. These games do not cover all reasoning abilities, particularly in computer programming. Strong performance in these games does not guarantee proficiency in all reasoning contexts. However, we believe that, the skills developed, like logical analysis and detail interpretation, are foundational and can be extended to other domains, holding significant potential for broader application.\\n\\nCost considerations are pivotal. Given that the script for each character often exceeds 5,000 words, the volume of data required to effectively test multimodal agents, such as GPT-4V, is substantial. Therefore, the evaluation on WhodunitBench is more expensive compared to existing benchmarks. Additionally, we believe our benchmark has minimal societal impact. However, as agents integrate into daily life, the accuracy of our evaluations could shape public perception of their capabilities, possibly leading to unintended consequences.\\n\\nConclusion\\n\\nIn this work, we propose WhodunitBench for evaluating LMAs\u2019 capability in multi-modal perception, interaction, multi-step reasoning and goal execution. It includes 50 meticulously curated scripts and over 3000 closed-ended multiple-choice questions, along with corresponding open-ended queries featuring human-annotated ground truth. This framework supports online arena-style evaluations and enables detailed chain-linked assessments to evaluate specific capabilities at each stage of the game. Experiments demonstrate that existing LMAs struggle to perform complex tasks requiring compositional skills in dynamic interactive environments; even the state-of-the-art GPT-4V achieves a low score. We hope this work will guide future advancements, establishing a solid foundation for the continued development of LMAs.\\n\\nAcknowledgement\\n\\nThis work was supported in part by the National Natural Science Foundation of China (NO. 62322608), in part by the Guangxi Key R&D Project (No. AB24010167), the Project (No. 20232ABC03A25), in part by the Futian Healthcare Research Project (No. FTWS002), and in part by the Longgang District Special Funds for Science and Technology Innovation (No. LGKCSDPT2023002).\\n\\nReferences\\n\\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\\n\\n[2] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 2024.\\n\\n[3] Mar\u00eda Arinbjarnar. Murder she programmed: Dynamic plot generating engine for murder mystery games. Bachelor of Science project at Reykjavik University http://nemendur.ru/maria01/greinar/BSc.pdf, 2006.\"}"}
{"id": "qmvtDIfbmS", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.\\n\\n[5] Gabriella Alves Bulhoes Barros, Michael Cerny Green, Antonios Liapis, and Julian Togelius. Who killed albert einstein? from open data to murder mystery games. IEEE Transactions on Games, 11(1):79\u201389, 2018.\\n\\n[6] Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi Meng, Tianyu Liu, and Baobao Chang. Pca-bench: Evaluating multimodal large language models in perception-cognition-action chain. arXiv preprint arXiv:2402.15527, 2024.\\n\\n[7] Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. M $\\\\hat{M}$ cot: A novel benchmark for multi-domain multi-step multi-modal chain-of-thought. arXiv preprint arXiv:2405.16473, 2024.\\n\\n[8] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. Advances in Neural Information Processing Systems, 36, 2024.\\n\\n[9] Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, et al. Mindagent: Emergent gaming interaction. arXiv preprint arXiv:2309.09971, 2023.\\n\\n[10] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024.\\n\\n[11] Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, and Ling Liu. A survey on large language model-based game agents. arXiv preprint arXiv:2404.02039, 2024.\\n\\n[12] Ann S Jennings. Creating an interactive science murder mystery game: The optimal experience of flow. IEEE transactions on professional communication, 45(4):297\u2013301, 2002.\\n\\n[13] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024.\\n\\n[14] Somnath Kumar, Yash Gadhia, Tanuja Ganu, and Akshay Nambi. Mmctagent: Multi-modal critical thinking agent framework for complex visual reasoning. arXiv preprint arXiv:2405.18358, 2024.\\n\\n[15] Jonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. Avalonbench: Evaluating llms playing the game of avalon. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023.\\n\\n[16] Gr\u00e9goire Mialon, Cl\u00e9mentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for general ai assistants. arXiv preprint arXiv:2311.12983, 2023.\\n\\n[17] Ronald K Siegel. Hallucinations. Scientific American, 237(4):132\u2013141, 1977.\\n\\n[18] Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, et al. Towards general computer control: A multimodal agent for red dead redemption ii as a case study. arXiv preprint arXiv:2403.03186, 2024.\\n\\n[19] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\\n\\n[20] Jiaqi Wang, Zhengliang Liu, Lin Zhao, Zihao Wu, Chong Ma, Sigang Yu, Haixing Dai, Qiushi Yang, Yiheng Liu, Songyao Zhang, et al. Review of large vision models and visual prompt engineering. Meta-Radiology, page 100047, 2023.\"}"}
{"id": "qmvtDIfbmS", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):1\u201326, 2024.\\n\\nShenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. Avalon\u2019s game of thoughts: Battle against deception through recursive contemplation. arXiv preprint arXiv:2310.01320, 2023.\\n\\nZekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, et al. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. arXiv preprint arXiv:2310.00746, 2023.\\n\\nZihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models. arXiv preprint arXiv:2311.05997, 2023.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022.\\n\\nDekun Wu, Haochen Shi, Zhiyuan Sun, and Bang Liu. Deciphering digital detectives: Understanding llm behaviors and capabilities in multi-agent mystery games. arXiv preprint arXiv:2312.00746, 2023.\\n\\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.\\n\\nJian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. Travelplanner: A benchmark for real-world planning with language agents. arXiv preprint arXiv:2402.01622, 2024.\\n\\nJunlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and Guanbin Li. Large multimodal agents: A survey. arXiv preprint arXiv:2402.15116, 2024.\\n\\nTianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. arXiv preprint arXiv:2404.07972, 2024.\\n\\nMing Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, and Ji Yan. Larp: Language-agent role play for open-world games. arXiv preprint arXiv:2312.17653, 2023.\\n\\nAlex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01.ai. arXiv preprint arXiv:2403.04652, 2024.\\n\\nCong Zhang, Deik Derrick Goh Xin, Dexun Li, Hao Zhang, and Yong Liu. Meta-task planning for language agents. arXiv preprint arXiv:2405.16510, 2024.\\n\\nYupeng Zheng, Zebin Xing, Qichao Zhang, et al. Planagent: A multi-modal large language agent for closed-loop vehicle motion planning. arXiv preprint arXiv:2406.01587, 2024.\\n\\nXuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, et al. Sotopia: Interactive evaluation for social intelligence in language agents. arXiv preprint arXiv:2310.11667, 2023.\"}"}
{"id": "qmvtDIfbmS", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Checklist\\n\\n1. For all authors...\\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] See Section 1.\\n(b) Did you describe the limitations of your work? [Yes] See Section 7.\\n(c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section 6.\\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n(b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Attached in the paper.\\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 4.1.\\n(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] See the limitation for the cost problem.\\n(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See the supplemental material.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n(a) If your work uses existing assets, did you cite the creators? [N/A] Not from existing assets.\\n(b) Did you mention the license of the assets? [Yes] See the supplemental materials.\\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\\n(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]\\n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See the supplemental materials.\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] See the supplemental materials.\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [Yes] See the supplemental materials.\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes] See the supplemental materials.\"}"}
{"id": "qmvtDIfbmS", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Appendix\\n\\nA.1 Preliminary: Murder Mystery Game\\n\\nTo better demonstrate our game process and testing procedures, we have created a demo and placed it in our open-source dataset and code repository.\\n\\nA.1.1 Game Introduction\\n\\nIn this section, we provide a detailed introduction to the rules and key elements of the murder mystery game, elucidating its suitability as an ideal dynamic environment platform for evaluating the multifaceted capabilities of LMAs. In the murder mystery game, each session is orchestrated through a meticulously designed script that constructs a self-contained fictional world. This world is enriched with elaborate backstories, sophisticated character development, and complex narrative structures.\\n\\nThe script comprises the following primary elements, as shown in Figure 5:\\n\\n\u2022 **Script.** Each murder mystery game unfolds according to a detailed script that establishes the game's universe and background. Within this setting, a unique murder mystery unfolds, featuring a core group of suspects, each equipped with their own character script. These scripts furnish extensive details about each character's name, faction, and backstory.\\n\\n\u2022 **Role.** In real-life murder mystery games, each script is populated with multiple roles, each possessing a unique background and crucial clues that offer diverse perspectives on the script's virtual world. While any role could be suspected of the murder, only one is the true murderer. This divides the roles into two groups: non-murder suspects and the murder suspect.\\n\\n\u2022 **Clue.** Clues are the pieces of information necessary to solve the murder mystery within the game, encompassing both the textual clues found in each character's script and the publicly shared visual clues. Each role receives different clues at the start of the game. Some roles will possess key clues. Additionally, certain clues may be misleading, requiring players to engage in deeper reasoning to fully understand their implications.\\n\\nGame Process:\\n\\nWe have simplified the entire game process into four stages, \u2460 Initialization phase \u2461 Discussion phase \u2462 Reasoning phase \u2463 Voting phase. In the **Initialization Phase** of the scripted murder mystery game, participants thoroughly examine their character scripts, which include essential details such as name, identity, interpersonal relationships, and pivotal text clues regarding the sequence of events on the day of the incident. Then they introduce their respective roles to start the game. The players are also given visual clues outside of their scripts needed to solve the puzzle.\\n\\nIn the **Discussion Phase**, they engage in in-depth discussions, using both individual clues and shared evidence to analyze and share insights. During the **Reasoning phase**, each player needs to combine the direct clues they have obtained (including textual and visual clues) and the content of discussions to infer implicit clues. And then deduce the murderer and his modus operandi from this. The game culminates in the **Voting Phase**, where players, based on the evidence and discussions, decide on the murderer's identity.\\n\\nOverall, the primary objective of these games is to decipher a murder mystery, which entails identifying the murderer, elucidating the method of the crime, and understanding the underlying motives. At the commencement of each game, players choose and embody specific roles, each equipped with a script that provides a unique vantage point and critical information pertinent to their role within this fabricated world. As the game progresses, players must analyze overt clues and engage in substantive discussions with fellow participants to amass information related to the crime. This ongoing process of information collection and inference through interactive collaboration enables players to gradually synthesize a comprehensive portrait of the case, uncovering the veritable truth.\\n\\nA.1.2 Action and Observation\\n\\n**Observation space for each Phase.** The observation space varies across these stages, which will be detailed further. (1) **Initialization:** In this phase, the observation space for character-playing agents includes clues from role scripts and image clues, as well as introductions to other characters. This covers each character's background, identity, relationships, and the circumstances on the incident day, alongside public clues. Notably, observations can vary even within the same script, as agents have access to different information, leading to inconsistencies and incompleteness in the observed data.\\n\\n(2) **Discussion:** In this phase, the primary observation space for each role-playing agent includes...\"}"}
{"id": "qmvtDIfbmS", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fig. 5: Illustration of primary elements in murder mystery games: scripts, roles and clues. Due to space limitations, the role scripts are detailed in Fig. 6.\\n\\nThe statements and discussions among participants. This encompasses interpretations and inferences regarding the case facts, analyses of pictorial and textual clue cards, exchanges of questions and answers, interrogation and verification of case evidence, and debates over various deductive paths and conclusions. (3)\\n\\n**Reasoning:**\\nIn this phase, the observation space involves other roles discussing the current murderer's motive and method. Each role can refine or optimize their statements based on what others have said. (4)\\n\\n**Final Voting:**\\nIn this stage of the game, the observation space consists of other roles' voting. Each role can adjust their own response based on the votes of others.\\n\\n**Action space for each Phase.**\\nThe action space varies across these stages, which will be detailed further. (1)\\n\\n**Initialization:**\\nAt this stage, the actions of the multimodal intelligent agents are based on the current observation to perform self-introduction. This includes not only conveying their basic information and functionalities but also demonstrating their understanding of the environment and how to navigate and interact within it effectively. (2)\\n\\n**Discussion:**\\nAt this stage, the multimodal intelligent agents should execute two actions:\\n\\n1. Share and analyze the clues in the script and discuss how these clues are connected to the current situation.\\n2. Pose questions to other roles, which can be about clues related to that role or about suspicions concerning that role. (3)\\n\\n**Reasoning:**\\nIn this phase, the agent's action is to articulate their thoughts on the current suspect's motive and reasons for...\"}"}
{"id": "qmvtDIfbmS", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"committing the crime. (4)\\n\\nFinal Voting.\\n\\nAt this stage, there is only one action, which is to identify the Murderer and provide the motive and method of the crime.\\n\\nA.2 Additional Details on the Dataset\\n\\nA.2.1 Dataset Collection and Access\\n\\nIn our research, we introduce a benchmark based on murder mystery games, named WhodunitBench, designed to evaluate large multimodal agents (LMAs). We provide a detailed description of the dataset collection and verification processes as follows:\\n\\nCollection.\\n\\nIn this study, we invited five seasoned experts from the murder mystery game domain to select and extract suitable scripts. Each expert meticulously screened the provided script library based on detailed selection criteria, choosing the top 50 scripts that best met the standards, for which they received a compensation of $100 each. Furthermore, the author personally conducted a thorough review to ensure all selected scripts met the expected quality and thematic requirements. The primary resources we utilize in our work are the cloud services provided by the agent's company. Running all 50 scripts through a single model at once requires approximately 20 million tokens. We estimate the average hourly wage for the annotators to be approximately $8 per hour.\\n\\nDuring the question annotation phase, we employed ten experienced scripted murder mystery game experts to annotate the data, with a compensation of $0.50 per question. Given the importance and complexity of constructing intricate reasoning chains, each reasoning chain was priced at $2 for annotation. To ensure the quality of annotations, we also hired three experts to review the questions at a cost of $0.20 per question. All data reviews were conducted strictly to ensure accuracy and reliability.\\n\\nRefinement and Verification.\\n\\nTo ensure the standardization and accuracy of our questions, we have established a meticulous proofreading and verification process. After the initial tagging is completed, each tagged question and reasoning chain undergoes a three-stage review to ensure it meets preset standards and logical correctness:\\n\\n\u2460 Preliminary Review Stage: This stage is conducted by three experts who are separate from the tagging team. They are primarily responsible for checking the grammar, spelling, and format of the questions to ensure they meet predetermined standards and for confirming the basic logic and factual accuracy of the questions. This step is designed to ensure that all questions are clear and accurate before proceeding to a more in-depth logical review.\\n\\n\u2461 Logic and Consistency Review: After passing the preliminary review, each question and reasoning chain enters the logic review stage. At this stage, the review team delves into the logical processes and structure within the questions and reasoning chains to ensure that each link is logically rigorous and closely connected to the overall plot of the script. In particular, reviewers check for logical gaps or leaps in reasoning.\\n\\n\u2462 Final Confirmation Stage: After rigorous reviews in the previous two stages, all questions and reasoning chains are submitted to the authors for final review. In this stage, in addition to reconfirming the accuracy and logic of the questions, the difficulty level is also assessed to ensure it is appropriate. Furthermore, the authors integrate all review comments to refine and optimize the questions, ensuring each one meets the highest quality standards.\\n\\nA.2.2 Additional Quantitative Examples\\n\\nIn this section, we provide additional examples of scripts used to configure the arena and questions for evaluation: (1) Further script examples are shown in Figure 7, 8. (2) Additional examples of question-answer (QA) are illustrated in Figure 9 and 10, which displays the multi-step reasoning questions we have marked, as well as the specific reasoning chains used during the marking process.\\n\\nA.3 Dataset Documentation\\n\\nA.3.1 Motivation\\n\\nFor what purpose was the dataset created?\\n\\nWe created this dataset as a comprehensive test-bed for evaluating the perception, role-playing interaction, cognition and goal-achievement capabilities of large multimodal agents via murder mystery games.\"}"}
