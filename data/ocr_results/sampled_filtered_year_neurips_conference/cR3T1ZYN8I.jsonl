{"id": "cR3T1ZYN8I", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Model Zoo\\n\\nLlaVa-1.5 is an extension of the LlaVa model. We use the variant with CLIP-ViT-L-14 as a vision encoder and Vicuna-7b language model.\\n\\nBLIP-2 uses a QFormer architecture to bridge frozen language and vision encoders. We use the variant with CLIP-ViT-L-14 as a vision encoder and the 2.7b OPT language model.\\n\\nInstructBLIP is a family of VLLMs that exploits the basic BLIP-2 architecture and advances the task by giving the instruction to both the QFormer and the LLM. We use two variants of the architecture, with two different LLMs from the T5 family; in the base architecture, we use the CLIP-ViT-L-14 as a vision encoder and the T5-xl LLM. InstructBLIP-xxl uses the same vision encoder and the T5-xxl language model.\\n\\nA comparison of all architectures in terms of parameters and pre-training datasets can be seen in Tab. 4.\\n\\nIt is worth noting, that none of the pre-training datasets are related to deepfakes, making the task more challenging. For the ensemble, we chose BLIP-2 and LlaVa-15, based of two main factors: a) they show the most competitive performance on most datasets as seen in Fig. and b) they have the least overlap in terms of pre-training data thus we intuitively expect them having complementary information. The ensembling method adopted in this work is using score fusion with majority voting; in the occasions where the models disagree, the mean is taken.\\n\\nGPT4v is used for comparison in the binary tasks, however as the training details of this model are unknown, it should only be treated as an upper bound.\\n\\n| Architecture   | FLOPS | # Params | Pre-training Data                                                                 |\\n|---------------|-------|----------|----------------------------------------------------------------------------------|\\n| BLIP-2        | 0.38T | 7.06B    | COCO, Visual Genome, CC3M, CC12M, SBU, and 115M images from the LAION400M        |\\n| InstructBLIP  | 0.33T | 4.02B    | COCO, WebCapFilt, TextCaps, VQAv2, OK-VQA, AOK-VQA, OCR-VQA, LLaV A-Instruct-150K |\\n| InstructBLIP-xxl | 0.53T | 12.31B   | COCO, WebCapFilt, TextCaps, VQAv2, OK-VQA, AOK-VQA, OCR-VQA, LLaV A-Instruct-150K |\\n| LlaVa-1.5     | 0.14T | 4.14T    | LLaV A-Instruct-150K, VQAv2, OK-VQA, OCR-VQA                                     |\\n\\nImplementation Details.\\nAll experiments were conducted using four NVIDIA A100 GPUs, with 40GB of memory. We use the PyTorch deep learning framework for all model evaluation tasks and weights published on HuggingFace.\\n\\nThe term deepfake is a colloquial term for a wide range of manipulations using generative models, from altering one small area all the way to fully generated images and videos. As such, the class name itself has several synonyms that can describe it. To assess the model's robustness to instruction, we first prompt an LLM \u2013specifically ChatGPT3.5 to give us synonyms for a deepfake. This is done as an automation step to incorporate the general consensus into the method without the author's bias, following previous works. The following synonyms are tested for the positive class: \u201cmanipulated\u201d, \u201cdeepfake\u201d, \u201csynthetic\u201d, \u201caltered\u201d, \u201cfabricated\u201d, \u201cface forgery\u201d and \u201cfalsified\u201d. We show the detailed performance of all models on all synonyms in Tab. To provide context, we also provide the cross-dataset performance of several discriminative SOTA works in Tab. The models show the most consistent performance on synonyms \u201cmanipulated\u201d, \u201csynthetic\u201d and \u201caltered\u201d; therefore, we do all subsequent analyses on these prompts.\\n\\nhttps://huggingface.co/\"}"}
{"id": "cR3T1ZYN8I", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Reported cross-dataset performance of purpose-built discriminative SoTA models.\\n\\n| Model       | UCF AUC | X-Ray AUC | Xception AUC | REECE AUC | CORE AUC | LAA-Net AUC | C Vision Encoder Fine-tuning |\\n|-------------|---------|-----------|--------------|-----------|----------|--------------|-------------------------------|\\n|             | 81.40   | 82.40     | 61.18        | 70.93     | 74.28    | 86.28        |                               |\\n|             |         |           | 66.93        | 70.35     |          | 91.93        |                               |\\n|             | 80.50   |           | 55.37        | 54.41     |          | 73.41        |                               |\\n|             |         |           | 69.90        |          |          | 93.67        |                               |\\n\\nWe fine-tune the CLIP-L/14-336 Vision encoder using LoRA adaptors. Specifically, we add 32 adaptors to the queries, keys, values and out projection of the attention heads, with an alpha of 32, a dropout rate of 0.2 and 4bit quantization. As the available datasets lack sample level descriptions, we use the text embeddings of the synonyms for the positive category and the embeddings of \\\"real\\\", \\\"original\\\", \\\"unaltered\\\", \\\"authentic\\\", \\\"legitimate\\\", \\\"genuine\\\", \\\"bona fide\\\" for the negative. We apply a Sigmoid loss over the cosine similarity of all relevant text embeddings in the batch and train the vision encoder for 50 epochs. The updated weights are then used in the LlaVa-1.5 architecture.\\n\\nD CLIP Embeddings\\n\\nAs most models use CLIP variants as backbone vision encoders, we assess the separability of samples in real and manipulated images in each dataset by visualising them in a two-dimensional plane using t-SNE. The resulting visualisations can be seen in Fig. 5. The samples appear more separable in some datasets. Interestingly, the StyleGAN datasets, where the images are fully generated, seem to have more distinguishable latent representations from real images. The separability is not necessarily reflected in the language generation as seen in Sec. 5.1; to further examine the root cause of this, we first calculate the average image embedding of each class so as to create a class prototype and retrieve the top-10 nearest language token embeddings, seen in Tab. 8. The first observation that can be made is that none of the tokens seems related to the task at hand, which would potentially inform prompt selection, so without the reasoning capabilities of the LLM, the token retrieval on its own is not very informative. Secondly, we see that a number of tokens are repeated across datasets and for both classes, which can be attributed to the much smaller sub-space of the deepfake task compared to the CLIP latent space; so, while the image embeddings are somewhat separable for several datasets retrieving the nearest language tokens shows that the subspace is not very much related to face forgery. From these two observations, we can better understand the zero-shot performance of the tested foundation models. Finally, we show the performance of CLIP on the binary classification task in Tab. 7. We use an ensemble of prompts using the Imagenet prompt templates; for the positive class, we average the embeddings of the prompts for synonyms \\\"manipulated\\\", \\\"synthetic\\\" and \\\"altered\\\"; for the negative, we use \\\"real\\\", \\\"original\\\" and \\\"unaltered\\\". Contrary to previous VQA works that use CLIP retrieval as an upper bound, we see this is not the case in the task. This can be attributed to the more abstract definition of the class in the deepfake detection task, as well as the pre-training dataset, which is more object-oriented.\\n\\nE Human Evaluation\\n\\nThe human evaluation is based upon previous studies in VQA evaluation. The humans are asked to rate model predictions on a scale from 1 to 5, where 1 is completely wrong and 5 is completely correct. Evaluation dataset: We use a subset of the pseudo-fake R-Splicer dataset for the human evaluation study, as it has artefacts visible to the human eye and is thus easier for the annotators to assess the response quality.\"}"}
{"id": "cR3T1ZYN8I", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1 is completely wrong, and 5 is completely correct. Each annotator is shown 50 samples. To reduce Annotator agreement:\\nas in previous works [a general description of the image content when they fail to identify specific areas of manipulation. By examining a few samples in Fig. F Qualitative Samples multi-label task. We average the annotator scores to receive the final gold standard for each sample inter-annotator agreement and obtain .\\n\\nAn example of the form shown to human evaluators can be seen in Fig. 1. An example of the form shown to human evaluators can be seen in Fig. 1.\\n\\nWe select 100 samples from the dataset and generate responses using the open\\n\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake\\nmanipulated\\ndeeppfake"}
{"id": "cR3T1ZYN8I", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 7: CLIP baseline performance on the binary task of the nine selected benchmarks.\\n\\n| Dataset       | Accuracy | AUC  | F1  |\\n|---------------|----------|------|-----|\\n| SeqDeepFake attributes | 63.54   | 0.62 | 0.46 |\\n| SeqDeepFake components | 35.80   | 0.49 | 0.38 |\\n| R-splice      | 17.14   | 0.44 | 0.33 |\\n| FF++          | 60.85   | 0.50 | 0.38 |\\n| DFDC          | 46.84   | 0.75 | 0.40 |\\n| CelebDF       | 41.59   | 0.60 | 0.49 |\\n| DFW           | 90.67   | 0.90 | 0.31 |\\n| StyleGan2     | 28.69   | 0.69 | 0.20 |\\n| StyleGan3     | 29.40   | 0.63 | 0.35 |\\n\\nTable 8: Top 10 closest tokens to the class prototypes. Unique to the class tokens are in bold.\\n\\n| Dataset       | Original | DeepFake |\\n|---------------|----------|----------|\\n| SeqDeepFake attributes | natives, labeling, liz, saving, rink, demon, pitch, creole, godis, wentz | natives, liz, labeling, rink, wentz, ronda, godis, creole, % |\\n| SeqDeepFake components | natives, labeling, qld, romo, liz, creole, dhoni | natives, qld, liz, gaining, romo, %, cuomo, klo, minions |\\n| R-splice      | natives, wentz, anglo, liz, %, qld, anca, romo, ronda | anglo, %, wentz, liz, natives, klo, anca, qld, ural, weed |\\n| FF++          | natives, qld, wentz, anglo, cuomo, anca, liz, labeling, romo, melissa | natives, qld, wentz, anglo, liz, cuomo, anca, %, weed, romo |\\n| DFDC          | natives, wentz, anglo, liz, ronda, qld, %, anca, romo | anglo, wentz, natives, liz, %, ronda, qld, anca, %, romo |\\n| CelebDF       | natives, liz, wentz, ronda, %, anglo, qld, romo, labeling, anca | anglo, natives, ural, klo, creole, %, weed, wentz, romo, minions |\\n| DFW           | anglo, %, ronda, wentz, ural, natives, liz, klo, rene, ator | anglo, %, natives, ural, wentz, ator, klo, anca, qld |\\n| StyleGan2     | natives, liz, wentz, ronda, %, anglo, qld, romo, labeling, anca | anca, %, natives, labeling, liz, rink, ronda, romo, creole, melissa, wentz, saving |\\n| StyleGan3     | natives, liz, wentz, ronda, romo, labeling, aa, anca, % | anca, %, natives, labeling, liz, rink, wentz, romo, saving, creole, melissa |\\n\\nAs the available datasets lack sample specific descriptions of manipulation areas, we include BertScore of the tested VLLMs on the MagicBrush dataset to assess response quality on the neighbouring task of image editing detection, that is language driven in Table Tab. 9.\\n\\nH Open-ended VQA\\n\\nThe detailed performance of each model on the open-ended fine-grained detection can be seen on Tables 11 to 14. The matching strategy has a significant impact on Recall, as mentioned in Sec. 5.2, thus also increasing the F1 score.\\n\\nTable 9: BertScore of selected VLLMs on MagicBrush dataset\\n\\n| Dataset       | Precision | Recall   | F1   |\\n|---------------|-----------|----------|------|\\n| Blip          | 85.44     | 84.37    | 84.57|\\n| InstructBlip  | 84.56     | 85.09    | 84.82|\\n| InstructBlip-xxl | 81.07   | 84.55    | 81.88|\\n| LlaVa 1.5     | 83.08     | 84.77    | 83.87|\"}"}
{"id": "cR3T1ZYN8I", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 5: t-SNE Visualisation of CLIP image embeddings on the test set of the selected datasets (perplexity=50)\"}"}
{"id": "cR3T1ZYN8I", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The face of the man in the middle of the image has been made to look like he has a mouth full of teeth. The area of the image that is manipulated is the man's face. The image features a man in a suit and tie speaking into a microphone. He is wearing glasses and a tie, and his face has been digitally manipulated to appear as if he has a mustache.\\n\\n(a) Ground Truth: Mouth, Faceswap\\n\\nThe eyes, nose, and mouth are the only parts of the image that have been manipulated. The area of the image that is manipulated in the image is the man's face. The image features a man with a tattoo on his neck and face, with the tattoo visible on the right side of his neck. The area of the image that is manipulated is the man's eyes. They have been digitally altered to appear as if they are g...[text cut]\\n\\n(b) Ground Truth: Eyebrows, Eyes\\n\\nThe face of the man in the green shirt. The image features a man with a green background and a distorted facial expression. The man's face is the primary focus of the image. In the image, the area that is manipulated is the man's face. Specifically, his eyes, nose, and mouth have been altered.\\n\\nThe face of the man in the image is manipulated, as it appears to have been retouched. (c) Ground Truth: Nose, Mouth, Faceswap\\n\\nFigure 7: Samples of generated responses in an open-ended setting.\"}"}
{"id": "cR3T1ZYN8I", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 10: Performance of tested VLLMs in multiple-choice setting\\n\\n|               | AUC | F1  | mAP  | Recall |\\n|---------------|-----|-----|------|--------|\\n| Bangs        | 49.8| 0.0 | 60.3 | 0.0    |\\n| Eyeglasses   | 49.9| 8.0 | 63.4 | 4.6    |\\n| Beard        | 49.8| 1.0 | 61.0 | 0.5    |\\n| Smiling      | 50.0| 0.0 | 58.3 | 0.0    |\\n| Young        | 50.0| 0.0 | 62.3 | 0.0    |\\n| Total        | 49.9| 4.5 | 61.1 | 2.5    |\\n\\n(a) SeqDeepFake attributes\\n\\n|               | AUC | F1  | mAP  | Recall |\\n|---------------|-----|-----|------|--------|\\n| Nose          | 50.0| 0.0 | 55.4 | 0.0    |\\n| Eye           | 50.8| 10.2| 66.8 | 5.5    |\\n| Eyebrow       | 51.0| 11.0| 58.3 | 6.0    |\\n| Lip           | 50.0| 0.0 | 67.6 | 0.0    |\\n| Hair          | 50.4| 9.7 | 48.9 | 5.4    |\\n| Total         | 50.4| 10.3| 59.4 | 5.7    |\\n\\n(b) SeqDeepFake comp\\n\\n|               | AUC | F1  | mAP  | Recall |\\n|---------------|-----|-----|------|--------|\\n| Nose          | 51.3| 46.9| 52.5 | 42.2   |\\n| Eyebrows     | 50.4| 57.0| 52.0 | 63.1   |\\n| Eyes          | 50.2| 56.2| 51.5 | 61.9   |\\n| Mouth         | 49.9| 45.4| 51.9 | 40.9   |\\n| Faceswap      | 44.6| 9.2 | 50.2 | 5.5    |\\n| Total         | 49.3| 42.9| 51.6 | 42.7   |\\n\\n(c) R-Splicer\\n\\n|               | AUC | F1  | mAP  | Recall |\\n|---------------|-----|-----|------|--------|\\n| Bangs        | 50.2| 1.0 | 60.4 | 0.5    |\\n| Eyeglasses   | 58.4| 51.3| 68.0 | 41.6   |\\n| Beard        | 50.0| 0.3 | 61.0 | 0.2    |\\n| Smiling      | 50.0| 0.0 | 58.3 | 0.0    |\\n| Young        | 50.4| 29.0| 62.6 | 21.4   |\\n| Total        | 51.8| 20.4| 62.1 | 15.9   |\\n\\n(d) SeqDeepFake attributes\\n\\n|               | AUC | F1  | mAP  | Recall |\\n|---------------|-----|-----|------|--------|\\n| Nose          | 52.2| 11.7| 67.7 | 6.5    |\\n| Eye           | 50.6| 3.3 | 58.2 | 1.7    |\\n| Eyebrow       | 50.0| 0.0 | 67.6 | 0.0    |\\n| Hair          | 51.1| 24.7| 49.3 | 23.6   |\\n| Total         | 50.8| 13.2| 59.6 | 10.6   |\\n\\n(e) SeqDeepFake comp\\n\\n|               | AUC | F1  | mAP  | Recall |\\n|---------------|-----|-----|------|--------|\\n| Nose          | 56.5| 38.5| 56.2 | 30.4   |\\n| Eyebrows     | 52.5| 58.0| 53.1 | 66.5   |\\n| Eyes          | 52.6| 16.5| 53.1 | 10.1   |\\n| Mouth         | 52.9| 13.0| 54.3 | 7.1    |\\n| Faceswap      | 50.0| 0.0 | 51.6 | 0.0    |\\n| Total         | 52.9| 31.5| 53.7 | 28.5   |\\n\\n(f) R-Splicer\\n\\n|               | AUC | F1  | mAP  | Recall |\\n|---------------|-----|-----|------|--------|\\n| Bangs        | 50.4| 18.5| 60.7 | 13.2   |\\n| Eyeglasses   | 54.3| 75.9| 65.7 | 90.3   |\\n| Beard        | 50.0| 0.1 | 61.0 | 0.1    |\\n| Smiling      | 50.0| 0.0 | 58.3 | 0.0    |\\n| Young        | 50.0| 0.0 | 62.3 | 0.0    |\\n| Total        | 50.9| 31.5| 61.6 | 34.5   |\\n\\n(g) SeqDeepFake attributes\\n\\n|               | AUC | F1  | mAP  | Recall |\\n|---------------|-----|-----|------|--------|\\n| Nose          | 50.0| 3.2 | 55.4 | 1.7    |\\n| Eye           | 48.8| 76.2| 65.9 | 91.0   |\\n| Eyebrow       | 49.2| 71.8| 57.4 | 96.0   |\\n| Hair          | 52.2| 65.4| 49.9 | 94.9   |\\n| Total         | 49.9| 44.1| 59.2 | 57.1   |\\n\\n(h) SeqDeepFake comp\\n\\n|               | AUC | F1  | mAP  | Recall |\\n|---------------|-----|-----|------|--------|\\n| Nose          | 49.6| 67.6| 51.6 | 98.2   |\\n| Eyebrows     | 50.1| 67.9| 51.8 | 98.4   |\\n| Eyes          | 50.2| 67.7| 51.5 | 98.9   |\\n| Mouth         | 50.0| 68.0| 51.9 | 98.4   |\\n| Faceswap      | 50.4| 68.2| 51.8 | 99.8   |\\n| Total         | 50.1| 67.9| 51.7 | 98.7   |\\n\\n(i) R-Splicer\\n\\n|               | AUC | F1  | mAP  | Recall |\\n|---------------|-----|-----|------|--------|\\n| Bangs        | 52.2| 38.1| 61.4 | 27.3   |\\n| Eyeglasses   | 58.2| 73.0| 67.5 | 77.8   |\\n| Beard        | 57.1| 74.3| 64.7 | 86.7   |\\n| Smiling      | 54.7| 36.4| 61.1 | 25.2   |\\n| Young        | 49.5| 74.1| 62.1 | 92.6   |\\n| Total        | 54.3| 59.2| 63.3 | 61.9   |\\n\\n(j) SeqDeepFake attributes\\n\\n|               | AUC | F1  | mAP  | Recall |\\n|---------------|-----|-----|------|--------|\\n| Nose          | 50.0| 3.8 | 55.4 | 2.0    |\\n| Eye           | 49.5| 3.8 | 55.4 | 1.7    |\\n| Eyebrow       | 49.2| 71.8| 57.4 | 96.0   |\\n| Hair          | 52.2| 65.4| 49.9 | 94.9   |\\n| Total         | 49.9| 44.1| 59.2 | 57.1   |\\n\\n(k) SeqDeepFake comp\\n\\n|               | AUC | F1  | mAP  | Recall |\\n|---------------|-----|-----|------|--------|\\n| Nose          | 49.6| 67.6| 51.6 | 98.2   |\\n| Eyebrows     | 50.1| 67.9| 51.8 | 98.4   |\\n| Eyes          | 50.2| 67.7| 51.5 | 98.9   |\\n| Mouth         | 50.0| 68.0| 51.9 | 98.4   |\\n| Faceswap      | 50.4| 68.2| 51.8 | 99.8   |\\n| Total         | 50.1| 67.9| 51.7 | 98.7   |\\n\\n(l) R-Splicer\\n\\n|               | AUC | F1  | mAP  | Recall |\\n|---------------|-----|-----|------|--------|\\n| Bangs        | 52.2| 38.1| 61.4 | 27.3   |\\n| Eyeglasses   | 58.2| 73.0| 67.5 | 77.8   |\\n| Beard        | 57.1| 74.3| 64.7 | 86.7   |\\n| Smiling      | 54.7| 36.4| 61.1 | 25.2   |\\n| Young        | 49.5| 74.1| 62.1 | 92.6   |\\n| Total        | 54.3| 59.2| 63.3 | 61.9   |\\n\\n23\"}"}
{"id": "cR3T1ZYN8I", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Attribute   | AUC | F1  | mAP | Recall |\\n|-------------|-----|-----|-----|--------|\\n| Bangs       | 53.9| 33.7| 62.5| 22.2   |\\n| Eyeglasses  | 56.4| 40.3| 67.0| 28.0   |\\n| Beard       | 50.1| 2.1 | 61.1| 1.1    |\\n| Smiling     | 49.9| 1.0 | 58.3| 0.5    |\\n| Young       | 44.6| 24.9| 60.3| 18.2   |\\n| Total       | 51.0| 20.4| 61.8| 14.0   |\\n\\n(a) SeqDeepFake attributes with matching\\n\\n| Attribute   | AUC | F1  | mAP | Recall |\\n|-------------|-----|-----|-----|--------|\\n| Bangs       | 56.0| 73.5| 64.1| 99.9   |\\n| Eyeglasses  | 60.0| 75.9| 69.6| 99.9   |\\n| Beard       | 59.9| 74.1| 66.8| 99.9   |\\n| Smiling     | 50.2| 72.0| 56.4| 99.9   |\\n| Young       | 51.6| 75.1| 61.2| 100.0  |\\n| Total       | 55.5| 74.1| 63.6| 99.9   |\\n\\n(b) SeqDeepFake attributes with CLIP matching\\n\\n| Attribute   | AUC | F1  | mAP | Recall |\\n|-------------|-----|-----|-----|--------|\\n| Nose        | 49.4| 20.1| 55.2| 13.2   |\\n| Eye         | 50.1| 10.5| 66.6| 6.0    |\\n| Eyebrow     | 49.9| 2.2 | 57.7| 1.1    |\\n| Lip         | 50.6| 11.6| 68.0| 6.5    |\\n| Hair        | 52.6| 29.3| 50.3| 19.8   |\\n| Total       | 50.5| 14.7| 59.5| 9.3    |\\n\\nc) SeqDeepFake comp. with matching\\n\\n| Attribute   | AUC | F1  | mAP | Recall |\\n|-------------|-----|-----|-----|--------|\\n| Nose        | 51.6| 69.0| 54.8| 100.0  |\\n| Eye         | 49.8| 77.3| 64.1| 99.8   |\\n| Eyebrow     | 50.4| 70.9| 55.7| 100.0  |\\n| Lip         | 50.0| 78.2| 66.2| 100.0  |\\n| Hair        | 49.7| 63.2| 46.8| 99.8   |\\n| Total       | 50.3| 71.7| 57.5| 99.9   |\\n\\n(d) SeqDeepFake comp. with CLIP matching\\n\\n| Attribute   | AUC | F1  | mAP | Recall |\\n|-------------|-----|-----|-----|--------|\\n| Nose        | 55.8| 31.6| 55.9| 20.5   |\\n| Eyebrows    | 50.4| 2.5 | 52.0| 1.2    |\\n| Eyes        | 54.0| 37.3| 53.7| 27.8   |\\n| Mouth       | 52.7| 20.5| 53.7| 12.4   |\\n| Faceswap    | 66.0| 64.4| 62.5| 60.3   |\\n| Total       | 55.8| 31.3| 55.6| 24.5   |\\n\\n(e) R-splicer with matching\\n\\n| Attribute   | AUC | F1  | mAP | Recall |\\n|-------------|-----|-----|-----|--------|\\n| Nose        | 54.4| 66.6| 58.5| 99.9   |\\n| Eyebrows    | 51.7| 66.6| 52.7| 99.9   |\\n| Eyes        | 53.5| 66.3| 53.7| 100.0  |\\n| Mouth       | 52.8| 66.8| 54.1| 100.0  |\\n| Faceswap    | 55.1| 66.4| 57.1| 100.0  |\\n| Total       | 53.5| 66.5| 55.2| 100.0  |\\n\\n(f) R-splicer with CLIP matching\"}"}
{"id": "cR3T1ZYN8I", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Attribute  | AUC | F1 | mAP | Recall |\\n|------------|-----|----|-----|--------|\\n| Bangs      | 51.3| 9.1| 61.1| 5.2    |\\n| Eyeglasses | 50.5| 3.0| 63.7| 1.6    |\\n| Beard      | 50.2| 1.2| 61.1| 0.6    |\\n| Smiling    | 50.0| 1.2| 58.3| 0.6    |\\n| Young      | 50.0| 76.8| 62.3| 99.9   |\\n| Total      | 50.4| 18.3| 61.3| 21.6   |\\n| (a) SeqDeepFake attributes with matching |  |\\n| Bangs      | 49.6| 73.6| 58.5| 99.6   |\\n| Eyeglasses | 47.2| 75.7| 60.5| 99.1   |\\n| Beard      | 58.0| 73.9| 64.7| 99.3   |\\n| Smiling    | 49.2| 72.0| 56.0| 99.5   |\\n| Young      | 50.5| 74.9| 60.0| 99.5   |\\n| Total      | 50.9| 74.0| 59.9| 99.4   |\\n| (b) SeqDeepFake attributes with CLIP matching |  |\\n| nose       | 50.1| 0.9 | 55.5| 0.4    |\\n| eye        | 50.0| 0.1 | 66.4| 0.1    |\\n| eyebrow    | 50.0| 0.0 | 57.7| 0.0    |\\n| lip        | 49.7| 0.4 | 67.5| 0.2    |\\n| hair       | 50.4| 18.8| 49.0| 16.4   |\\n| Total      | 50.0| 5.1 | 59.2| 4.3    |\\n| (c) SeqDeepFake comp. with matching |  |\\n| nose       | 49.2| 69.0| 52.0| 99.9   |\\n| eye        | 46.9| 77.3| 61.7| 99.8   |\\n| eyebrow    | 49.5| 70.8| 54.1| 99.9   |\\n| lip        | 49.5| 78.2| 63.6| 99.9   |\\n| hair       | 49.6| 63.3| 46.1| 99.9   |\\n| Total      | 48.9| 71.7| 55.5| 99.9   |\\n| (d) SeqDeepFake comp. with CLIP matching |  |\\n| nose       | 54.0| 16.8| 55.2| 9.3    |\\n| eyebrows   | 50.4| 3.3 | 52.1| 1.7    |\\n| eyes       | 50.4| 9.3 | 51.6| 5.4    |\\n| mouth      | 53.0| 19.4| 54.0| 11.8   |\\n| faceswap   | 53.5| 68.7| 53.4| 96.4   |\\n| Total      | 52.3| 23.5| 53.2| 24.9   |\\n| (e) R-splicer with matching |  |\\n| nose       | 50.8| 66.6| 52.0| 99.9   |\\n| eyebrows   | 49.2| 66.6| 49.3| 99.9   |\\n| eyes       | 49.9| 66.3| 49.2| 99.9   |\\n| mouth      | 51.0| 66.8| 51.0| 100.0  |\\n| faceswap   | 41.3| 66.4| 45.3| 100.0  |\\n| Total      | 48.5| 66.5| 49.3| 99.9   |\\n| (f) R-splicer with CLIP matching | 25 |\"}"}
{"id": "cR3T1ZYN8I", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"|                | AUC | F1  | mAP | Recall |\\n|----------------|-----|-----|-----|--------|\\n| Bangs          | 54.4| 27.4| 62.9| 16.7   |\\n| Eyeglasses     | 60.6| 65.6| 69.1| 59.7   |\\n| Beard          | 51.0| 6.3 | 61.6| 3.3    |\\n| Smiling        | 51.9| 11.5| 59.5| 6.2    |\\n| Young          | 50.0| 76.8| 62.3| 100.0  |\\n| Total          | 53.6| 37.5| 63.1| 37.2   |\\n\\n(a) SeqDeepFake attributes with matching\\n\\n|                | AUC | F1  | mAP | Recall |\\n|----------------|-----|-----|-----|--------|\\n| Bangs          | 45.8| 46.3| 56.2| 40.7   |\\n| Eyeglasses     | 56.1| 65.3| 65.7| 65.9   |\\n| Beard          | 55.0| 60.3| 63.4| 59.2   |\\n| Smiling        | 46.3| 55.0| 54.7| 56.0   |\\n| Young          | 50.2| 50.6| 61.9| 43.7   |\\n| Total          | 50.7| 55.5| 60.4| 53.1   |\\n\\n(b) SeqDeepFake attributes with CLIP matching\\n\\n|                | AUC | F1  | mAP | Recall |\\n|----------------|-----|-----|-----|--------|\\n| nose           | 50.1| 0.9 | 55.5| 0.4    |\\n| eye            | 50.0| 0.1 | 66.4| 0.1    |\\n| eyebrow        | 50.0| 0.0 | 57.7| 0.0    |\\n| lip            | 49.7| 0.4 | 67.5| 0.2    |\\n| hair           | 50.4| 18.8| 49.0| 16.4   |\\n| Total          | 50.0| 5.1 | 59.2| 4.3    |\\n\\n(c) SeqDeepFake comp. with matching\\n\\n|                | AUC | F1  | mAP | Recall |\\n|----------------|-----|-----|-----|--------|\\n| nose           | 54.8| 56.2| 56.7| 56.6   |\\n| eye            | 58.2| 57.7| 68.5| 49.4   |\\n| eyebrow        | 51.2| 56.1| 55.1| 56.8   |\\n| lip            | 60.1| 71.9| 70.7| 76.4   |\\n| hair           | 54.1| 57.3| 48.3| 70.4   |\\n| Total          | 55.6| 59.8| 59.9| 61.9   |\\n\\n(d) SeqDeepFake comp. with CLIP matching\\n\\n|                | AUC | F1  | mAP | Recall |\\n|----------------|-----|-----|-----|--------|\\n| nose           | 53.1| 18.4| 54.0| 10.6   |\\n| eyebrows       | 50.5| 7.5 | 52.1| 4.0    |\\n| eyes           | 52.1| 38.5| 52.5| 29.9   |\\n| mouth          | 53.7| 29.4| 54.2| 19.2   |\\n| faceswap       | 59.5| 61.9| 57.1| 63.7   |\\n| Total          | 53.8| 31.1| 54.0| 25.5   |\\n\\n(e) R-splicer with matching\\n\\n|                | AUC | F1  | mAP | Recall |\\n|----------------|-----|-----|-----|--------|\\n| nose           | 53.5| 57.2| 53.5| 63.7   |\\n| eyebrows       | 52.7| 59.1| 51.9| 69.6   |\\n| eyes           | 51.4| 57.2| 50.7| 66.0   |\\n| mouth          | 54.3| 62.2| 53.7| 78.3   |\\n| faceswap       | 58.0| 65.7| 55.5| 90.1   |\\n| Total          | 54.0| 60.3| 53.1| 73.5   |\\n\\n(f) R-splicer with CLIP matching\"}"}
{"id": "cR3T1ZYN8I", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"|       | AUC   | F1    | mAP   | Recall |\\n|-------|-------|-------|-------|--------|\\n| Bangs| 48.9  | 56.9  | 59.9  | 55.9   |\\n| Eyeglasses | 53.6  | 39.0  | 65.2  | 27.4   |\\n| Beard | 52.5  | 16.7  | 62.5  | 9.5    |\\n| Smiling | 51.1  | 12.1  | 59.0  | 7.1    |\\n| Total | 51.1  | 40.0  | 61.7  | 39.2   |\\n\\n(a) SeqDeepFake attributes with matching:\\n\\n|       | AUC   | F1    | mAP   | Recall |\\n|-------|-------|-------|-------|--------|\\n| Bangs| 46.4  | 73.5  | 56.9  | 99.9   |\\n| Eyeglasses | 51.9  | 75.9  | 65.9  | 100.0  |\\n| Beard | 57.9  | 74.1  | 64.0  | 100.0  |\\n| Smiling | 52.7  | 72.0  | 59.2  | 100.0  |\\n| Total | 51.3  | 74.1  | 61.0  | 100.0  |\\n\\n(b) SeqDeepFake attributes with CLIP matching:\\n\\n|       | AUC   | F1    | mAP   | Recall |\\n|-------|-------|-------|-------|--------|\\n| Nose  | 48.8  | 19.7  | 54.9  | 12.7   |\\n| Eye   | 49.7  | 3.2   | 66.4  | 1.6    |\\n| Eyebrow | 49.9  | 7.1   | 57.7  | 3.8    |\\n| Lip   | 50.3  | 12.3  | 67.8  | 6.8    |\\n| Hair  | 49.3  | 43.2  | 48.4  | 43.9   |\\n| Total | 49.6  | 17.1  | 59.0  | 13.8   |\\n\\n(c) SeqDeepFake comp. with matching:\\n\\n|       | AUC   | F1    | mAP   | Recall |\\n|-------|-------|-------|-------|--------|\\n| Nose  | 51.9  | 68.9  | 53.6  | 99.9   |\\n| Eye   | 50.0  | 77.4  | 63.1  | 100.0  |\\n| Eyebrow | 48.7  | 70.8  | 54.1  | 100.0  |\\n| Lip   | 50.8  | 78.2  | 65.3  | 100.0  |\\n| Hair  | 46.8  | 63.3  | 44.5  | 100.0  |\\n| Total | 49.6  | 71.7  | 56.1  | 100.0  |\\n\\n(d) SeqDeepFake comp. with CLIP matching:\\n\\n|       | AUC   | F1    | mAP   | Recall |\\n|-------|-------|-------|-------|--------|\\n| Nose  | 57.6  | 34.8  | 57.4  | 22.5   |\\n| Eyebrow | 53.5  | 19.5  | 54.3  | 11.2   |\\n| Eye   | 57.4  | 43.2  | 56.2  | 33.3   |\\n| Mouth | 55.9  | 36.7  | 55.8  | 25.6   |\\n| Faceswap | 68.8  | 73.8  | 63.6  | 84.6   |\\n| Total | 58.7  | 41.6  | 57.4  | 35.5   |\\n\\n(e) R-splicer with matching:\\n\\n|       | AUC   | F1    | mAP   | Recall |\\n|-------|-------|-------|-------|--------|\\n| Nose  | 56.8  | 66.6  | 60.0  | 100.0  |\\n| Eyebrow | 54.5  | 66.6  | 56.2  | 100.0  |\\n| Eye   | 56.5  | 66.3  | 56.0  | 100.0  |\\n| Mouth | 55.0  | 66.8  | 55.6  | 100.0  |\\n| Faceswap | 60.7  | 66.4  | 59.3  | 100.0  |\\n| Total | 56.7  | 66.5  | 57.4  | 100.0  |\\n\\n(f) R-splicer with CLIP matching:\\n\\n|       | AUC   | F1    | mAP   | Recall |\\n|-------|-------|-------|-------|--------|\\n| Nose  | 57.6  | 34.8  | 57.4  | 22.5   |\\n| Eyebrows | 53.5  | 19.5  | 54.3  | 11.2   |\\n| Eyes  | 57.4  | 43.2  | 56.2  | 33.3   |\\n| Mouth | 55.9  | 36.7  | 55.8  | 25.6   |\\n| Faceswap | 68.8  | 73.8  | 63.6  | 84.6   |\\n| Total | 58.7  | 41.6  | 57.4  | 35.5   |\"}"}
{"id": "cR3T1ZYN8I", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Claims\\n\\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\\n\\nAnswer: [Yes]\\n\\nJustification: The paper is introducing a benchmarking method for fine-grained deepfake detection using VLLMs. The main contributions of the method are converting the multi-label problem to a VQA one, assesses the zero-shot capabilities of state-of-the-art VLLMs and ensures fair comparison for current and future models. This is followed up in the methodology and experimental sections.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.\\n\\n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.\\n\\n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.\\n\\n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.\\n\\n2. Limitations\\n\\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\\n\\nAnswer: [Yes]\\n\\nJustification: The paper briefly discusses limitations of zero-shot evaluation compared to purpose-built models. In addition, as the method is on Face Forgery, limitations with regards to potential bias are discussed.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\\n\\n\u2022 The authors are encouraged to create a separate \u201cLimitations\u201d section in their paper.\\n\\n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\\n\\n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\\n\\n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.\\n\\n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\\n\\n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.\\n\\n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.\"}"}
{"id": "cR3T1ZYN8I", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. Theory Assumptions and Proofs\\n\\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\\n\\nAnswer: [N/A]\\n\\nJustification:\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not include theoretical results.\\n\\n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.\\n\\n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorem.\\n\\n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.\\n\\n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.\\n\\n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.\\n\\n4. Experimental Result Reproducibility\\n\\nQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\\n\\nAnswer: [Yes]\\n\\nJustification: In addition to the methodology, we provide code and commit to make it public upon publication to ensure reproducibility.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not include experiments.\\n\\n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.\\n\\n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.\\n\\n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.\\n\\n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example:\\n  \\n  (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.\\n\\n  (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.\\n\\n  (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).\\n\\n  (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.\"}"}
{"id": "cR3T1ZYN8I", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.\\n\\n5. Open access to data and code\\n\\nQuestion: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?\\n\\nAnswer: [Yes]\\n\\nJustification: The method is using nine previously published benchmarks to compare open-source VLLM models. The benchmarking code will be made publicly available upon publication, including pre-processing steps.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that paper does not include experiments requiring code.\\n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\\n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).\\n\u2022 The instructions should contain the exact command and environment needed to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\\n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.\\n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.\\n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).\\n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.\\n\\n6. Experimental Setting/Details\\n\\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?\\n\\nAnswer: [Yes]\\n\\nJustification: Test details are explained in the paper and additional details are provided in the code.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not include experiments.\\n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.\\n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material.\\n\\n7. Experiment Statistical Significance\\n\\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?\\n\\nAnswer: [No]\\n\\nJustification: The evaluation is conducted with a fixed seed to account for the stochasticity in language generation, thus all experiments are ceteris paribus.\"}"}
{"id": "cR3T1ZYN8I", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The answer NA means that the paper does not include experiments.\\n\\nThe authors should answer \u201cYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.\\n\\nThe factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).\\n\\nThe method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.).\\n\\nThe assumptions made should be given (e.g., Normally distributed errors).\\n\\nIt should be clear whether the error bar is the standard deviation or the standard error of the mean.\\n\\nIt is OK to report 1-sigma error bars, but one should state it. The authors should preferentially report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.\\n\\nFor asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).\\n\\nIf error bars are reported in tables or plots, the authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.\"}"}
{"id": "cR3T1ZYN8I", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Justification: The paper includes an ethics statement regarding generative technology, which includes vision-language models.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that there is no societal impact of the work performed.\\n\\n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.\\n\\n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.\\n\\n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.\\n\\n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.\\n\\n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).\\n\\n11. Safeguards\\n\\nQuestion: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?\\n\\nAnswer: [N/A]\\n\\nJustification:\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper poses no such risks.\\n\\n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.\\n\\n\u2022Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.\\n\\n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.\\n\\n12. Licenses for existing assets\\n\\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?\\n\\nAnswer: [Yes]\\n\\nJustification: The paper is a method for benchmarking VLLMs on face forgery detection tasks to ensure a fair comparison; thus, no data is released. The datasets and models used are all referenced. Upon publication, links to the repositories/original datasets will be included in the code for reproducibility.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not use existing assets.\"}"}
{"id": "cR3T1ZYN8I", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Hitchhiker's Guide to Fine-Grained Face Forgery Detection Using Common Sense Reasoning\\n\\nNiki Maria Foteinopoulou\\nEnjie Ghorbel\\nDjamila Aouada\\n\\nCVI, SnT, University of Luxembourg\\nCristal Laboratory, National School of Computer Sciences, University of Manouba\\n{niki.foteinopoulou, enjie.ghorbel, djamila.aouada}@uni.lu\\n\\nAbstract\\n\\nExplainability in artificial intelligence is crucial for restoring trust, particularly in areas like face forgery detection, where viewers often struggle to distinguish between real and fabricated content. Vision and Large Language Models (VLLM) bridge computer vision and natural language, offering numerous applications driven by strong common-sense reasoning. Despite their success in various tasks, the potential of vision and language remains underexplored in face forgery detection, where they hold promise for enhancing explainability by leveraging the intrinsic reasoning capabilities of language to analyse fine-grained manipulation areas. For that reason, few works have recently started to frame the problem of deepfake detection as a Visual Question Answering (VQA) task, nevertheless omitting the realistic and informative multi-label setting. With the rapid advances in the field of VLLM, an exponential rise of investigations in that direction is expected. As such, there is a need for a clear experimental methodology that converts face forgery detection to a Visual Question Answering (VQA) task to systematically and fairly evaluate different VLLM architectures. Previous evaluation studies in deepfake detection have mostly focused on the simpler binary task, overlooking evaluation protocols for multi-label fine-grained detection and text-generative models. We propose a multi-staged approach that diverges from the traditional binary evaluation protocol and conducts a comprehensive evaluation study to compare the capabilities of several VLLMs in this context. In the first stage, we assess the models' performance on the binary task and their sensitivity to given instructions using several prompts. In the second stage, we delve deeper into fine-grained detection by identifying areas of manipulation in a multiple-choice VQA setting. In the third stage, we convert the fine-grained detection to an open-ended question and compare several matching strategies for the multi-label classification task. Finally, we qualitatively evaluate the fine-grained responses of the VLLMs included in the benchmark. We apply our benchmark to several popular models, providing a detailed comparison of binary, multiple-choice, and open-ended VQA evaluation across seven datasets.\\n\\nhttps://nickyfot.github.io/hitchhickersguide.github.io/\\n\\n1 Introduction\\n\\nRecent developments in deep generative modelling have resulted in hyper-realistic synthetic images/videos with no clear visible artefacts, making the viewers question whether they can still trust their eyes. Unfortunately, despite its relevance in a wide range of applications, such technology poses a threat to society as it can be used for malicious activities [16]. In a world where synthetic images of a person, known as deepfakes, can easily be generated, it becomes crucial to fight misinformation...\"}"}
{"id": "cR3T1ZYN8I", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"formation not only by identifying manipulated images/videos in an automated manner but also by explaining the decision behind this classification to reinstate trust in Artificial Intelligence.\\n\\nNumerous successful works on deepfake detection have been proposed in the literature to tackle the risks of face forgery \\\\[37, 77, 52, 7, 81\\\\]. Existing methods primarily rely on deep binary classifiers, resulting in black-box models that predict whether an input is real or fake. Consequently, explaining why those predictions are being made is not straightforward. To handle this issue, a few interpretable deepfake detection methods have been introduced by examining attention maps or weight activations \\\\[88, 72\\\\] to identify fine-grained areas of manipulation; however, these are based on a post-hoc analysis and thereby do not intrinsically incorporate an explainable mechanism. On the other hand, Vision Large Language Models (VLLMs) have emerged as a pioneering branch of generative Artificial Intelligence (AI), showcasing advancements in common sense reasoning and an inherent explainability that arises from the intrinsic nature of language \\\\[22\\\\]. They have demonstrated impressive capabilities in tasks such as Visual Question Answering (VQA) \\\\[47\\\\] and the generation of descriptive content for downstream applications \\\\[71\\\\], hence bridging the gap between vision-language understanding and contextual reasoning. However, despite these achievements, the explainable power of VLLMs remains under-explored in the field of deepfake detection, with only a handful of works mostly exploring the vision and language capabilities for the binary classification of fake/real images \\\\[26, 8, 31, 70\\\\] all of which are evaluated on different benchmarks and metrics. To the best of our knowledge, Zhang \\\\[87\\\\] et al. is the only work employing a VQA approach in deepfake detection by proposing to extend the FF++ dataset with captions in natural language generated by humans. However, this work targets only one manipulated region at once, while deepfakes can incorporate several stacked manipulations \\\\[63, 68\\\\]. Moreover, the provided augmented FF++ dataset does not allow for cross-dataset evaluation in a VQA setting without an extensive annotation effort, making it difficult to investigate the generalisation aspect. In addition, the fine-grained evaluation in previous works is limited as the more challenging open-ended VQA task is not explored.\\n\\nExplainable fine-grained detection \u2013that is, identifying manipulation beyond the binary fake/real decision\u2013 in natural language is still in its infancy. However, as VLLM works for deepfake detection are expected to appear following the overwhelming success of foundation models in other tasks, two research questions need to be addressed: 1) \\\"To what extent can existing VLLMs detect deepfake images and what rationale supports the decision?\\\" and 2) \\\"How do we fairly and comprehensively evaluate VLLMs in the fine-grained task?\\\". In deepfake detection, benchmarks have mainly focused on binary or multi-class decisions and discriminative networks \\\\[82, 87\\\\], making them unsuitable to answer these research questions. Indeed, they do not propose a unified method to match the generated responses to fine-grained multi-label categories. Similarly, existing benchmarks in Visual Question Answering (VQA) \\\\[19, 10\\\\] primarily address multi-class tasks, which may not be suitable for the multi-label nature of fine-grained deepfake detection as highlighted in \\\\[63\\\\].\\n\\nIn this work, our objective is to conduct a thorough quantitative and qualitative evaluation of VLLMs for the task of fine-grained multi-label deepfake detection in a systematic and scientific approach, employing a multi-stage protocol without costly human captioning efforts. In the first stage, we assess the models' performance on the binary task using various prompts while also evaluating the model's sensitivity to the provided instructions. In the second stage, we delve into multi-label fine-grained detection, aiming to identify areas of manipulation within a multiple-choice Visual Question Answering (VQA) framework, i.e. what areas from a given list are identified as manipulated. Subsequently, in the third stage, we extend our investigation by converting the fine-grained detection task into an open-ended question \u2013that is, identifying areas without instructing the model to select from a list of categories. Here, as the task is a multi-label problem, we compare two matching strategies: a) using the cosine similarity between the generated text and ground truth labels and b) counting the occurrence of the class name in the generated text. Finally, we qualitatively evaluate the fine-grained responses generated by the VLLMs included in our benchmark, providing nuanced and new insights into their performance.\\n\\nThe main contributions of this work can be thus summarised as follows:\\n\\n\u2022 We introduce a novel evaluation protocol for deepfake detection under the Visual Question Answering (VQA) multi-label setting and without the use of human annotations. This is different from \\\\[87\\\\] that is based on a succession of yes/no questions for fine-grained areas, resulting in a binary classification setting and relies on a relatively small dataset which cannot be extended without costly annotation efforts. In addition, our multi-stage protocol\"}"}
{"id": "cR3T1ZYN8I", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"allows for open-ended VQA evaluation, which is a more challenging task. To the best of our knowledge, this is the first work to do so in the field of deepfake detection, offering a fresh perspective on explainability through fine-grained multi-label analysis.\\n\\n\u2022 We present a systematic, unified evaluation study of current state-of-the-art (SOTA) VLLMs, facilitating consistent assessment across different models. This framework is designed to be extendable to any existing or future deepfake dataset, ensuring fair and comprehensive comparisons with future models, thus promoting transparency and reproducibility in the evaluation process.\\n\\n\u2022 Through extensive comparison and analysis of the tested models and an ensemble of models, our study yields new insights into the capabilities and limitations of VLLMs in the context of deepfake detection. We will use these insights to advance research in the domain and hope to inform future developments and optimisations in model design and evaluation strategies.\\n\\n2 Related Work\\n\\nDeepFake generation encompasses various forms of facial manipulation, including face reenactment [6], face swapping [24, 45], and entirely generated face images [32, 69].\\n\\nDeepfake detection algorithms classify samples as real or fake [7, 81, 77, 37], relying on artefacts left by manipulation methods, often analysed qualitatively for explainability [21, 91]. However, this qualitative analysis happens on a secondary stage and primarily depends on human observers. While generative methods often use natural language instructions [54, 75, 58, 55, 78], explaining manipulations in natural language \u2014a natural extension of the generation process to detection\u2014 is still an emerging area.\\n\\nWith the rise of VLLMs, recent works [8, 31, 70, 30, 73] explore vision and language for face forgery detection, primarily focusing on binary detection in a retrieval setting, with fewer [70, 52, 91] examining fine-grained areas usually as a secondary task. The latter have relied on generated pseudo-fake datasets to improve generalisation [70, 52, 91], which have a major drawback\u2014that is, the use of pseudo-fake datasets hampers fair comparisons and does not reflect the current state-of-the-art in deepfake generation.\\n\\nSeveral VLLMs foundation models [4, 15, 35, 44, 43], bridge the gap between vision and language. These are typically trained on very large datasets with general knowledge. As the computational and data resources needed to train VLLMs from scratch are very high, numerous works leverage the pre-trained networks in three main directions: a) exploring the latent feature space [14, 57] of vision and language, b) parameter efficient training in a downstream task [39, 74, 18] and c) evaluating foundation models in new domains [65, 76].\\n\\nBenchmarks for classification tasks [19, 38, 2] in a VQA setting typically address the multi-class paradigm, which may not be appropriate for addressing explainability in DeepFake detection by adopting a multi-label fine-grained strategy, as several areas can be manipulated at once. A few preliminary works in DeepFake detection [3, 26, 87] benchmark ChatGPT4 [https://openai.com/gpt-4] and Gemini [https://gemini.google.com], however, these have primarily focused on the more straightforward binary task and did not explore the reasoning capabilities of VLLMs for fine-grained labels. Furthermore, both these works focused on VLLMs that are not open-sourced with limited information regarding their training set and architecture; thus, it is not possible to assess whether the benchmarks are, in fact, zero-shot or whether they have been trained on deepfake-related image-language pairs. Zhang et al. [87] propose extending FF++ annotations with captions in natural language using human annotators. However, this method is limited to binary decisions, while a given deepfake image can undergo several manipulations. Furthermore, it does not explore the open-ended VQA setting and does not offer a method for cross-dataset evaluation without a costly annotation process. Within DeepFake detection, the vast majority of benchmarking works [60, 65, 46, 82, 34] have focused on binary discriminative networks and are therefore not fit to evaluate the capabilities of generative models such as VLLM, particularly for fine-grained labels.\\n\\nIn a nutshell, the main novelty of this benchmark compared to previous works [19, 82, 38, 2, 60, 65, 46, 34] is threefold: 1) it converts the multi-label classification task of face forgery detection to a\"}"}
{"id": "cR3T1ZYN8I", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Is this image manipulated? a) Yes b) No\\n\\na) Yes\\n\\nIn the image, the area that is manipulated is the woman's face. The photo has been digitally altered to enhance her features, such as her eyes, lips, and cheekbones, making her look more attractive.\\n\\nWhat areas are manipulated?\\n\\na) Nose, Eyebrows, Eyes, Lip, Hair\\n\\nFigure 1: Overview of the proposed benchmarking method, using multiple stages to evaluate the performance of VLLMs in the context of deepfake detection. In the first stage (a), we assess the binary classification performance of VLLMs. In the second stage (b), we perform a fine-grained classification using multiple-choice instruction. In the third and final stage (c), we ask the model to identify fine-grained areas in open-ended VQA. The image example is a sample from the SeqDeepFake dataset, and responses are generated using Llava-1.5.\\n\\nVQA task so that VLLM's common sense reasoning capabilities can be evaluated, 2) it systematically and consistently assesses VLLM capabilities on nine binary and three fine-grained benchmarks and 3) is offering an open source and extendable framework for future zero-shot or task-specific VLLMs, that ensures a fair comparison.\\n\\n3 Common Sense Reasoning for Face Forgery Detection\\n\\nPreliminaries:\\n\\nWe formalise the language generation process of VLLM architectures, akin to standard VQA models, where the model is prompted with an image and a query to produce an autoregressive answer. Given an image $X_v \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C}$ and a text prompt $X_t \\\\in \\\\mathbb{R}^{L \\\\times d}$ as input, a sentence $\\\\psi$ is generated represented as a sequence of word tokens. The generation can be represented by the function $p(\\\\psi | X_v, X_t) = \\\\prod_{|\\\\psi| \\\\geq j} p(\\\\psi_j | \\\\psi_{<j}, X_v, X_t)$, where $H \\\\times W \\\\times C$ represent the image dimension, $L$ is the number of tokens, $d$ is the embedding dimension, $\\\\psi = (\\\\psi_j)$ $0 \\\\leq j < |\\\\psi|$ is the generated sentence, and $|.|$ the cardinality. In VQA tasks, the model response aims to match human annotations. This task differs from typical classification problems due to the diverse semantic nature of questions and answers in natural language. The evaluation protocol is outlined for the binary case in Section 3.1 and for open-ended evaluation and multiple-choice of fine-grained labels in Section 3.2. An overview of the proposed method is shown in Figure 1.\\n\\n3.1 Binary Classification to VQA\\n\\nIn binary classification, the task is to predict whether the image sample is a product of face forgery. We create a benchmark to assess VLLM capability in binary Deepfake detection by transforming the discriminative task into a VQA problem. We consider only the positive category for each image $X_v$ to generate the relevant instruction; that is, we limit the prompt to identifying whether an image is a Deepfake and not whether it is a genuine sample. The prompt used is in the form:\\n\\n$$X_t = \\\\text{``Is this image [s_i]? a) Yes b) No''}$$ (1)\\n\\nwhere $s_i \\\\in S$ denotes a set of standard terms used to describe deep fakes in the English language. The synonyms are employed to assess the reasoning capabilities of each tested model by investigating whether the understanding of the model is robust to the given instruction.\"}"}
{"id": "cR3T1ZYN8I", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 Fine-Grained Labels:\\n\\nFine-grained labels typically refer to manipulation areas. Predicting them necessitates the use of multi-label classification, as multiple areas can be manipulated at once. Following the initial binary prompt, a follow-up prompt to explain what areas of manipulation are identified is given to the VLLM with the same image, as shown in Fig. 1. We propose using two versions of follow-up prompts, one as an open-ended question and one as a multiple-choice. Specifically, the open-ended follow-up prompt follows the template:\\n\\n\\\\[ X_t = \\\\text{``What area of this image is } [s_i] \\\\text{?''} \\\\] (2)\\n\\nFor the multiple-choice instruction, we follow the template:\\n\\n\\\\[ X_t = \\\\text{``Of the areas in the list } [\\\\text{cls}_0, \\\\ldots, \\\\text{cls}_|C|], \\\\text{which ones are } [s_i] \\\\text{?''} \\\\] (3)\\n\\nwhere \\\\( \\\\text{cls}_i \\\\in C \\\\) is the class name of the \\\\( i \\\\)-th class from the set of target classes \\\\( C \\\\).\\n\\n3.3 Matching Strategies:\\n\\nTo evaluate the generated responses, we employ several matching methods depending on the task. The stricter one uses an Exact Match (EM) approach, that estimates whether the generated sentence \\\\( \\\\psi \\\\) is exactly equal to the class name \\\\( \\\\text{cls}_i \\\\):\\n\\n\\\\[ p(\\\\hat{y}_i) = \\\\begin{cases} 1 & \\\\text{if } \\\\psi \\\\equiv \\\\text{cls}_i \\\\\\\\ 0 & \\\\text{if } \\\\psi \\\\neq \\\\text{cls}_i \\\\end{cases} \\\\] (4)\\n\\nwhere \\\\( \\\\hat{y}_i \\\\) is the prediction for the \\\\( i \\\\)-th class. In the given task, an answer is considered correct only if the model output exactly matches the class names or 'Yes'/'No' in the binary case. As the responses tend to be longer for fine-grained classification and reflect reasoning in natural language for a multi-label problem, a more appropriate matching strategy is to consider a response correct if the class name is Contained in the response, as proposed by Xu et al. [76] \u2013 that is \\\\( p(\\\\hat{y}_i) = 1 \\\\), if \\\\( \\\\text{cls}_i \\\\in \\\\psi \\\\) and 0 otherwise. We extend this to include synonyms of class names, as several ways exist to describe some areas (e.g. \\\"Bangs\\\" could also be described as \\\"Hair\\\"). Finally, we propose adapting the text-to-text score (CLIP distance) proposed by Conti et al. [14] for the multi-label task. This is done by using a sigmoid function over the cosine similarity matrix of the prediction embeddings and class name embeddings (obtained with a CLIP [59] text encoder), using an empirical temperature \\\\( t \\\\) of 0.5 so that \\\\( p(\\\\hat{y}_i) = \\\\sigma(<\\\\psi, \\\\text{cls}_i>_{1/t}) \\\\). The symbol \\\\(<,.,.>\\\\) denotes the cosine similarity of the text embeddings and \\\\( \\\\sigma(.\\\\) is the sigmoid function.\\n\\n4 Experimental Set-Up\\n\\n4.1 Tested VLLMs\\n\\nWe select four open-source state-of-the-art VLLMs to be included in this benchmark; specifically, we include LlaVa-1.5 [43] (an improved version of the LlaVa architecture [44]), BLIP2 [35] and finally InstructBlip [15] with Flan-T5 and Flan-T5-xxl language generators [13]. Finally, for the binary task, we include the CLIP [59] performance as a baseline and compare it against an ensemble of BLIP2 [35] and LlaVa-1.5 [43] following the ensembling strategies for VQA tasks [5], and GPT4v as an upper bound. Experiments using GPT4v are performed on a subset of 5k samples selected from each dataset, and thus, the results may be susceptible to some degree of bias from the sampling, which needs to be considered when comparing the models. The selection of the VLLM is guided by three factors. First, we select architectures with publicly available weights and training methods to ensure transparency and fairness in the evaluation. Second, we include methods that generate output in Natural Language rather than a set of features or classification predictions. Finally, we select methods that have achieved state-of-the-art performance on several zero-shot tasks. Additional model details, such as the number of parameters and pre-training information of the tested models, can be found in Appendix A. [https://openai.com/index/gpt-4v-system-card/](https://openai.com/index/gpt-4v-system-card/).\"}"}
{"id": "cR3T1ZYN8I", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2 Datasets\\nWe evaluate the performance of our method on seven published challenging benchmarks and one pseudo-fake dataset; more specifically, seven datasets for binary detection and two for the fine-grained task. All are evaluated at the frame level, as in previous image works [80, 52, 49].\\n\\n**FF++:** [60] consists of over 20k images of DeepFake images from 1000 videos, using four types of manipulation methods: Deepfakes, Face2Face, FaceSwap and NeuralTextures. The dataset is split into train, validation, and test with an 80%, 10%, and 10% split, respectively.\\n\\n**DFDC:** [17] is composed of 5k videos of real and manipulated faces split into 4,464 unique training clips and 780 unique test clips.\\n\\n**Celeb-DF:** [41] includes 590 original videos collected from YouTube with subjects of different ages, ethnic groups and genders, and 5639 corresponding manipulated videos.\\n\\n**WildDeepFake:** [90] is a challenging dataset for in-the-wild detection, which consists of 7,314 face sequences extracted from 707 videos that are collected completely from the internet.\\n\\n**StyleGAN:** Two sub-sets consist of curated images generated with StyleGAN3 [29] and StyleGAN2 [28] along with original ones for binary detection of facial images.\\n\\n**SeqDeepFake:** [63] dataset consists of 85k sequential manipulated face images based on two representative facial manipulation techniques, facial components manipulation [32] and facial attributes manipulation [27]. The labels include annotations of manipulation sequences with diverse sequence lengths.\\n\\n**R-splicer:** Augmenting real data by generating pseudo-fake images is a common practice in deepfake detection [36, 49, 11, 66, 89, 40]. Such methods simulate characteristic face-swap artefacts using simplistic operations on a predefined set of regions. In this work, we use a spliced dataset of 59k images to evaluate fine-grained labels of five regions \u2013entire face, mouth, nose, eyes, eyebrows\u2013 as implemented by Mejri et al. [49].\\n\\n4.3 Metrics\\nAccuracy and the Area Under the Receiver Operating Characteristics (ROC) (AUC) are the most common metrics used in DeepFake detection [82, 52, 65]. However, as the datasets in the task are massively imbalanced, we also use the harmonic mean of Precision and Recall (F1-score) for the binary task. Furthermore, we note that as AUC is a measure of the classifier's performance at different thresholds, it has very limited value in the VQA task where matching strategies result in polarised decisions; however, we include it for reference. In the fine-grained task, we use mean Average Precision (mAP), AUC and F1-score as indicators of classification performance.\\n\\n5 Results\\n5.1 Binary Classification\\nRobustness to different prompts: We use seven synonyms for the positive class: \\\"manipulated\\\", \\\"deepfake\\\", \\\"synthetic\\\", \\\"altered\\\", \\\"fabricated\\\", \\\"face forgery\\\" and \\\"falsified\\\". As the binary task is simple and the instruction format is a 'Yes' or 'No' question, we use EM as a matching approach in this evaluation. In Fig. 2, we see the performance of each tested model under the binary detection setting on the two sub-sets of SeqDeepFake [63] and the R-splicer dataset using the three best performing synonyms: \\\"manipulated\\\", \\\"synthetic\\\" and \\\"altered\\\". The first observation is that no VLLM clearly outperforms others across all datasets and metrics. However, we see that BLIP-2 [35] has the most robust performance to the given instruction, even though it is the smallest in terms of parameters. Furthermore, the additional parameters of T5-xxl [13] do not seem to aid the task compared to the base InstructBLIP [15] with T5 generator, as the base model performs comparably better across most benchmarks. We theorise that as the VLLMs have not been explicitly trained on image-language pairs of manipulated images, a large number of parameters on the language generation leads to more hallucinations [25, 79] for this simple but abstract task. Compared to the CLIP [59], models appear to have competitive performance with the exception of InstructBLIP with T5xxl LLM. When both base models, i.e. BLIP-2 [35] and LlaVa [43], have relatively good performance, the ensemble shows marginal improvement, particularly in terms of Accuracy and F1; however, this is not consistent therefore we do not continue the investigation to fine-grained labels.\\n\\nThe detailed performance of all models and synonyms across all datasets and additional analysis on CLIP [59] features can be found in the Appendix.\\n\\nOverall model performance: We average the performance of the three best-performing synonyms on all nine benchmarks in Fig. 3. No model clearly outperforms others across all metrics and...\"}"}
{"id": "cR3T1ZYN8I", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Exact Match (EM)\\nPerformance of each VLLM in terms of Accuracy (top), AUC (mid) and F1-score (bottom) for the top 3 synonyms \u201cmanipulated\u201d, \u201csynthetic\u201d and \u201caltered\u201d.\\n\\nFigure 3: Exact Match (EM)\\nPerformance of each VLLM on all nine benchmarks datasets; however, we can observe competitive performance from BLIP-2 on the binary task, even though it is the smallest model in terms of parameters. We also see that all models struggle with the more challenging in-the-wild datasets, such as CelebDF, which highlights the need for further development to achieve adequate generalisation. Performance of GPT4v should be treated as an upper bound as we cannot assess whether the model has been trained on samples of the selected datasets. We see that GPT4v vastly outperforms the selected VLLMs on three benchmarks and has comparable performance on the rest, with the exception of FF++.\\n\\nVision Encoder Finetuning:\\nWe finetune contrastively the vision encoder of LlaVa on FF++ using a sigmoid loss over an ensemble of prompts for the real/fake categories, and evaluate as described in the previous section. Training details for the vision encoder can be found in Appendix C.\"}"}
{"id": "cR3T1ZYN8I", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"architecture with the fine-tuned vision encoder shows improved within dataset and cross-dataset performance as shown in Tab. 1. Even without detailed captions or updating the LLM weights, we see there are still gains from a task specific vision encoder, particularly in terms of F1-score with an average improvement of nearly 4% within dataset and nearly 2% cross-dataset (for SeqDeepFake, CelebDF and StyleGAN2).\\n\\nTable 1: Binary performance of LlaVa-1.5 with a fine-tuned Vision Encoder against the zero-shot baseline.\\n\\n| Model       | Baseline | LlaVa w. fine-tuned Vision Encoder |\\n|-------------|----------|------------------------------------|\\n| FF++        | 64.54    | 64.57 (+0.03%)                     |\\n| SeqDeepfake Attr. | 58.92    | 61.22 (+2.30%)                     |\\n| SeqDeepfake Comp. | 84.62    | 84.24 (-0.37%)                     |\\n| R-Splicer   | 87.11    | 87.31 (+0.20%)                     |\\n| DFDC        | 54.02    | 53.86 (-0.16%)                     |\\n| CelebDF     | 35.67    | 37.60 (+1.93%)                     |\\n| DFW         | 53.35    | 53.45 (+0.10%)                     |\\n| StyleGAN2   | 33.67    | 35.20 (+1.53%)                     |\\n| StyleGAN3   | 39.80    | 39.30 (-0.50%)                     |\\n\\nMetrics:\\nThe selected metrics are limited information we can get from the standard Accuracy and AUC used in the binary task. Both are heavily skewed by the label distribution, which is typically imbalanced in deepfake datasets; however, the AUC may not be fit for VLLMs as AUC measures performance at different thresholds, which are not present with EM and contains matching strategies. As such, we argue that for the task at hand, the F1-score \u2013and consequently robust to imbalance metrics\u2013 are more appropriate.\\n\\n5.2 Fine-grained Evaluation\\nFor the fine-grained task, we evaluate the performance of the selected models in the open and closed vocabulary settings. The fine-grained labels are evaluated on samples where the ground truth is positive \u2013 i.e., on DeepFake samples.\\n\\nOpen-Ended VQA:\\nWe first evaluate the selected VLLMs under the open vocabulary VQA setting on the three fine-grained datasets. The results using contains and CLIP distance matching are reported in Tab. 2a and Tab. 2b respectively. An EM strategy is not possible in multi-label tasks, so no such evaluation is performed. No model clearly outperforms others across all metrics and datasets. In fact, we can observe that, in most cases, they have comparable performance. This holds true for both contains and CLIP distance metrics. In terms of matching strategy, using the CLIP distance consistently and greatly improves recall, as is evident by the improvement in the F1-score and explicitly shown in Appendix H. This matching approach slightly lowers the mAP and AUC scores compared to the contains metrics; however, using the cosine distance to match the open-ended responses to the class categories semantically may offer a more reliable output for the class of interest, as seen by the F1-score.\\n\\nMultiple choice VQA:\\nThe performance of the VLLMs on the multiple-choice instruction is shown in Fig. 4. Even though the open-ended setting is theoretically more challenging, the performances of all tested models are comparable to each other and worse on the multiple-choice instruction for both mAP and AUC. Regarding the F1-score, however, LlaVa consistently performs better than other models. Under the multiple-choice setting, we observe that the models tend to mention all label names, which raises the number of False Positives \u2013a significant limitation of the multiple choice setting\u2013 or respond with \u201cAll of them\u201d or \u201cNone of them\u201d, which makes matching of any sort more challenging and is reflected even more in the lower F1 score. Appendix G includes detailed metrics for each category.\"}"}
{"id": "cR3T1ZYN8I", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Model performance on open-ended fine-grained detection using CLIP matching\\n\\n|                  | mAP | AUC | F1 | mAP | AUC | F1 | mAP | AUC | F1 |\\n|------------------|-----|-----|----|-----|-----|----|-----|-----|----|\\n| SeqDeepFake      | 63.0| 53.6| 73.5| 59.9| 50.9| 71.0| 60.4| 50.7| 71.7|\\n| SeqDeepFake compelments | 59.5| 50.5| 14.7| 14.7| 14.7| 14.7| 14.7| 14.7| 14.7|\\n| R-Splicer        | 55.8| 55.6| 31.3| 52.3| 53.2| 23.5| 58.7| 57.5| 41.6|\\n\\n(a) Assessment of model performance during open-ended evaluation with CLIP matching.\\n\\n(b) Assessment of model performance during open-ended evaluation with CLIP distance matching.\\n\\nFigure 4: Assessment of model performance in multiple-choice settings, in terms of mAP, AUC, and F1 during multiple-choice evaluation with CLIP matching.\\n\\n5.3 Qualitative Evaluation\\n\\nAs the BertScore is shown to correlate with human evaluation, we first present the Bert-score precision, recall, and F1 scores achieved by each model for the fine-grained open-ended responses compared with ground truth references that have been formatted using the prompt: \\\"The areas that are [s] are [c].\\\" The results of this evaluation, along with the score of human annotators on a subset of the R-Splicer dataset, are shown in Tab. 3. As in previous sections, no model clearly outperforms others across all benchmarks; however, we see that Llava-1.5 has the most competitive performance for most benchmarks, closely followed by InstructBLIP. This is consistent with qualitative evaluations on VQA tasks.\\n\\nOverall performance: In the simpler binary setting, BLIP-2 is more robust to instruction than other models with more parameters; however, when it comes to fine-grained evaluation, larger models show an advantage in reasoning and identifying areas of manipulation in the open-ended and multiple-choice settings. It is, however, worth noting that no model clearly outperforms others across all metrics and datasets. All of the results presented are based on zero-shot evaluations, where models are tested without being specifically trained for deepfake detection. Despite this, the models are able to leverage a semantic mapping between language and visual input from their very vast pre-training, giving them an inherent concept of \\\"real\\\" versus \\\"fake.\\\" This capability suggests that these models possess some degree of understanding when it comes to identifying deepfakes. However, this general understanding falls far behind that of task-specific models. When we fine-tune the vision encoder, there is a notable improvement in performance. The vision-language models can...\"}"}
{"id": "cR3T1ZYN8I", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Open-ended qualitative evaluation with human annotators in Tab.\\n\\n| Model            | Human Eval. Score | Model | Precision | Recall | F1   |\\n|------------------|-------------------|-------|-----------|--------|------|\\n| BLIP-2           | 0.35              | InstructBLIP      | 0.36      | 0.33   | 0.35 |\\n| InstructBLIP-xxl |                   | LlaVa-1.5         | 0.38      |        | 0.40 |\\n\\n(a) Average score of Human Evaluation (R-splicer)\\n\\n| Model            | Precision | Recall | F1   |\\n|------------------|-----------|--------|------|\\n| BLIP-2           | 0.79      | 0.77   | 0.78 |\\n| InstructBLIP     | 0.86      | 0.53   | 0.83 |\\n| InstructBLIP-xxl | 0.80      | 0.73   | 0.81 |\\n| LlaVa-1.5        | 0.84      | 0.86   | 0.85 |\\n\\n(b) SeqDeepFake attributes\\n\\n| Model            | Precision | Recall | F1   |\\n|------------------|-----------|--------|------|\\n| BLIP-2           | 0.79      | 0.87   | 0.79 |\\n| InstructBLIP     | 0.81      | 0.89   | 0.86 |\\n| InstructBLIP-xxl | 0.82      |        | 0.81 |\\n| LlaVa-1.5        | 0.87      | 0.40   | 0.86 |\\n\\n(c) SeqDeepFake components\\n\\n| Model            | Precision | Recall | F1   |\\n|------------------|-----------|--------|------|\\n| BLIP-2           | 0.79      | 0.55   | 0.76 |\\n| InstructBLIP     | 0.83      | 0.47   | 0.85 |\\n| InstructBLIP-xxl | 0.82      | 0.53   | 0.81 |\\n| LlaVa-1.5        | 0.85      | 0.94   | 0.86 |\\n\\n(d) R-splicer better capture details and nuances in the input data, which enhances their deepfake detection capabilities. Nevertheless, due to the scarcity of high-quality captions and large-scale vision-language datasets tailored to deepfake detection, the improvements remain limited and only in the binary task. Overall, addressing these limitations by creating specialised datasets and foundation models could lead to substantial advancements in this area.\\n\\nLimitations and Future Work:\\n\\nAs the models in this work are all evaluated under zero-shot settings, their performance is below that of purpose-built networks seen in previous works, particularly for more challenging in-the-wild datasets. This further highlights the need for task-specific models and more fine-grained deepfake datasets, which is a key finding of the experiments conducted in this work. A significant limitation is the lack of detailed language descriptions in datasets, making qualitative evaluation harder. Additionally, current datasets lack fine-grained labels, restricting assessments of manipulations to pseudo-fakes and SeqDeepFake.\\n\\nFurthermore, as both the pertaining and evaluation datasets are not unbiased, the performance of all VLLMs is susceptible to the bias of the datasets, which is not addressed in this or previous benchmarks. Identifying these shortcomings is important for future works on the task, particularly as VLLMs gain traction.\\n\\nConclusion\\n\\nIn conclusion, our proposed benchmark has several contributions; first and foremost, we propose a method to transform deepfake detection into a VQA problem beyond binary classification to leverage common sense reasoning as an inherent explainability mechanism. We show how this can be achieved in both a multiple-choice and open-ended VQA \u2013 with the latter being the most important use-case for new and unknown face forgery methods. This approach is used to evaluate a multi-label problem that is not typical of classic VQA. By doing so, we can systematically and consistently evaluate the common sense reasoning capabilities of current and future VLLMs in fine-grained deepfake detection.\\n\\nOur selection of metrics and matching strategies allows for a fair evaluation of the proposed task. In particular, we include metrics that are robust to imbalance in both the binary and multi-label fine-grained tasks. Even though VLLMs in a zero-shot evaluation do not outperform purpose-built methods, the generated responses include reasoning, therefore holding promise for significant contributions in explainable deepfake detection, confirming the initial motivation behind examining the use of such models for the task and understanding the current capabilities. Moreover, as this benchmark can be extended in terms of models and datasets, it allows for a systematic and fair comparison of new language generation methods for explainable deepfake detection.\\n\\nEthics statement:\\n\\nThe authors of this paper acknowledge the crucial role of ethical considerations in AI research and development. Our dedication lies in upholding principles of fairness and impartiality. Recognising the societal implications of generative technology (including VLLMs), we commit to transparency by openly communicating our findings and advancements with the research community.\"}"}
{"id": "cR3T1ZYN8I", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgments and Disclosure of Funding\\n\\nThis work is supported by the Luxembourg National Research Fund, under the BRIDGES2021/IS/16353350/FaKeDeTeR, and by POST Luxembourg. Experiments were performed on the Luxembourg national supercomputer MeluXina. The authors gratefully acknowledge the LuxProvide teams for their expert support.\\n\\nReferences\\n\\n[1] Madhav Agarwal, Rudrabha Mukhopadhyay, Vinay P Namboodiri, and CV Jawahar. Audio-visual face reenactment. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2023.\\n\\n[2] Arjun Akula, Soravit Changpinyo, Boqing Gong, Piyush Sharma, Song-Chun Zhu, and Radu Soricut. Crossvqa: Scalably generating benchmarks for systematically testing vqa generalization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021.\\n\\n[3] Omar Mustafa Al-Janabi, Osamah Mohammed Alyasiri, and Elaf Ayyed Jebur. GPT-4 versus Bard and Bing: LLMs for Fake Image Detection. In 2023 3rd International Conference on Intelligent Cybernetics Technology & Applications (ICICyTA), December 2023.\\n\\n[4] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 2022.\\n\\n[5] Lisa Alazraki, Lluis Castrejon, Mostafa Dehghani, Fantine Huot, Jasper Uijlings, and Thomas Mensink. How (not) to ensemble lvlms for vqa. arXiv preprint arXiv:2310.06641, 2023.\\n\\n[6] Stella Bounareli, Christos Tzelepis, Vasileios Argyriou, Ioannis Patras, and Georgios Tzimiropoulos. Hyperreenact: one-shot reenactment via jointly learning to refine and retarget faces. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023.\\n\\n[7] Junyi Cao, Chao Ma, Taiping Yao, Shen Chen, Shouhong Ding, and Xiaokang Yang. End-to-end reconstruction-classification learning for face forgery detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\\n\\n[8] You-Ming Chang, Chen Yeh, Wei-Chen Chiu, and Ning Yu. AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors. arXiv preprint arXiv:2310.17419, November 2023.\\n\\n[9] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021.\\n\\n[10] Anthony Chen, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Mocha: A dataset for training and evaluating generative reading comprehension metrics. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.\\n\\n[11] Liang Chen, Yong Zhang, Yibing Song, Lingqiao Liu, and Jue Wang. Self-Supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\\n\\n[12] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.\\n\\n[13] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.\\n\\n[14] Alessandro Conti, Enrico Fini, Massimiliano Mancini, Paolo Rota, Yiming Wang, and Elisa Ricci. Vocabulary-free image classification. Advances in Neural Information Processing Systems, 2023.\\n\\n[15] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 2023.\"}"}
{"id": "cR3T1ZYN8I", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Audrey de Rancourt-Raymond and Nadia Smaili. The unethical use of deepfakes. *Journal of Financial Crime*, 2023.\\n\\nBrian Dolhansky, Russ Howes, Ben Pflaum, Nicole Baram, and Cristian Canton Ferrer. The deepfake detection challenge (dfdc) preview dataset, 2019.\\n\\nNiki Maria Foteinopoulou and Ioannis Patras. Emoclip: A vision-language method for zero-shot video facial expression recognition. In *2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG)*, pages 1\u201310. IEEE, 2024.\\n\\nSimon Ging, Maria Alejandra Bravo, and Thomas Brox. Open-ended VQA benchmarking of vision-language models by exploiting classification datasets and their semantic hierarchy. In *The Twelfth International Conference on Learning Representations (ICLR)*, 2024.\\n\\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, 2017.\\n\\nIjaz Ul Haq, Khalid Mahmood Malik, and Khan Muhammad. Multimodal neurosymbolic approach for explainable deepfake detection. *ACM Transactions on Multimedia Computing, Communications and Applications*, 2023.\\n\\nFeijuan He, Yaxian Wang, Xianglin Miao, and Xia Sun. Interpretable visual reasoning: A survey. *Image and Vision Computing*, 2021.\\n\\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In *International Conference on Learning Representations (ICLR)*, 2021.\\n\\nBaojin Huang, Zhongyuan Wang, Jifan Yang, Jiaxin Ai, Qin Zou, Qian Wang, and Dengpan Ye. Implicit identity driven deepfake face swapping detection. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023.\\n\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. *ACM Computing Surveys*, 2023.\\n\\nShan Jia, Reilin Lyu, Kangran Zhao, Yize Chen, Zhiyuan Yan, Yan Ju, Chuanbo Hu, Xin Li, Baoyuan Wu, and Siwei Lyu. Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics. *arXiv preprint arXiv:2403.14077*, March 2024. arXiv:2403.14077 [cs].\\n\\nYuming Jiang, Ziqi Huang, Xingang Pan, Chen Change Loy, and Ziwei Liu. Talk-to-edit: Fine-grained facial editing via dialog. In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, 2021.\\n\\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2020.\\n\\nTero Karras, Miika Aittala, Samuli Laine, Erik H\u00e4rk\u00f6nen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In *Advances in Neural Information Processing Systems*, 2021.\\n\\nMamadou Keita, Wassim Hamidouche, Hassen Bougueffa, Abdenour Hadid, and Abdelmalik Taleb-Ahmed. Harnessing the power of large vision language models for synthetic image detection. *arXiv preprint arXiv:2404.02726*, 2024.\\n\\nSohail Ahmed Khan and Duc-Tien Dang-Nguyen. CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection. *arXiv preprint arXiv:2402.12927*, February 2024.\\n\\nHyunsu Kim, Yunjey Choi, Junho Kim, Sungjoo Yoo, and Youngjung Uh. Exploiting spatial dimensions of latent in gan for real-time image editing. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, 2021.\\n\\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. *International journal of computer vision*, 123:32\u201373, 2017.\"}"}
{"id": "cR3T1ZYN8I", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chuqiao Li, Zhiwu Huang, Danda Pani Paudel, Yabin Wang, Mohamad Shahbazi, Xiaopeng Hong, and Luc Van Gool. A continual deepfake detection benchmark: Dataset, methods, and essentials. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), January 2023.\\n\\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning. PMLR, 2023.\\n\\nLingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong Chen, Fang Wen, and Baining Guo. Face X-Ray for More General Face Forgery Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.\\n\\nLinjie Li, Jie Lei, Zhe Gan, and Jingjing Liu. Adversarial vqa: A new benchmark for evaluating the robustness of vqa models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.\\n\\nRongjie Li, Yu Wu, and Xuming He. Learning by Correction: Efficient Tuning Task for Zero-Shot Generative Vision-Language Reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, April 2024.\\n\\nYuezun Li and Siwei Lyu. Exposing deepfake videos by detecting face warping artifacts. arXiv preprint arXiv:1811.00656, 2018.\\n\\nYuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu. Celeb-df: A large-scale challenging dataset for deepfake forensics. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, 2014.\\n\\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023.\\n\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems, 2023.\\n\\nZhian Liu, Maomao Li, Yong Zhang, Cairong Wang, Qi Zhang, Jue Wang, and Yongwei Nie. Fine-grained face swapping via regional gan inversion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\\n\\nYuhang Lu and Touradj Ebrahimi. Towards the Detection of AI-Synthesized Human Face Images. arXiv preprint arXiv:2402.08750, February 2024.\\n\\nJie Ma, Pinghui Wang, Dechen Kong, Zewei Wang, Jun Liu, Hongbin Pei, and Junzhou Zhao. Robust visual question answering: Datasets, methods, and future challenges. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.\\n\\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, 2019.\\n\\nNesryne Mejri, Enjie Ghorbel, and Djamila Aouada. UNTAG: Learning Generic Features for Unsupervised Type-Agnostic Deepfake Detection. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023.\\n\\nSachit Menon and Carl Von der Linden. Visual Classification via Description from Large Language Models, December 2022. arXiv:2210.07183 [cs].\\n\\nAnand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR). IEEE, 2019.\\n\\nDat Nguyen, Nesryne Mejri, Inder Pal Singh, Polina Kuleshova, Marcella Astrid, Anis Kacem, Enjie Ghorbel, and Djamila Aouada. Laa-net: Localized artifact attention network for high-quality deepfakes detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.\"}"}
{"id": "cR3T1ZYN8I", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yunsheng Ni, Depu Meng, Changqian Yu, Chengbin Quan, Dongchun Ren, and Youjian Zhao. Core: Consistent representation learning for face forgery detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12\u201321, 2022.\\n\\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning. PMLR, 2022.\\n\\nJames Oldfield, Christos Tzelepis, Yannis Panagakis, Mihalis Nicolaou, and Ioannis Patras. Parts of speech\u2013grounded subspaces in vision-language models. Advances in Neural Information Processing Systems, 2024.\\n\\nVicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems, 24, 2011.\\n\\nYassine Ouali, Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. Black box few-shot adaptation for vision-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023.\\n\\nOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation of stylegan imagery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning. PMLR, 2021.\\n\\nAndreas R\u00f6ssler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nie\u00dfner. FaceForensics++: Learning to detect manipulated facial images. In International Conference on Computer Vision (ICCV), 2019.\\n\\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\\n\\nDustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, pages 146\u2013162. Springer, 2022.\\n\\nRui Shao, Tianxing Wu, and Ziwei Liu. Detecting and recovering sequential deepfake manipulation. In European Conference on Computer Vision (ECCV), 2022.\\n\\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, 2018.\\n\\nYichen Shi, Yuhao Gao, Yingxin Lai, Hongyang Wang, Jun Feng, Lei He, Jun Wan, Changsheng Chen, Zitong Yu, and Xiaochun Cao. SHIELD: An Evaluation Benchmark for Face Spoofing and Forgery Detection with Multimodal Large Language Models. arXiv preprint arXiv:2402.04178, February 2024.\\n\\nKaede Shiohara and Toshihiko Yamasaki. Detecting deepfakes with self-blended images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\\n\\nOleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, 2020.\\n\\nInder Pal Singh, Nesryne Mejri, Van Dat Nguyen, Enjie Ghorbel, and Djamila Aouada. Multi-label deepfake classification. In 2023 IEEE 25th International Workshop on Multimedia Signal Processing (MMSP), pages 1\u20135. IEEE, 2023.\\n\\nMicha\u0142 Stypu\u0142kowski, Konstantinos Vougioukas, Sen He, Maciej Zi\u0229ba, Stavros Petridis, and Maja Pantic. Diffused heads: Diffusion models beat gans on talking-face generation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), January 2024.\\n\\nKe Sun, Shen Chen, Taiping Yao, Haozhe Yang, Xiaoshuai Sun, Shouhong Ding, and Rongrong Ji. Towards General Visual-Linguistic Face Forgery Detection. arXiv preprint arXiv:2307.16545, February 2024.\"}"}
{"id": "cR3T1ZYN8I", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D\u00eddac Sur\u00eds, Sachit Menon, and Carl von Ondruck. Vipergpt: Visual inference via Python execution for reasoning. Proceedings of IEEE International Conference on Computer Vision (ICCV), 2023.\\n\\nLoc Trinh, Michael Tsang, Sirisha Rambhatla, and Yan Liu. Interpretable and trustworthy deepfake detection via dynamic prototypes. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2021.\\n\\nYabin Wang, Zhiwu Huang, Zhiheng Ma, and Xiaopeng Hong. Linguistic profiling of deepfakes: An open database for next-generation deepfake detection. arXiv preprint arXiv:2401.02335, 2024.\\n\\nAlexandros Xenos, Niki Maria Foteinopoulou, Ioanna Ntinou, Ioannis Patras, and Georgios Tzimiropoulos. VLLMs Provide Better Context for Emotion Understanding Through Common Sense Reasoning. arXiv:2404.07078 [cs], April 2024.\\n\\nWeihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu. Tedigan: Text-guided diverse face image generation and manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\\n\\nPeng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. arXiv preprint arXiv:2306.09265, 2023.\\n\\nYuting Xu, Jian Liang, Gengyun Jia, Ziming Yang, Yanhao Zhang, and Ran He. TALL: Thumbnail Layout for Deepfake Video Detection. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), Paris, France, October 2023. IEEE.\\n\\nZipeng Xu, Enver Sangineto, and Nicu Sebe. Stylerdalle: Language-guided style transfer using a vector-quantized tokenizer of a large-scale generative model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023.\\n\\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is Inevitable: An Innate Limitation of Large Language Models. arXiv:2401.11817 [cs], January 2024.\\n\\nZhiyuan Yan, Yong Zhang, Yanbo Fan, and Baoyuan Wu. UCF: Uncovering common features for generalizable deepfake detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.\\n\\nZhiyuan Yan, Yong Zhang, Xinhang Yuan, Siwei Lyu, and Baoyuan Wu. DeepfakeBench: A Comprehensive Benchmark of Deepfake Detection. In Advances in Neural Information Processing Systems, 2023.\\n\\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11975\u201311986, 2023.\\n\\nKai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: A manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36, 2024.\\n\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\\n\\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020.\\n\\nYue Zhang, Ben Colman, Ali Shahriyari, and Gaurav Bharaj. Common sense reasoning for deep fake detection. arXiv preprint arXiv:2402.00126, 2024.\\n\\nCairong Zhao, Chutian Wang, Guosheng Hu, Haonan Chen, Chun Liu, and Jinhui Tang. ISTVT: Interpretable spatial-temporal video transformer for deepfake detection. IEEE Transactions on Information Forensics and Security, 2023.\\n\\nTianchen Zhao, Xiang Xu, Mingze Xu, Hui Ding, Yuanjun Xiong, and Wei Xia. Learning self-consistency for deepfake detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.\"}"}
{"id": "cR3T1ZYN8I", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bojia Zi, Minghao Chang, Jingjing Chen, Xingjun Ma, and Yu-Gang Jiang. Wilddeepfake: A challenging real-world dataset for deepfake detection. In Proceedings of the 28th ACM International Conference on Multimedia, 2020.\\n\\nDrago-Constantin \u00e2naru, Elisabeta Onea \u02d8a, and Dan Onea \u02d8a. Weakly-supervised deepfake localization in diffusion-generated images. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2024.\"}"}
{"id": "cR3T1ZYN8I", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The authors should cite the original paper that produced the code package or dataset.\\n\\nThe authors should state which version of the asset is used and, if possible, include a URL.\\n\\nThe name of the license (e.g., CC-BY 4.0) should be included for each asset.\\n\\nFor scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.\\n\\nIf assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.\\n\\nFor existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.\\n\\nIf this information is not available online, the authors are encouraged to reach out to the asset's creators.\\n\\n13. New Assets\\n\\nQuestion: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?\\n\\nAnswer: [N/A]\\n\\nJustification: Guidelines:\\n\\n- The answer NA means that the paper does not release new assets.\\n- Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.\\n- The paper should discuss whether and how consent was obtained from people whose asset is used.\\n- At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.\\n\\n14. Crowdsourcing and Research with Human Subjects\\n\\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?\\n\\nAnswer: [Yes]\\n\\nJustification: The details of the human evaluation are in the Appendix.\\n\\nGuidelines:\\n\\n- The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\\n- Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.\\n- According to the NeurIPS Code of Ethics, workers involved in data collection, cura-tion, or other labor should be paid at least the minimum wage in the country of the data collector.\\n\\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects\\n\\nQuestion: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?\\n\\nAnswer: [No]\\n\\nJustification: As the data is not sensitive or private, such approval is not required.\"}"}
{"id": "cR3T1ZYN8I", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Guidelines:\\n\\n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\\n\\n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.\\n\\n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.\\n\\n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.\"}"}
