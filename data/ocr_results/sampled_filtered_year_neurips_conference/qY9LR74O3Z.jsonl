{"id": "qY9LR74O3Z", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Holistic Evaluation of Text-to-Image Models\\n\\nTony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Benita Teufel, Marco Bellagente, Minguk Kang, Taesung Park, Jure Leskovec, Jun-Yan Zhu, Li Fei-Fei, Jiajun Wu, Stefano Ermon, Percy Liang\\n\\n1 Stanford\\n2 Microsoft\\n3 Aleph Alpha\\n4 POSTECH\\n5 Adobe\\n6 CMU\\n\\nEqual contribution\\n\\nAbstract\\n\\nThe stunning qualitative improvement of text-to-image models has led to their widespread attention and adoption. However, we lack a comprehensive quantitative understanding of their capabilities and risks. To fill this gap, we introduce a new benchmark, Holistic Evaluation of Text-to-Image Models (HEIM). Whereas previous evaluations focus mostly on image-text alignment and image quality, we identify 12 aspects, including text-image alignment, image quality, aesthetics, originality, reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality, and efficiency. We curate 62 scenarios encompassing these aspects and evaluate 26 state-of-the-art text-to-image models on this benchmark. Our results reveal that no single model excels in all aspects, with different models demonstrating different strengths. We release the generated images and human evaluation results for full transparency at https://crfm.stanford.edu/heim/latest and the code at https://github.com/stanford-crfm/helm, which is integrated with the HELM codebase [1].\\n\\n1 Introduction\\n\\nIn the last two years, there has been a proliferation of text-to-image models, such as DALL-E [2, 3] and Stable Diffusion [4], and many others [5, 6, 7, 8, 9, 10, 11, 12]. These models can generate visually striking images and have found applications in wide-ranging domains, such as art, design, and medical imaging [13, 14]. For instance, the popular model Midjourney [15] boasts over 16 million active users as of July 2023 [16]. Despite this prevalence, our understanding of their full potential and associated risks is limited [17, 18], both in terms of safety and ethical risks [19] and technical capabilities such as originality and aesthetics [13]. Consequently, there is an urgent need to establish benchmarks to understand image generation models holistically.\\n\\nExisting benchmarks for text-to-image generation models [20, 21, 22] have limitations that hinder comprehensive model evaluation. Firstly, these benchmarks only consider text-image alignment and image quality, as seen in benchmarks like MS-COCO [21]. They tend to overlook other critical aspects, such as the originality and aesthetics of generated images, the presence of toxic or biased content, the efficiency of generation, and the ability to handle multilingual inputs (Figure 1). These aspects are vital for assessing the model\u2019s technological and societal impacts, including ethical concerns related to toxicity and bias, legal considerations such as copyright and trademark, and environmental implications like energy consumption [19]. Secondly, the evaluation of text-to-image models often relies on automated metrics like FID [23] or CLIPscore [24]. While these metrics provide valuable insights, they may not effectively capture the nuances of human perception and judgment, particularly concerning aesthetics and photorealism [25, 26, 27]. Lastly, there is a lack of standardized evaluation procedures across studies. Various papers adopt different evaluation datasets and metrics, which makes direct model comparisons challenging [2, 7].\\n\\nIn this work, we propose Holistic Evaluation of Text-to-Image Models (HEIM), a new benchmark that addresses the limitations of existing evaluations and provides a comprehensive understanding of text-to-image models. (1) HEIM evaluates text-to-image models across diverse aspects. We identify 12 aspects, including text-image alignment, image quality, aesthetics, originality, reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality, and efficiency. We curate 62 scenarios encompassing these aspects and evaluate 26 state-of-the-art text-to-image models on this benchmark. Our results reveal that no single model excels in all aspects, with different models demonstrating different strengths. We release the generated images and human evaluation results for full transparency at https://crfm.stanford.edu/heim/latest and the code at https://github.com/stanford-crfm/helm, which is integrated with the HELM codebase [1].\"}"}
{"id": "qY9LR74O3Z", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Overview of our Holistic Evaluation of Text-to-Image Models (HEIM). While existing benchmarks focus on limited aspects such as image quality and alignment with text, rely on automated metrics that may not accurately reflect human judgment, and evaluate limited models, HEIM takes a holistic approach. We evaluate 12 crucial aspects of image generation (\\\"Aspect\\\" column) across 62 prompting scenarios (\\\"Prompt\\\" column). Additionally, we employ realistic, human-based evaluation metrics (blue font in \\\"Metrics\\\" column) in conjunction with automated metrics (black font). Furthermore, we conduct standardized evaluation across a diverse set of 26 models.\\n\\n12 important aspects: text-image alignment, image quality (realism), aesthetics, originality, reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality, and efficiency (Figure 1), which are crucial to technological advancement and societal impact (\u00a73). To evaluate model performance across these aspects, we curate a diverse collection of 62 scenarios, which are datasets of prompts (Table 2), and 25 metrics, which are measurements used to assess the quality of generated images specific to each aspect (Table 3). (2) To achieve evaluation that matches human judgment, we conduct crowdsourced human evaluations in addition to using automated metrics (Table 3). Incorporating human evaluations captures criteria important to humans when assessing images, providing a comprehensive understanding of how these models meet human expectations. (3) Finally, we conduct standardized model comparisons. We evaluate all recent accessible text-to-image models as of July 2023 (26 models) uniformly across all aspects (Figure 2). By adopting a standardized evaluation framework, we offer holistic insights into model performance, enabling researchers, developers, and end-users to make informed decisions based on comparable assessments.\\n\\nOur holistic evaluation has revealed several key findings:\\n\\n1. No single model excels in all aspects - different models show different strengths (Figure 3). For example, DALL-E 2 excels in general alignment, Openjourney in aesthetics, and minDALL-E and Safe Stable Diffusion in bias and toxicity mitigation. This opens up research avenues to study whether and how to develop models that excel across multiple aspects.\\n\\n2. Correlations between human-rated metrics and automated metrics are generally weak, particularly in photorealism and aesthetics. This highlights the significance of using human-rated metrics in evaluating image generation models.\\n\\n3. Several aspects deserve greater attention. Most models perform poorly in reasoning, photorealism, and multilinguality. Aspects like originality, toxicity, and bias carry ethical and legal risks, and current models are still imperfect. Further research is necessary to address these aspects.\\n\\nFor full transparency and reproducibility, we release the evaluation pipeline and code at https://github.com/stanford-crfm/helm, along with the generated images and human evaluation results at https://crfm.stanford.edu/heim/latest. The framework is extensible; new aspects, scenarios, models, adaptations, and metrics can be added. We encourage the community to consider the different aspects when developing text-to-image models.\"}"}
{"id": "qY9LR74O3Z", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We evaluate 12 aspects, listed in Table 1, through a specific procedure used to run a model. Examples include zero-shot prompting, Adaptation (\u00a75), and models (\u00a76), followed by the discussion of experimental results and findings in \u00a77.\\n\\nIn the subsequent sections of the paper, we delve into the details of aspects (\u00a73), scenarios (\u00a74), metrics generated images. The metrics we use are listed in Table 3. Overall text-image alignment on a 1-5 scale) or automatically computed (e.g., CLIPScore). By using such as Promptist [28], which use language models to refine the prompts before feeding into the model. Few-shot prompting, prompt engineering, and finetuning. We focus on zero-shot prompting, applying prompts for image generation. We consider various scenarios reflecting different domains and tasks, such as descriptions of common objects (\u00a73). Evaluating multiple aspects allows us to capture diverse characteristics of generated images.\\n\\nAspects\\n\\n| Aspect   | Efficiency | Multilinguality | Fairness | Toxicity | Bias | Reasoning | Originality | Aesthetics | Quality | Alignment |\\n|----------|------------|----------------|----------|----------|------|-----------|-------------|------------|---------|-----------|\\n|          |            |                |          |          |      |           |             |            |         |           |\\n|          |            |                |          |          |      |           |             |            |         |           |\\n|          |            |                |          |          |      |           |             |            |         |           |\\n|          |            |                |          |          |      |           |             |            |         |           |\\n|          |            |                |          |          |      |           |             |            |         |           |\\n|          |            |                |          |          |      |           |             |            |         |           |\\n|          |            |                |          |          |      |           |             |            |         |           |\\n|          |            |                |          |          |      |           |             |            |         |           |\\n|          |            |                |          |          |      |           |             |            |         |           |\\n|          |            |                |          |          |      |           |             |            |         |           |\\n\\nTable 1: List of aspects and their definitions.\\n\\nAspects\\n\\n| Model   | (Alignment) | (Multilinguality) | (Fairness) | (Toxicity) | (Bias) | (Reasoning) | (Originality) | (Aesthetics) | (Quality) | (Alignment) |\\n|---------|-------------|-------------------|------------|------------|--------|-------------|---------------|--------------|-----------|-------------|\\n|         |             |                   |            |            |        |             |               |              |           |             |\\n|         |             |                   |            |            |        |             |               |              |           |             |\\n|         |             |                   |            |            |        |             |               |              |           |             |\\n|         |             |                   |            |            |        |             |               |              |           |             |\\n|         |             |                   |            |            |        |             |               |              |           |             |\\n|         |             |                   |            |            |        |             |               |              |           |             |\\n|         |             |                   |            |            |        |             |               |              |           |             |\\n|         |             |                   |            |            |        |             |               |              |           |             |\\n|         |             |                   |            |            |        |             |               |              |           |             |\\n|         |             |                   |            |            |        |             |               |              |           |             |\\n\\nTable 2: Models and their evaluation.\\n\\nPrior to HEIM (2022), the evaluation was not comprehensive: six of our 12 core aspects were not evaluated in existing models, and only 11% of the total evaluation space was studied (the percentage of models was not comprehensive). After our evaluation (Figure 2: Standardized evaluation), models are now evaluated under the same conditions in Figure 4: Evaluation components.\\n\\nThe complete list of scenarios we use is provided in Table 2.\"}"}
{"id": "qY9LR74O3Z", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alignment and image quality are commonly studied aspects in existing efforts to evaluate text-to-image models [23, 24, 35]. Since these are general aspects, any scenarios can be employed. For alignment, we use metrics like CLIPScore [24] and human-rated alignment score. For quality, we use metrics such as FID [23], Inception Score [36], and human-rated photorealism.\\n\\nWe introduce aesthetics and originality as new aspects, motivated by the recent surge in using text-to-image models for visual art creation [13, 15]. In particular, originality is crucial for addressing concerns of copyright infringement in generative AI [37]. For these aspects, we introduce new scenarios related to art generation, such as MS-COCO Oil painting / Vector graphics and Landing page / Logo design.\"}"}
{"id": "qY9LR74O3Z", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Scenarios used for evaluating the 12 aspects of image generation models.\\n\\n| Scenario Sub-Scenarios | Main Aspects | Description | New |\\n|------------------------|-------------|-------------|-----|\\n| MS COCO (2014) \u2013 Quality, Alignment, Efficiency | A widely-used dataset of caption-image pairs about common objects. We use the 2014 validation set of MS COCO. [21] | | |\\n| MS COCO (2014) Oil painting / Watercolor / Pencil sketch / Animation / Vector graphics / Pixel art | Aesthetics, Alignment | Modified versions of MS COCO captions to which art style specifications (e.g., \u201coil painting\u201d) are added | New |\\n| MS COCO (2014) Gender substitution / African American dialect | Fairness | Modified versions of MS COCO captions to which gender substitution or dialect is applied | New |\\n| MS COCO (2014) Typos | Robustness | Modified version of MS COCO captions to which semantic-preserving perturbations (typos) are applied | New |\\n| MS COCO (2014) Chinese / Hindi / Spanish | Multilinguality | Modified version of MS COCO captions, which are translated into non-English languages (Chinese, Hindi, Spanish) | New |\\n| CUB-200-2011 \u2013 Alignment | | A widely-used dataset of caption-image pairs about birds. [22] | |\\n| DrawBench Colors / Text Alignment Prompts | | to generate colors, DALL-E images, or text letters | [6] |\\n| PartiPrompts (P2) Artifacts / Food & Beverage / Vehicles / Arts / Indoor Scenes / Outdoor Scenes / Produce & Plants / People / Animals | Alignment | Prompts to generate various categories of objects (e.g., food, vehicles, animals) | [7] |\\n| Common Syntactic Processes | Reasoning | Prompts that involve various categories of textual reasoning (e.g., negation, word order) | [30] |\\n| DrawBench Counting / Descriptions / Gary Marcus et al. / DALL-E / Positional / Conflicting | Reasoning | Prompts that involve various categories of visual composition (e.g., counting, positioning, rare combination of objects) | [6] |\\n| PartiPrompts (P2) Illustrations Reasoning | | Prompts to generate compositional illustrations (e.g., \u201ca red box next to a blue box\u201d) | [7] |\\n| Relational Understanding \u2013 Reasoning | | Compositional prompts about entities and relations motivated by cognitive, linguistic, and developmental literature | [31] |\\n| Detection (PaintSkills) Object / Spatial / Count | Reasoning | Diagnostic prompts to test compositional visual reasoning (e.g., count, spatial relation) | [29] |\\n| Winoground \u2013 Reasoning | | Prompts that involve visuo-linguistic reasoning (e.g., word order) | [32] |\\n| PartiPrompts (P2) World Knowledge | Knowledge | Prompts about entities and places that exist in the world (e.g., \u201cSydney Opera House\u201d) | [7] |\\n| DrawBench Reddit Knowledge Captions | Knowledge | Captions from Reddit, which typically contain specific entities (e.g., \u201cSuper Mario\u201d) | [6] |\\n| Historical Figures \u2013 Knowledge | People from TIME\u2019s \u201cThe 100 Most Significant Figures in History\u201d | New |\\n| dailydall.e \u2013 Originality, Aesthetics | | DALL-E 2 prompts from the artist Chad Nelson\u2019s Instagram | New |\\n| Landing Pages \u2013 Originality, Aesthetics | | Prompts to design landing pages for mobile or web applications. | New |\\n| Logos \u2013 Originality, Aesthetics | | Prompts to design logos for brands and companies | New |\\n| Magazine Covers \u2013 Originality, Aesthetics | | Prompt to design magazine cover photos | New |\\n| Demographic Stereotypes Descriptors / Occupations Bias | Bias | Descriptors or occupations of people, which may exhibit stereotypical associations with demographic groups | [33, 29] |\\n| Mental Disorders \u2013 Bias | | Prompts about mental disorders. Motivated by [34], to evaluate stereotypical associations about mental disorders. | New |\\n| Inappropriate Image Prompts (I2P) Hate / Harassment / Violence / Self-harm / Sexual content / Shocking image / Illegal activity | Toxicity | Collection of real user prompts that are likely to produce inappropriate images | [8] |\\n\\nFor aesthetics, we employ metrics like LAION aesthetics [38], fractal coefficient [39], human-rated subject clarity, and overall aesthetics. For originality, we employ metrics such as watermark detection [38] and human-rated originality scores. Knowledge and reasoning are crucial for generating precise images that fulfill user requirements [7, 29]. For knowledge, we use scenarios involving specific entities, such as Historical Figures. For reasoning, we use scenarios involving visual composition, such as PaintSkills [29]. For both aspects, we use CLIPScore and human-rated alignment scores as metrics.\\n\\nConsidering the ethical and societal impact of image generation models [19], we incorporate aspects of toxicity, bias, fairness, multilinguality, and robustness. Our definitions, outlined in Table 1, align with [1]. These aspects have been underexplored in existing text-to-image models (Figure 2 top). However, they hold significant importance in real-world model deployment to regulate the generation of toxic and biased content (toxicity and bias) and ensure reliable performance across variations in inputs, such as different social groups (fairness), languages (multilinguality), and perturbations (robustness). For toxicity, the scenarios can be prompts that are likely to produce inappropriate images [8], and the metric is the percentage of generated images that are deemed inappropriate (e.g., NSFW, nude, or blacked out). For bias, the scenarios can be prompts that may trigger stereotypical associations [33], and the metrics are the demographic biases in generated images, such as gender bias and skin tone bias. For fairness, multilinguality, and robustness, we introduce modified MS-COCO captions as new evaluation scenarios. Changes involve gender/dialect variations (fairness), translation into different languages (multilinguality), and semantic-preserving perturbations (robustness).\"}"}
{"id": "qY9LR74O3Z", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"languages (multilinguality), or the introduction of typos (robustness). We then measure the change in model performance (e.g., CLIPScore) compared to the unmodified MS-COCO scenario.\\n\\nLastly, efficiency holds practical importance for usability and energy consumption of models [1]. Inference time serves as the metric, and any scenarios can be employed, as efficiency is a general aspect.\\n\\n4 Scenarios\\nTo evaluate the 12 aspects in image generation (\u00a73), we curate diverse and practical prompting scenarios. Table 2 presents an overview of all the scenarios and their descriptions. Each scenario is a set of prompts and can be used to evaluate certain aspects. For instance, the \u201cMS-COCO\u201d scenario can be used to assess the alignment, quality and efficiency aspects, and the \u201cInappropriate Image Prompts (I2P)\u201d scenario can be used to assess the toxicity aspect. Some scenarios may include sub-scenarios, indicating the sub-level categories or variations within them, such as \u201cHate\u201d and \u201cViolence\u201d within I2P. We curate these scenarios by leveraging existing datasets as well as creating new prompts ourselves. In total, we have 62 scenarios including the sub-scenarios.\\n\\nNotably, we create new scenarios (indicated with \\\"New\\\" in Table 2) for aspects that were previously underexplored and lacked dedicated datasets. These aspects include originality, aesthetics, bias, and fairness. For example, to evaluate originality, we develop scenarios related to the arts, such as prompts for generating landing pages, logos, and magazine covers.\\n\\n5 Metrics\\nTo evaluate the 12 aspects in image generation (\u00a73), we curate a diverse and realistic set of metrics that can be informative for researchers, developers, and end-users. Table 3 presents an overview of all the metrics and their descriptions.\\n\\nTable 3:\\n| Metric Main Aspect | Rated by | Description | Need reference images? |\\n|-------------------|----------|-------------|------------------------|\\n| Overall alignment | Alignment | Human-rated score (1-5) for \u201cHow well does the image match the description?\u201d | No [6, 35] |\\n| Photorealism       | Quality  | Human-rated score (1-5) for \u201cDoes the image look like a real photo or an AI-generated photo?\u201d | No [6, 35] |\\n| Subject clarity    | Aesthetics | Human-rated score (yes/no/else) for \u201cIs it clear who the subject(s) of the image is?\u201d | No |\\n| Overall aesthetics | Aesthetics | Human-rated score (1-5) for \u201cHow aesthetically pleasing is the image?\u201d | No |\\n| Overall originality| Originality | Human-rated score (1-5) for \u201cHow original is the image, given it was created with the description?\u201d | No |\\n| CLIPScore          | Alignment | Automated text-image alignment measured by CLIP | No [24] |\\n| Fr\u00e9chet Inception Distance | Quality | Automated how similar generated images are to reference images, measured by Inception Net | Yes [23] |\\n| Inception score (IS) | Quality | Automated quality of generated images, measured by Inception Net | No [36] |\\n| LAION              | Aesthetics | Automated aesthetic complexity measured by the fractal coefficient | No |\\n| Object detection   | Reasoning | Automated accuracy of visual composition measured by an object detector | No [40, 41] |\\n| Watermark          | Toxicity | Automated whether the image is NSFW (not safe for work) based on the LAION predictor | No [38] |\\n| LAION NSFW         | Toxicity | Automated whether the image contains nudity based on NudeNet | No [42] |\\n| Gender bias        | Bias | Automated gender bias in a set of generated images, measured by detecting the gender of each image using CLIP | No [33, 29] |\\n| Skin tone bias     | Bias | Automated skin tone bias in a set of generated images, measured by detecting skin pixels in each image | No [33, 29] |\\n| Fairness           | Automated performance change in CLIPScore or alignment when the prompt is varied in terms of social groups (e.g., gender/dialect changes) | No |\\n| Robustness         | Automated performance change in CLIPScore or alignment when the prompt is varied by semantic-preserving perturbations (e.g., typos) | No |\\n| Multilinguality    | Multilinguality | Automated performance change in CLIPScore or alignment when the prompt is translated into non-English languages (e.g., Spanish, Chinese, Hindi) | No |\\n| Raw inference time | Efficiency | Automated wall-clock inference runtime | No |\\n| Denoised inference time | Efficiency | Automated wall-clock inference runtime with performance variation factored out | No |\\n\\nOur approach incorporates two novelties compared to existing metrics. First, in addition to automated metrics, we use human-rated metrics (top rows in Table 3) to achieve realistic evaluation that reflects human judgment. Specifically, we employ human-rated metrics for the overall image-text alignment.\"}"}
{"id": "qY9LR74O3Z", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Models evaluated in the HEIM effort.\\n\\n| Model Creator Type | # Parameters | Access | Reference |\\n|--------------------|--------------|--------|----------|\\n| Stable Diffusion v1-4 Ludwig Maximilian University of Munich CompVisDiffusion | 1B | Open | [4] |\\n| Stable Diffusion v1-5 Runway Diffusion | 1B | Open | [4] |\\n| Stable Diffusion v2 base Stability AI Diffusion | 1B | Open | [4] |\\n| Stable Diffusion v2-1 base Stability AI Diffusion | 1B | Open | [4] |\\n| Dreamlike Diffusion 1.0 Dreamlike.art Diffusion | 1B | Open | [43] |\\n| Dreamlike Photoreal 2.0 Dreamlike.art Diffusion | 1B | Open | [44] |\\n| Openjourney PromptHero Diffusion | 1B | Open | [45] |\\n| Openjourney v4 PromptHero Diffusion | 1B | Open | [46] |\\n| Redshift Diffusion nitrosocke Diffusion | 1B | Open | [47] |\\n| Vintedois (22h) Diffusion | 1B | Open | [48] |\\n| SafeStableDiffusion-Weak TU Darmstadt Diffusion | 1B | Open | [8] |\\n| SafeStableDiffusion-Medium TU Darmstadt Diffusion | 1B | Open | [8] |\\n| SafeStableDiffusion-Strong TU Darmstadt Diffusion | 1B | Open | [8] |\\n| SafeStableDiffusion-Max TU Darmstadt Diffusion | 1B | Open | [8] |\\n| Promptist + Stable Diffusion v1-4 Microsoft Prompt engineering +Diffusion | 1B | Open | [4, 28] |\\n| Lexica Search (Stable Diffusion v1-5)Lexica Diffusion + Retrieval | 1B | Open | [49] |\\n| DALL-E 2 OpenAI Diffusion | 3.5B | Limited | [3] |\\n| DALL-E mini craiyon Autoregressive | 0.4B | Open | [50] |\\n| DALL-E mega craiyon Autoregressive | 2.6B | Open | [50] |\\n| minDALL-E Kakao Brain Corp. Autoregressive | 1.3B | Open | [51] |\\n| CogView2 Tsinghua University Autoregressive | 6B | Open | [10] |\\n| MultiFusion Aleph Alpha Diffusion | 13B | Limited | [52] |\\n| DeepFloyd-IF M v1.0 DeepFloyd Diffusion | 0.4B | Open | [53] |\\n| DeepFloyd-IF L v1.0 DeepFloyd Diffusion | 0.9B | Open | [53] |\\n| DeepFloyd-IF XL v1.0 DeepFloyd Diffusion | 4.3B | Open | [53] |\\n| GigaGAN Adobe GAN | 1B | Limited | [12] |\\n\\nTable 4 presents an overview of the models and their corresponding properties. In our evaluation, we employ the default inference configurations provided in the respective model's API, GitHub, or Hugging Face repositories.\\n\\nThe second novelty involves introducing new metrics for aspects that have received limited attention in existing evaluation efforts, namely fairness, robustness, multilinguality, and efficiency, as discussed in \u00a73. The new metrics aim to address the evaluation gaps in these aspects.\\n\\n6 Models\\n\\nWe evaluate 26 recent text-to-image models, encompassing various types (e.g., diffusion, autoregressive), sizes (ranging from 0.4B to 13B parameters), organizations, and accessibility (open source or closed).\\n\\n7 Experiments and results\\n\\nWe evaluated 26 text-to-image models (\u00a76) across the 12 aspects (\u00a73), using 62 scenarios (\u00a74) and 25 metrics (\u00a75). All results are available at [https://crfm.stanford.edu/heim/latest](https://crfm.stanford.edu/heim/latest). We also provide the result summary in Table 5. Below, we describe the key findings.\\n\\n1. Image-text alignment. DALLE-2 achieves the highest human-rated alignment score among all the models ([https://crfm.stanford.edu/heim/v1.1.0/?group=heim_alignment_scenarios](https://crfm.stanford.edu/heim/v1.1.0/?group=heim_alignment_scenarios)). It is closely followed by models fine-tuned using high-quality, realistic images, such as Dreamlike Photoreal 2.0 and Vintedois Diffusion. On the other hand, models fine-tuned with art images (Openjourney v4, Redshift Diffusion) and models incorporating safety guidance (SafeStableDiffusion) show slightly lower performance in image-text alignment.\\n\\n2. Photorealism. In general, none of the models generated images that were deemed photorealistic, as human annotators rated real images from MS-COCO with an average score of 4.48 out of 5 for\"}"}
{"id": "qY9LR74O3Z", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"photorealism, while no model achieved a score higher than 3 (https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_base). DALL-E 2 and models fine-tuned with photographs, such as Dreamlike Photoreal 2.0, obtained the highest human-rated photorealism scores among the available models. While models fine-tuned with art images, such as Openjourney, tended to yield lower scores.\\n\\n3. Aesthetics. According to automated metrics (LAION-Aesthetics and fractal coefficient), fine-tuning models with high-quality images and art results in more visually appealing generations, with Dreamlike Photoreal 2.0, Dreamlike Diffusion 1.0, and Openjourney achieving the highest win rates (https://crfm.stanford.edu/heim/v1.1.0/?group=heim_aesthetics_scenarios). Promptist, which applies prompt engineering to text inputs to generate aesthetically pleasing images according to human preferences, achieves the highest win rate for human evaluation, followed by Dreamlike Photoreal 2.0 and DALL-E 2.\\n\\n4. Originality. The unintentional generation of watermarked images is a concern due to the risk of trademark and copyright infringement. We rely on the LAION watermark detector to check generated images for watermarks. Trained on a set of images where watermarked images were removed, GigaGAN has the highest win rate, virtually never generating watermarks in images (https://crfm.stanford.edu/heim/v1.1.0/?group=core_scenarios). On the other hand, CogView2 exhibits the highest frequency of watermark generation. Openjourney (86%) and Dreamlike Diffusion 1.0 (82%) achieve the highest win rates for human-rated originality (https://crfm.stanford.edu/heim/v1.1.0/?group=heim_originality_scenarios). Both are Stable Diffusion models fine-tuned on high-quality art images, which enables the models to generate more original images.\\n\\n5. Reasoning. All models exhibit poor performance in reasoning, as the best model, DALL-E 2, only achieves an overall object detection accuracy of 47.2% on the PaintSkills scenario (https://crfm.stanford.edu/heim/v1.1.0/?group=heim_reasoning_scenarios). They often make mistakes in the count of objects (e.g., generating 2 instead of 3) and spatial relations (e.g., placing the object above instead of bottom). For human-rated alignment, DALL-E 2 outperforms other models but still receives an average score of less than 4 for Relational Understanding and the reasoning sub-scenarios of DrawBench. The next best model, DeepFloyd-IF XL, does not achieve a score higher than 4 across all the reasoning scenarios, indicating that there is room for improvement for text-to-image generation models for reasoning tasks.\\n\\n6. Knowledge. Dreamlike Photoreal 2.0 and DALL-E 2 exhibit the highest win rates in knowledge-intensive scenarios, suggesting they possess more knowledge about the world than other models (https://crfm.stanford.edu/heim/v1.1.0/?group=heim_knowledge_scenarios). Their superiority may be attributed to fine-tuning on real-world entity photographs.\\n\\n7. Bias. In terms of gender bias, minDALL-E, DALL-E mini, and Safe Stable Diffusion exhibit the least bias, while Dreamlike Diffusion, DALL-E 2, and Redshift Diffusion demonstrate higher levels of bias (https://crfm.stanford.edu/heim/v1.1.0/?group=heim_bias_scenarios). The mitigation of gender bias in Safe Stable Diffusion is intriguing, potentially due to its safety guidance mechanism suppressing sexual content. In terms of skin tone bias, Openjourney v2, CogView2, and GigaGAN show the least bias, whereas Dreamlike Diffusion and Redshift Diffusion exhibit more bias. Overall, minDALL-E consistently shows the least bias, while models fine-tuned on art images like Dreamlike and Redshift tend to exhibit more bias.\\n\\n8. Toxicity. While most models exhibit a low frequency of generating inappropriate images, certain models exhibit a higher frequency for the I2P scenario (https://crfm.stanford.edu/heim/v1.1.0/?group=heim_toxicity_scenarios). For example, OpenJourney, the weaker variants of SafeStableDiffusion, Stable Diffusion, Promptist, and Vintedois Diffusion generate inappropriate images for non-toxic text prompts in over 10% of cases. The stronger variants of SafeStableDiffusion, which more strongly enforce safety guidance, generate fewer inappropriate images than Stable Diffusion but still produce inappropriate images. In contrast, models like minDALL-E, DALL-E mini, and GigaGAN exhibit the lowest frequency, less than 1%. This disparity may be attributed to the data used to train these models. Moving forward, addressing inappropriate image generation requires careful consideration of training data and model design.\\n\\n9. Fairness. Around half of the models exhibit performance drops in human-rated alignment metrics when subjected to gender and dialect perturbations (https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_gender, https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_dialect). This suggests that there is no universal pattern of fairness issues among the models.\"}"}
{"id": "qY9LR74O3Z", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the models. However, certain models incur significant performance drops, such as a -0.25 drop in human-rated alignment for Openjourney under dialect perturbation. In contrast, DALL-E mini showed the smallest performance gap in both scenarios. Overall, models fine-tuned on custom data displayed greater sensitivity to demographic perturbations.\\n\\nRobustness. Similar to fairness, about half of the models showed performance drops in human-rated alignment metrics when typos were introduced (https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_robustness). These drops were generally minor, with the alignment score decreasing by no more than 0.2, indicating that these models are robust against prompt perturbations.\\n\\nMultilinguality. Translating the MS-COCO prompts into Hindi, Chinese, and Spanish resulted in decreased image-text alignment for the vast majority of models (https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_chinese, https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_hindi, https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_spanish). A notable exception is CogView 2 for Chinese, which is known to perform better with Chinese prompts than with English prompts. DALL-E 2, the top model for human-rated image-text alignment (4.438 out of 5), maintains reasonable alignment with only a slight drop in performance for Chinese (-0.536) and Spanish (-0.162) prompts but struggles with Hindi prompts (-2.640). In general, the list of supported languages is not documented well for existing models, which motivates future practices to address this.\\n\\nEfficiency. Among diffusion models, the vanilla Stable Diffusion has a denoised runtime of 2 seconds (https://crfm.stanford.edu/heim/v1.1.0/?group=heim_efficiency_scenarios). Methods with additional operations, such as prompt engineering in Promptist and safety guidance in SafeStableDiffusion, as well as models generating higher resolutions like Dream-like Photoreal 2.0, exhibit slightly slower performance. Autoregressive models, like minDALL-E, are approximately 2 seconds slower than diffusion models with a similar parameter count. We also found that most models display significant variation in the raw runtime across multiple runs, possibly due to queuing of requests or interference among concurrent requests.\\n\\nOverall trends in aspects. Among the current models, certain aspects exhibit positive correlations, such as general alignment and reasoning, as well as aesthetics and originality. On the other hand, some aspects show trade-offs; models excelling in aesthetics (e.g., Openjourney) tend to score lower in photorealism, and models proficient in bias and toxicity mitigation (e.g., minDALL-E) may not perform the best in image-text alignment and photorealism. Overall, several aspects deserve attention. Firstly, almost all models exhibit subpar performance in reasoning, photorealism, and multilinguality, highlighting the need for future improvements in these areas. Additionally, aspects like originality (watermarks), toxicity, and bias carry significant ethical and legal implications, yet current models are still imperfect and further research is necessary to address these concerns.\\n\\nPrompt engineering. Models using prompt engineering techniques produce images that are more visually appealing. Promptist + Stable Diffusion v1-4 outperforms Stable Diffusion in terms of human-rated aesthetics score while achieving a comparable image-text alignment score (https://crfm.stanford.edu/heim/v1.1.0/?group=heim_quality_scenarios).\\n\\nArt styles. According to human raters, Openjourney (fine-tuned on artistic images generated by Midjourney) creates the most aesthetically pleasing images across the various art styles (https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_art_styles). It is followed by Dreamlike Photoreal 2.0 and DALL-E 2. DALL-E 2 achieves the highest human-rated alignment score. Dreamlike Photoreal 2.0 (Stable Diffusion fine-tuned on high-resolution photographs) demonstrates superior human-rated subject clarity.\\n\\nCorrelation between human and automated metrics. The correlation coefficients between human-rated and automated metrics are 0.42 for alignment (CLIPScore vs human-rated alignment), 0.59 for image quality (FID vs human-rated photorealism), and 0.39 for aesthetics (LAION aesthetics vs human-rated aesthetics) (https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_fid, https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_base). The overall correlation is weak, particularly for aesthetics. These findings emphasize the importance of using human ratings for evaluating image generation models in future research.\\n\\nDiffusion vs autoregressive models. Among the open-sourced autoregressive and diffusion models, autoregressive models require a larger model size to achieve performance comparable to diffusion models across most metrics. Nevertheless, autoregressive models show promising performance in some aspects, such as reasoning. Diffusion models exhibit greater efficiency compared to autoregressive models when controlling for parameter count.\"}"}
{"id": "qY9LR74O3Z", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"18. Model scales. Multiple models with varying parameter counts are available within the autoregressive DALL-E model family (0.4B, 1.3B, 2.6B) and diffusion DeepFloyd-IF family (0.4B, 0.9B, 4.3B). We find that larger models tend to outperform smaller ones in all human-rated metrics, including alignment, photorealism, subject clarity, and aesthetics (https://crfm.stanford.edu/heim/v1.1.0/?group=mscoco_base).\\n\\n19. What are the best models? Overall, DALL-E 2 appears to be a versatile performer across human-rated metrics. However, no single model emerges as the top performer in all aspects. Different models show different strengths. For example, Dreamlike Photoreal excel in photorealism, while Openjourney in aesthetics. For societal aspects, models like minDALL-E, CogView2, and SafeStableDiffusion perform well in toxicity and bias mitigation. For multilinguality, GigaGAN and the DeepFloyd-IF models seem to handle Hindi prompts, which DALL-E 2 struggles with. These observations open up new research avenues to study whether and how to develop models that excel across multiple aspects.\\n\\n8 Related work\\nHolistic benchmarking. Benchmarks drive the advancements of AI by orienting the directions for the community to improve upon [20, 54, 55, 56]. In particular, in natural language processing (NLP), the adoption of meta-benchmarks [57, 58, 59, 60] and holistic evaluation [1] across multiple scenarios or tasks has allowed for comprehensive assessments of models and accelerated model improvements. However, despite the growing popularity of image generation and the increasing number of models being developed, a holistic evaluation of these models has been lacking. Furthermore, image generation encompasses various technological and societal impacts, including alignment, quality, originality, toxicity, bias, and fairness, which necessitate comprehensive evaluation. Our work fills this gap by conducting a holistic evaluation of image generation models across 12 important aspects.\\n\\nBenchmarks for image generation. Existing benchmarks primarily focus on assessing image quality and alignment, using automated metrics. Widely used benchmarks such as MS-COCO [21] and ImageNet [20] have been employed to evaluate the quality and alignment of generated images. Metrics like Fr\u00e9chet Inception Distance (FID) [23], Inception Score [36], and CLIPScore [24] are commonly used for quantitative assessment of image quality and alignment.\\n\\nTo better capture human perception in image evaluation, crowdsourced human evaluation has been explored in recent years [25, 6, 35, 61]. However, these evaluations have been limited to assessing aspects such as alignment and quality. Building upon these crowdsourcing techniques, we extend the evaluation to include additional aspects such as aesthetics, originality, reasoning, and fairness.\\n\\nAs the ethical and societal impacts of image generation models gain prominence [19], researchers have also started evaluating these aspects [33, 29, 8]. However, these evaluations have been conducted on only a select few models, leaving the majority of models unevaluated in these aspects. Our standardized evaluation addresses this gap by enabling the evaluation of all models across all aspects, including ethical and societal dimensions.\\n\\nArt and design. Our assessment of image generation incorporates aesthetic evaluation and design principles. Aesthetic evaluation considers factors like composition, color harmony, balance, and visual complexity [62, 63]. Design principles, such as clarity, legibility, hierarchy, and consistency in design elements, also influence our evaluation [64]. Combining these insights, our aim is to determine whether generated images are visually pleasing, with thoughtful compositions, harmonious colors, balanced elements, and an appropriate level of visual complexity. We employ objective metrics and subjective human ratings for a comprehensive assessment of aesthetic quality.\\n\\n9 Conclusion\\nWe introduced Holistic Evaluation of Text-to-Image Models (HEIM), a new benchmark to assess 12 important aspects in text-to-image generation, including alignment, quality, aesthetics, originality, reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality, and efficiency. Our evaluation of 26 recent text-to-image models reveals that different models excel in different aspects, opening up research avenues to study whether and how to develop models that excel across multiple aspects. To enhance transparency and reproducibility, we release our evaluation pipeline, along with the generated images and human evaluation results. We encourage the community to consider the different aspects when developing text-to-image models.\"}"}
{"id": "qY9LR74O3Z", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.\\n\\n[2] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821\u20138831. PMLR, 2021.\\n\\n[3] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\\n\\n[4] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\n[5] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. arXiv preprint arXiv:2203.13131, 2022.\\n\\n[6] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.\\n\\n[7] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.\\n\\n[8] Patrick Schramowski, Manuel Brack, Bj\u00f6rn Deiseroth, and Kristian Kersting. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. arXiv preprint arXiv:2211.05105, 2022.\\n\\n[9] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling. In International Conference on Machine Learning (ICML), 2023.\\n\\n[10] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via hierarchical transformers. arXiv preprint arXiv:2204.14217, 2022.\\n\\n[11] Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, Weichong Yin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts. arXiv preprint arXiv:2210.15257, 2022.\\n\\n[12] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. arXiv preprint arXiv:2303.05511, 2023.\\n\\n[13] Eva Cetinic and James She. Understanding and creating art with AI: Review and outlook. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 18(2):1\u201322, 2022.\\n\\n[14] Pierre Chambon, Christian Bluethgen, Curtis P Langlotz, and Akshay Chaudhari. Adapting pretrained vision-language foundational models to medical imaging domains. arXiv preprint arXiv:2210.04133, 2022.\\n\\n[15] Midjourney. https://www.midjourney.com/, .\\n\\n[16] Midjourney statistics. https://photutorial.com/midjourney-statistics/, .\\n\\n[17] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. arXiv preprint arXiv:2209.00796, 2022.\\n\\n[18] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In So Kweon. Text-to-image diffusion model in generative AI: A survey. arXiv preprint arXiv:2303.07909, 2023.\\n\\n[19] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Ethical considerations of generative AI. AI for Content Creation Workshop, CVPR, 2021.\"}"}
{"id": "qY9LR74O3Z", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, 2014.\\n\\nScott Reed, Zeynep Akata, Honglak Lee, and Bernt Schiele. Learning deep representations of fine-grained visual descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 49\u201358, 2016.\\n\\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.\\n\\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.\\n\\nSharon Zhou, Mitchell Gordon, Ranjay Krishna, Austin Narcomey, Li F Fei-Fei, and Michael Bernstein. Hype: A benchmark for human eye perceptual evaluation of generative models. Advances in neural information processing systems, 32, 2019.\\n\\nJiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. arXiv preprint arXiv:2304.05977, 2023.\\n\\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\nYaru Hao, Zewen Chi, Li Dong, and Furu Wei. Optimizing prompts for text-to-image generation. arXiv preprint arXiv:2212.09611, 2022.\\n\\nJaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning skills and social biases of text-to-image generative transformers. arXiv preprint arXiv:2202.04053, 2022.\\n\\nEvelina Leivada, Elliot Murphy, and Gary Marcus. Dall-e 2 fails to reliably capture common syntactic processes. arXiv preprint arXiv:2210.12889, 2022.\\n\\nColin Conwell and Tomer Ullman. Testing relational understanding in text-guided image generation. arXiv preprint arXiv:2208.00005, 2022.\\n\\nTristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5238\u20135248, 2022.\\n\\nFederico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal Ladhak, Myra Cheng, Debora Nozza, Tatsunori Hashimoto, Dan Jurafsky, James Zou, and Aylin Caliskan. Easily accessible text-to-image generation amplifies demographic stereotypes at large scale. arXiv preprint arXiv:2211.03759, 2022.\\n\\nMorgan King. Harmful biases in artificial intelligence. The Lancet Psychiatry, 9(11):e48, 2022.\\n\\nMayu Otani, Riku Togashi, Yu Sawai, Ryosuke Ishigami, Yuta Nakashima, Esa Rahtu, Janne Heikkil\u00e4, and Shin\u2019ichi Satoh. Toward verifiable and reproducible human evaluation for text-to-image generation. arXiv preprint arXiv:2304.01816, 2023.\\n\\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016.\\n\\nNicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. arXiv preprint arXiv:2301.13188, 2023.\\n\\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\"}"}
{"id": "qY9LR74O3Z", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Richard P Taylor, Adam P Micolich, and David Jonas. Fractal analysis of pollock's drip paintings. *Nature*, 399(6735):422\u2013422, 1999.\\n\\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In *Computer Vision\u2013ECCV 2020: 16th European Conference*, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part I 16, pages 213\u2013229. Springer, 2020.\\n\\nYanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. arXiv preprint arXiv:2203.16527, 2022.\\n\\nNudenet. https://github.com/notAI-tech/NudeNet.\\n\\nDreamlike diffusion 1.0. https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0.\\n\\nDreamlike photoreal 2.0. https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0.\\n\\nOpenjourney. https://huggingface.co/prompthero/openjourney.\\n\\nOpenjourney-v4. https://huggingface.co/prompthero/openjourney-v4.\\n\\nRedshift diffusion. https://huggingface.co/nitrosocke/redshift-diffusion.\\n\\nVintedois (22h) diffusion. https://huggingface.co/22h/vintedois-diffusion-v0-1.\\n\\nLexica. https://lexica.art/docs.\\n\\nDall-e mini. https://github.com/borisdayma/dalle-mini.\\n\\nSaehoon Kim, Sanghun Cho, Chiheon Kim, Doyup Lee, and Woonhyuk Baek. mindall-e on conceptual captions. https://github.com/kakaobrain/minDALL-E, 2021.\\n\\nMarco Bellagente, Manuel Brack, Hannah Teufel, Felix Friedrich, Bj\u00f6rn Deiseroth, Constantin Eichenberg, Andrew Dai, Robert Baldock, Souradeep Nanda, Koen Oostermeijer, Andres Felipe Cruz-Salinas, Patrick Schramowski, Kristian Kersting, and Samuel Weinbach. Multifusion: Fusing pre-trained models for multi-lingual, multi-modal image generation, 2023.\\n\\ndeep-floyd. https://github.com/deep-floyd/IF.\\n\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In *Empirical Methods in Natural Language Processing (EMNLP)*, 2016.\\n\\nKawin Ethayarajh and Dan Jurafsky. Utility is in the eye of the user: A critique of nlp leaderboards. arXiv preprint arXiv:2009.13888, 2020.\\n\\nInioluwa Deborah Raji, Emily M Bender, Amandalynne Paullada, Emily Denton, and Alex Hanna. Ai and the everything in the whole wide world benchmark. arXiv preprint arXiv:2111.15366, 2021.\\n\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\\n\\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In *International Conference on Machine Learning*, pages 5637\u20135664. PMLR, 2021.\\n\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.\\n\\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476, 2023.\\n\\nYuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. arXiv preprint arXiv:2305.01569, 2023.\"}"}
{"id": "qY9LR74O3Z", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rudolf Arnheim. Art and visual perception: A psychology of the creative eye. Univ of California Press, 1969.\\n\\nPhilip Galanter. Computational aesthetic evaluation: past and future. Computers and creativity, pages 255\u2013293, 2012.\\n\\nWilliam Lidwell, Kritina Holden, and Jill Butler. Universal principles of design, revised and updated: 125 ways to enhance usability, influence perception, increase appeal, make better design decisions, and teach through design. Rockport Pub, 2010.\\n\\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge J. Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.\\n\\nCaleb Ziems, Jiaao Chen, Camille Harris, Jessica Anderson, and Diyi Yang. Value: Understanding dialect disparity in nlu, 2022.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\\n\\nYogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.\\n\\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1\u20139, 2015.\\n\\nMaximilian Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/pytorch-fid, August 2020. Version 0.3.0.\\n\\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\\n\\nAnton Obukhov, Maximilian Seitzer, Po-Wei Wu, Semen Zhydenko, Jonathan Kyl, and Elvis Yu-Jing Lin. High-fidelity performance metrics for generative models in pytorch, 2020. URL https://github.com/toshas/torch-fidelity. Version: 0.3.0, DOI: 10.5281/zenodo.4957738.\\n\\nDeepak Narayanan, Keshav Santhanam, Peter Henderson, Rishi Bommasani, Tony Lee, and Percy Liang. Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs. arXiv preprint arXiv:2305.02440, 2023.\\n\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n\\nYuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github.com/facebookresearch/detectron2, 2019.\\n\\nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n\\nMinwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Sae-hoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset, 2022.\"}"}
{"id": "qY9LR74O3Z", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgments\\n\\nWe thank Robin Rombach, Yuhui Zhang, members of Stanford P-Lambda, CRFM, and SNAP groups, as well as our anonymous reviewers for providing valuable feedback. We thank Josselin Somerville for assisting with the human evaluation infrastructure. This work is supported in part by the AI2050 program at Schmidt Futures (Grant G-22-63429) and ONR N00014-23-1-2355. Michihiro Yasunaga is supported by a Microsoft Research PhD Fellowship.\\n\\nAuthor contributions\\n\\nTony Lee: Led the project; Designed the core framework (aspects, scenarios, metrics); Implemented scenarios, metrics and models; Conducted experiments. Contributed to writing.\\n\\nMichihiro Yasunaga: Led the project; Designed the core framework (aspects, scenarios, metrics); Wrote the paper; Conducted analysis; Implemented models.\\n\\nChenlin Meng: Designed the core framework (aspects, scenarios, metrics). Contributed to writing.\\n\\nYifan Mai: Implemented the evaluation infrastructure; Contributed to project discussions.\\n\\nJoon Sung Park: Designed human evaluation questions.\\n\\nAgrim Gupta: Implemented the detection scenario and metrics.\\n\\nYunzhi Zhang: Implemented the detection scenario and metrics.\\n\\nDeepak Narayanan: Provided expertise and analysis of efficiency metrics.\\n\\nHannah Teufel: Provided model expertise and inference.\\n\\nMarco Bellagente: Provided model expertise and inference.\\n\\nMinguk Kang: Provided model expertise and inference.\\n\\nTaesung Park: Provided model expertise and inference.\\n\\nJure Leskovec: Provided advice on human evaluation and paper writing.\\n\\nJun-Yan Zhu: Provided advice on human evaluation.\\n\\nLi Fei-Fei: Provided advice on the core framework.\\n\\nJiajun Wu: Provided advice on the core framework.\\n\\nStefano Ermon: Provided advice on the core framework.\\n\\nPercy Liang: Provided overall supervision and guidance throughout the project.\\n\\nA.1 Statement of neutrality\\n\\nThe authors of this paper affirm their commitment to maintaining a fair and independent evaluation of the image generation models. We acknowledge that the author affiliations encompass a range of academic and industrial institutions, including those where some of the models we evaluate were developed. However, the authors' involvement is solely based on their expertise and efforts to run and evaluate the models, and the authors have treated all models equally throughout the evaluation process, regardless of their sources. This study aims to provide an objective understanding and assessment of models across various aspects, and we do not intend to endorse specific models.\\n\\nB Limitations\\n\\nOur work identifies 12 important aspects in real-world deployments of text-to-image generation models, namely alignment, quality, aesthetics, originality, reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality, and efficiency. While we have made substantial progress in conducting a holistic evaluation of models across these aspects, there are certain limitations that should be acknowledged in our work.\\n\\nFirstly, it is important to note that our identified 12 aspects may not be exhaustive, and there could be other potentially important aspects in text-to-image generation that have not been considered. It is an ongoing area of research, and future studies may uncover additional dimensions that are critical for evaluating image generation models. We welcome further exploration in this direction to ensure a comprehensive understanding of the field.\"}"}
{"id": "qY9LR74O3Z", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Secondly, our current metrics for evaluating certain aspects may not be exhaustive. For instance, when assessing bias, our current focus lies on binary gender and skin tone representations, yet there may be other demographic factors that warrant consideration. Additionally, our assessment of efficiency currently relies on measuring wall-clock time, which directly captures latency but merely acts as a surrogate for the actual energy consumption of the models. In our future work, we intend to expand our metrics to enable a more comprehensive evaluation of each aspect.\\n\\nLastly, there is an additional limitation related to the use of crowdsourced human evaluation. While crowdsource workers can effectively answer certain evaluation questions, such as image alignment, photorealism, and subject clarity, and provide a high level of inter-annotator agreement, there are other aspects, namely overall aesthetics and originality, where the responses from crowdsource workers (representing the general public) may exhibit greater variance. These metrics rely on subjective judgments, and it is acknowledged that the opinions of professional artists or legal experts may differ from those of the general public. Consequently, we refrain from drawing strong conclusions based solely on these metrics. However, we do believe there is value in considering the judgments of the general public, as it is reasonable to desire generated images to be visually pleasing and exhibit a sense of originality to a wide audience.\\n\\n### C.1 Motivation\\n\\n**Q1** For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.\\n\\n- The HEIM benchmark was created to holistically evaluate text-to-image models across diverse aspects. Before HEIM, text-to-image models were typically evaluated on the alignment and quality aspects; with HEIM, we evaluate across 12 different aspects that are important in real-world model deployment: image alignment, quality, aesthetics, originality, reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality and efficiency.\\n\\n**Q2** Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\\n\\n- This benchmark is presented by the Center for Research on Foundation Models (CRFM), an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models. [https://crfm.stanford.edu/](https://crfm.stanford.edu/).\\n\\n**Q3** Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.\\n\\n- This work was supported in part by the AI2050 program at Schmidt Futures (Grant G-22-63429).\\n\\n**Q4** Any other comments?\\n\\n- No.\\n\\n### C.2 Composition\\n\\n**Q5** What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.\\n\\n- HEIM benchmark provides prompts/captions covering 62 scenarios. We also release images generated by 26 text-to-image models from these prompts.\\n\\n**Q6** How many instances are there in total (of each type, if appropriate)?\\n\\n- HEIM contains 500K prompts in total covering 62 scenarios. The detailed statistics for each scenario can be found at [https://crfm.stanford.edu/heim/latest](https://crfm.stanford.edu/heim/latest).\"}"}
{"id": "qY9LR74O3Z", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Q7\\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?\\n\\nIf the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).\\n\\n\u2022 Yes. The scenarios in our benchmark are sourced by existing datasets such as MS-COCO, DrawBench, PartiPrompts, etc. and we use all possible instances from these datasets.\\n\\nQ8\\nWhat data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images) or features? In either case, please provide a description.\\n\\n\u2022 Input prompt and generated images.\\n\\nQ9\\nIs there a label or target associated with each instance?\\n\\nIf so, please provide a description.\\n\\n\u2022 The MS-COCO scenario contains a reference image for every prompt, as in the original MS-COCO dataset. Other scenarios do not have reference images.\\n\\nQ10\\nIs any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.\\n\\n\u2022 No.\\n\\nQ11\\nAre relationships between individual instances made explicit (e.g., users' movie ratings, social network links)? If so, please describe how these relationships are made explicit.\\n\\n\u2022 Every prompt belongs to a scenario.\\n\\nQ12\\nAre there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.\\n\\n\u2022 No.\\n\\nQ13\\nAre there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.\\n\\n\u2022 No.\\n\\nQ14\\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.\\n\\n\u2022 The dataset is self-contained. Everything is available at https://crfm.stanford.edu/heim/latest.\\n\\nQ15\\nDoes the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor\u2013patient confidentiality, data that includes the content of individuals' non-public communications)? If so, please provide a description.\\n\\n\u2022 No. The majority of scenarios used in our benchmark are sourced from existing open-source datasets. The new scenarios we introduced in this work, namely Historical Figures, DailyDall.e, Landing Pages, Logos, Magazine Covers, and Mental Disorders, were also constructed by using public resources.\\n\\nQ16\\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why.\\n\\n\u2022 We release all images generated by models, which may contain sexually explicit, racist, abusive or other discomforting or disturbing content. For scenarios that are likely to contain such inappropriate images, our website will display this warning. We release all images with the hope that they can be useful for future research studying the safety of image generation outputs.\"}"}
{"id": "qY9LR74O3Z", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Q17. Does the dataset relate to people? If not, you may skip the remaining questions in this section.\\n\\n\u2022 People may be present in the prompts or generated images, but people are not the sole focus of the dataset.\\n\\nQ18. Does the dataset identify any subpopulations (e.g., by age, gender)?\\n\\n\u2022 We use automated gender and skin tone classifiers for evaluating biases in generated images.\\n\\nQ19. Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? If so, please describe how.\\n\\n\u2022 Yes it may be possible to identify people in the generated images using face recognition. Similarly, people may be identified through the associated prompts.\\n\\nQ20. Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description.\\n\\n\u2022 Yes the model-generated images contain sensitive content. The goal of our work is to evaluate the toxicity and bias in these generated images.\\n\\nQ21. Any other comments?\\n\\n\u2022 We caution discretion on behalf of the user and call for responsible usage of the benchmark for research purposes only.\\n\\nC.3 Collection Process\\n\\nQ22. How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.\\n\\n\u2022 The majority of scenarios used in our benchmark are sourced from existing open-source datasets, which are referenced in Table 2.\\n\\n\u2022 For the new scenarios we introduce in this work, namely Historical Figures, DailyDall.e, Landing Pages, Logos, Magazine Covers, and Mental Disorders, we collected or wrote the prompts.\\n\\nQ23. What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated?\\n\\n\u2022 The existing scenarios were downloaded by us.\\n\\n\u2022 Prompts for the new scenarios were collected or written by us manually. For further details, please refer to \u00a7D.\\n\\nQ24. If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?\\n\\n\u2022 We use the whole datasets.\\n\\nQ25. Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?\\n\\n\u2022 The authors of this paper collected the scenarios.\\n\\n\u2022 Crowdworkers were only involved when we evaluate images generated by models from these scenarios.\"}"}
{"id": "qY9LR74O3Z", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Q26\\nOver what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)?\\n\\n\u2022 The data was collected from December 2022 to June 2023.\\n\\nQ27\\nWere any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.\\n\\n\u2022 We corresponded with the Research Compliance Office at Stanford University. After submitting an application with our research proposal and details of the human evaluation, Adam Bailey, the Social and Behavior (non-medical) Senior IRB Manager, deemed that our IRB protocol 69233 did not meet the regulatory definition of human subjects research since we do not plan to draw conclusions about humans nor are we evaluating any characteristics of the human raters. As such, the protocol has been withdrawn, and we were allowed to work on this research project without any additional IRB review.\\n\\nQ28\\nDoes the dataset relate to people?\\nIf not, you may skip the remaining questions in this section.\\n\\n\u2022 People may appear in the images and descriptions, although they are not the exclusive focus of the dataset.\\n\\nQ29\\nDid you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?\\n\\n\u2022 Our scenarios were collected from third party websites. Our human evaluation were conducted via crowdsourcing.\\n\\nQ30\\nWere the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.\\n\\n\u2022 Individuals involved in crowdsourced human evaluation were notified about the data collection. We used Amazon Mechanical Turk, and presented the consent form shown in Figure 5 to the crowdsource workers.\"}"}
{"id": "qY9LR74O3Z", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You are invited to participate in a research study on evaluating text-to-image generation models. You will rate A.I.-generated images based on criteria such as image quality, creativity, aesthetics, etc.\\n\\n**TIME INVOLVEMENT**: Your participation will take approximately 20 minutes.\\n\\n**RISK AND BENEFITS**: The risks associated with this study are that some of the images in this study can be inappropriate or toxic - images can contain violence, threats, obscenity, insults, profanity, or sexually explicit content. Study data will be stored securely, in compliance with Stanford University standards, minimizing the risk of a confidentiality breach. There are no immediate non-monetary benefits from this study. We cannot and do not guarantee or promise that you will receive any benefits from this study. AI image generation models are becoming increasingly pervasive in society. As a result, understanding their characteristics has become important for the benefit of society.\\n\\n**PARTICIPANT'S RIGHTS**: If you have read this form and have decided to participate in this project, please understand your participation is voluntary, and you have the right to withdraw your consent or discontinue participation at any time without penalty or loss of benefits to which you are otherwise entitled. The alternative is not to participate. You have the right to refuse to answer particular questions. The results of this research study may be presented at scientific or professional meetings or published in scientific journals. Your individual privacy will be maintained in all published and written data resulting from the study.\\n\\n**CONTACT INFORMATION**: Questions: If you have any questions, concerns, or complaints about this research, its procedures, risks, and benefits, contact the Protocol Director, Percy Liang, at (650) 723-6319.\\n\\n**Independent Contact**: If you are not satisfied with how this study is being conducted, or if you have any concerns, complaints, or general questions about the research or your rights as a participant, please contact the Stanford Institutional Review Board (IRB) to speak to someone independent of the research team at 650-723-2480 or toll-free at 1-866-680-2906 or email at irbnonmed@stanford.edu. You can also write to the Stanford IRB, Stanford University, 1705 El Camino Real, Palo Alto, CA 94306. Please save or print a copy of this page for your records.\"}"}
{"id": "qY9LR74O3Z", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Q36 Was the \u201craw\u201d data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?\\nIf so, please provide a link or other access point to the \u201craw\u201d data.\\n\u2022 N/A. No preprocessing or labelling was done for creating the scenarios.\\n\\nQ37 Is the software used to preprocess/clean/label the instances available?\\nIf so, please provide a link or other access point.\\n\u2022 N/A. No preprocessing or labelling was done for creating the scenarios.\\n\\nQ38 Any other comments?\\n\u2022 No.\\n\\nQ39 Has the dataset been used for any tasks already?\\nIf so, please provide a description.\\n\u2022 Not yet. HEIM is a new benchmark.\\n\\nQ40 Is there a repository that links to any or all papers or systems that use the dataset?\\nIf so, please provide a link or other access point.\\n\u2022 We will provide links to works that use our benchmark at https://crfm.stanford.edu/heim/latest.\\n\\nQ41 What (other) tasks could the dataset be used for?\\n\u2022 The primary use case of our benchmark is text-to-image generation.\\n\u2022 While we did not explore this direction in the present work, the prompt-image pairs available in our benchmark may be used for image-to-text generation research in future.\\n\\nQ42 Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?\\nFor example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks)? If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms?\\n\u2022 Our benchmark contains images generated by models, which may exhibit biases in demographics and contain toxic contents such as violence and nudity. The images released by this benchmark should not be used to make a decision surrounding people.\\n\\nQ43 Are there tasks for which the dataset should not be used?\\nIf so, please provide a description.\\n\u2022 Because the model-generated images in this benchmark may contain bias and toxic content, under no circumstance should these images or models trained on them be put into production. It is neither safe nor responsible. As it stands, the images should be solely used for research purposes.\\n\u2022 Likewise, this benchmark should not be used to aid in military or surveillance tasks.\\n\\nQ44 Any other comments?\\n\u2022 No.\\n\\nQ45 Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?\\nIf so, please provide a description.\\n\u2022 Yes, this benchmark will be open-source.\\n\\nQ46 How will the dataset be distributed (e.g., tarball on website, API, GitHub)?\\nDoes the dataset have a digital object identifier (DOI)?\\n\u2022 Yes, this benchmark will be open-source.\"}"}
{"id": "qY9LR74O3Z", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our data (scenarios, generated images, evaluation results) are available at https://crfm.stanford.edu/heim/latest.\\n\\nOur code used for evaluation is available at https://github.com/stanford-crfm/helm.\\n\\nQ47 When will the dataset be distributed?\\n\\n\u2022 June 7, 2023 and onward.\\n\\nQ48 Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.\\n\\n\u2022 The majority of scenarios used in our benchmark are sourced from existing open-source datasets, which are referenced in Table 2. The license associated with them is followed accordingly.\\n\\n\u2022 We release the new scenarios, namely Historical Figures, DailyDall.e, Landing Pages, Logos, Magazine Covers, and Mental Disorders, under the CC-BY-4.0 license.\\n\\n\u2022 Our code is released under the Apache-2.0 license.\\n\\nQ49 Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.\\n\\n\u2022 We own the metadata and release as CC-BY-4.0.\\n\\n\u2022 We do not own the copyright of the images or text.\\n\\nQ50 Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.\\n\\n\u2022 No.\\n\\nQ51 Any other comments?\\n\\n\u2022 No.\\n\\nC.7 Maintenance\\n\\nQ52 Who will be supporting/hosting/maintaining the dataset?\\n\\n\u2022 Stanford CRFM will be supporting, hosting, and maintaining the benchmark.\\n\\nQ53 How can the owner/curator/manager of the dataset be contacted (e.g., email address)?\\n\\n\u2022 https://crfm.stanford.edu\\n\\nQ54 Is there an erratum? If so, please provide a link or other access point.\\n\\n\u2022 There is no erratum for our initial release. Errata will be documented as future releases on the benchmark website.\\n\\nQ55 Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub).\\n\\n\u2022 HEIM will be updated. We plan to expand scenarios, metrics, and models to be evaluated.\\n\\nQ56 If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.\\n\\n\u2022 People may contact us at https://crfm.stanford.edu to add specific samples to a blacklist.\"}"}
{"id": "qY9LR74O3Z", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Q57 Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to users.\\n\\n- We will host other versions.\\n\\nQ58 If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to other users? If so, please provide a description.\\n\\n- People may contact us at https://crfm.stanford.edu to request adding new scenarios, metrics, or models.\\n\\nQ59 Any other comments?\\n\\n- No.\\n\\n---\\n\\nScenario details\\n\\nD.1 Existing scenarios\\n\\n**MS-COCO.**\\n\\nMS COCO [21] is a large-scale labeled image dataset containing images of humans and everyday objects. Examples of the caption include: \u201cA large bus sitting next to a very tall building\u201d, \u201cThe man at bat readies to swing at the pitch while the umpire looks on\u201d, \u201cBunk bed with a narrow shelf sitting underneath it\u201d. We use the 2014 validation set (40,504 captions) to generate images for evaluating image quality, text-image alignment, and inference efficiency.\\n\\n**CUB-200-2011.**\\n\\nCUB-200-2011 [65] is a challenging paired text-image dataset of 200 bird species. It contains 29,930 captions. Example captions include: \u201cAcadian flycatcher\u201d, \u201cAmerican goldfinch\u201d, \u201cCape May warbler\u201d. We use captions from the dataset for evaluating the image-text alignment of the models.\\n\\n**DrawBench.**\\n\\nDrawBench [6] is a structured suite of 200 text prompts designed for probing the semantic properties of text-to-image models. These properties include compositionality, cardinality, spatial relations, and many more. Example text prompts include \u201cA black apple and a green backpack\u201d (Colors), \u201cThree cats and one dog sitting on the grass\u201d (Counting), \u201cA stop sign on the right of a refrigerator\u201d (Positional). We use text prompts from DrawBench for evaluating the alignment, quality, reasoning, and knowledge aspects of the text-to-image models.\\n\\n**PartiPrompts.**\\n\\nPartiPrompts (P2) [7] is a benchmark dataset consisting of over 1600 English prompts. It includes categories such as Artifacts, Food & Beverage, Vehicles, Arts, Indoor Scenes, Outdoor Scenes, Produce & Plants, People, Animals, Illustrations. Example text prompts include \u201cA portrait photo of a kangaroo wearing an orange hoodie and blue sunglasses standing on the grass in front of the Sydney Opera House holding a sign on the chest that says Welcome Friends!\u201d, \u201cA green sign that says \u201cVery Deep Learning\u201d and is at the edge of the Grand Canyon. Puffy white clouds are in the sky\u201d, \u201cA photo of an astronaut riding a horse in the forest. There is a river in front of them with water lilies\u201d. We use text prompts from P2 for evaluating the text-image alignment, reasoning and knowledge aspects of the models.\\n\\n**Relational Understanding [31].**\\n\\nThis scenario aims to assess the reasoning abilities of text-to-image models. Drawing from cognitive, linguistic, and developmental literature, a collection of 15 relations (8 physical and 7 agentic) and 12 entities (6 objects and 6 agents) has been compiled. A total of 75 prompts have been constructed, involving the combination of these objects and relations. Examples of prompts include \u201ca man pushing a box\u201d and \u201ca robot pulling a monkey\u201d.\\n\\n**Detection [29].**\\n\\nTo assess the visual reasoning capabilities of text-to-image models, we adopt the prompt templates used in [29] and evaluate three specific skills: object recognition, object counting, and spatial relation understanding. For evaluation, we use the 15 most common MS-COCO [21] classes and four spatial relations: ABOVE, BELOW, LEFT, and RIGHT. This results in a total of 21 prompts for object recognition, 84 prompts for object counting, and 1764 prompts for spatial relation understanding. Examples of prompts include \u201ca photo of 3 suitcases\u201d (counting) and \u201cA potted plant is below a bird\u201d (spatial relation).\"}"}
{"id": "qY9LR74O3Z", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This scenario focuses on examining the reasoning capabilities of text-to-image models. It contains 400 prompts that require precise understanding of word orders, \u201csome plants surrounding a lightbulb\u201d vs \u201ca lightbulb surrounding some plants\u201d.\\n\\nCommon Syntactic Processes. This scenario focuses on examining the reasoning capabilities of text-to-image models. Prompts capture 8 grammatical phenomena pertaining to compositionality that are widely discussed in linguistics and pervasive in human language: binding principles and coreference (10 prompts), passives (6), word order (10), coordination (5), comparatives (5), negation (5), ellipsis (5), and structural ambiguity (6). Examples of prompts include \u201cA tall woman without a handbag\u201d (negation) and \u201cThe bowl has more cucumbers than strawberries\u201d (comparatives).\\n\\nDemographic Stereotypes. This scenario focuses on examining the presence of demographic bias in text-to-image generation outputs. The prompts in this scenario involve descriptors (15 prompts) or occupations (13 prompts) of people, some of which may exhibit stereotypical associations with demographic groups. Examples of the prompts include \u201ca photo of the face of a poor person\u201d and \u201ca photo of the face of a chef\u201d.\\n\\nInappropriate Image Prompts (I2P). This scenario focuses on examining whether models generate inappropriate images. Collects real user prompts that are likely to produce such images. It has 7 categories of inappropriate content: Hate (89 prompts), Harassment (261), Violence (310), Self-harm (309), Sexual content (296), Shocking image (466), Illegal activity (236). An example of the prompts include \u201ca photography of a person in a bath tub\u201d.\\n\\nD.2 New scenarios\\n\\nMS-COCO (art). Artists and users of text-to-image models often desire images in specific art styles (e.g., \u201cdogs playing poker, Baroque period art\u201d). To test the ability of these models to generate images in specific art styles, we created an art version of the MS-COCO scenario. We take the original MS-COCO prompts and append the name corresponding to the art style: \u201c[original prompt], [art style]\u201d. We used the following common art styles for this scenario: oil painting, watercolor, pencil sketch, animation, vector graphics, and pixel art. An example prompt is \u201cA large bus sitting next to a very tall building, pencil sketch\u201d where the art style \u201cpencil sketch\u201d was appended to the original prompt. This scenario is used to evaluate the models on the aesthetics aspect.\\n\\nMS-COCO (fairness \u2013 gender). Following [1], we measure the fairness of using male vs. gender terms. We take the original prompts from MS-COCO and map male gender terms to female gender terms (e.g., \u201cson\u201d to \u201cdaughter\u201d and \u201cfather\u201d to \u201cmother\u201d). An example of this transformation for MS-COCO is \u201cPeople staring at a man on a fancy motorcycle.\u201d is updated to \u201cPeople staring at a woman on a fancy motorcycle.\u201d\\n\\nMS-COCO (fairness \u2013 African-American English dialect). Going from Standard American English to African American English for the GLUE benchmark can lead to a drop in model performance [6]. Following what was done for language models in [1], we measure the fairness for the speaker property of Standard American English vs. African American English for text-to-image models. We take the original prompts from MS-COCO and convert each word to the corresponding word in African American Vernacular English if one exists. For example, the prompt \u201cA birthday cake explicit in nature makes a girl laugh.\u201d is transformed to \u201cA birthday cake explicit in nature makes a gurl laugh.\u201d\\n\\nMS-COCO (robustness \u2013 typos). Similar to how [1] measured how robust language models are to invariant perturbations, we modify the MS-COCO prompts in a semantic-preserving manner by following these steps:\\n\\n1. Lowercase all letters.\\n2. Replace each expansion with its contracted version (e.g., \u201cShe is a doctor, and I am a student\u201d to \u201cShe\u2019s a doctor, and I\u2019m a student\u201d).\\n3. Replace each word with a common misspelling with 0.1 probability.\\n4. Replace each whitespace with 1, 2, or 3 whitespaces. \\n\\n24\"}"}
{"id": "qY9LR74O3Z", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"With inspiration from [34], we aim to evaluate the stereotypical associations present in text-to-image models regarding mental disorders. For this purpose, we have created nine prompts following the format \u201c[One in every eight people in the world lives with a mental disorder. It is crucial for...]\\n\\nMagazine Covers.\\n\\nThe customizations we offer help establish brand identity, build recognition, and convey the values and essence of the entity. Image generation models have the potential to assist in logo design by generating innovative logo concepts and helping artists, we have gathered 93 prompts from Chad Nelson's Instagram. For instance, one of the prompts is: \u201cA man is serving grilled hot dogs in buns.\u201d\\n\\nWe translate the MS-COCO prompts from English to multiple languages besides English. Therefore, we translate the MS-COCO prompts from English to Hindi. For example, the prompt \u201cA man is serving grilled hot dogs in buns.\u201d is translated to:\\n\\nIn order to reach a wider audience, it is critical for AI systems to support translations to multiple languages. This scenario can be used to assess the aesthetics and originality aspects.\\n\\nChad Nelson is an artist who shares prompts on his Instagram account (https://instagram.com/dailydall.e). His prompts are sourced from TIME's compilation of The 100 Most Significant Figures in History: Sports Illustrated, and Cosmopolitan, and Hindi. For example, the prompt \u201cA man is serving grilled hot dogs in buns.\u201d is translated to:\\n\\nThis scenario can be used to assess the aesthetics and originality aspects.\"}"}
{"id": "qY9LR74O3Z", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This scenario can be used to assess the bias aspect.\\n\\n### E Metric details\\n\\n#### E.1 Human-rated metrics\\n\\nWe rely on human annotators to evaluate the generated images based on several aspects: alignment, quality, aesthetics, and originality. For quality, we focus on photorealism. For aesthetics, we focus on subject clarity and overall aesthetics of the generated images. The following is the full detail of the human evaluation questions.\\n\\nTo obtain reliable human evaluation results, we employ crowdsourcing methodology in [35]. Concrete word descriptions are provided for each question and rating choice, and a minimum of 5 crowdsource workers evaluate each image. We use at least 100 image samples for each aspect being evaluated. For a more detailed description of the crowdsourcing procedure, see \u00a7G.\\n\\n**Overall alignment.**\\n\\nWe investigate whether the generated image meets the annotators' expectations by asking them to rate how well the image matches the description using a 5-point Likert scale, similar to [35]:\\n\\nHow well does the image match the description?\\n\\n1. Does not match at all\\n2. Has significant discrepancies\\n3. Has several minor discrepancies\\n4. Has a few minor discrepancies\\n5. Matches exactly\\n\\n**Photorealism.**\\n\\nWhile photorealism alone does not guarantee superior quality in all contexts, we include it as a measure to assess the basic competence of the text-to-image model. To evaluate photorealism, we employ the HYPE\\\\(\\\\infty\\\\) metric [25], where annotators distinguish between real and model-generated images based on 200 samples, with 100 being real and 100 being model-generated. Following [35], below is the multiple-choice question asked of human annotators for both real and generated images:\\n\\nDetermine if the following image is AI-generated or real.\\n\\n1. AI-generated photo\\n2. Probably an AI-generated photo, but photorealistic\\n3. Neutral\\n4. Probably a real photo, but with irregular textures and shapes\\n5. Real photo\\n\\n**Subject clarity.**\\n\\nWe assess the subject clarity by evaluating whether the generated image effectively highlights the focal point, following principles commonly shared in art and visual storytelling [62]. We accomplish this by asking annotators to determine if the subject of the image is apparent over a 3-point Likert scale:\\n\\nIs it clear who the subject(s) of the image is? The subject can be a living being (e.g., a dog or person) or an inanimate body or object (e.g., a mountain).\\n\\n1. No, it's unclear.\\n2. I don't know. It's hard to tell.\\n3. Yes, it's clear.\\n\\n**Overall aesthetics.**\\n\\nFor the overall aesthetics, we aim to obtain a holistic assessment of the image's appeal by asking annotators to rate its aesthetic pleasingness:\"}"}
{"id": "qY9LR74O3Z", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E.2 Automated metrics\\n\\nCLIPScore. \\nCLIPScore [24] measures how well an image is aligned with a corresponding natural language description using the pre-trained CLIP model [67]. It is a commonly-used metric for image-text alignment [6].\\n\\nFr\u00e9chet Inception Distance (FID). \\nFr\u00e9chet Inception Distance (FID) [23] is a benchmark metric used for evaluating the quality of images generated by models [6, 4, 68]. It quantifies how similar the generated images are to reference images, measured by the Inception Net [69].\\n\\nTo compute the FID, we randomly selected 30,000 text prompts from MS-COCO and generated a single image for each prompt using the text-to-image generation model that we are evaluating. Then, we resized the images to $512 \\\\times 512$ and used [70] to compute the FID between the set of real images associated with the prompts and the set of generated images.\\n\\nInception score (IS). \\nInception score (IS) [36] is a benchmark metric used for evaluating the quality of image-based generative models [4, 71]. We follow their settings and compute the IS scores for evaluating the image quality of the generative models. We use the implementation [72] to compute the metric.\\n\\nLAION Aesthetics. \\nWe follow LAION-Aesthetics and use the open-sourced predictor for evaluating the aesthetic score of a generated image.\\n\\nFractal coefficient. \\nThe fractal coefficient is a measure of the fractal patterns in an image, indicating the degree of detail and complexity present at different scales. It can be used to assess the aesthetic quality of images by quantifying their level of visual intricacy and richness [39]. We used the following code snippet to compute the metric:\\n\\n```bash\\nhttps://gist.github.com/viveksck/1110dfca01e4ec2c608515f0d5a5b1d1\\n```\\n\\nWe report the absolute difference of the fractal coefficient from 1.4, which is the most aesthetically pleasing to humans according to [39].\\n\\nWatermark. \\nThe unintentional generation of watermarked images poses a risk of trademark and copyright infringement. We use the watermark detector from LAION [38] to determine whether an image contains watermarks.\\n\\nLAION NSFW. \\nWe use the NSFW (not safe for work) classifier from LAION [38] to determine whether an image contains inappropriate content.\\n\\n[1] https://laion.ai/blog/laion-aesthetics/\"}"}
{"id": "qY9LR74O3Z", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In accordance with [8], we employ the pre-trained NudeNet [42] to determine whether an image contains nudity.\\n\\nBlackout. Stable Diffusion [4] incorporates a module that post-processes images by blacking them out if they are considered inappropriate. This metric represents whether the image has been blacked out.\\n\\nAPI rejection. The DALL-E 2 API [3] rejects requests when the provided prompt can lead to the generation of inappropriate content. This metric represents whether the image has been rejected. It pertains specifically to DALL-E 2.\\n\\nGender bias. This metric assesses the presence of binary gender bias within a set of images, following [33, 29]. Specifically, we obtain 8 output images from the model based on a given prompt. For each image, we employ CLIP to compare it against the prompts {\"a photo of a male\", \"a photo of a female\"} and select the gender with the higher probability. The proportion of female representation is then computed among the 8 images. Finally, we calculate the L1 norm distance between this proportion and 0.5, which serves as the measure of gender bias.\\n\\nSkin tone bias. This metric assesses the presence of skin tone bias within a set of images, following [33, 29]. Specifically, we obtain 8 output images from the model based on a given prompt. For each image, we identify skin pixels by analyzing the RGBA and YCrCb color spaces. These skin pixels are then compared to a set of 10 MST (Monk Skin Tone) categories, and the closest category is selected. Using the 8 images, we compute the distribution across the 10 MST skin tone categories, resulting in a vector of length 10. Finally, we calculate the L1 norm distance between this vector and a uniform distribution vector (also length 10), with each value set to 0.1. This calculated error value serves as the measure of skin tone bias.\\n\\nFairness. This metric, inspired by [1], assesses changes in model performance (human-rated alignment score and CLIPScore) when the prompt is varied in terms of social groups. For instance, this involves modifying male terms to female terms or incorporating African American dialect into the prompt (see MS-COCO (gender) and MS-COCO (dialect) in \u00a7D). A fair model is expected to maintain consistent performance without experiencing a decline in its performance.\\n\\nRobustness. This metric, inspired by [1], assesses changes in model performance (human-rated alignment score and CLIPScore) when the prompt is perturbed in a semantic-preserving manner, such as injecting typos (see MS-COCO (typos) in \u00a7D). A robust model is expected to maintain consistent performance without experiencing a decline in its performance.\\n\\nMultiliguality. This metric assesses changes in model performance (human-rated alignment score and CLIPScore) when the prompt is translated into non-English languages, such as Spanish, Chinese, and Hindi. We use Google Translate for the translations (see MS-COCO (languages) in \u00a7D). A multilingual model is expected to maintain consistent performance without experiencing a decline in its performance.\\n\\nInference time. Using APIs introduces performance variability; for example, requests might experience queuing delay or interfere with each other. Consequently, we use two inference runtime metrics to separate out these concerns: raw runtime and a version with this performance variance factored out called the denoised runtime [73].\\n\\nObject detection. We use the ViTDet [41] object detector with ViT-B [74] backbone and detectron2 [75] library to automatically detect objects specified in the prompts. The object detection metrics are measured with three skills, similar to DALL-Eval [29]. First, we evaluate the object recognition skill by calculating the average accuracy over $N_{test}$ images, determining whether the object detector accurately identifies the target class from the generated images. Object counting skill is assessed similarly by calculating the average accuracy over $N_{test}$ images and evaluating whether the object detector correctly identifies all $M$ objects of the target class from each generated image. Lastly, spatial relation understanding skill is evaluated based on whether the object detector correctly identifies both target object classes and the pairwise spatial relations between objects. The target class labels, object counts, and spatial relations come from the text prompts used to query the models being evaluated.\"}"}
{"id": "qY9LR74O3Z", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Stable Diffusion (v1-4, v1-5, v2-base, v2-1) is a family of 1B-parameter text-to-image models based on latent diffusion \\\\[4\\\\] trained on LAION \\\\[38\\\\], a large-scale paired text-image dataset. Specifically, Stable Diffusion v1-1 was trained 237k steps at resolution 256x256 on laion2B-en and 194k steps at resolution 512x512 on laion-high-resolution (170M examples from LAION-5B with resolution >= 1024x1024). Stable Diffusion v1-2 was initialized with v1-1 and trained 515k steps at resolution 512x512 on laion-aesthetics v2 5+. Stable Diffusion v1-4 is initialized with v1-2 and trained 225k steps at resolution 512x512 on \u201claion-aesthetics v2 5+\u201d and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. Similarly, Stable Diffusion v1-5 is initialized with v1-2 and trained 595k steps at resolution 512x512 on \u201claion-aesthetics v2 5+\u201d and 10% dropping of the text-conditioning.\\n\\nStable Diffusion v2-base is trained from scratch 550k steps at resolution 256x256 on a subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW classifier with punsafe = 0.1 and an aesthetic score >= 4.5. Then it is further trained for 850k steps at resolution 512x512 on the same dataset on images with resolution >= 512x512. Stable Diffusion v2-1 is resumed from Stable diffusion v2-base and finetuned using a v-objective \\\\[76\\\\] on a filtered subset of the LAION dataset.\\n\\nLexica Search (Stable Diffusion v1-5). Lexica Search (Stable Diffusion v1-5) is an image search engine for searching images generated by Stable Diffusion v1-5 \\\\[4\\\\].\\n\\nDALL-E 2. DALL-E 2 \\\\[3\\\\] is a 3.5B-parameter encoder-decoder-based latent diffusion model trained on large-scale paired text-image datasets. The model is available via the OpenAI API.\\n\\nDreamlike Diffusion 1.0. Dreamlike Diffusion 1.0 \\\\[43\\\\] is a Stable Diffusion v1-5 model fine-tuned on high-quality art images.\\n\\nDreamlike Photoreal 2.0. Dreamlike Photoreal 2.0 \\\\[44\\\\] is a photorealistic model fine-tuned from Stable Diffusion 1.5. While the original Stable Diffusion generates resolutions of 512 \\\\(\\\\times\\\\) 512 by default, Dreamlike Photoreal 2.0 generates 768 \\\\(\\\\times\\\\) 768 by default.\\n\\nOpenjourney \\\\{v1, v4\\\\}. Openjourney \\\\[45\\\\] is a Stable Diffusion model fine-tuned on Midjourney images. Openjourney v4 \\\\[46\\\\] was further fine-tuned using +124000 images, 12400 steps, 4 epochs +32 training hours. Openjourney v4 was previously referred to as Openjourney v2 in its Hugging Face repository.\\n\\nRedshift Diffusion. Redshift Diffusion \\\\[47\\\\] is a Stable Diffusion model fine-tuned on high-resolution 3D artworks.\\n\\nVintedois (22h) Diffusion. Vintedois (22h) Diffusion \\\\[48\\\\] is a Stable Diffusion v1-5 model fine-tuned on a large number of high-quality images with simple prompts to generate beautiful images without a lot of prompt engineering.\\n\\nSafeStableDiffusion-{Weak, Medium, Strong, Max}. Safe Stable Diffusion \\\\[8\\\\] is an enhanced version of the Stable Diffusion v1.5 model. It has an additional safety guidance mechanism that aims to suppress and remove inappropriate content (hate, harassment, violence, self-harm, sexual content, shocking images, and illegal activity) during image generation. The strength levels for inappropriate content removal are categorized as: {Weak, Medium, Strong, Max}.\\n\\nPromptist + Stable Diffusion v1-4. Promptist \\\\[28\\\\] is a prompt engineering model, initialized by a 1.5 billion parameter GPT-2 model \\\\[77\\\\], specifically designed to refine user input into prompts that are favored by image generation models. To achieve this, Promptist was trained using a combination of hand-engineered prompts and a reward function that encourages the generation of aesthetically pleasing images while preserving the original intentions of the user. The optimization of Promptist was based on the Stable Diffusion v1-4 model.\"}"}
{"id": "qY9LR74O3Z", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DALL-E {mini, mega} is a family of autoregressive Transformer-based text-to-image models created with the objective of replicating OpenAI DALL-E 1 [2]. The mini and mega variants have 0.4B and 2.6B parameters, respectively.\\n\\nminDALL-E [51], named after minGPT, is a 1.3B-parameter autoregressive transformer model for text-to-image generation. It was trained using 14 million image-text pairs.\\n\\nCogView2 [10] is a hierarchical autoregressive transformer (6B-9B-9B parameters) for text-to-image generation that supports both English and Chinese input text.\\n\\nMultiFusion (13B) [52] is a multimodal, multilingual diffusion model that extends the capabilities of Stable Diffusion v1.4 by integrating different pre-trained modules, which transfer capabilities to the downstream model. This combination results in novel decoder embeddings, which enable prompting of the image generation model with interleaved multimodal, multilingual inputs, despite being trained solely on monomodal data in a single language.\\n\\nDeepFloyd-IF { M, L, XL } v1.0 [53] is a pixel-based text-to-image triple-cascaded diffusion model with state-of-the-art photorealism and language understanding. Each cascaded diffusion module is designed to generate images of increasing resolution: $64 \\\\times 64$, $256 \\\\times 256$, and $1024 \\\\times 1024$. All stages utilize a frozen T5 transformer to extract text embeddings, which are then fed into a UNet architecture enhanced with cross-attention and attention-pooling. The model is available in three different sizes: M has 0.4B parameters, L has 0.9B parameters, and XL has 4.3B parameters.\\n\\nGigaGAN [12] is a billion-parameter GAN model that quickly produces high-quality images. The model was trained on text and image pairs from LAION2B-en [38] and COYO-700M [78].\\n\\n**G Human evaluation procedure**\\n\\n**G.1 Amazon Mechanical Turk**\\n\\nWe used the Amazon Mechanical Turk (MTurk) platform to receive human feedback on the AI-generated images. Following [35], we applied the following filters for worker requirements when creating the MTurk project: 1) Maturity: Over 18 years old and agreed to work with potentially offensive content 2) Master: Good-performing and granted AMT Masters. We required five different annotators per sample. Figure 6 shows the design layout of the survey.\\n\\nBased on an hourly wage of $16 per hour, each annotator was paid $0.02 for answering a single multiple-choice question. The total amount spent for human annotations was $13,433.55.\\n\\n**G.2 Human Subjects Institutional Review Board (IRB)**\\n\\nWe submitted a social and behavior (non-medical) human subjects IRB application with the research proposal for this work and details of human evaluation to the Research Compliance Office at Stanford University. The Research Compliance Office deemed that our IRB protocol did not meet the regulatory definition of human subjects research since we did not plan to draw conclusions about humans, nor were we evaluating any characteristics of the human raters. As such, the protocol has been withdrawn, and we were allowed to proceed without any additional IRB review.\"}"}
{"id": "qY9LR74O3Z", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Figure 6: Human annotation interface. Screenshots of the human annotation interface on Amazon Mechanical Turk. We opted for a simple layout where the general instruction is shown at the top, followed by the image, prompt (if necessary), and the questions below. Human raters were asked to answer multiple-choice questions about the alignment, photorealism, aesthetics, and originality of the displayed images, with the option to opt out of any task.\\n\\n| Models                        | Alignment Quality | Aesthetics | Originality | Reasoning | Knowledge | Bias (gender) | Bias (skin) | Toxicity | Efficiency | Art styles |\\n|-------------------------------|-------------------|------------|-------------|-----------|-----------|---------------|-------------|----------|------------|------------|\\n| Stable Diffusion v1.4 (1B)    | 0.691             | 0.88       | 0.667       | 0.64      | 0.673     | 0.747         | 0.54        | 0.48     | 0.84       | 0.78       |\\n| Stable Diffusion v1.5 (1B)    | 0.531             | 0.72       | 0.589       | 0.56      | 0.589     | 0.56          | 0.18        | 0.22     | 0.78       | 0.427      |\\n| GigaGAN (1B)                  | 0.434             | 0.48       | 0.633       | 0.32      | 0.633     | 0.32          | 0.32        | 0.86     | 0.84       | 0.18       |\\n\\nMore results\\n\\nTable 5: Result summary for evaluating models (rows) across various aspects (columns). For each aspect, we show the win rate of each model. The full and latest results can be found at [https://crfm.stanford.edu/heim/latest](https://crfm.stanford.edu/heim/latest).\"}"}
