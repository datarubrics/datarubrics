{"id": "yZQDF9f6bR", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Hypothetic Explanation for Experimental Results of RoBERTa base outperforms RoBERTa large\\n\\nConventional wisdom suggests that larger models, with more parameters, tend to outperform smaller models. However, as indicated in Table 3, a notable phenomenon emerges in the context of the PIR experiment: RoBERTa base outperforms RoBERTa large. This unexpected observation leads us to formulate a two-fold hypothesis:\\n\\n1. Domain discrepancy between pragmatic data and LM\u2019s pretraining data: Larger models exhibit a consistent influence of their pretraining phase, whereas smaller models are more susceptible to adaptation during the fine-tuning process. Importantly, beyond the application of semantic knowledge, our pragmatic data introduces deeper challenges for language models, leading smaller models to potentially \u201csuperficially\u201d memorize patterns from the fine-tuning data. In contrast, larger LMs tend to adhere more closely to their original pretraining understanding. Similar trends have been observed in other domains, such as multi-step math reasoning [64] and few-shot learning [65].\\n\\n2. Low-resource and diversity of pragmatic data: Pragmatic data is both scarce and diverse. Effectively tackling this data needs language models to move beyond mere semantic pattern memorization and engage in more nuanced reasoning over contextual information, social common sense, and even theory-of-mind reasoning. Such low-resource conditions challenge the conventional fine-tuning requirements for large language models, which typically benefit from substantial data for achieving superior performance.\\n\\nTo empirically substantiate these hypotheses, we undertake a zero-shot version of Task 1 (PIR). The results of this zero-shot experiment are reported in Tab. 6, and when compared to the results presented in Table 3, it becomes evident that the improvement in performance of RoBERTa base following fine-tuning is considerably more pronounced than that of RoBERTa large.\\n\\nTable 6: Zero-Shot PIR\\n\\n|            | C\u2192P | CP\u2192R | C\u2192PR |\\n|------------|-----|------|------|\\n| RoBERTa large | 61.1| 21.8 | 1.7  |\\n| RoBERTa base | 57.2| 21.5 | 5.1  |\\n\\nB Annotation Details\\n\\nDifferent methodologies are employed to address various pragmatic phenomena. To leverage prior advancements in the field, we begin by segmenting each dialogue into individual utterances. Subsequently, we employ two distinct approaches, namely string matching and pretrained model classification, to identify these phenomena within our source data. In the case of scalar implicature, which exhibits a noticeable pattern characterized by word pairs such as (some, all) appearing in adjacent turns of dialogues, we employ string matching to annotate instances of scalar implicature in conversations. Similarly, for popeq implicature, which often features a continuous question mark, we utilize this characteristic as a means of detection. With regards to idioms, which exhibit more evident patterns, we employ the idiom set proposed by Saxena and Paul [18] to conduct searches. For other types of phenomena that lack obvious patterns, we leverage a pretrained RoBERTa base model [49], and fine-tune it for our specific task. The sarcasm dataset by Misra [66] is used for finetuning the sarcasm model, the MOVER dataset by Zhang and Wan [42] for hyperbole and the ColBERT dataset by Annamoradnejad and Zoghi [20] for paronomasia. Several models have been proposed for metaphor detection, thus we utilize an existing model [67] specifically designed for metaphor identification.\\n\\nB.2 Details For Fine-grained Annotation\\n\\nAMT is integral to our process. To ensure clarity and consistency, we provide explicit instructions to the workers. Additionally, to further elucidate the objectives of our study, we offer illustrative...\"}"}
{"id": "yZQDF9f6bR", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The task itself is presented below the instructions and examples, with the dialogue and corresponding turn numbers provided for workers to select. Furthermore, as workers check a checkbox, we prompt them to select a confidence score and provide a rationale. In order to strike a balance between our budget, the quality of annotations, and the speed of annotation, we have determined the compensation of $0.1 per completed task. After the annotation process, we collect responses that are assigned with a confidence score of 4 or higher.\\n\\nSpecifically, we surveyed 10 users to accomplish our task. All users can complete a single task within 45 seconds, leading to a wage pay of around 8 dollars per hour, which is about a dollar higher than the federal minimum hourly wage of the United States.\\n\\nB.3 Details on Human Refinements\\n\\nDisturbing choices are chosen based on the BERTScore metric [51]. The rationale with the highest similarity, as determined by other dialogues, is selected and included in the pool of candidate options. The instructions provided to the workers align with those used for Fine-grained Annotation, wherein they are also instructed to assign a confidence score to their responses. The remuneration for workers is set at $0.05 per task.\\n\\nAMT Workers Requirements\\n\\nIn order to guarantee the quality of annotated data, the qualification rules for workers are strict and can be found in Tab. 7.\\n\\n| Country          | # Tasks approved | Tasks approved Rate |\\n|------------------|------------------|---------------------|\\n| United States    | GreaterThanOrEqualTo 1300 | GreaterThanOrEqualTo 95% |\\n| Canada           |                  |                     |\\n| Great Britain    |                  |                     |\\n| Australia        |                  |                     |\\n| Singapore        |                  |                     |\\n| Ireland          |                  |                     |\\n| New Zealand      |                  |                     |\\n\\nC Experimental Detail\\n\\nC.1 Pragmatic Identification and Reasoning (PIR)\\n\\nBERT (Bidirectional Encoder Representations from Transformers) is a revolutionary language representation model that has had a significant impact on natural language processing (NLP) tasks. It has achieved remarkable performance across various NLP benchmarks, including question answering, sentiment analysis, named entity recognition, and many others. Its birth brings profound influence on pretrained language models.\\n\\nRoBERTa improves upon BERT by incorporating enhancements such as larger and more diverse training data, longer pretraining duration, dynamic masking, and advanced training strategies. These improvements enable RoBERTa to achieve even better performance on a wide range of NLP benchmarks. While BERT paved the way for contextualized representations in NLP, RoBERTa further refines and pushes the boundaries of language understanding, making it a powerful and preferred choice for many researchers and practitioners in the field.\\n\\nALBERT (A Lite BERT) is a highly efficient and compact variant of the BERT model that addresses the computational limitations of the original architecture. It incorporates parameter-reduction techniques to alleviate training time constraints and achieve improved performance compared to BERT.\\n\\nDeBERTa (Decoding-enhanced BERT with Disentangled Attention) is a state-of-the-art language representation model that builds upon the BERT architecture and introduces several key innovations, including disentangled attention mechanism. The performance of DeBERTa has been demonstrated to surpass that of BERT on a wide range of NLP tasks.\\n\\nGPT2 is a significant breakthrough in natural language processing and generation. One of the most notable features of GPT-2 is its ability to generate coherent and contextually relevant text. Through unsupervised pretraining on a large corpus of internet text, GPT-2 learns to predict the next word in a sequence of text, enabling it to generate human-like responses.\"}"}
{"id": "yZQDF9f6bR", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Hyperparameters for models on CP \u2192 R\\n\\n| Model         | Learning rate | Batch Size | Weight Decay | Epochs |\\n|---------------|---------------|------------|--------------|--------|\\n| BERT base     | 5e-5          | 12         | 0.001        | 50     |\\n| BERT large    | 5e-5          | 12         | 0.001        | 50     |\\n| ALBERT base   | 5e-5          | 12         | 0.001        | 50     |\\n| ALBERT large  | 5e-5          | 12         | 0.001        | 50     |\\n| DeBERTa base  | 5e-5          | 12         | 0.001        | 50     |\\n| RoBERTa base  | 5e-5          | 12         | 0.001        | 50     |\\n| RoBERTa large | 5e-5          | 12         | 0.001        | 50     |\\n| GPT2 base     | 0.001         | 8          | 0.01         | 50     |\\n| DialoGPT medium | 0.001     | 2          | 0.01         | 50     |\\n\\nTable 9: Batch size for models on C \u2192 P\\n\\n| Model         | Batch Size |\\n|---------------|------------|\\n| BERT base     | 80         |\\n| ALBERT base   | 24         |\\n| ALBERT large  | 24         |\\n| DeBERTa base  | 24         |\\n| RoBERTa base  | 80         |\\n| RoBERTa large | 24         |\\n| GPT2 base     | 24         |\\n| DialoGPT medium | 8          |\\n\\nDialogGPT is dialogue-oriented GPT. It builds upon the GPT architecture and extends it to support interactive conversations. DialogGPT is trained in a supervised manner using a dialogue dataset, which allows it to understand and generate responses in a conversational context.\\n\\nThe PIR task encompasses three distinct settings: C \u2192 P, CP \u2192 R, and C \u2192 PR. In the C \u2192 P setting, models are trained for 20 epochs, employing a batch size as indicated in Tab. 9, a learning rate of 2e\u22125, and weight decay of 0.01. As for CP \u2192 R, the hyperparameters adopted are listed in Tab. 8. For the C \u2192 PR setting, there is no training required; instead, we simply load the best checkpoint obtained from the previous training for this task. The concrete implementation is as follows: we initially flatten the test dataset of C \u2192 P, ensuring that each instance contains both a dialogue and a pragmatic turn extracted from the same dialogue. As for the test dataset of CP \u2192 R, no modifications are made. It should be noted that, following the processing steps, both datasets own the same dialogues and corresponding pragmatic turns, resulting in identical instance numbers. For an instance to be deemed correct, the models must successfully accomplish both component tasks, i.e., succeed in Identification and Reasoning.\\n\\nC.2 Conversational Question Answering (CQA)\\n\\nChatGPT was instructed to generate questions for our tasks. The prompt template that starts the questions with \u201cWhich\u201d is depicted in Tab. 10. Through this methodology, we collected a total of 19,482 questions. To ensure the reliability of the answers provided to these questions, AMT is utilized. In our experiment, the hyperparameters adopted are illustrated in Tab. 11. To assess the performance of ChatGPT, we conducted testing using the template outlined in Tab. 13.\\n\\nC.3 Zero-Shot Natural Language Inference\\n\\nDetails are provided as follows. T5-XXL, and DeBERTa-v3 are tested with the pragmatic turn as premise and implied meaning as a hypothesis. The context is out of reach for these models. In contrast, as shown in Tab. 10, ChatGPT is given the context, and the red line labeled \u201cThink step by step\u201d represents two distinct configurations: one with step-by-step and one without it.\"}"}
{"id": "yZQDF9f6bR", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: ChatGPT question generation template: using \u201cWhich\u201d to start the question.\\n\\nYou are sensitive and always view others\u2019 words as having some implied meanings.\\n\\nFor the dialogue between \u201cA\u201d and \u201cB\u201d in this task, we have offered a statement that is the implied meaning of a turn, please only offer one reading comprehension question that can be answered with only one word based on the dialogue and mostly focuses on the turn the statement mentions. The question will be tested by only by viewing the dialogue, so please make the question hard enough that it\u2019s impossible to answer without viewing.\\n\\nUse \u201cWhich\u201d to ask the question!\\n\\nFollowing is the dialogue: {dialogue}\\n\\nFollowing is the statement: {statement}\\n\\nUse \u201cWhich\u201d to ask the question! And please make the question hard enough that it's impossible to answer without viewing.\\n\\nTable 11: Hyperparameters for models on CQA.\\n\\n| Training Epoch | 50 |\\n|----------------|----|\\n| Learning Rate  | 5 \\\\times 10^{-5} |\\n| Batch Size     | 24 |\\n| Weight Decay   | 0.001 |\\n\\nTable 12: Test ChatGPT: answer questions with only one word.\\n\\nFor the dialogue between \u201cA\u201d and \u201cB\u201d in this task, please answer a question according to the dialogue with only one word. Following is the dialogue: {dialogue}.\\n\\nFollowing is the question: {question}\\n\\nTable 13: ChatGPT test template of Zero-Shot CoT\\n\\nThis is a natural language inference task. Given the dialogue context: {context}, Does {pragmatic turn} entails {implied meaning}? Reply \u2018entails\u2019 or \u2018not entails\u2019.\\n\\nThink step by step.\\n\\nMore Detail on Diplomat\\n\\nIn this section, we will propose more examples of our dataset in Tab. 14, Tab. 15, Tab. 16, Tab. 17, and Tab. 18.\"}"}
{"id": "yZQDF9f6bR", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 14: Contextual reasoning examples of DiPlomat\\n\\nA: Yeah. They say that he's the fastest pitcher there ever was. It's just he really couldn't find home plate. I mean, some of the stories you learn about this guy, it reads like fiction. When he was - I think this is around 1960. He's pitching in the minor leagues, and he pitched so fast he ripped the man's ear off.\\n\\nB: Oh.\\n\\nA: Yeah.\\n\\nRationale: The literal meaning is a simple expression of agreement, while the implied meaning is that the speaker is amazed by the story of Steve Dalkowski's feats.\\n\\nB: We're talking about 2.8 million people. Has the rise of temporary workers figured into, at least, the statistical improvement of the U.S. economy for some people?\\n\\nA: It has. Overall, about one seventh of the total job growth has been in the temp sector. The temp sector is growing nine times faster than the overall private sector as a whole. And the 2.9 million workers represents a record number, both in the number of temp workers and in the percentage of the economy that they make up.\\n\\nB: You know in \\\"Harvest Of Shame,\\\" Edward R. Murrow very famously said, the people we're showing you in this documentary have picked your Thanksgiving bounty with their bare hands, and this is how they live.\\n\\nRationale: The implied meaning of this turn is to reflect on our reliance on temporary workers in our day-to-day lives.\\n\\nA: And so I got up and ran. And it wasn't too far. But I just - at that moment, I thought, I don't want to be shot in the back, and I need to find some cover. And there's really no place to hide. But there are these\\n\\nB: You found a little, like, alcove that you could duck into.\\n\\nA: There was a little alcove, yeah. And I just made myself as small as I could in that little corner.\\n\\nRationale: The speaker tried to protect itself from danger.\\n\\nA: Well, there's a big argument in the United States about this. There's one group of folks who think that engagement policy failed. We engaged with China from 1979 until about 2013 when Xi Jinping came into power. And the idea of engagement was that coevolution was in the American interest as well as in China's interest. And you could bring China along to be a responsible player to some degree.\\n\\nA: Many hardliners in the United States government - and outside and including in the expert community - now claim that engagement was a sucker's game and that we have raised up a tiger which could now devour us. But there are different schools of thought about this, and many of us think that we still need to engage with China, albeit more strategically.\\n\\nB: That image of raising a tiger that will devour us is very dramatic.\\n\\nRationale: The situation is not necessarily an 'either/or' between China and the United States.\"}"}
{"id": "yZQDF9f6bR", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 15: Figurative language reasoning examples of DiPlomat\\n\\nA: Thank you. How are you?\\nB: I'm pretty good. Thank you. You must be stuck like glue on this, but, you know, you've played in three World Cups, including one of the wins for the U.S. team in 1999. How would you describe what it's like to be out there on that field in that final game?\\n\\nRationale: Stuck like glue means to be attached to something, which is a particular issue or a person.\\n\\nB: So in terms of what to do about it, we've said Twitter and Facebook have shut down these accounts, which prompts me to wonder - does shutting down a fake account do that much? Can't the Chinese government, if it's determined to go down this path, just open up two new ones in place of the one that was closed?\\n\\nA: It is a cat-and-mouse game, and the companies are constantly trying to get ahead of it. As you said, they can always set up new accounts.\\n\\nRationale: Mice are constantly trying to get away from cats and cats are constantly trying to catch mice. In the same way, the Chinese government will always be trying to escape restrictions on social media accounts and media companies will always be trying to find fake accounts.\\n\\nA: I really didn't feel safe because the Turkish government is very famous for hunting down those who oppose Erdogan. So, I mean, I just didn't want to really risk my life by going to Europe. But, you know, I talked to my team. I told them all, like, how many times I want to come because I want to be with you guys there, and I want to get a win with you guys. And then, later on, they came back with the news and said, you know what? I think the best decision is if you don't come. Let's just not risk it for one game.\\n\\nB: Do you feel safe in New York and elsewhere in the U.S.?\\nA: I have been getting last two, three days hundreds death threats, but I think I feel safe in America. But anywhere else in the world, I wouldn't really feel safe.\\n\\nRationale: He is implying that he is still not safe.\\n\\nTable 16: Commonsense reasoning examples of DiPlomat\\n\\nB: Yeah - African-American mayor from Tallahassee.\\nA: Yes. So this is sort of a test of whether real progressive candidates can win in these sort of purplish states. ...\\n\\nRationale: \\\"Purplish\\\" states are not really colored. They refer to US states that are neither clearly Republican (red) nor Democrat (blue) in their voting.\\n\\nB: He wrote a lot of letters by hand, didn't he?\\nA: He wrote tons of letters. I bet there are a hundred thousand - hundreds out there...\\n\\nRationale: tons of letters implies a very large number and not to full a ton.\\n\\nB: Well, Pluto's official designation is a dwarf planet. And I have to tell you the people who sent this probe all the way out to Pluto are a little angry about that because when they launched it a decade ago, Pluto was still a planet.\\nA: (Laughter)\\nB: It got downgraded in the intervening years.\\nA: That seems so unfair.\\n\\nRationale: A is expressing sympathy for the people who sent the probe, showing that they understand why they feel so disappointed.\"}"}
{"id": "yZQDF9f6bR", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 17: External knowledge reasoning examples of DiPlomat\\n\\nB: Inside of his house, family pictures decorate the walls and the fridge. Les has 15 great grandchildren. He grew up in an orphanage, and he couldn't wait to leave to join the military. And so in early 1944, he boarded a ship and crossed the Atlantic Ocean to go to the frontline.\\n\\nA: I loved that sailing on, of course. It was so dramatic. You could see all these ships bobbing up and down on the ocean. And destroyers were weaving in and out of them to make sure they uncovered any mines or anything.\\n\\nRationale: Sailing across the ocean during wartime was a perilous experience.\\n\\nA: . . . equivalent to a nuclear bomb?\\n\\nB: Well, it's about - its equivalent - the energy in that explosion is about 10 times the energy in the first atomic bomb. . .\\n\\nRationale: The energy released in the explosion is incredibly powerful.\\n\\nB: So in your polling, in your research, do you find that it's going to come down to maybe a couple thousand votes from these unaffiliated voters and on what issues? Or will they vote?\\n\\nA: It is likely at the moment to be a very narrow victory. President Bush won in 2004 with five percent. That was 100,000 votes. In other words, if it is one percent, that would be 20,000 votes, and right now, the polls are moving around in just single percentage points. So it could be that narrow.\\n\\nB: Now, I have read that Colorado is going to be this year's Florida and Ohio, that this is going to be the state that decides the election.\\n\\nA: I think it could be, and the interesting thing is that Obama and Palin were both in Jefferson County a couple of days ago, indicating that there may be actually even a county that could be looked at to be beyond an entire state.\\n\\nRationale: The turn is suggesting that the county of Jefferson in Colorado could be a key factor in deciding the election, despite the fact that it is only one of many counties in the state and there are other swing states in the election.\\n\\nTable 18: Others examples of DiPlomat\\n\\nA: There's that feeling - I mean, so many of us have parents in the industry. I mean, that's what this region is about, especially around Detroit, and Wayne State's in Detroit, the heart of Detroit. So, it's nerve-racking. Everyone is nervous. Everyone doesn't know what's going to happen next. We're all watching the news very closely. But at the same time, it's interesting, because with my generation, we almost seem to, kind of, not be as directly impacted. I mean, our family is, it puts stress on us, but the day to day of the university and the day to day at school doesn't seem to have changed that much.\\n\\nB: I understand you have friends there who are engineering majors. Do they have any sense of what their future looks like, and will it be there in Michigan?\\n\\nA: Everybody is secure in their choices and secure in their decision. Everybody thinks that the industry will come around, especially now with the news that GM is getting money from the government. And everybody is more hopeful, and I mean, the auto industry has always been one of the largest industries and a staple in America, and to think that that industry is just going to vanish, nobody is willing to concede that.\\n\\nRationale: A believes that the auto industry will not vanish despite the current situation.\\n\\nB: In the meantime, what more have you learned in your reporting about the death of Carlos Hernandez Vasquez?\\n\\nA: Well, a couple of things. One thing that really stands out is that Carlos Hernandez Vasquez died in a Border Patrol station. The previous migrant children who died were taken to the hospital first; Hernandez Vasquez was not even though immigration authorities clearly knew that he was sick. He was diagnosed with the flu by a nurse practitioner.\\n\\nRationale: The death of Carlos Hernandez Vasquez could have been prevented if he had been taken to the hospital.\\n\\nB: So, how do you and the retired general, James Jones, know each other?\\n\\nA: My gosh, I think - I can't even remember when I first met him. It's been so long ago. I'm sure I met him when he was head of the legislative liaison over the Senate. But I really became acquainted with him when he became a brigadier general, and, of course, I followed his career. Of course, he served very ably as a commandant in the marine corps and then as the European commander, just been with him from time to time. And I just consider him a very good friend.\\n\\nRationale: A has a high opinion of James Jones' character and career.\"}"}
{"id": "yZQDF9f6bR", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E Grice Maxims and Pragmatic Reasoning\\n\\nThe Gricean maxims have garnered substantial attention as a foundational theory within the domain of pragmatics. This theoretical framework comprises four distinct maxims: (1) The Maxim of Quality, (2) The Maxim of Quantity, (3) The Maxim of Relevance, and (4) The Maxim of Manner [16, 15]. In contrast to rigid rules or theorems, the Gricean maxims, which capture the prevalent dynamics of conversations, are susceptible to frequent breaches in the context of human communication. These breaches, stemming from the intricacies of real-world interaction, notably manifest in the violation of one or more of these maxims. Such breaches, aligned with the cooperative principle, give rise to pragmatic phenomena that necessitate the engagement of pragmatic reasoning by recipients of the communication [15].\\n\\nF Computational Resources\\n\\nFor our experiment, we utilized two A100s and one 3090. The majority of our experiments were conducted on the A100s, while for practical reasons, only Unified-QA-base, BART-base, and T5-small were tested on the 3090. It is important to mention that each experiment was run on a single GPU. We record the training time of models in Appendix F.\\n\\n| Model         | Base/Large | Training Time (min/epoch) | Device |\\n|---------------|------------|---------------------------|--------|\\n| BERT base     |            | 0.8/0.9                   | A100   |\\n| RoBERTa base  |            | 0.8/0.9                   | A100   |\\n| RoBERTa large |            | 2.5/2.8                   | A100   |\\n| GPT-2 base    |            | 5.8/6.2                   | A100   |\\n| DialoGPT medium |              | 2.4/4.2                   | A100   |\\n| DeBERTa base  |            | 0.9/0.9                   | A100   |\\n| ALBERT base   |            | 0.5/0.8                   | A100   |\\n\\nTable 19: Training Time of Models\\n\\nG Limitations & Negative Societal Impacts\\n\\nWe acknowledge two limitations in our study: bias and subjectivity. Since our dialogues primarily stem from an interview dataset, a considerable focus is placed on political topics. This is reasonable, as pragmatic phenomena frequently emerge in the statements of politicians to advance their specific goals. However, this focus introduces a certain degree of bias into our dataset. The second limitation relates to the absence of subjectivity. In our methodology, the data undergoes two stages of human annotation, ensuring higher quality and objectivity. However, pragmatic reasoning is inherently subjective, and prioritizing objectivity compromises the preservation of subjectivity, resulting in a limitation in terms of subjectivity coverage. Our dataset exhibits minimal negative societal impacts. This is primarily due to the fact that our dialogues are transcriptions of publicly available TV shows, which inherently limits the potential for negative effects.\\n\\nH Ethics Concern\\n\\nWere any ethical review processes conducted (e.g., by an institutional review board)?\\n\\nOur dataset does not involve human subjects. The dataset has undergone the institute's internal ethical review process.\\n\\nDoes the dataset contain data that might be considered confidential?\\n\\nNo, the data source derives from an existing public interview dataset.\\n\\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why.\\n\\nFew of the dialogues may talk about offensive topics.\\n\\nDoes the dataset identify subpopulations (e.g., by age or gender)?\\n\\nNot explicitly.\"}"}
{"id": "yZQDF9f6bR", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Is it possible to identify individuals (i.e., one or more natural persons) directly or indirectly (i.e., in combination with other data) from the dataset? Yes, our data contains the names of celebrities.\\n\\nResponsibility & Dataset License\\nWe bear all responsibility in case of violation of rights and our dataset is under the license of CC BY-NC-SA (Attribution-NonCommercial-ShareAlike).\\n\\nJ Datasheets for Our Dataset\\n\\nJ.1 Motivation\\n1. For what purpose was the dataset created? (Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.)\\nThis dataset was created to study pragmatic reasoning in dialogues, a specific gap is mentioned above in Appendix G.\\n\\n2. Who created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\\nThis dataset was created by the authors of this paper.\\n\\n3. Who funded the creation of the dataset? (If there is an associated grant, please provide the name of the grantor and the grant name and number.)\\nThe institute of the authors funded the creation of the dataset.\\n\\n4. Any other comments?\\nNone.\\n\\nJ.2 Composition\\n1. What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? (Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.)\\nAn instance of our dataset represent a piece of dialogue. Description is provided in our paper.\\n\\n2. How many instances are there in total (of each type, if appropriate)?\\nOur dataset owns 4,177 dialogues.\\n\\n3. Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? (If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).)\\nIt is a sample of all possible cases. As pragmatic phenomena aren\u2019t proved to be limited, we can\u2019t guarantee a full sampling of them.\\n\\n4. What data does each instance consist of?\\nWe mention it in our paper.\\n\\n5. Is there a label or target associated with each instance? If so, please provide a description.\\nYes. The description is in our paper.\\n\\n6. Is any information missing from individual instances? (If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.)\\nNo. We leverage the original dialogues.\\n\\n7. Are relationships between individual instances made explicit (e.g., users\u2019 movie ratings, social network links)? ( If so, please describe how these relationships are made explicit.)\\nNo. Instances are weakly related, but focus on the same phenomenon.\"}"}
{"id": "yZQDF9f6bR", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. Are there recommended data splits (e.g., training, development/validation, testing)? (If so, please provide a description of these splits, explaining the rationale behind them.)\\n\\nYes. We provide it.\\n\\n9. Are there any errors, sources of noise, or redundancies in the dataset? (If so, please provide a description.)\\n\\nYes. Some workers try to finish the work as quickly as possible, therefore when we ask them to offer a rationale for choosing a certain turn as a pragmatic turn, they simply type an \u201ca\u201d in the box. However, the situation is rare, and we blocked the workers and clean the data out of our dataset.\\n\\n10. Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? (If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.)\\n\\nIt\u2019s self-contained.\\n\\n11. Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals\u2019 non-public communications)? (If so, please provide a description.)\\n\\nNo.\\n\\n12. Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? (If so, please describe why.)\\n\\nYes. Some of the topic are big events, they may be offensive for some people. However, we consider our dataset\u2019s offensiveness to be limited, for the source dataset is a TV show transcript.\\n\\n13. Does the dataset relate to people? (If not, you may skip the remaining questions in this section.)\\n\\nYes.\\n\\n14. Does the dataset identify any subpopulations (e.g., by age, gender)? (If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset.)\\n\\nNo. This is not explicitly identified.\\n\\n15. Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? (If so, please describe how.)\\n\\nYes; their names are given in running text.\\n\\n16. Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? (If so, please provide a description.)\\n\\nYes. Our dataset may have dialogues talking about religious, politics and so on.\\n\\n17. Any other comments?\\n\\nNone.\\n\\nJ.3 Collection Process\\n\\n1. How was the data associated with each instance acquired? (Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.)\\n\\nThe data all comes from an interview dataset already published. (See our paper)\"}"}
{"id": "yZQDF9f6bR", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? (How were these mechanisms or procedures validated?)\\n\\nSoftware program and manual human curation (2 times). See our paper for details.\\n\\n3. If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?\\n\\nRandomly.\\n\\n4. Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?\\n\\nCrowdworkers. They are paid nicely. See Appendix for detail.\\n\\n5. Over what timeframe was the data collected? (Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.)\\n\\nThe dataset was collected in the early Spring of 2023, which does not necessarily reflect the timeframe of the data collected.\\n\\n6. Were any ethical review processes conducted (e.g., by an institutional review board)? (If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.)\\n\\nNo review processes were conducted with respect to the collection and annotation of this data (though review was done for other aspects of this work; see the paper linked at the top of the datasheet).\\n\\n7. Does the dataset relate to people? (If not, you may skip the remaining questions in this section.)\\n\\nYes.\\n\\n8. Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?\\n\\nOther sources. By curating a published dataset.\\n\\n9. Were the individuals in question notified about the data collection? (If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.)\\n\\nNo.\\n\\n10. Did the individuals in question consent to the collection and use of their data? (If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.)\\n\\nNo. All data are public.\\n\\n11. If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? (If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).)\\n\\nN/A.\\n\\n12. Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? (If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.)\\n\\nNo. We consider our dataset having a limited negative effect, for all of our data has been published for more than a year.\\n\\n13. Any other comments? None.\\n\\nJ.4 Preprocessing/cleaning/labeling\\n\\n1. Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? (If so, please provide a description. If not, you may skip the remainder of the questions in this section.)\\n\\nYes, preprocessing has been done.\"}"}
{"id": "yZQDF9f6bR", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"J.5 Uses\\n\\n1. Has the dataset been used for any tasks already? (If so, please provide a description.)\\n   Yes. See our paper for details.\\n\\n2. Is there a repository that links to any or all papers or systems that use the dataset? (If so, please provide a link or other access point.)\\n   No.\\n\\n3. What (other) tasks could the dataset be used for?\\n   Many more. Such as generation of implied meanings.\\n\\n4. Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? (For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms?)\\n   No.\\n\\n5. Are there tasks for which the dataset should not be used? (If so, please provide a description.)\\n   No.\\n\\n6. Any other comments?\\n   None.\\n\\nJ.6 Distribution\\n\\n1. Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? (If so, please provide a description.)\\n   Yes, the dataset is freely available.\\n\\n2. How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? (Does the dataset have a digital object identifier (DOI)?)\\n   On our website.\\n\\n3. When will the dataset be distributed?\\n   It's already been distributed.\\n\\n4. Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? (If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.)\\n   The dataset is licensed under a CC license.\\n\\n5. Have any third parties imposed IP-based or other restrictions on the data associated with the instances? (If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.)\\n   Not to our knowledge.\\n\\n6. Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? (If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.)\\n   Not to our knowledge.\\n\\n7. Any other comments?\\n   None.\"}"}
{"id": "yZQDF9f6bR", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Who is supporting/hosting/maintaining the dataset?\\nThe authors.\\n\\n2. How can the owner/curator/manager of the dataset be contacted (e.g., email address)?\\nWe will post our email address.\\n\\n3. Is there an erratum? (If so, please provide a link or other access point.)\\nCurrently, no. As errors are encountered, future versions of the dataset may be released (but will be versioned). They will all be provided in the same location.\\n\\n4. Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances')? (If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)?)\\nYes. However, the frequency isn't determined, and we'll publish the updated dataset on the same website if an renewal occurs, and we'll announce it on the website.\\n\\n5. If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)? (If so, please describe these limits and explain how they will be enforced.)\\nNo.\\n\\n6. Will older versions of the dataset continue to be supported/hosted/maintained? (If so, please describe how. If not, please describe how its obsolescence will be communicated to users.)\\nYes. The older versions of the dataset will be available on the website.\\n\\n7. If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? (If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to other users? If so, please provide a description.)\\nYes. They can email us.\\n\\n8. Any other comments?\\nNone.\"}"}
{"id": "yZQDF9f6bR", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"If we are negotiating in public, we're not really negotiating at all.\\n\\nWell, I doubt that, Senator.\\n\\nWell, I doubt that, Senator.\\n\\na. [A] is being polite and respectful towards [B].\\nb. [A]'s mind, political figures always tell lies, therefore he questions [B]'s words.\\nc. [A] doubts whether he would have had the senator on the TV.\\nd. [A] thinks that he will not only be tired of [B]'s words but also of the [B]'s face.\\n\\nWhich turn of the conversation has a pragmatic meaning?\\n\\nBecause I choose\\n\\nThat's correct. I'm out of the Senate three years and a half, but I was there for 36 years.\\n\\nMuch of my work was budgeting. You would have had me on the TV so many times, and you would have said I don't want to see you anymore.\\n\\nFigure 1: Illustration of DiPlomat dataset. Left: Example of a pragmatic conversation. Right: Pragmatic Identification and Reasoning task.\\n\\nAbstract\\n\\nThe ability to discern and comprehend pragmatic meanings is a cornerstone of social and emotional intelligence, referred to as pragmatic reasoning. Despite the strides made in the development of Large Language Models (LLMs), such as ChatGPT, these models grapple with capturing the nuanced and ambiguous facets of language, falling short of the aspiration to build human-like conversational agents.\\n\\nIn this work, we introduce a novel benchmark, the DiPlomat, which delves into the fundamental components of conversational pragmatic reasoning, encompassing situational context reasoning, open-world knowledge acquisition, and unified figurative language understanding. We start by collecting a new human-annotated dialogue dataset, composed of 4,177 multi-turn dialogues and a vocabulary of 48,900 words. Along with the dataset, two tasks are proposed to evaluate machines' pragmatic reasoning capabilities, namely, Pragmatic Reasoning and Identification (PIR) and Conversational Question Answering (CQA). Furthermore, we probe into a zero-shot natural language inference task, where the significance of context in pragmatic reasoning is underscored. Experimental findings illustrate the existing limitations of current prevailing LLMs in the realm of pragmatic reasoning, shedding light on the pressing need for further research to facilitate the emergence of emotional intelligence within human-like conversational agents.\"}"}
{"id": "yZQDF9f6bR", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1 Introduction\\n\\nThe fabric of human sociality is made up of complicated relations that evolve through different dimensions of interaction and communication channels [1, 2]. Social consensuses, such as social norms and values, are thereby formed between humans that convey meanings of individual minds, including beliefs, intentions and desires [3]. In a process of effective negotiation and social conversation, particularly, such social behaviors are only partly driven by literal meanings that are objective, rational and explicit [4, 5]. Instead, these behaviors are commonly governed by affective or pragmatic meanings of dialogue utterances that refer to the emotional and cultural meanings of conversational partners and are subjective, emotional and implicit. For instance in Fig. 1, the lady responds with \u201cI doubt that\u201d rather than \u201cI am not tired of you\u201d to express a sense of humor and politeness. The competency of perceiving such pragmatic meanings is crucial to social and emotional intelligence (EI) [6] and is referred to as pragmatic reasoning.\\n\\nThe rapid developments of large language models (LLMs), such as ChatGPT and InstructGPT [7], have set off a wave of the next generation of conversational AI over the recent years. Despite the inspiring capabilities of language generation [8] and reasoning [9] achieved with massive computational resources and tremendous natural language data, LLMs barely show convincing communicative skills, i.e., they fail to capture pragmatic and ambiguous meanings of input prompts [10]. Critically, current neural generative models are trained to be objective with safe and satisfiable responses [13, 14], which largely deviates from the long-standing goal of building a human-like agent. Recently, Meta Research Team et al. [5] introduce a ChatBot that demonstrates human-level play in a language board game Diplomacy where lying and misleading commonly occur. However, their main focus is on game policy learning rather than pragmatic reasoning.\\n\\nWhat are the core components of real-life conversational pragmatic reasoning? Motivated by theories of cognitive linguistics [1\u20133] and conversational modeling [4, 5, 15, 16], we anticipate it to be three-fold:\\n\\n\u2022 Situational Context Reasoning. Understanding pragmatic meaning requires a detailed understanding of conversational contexts. Consider the utterance \u201cYou are making the rest of us looking bad\u201d, under different situations of praise and sarcasm, the sentence may convey completely opposite meanings. Furthermore, typical conversational reasoning challenges such as coreference resolution and intention prediction are largely dependent on the success of situated context modeling.\\n\\n\u2022 Open-world Knowledge Acquisition. The open-world knowledge includes commonsense knowledge (e.g., social ethics) that can be learned from different domains of dialogue corpus and domain-specific knowledge (e.g., American histories). Successful pragmatic reasoning requires the acquisition of open-world knowledge and joint reasoning over the conversation.\\n\\n\u2022 Unified figurative language understanding. Figurative language is one of the most frequently used tricks for conveying implicit meanings with subjective emotions. Previous works treat different forms of figurative language understanding as individual tasks, such as metaphors [17], idioms [18, 19], pun [20], etc. Pragmatic reasoning provides a feasible unified perspective that considers all these tasks as recovering their literal meanings.\\n\\nIn order to step towards a general human-like communicative agent, in this work, we introduce DiPlomat, a real-life Dialogue dataset that focuses on Pragmatic reasoning. DiPlomat stems from an interview dataset [21], and experiences three steps of curation: automatic selection, fine-grained manual annotation and human refinement (Sec. 3). Our dataset comprises 4,177 dialogues and covers a vocabulary of 48,900 words. More than that, human-annotated answers reach the amount of 6,494 and hold a vocabulary size of 20,000. Along with the dataset, we propose two tasks, Pragmatic Identification and Reasoning (PIR) and Conversational Question Answering (CQA), to benchmark machines\u2019 pragmatic reasoning capabilities (Sec. 4). The CQA task possesses 19,482 questions concerning the content of collected dialogues and the answers to the questions are written by humans. We run extensive experiments on previous state-of-the-art models on DiPlomat (Sec. 5). The best model achieves less than 0.70 accuracy score in PIR, and none of the models achieve more than 0.50 accuracy score for CQA. Moreover, we test previous pre-trained LLMs\u2019 (including ChatGPT) zero-shot reasoning capability with a natural language inference (NLI) task. Regarding the experimental results provided, the significance of pragmatic reasoning speaks for itself. Throughout a thorough analysis of the limitations of current models, we aim to shed light on future research toward building general conversational agents.\"}"}
{"id": "yZQDF9f6bR", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1 provides a comparative analysis of our dataset with similar conversational datasets. The Dream [30] dataset formalizes dialogues from English exams into a question-answering task with a focus on in-depth dialogue comprehension. With question-answer pairs compiled by two annotators, CoQA [29] focuses on reasoning in conversation understanding. By utilizing English listening comprehension tests, MuTual [33] is built to address the issue of general dialogue reasoning. In contrast to these preceding datasets, the GRICE [12] dataset represents a significant advancement in the field of pragmatic reasoning as it incorporates implicature and reasoning. However, both MuTual and GRICE exhibit a shared limitation in that they do not possess data that closely resembles real-world interactions, leading to a lack of diversity in their respective datasets. Additionally, other relevant datasets, including commonsense [28], reasoning [23], and natural language inference (NLI) [31, 25], are also included in Tab. 1. By examining various dimensions such as domain, manual annotation, task variety, implicature, reasoning, and multi-turn interactions, our dataset offers unique advantages in addressing the challenge of pragmatic reasoning in dialogues.\\n\\nLanguage Models for Dialogue Generation\\nConversational AI has emerged as a prominent research area within the field of natural language processing (NLP), attracting significant attention and interest. Numerous pre-trained models have been proposed to tackle dialogue generation tasks, such as DialogGPT [35], GODEL [36], LaMDA [37], and Meena [38], and they have achieved marvelous results on competitions [39]. Furthermore, a pivotal milestone has been achieved with the advent of ChatGPT, garnering widespread interest and stimulating further investigation in the domain of conversational AI. ChatGPT, built upon the principles of transformer models [40], undergoes training on an extensive corpus of data, resulting in its profound efficacy. Notably, this system boasts an impressive magnitude of billions of parameters. In a notable study conducted by OpenAI [13], it has been demonstrated that the augmentation of parameters, referred to as the Scaling Law, substantially enhances the model's capabilities. Also, the enlargement of the number of parameters triggers the emergence of miraculous ability. After the success of ChatGPT, more models such as PaLM 2 [41] appeared in the field.\\n\\nPragmatic Reasoning\\nPragmatic reasoning is a significant subject within the field of pragmatics, attracting considerable attention from linguists. The Gricean maxims, which is one of the most important achievements, serves as a foundational theory within the domain of pragmatics. This theoretical framework comprises four distinct maxims: (1) The Maxim of Quality, (2) The Maxim of Quantity, (3) The Maxim of Relevance, and (4) The Maxim of Manner [15, 16]. In contrast to rigid\"}"}
{"id": "yZQDF9f6bR", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figurative Language Reasoning (26.6%)\\nExternal Knowledge Reasoning (12.9%)\\nCansense Reasoning (16.4%)\\nOthers (8.3%)\\n\\nA: What's remarkable about the Edenton Tea Party is that it was all women - possibly the first women-driven political protest in U.S. history.\\n\\nB: And I guess we can keep beating up on the British for just a moment here.\\n\\nA: Why not?\\n\\nPragmatic Identification\\n\\n2nd turn of PersonA.\\nRationale: This is a humorous response indicating agreement with continuing to criticize the British.\\n\\n1st turn of PersonB.\\nRationale: The implied meaning is that the technology has improved by a large margin.\\n\\n1st turn of PersonB.\\nRationale: The implied meaning is that PersonB is emphasizing the weather is obviously getting hotter.\\n\\n1st turn of PersonB.\\nRationale: The law punishing those who suggest Poland was complicit in the stifling of discussion about atrocities.\\n\\n1st turn of PersonB.\\nRationale: The implied meaning is that PersonB suggests not to trust Twitter as a reliable source of information.\\n\\nA: How has technology changed the way that vaccines are made? Is it easier to sort of see responses in living things as they happen now?\\n\\nB: Yes, the technology has improved by leaps and bounds.\\n\\nA: Poland recently has gone a step further, legislating how to talk about one of the darrest periods in its history.\\n\\nB: We are in a moment when we decide about how we're going to teach about and remember the Holocaust in the future.\\n\\nA: But what's the week been like?\\n\\nB: You know, hard for people in a country that's not used to heat and doesn't have much air conditioning. [...]\\n\\nDoes a chateau have to fall on your head to see that the earth, France included, is getting hotter?\\n\\nA: I am getting reports on Twitter. [...]\\n\\nB: We have not confirmed those reports. And Twitter, I mean, it's a good source of information, but we can't report it until we confirm it.\\n\\nA: It's sometimes a good source of misinformation too, isn't it?\"}"}
{"id": "yZQDF9f6bR", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we consider various types of pragmatic phenomena and utilize different techniques to extract relevant instances from the source dataset. For instance, we utilize the EPIE list [18] for a string-matching method to identify idioms in dialogues, and we train RoBERTa [49] on Hypo-XL [50] for hyperbole detection; refer to Appendix B.\\n\\nTopic Segmentation\\n\\nTopic segmentation is a small operation taken after automatic selection. The original dialogues employed in our study consist of lengthy and multi-turn exchanges, which are ill-suited for our research objectives. Consequently, we implement a segmentation process to break down these dialogues into shorter units. To achieve this, we employ two techniques, namely BERTScore [51] and TextTiling [52]. The segmentation procedure starts with computing the BERTScore between adjacent turns and subsequently applying the TextTiling algorithm to the generated BERTScores.\\n\\nStep II. Fine-grained Annotation.\\n\\nWe leverage Amazon Mechanical Turk (AMT) to conduct detailed annotation of pragmatic turns within our dialogues. Workers participating in the annotation task are instructed to select all turns that exhibit a divergence between their literal meaning and their intended meaning. Due to the subjective nature of pragmatic reasoning, we request the workers to provide confidence scores along with reasons for their choices and for each dialogue two workers are recruited. All annotators shall meet the following criteria: (i) from English-speaking countries; (ii) Completion of a minimum of 1,300 tasks with a 98% approval rate. We also present them with detailed instructions and four examples so that workers are clear about our objectives and requirements. The instructions part outlines the step-by-step procedures for accomplishing the assigned tasks and highlights some key points that workers should pay attention to. The four examples offered are representative of classical pragmatic conversations drawn from the field of linguistics, serving as practical references for the workers. To mitigate the intricacies arising from the identities of dialogue communicators, a simplified representation is adopted, whereby the speakers are denoted as PersonA and PersonB. To ensure the reasonableness and quality of the data, we manually examined 30 data samples and blocked workers who are unqualified. As a result, a total of 5,869 dialogues are selected; refer to Appendix B for details.\\n\\n| Table 2: Statistical feature of DiPlomat |\\n|-----------------------------------------|\\n| # Dialogues                           | 4.17 \u00d7 10^3 |\\n| Avg. Turns per Dialogue                | 4.10                |\\n| Avg. Words per Turn                    | 42.80              |\\n| Avg. Human Reason per Dialogue         | 1.56               |\\n| Avg. Words per Human Annotated Reason  | 25.31              |\\n| Vocabulary Size (dialogue)             | 4.89 \u00d7 10^4       |\\n| Vocabulary Size (human-annotated reasons) | 2.00 \u00d7 10^4 |\\n\\nStep III. Human Refinement.\\n\\nIn this process, tasks for workers are formulated as multiple-choice questions. Previously collected human-annotated reasons are transformed into choices, utilizing a template format: [turn {turn_id}: {reason}]. In addition, to mitigate the impact of careless workers, we introduce a distractor choice for each gold choice. These distractor choices are generated using BERTScore [51] by selecting the reason with the highest score from other unrelated dialogues. Of note, for each dialogue, an equal number of gold choices and distractor choices are provided. Workers are requested to select all reasonable choices for each conversation and are warned of the presence of distractor choices. Workers who frequently select distractor choices are blocked, and their work is rejected. Through this refinement process, 1,692 dialogues are filtered out, while 4,177 dialogues are preserved, ensuring the integrity and reliability of our dataset; refer to Appendix B for more details.\\n\\n3.2 Dataset Statistics\\n\\nThe DiPlomat dataset comprises a total of 4,177 multi-turn dialogues, with each dialogue averaging 4.1 turns. On average, there are 1.56 pragmatic turns per dialogue. The distribution of dialogues with different quantities of pragmatic turns is illustrated in Fig. 4; see Tab. 2 for detailed dataset statistics.\\n\\nWith respect to the motivation introduced in Sec. 1, we categorize the process of transitioning from dialogue to human-annotated rationales into five reasoning types:\\n\\n\u2022 Contextual Reasoning: The comprehension of the context is paramount for this reasoning process.\\n\u2022 Figurative Language Reasoning: Proficiency in understanding figurative language, such as idioms and metaphors, is indispensable for advancing this type of reasoning.\\n\u2022 Commonsense Reasoning: The utilization of commonsense knowledge, such as recognizing that a chateau cannot fall from the sky, is vital for this category.\"}"}
{"id": "yZQDF9f6bR", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 External Knowledge Reasoning: This form of reasoning necessitates knowledge that extends beyond commonsense and is not explicitly mentioned in the dialogue.\\n\\n\u2022 Others: This category includes pragmatic dialogues that fit none of the above.\\n\\nFig. 2 demonstrates the proportion of each type. The prevalence of data within the context partition prove the significance of context in pragmatic reasoning of real life. Fig. 3a depicts a sunburst visualization illustrating the distribution of trigram words within pragmatic turns. The diverse range of trigram words indicates that the DiPlomat dataset enjoys the rich diversity from real-life corpora, and covers a wide array of topics. In addition, the recurring occurrence of the words \u201cpresident'' and \u201cworld'' is observed, demonstrating DiPlomat's slight bias to politics and world-wide events.\\n\\n4 Task Definition\\nWe propose 2 distinct tasks for our dataset: (i) Pragmatics Identification and Reasoning and (ii) Conversational Question Answering. The former task focuses on assessing the capability of models to identify the presence of pragmatic phenomena and their ability to select a suitable answer for such identification. The latter task aims to evaluate the models' adeptness in employing pragmatic reasoning by presenting them with carefully designed questions.\\n\\n4.1 Task 1: Pragmatics Identification and Reasoning (PIR)\\nIn this task, models are provided with dialogues and are required to identify turns whose actual meanings deviate from their literal interpretations, commonly referred to as pragmatic turns. If their selections are accurate, a set of rationales is presented and they are expected to choose the most plausible reason for each pragmatic turn. For each turn, there are 5 candidate reasons available, comprising one gold choice and four disturbing choices. The model's success in this task depends on the precise execution of both steps. We consider three diagnostic settings to test the machine's capability on pragmatic understanding:\\n\\n\u2022 Conversation \u2192 Pragmatic Turn (C \u2192 P). For each instance, models are presented with a dialogue and a specific turn extracted from that dialogue. They are then required to determine whether the given turn qualifies as a pragmatic turn. Consequently, the dataset is flattened to a total of 17,129 instances, with each instance corresponding to a single queried turn. It's important to highlight that turns without pragmatic meanings are also extracted for evaluation.\\n\\n\u2022 Conversation + Pragmatic Turn \u2192 Rationale (CP \u2192 R). In this subtask, we offer the model both the dialogue and the pragmatic turn and it needs to choose the most plausible rationale out of five candidate choices.\\n\\n\u2022 Conversation \u2192 Pragmatic Turn + Rationale (C \u2192 PR). In this subtask, models pre-trained on the previous two subtasks are combined to infer the final results. Specifically, the model obtained from the first subtask is utilized for determining pragmatic turns, while its version finetuned on the second subtask is employed for selecting the most suitable rationale. It is worth noting that, in contrast to C \u2192 P, in this subtask, extracted turns are limited to pragmatic turns only.\"}"}
{"id": "yZQDF9f6bR", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Recall that in our data collecting procedure outlined in Sec. 3.1, besides asking workers to select pragmatic turns, they are also instructed to fill in reasons to explain their choices. To simplify the evaluation process, the selected turns are directly utilized as answers for the first subtask, while the reasons provided by the workers serve as the designated correct choices (referred to as \\\"gold choices\\\") for the second subtask.\\n\\nDistractor Choice\\n\\nAs a result of the time-consuming nature of BERTScore [51], an alternative approach is adopted for measuring sentence similarity using Sentence-Transformers [53], which is a significantly faster method. In our methodology, for each gold choice, four alternative choices with high similarity scores are selected from the pool of gold choices to serve as distracting answers. Despite their high similarity scores, upon careful examination within the given context, it's apparent that the distracting answers convey entirely different meanings from the gold answer. This characteristic makes them appropriate components to build our task.\\n\\n4.2 Task 2: Conversational Question Answering (CQA)\\n\\nA: Finally, Ron, lots of talk about Congress releasing the second half of this $700 billion bailout this week. Where do we stand with that?\\n\\nB: Quite possible that Congress will get that done this week now that Barack Obama has asked George Bush, has asked the current president, to formally put in a request for that money. Congress has got a lot of questions about how this money is going to be spent, as it has questions about how the first half of the money was spent.\\n\\nRationale:\\nBarack Obama's request for the $700 billion bailout may expedite the process.\\n\\nQ1: What may expedite the process?\\nA1: Request\\n\\nQ2: Which president-elect requested the $700 billion bailout to be released?\\nA2: Obama\\n\\nFigure 5: Conversational Question Answering example.\\n\\nThe ability to apply pragmatic reasoning is crucial for effective communication and achieving a thorough grasp of the natural language system of human beings. To address this, we propose conversational question-answering, wherein multiple questions are formulated for each dialogue, and an example is shown in Fig. 5. The questions focus on delving deeper into dialogues, often needs insights into intended meanings to answer. ChatGPT plays a pivotal role in question generation and thanks to AMT, we can ensure the collection of high-quality answers. Ultimately, 19,482 question-answer pairs are assembled.\\n\\nQuestion Generation\\nChatGPT is employed to generate questions with prompts consisting of dialogues and human-annotated reasons. We task it to generate questions challenging for individuals who are unaware of the dialogues' intended meanings. More than that, for the convenience of evaluation, the question is also asked to be able to answer within one or two words. Furthermore, to ensure diversity, we instruct ChatGPT to start the questions with \\\"What\\\", \\\"Which\\\" or \\\"How\\\". A preliminary assessment is carried out by sampling a few examples out of the question pool to guarantee quality.\\n\\nAnswer Collection\\nThe answers to the questions are obtained through the utilization of AMT. Each worker is provided with a dialogue along with several associated questions and it is requested to answer the questions in one single word. To minimize the potential for misinterpretation, we offer an example coming from our dataset, which is annotated by the author itself. Through the process of human annotation, we consistently evaluate the collected data and reject unqualified answers as well as block workers who fail to meet our standards.\\n\\nStatistical features of Questions\\n\\nFig. 3b showcases the diverse range of our questions. These questions encompass a variety of sentence structures, starting with interrogative words: What, Which, and How, and possessing a large diversity of the words that follow the interrogative ones. Apart from the questions, the answer set holds a vocabulary size of 8,179, which is also of great diversity and raises a challenge for models.\\n\\n5 Experiment\\n5.1 Pragmatics Identification and Reasoning\\n\\nFor $C \\\\rightarrow P$, we partitioned the dataset into distinct subsets for training, validation, and testing. The training set consists of 13,708 examples, surpassing the 1,361 instances in the validation set and the 2,060 instances in the test set in terms of size. Models are trained on the training set and their...\"}"}
{"id": "yZQDF9f6bR", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Pragmatics Identification and Reasoning\\n\\nResults. The numerical results are accuracy scores in their percentage.\\n\\n| Model            | CP \u2192 P | CP \u2192 R | C \u2192 PR |\\n|------------------|--------|--------|--------|\\n| Random           | 50     | 10     | 50.2   |\\n| BERT base        | 63.2 \u00b1 1.1 | 91.3 \u00b1 0.7 | 50.2 \u00b1 6.8 |\\n| RoBERTa base     | 64.4 \u00b1 1.3 | 92.0 \u00b1 0.4 | 50.0 \u00b1 11.2 |\\n| RoBERTa large    | 63.8 \u00b1 0.0 | 60.8 \u00b1 0.5 | 0.0 \u00b1 0.0 |\\n| GPT-2 base       | 64.4 \u00b1 0.7 | 90.9 \u00b1 0.9 | 13.06 \u00b1 1.1 |\\n| DialoGPT medium  | 65.0 \u00b1 0.6 | 24.5 \u00b1 1.9 | 3.8 \u00b1 1.5 |\\n| DeBERTa base     | 64.9 \u00b1 0.2 | 92.6 \u00b1 0.6 | 43.9 \u00b1 1.2 |\\n| ALBERT base      | 65.1 \u00b1 0.4 | 90.6 \u00b1 0.2 | 34.9 \u00b1 1.8 |\\n\\nThe performance is evaluated on the validation set after each epoch to determine the optimal checkpoint. The best checkpoint is subsequently loaded for the final evaluation on the test dataset. The evaluation metric employed is the accuracy score, calculated as the ratio of correct predictions to the total number of instances. Similarly, for the task of CP \u2192 R, the dataset is also partitioned into training, validation, and test subsets. The respective sizes of these subsets are 5,188, 244, and 1,062 examples. The training and evaluation procedures are identical to those of the previous subtask. For C \u2192 PR, the test sets in previous subtasks are taken for evaluation, and it's worth noting that their test sets consist of exactly the same instances. This design ensures the prevention of any leakage of the test set into the training set, thereby maintaining the integrity of the evaluation process.\\n\\nFigure 6: Models performances of different reasoning types. The taxonomy of reasoning types comes from Fig. 2.\\n\\nResults and Analysis\\n\\nWe present four key observations for this task:\\n\\n\u2662 The primary performance bottleneck lies in the subtask C \u2192 P. As shown in Tab. 3, the best model achieves an accuracy score of 92.0% in the CP \u2192 R subtask, indicating that models possess the capability to reason to some extent. However, when it comes to the C \u2192 PR task, the best-performing model achieves only 50.2% accuracy, while the highest accuracy achieved in the C \u2192 P task is 65.0%. The substantial difference between the score of 92.0% and the score of 65.0% suggests that the difficulty in accomplishing the task primarily stems from the C \u2192 P subtask.\\n\\n\u2662 Accumulated variance in the C \u2192 PR subtask. The models exhibit significant variance in the results of the third subtask, which can be attributed to the variance introduced by its constituent tasks.\\n\\n\u2662 The significance of pragmatic awareness in language models. Both the C \u2192 P and C \u2192 PR subtasks require pragmatic awareness, and the poor performances of the models on these subtasks highlight their limitations in accurately determining the optimal timing for deploying reasoning abilities.\\n\\n\u2662 As depicted in Fig. 6, we assess the performance of existing models across five dimensions by leveraging the taxonomy outlined in Fig. 2. It becomes evident that models exhibit a nearly uniform performance across these dimensions, leading us to the inference that pragmatic reasoning constitutes a cohesive task, and a segregated approach is ill-suited for its treatment.\\n\\n5.2 Conversational Question Answering\\n\\nSimilarly to the previous task, the question-answering dataset is divided into training, validation, and test sets, comprising 15,585, 1,559, and 2,338 instances, respectively. Experimental subjects include BART [54], T5 [55], UnifiedQA [56], and mT5 [57]. The metric adopted is also accuracy score. Given ChatGPT's impressive performance in MMLU and AI2 Reasoning Challenge [13], we further examine ChatGPT's capability in the context of CQA by prompting it to provide one-word answers to questions. However, due to its uncontrollable nature, the generated answers may not always align with our desired settings. Hence, we introduce two evaluation metrics for ChatGPT:\\n\\n1. **em (exact match)**, which requires ChatGPT to produce the exact same word as our answer,\\n2. **pm (partially match)**, where we consider ChatGPT to be correct as long as our answer appears in its generated output.\\n\\nTwo configurations are employed for evaluation. In the first configuration, models receive dialogues and questions while remaining blind to human-annotated rationales.\"}"}
{"id": "yZQDF9f6bR", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: CQA task results with/without human annotated rationales. The numerical results are accuracy scores in their percentage. em: exact match, pm: partially match.\\n\\n| Model          | w/o rationale | w/ rationale |\\n|---------------|---------------|--------------|\\n| BART-base     | 20.2 \u00b1 1.1    | 29.6 \u00b1 0.6   |\\n| T5-small      | 24.9 \u00b1 0.7    | 34.1 \u00b1 1.4   |\\n| mT5-small     | 19.7 \u00b1 0.4    | 29.8 \u00b1 0.8   |\\n| UnifiedQA-base| 28.9 \u00b1 2.0    | 38.8 \u00b1 0.2   |\\n| UnifiedQA-large| 32.8 \u00b1 0.0   | 42.4 \u00b1 0.6   |\\n| ChatGPT (em)  | 40.6 \u00b1 1.0    | 45.1 \u00b1 1.5   |\\n| ChatGPT (pm)  | 32.8 \u00b1 1.0    | 45.1 \u00b1 1.5   |\\n\\n\u0394: +9.4 (46.5%) +9.0 (35.8%) +10.1 (51.3%) +9.9 (34.1%) +9.6 (29.3%) +0.5 (60%) +5 (12.3%)\\n\\nResults and Analysis\\n\\nThe experimental results are presented in Tab. 4. Our observations can be categorized into three main aspects. Firstly, the importance of pragmatic meaning is proved. As shown in Tab. 4, there exists a notable disparity between the results of models that have access to human-annotated answers and those that do not. On average, the model performances improve by 38.47% after the introduction of human-annotated rationales. Even the lowest-performing model in the initial experiment, mT5-small, demonstrates a 9.4% increase in accuracy. The substantial discrepancy in results between the two configurations underscores the significance of elucidating intended meanings in the development of effective communicators. Second, the models display deficiencies in applying pragmatic reasoning. Our questions are designed to demand a deeper understanding of conversations, however, the models struggle to perform well on our task. The best-performing model, ChatGPT, achieves an accuracy score of 40.6%. It is worth noting that our questions are generated by ChatGPT itself, and our source dataset, Interview, was proposed prior to the emergence of ChatGPT, which means that ChatGPT may have encountered our text during training. These characteristics render its result unsatisfactory. Third, generalization across different types of pragmatic reasoning poses challenges. In this analysis, we focus exclusively on models other than ChatGPT due to the lack of clarity regarding its training process. As demonstrated in Tab. 4, these models showcase a substantial improvement in performance following the inclusion of human-annotated rationale. The extent of this improvement exhibits slight fluctuations among the various models, suggesting a shared obstacle that hinders their overall performance. Noticed that the models are fine-tuned on a training set that is 5.2 times larger than the test set, we can conclude that achieving effective generalization from one pragmatic reasoning process to another remains a formidable and challenging task.\\n\\n5.3 Zero-Shot Natural Language Inference\\n\\nTable 5: Results of Natural Language Inference Task. \u22c4: T5-XXL fine-tuned on true NLI mixture [58]. \u2020: DeBERTa-v3 trained on MNLI [25] and SNLI [59].\\n\\n| Method          | Acc. |\\n|-----------------|------|\\n| Random          | 50.0 |\\n| DeBERTa-Large   | 44.3 |\\n| T5-XXL          | 45.3 |\\n| ChatGPT         | 85.7 |\\n| ChatGPT w/ CoT  | 63.8 |\\n\\nWe conduct the natural language inference (NLI) task [60] to evaluate the model\u2019s comprehension of language and to emphasize the importance of context in pragmatic reasoning. Different from previous tasks, zero-shot NLI task sheds a light on models\u2019 initial ability as they are tested without fine-tuning. The task involves providing two sentences: a premise and a hypothesis, and models are required to determine the relationship between the two sentences, which can be entailment, contradiction, or neutral. As there are no negative samples in our dataset, we simplify the task by asking the model to judge whether there is entailment. In this task, models are presented with a dialogue, a turn of the dialogue, and an intended meaning, they need to judge whether the turn entails the intended meaning. Noticed that collected data as described in Sec. 3.1 consists of reasons and implied meanings, to better fit our task, we abandon the reasons and preserve the implied meanings. Models are tested under zero-shot setting, which means that they are not allowed to train on any data before testing. Thus the innate abilities of models play a decisive role. Baseline models include T5 [55], DeBERTa [61], and ChatGPT. It\u2019s important to note that ChatGPT and the other two models are tested on different settings. ChatGPT is tested with the whole dialogue and the implied meaning as a prompt. However, to inspect the significance of context, the other two models are only provided with the pragmatic turn and the corresponding pragmatic meaning. ChatGPT is evaluated using two types.\"}"}
{"id": "yZQDF9f6bR", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Let's think step by step is a prompt discovered to improve the model's reasoning ability; refer to Appendix C for more details.\\n\\nResults and Analysis\\n\\nResults are listed in Tab. 5. As this task shares similar settings as binary classification, randomized answer accuracy is expected to be 50%. We observe below randomized performance on some previous SOTA models. Note that, each of the data is annotated by two humans, thus it's reasonable to view human performance as 100%. ChatGPT achieves the highest result but still shows a huge gap compared with human annotations. The outcomes highlight the imperfectness of the models' reasoning abilities. For T5-large and DeBERTa, context is blinded, but for ChatGPT, it is reachable. Hence, the performance gap among T5-large, DeBERTa, and ChatGPT shows the importance of context in our task. Interestingly, CoT doesn't offer help to ChatGPT but is harmful to ChatGPT's performance.\\n\\n6 Discussions and Future Work\\n\\nIn this paper, we propose DiPlomat, a high-quality manually annotated multi-turn dataset of pragmatic reasoning in conversations. Along with the dataset, we propose two tasks and baselines. Comparing experimental results, we emphasize the nonnegligible impact of contexts and reasoning on building perfect communicators. We also highlight the importance of pragmatic awareness and its bottleneck effect on our tasks. There is still a significant disparity between current performances and established standards.\\n\\nMemorization vs. Reasoning\\n\\nNoticed that models exhibit outstanding performance on CP \u2192 R of Sec. 5.1. On the contrary, for C \u2192 PR, models achieve poor results. Since the underlying knowledge is consistent for both tasks, the disparity in performance is hypothesized to be attributed to memorization. Instead of truly understanding the knowledge, the models tend to memorize patterns.\\n\\nSubjectiveness vs. Objectiveness\\n\\nJi et al. [63] emphasize the importance of modeling a distribution that encompasses a diverse range of possibilities, rather than solely relying on a single \u201cbest\u201d prediction. During the annotation process, we observe a phenomenon that different workers hold diverse opinions regarding pragmatic turns and their intended meanings. Their annotations often exhibit significant variations, sometimes even presenting completely opposing interpretations. We maintain the possibility of subjectiveness with careful task metric design (Sec. 4.1).\\n\\nLarger Models vs. Smaller Models\\n\\nConventional wisdom suggests that larger models, endowed with a greater number of parameters, generally exhibit superior performance in comparison to their smaller counterparts. However, an intriguing observation comes to the forefront within the context of the Pragmatic Identification and Reasoning (PIR) experiment, as elucidated in Table 3: RoBERTa base surpasses RoBERTa large in performance. We posit that this phenomenon can be attributed to a dual-fold rationale. First and foremost, a divergence emerges between the domain of pragmatic data and the domain of pretraining data for language models. Consequently, larger models exhibit a more substantial and consistent influence of their pretraining data, rendering them less adaptable to domain shifts towards pragmatic data. Secondly, the current landscape of pragmatic data is characterized by its inherent diversity and relative scarcity. This inherent diversity poses a particular challenge for larger models in adapting to such heterogeneity; refer to Appendix A.\\n\\nFuture Work\\n\\nAchieving generalization across multiple types of pragmatic reasoning processes poses significant challenges. Consequently, we propose that the construction of a proficient communicator necessitates the incorporation of methods beyond purely data-driven approaches. Furthermore, the availability of comprehensive evaluation data is of utmost importance. As a result, we target more high-quality datasets and new methods other than data-driven for the problem.\\n\\nAcknowledgements\\n\\nThe authors thank Jiaming Yu at Oxford for the preliminary survey, Dr. Xue Feng and Dr. Lifeng Fan at BIGAI for helpful discussions on data collection, Ms. Zhen Chen at BIGAI for designing the teaser figure, Junpeng Li and Dr. Zixia Jia at BIGAI for application experiments. This work presented herein is supported by the National Key R&D Program of China (2022ZD0114900) and the National Natural Science Foundation of China (62376031).\"}"}
{"id": "yZQDF9f6bR", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Jens Ambrasat, Christian von Scheve, Markus Conrad, Gesche Schauenburg, and Tobias Schr\u00f6der. Consensus and stratification in the affective meaning of human sociality. Proceedings of the National Academy of Sciences (PNAS), 111(22):8001\u20138006, 2014.\\n\\n[2] Alan P Fiske. The four elementary forms of sociality: framework for a unified theory of social relations. Psychological review, 99(4):689, 1992.\\n\\n[3] John A Bargh and Tanya L Chartrand. The unbearable automaticity of being. American psychologist, 54(7):462, 1999.\\n\\n[4] Edward Finegan. Language: Its structure and use. Cengage Learning, 2014.\\n\\n[5] Meta Fundamental AI Research Diplomacy Team (FAIR)\u2020, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378(6624):1067\u20131074, 2022.\\n\\n[6] Silke Anders, Roos de Jong, Christian Beck, John-Dylan Haynes, and Thomas Ethofer. A neural link between affective understanding and interpersonal attraction. Proceedings of the National Academy of Sciences (PNAS), 113(16):E2248\u2013E2257, 2016.\\n\\n[7] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems (NeurIPS), 35:27730\u201327744, 2022.\\n\\n[8] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\n[9] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n\\n[10] Laura Ruis, Akbir Khan, Stella Biderman, Sara Hooker, Tim Rockt\u00e4schel, and Edward Grefenstette. Large language models are not zero-shot communicators. arXiv preprint arXiv:2210.14986, 2022.\\n\\n[11] Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West, Alexander Koller, Swabha Swayamdipta, Noah A. Smith, and Yejin Choi. We\u2019re afraid language models aren\u2019t modeling ambiguity. 2023.\\n\\n[12] Zilong Zheng, Shuwen Qiu, Lifeng Fan, Yixin Zhu, and Song-Chun Zhu. GRICE: A grammar-based dataset for recovering implicature and conversational reasoning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2074\u20132085, Online, August 2021. Association for Computational Linguistics.\\n\\n[13] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\\n\\n[14] Amelia Glaese, Nat McAleese, Maja Tr\u0119bacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.\\n\\n[15] Alan Cruse. Meaning in language: An introduction to semantics and pragmatics.\\n\\n[16] Herbert P Grice. Logic and conversation. 1975.\\n\\n[17] Tuhin Chakrabarty, Xurui Zhang, Smaranda Muresan, and Nanyun Peng. Mermaid: Metaphor generation with symbolism and discriminative decoding. In Annual Meeting of the Association for Computational Linguistics (ACL), 2021.\\n\\n[18] Prateek Saxena and Soma Paul. Epie dataset: A corpus for possible idiomatic expressions. In Text, Speech, and Dialogue - 23rd International Conference, TSD 2020, Brno, Czech Republic, September 8-11, 2020, Proceedings, 2020.\"}"}
{"id": "yZQDF9f6bR", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tosin P. Adewumi, Roshanak Vadoodi, Aparajita Tripathy, Konstantina Nikolaidou, Foteini Liwicki, and Marcus Liwicki. Potential idiomatic expression (pie)-english: Corpus for classes of idioms. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, 2022.\\n\\nIssa Annamoradnejad and Gohar Zoghi. Colbert: Using bert sentence embedding in parallel neural networks for computational humor, 2022.\\n\\nBodhisattwa Prasad Majumder, Shuyang Li, Jianmo Ni, and Julian McAuley. Interview: Large-scale modeling of media dialog with discourse patterns and knowledge grounding. In Annual Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8129\u20138141, Online, November 2020. Association for Computational Linguistics.\\n\\nRyan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. The Ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems. In Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 285\u2013294, Prague, Czech Republic, September 2015. Association for Computational Linguistics.\\n\\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. In Annual Conference on Empirical Methods in Natural Language Processing (EMNLP), 2017.\\n\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018.\\n\\nAdina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2018.\\n\\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. Personalizing dialogue agents: I have a dog, do you have pets too? In Annual Meeting of the Association for Computational Linguistics (ACL), 2018.\\n\\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Annual Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018.\\n\\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos qa: Machine reading comprehension with contextual commonsense reasoning. In Annual Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019.\\n\\nSiva Reddy, Danqi Chen, and Christopher D. Manning. Coqa: A conversational question answering challenge. In Transactions of the Association for Computational Linguistics (TACL), 2019.\\n\\nKai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, and Claire Cardie. Dream: A challenge dataset and models for dialogue-based reading comprehension. In Transactions of the Association for Computational Linguistics (TACL), 2019.\\n\\nSean Welleck, Jason Weston, Arthur Szlam, and Kyunghyun Cho. Dialogue natural language inference. In Annual Meeting of the Association for Computational Linguistics (ACL), 2019.\\n\\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2019.\\n\\nLeyang Cui, Yu Wu, Shujie Liu, Yue Zhang, and Ming Zhou. Mutual: A dataset for multi-turn dialogue reasoning. In Annual Meeting of the Association for Computational Linguistics (ACL), 2020.\\n\\nPaloma Jeretic, Alex Warstadt, Suvrat Bhooshan, and Adina Williams. Are natural language inference models impressive? learning implicature and presupposition. In Annual Meeting of the Association for Computational Linguistics (ACL), 2020.\\n\\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. Dialogpt: Large-scale generative pre-training for conversational response generation. In Annual Meeting of the Association for Computational Linguistics (ACL), 2020.\\n\\nBaolin Peng, Michel Galley, Pengcheng He, Chris Brockett, Lars Liden, Elnaz Nouri, Zhou Yu, Bill Dolan, and Jianfeng Gao. Godel: Large-scale pre-training for goal-directed dialog. arXiv preprint arXiv:2206.11309, 2022.\"}"}
{"id": "yZQDF9f6bR", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lambda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.\\n\\nDaniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, and Quoc V. Le. Towards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977, 2020.\\n\\nChulaka Gunasekara, Seokhwan Kim, Luis Fernando D\u2019Haro, Abhinav Rastogi, Yun-Nung Chen, Mihail Eric, Behnam Hedayatnia, Karthik Gopalakrishnan, Yang Liu, Chao-Wei Huang, Dilek Hakkani-T\u00fcr, Jinchao Li, Qi Zhu, Lingxiao Luo, Lars Liden, Kaili Huang, Shahin Shayandeh, Runze Liang, Baolin Peng, Zheng Zhang, Swadheen Shukla, Minlie Huang, Jianfeng Gao, Shikib Mehri, Yulan Feng, Carla Gordon, Seyed Hossein Alavi, David Traum, Maxine Eskenazi, Ahmad Beirami, Eunjoon Cho, Paul A. Crook, Ankita De, Alborz Geramifard, Satwik Kottur, Seungwhan Moon, Shivani Poddar, and Rajen Subba. Overview of the ninth dialog system technology challenge: Dstc9, 2020.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017.\\n\\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.\\n\\nYunxiang Zhang and Xiaojun Wan. MOVER: Mask, over-generate and rank for hyperbole generation. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 6018\u20136030, Seattle, United States, July 2022. Association for Computational Linguistics.\\n\\nOrion Weller and Kevin Seppi. The rJokes dataset: a large scale humor collection. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 6136\u20136141, Marseille, France, May 2020. European Language Resources Association.\\n\\nKevin Stowe, Prasetya Utama, and Iryna Gurevych. IMPLI: Investigating NLI models\u2019 performance on figurative language. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 5375\u20135388, Dublin, Ireland, May 2022. Association for Computational Linguistics.\\n\\nTuhin Chakrabarty, Yejin Choi, and Vered Shwartz. It's not rocket science: Interpreting figurative language in narratives. In Annual Meeting of the Association for Computational Linguistics (ACL), 2022.\\n\\nDeepanway Ghosal, Siqi Shen, Navonil Majumder, Rada Mihalcea, and Soujanya Poria. Cicero: A dataset for contextualized commonsense inference in dialogues. In Annual Meeting of the Association for Computational Linguistics (ACL), 2022.\"}"}
{"id": "yZQDF9f6bR", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "yZQDF9f6bR", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models towards multi-step reasoning. In Proceedings of Machine Learning Research, volume 202, pages 10421\u201310430, 23\u201329 Jul 2023.\\n\\nTimo Schick and Hinrich Sch\u00fctze. It's not just size that matters: Small language models are also few-shot learners. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 2339\u20132352, Online, June 2021. Association for Computational Linguistics.\\n\\nRishabh Misra. News headlines dataset for sarcasm detection, 2022.\\n\\nLennart Wachowiak, Dagmar Gromann, and Chao Xu. Drum up SUPPORT: Systematic analysis of image-schematic conceptual metaphors. In Proceedings of the 3rd Workshop on Figurative Language Processing (FLP), pages 44\u201353, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Annual Meeting of the Association for Computational Linguistics (ACL), 2019.\\n\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations (ICLR), 2020.\\n\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.\"}"}
{"id": "yZQDF9f6bR", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendices\\n\\nContents\\n\\nA Hypothetic Explanation for Experimental Results of RoBERTa\\n\\n17 RoBERTa base outperforms RoBERTa large\\n\\nB Annotation Details\\n\\nB.1 Details For Automatic Selection\\nB.2 Details For Fine-grained Annotation\\nB.3 Details on Human Refinements\\n\\nC Experimental Detail\\n\\nC.1 Pragmatic Identification and Reasoning (PIR)\\nC.2 Conversational Question Answering (CQA)\\nC.3 Zero-Shot Natural Language Inference\\n\\nD More Detail on DiPlomat\\n\\nE Grice Maxims and Pragmatic Reasoning\\n\\nF Computational Resources\\n\\nG Limitations & Negative Societal Impacts\\n\\nH Ethics Concern\\n\\nI Responsibility & Dataset License\\n\\nJ Datasheets for Our Dataset\\n\\nJ.1 Motivation\\nJ.2 Composition\\nJ.3 Collection Process\\nJ.4 Preprocessing/cleaning/labeling\\nJ.5 Uses\\nJ.6 Distribution\\nJ.7 Maintenance\"}"}
