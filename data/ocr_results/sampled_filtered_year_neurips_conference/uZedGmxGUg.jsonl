{"id": "uZedGmxGUg", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Continuous-time event sequences play a vital role in real-world domains such as healthcare, finance, online shopping, social networks, and so on. To model such data, temporal point processes (TPPs) have emerged as the most natural and competitive models, making a significant impact in both academic and application communities. Despite the emergence of many powerful models in recent years, there hasn\u2019t been a central benchmark for these models and future research endeavors. This lack of standardization impedes researchers and practitioners from comparing methods and reproducing results, potentially slowing down progress in this field.\\n\\nIn this paper, we present EasyTPP, the first central research as sets (e.g., data, models, evaluation programs, documents) in the area of event sequence modeling. Our EasyTPP makes several unique contributions to this area: a unified interface of using existing datasets and adding new datasets; a wide range of evaluation programs that are easy to use and extend as well as facilitate reproducible research; implementations of popular neural TPPs, together with a rich library of modules by composing which one could quickly build complex models.\\n\\nOur benchmark is open-sourced: all the data and implementation can be found at this Github repository. We will actively maintain this benchmark and welcome contributions from other researchers and practitioners. Our benchmark will help promote reproducible research in this field, thus accelerating research progress as well as making more significant real-world impacts.\\n\\n1 Introduction\\n\\nContinuous-time event sequences are ubiquitous in various real-world domains, such as neural spike trains in neuroscience (Williams et al., 2020), orders in financial transactions (Jin et al., 2020), and user page viewing behavior in the e-commerce platform (Hernandez et al., 2017). To model these event sequences, temporal point processes (TPPs) are commonly used, which specify the probability of each event type\u2019s instantaneous occurrence, also known as the intensity function, conditioned on the past event history. Classical TPPs, such as Poisson processes (Daley & Vere-Jones, 2007) and Hawkes processes (Hawkes, 1971), have a well-established mathematical foundation and have been widely used to model traffic (Cram\u00e9r, 1969), finance (Hasbrouck, 1991) and seismology (Ogata, 1988) for several decades. However, the strong parametric assumptions in these models constrain...\"}"}
{"id": "uZedGmxGUg", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In section 3, model architectures for neural TPPs are presented. Following the discussion of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of research papers on TPPs, indicating the growing interest and potential impact of this research area. These methodological developments in TPPs, which have expanded their applicability to various real-world scenarios, are captured in their ability to capture the complexity of real-world phenomena. To overcome the limitations of classical TPPs, many researchers have been developing neural versions of TPPs, which leverage the expressiveness of neural networks to learn complex dependencies; see section 7 for a comprehensive discussion. Since then, numerous advancements have been made in this field, as evidenced by the steadily increasing number of"}
{"id": "uZedGmxGUg", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Event Sequence\\n\\nFigure 1: Drawing an event stream from a neural TPP. The model reads the sequence of past events (polygons) to arrive at a hidden state (blue). That state determines the future \\\"in tensileities\\\" of the two types of events\u2014i.e., their time-varying instantaneous probabilities. The intensity functions are continuous parametric curves (solid lines) determined by the most recent RNN state. In this example, events of type 1 excite type 1 but inhibit type 2. Type 2 excites itself and type 1. Those are immediate effects, shown by the sudden jumps in intensity.\\n\\nFormally, the distribution of a TPP can be characterized by the intensity \\\\( \\\\lambda_k(t|x[0,t]) \\\\geq 0 \\\\) for each event type \\\\( k \\\\) at each time \\\\( t > 0 \\\\) such that:\\n\\n\\\\[\\n\\\\begin{align*}\\n    p_k(t|x[0,t]) &= \\\\lambda_k(t|x[0,t]) dt.\\n\\\\end{align*}\\n\\\\]\\n\\nNeural TPPs.\\n\\nA neural TPP model autoregressively generates events one after another via neural networks. A schematic example is shown in Figure 1 and a detailed description on data samples can be found at our online documentation. For the \\\\( i \\\\)-th event \\\\( (t_i, k_i) \\\\), it computes the embedding of the event \\\\( e_i \\\\in \\\\mathbb{R}^D \\\\) via an embedding layer and the hidden state \\\\( h_i \\\\) gets updated conditioned on \\\\( e_i \\\\) and the previous state \\\\( h_{i-1} \\\\). Then one can draw the next event conditioned on the hidden state \\\\( h_i \\\\):\\n\\n\\\\[\\n\\\\begin{align*}\\n    t_{i+1}, k_{i+1} &\\\\sim P_\\\\theta(t_{i+1}, k_{i+1}|h_i), \\\\\\\\\\n    h_i &= f_{\\\\text{update}}(h_{i-1}, e_i),\\n\\\\end{align*}\\n\\\\]\\n\\nwhere \\\\( f_{\\\\text{update}} \\\\) denotes a recurrent encoder, which could be either RNN (Du et al., 2016; Mei & Eisner, 2017) or more expressive attention-based recursion layer (Zhang et al., 2020; Zuo et al., 2020; Yang et al., 2022). A new line of research models the evolution of the states completely in continuous time:\\n\\n\\\\[\\n\\\\begin{align*}\\n    h_i &= f_{\\\\text{evo}}(h_{i-1}, t_{i-1}, t_i)\\n\\\\end{align*}\\n\\\\]\\n\\nThe state evolution in Equation (2) is generally governed by an ordinary differential equation (ODE) (Rubanova et al., 2019). For a broad and fair comparison, in EasyTPP, we implement not only recurrent TPPs but also an ODE-based continuous-time state model.\\n\\nLearning TPPs.\\n\\nNegative log-likelihood (NLL) is the default training objective for both classical and neural TPPs. The NLL of a TPP given the entire event sequence \\\\( x[0,T] \\\\) is:\\n\\n\\\\[\\n\\\\begin{align*}\\n    \\\\mathcal{L} &= \\\\sum_{i=1}^{T} \\\\log \\\\lambda_k(t_i|x[0,t_i]) - \\\\int_0^T \\\\sum_{k=1}^{K} \\\\lambda_k(t|x[0,t]) dt\\n\\\\end{align*}\\n\\\\]\\n\\nDerivations of this formula can be found in previous work Hawkes (1971); Mei & Eisner (2017).\"}"}
{"id": "uZedGmxGUg", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Data Preprocess.\\nFollowing common practices, we split the set of sequences into the train, validation, and test set with a fixed ratio. To feed the sequences of varying lengths into the model, in EasyTPP, we pad all sequences to the same length, then use the \\\"sequence_mask\\\" tensor to identify which event tokens are padding. As we implemented several variants of attention-based TPPs, we also generated the \\\"attention_mask\\\" to mask all the future positions at each event to avoid \\\"peeking into the future\\\".\\n\\nModel Implementation.\\nOur EasyTPP library provides a suite of modules, and one could easily build complex models by composing these modules. Specifically, we implemented the models (see section 5.1) evaluated in this paper with our suite of modules (e.g., continuous-time LSTM, continuous-time attention). Moreover, some modules are model-agnostic methods for training and inference, which will further speed up the development speed of future methodology research. Below are two signature examples:\\n\\n- `compute_loglikelihood` (function), which calculates log-likelihood of a model given data. It is non-trivial to correctly implement it due to the integral term of log-likelihood in Equation (4), and we have found errors in popular implementations.\\n- `EventSampler` (class), which draws events from a given point process via the thinning algorithm. The thinning algorithm is commonly used in inference but it is non-trivial to implement (and rare to see) an efficient and batched version. Our efficient and batched version (which we took great efforts to implement) will be useful for nearly all intensity-based event sequence models.\\n\\nTraining.\\nWe can estimate the model parameters by locally maximizing the NLL in Equation (4) with any stochastic gradient method. Note that computing the NLL can be challenging due to the presence of the integral in the second term in Equation (4). In EasyTPP, by default, we approximate the integral by Monte-Carlo estimation to compute the overall NLL (see Appendix C.1). Nonetheless, EasyTPP also incorporates some neural TPPs (e.g., the intensity-free model (Shchur et al., 2020)), which allow us to compute the NLL analytically, which is more computationally efficient.\\n\\nSampling.\\nGiven the learned parameters, we apply the minimum Bayes risk (MBR) principle to predict the time and type with the lowest expected loss. A recipe can be found in Appendix C.2. Note that other methods exist for predicting a TPP, such as adding an MLP layer to directly output the time and type prediction (Zuo et al., 2020; Zhang et al., 2020). However, as we aim to build a generative model of event sequences, we believe the principal way to make predictions based on continuous-time generative model is thinning algorithm (Ogata, 1988). In EasyTPP, a batch-wise thinning algorithm is consistently used when evaluating the predictive performance of TPPs.\\n\\nHyperparameter Tuning.\\nMost studies specified the detailed hyper-parameters of their models in the papers. However, with the modified code fitted in the EasyTPP framework or the new splits of datasets, it may be inappropriate to use the same hyper-parameters. Besides the classical grid search method, we also integrate Optuna (Akiba et al., 2019) in our framework to automatically search optimal hyperparameters and prune unpromising trials for faster results. We hope that the definition of our open benchmarking pipeline could provide guidance for fair comparisons and reproducible works in TPPs.\\n\\n4 EasyTPP's Software Interface\\nHigh Level Software Architecture.\\nThe purpose of building EasyTPP is to provide a simple and standardized framework to allow users to apply different state-of-the-art (SOTA) TPPs to arbitrary data sets. For researchers, EasyTPP provides an implementation interface to integrate new recourse methods in an easy-to-use way, which allows them to compare their method to already existing methods. For industrial practitioners, the availability of benchmarking code helps them easily assess the applicability of TPP models for their own problems.\\n\\nHigh-level visualization of the EasyTPP's software architecture is depicted in Figure 9.\"}"}
{"id": "uZedGmxGUg", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 4: Architecture of the EasyTPP library. The dashed arrows show the different implementation possibilities, either to use pre-defined SOTA TPP models or provide a custom implementation. All dependencies between the configurations and modules are visualized by solid arrows with additional descriptions. Overall, the running of the pipeline is parameterized by the configuration classes - RunnerConfig (w/o hyper tuning) and HPOConfig (with hyper tuning).\\n\\nWhy and How Does EasyTPP Support Both PyTorch and TensorFlow?\\n\\nPyTorch and TensorFlow are the two most popular Deep Learning (DL) frameworks today. PyTorch has a reputation for being a research-focused framework, and indeed, most of the authors have implemented TPPs in PyTorch, which are used as references by EasyTPP. On the other hand, TensorFlow has been widely used in real-world applications. For example, Microsoft recommender, NVIDIA Merlin, and Alibaba EasyRec are well-known industrial user modeling systems with TensorFlow as the backend. In recent works, TPPs have been introduced to better capture the evolution of the user preference in continuous-time (Bao & Zhang, 2021; Fan et al., 2021; Bai et al., 2019). To support the use of TPPs by industrial practitioners, we implement an equivalent set of TPPs in TensorFlow. As a result, EasyTPP not only helps researchers analyze the strengths and bottlenecks of existing models, but also facilitates the deployment of TPPs in industrial applications.\\n\\nSee Appendix B for more details on the interface and examples of different user cases.\\n\\n5 Experimental Evaluation\\n\\n5.1 Experimental Setup\\n\\nWe comprehensively evaluate 9 models in our benchmark, which include the classical Multivariate Hawkes Process (MHP) and 8 widely-cited state-of-the-art neural models:\\n\\n- Two RNN-based models: Recurrent marked temporal point process (RMTPP) (Du et al., 2016) and neural Hawkes Process (NHP) (Mei & Eisner, 2017).\\n\\n2 https://github.com/microsoft/recommenders.\\n3 https://developer.nvidia.com/nvidia-merlin.\\n4 https://github.com/alibaba/EasyRec.\\n5\"}"}
{"id": "uZedGmxGUg", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Three attention-based models: \\n\\n- self-attentive Hawkes process (SAHP) \\\\(^{(Zhang \\\\text{ et al., } 2020)}\\\\) \\n- trans-former Hawkes process (THP) \\\\(^{(Zuo \\\\text{ et al., } 2020)}\\\\) \\n- attentive neural Hawkes process (AttNHP) \\\\(^{(Yang \\\\text{ et al., } 2022)}\\\\) \\n\\nOne TPP with the fully neural network based intensity: FullyNN \\\\(^{(Omi \\\\text{ et al., } 2019)}\\\\). \\n\\nOne intensity-free model IFTP \\\\(^{(Shchur \\\\text{ et al., } 2020)}\\\\). \\n\\nOne TPP with the hidden state evolution governed by a neural ODE: ODETPP. It is a simplified version of the TPP proposed by Chen et al. \\\\(^{(2021)}\\\\) by removing the spatial component.\\n\\nWe conduct experiments on synthetic and real-world datasets from popular works that contain diverse characteristics in terms of their application domains and temporal statistics (see Table 2):\\n\\n- **Synthetic.** This dataset contains synthetic event sequences from a univariate Hawkes process sampled using Tick \\\\(^{(Bacry \\\\text{ et al., } 2017)}\\\\) whose conditional intensity function is defined by:\\n  \\\\[\\n  \\\\lambda(t) = \\\\mu + \\\\sum_{t_i < t} \\\\alpha \\\\cdot \\\\beta \\\\cdot \\\\exp(-\\\\beta(t - t_i))\\n  \\\\]\\n  with \\\\(\\\\mu = 0\\\\), \\\\(\\\\alpha = 0.8\\\\), \\\\(\\\\beta = 1.0\\\\). We randomly sampled disjoint train, dev, and test sets with 1200, 200, and 400 sequences.\\n\\n- **Amazon** \\\\(^{(Ni, 2018)}\\\\). This dataset includes time-stamped user product reviews behavior from January, 2008 to October, 2018. Each user has a sequence of produce review events with each event containing the timestamp and category of the reviewed product, with each category corresponding to an event type. We work on a subset of 5200 most active users with an average sequence length of 70 and then end up with \\\\(K = 16\\\\) event types.\\n\\n- **Retweet** \\\\(^{(Ke \\\\text{ Zhou \\\\\\\\ & Song.}, 2013)}\\\\). This dataset contains time-stamped user retweet event sequences. The events are categorized into \\\\(K = 3\\\\) types: retweets by \\\"small,\\\" \\\"medium\\\" and \\\"large\\\" users. Small users have fewer than 120 followers, medium users have fewer than 1363, and the rest are large users. We work on a subset of 5200 active users with an average sequence length of 70.\\n\\n- **Taxi** \\\\(^{(Whong, 2014)}\\\\). This dataset tracks the time-stamped taxi pick-up and drop-off events across the five boroughs of the New York City; each (borough, pick-up or drop-off) combination defines an event type, so there are \\\\(K = 10\\\\) event types in total. We work on a randomly sampled subset of 2000 drivers with an average sequence length of 39.\\n\\n- **Taobao** \\\\(^{(Xue \\\\text{ et al., } 2022)}\\\\). This dataset contains time-stamped user click behaviors on Taobao shopping pages from November 25 to December 03, 2017. Each user has a sequence of item click events with each event containing the timestamp and the category of the item. The categories of all items are first ranked by frequencies and the top 19 are kept while the rest are merged into one category, with each category corresponding to an event type. We work on a subset of 4800 most active users with an average sequence length of 150 and then end up with \\\\(K = 20\\\\) event types.\\n\\n- **StackOverflow** \\\\(^{(Leskovec \\\\\\\\ & Krevl, 2014)}\\\\). This dataset has two years of user awards on a question-answering website: each user received a sequence of badges and there are \\\\(K = 22\\\\) different kinds of badges in total. We work on a subset of 2200 active users with an average sequence length of 65.\\n\\nAll preprocessed datasets are available at Google Drive.\\n\\n**Evaluation Protocol.** We keep the model architectures as the original implementations in their papers. For a fair comparison, we use the same training procedure for all the models: we used the same optimizer (Adam \\\\(^{(Kingma \\\\& Ba, 2015)}\\\\) with default parameters), biases initialized with zeros, no learning rate decay, the same maximum number of training epochs, and early stopping criterion (based on log-likelihood on the held-out dev set) for all models.\\n\\nWe mainly examine the models in two standard scenarios:\\n\\n- **Goodness-of-fit:** we fit the models on the train set and measure the log-probability they assign to the held-out data.\\n- **Next-event prediction:** we use the minimum Bayes risk (MBR) principle to predict the next event time given only the preceding events, as well as its type given both its true time and the preceding events. We evaluate the time and type prediction by RMSE and error rate, respectively.\"}"}
{"id": "uZedGmxGUg", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Performance of all the methods on the goodness-of-fit task on synthetic Hawkes, Retweet, and Taxi data. A higher score is better. All methods are implemented in PyTorch.\\n\\nIn addition, we propose a new evaluation task: the long-horizon prediction. Given the prefix of each held-out sequence \\\\( x_{[0,T]} \\\\), we autoregressively predict the next events in a future horizon \\\\( \\\\hat{x}_{(T,T')] \\\\). It is evaluated by measuring the optimal transport distance (OTD), a type of edit distance for event sequences (Mei et al., 2019), between the prediction \\\\( \\\\hat{x}_{(T,T']} \\\\) and ground truth \\\\( x_{(T,T']} \\\\). As pointed out by Xue et al. (2022), long-horizon prediction of event sequences is essential in various real-world domains, and this task provides new insight into the predictive performance of the models.\\n\\nIt is worth noting that FullyNN, faithfully implemented based on the author's version, does not support multi-type event sequences. Therefore, it is excluded from the type prediction task.\\n\\n5.2 Results and Analysis\\n\\nMain Results on Goodness-of-Fit and Next-Event Prediction.\\n\\n\u2022 Figure 5 reports the log-likelihood on three held-out datasets for all the methods. We find IFTPP outperforms all the competitors because it evaluates the log-likelihood in a close form while the others (RMTPP, NHP, THP, AttNHP, ODETPP) compute the intensity function via Monte Carlo integration, causing numerical approximation errors. FullyNN method, which also exactly computes the log-likelihood, has worse fitness than other neural competitors. As Shchur et al. (2020) points out, the PDF of FullyNN does not integrate to 1 due to a suboptimal choice of the network architecture, therefore causing a negative impact on the performance.\\n\\n\u2022 Figure 6 reports the time and type prediction results on three real datasets. We find there is no single winner against all the other methods. Attention-based methods (SAHP, THP, AttNHP) generally perform better than or close to non-attention methods (RMTPP, NHP, ODETPP, FullyNN and IFTPP) on Amazon, Taobao, and Stackoverflow, while NHP is the winner on both Retweet and Taxi. We see that NHP is a comparably strong baseline with attention-based TPPs. This is not too surprising because similar results have been reported in previous studies (Yang et al., 2022).\\n\\n\u2022 Not surprisingly, the performance of the classic model MHP is worse than the neural models across most of the evaluation tasks, consistent with the previous findings that neural TPPs have demonstrated to be more effective than classical counter parts at fitting data and making predictions.\\n\\nPlease see Appendix E.3 for the complete results (in numbers) on all the datasets. With a growing number of TPP methods proposed, we will continuously expand the catalog of models and datasets and actively update the benchmark in our Github repository.\\n\\nAnalysis-I: Long Horizon Prediction.\\n\\nWe evaluate the long horizon prediction task on Retweet and Taxi datasets. On both datasets, we set the prediction horizon to be the one that approximately has 5 and 10 events, respectively. Shown in Figure 7 and Figure 8, we find that AttNHP and THP are two co-winners on Retweet and THP is a single winner on Taxi. Nonetheless, the margin of the winner over the competitors is small. The exact numbers shown in these two figures could be found in Table 5 in Appendix E.3. Due to the fact that these models are autoregressive and locally normalized,\"}"}
{"id": "uZedGmxGUg", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset       | Time RMSE | Type Error Rate |\\n|--------------|-----------|-----------------|\\n| Amazon       | 0.61      | 64.0            |\\n| Retweet      | 0.36      | 8.0             |\\n| Taxi         | 0.52      | 52.0            |\\n| Taobao       | 1.36      | 54.0            |\\n| StackOverflow| 64.0      | 38.0            |\\n\\nAs clarified, FullyNN is not applicable for the type prediction tasks.\\n\\nTo fix this issue, one could resort to globally normalized models (Xue et al., 2022), which is out of the scope of the paper.\\n\\nAnalysis-II: Models with Different Frameworks: PyTorch vs. TensorFlow. Researchers normally implement their experiments and models for specific ML frameworks. For example, recently proposed methods are mostly restricted to PyTorch and are not applicable to TensorFlow models. As explained in Section 4, to facilitate the use of TPPs, we implement two equivalent sets of methods in PyTorch and TensorFlow. Table 1 shows the relative difference between the results of Torch and TensorFlow implementations are all within [\u22121.5%, 1.5%]. To conclude, although the code could not be exactly the same, the two sets of models produce similar performance in terms of predictive ability.\"}"}
{"id": "uZedGmxGUg", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Long horizon prediction on Retweet data: left (avg prediction horizon 5 events) vs. right (avg prediction horizon 10 events).\\n\\nFigure 8: Long horizon prediction on Taxi data: left (avg prediction horizon 5 events) vs. right (avg prediction horizon 10 events).\\n\\nTable 1: Relative difference between Torch and TensorFlow implementations of methods in Figure 6.\\n\\nFuture Research Opportunities\\n\\nWe summarize our thoughts on future research opportunities inspired by our benchmarking results. Most importantly, the results seem to be signaling that we should think beyond architectural design. For the past decade, this area has been focusing on developing new architectures, but the performance of new models on the standard datasets seem to be saturating. Notably, all the best-to-date models make poor predictions on time of future events. Moreover, on type prediction, attention-based models only outperform other architectures by a small margin. Looking into the future, we advocate for a few new research directions that may bring significant contributions to the field. The first is to build foundation models for event sequence modeling. The previous model-building work all learns data-specific weights, and does not test the transferring capabilities of the learned models. Inspired by the emergence of foundation models in other research areas, we think it will be beneficial to explore the possibility to build foundation models for event sequences. Conceptually, learning from a large corpus of diverse datasets\u2014like how GPTs (Nakano et al., 2021) learn by reading open web text\u2014has great potential to improve the model performance and generalization beyond what could be achieved in the current in-domain data learning paradigm. Our library can facilitate exploration in this direction since we unify the data formats and provide an easy-to-use interface that users can seamlessly plug and play any set of datasets. Challenges in this direction arise as different datasets tend to have disjoint sets of event types and different scales of time units.\"}"}
{"id": "uZedGmxGUg", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The second is to go beyond event data itself and utilize external information sources to enhance event sequence modeling. Seeing the performance saturation of the models, we are inspired to think whether the performance has been bounded by the intrinsic signal-to-noise ratio of the event sequence data. Therefore, it seems natural and beneficial to explore the utilization of other information sources, which include but are not limited to: (i) sensor data such as satellite images and radionavigation signals; (ii) structured and unstructured knowledge bases (e.g., databases, Wikipedia, text books); (iii) large pre-trained models such as ChatGPT (Brown et al., 2020) and GPT-4 (OpenAI, 2023), whose rich knowledge and strong reasoning capabilities may assist event sequence models in improving their prediction accuracies.\\n\\nThe third is to go beyond observational data and embed event sequence models into real-world interactions (Qu et al., 2023). With interactive feedback from the real world, an event sequence model would have the potential to learn real causal dynamics of the world, which may significantly improve prediction accuracy. All the aforementioned directions open up research opportunities for technical innovations.\\n\\nRelated work\\n\\nTemporal Point Processes. Over recent years, a large variety of TPP models have been proposed, many of which are built on recurrent neural networks (Du et al., 2016; Mei & Eisner, 2017; Xiao et al., 2017; Omi et al., 2019; Shchur et al., 2020; Mei et al., 2020; Boyd et al., 2020). Models of this kind enjoy continuous state spaces and flexible transition functions, thus achieving superior performance on many real-world datasets, compared to the classical Hawkes process (Hawkes, 1971). To properly capture the long-range dependency in the sequence, the attention and transformer techniques (Vaswani et al., 2017) have been adapted to TPPs (Zuo et al., 2020; Zhang et al., 2020; Yang et al., 2022; Wen et al., 2023) and makes further improvements on predictive performance. Despite significant progress made in academia, the existing studies usually perform model evaluations and comparisons in an ad-hoc manner, e.g., by using different experimental settings or different ML frameworks. Such conventions not only increase the difficulty in reproducing these methods but also may lead to inconsistent experimental results among them.\\n\\nOpen Benchmarking on TPPs. The significant attention attracted by TPPs in recent years naturally leads to a high demand for an open benchmark to fairly compare against baseline models. While many efforts have been made in the domains of recommender systems (Zhu et al., 2021), computer vision (Deng et al., 2009), and natural language processing (Wang et al., 2019), benchmarking in the field of TPPs is an under-explored topic.\\n\\nTick (Bacry et al., 2017) and pyhawkes are two well-known libraries that focus on statistical learning for classical TPPs, which are not suitable for the state-of-the-art neural models. Poppy (Xu, 2018) is a PyTorch-based toolbox for neural TPPs, but it has not been actively maintained since three years ago and has not implemented any recent state-of-the-art methods. To the best of our knowledge, EasyTPP is the first package that provides open benchmarking for the popular neural TPPs.\\n\\nConclusion\\n\\nIn this work, we presented EasyTPP, a versatile benchmarking platform for the standardized and transparent comparison of TPP methods on different integrated data sets. With a growing open-source community, EasyTPP has the potential to become the main library for benchmarking TPPs. The community seems to really appreciate this initiative: without any advertising, our library has collected around 90 stars on Github and has been downloaded around 700 times from PyPi since it was released 3 months ago. We hope that this work continuously contributes to further advances in the research.\"}"}
{"id": "uZedGmxGUg", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al. Tensorflow: a system for large-scale machine learning. In OSDI'16, volume 16, pp. 265\u2013283, 2016.\\n\\nAkiba, T., Sano, S., Yanase, T., Ohta, T., and Koyama, M. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2019.\\n\\nBacry, E., Bompaire, M., Ga\u00efffas, S., and Poulsen, S. tick: a Python library for statistical learning, with a particular emphasis on time-dependent modeling. ArXiv e-prints, 2017.\\n\\nBai, T., Zou, L., Zhao, W. X., Du, P., Liu, W., Nie, J.-Y., and Wen, J.-R. Ctrec: A long-short demands evolution model for continuous-time recommendation. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR'19, pp. 675\u2013684, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450361729.\\n\\nBao, J. and Zhang, Y. Time-aware recommender system via continuous-time modeling. In Proceedings of the 30th ACM International Conference on Information Knowledge Management, CIKM'21, pp. 2872\u20132876, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450384469.\\n\\nBoyd, A., Bamler, R., Mandt, S., and Smyth, P. User-dependent neural sequence models for continuous-time event data. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\nChen, R. T., Amos, B., and Nickel, M. Neural spatio-temporal point processes. ICLR, 2021.\\n\\nCram\u00e9r, H. Historical review of Filip Lundberg's works on risk theory. Scandinavian Actuarial Journal, 1969(sup3):6\u201312, 1969.\\n\\nDaley, D. J. and Vere-Jones, D. An Introduction to the Theory of Point Processes, Volume II: General Theory and Structure. Springer, 2007.\\n\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248\u2013255, 2009.\\n\\nDu, N., Dai, H., Trivedi, R., Upadhyay, U., Gomez-Rodr\u00edguez, M., and Song, L. Recurrent marked temporal point processes: Embedding event history to vector. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016.\\n\\nFan, Z., Liu, Z., Zhang, J., Xiong, Y., Zheng, L., and Yu, P. S. Continuous-time sequential recommendation with temporal graph collaborative transformer. In Proceedings of the 30th ACM International Conference on Information Knowledge Management, CIKM '21, pp. 433\u2013442, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450384469.\\n\\nHasbrouck, J. Measuring the information content of stock trades. The Journal of Finance, 46(1):179\u2013207, 1991.\\n\\nHawkes, A. G. Spectra of some self-exciting and mutually exciting point processes. Biometrika, 1971.\"}"}
{"id": "uZedGmxGUg", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hernandez, S., \u00c1lvarez, P., Fabra, J., and Ezpeleta, J. Analysis of users' behavior in structured e-commerce websites. IEEE Access, 5:11941\u201311958, 2017.\\n\\nJin, Z., Guo, S., Chen, N., Weiskopf, D., Gotz, D., and Cao, N. Visual causality analysis of event sequence data. IEEE Transactions on Visualization and Computer Graphics, 27(2):1343\u20131352, 2020.\\n\\nKe Zhou, H. Z. and Song., L. Learning triggering kernels for multi-dimensional hawkes processes. In Proceedings of the International Conference on Machine Learning (ICML), 2013.\\n\\nKingma, D. and Ba, J. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations (ICLR), 2015.\\n\\nLeskovec, J. and Krevl, A. SNAP Datasets: Stanford large network dataset collection, 2014.\\n\\nLewis, P. A. and Shedler, G. S. Simulation of nonhomogeneous Poisson processes by thinning. Naval Research Logistics Quarterly, 1979.\\n\\nLiniger, T. J. Multivariate Hawkes processes. Diss., Eidgen\u00f6ssische Technische Hochschule ETH Z\u00fcrich, Nr. 18403, 2009.\\n\\nMei, H. and Eisner, J. The neural Hawkes process: A neurally self-modulating multivariate point process. In Advances in Neural Information Processing Systems (NeurIPS), 2017.\\n\\nMei, H., Qin, G., and Eisner, J. Imputing missing events in continuous-time event streams. In Proceedings of the International Conference on Machine Learning (ICML), 2019.\\n\\nMei, H., Qin, G., Xu, M., and Eisner, J. Neural Datalog through time: Informed temporal modeling via logical specification. In Proceedings of the International Conference on Machine Learning (ICML), 2020.\\n\\nNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\\n\\nNi, J. Amazon review data, 2018.\\n\\nOgata, Y. Statistical models for earthquake occurrences and residual analysis for point processes. J. Am. Stat. Assoc., 83(401):9\u201327, 1988.\\n\\nOmi, T., Ueda, N., and Aihara, K. Fully neural network based model for general temporal point processes. In Advances in Neural Information Processing Systems (NeurIPS), 2019.\\n\\nOpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\\n\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019.\\n\\nQu, C., Tan, X., Xue, S., Shi, X., Zhang, J., and Mei, H. Bellman meets hawkes: Model-based reinforcement learning via temporal point processes. In Proceedings of the AAAI Conference on Artificial Intelligence, 2023.\\n\\nRubanova, Y., Chen, R. T., and Duvenaud, D. K. Latent ordinary differential equations for irregularly-sampled time series. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019.\\n\\nShchur, O., Bilo\u0161, M., and G\u00fcnnemann, S. Intensity-free learning of temporal point processes. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.\"}"}
{"id": "uZedGmxGUg", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017.\\n\\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A multi-task benchmark and analysis platform for natural language understanding. ICLR, 2019.\\n\\nWen, Q., Zhou, T., Zhang, C., Chen, W., Ma, Z., Yan, J., and Sun, L. Transformers in time series: A survey. In International Joint Conference on Artificial Intelligence (IJCAI), 2023.\\n\\nWhong, C. FOILing NYC\u2019s taxi trip data, 2014.\\n\\nWilliams, A., Degleris, A., Wang, Y., and Linderman, S. Point process models for sequence detection in high-dimensional neural spike trains. Advances in neural information processing systems, 33:14350\u201314361, 2020.\\n\\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38\u201345, Online, October 2020. Association for Computational Linguistics.\\n\\nXiao, S., Yan, J., Yang, X., Zha, H., and Chu, S. Modeling the intensity function of point process via recurrent neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, 2017.\\n\\nXu, H. Poppy: A point process toolbox based on pytorch. arXiv preprint arXiv:1810.10122, 2018.\\n\\nXue, S., Shi, X., Zhang, Y. J., and Mei, H. Hypro: A hybridly normalized probabilistic model for long-horizon prediction of event sequences. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n\\nYang, C., Mei, H., and Eisner, J. Transformer embeddings of irregularly spaced events and their participants. In Proceedings of the International Conference on Learning Representations (ICLR), 2022.\\n\\nZhang, Q., Lipani, A., Kirnap, O., and Yilmaz, E. Self-attentive Hawkes process. In Proceedings of the International Conference on Machine Learning (ICML), 2020.\\n\\nZhu, J., Liu, J., Yang, S., Zhang, Q., and He, X. Open benchmarking for click-through rate prediction. In Demartini, G., Zuccon, G., Culpepper, J. S., Huang, Z., and Tong, H. (eds.), CIKM \u201921: The 30th ACM International Conference on Information and Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021, pp. 2759\u20132769. ACM, 2021.\\n\\nZuo, S., Jiang, H., Li, Z., Zhao, T., and Zha, H. Transformer Hawkes process. In International Conference on Machine Learning, pp. 11692\u201311702. PMLR, 2020.\"}"}
