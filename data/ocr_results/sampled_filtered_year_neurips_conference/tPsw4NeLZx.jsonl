{"id": "tPsw4NeLZx", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MM-WLAuslan: Multi-View Multi-Modal Word-Level Australian Sign Language Recognition Dataset\\n\\nXin Shen Heming Du Hongwei Sheng Shuyun Wang Hui Chen\\n\\nHuiqiang Chen\\n\\nZhuojie Wu Xiaobiao Du\\n\\nJiaying Ying Ruihan Lu Qingzheng Xu Xin Yu\\n\\nThe University of Queensland\\n\\nx.shen3@uqconnect.edu.au\\n\\nAbstract\\n\\nIsolated Sign Language Recognition (ISLR) focuses on identifying individual sign language signs. Considering the diversity of sign languages across geographic regions, developing region-specific ISLR datasets is crucial for supporting communication and research. Auslan, as a sign language specific to Australia, still lacks a dedicated large-scale word-level dataset for the ISLR task. To fill this gap, we curate the first large-scale Multi-view Multi-modal Word-Level Australian Sign Language recognition dataset, dubbed MM-WLAuslan. Compared to other publicly available datasets, MM-WLAuslan exhibits three significant advantages: (1) the largest amount of data, (2) the most extensive vocabulary, and (3) the most diverse of multi-modal camera views. Specifically, we record 282K+ sign videos covering 3,215 commonly used Auslan glosses presented by 73 signers in a studio environment. Moreover, our filming system includes two different types of cameras, i.e., three Kinect-V2 cameras and a RealSense camera. We position cameras hemispherically around the front half of the model and simultaneously record videos using all four cameras. Furthermore, we benchmark results with state-of-the-art methods for various multi-modal ISLR settings on MM-WLAuslan, including multi-view, cross-camera, and cross-view. Experiment results indicate that MM-WLAuslan is a challenging ISLR dataset, and we hope this dataset will contribute to the development of Auslan and the advancement of sign languages worldwide. All datasets and benchmarks are available at /gtb/MM-WLAuslan.\\n\\n* Work done while visiting the University of Queensland.\\n\\n\u2020 Corresponding author.\"}"}
{"id": "tPsw4NeLZx", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1 Introduction\\n\\nSign language (SL) is the primary mode of communication for many deaf or hard-of-hearing individuals. Each sign language possesses its own vocabulary and grammatical rules, akin to spoken languages [1, 2, 3]. Notably, even within regions that share a commonly spoken language, such as the United States, Australia, and the United Kingdom, distinct native sign languages are prevalent.\\n\\nTo facilitate communication between the deaf and hearing communities, Isolated Sign Language Recognition (ISLR) is highlighted as a fundamental sign language understanding task. ISLR aims to recognize an individual sign gloss, which is a written representation of signs using words from a spoken language, into a corresponding word or phrase in spoken languages [4, 5].\\n\\nWith emerging deep learning techniques [6, 7, 8, 9] and large-scale sign language datasets [4, 10, 11, 12, 13], ISLR achieves promising progress recently [14, 4, 15]. As shown in Table 1, researchers from various countries construct word-level sign language datasets and thus promote the development of ISLR in the respective sign languages, such as American Sign Language (ASL) [4, 16, 17, 18, 19, 10], British Sign Language (BSL) [20, 21], Chinese Sign Language (CSL) [22, 23] and German Sign Language (DGS) [12, 24]. Meanwhile, according to the Australian Federal Department of Health and Aged Care (DHAC)[3], as of 14 May 2024, one in six Australians, over 3.6 million people, had hearing loss affecting them, and the number is expected to reach 7.8 million people by 2060. However, to the best of our knowledge, there is no publicly available large-scale Auslan dataset for ISLR. Due to the regional nature of sign languages and the societal commitment to supporting individuals with hearing impairments, word-level Australian Sign Language (Auslan) datasets are inevitably and urgently needed in order to investigate automatic recognition.\\n\\nMoreover, the volume of data, the breadth of data categories, and the diversity of data modalities are three critical points that influence the fundamental quality of an ISLR dataset. The larger the volume, the wider the range of categories, and the richer the modalities of data mean the higher the value of the dataset for scientific research and practical applications, such as sign language education [25] and dictionary [26]. Specifically, a large volume of data and an extensive gloss dictionary within the dataset enhance the robustness and capability of the sign recognition system. Additionally, the captured multi-view sign data and depth information improve the accuracy of recognizing complex hand movements and reduce the issues caused by occlusion and single-view ambiguities. However, most existing publicly available ISLR corpora either contain the limited gloss videos and vocabulary [5, 16, 27, 12, 13, 24, 28] or are only captured in a single viewpoint without depth information [4, 10, 19, 18, 20].\\n\\nIn this work, we record the first word-level Auslan recognition dataset, named MM-WLAuslan, that contains the largest number of data samples, the most extensive vocabulary, and the most diverse multi-modal camera views compared to other publicly available datasets, as shown in Table 1. Specifically, we select 3,215 commonly used glosses that contain a sufficient variety of classes and training instances for a practical word-level Auslan recognition model. We collect the glosses from Auslan SignBank[4], the most authoritative Auslan dictionary in Australia. We ask Auslan experts to help select glosses that are widely used throughout Australia, including fingerspelling glosses, such as \\\"TV\\\" and \\\"NEWS\\\". The collected glosses correspond to over 7,900 English words or phrases, covering most of the vocabulary commonly used in daily life. We invite sign language experts, deaf individuals, and volunteers to participate in the recording process. After 2,500+ hours of preparation and recording, we capture over 282K+ high-quality isolated Auslan gloss videos with the assistance of 73 signers. Each video recording is supervised by at least one Auslan expert to ensure the precision of the sign language expression.\\n\\nTo record multi-view, multi-modal, and high-quality isolated Auslan gloss videos, we set up a recording studio equipped with a green screen backdrop. We position two different types of RGB-D cameras, i.e., three Kinect-V2 cameras and a RealSense camera, hemispherically around the front half of the model. As shown in Figure 1, we place the cameras to the left-front, front, and right-front of the subject and simultaneously record videos. Unlike the previous dataset [24] that only provides depth video from the front view, we record both RGB-D videos from every camera.\"}"}
{"id": "tPsw4NeLZx", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison between MM-WLAuslan and existing ISLR datasets.\\n\\n| Dataset          | Country | Signs | Signers | Videos | Ave. Videos/Sign | Cross-Cam Depth | Source |\\n|------------------|---------|-------|---------|--------|------------------|-----------------|--------|\\n| Purdue RVL-SLLL  | USA     | 39    | 14      | 0.5K   | 14               | !               | Studio |\\n| RWTH-BOSTON      | USA     | 50    | 3       | 0.5K   | 9.66             | !               | Studio |\\n| ASLLVD           | USA     | 3,000 | 6       | 9.8K   | 3.27             | !               | Studio |\\n| WLASL            | USA     | 2,000 | 119     | 21.1K  | 10.54            | !               | Web    |\\n| MS-ASL           | USA     | 1,000 | 222     | 25.5K  | 25.51            | !               | Web    |\\n| ASL Citizen      | USA     | 2,731 | 52      | 83.9K  | 30.73            | !               | Webcam |\\n| PopSign ASL v1.0 | USA     | 250   | 47      | 214.3K | 857.30           | !               | Smartphone |\\n| BSL-1K           | GBR     | 1,064 | 40      | 273.0K | 257              | !               | Web    |\\n| DEVISIGN-L       | CHN     | 2,000 | 8       | 24.0K  | 12.00            | !               | Studio |\\n| CSL 500          | CHN     | 500   | 50      | 125.0K | 250.00           | !               | Studio |\\n| DGS Kinect 40    | DEU     | 40    | 14      | 2.8K   | 70.00            | !               | Studio |\\n| SMILE            | DEU/CHE | 100   | 30      | -      | -                | !               | !      |\\n| GSL 982          | GRC     | 982   | 1       | 4.9K   | 5.00             | !               | Studio |\\n| INCLUDE          | ISR     | 263   | 7       | 4.3K   | 16.30            | !               | Studio |\\n| KL-MV2DSL        | ISR     | 200   | -       | 5.0K   | 25               | !               | Studio |\\n| LSA64            | ARG     | 64    | 10      | 3.2K   | 50.00            | !               | Studio |\\n| LSE-Sign         | ESP     | 2,400 | 2       | 2.4K   | 1.00             | !               | Studio |\\n| LSFB-ISOL        | FRA/BEL | 395   | 100     | 47.6K  | 120.38           | !               | Studio |\\n| BosphorusSign22K | TUR     | 744   | 6       | 22.5K  | 30.30            | !               | Studio |\\n| AUTSL            | TUR     | 226   | 43      | 38.3K  | 169.63           | !               | Studio |\\n| Auslan-Daily     | AUS     | 600   | 21      | 3.0K   | 5.00             | !               | Web    |\\n| MM-WLAuslan      | AUS     | 3,215 | 73      | 282.9K | 88.00            | !               | Studio |\\n\\nFurthermore, for an unbiased performance evaluation of ISLR systems, we involve nearly 20 signers in the test set who are not exposed to the training and validation sets. Concurrently, we split the test set into four distinct subsets to mimic the various scenarios in the real world. Videos in three subsets are designed to incorporate diverse backgrounds or potential temporal disturbances. After obtaining the realistic test sets, we utilize the collected multi-modal, multi-view, and multi-camera videos to benchmark various multi-modal ISLR settings. Extensive experiments demonstrate the limitations of current state-of-the-art (SOTA) methods when these methods are applied across various cameras and views. This manifests the potential of MM-WLAuslan to advance the future research and development of ISLR systems. Overall, our major contributions are summarized as follows:\\n\\n\u2022 We construct the first word-level Australian ISLR dataset, dubbed MM-WLAuslan. MM-WLAuslan consists of the largest number of gloss videos and the most extensive vocabulary.\\n\u2022 We provide the most diverse multi-modal camera views and enable the investigation of a variety of multi-modal ISLR settings, including multi-view, cross-camera and cross-view.\\n\u2022 We establish a leaderboard and an evaluation benchmark to promote future Australian ISLR research and development of applications.\\n\\n2 Related Work\\n\\n2.1 Isolated Sign Language Recognition Datasets\\n\\nAs shown in Table 1, several datasets are developed to facilitate research and application development of ISLR. However, most datasets have limitations in gloss dictionary size, depth information, and recording perspectives. For example, Purdue RVL-SLLL dataset [16] exhibits methodological rigor in a laboratory setting, but its applicability for sign language recognition is limited because it only covers 39 signs. Furthermore, despite ASLLVD dataset [17] including a large lexicon of 3,000 glosses, it is limited by its single perspective and lack of depth information, crucial for capturing the three-dimensional motion of sign language. WLASL [4] and MS-ASL [10] datasets expand on the number of signs and signers but still restrict their recordings to single-view videos without depth, missing critical spatial dynamics essential for accurate sign interpretation. In contrast, datasets like CSL 500 [11] and DGS Kinect 40 [12] include depth information but cover only a small number of glosses, limiting their usefulness for extensive sign language applications.\\n\\nDifferent from all of the above datasets, the proposed MM-WLAuslan dataset is a comprehensive ISLR dataset. It encompasses 3,215 signs from 73 signers, with each sign captured from four distinct viewpoints along with depth information, significantly enhancing the diversity and utility of the dataset. Moreover, MM-WLAuslan is currently the largest sign language recognition dataset in Australia, with extensive lexicon and high-quality data.\"}"}
{"id": "tPsw4NeLZx", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.2 Isolated Sign Language Recognition Methods\\n\\nISLR aims to identify the gloss labels of short-term videos. Previous research can be categorized into three types based on the input modality: pixel-based, pose-based and multi-modal-based approaches.\\n\\n**Pixel-based ISLR:**\\nSignificant advances in CNN-based action recognition inspire the development of pixel-based ISLR models. Early efforts [36, 37, 38] utilize convolutional neural networks (CNN) to extract frame-wise features, which are then temporally encoded using recurrent neural networks to capture time-series information. Meanwhile, 3D CNNs, such as C3D model [39, 40, 41] and I3D model [6], are commonly used in ISLR [10, 4, 42, 19, 43, 44].\\n\\n**Pose-based ISLR:**\\nUnlike RGB pixel-based methods, pose-based ISLR models are robust against background clutters, lighting conditions, and occlusions, while explicitly depicting human hand and limb movements [45, 46, 47, 48]. ST-GCN [47], the first to apply a spatio-temporal graph convolutional network for action recognition, encodes motions across the human kinetic chain. Subsequent studies utilize this spatio-temporal architecture, employing both graph convolutional networks [4, 49, 50, 51] and Transformers [52, 53, 54, 55] to embedding and analyze sign pose data.\\n\\n**Multi-modal-based ISLR:**\\nRecent studies show that combining pose, depth, and RGB modalities significantly improves ISLR. Zuo et al. [14] use the S3D model to extract RGB and pose heatmap features, enhancing recognition on the WLASL [4] dataset. Moreover, Jiang et al. [15] integrate depth information into the model, enabling recognition results to exceed 99% on the AUTSL dataset [35].\\n\\n2.3 Multi-view and Multi-modal Action Recognition\\n\\nPrevious research [56] argues that Action Recognition (AR) methods can be applied on sign language recognition. To build an effective and robust real-world ISLR and AR system, initiating multi-view and multi-modal learning is essential [28]. Recent advancements in AR introduce various approaches for multi-view learning [57], including dictionary learning [58], neural networks with adjustable views [59], convolutional neural networks [60], and attention mechanisms [61]. Additionally, Zhu et al. [62] adopt vision transformer models as robust solutions for multi-view learning.\\n\\nRecent approaches [63, 64] develop robust view-invariant representations for downstream tasks, while DA-Net [65] merges view-specific and independent modules for effective prediction. A feature factorization approach in [66] and a cascaded residual autoencoder in [67] address challenges in RGB-D action recognition and incomplete view classification, respectively.\\n\\n3 Proposed MM-WLAuslan Dataset\\n\\nIn this section, we describe our recording setup and workflow, detail the data processing and augmentation, and provide statistics for the MM-WLAuslan dataset.\\n\\n3.1 Recording Setup and Workflow\\n\\nOur recording setup is located in a studio environment surrounded by a green screen. In the studio, we position Kinect-V2 cameras at the left-front, front, and right-front views, along with a centrally placed RealSense camera. Both Kinect-V2 and RealSense are capable of recording high-quality videos with depth information. In the Appendix, we compare the different parameters of these two types of cameras. Most importantly, the imaging principles of Kinect-V2 and RealSense cameras are different. The former employs time-of-flight technology to measure depth, while the latter utilizes stereo vision to capture depth information. Moreover, Kinect-V2 offers high resolution and excellent depth sensing, while RealSense provides a higher frame rate and portability. We record data using these two types of RGB-D cameras to investigate the cross-camera robustness of methods.\\n\\nWe recruit signers with diverse experience in Auslan, including Auslan experts, deaf individuals who use Auslan, and volunteers interested in sign language, to sign glosses [6]. The involvement of Auslan experts and deaf individuals ensures the precision of a subset of the data, which is crucial for precise research and applications of sign language. The extensive participation of volunteers enhances the\\n\\n---\\n\\n6 Our dataset follows the copyright Creative Commons BY-NC-SA 4.0 license. Please note that we obtain the consent of the signers before recording them.\\n\\n7 Auslan experts refer to non-deaf individuals who are proficient in Australian Sign Language, while non-expert deaf signers only refers to deaf individuals who use Auslan.\"}"}
{"id": "tPsw4NeLZx", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 Data Processing and Augmentation\\n\\nAfter recording all the sign language videos, we notice that a significant portion of the footage consists of a green screen background. Therefore, we utilize the keypoints estimated by AlphaPose [68, 69, 70] to remove the background that is irrelevant to the sign language. We crop videos based on a fixed-size box that can cover every signer and align their eyes on the same horizontal level.\\n\\nTo evaluate the performance of ISLR systems under real-world scenarios, we provide a diverse test set with four distinct subsets, including studio (STU) set, in-the-wild (ITW) set, synthetic background (SYN) set, and temporal disturbance (TED) set. Each subset encompasses videos for all gloss vocabulary. The STU set includes consistent scene settings with the training set. In the ITW set, green screens are removed and replaced with dynamic or static backgrounds to simulate videos recorded in diverse environments, as shown in Figure 2. We utilize the Background Remover to extract signers from videos and synthesize indoor and outdoor backgrounds in the SYN set. The TED set simulates potential recording time discrepancies in real-world scenarios by randomly adjusting video segments through removal or altering playback speed.\\n\\nOverall, each data sample in our dataset includes: (1) RGB-D videos captured by a Kinect-V2 camera or a RealSense camera; (2) intrinsic and extrinsic parameters for the captured camera; (3) pose sequences corresponding to the RGB video; (4) gloss identities; (5) spoken English words or phrases corresponding to the gloss and (6) signer identities. These various views and modalities of sign language video samples can be further investigated for different word-level Auslan ISLR settings.\\n\\n3.3 Data Statistics\\n\\n| Split       | Train | Val  | Test-STU | Test-ITW | Test-SYN | Test-TED |\\n|-------------|-------|------|----------|----------|----------|----------|\\n| Num. Videos | 154.3k| 25.7k| 25.7k    | 25.7k    | 25.7k    | 25.7k    |\\n| Num. Signers| 55    | 53   | 12       | 15       | 62       | 63       |\\n| Num. OOS    | -     | -    | 10       | 2        | 15       | 10       |\\n\\n| BG Interference | TP Disturbance |\\n|-----------------|---------------|\\n| !                | !             |\\n\\nWe select 3,215 commonly used Auslan glosses, corresponding to over 7,900 English words or phrases. As illustrated in Figure 3(b), there are more than 2,000 glosses with multiple meanings, highlighting the contextual variability of sign language similar to natural languages. Additionally, these terms are finely categorized into 49 groups, including health, education, and others, as shown in Figure 3(d). The extensive vocabulary and semantic richness of MM-WLAuslan demonstrate its potential to advance sign language research and applications.\"}"}
{"id": "tPsw4NeLZx", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Statistics of signers and glosses.\\n(a) Ethnicity and gender distribution. (b) Frequency of polysemous glosses. (c) Distribution of Auslan proficiency. (d) Categories of words.\\n\\nThe data was split into training, validation, and testing sets following a ratio of 6:1:4. Note that the test set contains 18 signers who do not appear in either the training or validation sets. Additionally, we further divide the testing set into the STU set, the ITW set, the SYN set, and the TED set in a 1:1:1:1 ratio. The detailed split statistics are demonstrated in Table 2.\\n\\nMoreover, as illustrated in Figure 3(a), we provide the ethnic and gender distribution of signers in MM-WLAuslan. The signers are categorized into three primary ethnic groups: Caucasian, African, and Asian. The male-to-female ratios are relatively balanced across the different ethnic groups. The near-equitable gender balance within each ethnic group not only enhances the representativeness of the dataset but also underscores its gender fairness. Meanwhile, we include a broader range of ethnicities to enhance the inclusivity and representativeness of the dataset further. Thus, this composition ensures that the ISLR models developed from this dataset mitigate biases and offer equitable performance across the diverse Australian population. Furthermore, as shown in Figure 3(c), we demonstrate the distribution of participants involved in recording, segmented by their proficiency in Auslan. We make concerted efforts to include as many Auslan experts and deaf individuals as possible for the quality of the recordings. Additionally, we recruit many volunteers to further increase the diversity of the signers, and thus, enrich the representativeness of the dataset.\\n\\n4 MM-WLAuslan Benchmark\\n\\nIn this section, we present and analyze benchmark results of various multi-modal ISLR settings on MM-WLAuslan. More experiments and details are included in the Appendix.\\n\\n4.1 Isolated Sign Language Recognition Settings\\n\\nSingle-view RGB-based ISLR involves recognizing isolated sign language from video sequences captured from a single fixed camera. The input consists of RGB frames, denoted as \\\\{F_1, F_2, \\\\ldots, F_n\\\\}, where \\\\(n\\\\) represents the total number of frames in a video sequence. The single-view RGB setting utilizes spatial and temporal information from a singular perspective.\\n\\nSingle-view RGB-D-based ISLR aims to enhance the recognition of isolated signs by incorporating depth information along with RGB data. The input is represented as \\\\{(F_1, D_1), (F_2, D_2), \\\\ldots, (F_n, D_n)\\\\}, where \\\\(D_i\\\\) indicates the depth information corresponding to the \\\\(i\\\\)-th frame. This approach facilitates a richer interpretation of spatial dynamics.\\n\\nMulti-view RGB-based ISLR employs multiple cameras to capture the sign language videos. The input from each camera \\\\(k\\\\) is represented as a sequence of RGB frames \\\\{F_{k1}, F_{k2}, \\\\ldots, F_{kn}\\\\}. Multi-view RGB data helps in mitigating issues related to occlusions and varied angles.\"}"}
{"id": "tPsw4NeLZx", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 3: Baseline of Single-view RGB-based ISLR on MM-WLAuslan\\n\\n| Model          | Data Type     | STU   | ITW   | SYN   | TED   | A VG. |\\n|----------------|---------------|-------|-------|-------|-------|-------|\\n|                | Top-1 Top-5   |       |       |       |       |       |\\n| ResNet2+1D     | Pixel         | 58.71 | 77.03 | 13.83 | 26.14 | 37.45 |\\n| TSN            | Pixel         | 51.17 | 68.60 | 11.06 | 31.01 | 33.41 |\\n| I3D            | Pixel         | 63.97 | 84.93 | 14.18 | 36.17 | 43.82 |\\n| S3D            | Pixel         | 75.55 | 94.11 | 29.41 | 44.60 | 52.94 |\\n| SlowFast       | Pixel         | 80.68 | 96.08 | 32.22 | 62.21 | 58.07 |\\n| Timesformer    | Pixel         | 73.20 | 81.40 | 21.14 | 41.88 | 51.15 |\\n| UMDR           | Pixel         | 80.86 | 95.88 | 13.57 | 44.60 | 47.78 |\\n| KVNet-V        | 2D pose       | 84.51 | 97.57 | 39.88 | 56.56 | 62.82 |\\n| SL-GCN         | 2D pose       | 71.07 | 91.21 | 66.59 | 63.20 | 67.71 |\\n| SPOTER         | 2D pose       | 72.81 | 92.69 | 64.12 | 66.81 | 68.29 |\\n| DSTA-SLR       | 2D pose       | 82.33 | 96.31 | 74.96 | 78.10 | 75.55 |\\n| STC-SLR        | 2D pose       | 79.92 | 95.88 | 74.35 | 76.02 | 73.35 |\\n| KVNet-K        | 2D pose       | 82.88 | 96.70 | 76.29 | 79.07 | 76.82 |\\n| SAM-SLR        | 2D pose + Pixel| 83.98 | 97.12 | 74.30 | 80.73 | 77.55 |\\n| NLA-SLR        | 2D pose + Pixel| 86.32 | 97.79 | 79.05 | 84.26 | 81.90 |\\n\\n### Table 4: Baseline of Single-view RGB-D-based ISLR on MM-WLAuslan\\n\\n| Model          | Data Type     | STU   | ITW   | SYN   | TED   | A VG. |\\n|----------------|---------------|-------|-------|-------|-------|-------|\\n|                | Top-1 Top-5   |       |       |       |       |       |\\n| I3D            | Pixel + Depth | 65.74 | 88.57 | 21.71 | 61.06 | 48.94 |\\n| S3D            | Pixel + Depth | 79.70 | 95.93 | 64.97 | 76.38 | 71.79 |\\n| KVNet-V        | Pixel + Depth | 82.22 | 96.75 | 38.79 | 57.88 | 61.46 |\\n| UMDR           | Pixel + Depth | 91.65 | 98.81 | 72.52 | 83.77 | 84.07 |\\n| TGCN           | 3D pose       | 70.19 | 89.78 | 59.52 | 66.35 | 61.88 |\\n| SPOTER         | 3D pose       | 74.95 | 95.88 | 66.75 | 70.22 | 70.89 |\\n| SL-GCN         | 3D pose       | 77.76 | 96.98 | 72.26 | 74.91 | 74.30 |\\n| NLA-SLR        | 2D pose + Pixel + Depth | 85.65 | 95.65 | 80.20 | 83.36 | 83.34 |\\n| SAM-SLR        | 3D pose + Pixel + Depth | 87.05 | 98.93 | 81.29 | 83.03 | 85.07 |\\n\\n**Multi-view RGB-D-based ISLR** incorporates depth data in a multi-view setup, the input for each camera $k$ is represented as $\\{(F_{k1}, D_{k1}), (F_{k2}, D_{k2}), \\\\ldots, (F_{kn}, D_{kn})\\\\}$. This method enhances the model's capability to interpret complex gestures from multiple perspectives.\\n\\n**Cross-Camera ISLR** aims to test the robustness of the model against variations in camera specifications and settings. Training and testing data are captured from different cameras. It is challenging for the model to generalize across hardware-induced discrepancies.\\n\\n**Cross-View ISLR** requires the model to recognize signs from views not seen during training. With training views denoted as $V_{train}$ and testing views as $V_{test}$, the model must handle the appearance changes due to different viewing angles, thus testing its view-invariance capabilities.\\n\\n### 4.2 Evaluation Metric\\n\\nTop-$k$ **Accuracy** is quantitatively defined as the proportion of test instances for which the true label is among the top $k$ predictions made by the model. It is particularly suitable for ISLR task with a large set of possible outcomes. The expression is shown by the following equation:\\n\\n$$\\n\\\\text{Top-}k\\\\text{ Accuracy} = \\\\frac{1}{N} \\\\sum_{i=1}^{N} 1(y_i \\\\in \\\\hat{Y}_k^i),\\n$$\\n\\nwhere $N$ is the total number of instances in the test set, $1$ is a binary indicator that returns 1 if the true label of the $i$-th instance $y_i$ is within the set of the top-$k$ predicted labels $\\\\hat{Y}_k^i$ for that instance.\"}"}
{"id": "tPsw4NeLZx", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5: The baseline of Multi-view RGB-based ISLR on MM-WLAuslan.\\n\\nTable 6: The baseline of Multi-view RGB-D-based ISLR on MM-WLAuslan.\\n\\nSingle-view RGB-based ISLR: Following previous works [4, 10, 18], we adopt this setting as a central focus of ISLR research. We utilize publicly available ISLR models, such as KVNet [14], SPOTER [74], DSTA-SLR [50], STC-SLR [51], SAM-SLR [15] and NLA-SLR [14]. Meanwhile, we incorporate models that have demonstrated strong performance in action recognition, including I3D [6], SlowFast [9] and Timesformer [72]. As indicated by Table 3, pixel-based models perform well in controlled STU. This suggests that pixel models are effective in settings with minimal noise and well-defined conditions. Conversely, pose-based models are robust in challenging environments, like ITW and SYN, because they focus on structural rather than textural information. Furthermore, NLA-SLR [14] is the SOTA model for ISLR. It ensembles the high-performance KVNet-V and KVNet-K models for pixel and pose data, respectively. The model demonstrates high accuracy across all test subsets consistently.\\n\\nSingle-view RGB-D-based ISLR: As shown in Table 4, the combination of pixel and depth data generally improves recognition accuracy on most methods, highlighting the benefits brought by depth data. However, the performance of the KVNet-V [14] model declines, indicating its insufficient processing of depth information alongside pixel data. In contrast, the UMDR [73] model, a SOTA model for RGB-D action recognition, leads to significant performance improvements across various test subsets. Additionally, pose-based models with 3D pose data as the input also show improved performance, further supporting the benefits of integrating depth information into pose-based models.\\n\\nMulti-view RGB-based & RGB-D-based ISLR: In Table 5 and Table 6, we show performances of several RGB-based and RGB-D-based models on multi-view ISLR. The results highlight that using multiple views and additional modalities generally improves model performance. Models like UMDR and SAM-SLR, incorporating depth or 3D pose data, consistently achieve better results. This suggests these models effectively capture more comprehensive gesture information. However, these benefits come at the cost of increased model complexity. The introduction of multi-view RGBD data inevitably raises the training costs of the model. Additionally, information redundancy in the data can potentially interfere with the model's learning process. For instance, the recognition accuracy of the NLA-SLR model, when trained on multi-view RGBD data, is lower compared to when it is trained solely on RGB data. For future research, we focus on developing more efficient methods to optimize performance without increasing complexity for multi-view and multi-modal data.\\n\\nCross-camera ISLR: As illustrated in Table 7, there is a challenge in cross-camera ISLR on the MM-WL Auslan dataset. The results show a significant decline in accuracy when models trained on one type of camera are tested on the other one. Although two models, i.e., KVNet-V [14] and,...\"}"}
{"id": "tPsw4NeLZx", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 7: The baseline of Cross-Camera ISLR on MM-WLAuslan.\\n\\n\\\"K\\\", \\\"RS\\\" and \\\"K+\\\" represent Front Kinect-v2, Front RealSence and Left-Front + Right-Front Kinect-v2, respectively. \\\"STU\\\", \\\"ITW\\\", \\\"SYN\\\", \\\"TED\\\", and \\\"AVG.\\\" represent the studio set, in-the-wild set, synthetic background set, temporal disturbance set and average performance across the four subsets, respectively.\\n\\n| Model   | Train | Test | Data Type | STU  | ITW  | SYN  | TED  | AVG. |\\n|---------|-------|------|-----------|------|------|------|------|------|\\n|         | Top-1 | Top-5|           |      |      |      |      |      |\\n| KVNet-V |       |      | Pixel     | 84.51| 97.57| 39.88| 68.00| 56.56| 82.18|\\n| RS      |       |      | Pixel     | 66.41| 89.58| 26.82| 52.05| 41.70| 68.52|\\n| K       |       |      | RS Pixel  | 53.33| 81.06| 18.88| 41.58| 32.32| 60.09|\\n| RS      |       |      | K Pixel   | 31.28| 55.30| 5.85 | 15.73| 14.35| 30.39|\\n| RS      |       |      | K+ Pixel  | 5.36 | 14.45| 1.97 | 6.36 | 1.97 | 6.39 |\\n\\nUMDR [73], perform well with data from the same camera, their performance drops across the cameras. This highlights the substantial differences between the two cameras, emphasizing the complexity of achieving robust ISLR across varied hardware. The challenge of cross-camera ISLR underscores the need for developing models that can better generalize on data from various cameras.\\n\\nCross-view ISLR: We report the performance of two models, i.e., KVNet-V [14] and UMDR [73], training and evaluating on the data from different Kinect-v2 views, as shown in Table 8. UMDR, incorporating depth information alongside pixel data, generally exhibits greater resilience and performance compared to KVNet-V. Both models exhibit high accuracy under the single-view setting of our dataset, yet experience a significant drop in accuracy in the cross-view context. This indicates that models capable of adapting to diverse visual inputs are necessary to address the challenges posed by cross-view.\\n\\n| Model   | Train | Test | Data Type | STU  | ITW  | SYN  | TED  | AVG. |\\n|---------|-------|------|-----------|------|------|------|------|------|\\n|         | Top-1 | Top-5|           |      |      |      |      |      |\\n| KVNet-V |       |      | Pixel     | 84.51| 97.57| 39.88| 68.00| 56.56| 82.18|\\n| RS      |       |      | L Pixel   | 80.59| 95.74| 45.17| 71.29| 57.93| 82.92|\\n| R       |       |      | R Pixel   | 80.82| 95.68| 37.97| 65.94| 37.62| 64.82|\\n| F       |       |      | L+R Pixel | 23.60| 48.10| 8.70 | 23.28| 9.94 | 26.53|\\n| L       |       |      | F+R Pixel | 29.18| 48.41| 12.48| 27.28| 21.84| 40.21|\\n| R       |       |      | F+L Pixel | 24.93| 44.53| 16.93| 34.15| 20.10| 39.26|\\n\\nUMDR [73], perform well with data from the same camera, their performance drops across the cameras. This highlights the substantial differences between the two cameras, emphasizing the complexity of achieving robust ISLR across varied hardware. The challenge of cross-camera ISLR underscores the need for developing models that can better generalize on data from various cameras.\\n\\n5 Limitation and Future Work\\n\\nLimited Diversity in Data: As shown in Figure 3(a) of the main paper, we analyze the distribution of Caucasian, African, and Asian signers within the MM-WLAuslan dataset. We observe that the proportion of African signers is significantly lower than that of Caucasian and Asian signers. Consequently, the signers in our recordings do not fully represent the demographic diversity of the Auslan community. Australia, being a multi-cultural nation, encompasses a wide range of ethnicities, and the representation of these ethnicities in our dataset is crucial. Therefore, we will continue recording to achieve a more balanced representation.\\n\\nLack of Real-world Scenarios: Although we attempt to simulate real-life environments by altering backgrounds and capturing some data \\\"in the wild\\\", these settings still fall short of fully representing the complexities of real-world scenarios, such as multi-person interactions and intricate backgrounds. Moving forward, we intend to capture real-world Auslan glosses for a more authentic dataset. This initiative aims to more accurately reflect the dynamic and diverse contexts in which Auslan is naturally used, thereby improving the relevance and applicability of the dataset.\"}"}
{"id": "tPsw4NeLZx", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Existing Model Limitations:\\nIn this work, we utilize publicly available deep learning models, some of which are not specifically designed for sign language. Consequently, developing more effective multi-modal fusion and multi-view techniques tailored to the unique characteristics of our dataset is essential. This approach will enhance the accuracy and applicability of the models, ensuring they are better suited to address the specific challenges and nuances of isolated sign language recognition.\\n\\nInvestigating Isolated Sign Language Production Task:\\nSign Language Production is currently a popular task, involving not only the generation of isolated glosses [75] but also continuous sign language [76, 77]. Unlike previous datasets, ours incorporates multi-view and multi-modal capabilities, enabling the creation of more accurate 2D or 3D sign language representations. We plan to further explore this task using our dataset in future research. This will enhance the precision and effectiveness of sign language modelling, providing more robust tools for communication within the deaf and hard-of-hearing community.\\n\\n6 Conclusion\\nIn this work, we introduce the first large-scale, multi-view, multi-modal word-level dataset for Australian Sign Language (Auslan), named MM-WLAuslan. The dataset includes 282K+ videos encompassing 3,215 distinct Auslan glosses performed by 73 signers. To the best of our knowledge, MM-WLAuslan has the largest amount of data, the most extensive vocabulary, and the most diverse set of multi-modal camera views. We position four RGB-D cameras, i.e., three Kinect-V2 cameras and a RealSense camera, hemispherically around the signers. Extensive experiments demonstrate the validity and challenges of MM-WLAuslan. Thanks to the cross-camera, multi-view, and multi-modal gloss videos, our dataset can be used for practical applications related with Auslan. Furthermore, the presented benchmark results can act as strong baselines for future research.\\n\\nAcknowledgement\\nThis research is funded in part by ARC-Discovery grant (DP220100800 to XY), ARC-DECRA grant (DE230100477 to XY) and Google Research Scholar Program. We express our gratitude to Professor Trevor Cohn for his valuable feedback on this work. We also gratefully thank all the anonymous reviewers and ACs for their constructive comments.\\n\\nBroader Impact\\nThe development of the word-level Australian Sign Language (Auslan) dataset has several impacts on technology, education, and society. Our proposed MM-WLAuslan, recorded using multi-view RGB-D cameras and focused on isolated Auslan glosses, brings about a wide range of positive effects:\\n\\n\u2022 **Improving Accuracy and Efficiency in ISLR**: The high-quality data provided by multi-view RGB-D cameras enhance the detailed capture of sign language gestures, which is crucial for developing efficient and accurate ISLR systems.\\n\\n\u2022 **Facilitating Social Integration for the Deaf**: Improved by our MM-WLAuslan dataset, the ISLR technology can provide the deaf and hard-of-hearing community with more efficient communication capabilities.\\n\\n\u2022 **Expanding Educational Resources**: Our dataset can support Auslan education [25, 78]. By providing multi-view demonstrations, the dataset allows Auslan learners to observe signs from different views, enhancing their understanding and accuracy in sign language.\\n\\n\u2022 **Driving Technological Innovation**: Our dataset offers valuable resources for research in computer vision and machine learning, promoting technological development and innovation in these fields [79, 80, 81, 82].\\n\\n\u2022 **Preserving and Promoting Culture**: By recording and utilizing the MM-WLAuslan dataset, we preserve and disseminate the unique cultural heritage of Australian Sign Language, enhancing public awareness of its cultural value [83].\\n\\nThese societal impacts demonstrate that the development and application of the Auslan dataset are not only technically significant but also have profound positive values on social and cultural levels.\"}"}
{"id": "tPsw4NeLZx", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Natasha Abner, Carlo Geraci, Shi Yu, Jessica Lettieri, Justine Mertz, and Anah Salgat. Getting the upper hand on sign language families: Historical analysis and annotation methods. FEAST. Formal and Experimental Advances in Sign language Theory, 3:17\u201329, 2020.\\n\\n[2] William C Stokoe Jr. Sign language structure: An outline of the visual communication systems of the american deaf. Journal of deaf studies and deaf education, 10(1):3\u201337, 2005.\\n\\n[3] William C Stokoe. Sign language structure. Annual review of anthropology, 9(1):365\u2013390, 1980.\\n\\n[4] Dongxu Li, Cristian Rodriguez, Xin Yu, and Hongdong Li. Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison. In The IEEE Winter Conference on Applications of Computer Vision, pages 1459\u20131469, 2020.\\n\\n[5] Xin Shen, Shaozu Yuan, Hongwei Sheng, Heming Du, and Xin Yu. Auslan-daily: Australian sign language translation for daily communication and news. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.\\n\\n[6] Jo\u00e3o Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 4724\u20134733. IEEE Computer Society, 2017.\\n\\n[7] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 6450\u20136459. Computer Vision Foundation / IEEE Computer Society, 2018.\\n\\n[8] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XV, volume 11219 of Lecture Notes in Computer Science, pages 318\u2013335. Springer, 2018.\\n\\n[9] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 6201\u20136210. IEEE, 2019.\\n\\n[10] Hamid Reza Vaezi Joze and Oscar Koller. MS-ASL: A large-scale data set and benchmark for understanding american sign language. In 30th British Machine Vision Conference 2019, BMVC 2019, Cardiff, UK, September 9-12, 2019, page 100. BMVA Press, 2019.\\n\\n[11] Jie Huang, Wengang Zhou, Houqiang Li, and Weiping Li. Attention-based 3d-cnns for large-vocabulary sign language recognition. IEEE Transactions on Circuits and Systems for Video Technology, 29(9):2822\u20132832, 2018.\\n\\n[12] Kinect gesture dataset. https://www.microsoft.com/en-us/download/details.aspx?id=52283, 2019. Accessed: 2019-07-16.\\n\\n[13] Franco Ronchetti, Facundo Manuel Quiroga, C\u00e9sar Estrebou, Laura Lanzarini, and Alejandro Rosete. Lsa64: an argentinian sign language dataset. arXiv preprint arXiv:2310.17429, 2023.\\n\\n[14] Ronglai Zuo, Fangyun Wei, and Brian Mak. Natural language-assisted sign language recognition. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 14890\u201314900. IEEE, 2023.\\n\\n[15] Songyao Jiang, Bin Sun, Lichen Wang, Yue Bai, Kunpeng Li, and Yun Fu. Skeleton aware multi-modal sign language recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2021.\\n\\n[16] Aleix M. Mart\u00ednez, Ronnie B. Wilbur, Robin Shay, and Avinash C. Kak. Purdue RVL-SLLL ASL database for automatic recognition of american sign language. In 4th IEEE International Conference on Multimodal Interfaces (ICMI 2002), 14-16 October 2002, Pittsburgh, PA, USA, pages 167\u2013172. IEEE Computer Society, 2002.\"}"}
{"id": "tPsw4NeLZx", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[108x710]\\n\\n[128x700]\\n\\n[148x690]\\n\\n[168x680]\\n\\n[188x662]\\n\\n[208x652]\\n\\n[228x642]\\n\\n[248x632]\\n\\n[268x622]\\n\\n[288x622]\\n\\n[308x622]\\n\\n[328x622]\\n\\n[348x622]\\n\\n[368x622]\\n\\n[388x622]\\n\\n[408x622]\\n\\n[428x622]\\n\\n[448x622]\\n\\n[468x622]\\n\\n[488x622]\\n\\n[508x622]\\n\\n[528x622]\\n\\n[548x622]\\n\\n[568x622]\\n\\n[588x622]\\n\\n[608x622]\\n\\n[628x622]\\n\\n[648x622]\\n\\n[668x622]\\n\\n[688x622]\\n\\n[708x622]\\n\\n[728x622]\\n\\n[748x622]\\n\\n[768x622]\\n\\n[788x622]\\n\\n[808x622]\\n\\n[828x622]\\n\\n[848x622]\\n\\n[868x622]\\n\\n[888x622]\\n\\n[908x622]\\n\\n[928x622]\\n\\n[948x622]\\n\\n[968x622]\\n\\n[988x622]\\n\\n[1008x622]\\n\\n[1028x622]\\n\\n[1048x622]\\n\\n[1068x622]\\n\\n[1088x622]\\n\\n[1108x622]\\n\\n[1128x622]\\n\\n[1148x622]\\n\\n[1168x622]\\n\\n[1188x622]\\n\\n[1208x622]\\n\\n[1228x622]\\n\\n[1248x622]\\n\\n[1268x622]\\n\\n[1288x622]\\n\\n[1308x622]\\n\\n[1328x622]\\n\\n[1348x622]\\n\\n[1368x622]\\n\\n[1388x622]\\n\\n[1408x622]\\n\\n[1428x622]\\n\\n[1448x622]\\n\\n[1468x622]\\n\\n[1488x622]\\n\\n[1508x622]\\n\\n[1528x622]\\n\\n[1548x622]\\n\\n[1568x622]\\n\\n[1588x622]\\n\\n[1608x622]\\n\\n[1628x622]\\n\\n[1648x622]\\n\\n[1668x622]\\n\\n[1688x622]\\n\\n[1708x622]\\n\\n[1728x622]\\n\\n[1748x622]\\n\\n[1768x622]\\n\\n[1788x622]\\n\\n[1808x622]\\n\\n[1828x622]\\n\\n[1848x622]\\n\\n[1868x622]\\n\\n[1888x622]\\n\\n[1908x622]\\n\\n[1928x622]\\n\\n[1948x622]\\n\\n[1968x622]\\n\\n[1988x622]\\n\\n[2008x622]\\n\\n[2028x622]\\n\\n[2048x622]\\n\\n[2068x622]\\n\\n[2088x622]\\n\\n[2108x622]\\n\\n[2128x622]\\n\\n[2148x622]\\n\\n[2168x622]\\n\\n[2188x622]\\n\\n[2208x622]\\n\\n[2228x622]\\n\\n[2248x622]\\n\\n[2268x622]\\n\\n[2288x622]\\n\\n[2308x622]\\n\\n[2328x622]\\n\\n[2348x622]\\n\\n[2368x622]\\n\\n[2388x622]\\n\\n[2408x622]\\n\\n[2428x622]\\n\\n[2448x622]\\n\\n[2468x622]\\n\\n[2488x622]\\n\\n[2508x622]\\n\\n[2528x622]\\n\\n[2548x622]\\n\\n[2568x622]\\n\\n[2588x622]\\n\\n[2608x622]\\n\\n[2628x622]\\n\\n[2648x622]\\n\\n[2668x622]\\n\\n[2688x622]\\n\\n[2708x622]\\n\\n[2728x622]\\n\\n[2748x622]\\n\\n[2768x622]\\n\\n[2788x622]\\n\\n[2808x622]\\n\\n[2828x622]\\n\\n[2848x622]\\n\\n[2868x622]\\n\\n[2888x622]\\n\\n[2908x622]\\n\\n[2928x622]\\n\\n[2948x622]\\n\\n[2968x622]\\n\\n[2988x622]\\n\\n[3008x622]\\n\\n[3028x622]\\n\\n[3048x622]\\n\\n[3068x622]\\n\\n[3088x622]\\n\\n[3108x622]\\n\\n[3128x622]\\n\\n[3148x622]\\n\\n[3168x622]\\n\\n[3188x622]\\n\\n[3208x622]\\n\\n[3228x622]\\n\\n[3248x622]\\n\\n[3268x622]\\n\\n[3288x622]\\n\\n[3308x622]\\n\\n[3328x622]\\n\\n[3348x622]\\n\\n[3368x622]\\n\\n[3388x622]\\n\\n[3408x622]\\n\\n[3428x622]\\n\\n[3448x622]\\n\\n[3468x622]\\n\\n[3488x622]\\n\\n[3508x622]\\n\\n[3528x622]\\n\\n[3548x622]\\n\\n[3568x622]\\n\\n[3588x622]\\n\\n[3608x622]\\n\\n[3628x622]\\n\\n[3648x622]\\n\\n[3668x622]\\n\\n[3688x622]\\n\\n[3708x622]\\n\\n[3728x622]\\n\\n[3748x622]\\n\\n[3768x622]\\n\\n[3788x622]\\n\\n[3808x622]\\n\\n[3828x622]\\n\\n[3848x622]\\n\\n[3868x622]\\n\\n[3888x622]\\n\\n[3908x622]\\n\\n[3928x622]\\n\\n[3948x622]\\n\\n[3968x622]\\n\\n[3988x622]\\n\\n[4008x622]\\n\\n[4028x622]\\n\\n[4048x622]\\n\\n[4068x622]\\n\\n[4088x622]\\n\\n[4108x622]\\n\\n[4128x622]\\n\\n[4148x622]\\n\\n[4168x622]\\n\\n[4188x622]\\n\\n[4208x622]\\n\\n[4228x622]\\n\\n[4248x622]\\n\\n[4268x622]\\n\\n[4288x622]\\n\\n[4308x622]\\n\\n[4328x622]\\n\\n[4348x622]\\n\\n[4368x622]\\n\\n[4388x622]\\n\\n[4408x622]\\n\\n[4428x622]\\n\\n[4448x622]\\n\\n[4468x622]\\n\\n[4488x622]\\n\\n[4508x622]\\n\\n[4528x622]\\n\\n[4548x622]\\n\\n[4568x622]\\n\\n[4588x622]\\n\\n[4608x622]\\n\\n[4628x622]\\n\\n[4648x622]\\n\\n[4668x622]\\n\\n[4688x622]\\n\\n[4708x622]\\n\\n[4728x622]\\n\\n[4748x622]\\n\\n[4768x622]\\n\\n[4788x622]\\n\\n[4808x622]\\n\\n[4828x622]\\n\\n[4848x622]\\n\\n[4868x622]\\n\\n[4888x622]\\n\\n[4908x622]\\n\\n[4928x622]\\n\\n[4948x622]\\n\\n[4968x622]\\n\\n[4988x622]\\n\\n[5008x622]\\n\\n[5028x622]\\n\\n[5048x622]\\n\\n[5068x622]\\n\\n[5088x622]\\n\\n[5108x622]\\n\\n[5128x622]\\n\\n[5148x622]\\n\\n[5168x622]\\n\\n[5188x622]\\n\\n[5208x622]\\n\\n[5228x622]\\n\\n[5248x622]\\n\\n[5268x622]\\n\\n[5288x622]\\n\\n[5308x622]\\n\\n[5328x622]\\n\\n[5348x622]\\n\\n[5368x622]\\n\\n[5388x622]\\n\\n[5408x622]\\n\\n[5428x622]\\n\\n[5448x622]\\n\\n[5468x622]\\n\\n[5488x622]\\n\\n[5508x622]\\n\\n[5528x622]\\n\\n[5548x622]\\n\\n[5568x622]\\n\\n[5588x622]\\n\\n[5608x622]\\n\\n[5628x622]\\n\\n[5648x622]\\n\\n[5668x622]\\n\\n[5688x622]\\n\\n[5708x622]\\n\\n[5728x622]\\n\\n[5748x622]\\n\\n[5768x622]\\n\\n[5788x622]\\n\\n[5808x622]\\n\\n[5828x622]\\n\\n[5848x622]\\n\\n[5868x622]\\n\\n[5888x622]\\n\\n[5908x622]\\n\\n[5928x622]\\n\\n[5948x622]\\n\\n[5968x622]\\n\\n[5988x622]\\n\\n[6008x622]\\n\\n[6028x622]\\n\\n[6048x622]\\n\\n[6068x622]\\n\\n[6088x622]\\n\\n[6108x622]\\n\\n[6128x622]\"}"}
{"id": "tPsw4NeLZx", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Eva Gutierrez-Sigut, Brendan Costello, Cristina Baus, and Manuel Carreiras. Lse-sign: A lexical database for Spanish sign language. Behavior Research Methods, 48:123\u2013137, 2016.\\n\\nJ\u00e9r\u00f4me Fink, Beno\u00eet Fr\u00e9nay, Laurence Meurant, and Anthony Cleve. LSFB-CONT and LSFB-ISOL: two new datasets for vision-based sign language recognition. In International Joint Conference on Neural Networks, IJCNN 2021, Shenzhen, China, July 18-22, 2021, pages 1\u20138. IEEE, 2021.\\n\\nOgulcan \u00d6zdemir, Ahmet Alp Kindiroglu, Necati Cihan Camg\u00f6z, and Lale Akarun. Bosphorussign22k sign language recognition dataset. CoRR, abs/2004.01283, 2020.\\n\\nOzge Mercanoglu Sincan and Hacer Yalim Keles. AUTSL: A large scale multi-modal Turkish sign language dataset and baseline methods. IEEE Access, 8:181340\u2013181355, 2020.\\n\\nRunpeng Cui, Hu Liu, and Changshui Zhang. Recurrent convolutional neural networks for continuous sign language recognition by staged optimization. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 1610\u20131618. IEEE Computer Society, 2017.\\n\\nRunpeng Cui, Hu Liu, and Changshui Zhang. A deep neural framework for continuous sign language recognition by iterative training. IEEE Trans. Multim., 21(7):1880\u20131891, 2019.\\n\\nOscar Koller, Necati Cihan Camgoz, Hermann Ney, and Richard Bowden. Weakly supervised learning with multi-stream CNN-LSTM-HMMs to discover sequential parallelism in sign language videos. IEEE transactions on pattern analysis and machine intelligence, 42(9):2306\u20132320, 2019.\\n\\nYangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross B. Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In Kien A. Hua, Yong Rui, Ralf Steinmetz, Alan Hanjalic, Apostol Natsev, and Wenwu Zhu, editors, Proceedings of the ACM International Conference on Multimedia, MM \u201814, Orlando, FL, USA, November 03 - 07, 2014, pages 675\u2013678. ACM, 2014.\\n\\nAndrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014, pages 1725\u20131732. IEEE Computer Society, 2014.\\n\\nDu Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3D convolutional networks. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 4489\u20134497. IEEE Computer Society, 2015.\\n\\nAl Amin Hosain, Panneer Selvam Santhalingam, Parth H. Pathak, Huzefa Rangwala, and Jana Koseck\u00e1. Hand pose guided 3D pooling for word-level sign language recognition. In IEEE Winter Conference on Applications of Computer Vision, WACV 2021, Waikoloa, HI, USA, January 3-8, 2021, pages 3428\u20133438. IEEE, 2021.\\n\\nHongyu Fu, Chen Liu, Xingqun Qi, Beibei Lin, Lincheng Li, Li Zhang, and Xin Yu. Sign spotting via multi-modal fusion and testing time transferring. In Leonid Karlinsky, Tomer Michaeli, and Ko Nishino, editors, Computer Vision - ECCV 2022 Workshops - Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VIII, volume 13808 of Lecture Notes in Computer Science, pages 271\u2013287. Springer, 2022.\\n\\nDongxu Li, Xin Yu, Chenchen Xu, Lars Petersson, and Hongdong Li. Transferring cross-domain knowledge for video sign language recognition. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 6204\u20136213. Computer Vision Foundation / IEEE, 2020.\\n\\nPhilippe Weinzaepfel, Za\u00efd Harchaoui, and Cordelia Schmid. Learning to track for spatio-temporal action localization. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 3164\u20133172. IEEE Computer Society, 2015.\\n\\nChenyang Si, Ya Jing, Wei Wang, Liang Wang, and Tieniu Tan. Skeleton-based action recognition with spatial reasoning and temporal stack learning. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part I, volume 11205 of Lecture Notes in Computer Science, pages 106\u2013121. Springer, 2018.\\n\\nSijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for skeleton-based action recognition. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, pages 7444\u20137452. AAAI Press, 2018.\"}"}
{"id": "tPsw4NeLZx", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shannan Guan, Xin Yu, Wei Huang, Gengfa Fang, and Haiyan Lu. DMMG: dual min-max games for self-supervised skeleton-based action recognition. IEEE Trans. Image Process., 33:395\u2013407, 2024.\\n\\nAnirudh Tunga, Sai Vidyaranya Nuthalapati, and Juan P. Wachs. Pose-based sign language recognition using GCN and BERT. In IEEE Winter Conference on Applications of Computer Vision Workshops, WACV Workshops 2021, Waikoloa, HI, USA, January 5-9, 2021, pages 31\u201340. IEEE, 2021.\\n\\nLianyu Hu, Liqing Gao, Zekang Liu, and Wei Feng. Dynamic spatial-temporal aggregation for skeleton-aware sign language recognition. In Nicoletta Calzolari, Min-Yen Kan, V\u00e9ronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy, pages 5450\u20135460. ELRA and ICCL, 2024.\\n\\nWeichao Zhao, Wengang Zhou, Hezhen Hu, Min Wang, and Houqiang Li. Self-supervised representation learning with spatial-temporal consistency for sign language recognition. IEEE Trans. Image Process., 33:4188\u20134201, 2024.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\nMaty\u00e1s Boh\u00e1cek and Marek Hr\u00faz. Sign pose-based transformer for word-level sign language recognition. In IEEE/CVF Winter Conference on Applications of Computer Vision Workshops, WACV - Workshops, Waikoloa, HI, USA, January 4-8, 2022, pages 182\u2013191. IEEE, 2022.\\n\\nHezhen Hu, Weichao Zhao, Wengang Zhou, Yuechen Wang, and Houqiang Li. Signbert: pre-training of hand-model-aware representation for sign language recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pages 11087\u201311096, 2021.\\n\\nTaeryung Lee, Yeonguk Oh, and Kyoung Mu Lee. Human part-wise 3d motion context learning for sign language recognition. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 20683\u201320693. IEEE, 2023.\\n\\nNoha Sarhan and Simone Frintrop. Transfer learning for videos: from action recognition to sign language recognition. In 2020 IEEE International Conference on Image Processing, pages 1811\u20131815. IEEE, 2020.\\n\\nMuhammed Kocabas, Salih Karagoz, and Emre Akbas. Self-supervised learning of 3d human pose using multi-view geometry. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1077\u20131086, 2019.\\n\\nZan Gao, Hai-Zhen Xuan, Hua Zhang, Shaohua Wan, and Kim-Kwang Raymond Choo. Adaptive fusion and category-level dictionary learning model for multiview human action recognition. IEEE Internet of Things Journal, 6(6):9280\u20139293, 2019.\\n\\nPengfei Zhang, Cuiling Lan, Junliang Xing, Wenjun Zeng, Jianru Xue, and Nanning Zheng. View adaptive neural networks for high performance skeleton-based human action recognition. IEEE Trans. Pattern Anal. Mach. Intell., 41(8):1963\u20131978, 2019.\\n\\nTong Hao, Dan Wu, Qian Wang, and Jin-Sheng Sun. Multi-view representation learning for multi-view action recognition. Journal of Visual Communication and Image Representation, 48:453\u2013460, 2017.\\n\\nYisheng Zhu and Guangcan Liu. Fine-grained action recognition using multi-view attentions. Vis. Comput., 36(9):1771\u20131781, 2020.\\n\\nKaijun Zhu, Ruxin Wang, Qingsong Zhao, Jun Cheng, and Dapeng Tao. A cuboid cnn model with an attention mechanism for skeleton-based action recognition. IEEE Transactions on Multimedia, 22(11):2977\u20132989, 2019.\\n\\nJingjing Zheng, Zhuolin Jiang, and Rama Chellappa. Cross-view action recognition via transferable dictionary learning. IEEE Transactions on Image Processing, 25(6):2542\u20132556, 2016.\\n\\nHeng Wang, Alexander Kl\u00e4ser, Cordelia Schmid, and Cheng-Lin Liu. Dense trajectories and motion boundary descriptors for action recognition. Int. J. Comput. Vis., 103(1):60\u201379, 2013.\\n\\nDongang Wang, Wanli Ouyang, Wen Li, and Dong Xu. Dividing and aggregating network for multi-view action recognition. In Proceedings of the European conference on computer vision, pages 451\u2013467, 2018.\\n\\nAmir Shahroudy, Tian-Tsong Ng, Yihong Gong, and Gang Wang. Deep multimodal feature analysis for action recognition in rgb+ d videos. IEEE transactions on pattern analysis and machine intelligence, 40(5):1045\u20131058, 2017.\"}"}
{"id": "tPsw4NeLZx", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Luan Tran, Xiaoming Liu, Jiayu Zhou, and Rong Jin. Missing modalities imputation via cascaded residual autoencoder. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1405\u20131414, 2017.\\n\\nHao-Shu Fang, Jiefeng Li, Hongyang Tang, Chao Xu, Haoyi Zhu, Yuliang Xiu, Yong-Lu Li, and Cewu Lu. Alphapose: Whole-body regional multi-person pose estimation and tracking in real-time. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\\n\\nHao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu. RMPE: Regional multi-person pose estimation. In ICCV, 2017.\\n\\nJiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang, and Cewu Lu. Crowdpose: Efficient crowded scenes pose estimation and a new benchmark. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10863\u201310872, 2019.\\n\\nLimin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In European conference on computer vision, pages 20\u201336. Springer, 2016.\\n\\nGedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, volume 2, page 4, 2021.\\n\\nBenjia Zhou, Pichao Wang, Jun Wan, Yanyan Liang, and Fan Wang. A unified multimodal de- and re-coupling framework for RGB-D motion recognition. IEEE Trans. Pattern Anal. Mach. Intell., 45(10):11428\u201311442, 2023.\\n\\nMaty\u00e1\u0161 Boh\u00e1\u02c7cek and Marek Hr\u00faz. Sign pose-based transformer for word-level sign language recognition. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) Workshops, pages 182\u2013191, January 2022.\\n\\nRotem Shalev-Arkushin, Amit Moryossef, and Ohad Fried. Ham2pose: Animating sign language notation into pose sequences. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 21046\u201321056. IEEE, 2023.\\n\\nBen Saunders, Necati Cihan Camg\u00f6z, and Richard Bowden. Signing at scale: Learning to co-articulate signs for large-scale photo-realistic sign language production. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 5131\u20135141. IEEE, 2022.\\n\\nBen Saunders, Necati Cihan Camg\u00f6z, and Richard Bowden. Progressive transformers for end-to-end sign language production. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XI, volume 12356 of Lecture Notes in Computer Science, pages 687\u2013705, 2020.\\n\\nChenchen Xu, Dongxu Li, Hongdong Li, Hanna Suominen, and Ben Swift. Automatic gloss dictionary for sign language learners. In Valerio Basile, Zornitsa Kozareva, and Sanja Stajner, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL 2022 - System Demonstrations, Dublin, Ireland, May 22-27, 2022, pages 83\u201392. Association for Computational Linguistics, 2022.\\n\\nDongxu Li, Chenchen Xu, Xin Yu, Kaihao Zhang, Benjamin Swift, Hanna Suominen, and Hongdong Li. Tspnet: Hierarchical feature learning via temporal semantic pyramid for sign language translation. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\\n\\nYiwei Wei, Shaozu Yuan, Meng Chen, Xin Shen, Longbiao Wang, Lei Shen, and Zhiling Yan. Mpp-net: Multi-perspective perception network for dense video captioning. Neurocomputing, 552:126523, 2023.\\n\\nMaria Zelenskaya, Scott Whittington, Julie Lyons, Adele Vogel, and Jessica Korte. Visual-gestural interface for auslan virtual assistant. In June Kim, Miu Ling Lam, and Kouta Minamizawa, editors, SIGGRAPH Asia 2023 Emerging Technologies, Sydney, NSW, Australia, December 12-15, 2023, pages 21:1\u201321:2, 2023.\\n\\nLei Shen, Haolan Zhan, Xin Shen, Yonghao Song, and Xiaofang Zhao. Text is NOT enough: Integrating visual impressions into open-domain dialogue generation. In Heng Tao Shen, Yueting Zhuang, John R. Smith, Yang Yang, Pablo C\u00e9sar, Florian Metze, and Balakrishnan Prabhakaran, editors, MM '21: ACM Multimedia Conference, Virtual Event, China, October 20 - 24, 2021, pages 4287\u20134296. ACM, 2021.\"}"}
{"id": "tPsw4NeLZx", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default `[TODO]` to `[Yes]`, `[No]`, or `[N/A]`. You are strongly encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description. For example:\\n\\n- Did you include the license to the code and datasets? [Yes] See Section ??.\\n- Did you include the license to the code and datasets? [No] The code and the data are proprietary.\\n- Did you include the license to the code and datasets? [N/A]\\n\\nPlease do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n   (b) Did you describe the limitations of your work? [Yes] Section 5.\\n   (c) Did you discuss any potential negative societal impacts of your work? [No] Our work does not pose any negative societal impacts.\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [No] We do not have theoretical results.\\n   (b) Did you include complete proofs of all theoretical results? [No] We do not have theoretical results.\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] All datasets and benchmarks are available at /gtb/MM-WLAuslan.\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 3.\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No]\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Appendix Section C.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes] We cite the papers of the model.\\n   (b) Did you mention the license of the assets? [No]\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\\n   (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes]\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [No]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [No]\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [No]\\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes]\"}"}
