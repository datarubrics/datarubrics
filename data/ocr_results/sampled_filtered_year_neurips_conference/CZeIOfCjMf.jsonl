{"id": "CZeIOfCjMf", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HAPI: A Large-scale Longitudinal Dataset of Commercial ML API Predictions\\n\\nLingjiao Chen, Zhihua Jin, Sabri Eyuboglu, Christopher R\u00e9, Matei Zaharia, James Zou\\n\\nStanford University, Hong Kong University of Science and Technology\\n\\nAbstract\\n\\nCommercial ML APIs offered by providers such as Google, Amazon and Microsoft have dramatically simplified ML adoption in many applications. Numerous companies and academics pay to use ML APIs for tasks such as object detection, OCR and sentiment analysis. Different ML APIs tackling the same task can have very heterogeneous performance. Moreover, the ML models underlying the APIs also evolve over time. As ML APIs rapidly become a valuable marketplace and a widespread way to consume machine learning, it is critical to systematically study and compare different APIs with each other and to characterize how APIs change over time. However, this topic is currently underexplored due to the lack of data. In this paper, we present HAPI (History of APIs), a longitudinal dataset of 1,761,417 instances of commercial ML API applications (involving APIs from Amazon, Google, IBM, Microsoft and other providers) across diverse tasks including image tagging, speech recognition and text mining from 2020 to 2022. Each instance consists of a query input for an API (e.g., an image or text) along with the API's output prediction/annotation and confidence scores. HAPI is the first large-scale dataset of ML API usages and is a unique resource for studying ML-as-a-service (MLaaS).\\n\\nAs examples of the types of analyses that HAPI enables, we show that ML APIs' performance change substantially over time\u2014several APIs' accuracies dropped on specific benchmark datasets. Even when the API's aggregate performance stays steady, its error modes can shift across different subtypes of data between 2020 and 2022. Such changes can substantially impact the entire analytics pipelines that use some ML API as a component. We further use HAPI to study commercial APIs' performance disparities across demographic subgroups over time. HAPI can stimulate more research in the growing field of MLaaS.\\n\\n1 Introduction\\n\\nMachine learning (ML) prediction APIs have dramatically simplified ML adoption. For example, one can use the Google speech API to transform an utterance to a text paragraph, or the Microsoft vision API to recognize all objects in an image. The ML-as-a-Service (MLaaS) market powered by these APIs is increasingly growing and expected to exceed $16 billion USD in the next five years [1].\\n\\nDespite its increasing popularity, systematic analysis of this MLaaS ecosystem is limited, and many phenomena are not well understood. For example, APIs from different providers can have heterogeneous performance on the same dataset. Deciding which API or combination of APIs to use on a specific dataset can be challenging. Moreover, providers can update their ML APIs due to new data availability and model architecture advancements, but users often do not know how the API's behavior on their data changes. Such API shifts can substantially affect (and hurt) the performance of downstream applications. Certain biases or stereotypes in the ML APIs [30] can also be amplified or mitigated by API shifts. Understanding the dynamics of ML APIs is critical for ensuring the reliability of the entire user pipeline, for which the API is one component. It also helps users to adjust their API usage strategies timely and appropriately. For example, one may trust a speech API's...\"}"}
{"id": "CZeIOfCjMf", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"prediction if its confidence score is higher than 90% and invoke a human expert otherwise. Suppose\\nthe API is updated so that its confidence is reduced by 10% while its prediction remains the same\\n(this happens in practice, as we will show). Then the human invocation threshold also needs to\\nbe adjusted to ensure consistent overall performance and human workload.\\n\\nOur contributions\\nIn this paper, we present HAPI (History of APIs), a longitudinal dataset of\\n1,761,417 data points annotated by a range of different ML APIs from Google, Microsoft, Amazon\\nand other providers from 2020 to 2022. This covers ML APIs for both standard classification such\\nas sentiment analysis and structured prediction tasks including multi-label image classification. We\\nhave released our dataset on the project website\\n1\\nhttp://hapi.stanford.edu/\\nand will keep updating it by querying all ML\\nAPIs every few months. To the best of our knowledge, HAPI is the first systematic dataset of ML\\nAPI applications. It is a unique resource that facilitates studies of the increasingly critical MLaaS\\necosystem. Furthermore, we use HAPI to characterize interesting findings on API shifts between\\n2020 and 2022. Our analysis shows that API shifts are common: more than 60% of the 63 evaluated\\nAPI-dataset pairs encounter performance shifts. Those API shifts lead to both accuracy improvements\\nand drops. For example, Google vision API's shift from 2020 to 2022 brings a 1% performance\\ndrop on the PASCAL dataset but a 3.7% improvement on the MIR data. Interestingly, the fraction\\nof changed predictions is often larger than the accuracy change, indicating that an API update may\\nfix certain mistakes but introduce additional errors. ML APIs' confidence scores can also change\\neven if the predictions do not. For example, from 2020 to 2021, the average confidence score of\\nthe Microsoft speech API increased by 30% while its accuracy became lower; in contrast, IBM\\nAPI's confidence dropped by 1% but its accuracy actually improved. We also observe that subgroup\\nperformance disparity produced by different ML APIs is consistent over time. HAPI provides a rich\\nresource to stimulate research on the under-explored but increasingly important topic of MLaaS.\\n\\nRelated Work\\nTo the best of our knowledge, HAPI is the first large-scale ML API dataset. We discuss relevant\\nliterature below.\\n\\nMLaaS. MLaaS APIs\\n2\\nhttp://hapi.stanford.edu/\\n\\nMLaaS APIs [36] have been developed and sold by giant companies including Google [9]\\nand Amazon [2] as well as startups such as Face++ [6] and EPixel [5]. Many applications have been\\ndiscovered [30, 50, 64], and prior work on ML APIs has spanned on their robustness [49] and pricing\\nmechanisms [32]. One challenge in MLaaS is to determine which API or combination of them to\\nuse given a user budget constraint. This requires adaptive API calling strategies to jointly consider\\nperformance and cost, studied in recent work such as FrugalML [36] and FrugalMCT [33]. While\\nthey also released datasets of ML API predictions, their dataset only contain evaluation in one year.\\nRecent work on API shift estimation [35] evaluated a few classification ML APIs in two years. On\\nthe other hand, HAPI provides a systematical evaluation of a number of ML APIs over a couple of\\nyears and thus enables more research on ML APIs evolution over time.\\n\\nDynamics of ML systems. ML is a fast growing community [66] and the update of one component\\nmay impact an ML system significantly. For example, a recent study on dataset dynamics [55] implies\\na concentration on fewer and fewer datasets over time and thus potentially increasing biases in many\\nML systems. Various hardware optimization [68] are shown to accelerate training and inference\\nspeeds for many applications. HAPI focuses on the dynamics of ML APIs, another important\\ncomponent of many ML systems.\\n\\nML pipeline monitoring and assessments. Monitoring and assessing ML pipelines are critical in\\nreal world ML applications. Existing work studies on how to estimate the performance of a deployed\\nML model based on certain statistics such as confidence [47], rotation prediction [40] and feature\\nstatistics of the datasets sampled from a meta-dataset [41]. More general approaches exploit human\\nknowledge [37], white-box access to the ML models [31], or varying assumptions on label or feature\\ndistribution shifts [34, 38, 42]. Another line of work is identifying errors made by an ML model.\\nThis involves ML models for tabular data [29, 26] as well as multimedia data [54]. One common\\nassumption made by them is that the deployed ML models are fixed and the performance change or\\nerror emergence is due to data distribution shifts. However, our analysis on HAPI indicates that ML\\n\"}"}
{"id": "CZeIOfCjMf", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"systems powered by ML APIs may also change notably. This calls for monitoring and assessments under both model and data distribution shifts.\\n\\n3 Construction of HAPI: Tasks, Datasets, and ML APIs\\n\\nTable 1: Evaluated ML APIs. For each task, we have evaluated three popular ML APIs from different commercial providers. The valuation was conducted in the spring of 2020, 2021, and 2022 for classification tasks, and 2020 fall as well as 2022 spring for structured prediction tasks.\\n\\n| Task Type | Task   | ML API | Evaluation Period       |\\n|-----------|--------|--------|-------------------------|\\n| Classify  | SCR    | Google | March 2020, April 2021, May 2022 |\\n|           |        | Microsoft |  |\\n|           |        | IBM     |  |\\n|           | SA     | Google | March 2020, Feb 2021, May 2022 |\\n|           |        | Amazon  |  |\\n|           |        | Baidu   |  |\\n|           | FER    | Google | March 2020, Feb 2021, May 2022 |\\n|           |        | Microsoft |  |\\n|           |        | Face++  |  |\\n| Struc. Pred | MIC    | Google | October 2020, Feb 2022 |\\n|           |        | Microsoft |  |\\n|           |        | EPixel  |  |\\n|           | STR    | Google | September 2020, March 2022 |\\n|           |        | iFLYTEK |  |\\n|           |        | Tencent |  |\\n|           | NER    | Google | September 2020, March 2022 |\\n|           |        | Amazon  |  |\\n|           |        | IBM     |  |\\n\\nTable 2: Prices of ML services used for each task at their evaluation times. Price unit: USD/10,000 queries. We documented the price in 2020, 2021, and 2022 for standard classification tasks and 2020 and 2022 for structured predictions. Note that for the same task, the prices of different ML APIs are diverse. On the other hand, for a fixed ML API, its price is often stable over the past few years.\\n\\n| Task | ML API | Price 2020 | Price 2021 | Price 2022 |\\n|------|--------|------------|------------|------------|\\n| SCR  | Google | 60         | 60         | 60         |\\n|      | MS     | 41         | 41         | 41         |\\n|      | IBM    | 25         | 25         | 25         |\\n| SA   | Google | 2.5        | 2.5        | 2.5        |\\n|      | Amazon | 0.75       | 0.75       | 0.75       |\\n|      | Baidu  | 3.5        | 3.6        | 3.7        |\\n| FER  | Google | 15         | 15         | 15         |\\n|      | MS     | 10         | 10         | 10         |\\n|      | Face++ | 5          | 5          | 5          |\\n| MIC  | Google | 15         | 15         | 15         |\\n|      | MS     | 10         | 10         | 10         |\\n|      | EPixel | 6          | 6          | 6          |\\n| STR  | Google | 15         | 15         | 15         |\\n|      | iFLYTEK| 50         | 52         | 52         |\\n|      | Tencent| 210        | 210        | 210        |\\n| NER  | Google | 10         | 10         | 10         |\\n|      | Amazon | 3          | 3          | 3          |\\n|      | IBM    | 30         | 30         | 30         |\\n\\nLet us first introduce HAPI, a longitudinal dataset for ML prediction APIs. To assess ML APIs comprehensively, we designed HAPI to include evaluations of (i) a large set of popular commercial ML APIs for (ii) diverse tasks (iii) on a range of standard benchmark datasets (iv) across multiple years. For (ii), we consider six different tasks in two categories: standard classification tasks including spoken command recognition (SCR), sentiment analysis (SA), and facial emotion recognition (FER), and structured predictions including multi-label image classification (MIC), scene text recognition (STR), and named entity recognition (NER). To achieve (i) and (iv), we have evaluated three different APIs from leading companies for each task from 2020 to 2022, summarized in Table 1. Specifically, we have evaluated all classification APIs in the spring of 2020, 2021, and 2022, separately, and all structured prediction APIs in 2020 fall and 2022 spring respectively. The prices of all evaluated ML APIs are presented in Table 2. Note that, for any fixed task, the prices of different ML APIs vary in a large range. This implies selection of different ML APIs may impact the dollar cost of a downstream application. Interestingly, for a fixed ML API, there is almost no change in its price over the past few years. We will also continuously evaluate those APIs and update HAPI in the future.\\n\\nWhat remains is on which datasets the ML APIs have been evaluated. To ensure (iii), we choose four commonly-used benchmark datasets for each classification task, and three datasets for each structured prediction task. The dataset statistics are summarized in Table 3. Note that those datasets are diverse in their size and number of labels, and thus we hope they can represent a large range of real world ML API use cases. Some datasets come with additional meta data. For example, the speaker accents are available for the spoken command dataset DIGIT. Such information can be used to study how an ML API's bias changes over time. We leave more details in the appendix.\"}"}
{"id": "CZeIOfCjMf", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Datasets used to evaluate classification APIs (in tasks SCR, SA, FER) and structured prediction APIs (in tasks MIC, STR, NER). We queried each dataset on all three APIs that are relevant for that task.\\n\\n| Task                          | Dataset   | Size  | # Labels |\\n|-------------------------------|-----------|-------|----------|\\n| Speech Command Recognition    | DIGIT     | 2000  | 10       |\\n|                               | AMNIST    | 30000 | 10       |\\n|                               | CMD       | 64727 | 31       |\\n|                               | FLUENT    | 30043 | 31       |\\n| Sentiment Analysis            | IMDB      | 25000 | 2        |\\n|                               | YELP      | 20000 | 2        |\\n|                               | W AIMAI   | 11987 | 2        |\\n|                               | SHOP      | 62774 | 2        |\\n| Facial Emotion Recognition    | FER+      | 6358  | 7        |\\n|                               | RAFDB     | 15339 | 7        |\\n|                               | EXPW      | 31510 | 7        |\\n|                               | AFNET     | 287401| 7        |\\n| Multi-label Image Classification| PASCAL   | 11540 | 20       |\\n|                               | MIR       | 25000 | 25       |\\n|                               | COCO      | 123287| 80       |\\n| Scene Text Recognition        | MTWI      | 9742  | 4404     |\\n|                               | ReCTS     | 20000 | 4134     |\\n|                               | LSVT      | 30000 | 4852     |\\n| Named Entity Recognition      | CONLL     | 10898 | 9910     |\\n|                               | GMB       | 47830 | 14376    |\\n|                               | ZHNER     | 16915 | 4375     |\\n\\nThe output formats of different ML APIs are often different. For example, Google API generates a Google client object for each input data while Everypixel API simply returns a dictionary. To mitigate such heterogeneity, we propose a simple abstraction to represent an ML API's output. Given each data point \\\\(x\\\\) and evaluation time \\\\(t\\\\), a classification ML API's output is (i) a predicted label \\\\(f(x, t)\\\\) and (ii) the associated confidence score \\\\(q(x, t)\\\\). For structured prediction tasks, the output includes (i) a set of predicted labels \\\\(f(x, t)\\\\) (ii) associated with their quality scores \\\\(q(x, t)\\\\). For each ML API and dataset pair, we recorded the API's prediction \\\\(f(x, t)\\\\) and \\\\(q(x, t)\\\\) at each evaluation time. We also include the true label \\\\(y\\\\) for each dataset.\\n\\nAs a result, HAPI consists of 1,761,417 data samples from various tasks and datasets annotated by commercial ML APIs from 2020 to 2022. We provide download access on the project website, and also offer a few interesting examples for exploration purposes.\\n\\n4 Example Analyses Enabled by HAPI: Model Shifts Over Time\\n\\nWe demonstrate the utility of HAPI by showing interesting insights that we can learn from it regarding how APIs change over time. The analysis here is not meant to be exhaustive; indeed we leave many open directions of investigation and encourage the community to dive deeper using HAPI. Our preliminary analysis goal is four-fold: (i) assess whether an ML API's predictions change over time, (ii) quantify how much accuracy improvements or declines are incurred due to ML API shifts, (iii) estimate to which direction prediction confidences of the ML APIs move, and (iv) understand how an ML API's gender and race biases evolve.\\n\\n4.1 Findings on Classification APIs\\n\\nWe first study the shifts of ML APIs designed for simple classification tasks, including facial emotion recognition, sentiment analysis, and spoken command detection. To quantify shifts on classification APIs, we adopt the following metrics:\\n\\n- **Accuracy**: The percentage of correct predictions.\\n- **Precision**: The ratio of true positives to the sum of true positives and false positives.\\n- **Recall**: The ratio of true positives to the sum of true positives and false negatives.\\n- **F1 Score**: The harmonic mean of precision and recall.\\n\\nWe further analyze the confidence scores to understand how predictions change over time. The confidence score is a measure of the model's certainty in its prediction. A higher confidence score indicates a higher level of confidence in the prediction. We use the confidence score to identify any trends in the model's confidence over time.\\n\\nThe analysis reveals several interesting findings:\\n\\n- **Increasing Accuracy**: Across all APIs, we observe a trend of increasing accuracy over time. This suggests that the APIs are improving their performance as more data becomes available.\\n- **Decreasing Variability**: There is a decrease in the variability of predictions, indicating that the model is becoming more consistent in its predictions over time.\\n- **Consistent Confidence**: The confidence scores remain relatively constant over time, suggesting that the model's confidence in its predictions does not change significantly.\\n\\nThese findings provide valuable insights into the evolution of ML APIs and their accuracy over time.\"}"}
{"id": "CZeIOfCjMf", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prediction Overlap. Prediction overlap measures how often an ML API's prediction on the same input remains the same at different evaluation periods. Formally, it can be expressed as \\n\\\\[ \\\\text{PO}(t_1, t_2) \\\\triangleq \\\\frac{1}{|D|} \\\\sum_{x,y \\\\in D} \\\\{ f(x, t_1) = f(x, t_2) \\\\} \\\\]\\nHere, \\\\( t_1 \\\\) and \\\\( t_2 \\\\) are two evaluation time periods. \\\\( \\\\text{PO} = 1 \\\\) indicates an ML API's predictions do not change, and \\\\( \\\\text{PO} = 0 \\\\) means its predictions between \\\\( t_1 \\\\) and \\\\( t_2 \\\\) are completely different.\\n\\nConfidence Movement. API shifts include both prediction and confidence score changes. For a fixed data point, an ML API's prediction can remain the same, but its confidence may still move up and down over time. To measure this, we use confidence movement\\n\\\\[ \\\\text{CM}(t_1, t_2) \\\\triangleq \\\\frac{1}{|D|} \\\\sum_{x,y \\\\in D} \\\\{ f(x, t_1) = f(x, t_2) \\\\} \\\\cdot [q(x, t_1) - q(x, t_2)] \\\\]\\nIf \\\\( \\\\text{CM}(t_1, t_2) > 0 \\\\), then among all data points without prediction shifts, the evaluated ML API is more confident at time \\\\( t_1 \\\\) than at time \\\\( t_2 \\\\). If \\\\( \\\\text{CM}(t_1, t_2) < 0 \\\\), then on average, the API's confidence is less confident at time \\\\( t_1 \\\\) than at time \\\\( t_2 \\\\). It is worth noting that many applications are sensitive to confidence changes. For example, a customer review application may trust an ML API's prediction if its confidence is larger than a threshold, and involve a human expert otherwise. Even if all predictions stay the same, the confidence change over time may still mitigate or worsen the human expert's workload.\\n\\nModel Accuracy. One of the most widely adopted ML API assessments is accuracy, i.e., how often the ML API makes the right prediction. Given a dataset \\\\( D \\\\) and the label prediction \\\\( f(\\\\cdot, t) \\\\) by an ML API evaluated at time \\\\( t \\\\), accuracy is simply\\n\\\\[ a(t) \\\\triangleq \\\\frac{1}{|D|} \\\\sum_{x,y \\\\in D} \\\\{ f(x, t) = y \\\\} \\\\]\\nThus, it is natural to quantify how the accuracy of an ML API changes over time.\\n\\nGroup Disparity. Various metrics [24, 39, 58] have been proposed to quantify ML fairness. In this paper, we adopt one common metric called group disparity [39]. Suppose the dataset \\\\( D \\\\) is partitioned into \\\\( K \\\\) groups \\\\( D_1, D_2, \\\\cdots, D_K \\\\) by some sensitive feature (e.g., gender or race). Then group disparity is\\n\\\\[ \\\\text{GD}(t) \\\\triangleq \\\\max_{i} \\\\frac{1}{|D_i|} \\\\sum_{x,y \\\\in D_i} \\\\{ f(x, t) = y \\\\} - \\\\min_{i} \\\\frac{1}{|D_i|} \\\\sum_{x,y \\\\in D_i} \\\\{ f(x, t) = y \\\\} \\\\]\\nIn a nutshell, group disparity measures the accuracy difference between the most privileged group and the most disadvantaged group. Larger group disparity implies more unfairness, and \\\\( \\\\text{GD}(t) = 0 \\\\) implies the API achieves perfect fairness at time \\\\( t \\\\).\\n\\nA case study on DIGIT. We start with a case study on a spoken command recognition dataset, DIGIT [4]. DIGIT contains 2,000 short utterances corresponding to digits from 0 to 9, and the task is to predict which number each utterance indicates. We have evaluated three speech recognition APIs from IBM, Google, and Microsoft in year 2020, 2021, and 2022, separately. The utterances were spoken by people with US accent, French accent, and German accent. Thus, we use accent as the sensitive feature to group the data instances and then measure the group disparity.\\n\\nAs shown in Figure 1, there are many interesting observations in this case study. First, the accuracy changes are substantial: for example, as shown in Figure 1(a), Google API's accuracy increased by 20% from 2020 to 2021. The prediction changes are even more significant: For example, from 2020 to 2021, IBM API's accuracy rose by 4% (see Figure 1(a), but \\\\( 1 - 79.4\\\\% = 20.6\\\\% \\\\) predictions by IBM API were changed (see Figure 1(b)). This is perhaps because while some mistakes were fixed by the API update, some utterances previously correctly predicted may be predicted incorrectly by the updated version. Even when the predictions remain steady, the confidence score can still significantly move up or down. For example, the confidence produced by Microsoft API moved up by 31.7% from 2020 to 2021 (as shown in Figure 1(b)). Yet, its accuracy dropped by 1.5% (as shown in Figure 5).\"}"}
{"id": "CZeIOfCjMf", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 1: A case study on the dataset DIGIT. (a): accuracy over time. (b): prediction overlap and confidence movement of IBM, Google, and Microsoft APIs. (c): group disparity with respect to speaker accent. Overall, accuracy changes due to API shifts are notable, but the prediction changes are even more significant. For example, from 2020 to 2021, the accuracy of IBM API has increased by 4% (see (a)), but 20.6% predictions are actually different (see (b)). In addition, the confidence can move up by up to 31.7% (Microsoft from 2020 to 2021 in (a)) while the prediction accuracy slightly drops (Microsoft in (b)). This calls for cautions in confidence-sensitive applications. It is also worth noting that large group disparity exists for all evaluated APIs. Interestingly, API update over time may either improve or hurt overall accuracy as well as group fairness.\\n\\nDiverse API shifts across multiple classification tasks. Next we study API shifts across different tasks and datasets. For each API dataset pair, we calculate the prediction overlap and confidence movement between each evaluation time pair (2020\u20132021, 2020\u20132022, 2021\u20132022) and then report the results averaged over all time pairs. We also measure and compare its accuracy for each year. The results are shown in Figure 2.\\n\\nSeveral interesting findings exist. First, small accuracy changes may be the result of large prediction shifts, i.e., small prediction overlaps. For example, about 10% predictions made by Amazon sentiment analysis API on IMDB (as shown in Figure 2(b1)) have changed, but its accuracy only changes by about 1% (Figure 2(a1)). Similarly, a 3% prediction difference exists for Microsoft API on RAFDB (Figure 2(b3)) while there is almost no change in its accuracy (Figure 2(a3)). This indicates general phenomena in API shifts: many API updates fix certain errors but also make additional mistakes.\\n\\nNext, we note that the API shifts are diverse. For spoken command recognition, all evaluated APIs\u2019 predictions are changed significantly (Figure 2(b1)). However, the shifts in APIs for facial emotion recognition is almost negligible (Figure 2(b3)). This implies that different APIs may be updated in a different rate and thus detecting whether a shift may have happened is useful. Moreover, different APIs\u2019 confidence movements are not similar. Sometimes an ML API tends to be more and more conservative: for example, the average confidences of Google API for spoken command recognition have dropped notably for all evaluated datasets. Sometimes an ML API becomes more and more confident: for example, Microsoft API for spoken recognition has increased its confidence over time on three out of four datasets (Figure 2(c1)). More interestingly, its confidence may also depend on a dataset\u2019s property: as shown in Figure 2(c2), Amazon sentiment analysis API tends to be less confident on Chinese texts (WAIMAI and SHOP) but more confident on English texts (IMDB and YELP). Understanding how the confidence moves may help decision making in confidence-sensitive applications. We provide additional group disparity analysis in the appendix.\"}"}
{"id": "CZeIOfCjMf", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Summary on classification API shifts from 2020 to 2022. Tables on row 1, 2 and 3 correspond to spoken command recognition, sentiment analysis, and facial emotion recognition, respectively. (a1)-(a3): Accuracy of each year. (b1)-(b3): Average prediction overlap. (c1)-(c3): Average confidence movement. Units: %. Red and green indicate low and high values, respectively. The accuracy changes exhibit various patterns overall, while the API shifts are also diverse: all spoken command recognition APIs\u2019 predictions have been changed significantly during the past years, while significant changes exist for only one third of the APIs for the other two tasks. Confidence movements are also interesting. For example, Google API for spoken command recognition tends to be less confident (c1), while Amazon sentiment API is more confident on Chinese texts but less on English texts (c2).\\n\\n4.2 Findings on Structured Prediction APIs\\n\\nNext we turn to the structured prediction APIs. Similar to standard classification APIs, we use prediction overlap to measure prediction changes due to shifts of structured predictions APIs. For each data instance, we use the average of all predicted labels\u2019 confidences as an overall confidence, and then still apply confidence movement to quantify how an API\u2019s confidence shifts over time. To measure structured prediction API\u2019s performance, we adopt the standard multi-label accuracy $\\\\mathcal{M} \\\\equiv \\\\frac{1}{|D|} \\\\sum_{(x,y) \\\\in D} I(f(x, t) \\\\cap y = f(x, t) \\\\cup y)$.\\n\\nFinally, we keep using group disparity to evaluate fairness of an ML API, but replace the 0-1 loss $1\\\\{f(x, t) = y\\\\}$ by the Jaccard similarity $\\\\frac{|f(x, t) \\\\cap y|}{|f(x, t) \\\\cup y|}$.\\n\\nA case study on COCO.\\n\\nWe start with a case study on the dataset COCO. COCO contains more than a hundred thousand images, and the goal is to determine if one or more objects from 80 categories show up in each image. We have evaluated three APIs from Microsoft, Google, and EPixel, respectively. To measure group disparity, we adopt the gender labels [73] for a subset of COCO which contains a person, and then calculate the group disparity on this subset for all evaluated ML APIs. The results are summarized in Figure 3.\\n\\nOur first observation is that the accuracy shift can be quite large, leading to an \u201caccuracy cross\u201d. As shown in Figure 3(a), EPixel API\u2019s accuracy drops by more than 20% while Google API\u2019s accuracy increases by 1%. Consequentially, Google API becomes more accurate than EPixel, while the latter was more accurate in 2020. This implies that API shifts can be impactful in business decision making such as picking which ML API to use. In addition, the prediction shifts are much larger than those for simple classification APIs. For example, prediction overlap for EPixel is less than 30%, meaning...\"}"}
{"id": "CZeIOfCjMf", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: A case study on the dataset COCO. (a): accuracy over time. (b): prediction overlap (%) and confidence movement (%). (c): group disparity with respect to gender. Here, the accuracy change is quite significant. E.g., EPixel API update leads to 20% accuracy drop (as shown in (a)). Prediction shifts are also large: prediction overlap can be less than 30% (as shown in (b)). The confidence movement is relatively small. It is also worth noting that high accuracy does not imply better fairness. In fact, Microsoft API's accuracy is the highest, but its group disparity is also the largest. As shown in Figure 3(a) and (c), API shifts may improve the accuracy but simultaneously amplify the group disparity: Microsoft API's accuracy increases by 10% but its group disparity is also enlarged.\\n\\nVarious API shift patterns across structured prediction tasks. Finally, we dive deeply into various API shift patterns for more structured prediction tasks. The prediction overlaps, confidence movements, and accuracy changes for 27 API-dataset pairs are summarized in Figure 4.\\n\\nThere are several interesting observations. First, the accuracy changes are significant for multi-label image classification but relatively small for the other two tasks, as shown in Figure 4(a1)-(a3). However, API shifts for structured predictions are more common than classification tasks. In fact, as shown in Figure 4(b1)-(b3), prediction changes occur for almost all ML APIs. The magnitudes of the shifts are also larger. This is perhaps because structured prediction is more sensitive to model updates than those for classifications. The confidence movement is relatively small though. Note that confidence movements do not always reflect the APIs' performance changes. For instance, EPixel API's confidence increases on all evaluated datasets, but its accuracy actually drops. This is probably because EPixel's update removes a label due to low confidence but this label was part of the true label set. Detecting, estimating, and explaining such phenomena is needed for robustly adopting ML APIs.\\n\\n5 Additional Discussions and Maintenance Plans\\n\\nMore frequent evaluations. ML APIs are increasingly growing and updated frequently. Thus, we plan to enrich our database by continuously evaluating ML APIs more frequently, i.e., every 6 months. As of 2022 August, we have collected additional predictions of all structured prediction APIs. As shown in Figure 5, significant prediction changes already occurred in 6 months. For example, the accuracy of IBM named entity API on the GMB dataset dropped from 50% (March 2022) to 45% (August 2022). Those newly collections have been added to our database. More details can be found in the appendix.\\n\\nComparison with open-source ML models. As a baseline, we have also measured the performance of several open source ML models on all datasets. As shown in Table 4, the open source ML models' performance varies across different datasets, and can be sometimes better than that of commercial APIs. This further emphasizes the importance of monitoring commercial APIs' performance.\\n\\nExpansion of Datasets and ML APIs. Part of the future plan is to expand the scope of datasets and ML APIs. To allow this, we plan to solicit needs from the ML communities: a poll panel will be created on our website, and ML researchers, engineers, and domain experts are all welcome to vote.\"}"}
{"id": "CZeIOfCjMf", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Summary on structured prediction API shifts. Row 1, 2, and 3 correspond to multi-label image classification, scene text recognition and named entity recognition. The left, middle, and right column correspond to accuracy changes (%), prediction overlaps (%), and confidence movements (%), respectively, between 2020 and 2022. The accuracy change is large for multi-label image classification but relatively small for the other two tasks. Predicted labels change notably for many of the evaluated ML APIs. The confidence movement is relatively small, though.\\n\\n6 Conclusions and Open Questions\\n\\nML APIs play an increasingly important role in real world ML adoptions and applications, but there are only a limited number of papers studying the properties and dynamics of these commercial APIs. In this paper, we introduce HAPI, a large scale dataset consisting of samples from various tasks annotated by ML APIs over multiple years. Our analysis on HAPI shows interesting findings, including large price gaps among APIs for the same task, prevalent ML API shifts between 2020 and 2022, diverse performance differences between API vendors, and consistent subgroup performance disparity. And this is just scratching the surface. HAPI enables many interesting questions to be studied in the ML marketplaces. A few examples include:\\n\\n\u2022 How to determine which API or combination of APIs to use for any given application? HAPI can serve as a testbed to evaluate and compare different API calling strategies.\\n\\n\u2022 How to perform unsupervised or semi-supervised performance estimation under ML API shifts? This is useful for practical ML monitoring but not possible without a detailed ML API benchmark over time provided by HAPI.\\n\\n\u2022 Generally, how to estimate performance shifts when both ML APIs and data distributions shift?\\n\\n\u2022 How to explain the performance gap due to ML API shifts? More fine-grained understanding of how the API\u2019s prediction behavior changes over time would be useful for practitioners.\"}"}
{"id": "CZeIOfCjMf", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: API Shifts within 6 months. (a) and (b) correspond to the GMB and ReCTS datasets, respectively. Overall, significant prediction and accuracy occurred in 3 out of 6 ML APIs.\\n\\nTable 4: Performance of open source models on the evaluated datasets. For some tasks, open source models' performance can be even better than that of the commercial APIs.\\n\\n| Task                    | Open source model | Dataset       | Performance |\\n|-------------------------|-------------------|---------------|-------------|\\n| Speech Recognition      | DeepSpeech [23]   | DIGIT         | 0.60        |\\n|                         |                   | AMNIST        | 0.92        |\\n|                         |                   | CMD           | 0.80        |\\n|                         |                   | FLUENT        | 0.87        |\\n|                         |                   | PASCAL        | 0.64        |\\n|                         |                   | MIR           | 0.25        |\\n|                         |                   | COCO          | 0.40        |\\n| Multi-label Image Class | SSD [18]          |               |             |\\n| Sentiment Analysis      | Vader [52]        | IMDB          | 0.69        |\\n|                         |                   | YELP          | 0.75        |\\n|                         |                   | W AIMAI       | 0.64        |\\n|                         |                   | SHOP          | 0.78        |\\n|                         |                   | MTWI          | 0.63        |\\n|                         |                   | ReCTS         | 0.51        |\\n|                         |                   | LSVT          | 0.47        |\\n| Scene Text Recognition  | PP-OCR [43]       |               |             |\\n| Facial Emotion Recognition | A convolution neural network [15] | FER+ | 0.77 |\\n|                         |                   | RAFDB         | 0.60        |\\n|                         |                   | EXPW          | 0.56        |\\n|                         |                   | AFNET         | 0.64        |\\n|                         |                   | CONLL         | 0.53        |\\n|                         |                   | GMB           | 0.55        |\\n|                         |                   | ZHNER         | 0.63        |\\n| Named Entity Recognition | Spacy [17]       |               |             |\\n\\nThese and other open questions enabled by HAPI are increasingly critical with the growth of ML-as-a-service. HAPI can greatly stimulate more research on ML marketplace. All of the data in HAPI is openly available at [http://hapi.stanford.edu/](http://hapi.stanford.edu/).\\n\\nAcknowledgement\\n\\nThis project is supported in part by NSF CCF 1763191, NSF CAREER Award 1651570, and NSF CAREER Award 1942926. There is no industrial funding. We appreciate the anonymous reviewers for their invaluable feedback.\"}"}
{"id": "CZeIOfCjMf", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Machine Learning as a Service Market Report. https://www.mordorintelligence.com/industry-reports/global-machine-learning-as-a-service-mlaas-market. [Accessed May-2022].\\n\\n[2] Amazon Comprehend API. https://aws.amazon.com/comprehend.\\n\\n[3] Baidu API. https://ai.baidu.com/.\\n\\n[4] DIGIT dataset, https://github.com/Jakobovski/free-spoken-digit-dataset. [Accessed Feb-2020].\\n\\n[5] Everypixel (EPixel) Image Tagging API. https://labs.everypixel.com/api.\\n\\n[6] Face++ Emotion API. https://www.faceplusplus.com/emotion-recognition/.\\n\\n[7] Google NLP API. https://cloud.google.com/natural-language.\\n\\n[8] Google Speech API. https://cloud.google.com/speech-to-text.\\n\\n[9] Google Vision API. https://cloud.google.com/vision.\\n\\n[10] IBM NLP API. https://www.ibm.com/cloud/watson-natural-language-understanding.\\n\\n[11] IBM Speech API. https://cloud.ibm.com/apidocs/speech-to-text.\\n\\n[12] iFLYTEK Text Recognition API. https://global.xfyun.cn/products/wordRecg.\\n\\n[13] Microsoft computer vision API. https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision.\\n\\n[14] Microsoft speech API. https://azure.microsoft.com/en-us/services/cognitive-services/speech-to-text.\\n\\n[15] Pretrained facial emotion model from GitHub. https://github.com/WuJie1010/Facial-Expression-Recognition.Pytorch. [Accessed March-2020].\\n\\n[16] SHOP dataset, https://github.com/SophonPlus/ChineseNlpCorpus/tree/master/datasets/online_shopping_10_cats. [Accessed Feb-2020].\\n\\n[17] spaCy, a named entity recognition tool from GitHub. https://github.com/explosion/spaCy. [Accessed Oct-2020].\\n\\n[18] SSD, a multi-label image classification tool from GitHub/TensorflowHub. https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1. [Accessed Oct-2020].\\n\\n[19] Tencent Text Recognition API. https://intl.cloud.tencent.com/product/ocr.\\n\\n[20] WAIMAI dataset, https://github.com/SophonPlus/ChineseNlpCorpus/tree/master/datasets/waimai_10k. [Accessed Feb-2020].\\n\\n[21] YELP dataset, https://www.kaggle.com/yelp-dataset/yelp-dataset. [Accessed Feb-2020].\\n\\n[22] ZHNER dataset. https://github.com/zjy-ucas/ChineseNER/tree/master/data. [Accessed Feb-2020].\\n\\n[23] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse H. Engel, Linxi Fan, Christopher Fougner, Awni Y . Hannun, Billy Jun, Tony Han, Patrick LeGresley, Xiangang Li, Libby Lin, Sharan Narang, Andrew Y . Ng, Sherjil Ozair, Ryan Prenger, Sheng Qian, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Chong Wang, Yi Wang, Zhiqian Wang, Bo Xiao, Yan Xie, Dani Yogatama, Jun Zhan, and Zhenyao Zhu. Deep speech 2 : End-to-end speech recognition in english and mandarin. In ICML 2016.\\n\\n[24] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness in machine learning. Nips tutorial, 1:2, 2017.\\n\\n[25] Emad Barsoum, Cha Zhang, Cristian Canton Ferrer, and Zhengyou Zhang. Training deep networks for facial expression recognition with crowd-sourced label distribution. In Proceedings of the 18th ACM International Conference on Multimodal Interaction, pages 279\u2013283, 2016.\"}"}
{"id": "CZeIOfCjMf", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Denis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, et al. Tfx: A tensorflow-based production-scale machine learning platform. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1387\u20131395, 2017.\\n\\nS\u00f6ren Becker, Marcel Ackermann, Sebastian Lapuschkin, Klaus-Robert M\u00fcller, and Wojciech Samek. Interpreting and explaining deep neural networks for classification of audio signals. CoRR, abs/1807.03418, 2018.\\n\\nJohan Bos. The groningen meaning bank. In Proceedings of the Joint Symposium on Semantic Processing, page 2. ACL, 2013.\\n\\nEric Breck, Neoklis Polyzotis, Sudip Roy, Steven Whang, and Martin Zinkevich. Data validation for machine learning. In Proceedings of Machine Learning and Systems, 2019.\\n\\nJoy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on Fairness, Accountability and Transparency, volume 81, pages 77\u201391. PMLR, 2018.\\n\\nJiefeng Chen, Frederick Liu, Besim Avci, Xi Wu, Yingyu Liang, and Somesh Jha. Detecting errors and estimating accuracy on unlabeled data with self-training ensembles. In Advances in Neural Information Processing Systems, volume 34, pages 14980\u201314992, 2021.\\n\\nLingjiao Chen, Paraschos Koutris, and Arun Kumar. Towards model-based pricing for machine learning in a data marketplace. In Proceedings of the International Conference on Management of Data, pages 1535\u20131552, 2019.\\n\\nLingjiao Chen, Matei Zaharia, and James Zou. Efficient online ml api selection for multi-label classification tasks. In International Conference on Machine Learning, pages 3716\u20133746. PMLR, 2022.\\n\\nLingjiao Chen, Matei Zaharia, and James Zou. Estimating and explaining model performance when both covariates and labels shift. arXiv preprint arXiv:2209.08436, 2022.\\n\\nLingjiao Chen, Matei Zaharia, and James Y. Zou. How did the model change? efficiently assessing machine learning API shifts. In International Conference on Learning Representations, 2022.\\n\\nLingjiao Chen, Matei Zaharia, and James Zou. Frugalml: How to use ML prediction apis more accurately and cheaply. In Advances in Neural Information Processing Systems, volume 33, pages 10685\u201310696, 2020.\\n\\nMayee Chen, Karan Goel, Nimit S Sohoni, Fait Poms, Kayvon Fatahalian, and Christopher R\u00e9. Mandoline: Model evaluation under distribution shift. In Proceedings of the 38th International Conference on Machine Learning, pages 1617\u20131629. PMLR, 2021.\\n\\nChing-Yao Chuang, Antonio Torralba, and Stefanie Jegelka. Estimating generalization under distribution shifts via domain-invariant representations. In Proceedings of the 37th International Conference on Machine Learning, volume 119, pages 1984\u20131994. PMLR, 2020.\\n\\nSam Corbett-Davies and Sharad Goel. The measure and mismeasure of fairness: A critical review of fair machine learning. arXiv preprint arXiv:1808.00023, 2018.\\n\\nWeijian Deng, Stephen Gould, and Liang Zheng. What does rotation prediction tell us about classifier accuracy under varying testing environments? In Proceedings of the 38th International Conference on Machine Learning, volume 139, pages 2579\u20132589. PMLR, 2021.\\n\\nWeijian Deng and Liang Zheng. Are labels always necessary for classifier accuracy evaluation? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15069\u201315078, 2021.\\n\\nPinar Donmez, Guy Lebanon, and Krishnakumar Balasubramanian. Unsupervised supervised learning i: Estimating classification and regression errors without labels. Journal of Machine Learning Research, 11(4):1323\u20131351, 2010.\\n\\nYuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei Liu, Jun Zhou, Yifan Bai, Zilin Yu, Yehua Yang, Qingqing Dang, et al. Pp-ocr: A practical ultra lightweight ocr system. arXiv preprint arXiv:2009.09941, 2020.\\n\\nMark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and Andrew Zisserman. The pascal visual object classes challenge: A retrospective. International Journal of Computer Vision, 111(1):98\u2013136, 2015.\"}"}
{"id": "CZeIOfCjMf", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[46] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64(12):86\u201392, 2021.\\n\\n[47] Devin Guillory, Vaishaal Shankar, Sayna Ebrahimi, Trevor Darrell, and Ludwig Schmidt. Predicting with confidence on unseen distributions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1134\u20131144, 2021.\\n\\n[48] Mengchao He, Yuliang Liu, Zhibo Yang, Sheng Zhang, Canjie Luo, Feiyu Gao, Qi Zheng, Yongpan Wang, Xin Zhang, and Lianwen Jin. ICPR2018 contest on robust reading for multi-type web images. In 24th International Conference on Pattern Recognition, pages 7\u201312. IEEE Computer Society, 2018.\\n\\n[49] Hossein Hosseini, Baicen Xiao, and Radha Poovendran. Google\u2019s cloud vision API is not robust to noise. In Proceedings of the 16th IEEE International Conference on Machine Learning and Applications, pages 101\u2013105, 2017.\\n\\n[50] Hossein Hosseini, Baicen Xiao, and Radha Poovendran. Studying the live cross-platform circulation of images with computer vision API: An experiment based on a sports media event. International Journal of Communication, 13:1825\u20131845, 2019.\\n\\n[51] Mark J. Huiskes and Michael S. Lew. The MIR flickr retrieval evaluation. In Proceedings of the 1st ACM SIGMM International Conference on Multimedia Information Retrieval, pages 39\u201343, 2008.\\n\\n[52] Clayton J. Hutto and Eric Gilbert. VADER: A parsimonious rule-based model for sentiment analysis of social media text. In ICWSM 2014.\\n\\n[53] Mika Juuti, Sebastian Szyller, Samuel Marchal, and N Asokan. Prada: protecting against dnn model stealing attacks. In 2019 IEEE European Symposium on Security and Privacy (EuroS&P), pages 512\u2013527. IEEE, 2019.\\n\\n[54] Daniel Kang, Deepti Raghavan, Peter Bailis, and Matei Zaharia. Model assertions for monitoring and improving ml models. In Proceedings of Machine Learning and Systems, volume 2, pages 481\u2013496, 2020.\\n\\n[55] Bernard Koch, Emily Denton, Alex Hanna, and Jacob G Foster. Reduced, reused and recycled: The life of a dataset in machine learning research. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, 2021.\\n\\n[56] Shan Li, Weihong Deng, and JunPing Du. Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2584\u20132593, 2017.\\n\\n[57] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In European Conference on Computer Vision, volume 8693, pages 740\u2013755. Springer, 2014.\\n\\n[58] Lydia T Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair machine learning. In International Conference on Machine Learning, pages 3150\u20133158. PMLR, 2018.\\n\\n[59] Loren Lugosch, Mirco Ravanelli, Patrick Ignoto, Vikrant Singh Tomar, and Yoshua Bengio. Speech model pre-training for end-to-end spoken language understanding. In Proceedings of the 20th Annual Conference of the International Speech Communication Association, pages 814\u2013818, 2019.\\n\\n[60] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 142\u2013150, 2011.\\n\\n[61] Ali Mollahosseini, Behzad Hasani, and Mohammad H. Mahoor. Affectnet: A database for facial expression, valence, and arousal computing in the wild. IEEE Transactions on Affective Computing, 10(1):18\u201331, 2019.\\n\\n[62] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff nets: Stealing functionality of black-box models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4954\u20134963, 2019.\"}"}
{"id": "CZeIOfCjMf", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Prediction poisoning: Towards defenses against dnn model stealing attacks. arXiv preprint arXiv:1906.10908, 2019.\\n\\nArs\u00e9nio Reis, Dennis Paulino, V\u00edtor Filipe, and Jo\u00e3o Barroso. Using online artificial vision services to assist the blind - an assessment of microsoft cognitive services and google cloud vision. In World Conference on Information Systems and Technologies, volume 746, pages 174\u2013184. Springer, 2018.\\n\\nErik F. Tjong Kim Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning, pages 142\u2013147. ACL, 2003.\\n\\nJ\u00fcrgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:85\u2013117, 2015.\\n\\nYipeng Sun, Dimosthenis Karatzas, Chee Seng Chan, Lianwen Jin, Zihan Ni, Chee Kheng Chng, Yuliang Liu, Canjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, and Jingtuo Liu. ICDAR 2019 competition on large-scale street view text with partial labeling - RRC-LSVT. In International Conference on Document Analysis and Recognition, pages 1557\u20131562. IEEE, 2019.\\n\\nVivienne Sze, Yu-Hsin Chen, Joel Emer, Amr Suleiman, and Zhengdong Zhang. Hardware for machine learning: Challenges and opportunities. In 2017 IEEE Custom Integrated Circuits Conference (CICC), pages 1\u20138. IEEE, 2017.\\n\\nFlorian Tram\u00e8r, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. Stealing machine learning models via prediction {APIs}. In 25th USENIX security symposium (USENIX Security 16), pages 601\u2013618, 2016.\\n\\nPete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. CoRR, abs/1804.03209, 2018.\\n\\nRui Zhang, Mingkun Yang, Xiang Bai, Baoguang Shi, Dimosthenis Karatzas, Shijian Lu, C. V . Jawahar, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan Li, Kai Zhou, Lei Wang, Dong Wang, and Minghui Liao. ICDAR 2019 robust reading challenge on reading chinese text on signboard. In International Conference on Document Analysis and Recognition, pages 1577\u20131581. IEEE, 2019.\\n\\nZhanpeng Zhang, Ping Luo, Chen-Change Loy, and Xiaoou Tang. Learning social relation traits from face images. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3631\u20133639, 2015.\\n\\nDora Zhao, Angelina Wang, and Olga Russakovsky. Understanding and evaluating racial biases in image captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14830\u201314840, 2021.\\n\\nChecklist\\n1. For all authors...\\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] See Abstract and Section 1.\\n(b) Did you describe the limitations of your work? [Yes] See Section 6.\\n(c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section 6.\\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\"}"}
{"id": "CZeIOfCjMf", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n\\n(a) If your work uses existing assets, did you cite the creators? [Yes] See Section 3.\\n\\n(b) Did you mention the license of the assets? [N/A]\\n\\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes] See https://github.com/lchen001/HAPI/.\\n\\n(d) Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? [N/A]\\n\\n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\\n\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\"}"}
