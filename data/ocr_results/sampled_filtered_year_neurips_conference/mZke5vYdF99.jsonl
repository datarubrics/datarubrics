{"id": "mZke5vYdF99", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OLIVES Dataset: Ophthalmic Labels for Investigating Visual Eye Semantics\\n\\nMohit Prabhushankar\\\\(^1\\\\), Kiran Kokilepersaud\\\\(^1\\\\)\\\\(^\\\\ast\\\\), Yash-yee Logan\\\\(^1\\\\)\\\\(^\\\\ast\\\\), Stephanie Trejo Corona\\\\(^2\\\\)\\\\(^\\\\ast\\\\), Ghassan AlRegib\\\\(^1\\\\), and Charles Wykoff\\\\(^2\\\\)\\n\\n\\\\(^1\\\\)OLIVES at the Centre for Signal and Info. Processing, Georgia Tech, Atlanta, GA 30332, USA\\n\\\\(^2\\\\)Retina Consultants Texas, Retina Consultants of America, Houston, Texas 77030, USA\\n\\n\\\\{mohit.p, kpk6, ylogan3, alregib\\\\}@gatech.edu, \\\\{stephanie.trejo, ccwmd\\\\}@retinaconsultantstexas.com\\n\\nAbstract\\nClinical diagnosis of the eye is performed over multifarious data modalities including scalar clinical labels, vectorized biomarkers, two-dimensional fundus images, and three-dimensional Optical Coherence Tomography (OCT) scans. Clinical practitioners use all available data modalities for diagnosing and treating eye diseases like Diabetic Retinopathy (DR) or Diabetic Macular Edema (DME). Enabling usage of machine learning algorithms within the ophthalmic medical domain requires research into the relationships and interactions between all relevant data over a treatment period. Existing datasets are limited in that they neither provide data nor consider the explicit relationship modeling between the data modalities. In this paper, we introduce the Ophthalmic Labels for Investigating Visual Eye Semantics (OLIVES) dataset that addresses the above limitation. This is the first OCT and near-IR fundus dataset that includes clinical labels, biomarker labels, disease labels, and time-series patient treatment information from associated clinical trials. The dataset consists of 1268 near-IR fundus images each with at least 49 OCT scans, and 16 biomarkers, along with 4 clinical labels and a disease diagnosis of DR or DME. In total, there are 96 eyes' data averaged over a period of at least two years with each eye treated for an average of 66 weeks and 7 injections. We benchmark the utility of OLIVES dataset for ophthalmic data as well as provide benchmarks and concrete research directions for core and emerging machine learning paradigms within medical image analysis.\\n\\n1 Introduction\\nOphthalmology refers to the branch of medical science that deals with the structure, functions, diseases, and treatments of the eye. A stylized version of the diagnostic and treatment process for a known disease is shown in Fig. 1. A patient's visit to a clinic is met with an assessment that includes visual acuity tests and collecting demographic information. This provides Best Corrected Visual Acuity (BCVA) scores, Patient ID, and Eye ID among other data. We term these as clinical labels.\\n\\nNext, the patient undergoes diagnostic imaging that includes Fundus and OCT scans. Finally, a trained practitioner interprets the diagnostic scans for known biomarkers for diseases. The authors in (\\\\(^1\\\\)) describe biomarkers as objective indicators of medically quantifiable characteristics of biological processes which are often diseases. The biomarkers along with the scans and clinical labels are used to assess the presence and severity of a patient's disease and a recommendation of a treatment is provided. If the recommendation is yes, the patient is treated and asked to visit again after a gap.\\n\\n\\\\(^\\\\ast\\\\)Equal Contribution\\n\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks.\"}"}
{"id": "mZke5vYdF99", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A number of Machine Learning (ML) techniques have sought to either automate or interpret individual processes within Fig. 1. We annotate three such ML research directions within the pipeline for clinically aiding and monitoring disease diagnosis and treatment. The first direction involves assessing multi-modal data for clinical applications including predicting disease states. The second direction is an interpretation of biomarkers. Biomarkers act as intermediary data between medical scans and disease diagnosis that aid clinical reasoning. The last direction is analyzing time-series treatment data across the treatment period. This direction aids initial treatment prescription and patient monitoring. To the best of our knowledge, no existing dataset provides access to data that promotes all three stated research directions for the clinical process from Fig. 1. In this paper, we introduce the Ophthalmic Labels for Investigating Visual Eye Semantics (OLIVES) dataset that provides structured and curated data to promote holistic clinical research in ML for ophthalmic diagnosis.\\n\\nClinical studies for OLIVES dataset\\n\\nThe OLIVES dataset is derived from the PRIME (2) and TREX DME (3; 4; 5; 6) clinical studies. Both the studies are prospective randomized clinical trials that were run between December 2013 and April 2021 at the Retina Consultants of Texas (Houston, TX, USA). Prospective trials refer to studies that evaluate the outcome of a particular disease during treatment. PRIME evaluates Diabetic Retinopathy (DR) and TREX-DME evaluates Diabetic Macular Edema (DME). The trials provide access to near-IR fundus images and OCT scans along with de-identified Electronic Medical Records (EMR) data of 96 patients across an average of 66 weeks. Biomarkers are retrospectively added to this data by experienced graders upon open adjudication.\\n\\nChallenging dataset for ML research\\n\\nWhile challenges in natural images are generally contrived by intervening on top of data (7; 8; 9), the complexities in ophthalmic datasets arise because of issues in data collection, inversion, representation and annotation (10). OLIVES data modalities range from 1-dimensional numerical values (BCV A, Patient ID), vectorized biomarkers, 2-dimensional fundus images, and 3-dimensional scans (optical coherence tomography). Moreover, some of this data is objectively measured through instruments from patients (fundus, OCT), subjectively collected through eye tests (BCV A), while other data is interpreted and openly adjudicated through images (biomarkers). The variation within scans between visits can be minimal while the difference in manifestation of the same disease between patients may be substantial. This is shown in Fig. 2. The domain difference between OCT scans can arise due to pathology manifestation between patients (Fig. 2a and Fig. 2b), clinical labels (Fig. 2c), and the visit along the treatment process when the scan is taken (Fig. 2d). OLIVES provides access to these challenging data modalities that allow for innovative ML algorithms.\"}"}
{"id": "mZke5vYdF99", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: An illustration of some challenges within the dataset. In a) variation in OCT belonging to the same patient at different visits is minimal. In b) varied disease manifestations are among different patients. c) At times CST values are very similar but the OCT is visually dissimilar. d) Shows that CST values gradually decrease as time progresses.\\n\\nContributions and significance of the dataset\\n\\nThe **OLIVES** dataset is curated to foster research in ophthalmic ML. The retrospective additions to the **OLIVES** dataset from its base clinical trials and its ensuing contributions include:\\n\\n1. Sixteen biomarker labels are added to the OCT scans of every first and last visit of all patients. We experimentally validate the necessity of biomarkers and provide benchmarks in Sections 4.1 and 4.3. Along with biomarkers, **OLIVES** provides access to fundus, OCT scans, clinical labels and DR/DME diagnosis, thereby creating an ideal benchmarking mechanism for ophthalmic ML.\\n\\n2. We curate the clinical labels that have known correlations between the four data modalities. These include Best Corrected Visual Acuity (BCVA), Central Subfield Thickness (CST), Patient ID, and Eye ID. We demonstrate its utility for medically-grounded contrastive learning where augmentations are based on contrasting between clinical labels in Section 4.2. Hence, **OLIVES** dataset promotes research in core and emerging ML paradigms.\\n\\n3. The data and labels are made accessible to non-medical professionals. Biomarkers act as expert-annotated and interpretable visual indicators of diseases within OCT scans. The original labels from the clinical trials along with their data sheets are provided in Appendix B.5.2. Additionally, an ML-specific set of labels which is relevant to the three mentioned research directions in Fig. 1, is provided in Appendix D.3.\\n\\n2 Related Works\\n\\nOphthalmology datasets\\n\\nA number of publicly available ophthalmology datasets individually tackle each of the clinical modalities that exist in the **OLIVES** dataset. The authors in (11) provide a survey of 94 existing open access ophthalmic datasets. Among 54 of the 94 datasets, the underlying data is that of fundus images. 19 of the remaining datasets contain 3-dimensional OCT scans. The OCT scans provide structural information that enhances the performance of machine learning algorithms (11). Only three of the 94 considered open access datasets provide both OCT and fundus image modalities. The authors in (1) provide 650 OCT slices from a single volume. These are insufficient to leverage the data intensive machine learning algorithms to provide generalizable results. In contrast, the **OLIVES** dataset has 78,189 slices taken from 1,268 volumes. (12) provide OCT and fundus data from 50 healthy patients. However, these are all for healthy eyes and disease manifestation is not observed. Other datasets including (13) contains OCT scans for four OCT disease states: Healthy, Drusen, DME, and choroidal neovascularization (CNV). (14) and (15) introduced OCT datasets for age-related macular degeneration (AMD). (16) contains OCT scans labeled with segmentation of regions with DME. However, these datasets do not possess comprehensive clinical information or a wide range of expert-annotated biomarkers. A complete overview that considers clinical labels, biomarkers, disease labeling, and time-series analysis is provided in Tables 5 and 6.\"}"}
{"id": "mZke5vYdF99", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We refer to (11) to compare other statistics including number of image scans and applicability of existing datasets against OLIVES.\\n\\nMachine learning techniques on ophthalmic data\\n\\nA number of works have separately addressed the research directions identified in Fig. 1. The authors in (17) proposed transfer learning to screen for relative afferent pupillary defect due to lack of comprehensive data. (18) showed that transfer learning methods could be utilized to classify OCT scans based on the presence of key biomarkers. (19) introduced a dual-autoencoder framework with physician attributes to improve classification performance for OCT biomarkers. (20) expanded previous work towards segmentation of a multitude of different biomarkers and referred for different treatment decisions. Other work has demonstrated the ability to detect clinical information from OCT scans which is significant for suggesting correlations between different domains. (21) showed that a model trained entirely on OCT scans could predict the associated BCV A value. Similarly (22) showed that values such as retinal thickness could be learned from retinal fundus photos. The OLIVES dataset provides a standardized benchmark to conduct research across applications, data modalities and machine learning paradigms.\\n\\nTable 1: High-level overview of the OLIVES Dataset. The modality column details the type of data. The columns \u201cPer Visit\u201d and \u201cPer Eye\u201d indicate the amount of data in each modality on a respective visit or eye. NP is the number of visits that a patient P takes to the clinic. The statistics across all eyes across all visits are shown in the Total Statistics column. Biomarkers are binary values, clinical labels are integers, fundus are 2D images, and OCT are 3D slices.\\n\\n| OLIVES Dataset Summary | Modality | Per Visit | Per Eye | Total Statistics | Overview |\\n|-----------------------|----------|-----------|---------|------------------|----------|\\n|                       | OCT      |           |         |                  |          |\\n|                       | Fundus   |           |         |                  |          |\\n|                       | Clinical |           |         |                  | General: |\\n| Biomarker             |          |           |         |                  | 96 Eyes, Visits every 4-16 weeks, |\\n|                       |          |           |         |                  | Average 16 visits and 7 injections/patient |\\n|                       |          |           |         |                  | Clinical Labels obtained every visit: |\\n|                       |          |           |         |                  | BCV A, CST, Patient ID, Eye ID |\\n|                       |          |           |         |                  | Biomarkers labeled: |\\n|                       |          |           |         |                  | IRHRF, FA VF, IRF, DRT/ME PA VF, VD, Preretinal Tissue, EZ Disruption, IR Hemmorhages, SRF, VMT, Atrophy, SHRM, RPE Disruption, Serous PED |\\n\\nStatistics regarding the quantity of images and labels can be found in Table 1. The OLIVES dataset is derived from the PRIME and TREX-DME trials. At every visit for each patient, ocular disease state data (DR/DME), clinical labels including BCV A, CST, Patient and Eye ID, and detailed ocular imaging including OCT, and fundus photography were obtained per the protocol in Section B.5.2. This procurement of data continues across NP visits for every patient, where NP is the number of visits by a patient P. For instance, 3D longitudinal scans of the eye provide 49 OCT scans per patient per visit. Across NP visits where P can be any one of 96 patients, the total number of OCT scans in the dataset is 78,189. Note that on every visit, each patient undergoes testing to determine the requirement of a treatment per the clinical protocol described in Appendix D.1. Biomarkers are retrospectively added to each slice in the OCT scans for the first and last visits. Table 1 also indicates the total number of eyes, average number of visits and injections, and the time between visits.\\n\\n3.1 Biomarker Generation\\n\\nAfter the clinical data collection process, we retrospectively provide additional insight into the OCT scans by providing corresponding biomarker labels. Biomarkers are quantifiable characteristics of biological processes in the eye. In this paper, the biological processes are diseases and biomarkers indicate the presence or absence of such diseases. Under limited circumstances, the authors in (1) suggest that biomarkers can be surrogate endpoints in clinical trials. However, they caution against doing so unless the underlying clinical trial is specifically meant for the study. In both the PRIME and TREX DME studies, biomarkers are retrospectively labeled. As such, biomarkers may indicate the presence of diseases, but are not causal to these diseases. Hence, biomarkers are different from visual causal features from (23) or causal question-based analysis in (24) or causal factor analysis in (25).\"}"}
{"id": "mZke5vYdF99", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the PRIME and TREX DME studies, images, clinical information, and biomarker labels were retrospectively collected at the Retina Consultants of Texas (Houston, TX, USA). This study was approved by the Institutional Review Board (IRB)/Ethics Committee and adheres to the tenets of the Declaration of Helsinki and Health Insurance Portability and Accountability Act (HIPAA). Informed consent was not required due to the retrospective nature of the study. A trained grader performed interpretation on OCT scans for the presence of different biomarkers including: intraretinal hyperreflective foci (IRHRF), partially attached vitreous face (PA VF), fully attached vitreous face (FA VF), intraretinal fluid (IRF), and diffuse retinal thickening or macular edema (DRT/ME). A full list of the biomarkers as well as their characteristics is provided in Section B.5.1. The full form of the abbreviations are given in Table 7. These biomarkers are chosen because of their visual attributes that correlate with presence or absence of disease states. The trained grader was blinded to clinical information whilst grading each of 49 horizontal OCT B-scans of both the first and last study visit for each individual eye. Open adjudication was done with an experienced retina specialist for difficult cases. In total, there are 9,408 OCT scans that consist of a 16\u21e51 biomarker vector where a 1 indicates the presence of the corresponding biomarker and a 0 indicates its absence. We provide a histogram of the number of scans (y-axis) against their respective biomarkers in Fig. 3a. Note that the y-axis is in log-scale. We also depict the eye ID against the biomarkers in Fig. 3b. The green dots are eyes that indicate the presence of the corresponding biomarker on the y-axis that are diagnosed with DR. The red dots are for DME. It can be seen that a number of eyes have overlapping biomarkers even between diseases. Hence, biomarkers in isolation are insufficient to diagnose disease states, strengthening the case for multi-modal data.\\n\\n3.2 Clinical Labels\\nWithin the OLIVES dataset, we have explicit clinical information regarding the Best Central Visual Acuity (BCV A), Central Subfield Thickness (CST), and identity of the eye. ETDRS best-corrected visual acuity (BCV A) is a visual function assessment performed by certified examiners where a standard vision chart is placed 4-meters away from the patient. The patient is instructed to read...\"}"}
{"id": "mZke5vYdF99", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the chart from left to right from top to bottom until the subject completes 6 rows of letters or the subject is unable to read any more letters. The examiner marks how many letters were correctly identified by the patient. Central subfield thickness (CST) is the average macular thickness in the central 1-mm radius of the ETDRS grid. Both BCVA and CST are coarse measurements over the eye as opposed to Biomarkers that exist for fine-grained longitudinal slices of the eye. BCVA can range from 0 to 100 and CST from 100 to 1300. We show in Fig. 4 the number of eyes (y-axis) that have the associated value (x-axis) for both BCVA and CST. This graph shows that our dataset has a wide variation in terms of range of clinical values across a multitude of eyes in the dataset. This is advantageous as it shows the dataset is not biased to any specific range of values or localized to single eye instances. A full list of all clinical labels present in PRIME and TREX-DME clinical trials are shown in Section B.5.2. No personally identifiable information was included in compliance with HIPAA regulations.\\n\\nFigure 5: 1) A plot of average number of visits by patients that were an improvement or deterioration from previous week. Red bars indicate the standard deviation across all patients. 2) Plot of average change in BCVA with respect to the first week.\\n\\n3.3 Time-series data\\nA core novelty of the dataset is that data exists for each patient visit across a defined period of time. As a result, it is possible to analyze trends in the collected imaging and clinical data over the visiting period of the patient. This is shown in Fig. 5 with an overall progression analysis shown by the bar graphs. Graph 1 indicates whether on average there was an improvement from the previous visit. This was computed by assigning a value of 1 for improvements and -1 for deterioration. This is accumulated every visit and the average across all patients is calculated on a per visit basis. From this plot, it can be observed that in the dataset, eyes generally improve on every visit until about the tenth visit. However, graph 2 in Fig. 5 shows that while visit to visit improvement declines, the overall improvement when compared with the first visit is generally substantial. This graph was computed by taking the difference between the current visit\u2019s BCVA and first visit\u2019s BCVA and averaging across all patients. The statistics of the number of patients every visit and visualizations of patient treatment is shown in Figs. 8 and 9 respectively.\\n\\n3.4 Interaction between Data Modalities\\nClinical labels correspond to measurements that pertain to the entire visual system, including the visual mechanism in the eye. These measurements give an overview of the health of the eye, but they do not enable fine-grained analysis of structures that exist within the eye. Biomarker labels exist at the longitudinal slice level. They are detailed labels for every slice of the eye and provide a fine-grained analysis of the biological structures that exist within the eye. Clinical studies such as (26) and (27) suggest that measured clinical labels can act as indicators of structural changes that manifest themselves in OCT scans and fundus images as well as the severity of disease associated with the patient. For example, visually, it can be observed that OCT scans with the same BCVA values exhibit more common structural characteristics than scans with different BCVA values. Furthermore, all data modalities exhibit visual, structural and clinical changes across the treatment period.\"}"}
{"id": "mZke5vYdF99", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"allows for exploiting these correlations between OCT, fundus, clinical labels, biomarkers, diseases, and treatment states.\\n\\n4 Clinical Applications\\n\\nWith the multitude of modalities that exist within the OLIVES dataset, there is potential for research in a wide variety of ML applications. Within this section, we focus on applications, and benchmarks, that showcase key features of the dataset identified from Fig. 1, but acknowledge that other novel setups and formulations of the problem are possible and intended. These applications include multi-modal integration of OCT scans and biomarker/clinical labels, biomarker detection and interpretation using contrastive learning, and time-series treatment analysis.\\n\\n4.1 Multi-Modal Integration Between OCT and Biomarkers/Clinical Labels\\n\\nTable 2: Benchmark results for DR/DME detection.\\n\\n| Experiments                     | Model       | Balanced Accuracy | Specificity | Sensitivity |\\n|---------------------------------|-------------|-------------------|-------------|-------------|\\n| OCT R-18                        | 70.15% \u00b1 4.69 | 0.608             | 0.794       |\\n| Clinical MLP                    | 75.49% \u00b1 1.98 | 0.758             | 0.751       |\\n| Biomarker MLP                   | 79.87% \u00b1 3.03 | 0.826             | 0.771       |\\n| OCT + Clinical R-18 + MLP       | 75.92% \u00b1 3.05 | 0.566             | 0.952       |\\n| OCT + Biomarker R-18 + MLP      | 82.33% \u00b1 3.59 | 0.742             | 0.904       |\\n\\nBaseline Detection of DR/DME with OCT\\n\\nSince biomarkers are only available for the first and last clinical visits, we use the corresponding OCT at those visits for this baseline analysis. The entire dataset is partitioned by eyes into train, test and validation splits. Additional details about train/test/validation splits is in Appendix C.1. We evaluate performance with balanced accuracy, precision and recall performance metrics. The results for the baseline OCT model is shown in the first row of Table 2. Additional results showing specificity and sensitivity are in Table 9 in the Appendix. This and subsequent experiments are conducted using multiple random seeds for DR/DME detection and an average score and standard deviation is reported for balanced accuracy.\\n\\nSupervised Learning with Clinical Labels\\n\\nWe aim to use clinical labels as an additional modality to aid the baseline model. However, to determine the suitability of this auxiliary data type, we first evaluate its impact on the classification of DR and DME. To do this we first find all unique clinical labels present in the dataset with their associated disease labels. Then, we create a training set with 70% of these clinical labels along with test and validation sets of 20% and 10% proportions respectively. This yields 1107 unique clinical labels for training, 306 for testing and 122 for validation. Within the test set, half the samples are DR and the remaining DME. The second row on Table 2 shows that CST and BCV A used as clinical features are more effective than the unimodal OCT baseline for DR/DME detection.\\n\\nSupervised Learning with Biomarkers\\n\\nWe perform a similar analysis as described in supervised learning with clinical labels but using biomarkers as features. Hence, we substitute the clinical labels with biomarkers to characterize the diseases. There are 286 unique biomarker label features among which 200, 58, 28 samples are used for train, test and validation sets respectively. From the third row in Table 2, we observe that using biomarkers on their own leads to a 9.72% increase in DR and DME classification over baseline results.\\n\\nMulti-Modal Learning with OCT and Clinical Labels\\n\\nHaving seen that clinical labels are more effective than the baseline model at DR/DME classification, we now investigate how to use the clinical label modality to aid the OCT model. Clinical labels and OCT are independently given as input to their models as described previously. We optimize both models jointly with a loss function that allows knowledge, in the form of logits, from the clinical model to guide the optimization of the OCT model. A detailed description of this optimization scheme can be seen in Appendix C.1. During testing, only the OCT model, having been optimized jointly with the other model, is used to classify the disease states. The fourth row of Table 2 shows that clinical labels also aid the OCT model at characterizing the diseases albeit not the most effectively.\"}"}
{"id": "mZke5vYdF99", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Benchmark of the performance of supervised contrastive training on images with clinical and biomarker data. The standard deviations are shown in Table 10.\\n\\n| Method     | Biomarkers | Metrics          | AUROC | Average Specificity | Average Sensitivity | Multimodality Learning with OCT and Biomarkers\\n|------------|------------|------------------|-------|---------------------|---------------------|--------------------------------------------------\\n| PCL (28)   | IRF        | DRT/ME           | IRHRF | FA                  | VF                  | PA VF                                            |\\n|            |            |                  |       |                     |                     | Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy F1-Score Accuracy"}
{"id": "mZke5vYdF99", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"label classification of biomarkers. Performance is measured by average AUROC, specificity, and sensitivity. We observe that a training strategy that chooses positives based on the clinical data Eye ID, BCV A, and CST outperforms baseline self-supervised methods in both a multi-label classification task as well as individual biomarker detection performance. While the results in Section 4.1 make use of correlations between the biomarkers and clinical labels with disease states, these results depict the correlations between the label modalities.\\n\\nIn Figure 6, we visualize the test set t-SNE embeddings of two different biomarkers from a model trained using the BCV A clinical label. We observe that even without any fine-tuning on the actual biomarker label of interest, we are able to get an embedding space where the absence and presence of DME and IRF form distinct clusters. This gives credence to the idea that there exists relationships between the biomarker and clinical label domains as training on only clinical labels leads to a separable space within the biomarker domain.\\n\\n4.3 Time-Series Treatment Analysis\\n\\nThe multi-modal nature of OLIVES dataset allows for a large combination of experimental setups to analyze treatments. We present two experimental manifestations based off the temporal nature of the data: a) Predicting visit-by-visit successive treatment effects and b) Predicting the final ocular state using Biomarkers. A key metric used to evaluate treatment progression or regression is BCV A. At each visit to the clinic, patients' ocular disease states are evaluated and BCV A and other clinical labels are recorded. From a machine learning perspective, this motivates an analysis of treatment effect over consecutive weeks to predict how BCV A scores will change based on the state of the eye captured via OCT or Fundus. We detail the exact experimental procedure in Appendix C.3. We evaluate the performance of this strategy on both fundus images and 3D OCT volumes. We use a Resnet-18 (37), ResNet-50 (37), DenseNet-121 (38), EfficientNet (39), and Vision Transformer (40) (using a patch size of 32, 16 transformer blocks, 16 heads in multi-attention layer). For the OCT volumes, we utilize a version of each architecture that uses three-dimensional convolution layers. Performance in both modalities is reported in Table 4. We observe that the model is able to learn distinguishing features between the two classes, with better performance when using the OCT volumetric data. Additionally, we present results for predicting the final state of 16\u21e51 biomarker vector given the initial biomarker vector for individual patients in Fig. 10. Similar to the week-wise case, these results indicate correlation among multiple modalities as well as the ability of ML algorithms to predict ocular states given treatment.\\n\\nTable 4: Benchmark Performance of predicting treatment effects from time-series Fundus and OCT data.\\n\\n| Model       | Image Modality | Accuracy | Precision | Recall |\\n|-------------|----------------|----------|-----------|--------|\\n| ResNet-18   | Fundus         | 55.19%   | 0.256     | 0.343  |\\n|             | OCT Volume     | 57.59%   | 0.359     | 0.326  |\\n| ResNet-50   | Fundus         | 48.73%   | 0.372     | 0.3296 |\\n|             | OCT Volume     | 57.70%   | 0.301     | 0.1826 |\\n| DenseNet-121| Fundus         | 53.00%   | 0.273     | 0.259  |\\n|             | OCT Volume     | 54.75%   | 0.219     | 0.188  |\\n| EfficientNet| Fundus         | 56.06%   | 0.292     | 0.217  |\\n|             | OCT Volume     | 60.65%   | 0.3613    | 0.1633 |\\n| ViT         | Fundus         | 55.01%   | 0.285     | 0.350  |\\n\\n5 Discussion and Conclusion\\n\\nDomain Difference and Adaptation in Multi-Modal Data\\n\\nThe data in OLIVES is derived from two studies. As mentioned in Section 1, the domain difference in ophthalmic data can arise from sources such as treatment, disease manifestation, and clinical labels. In natural images, one source of domain difference is the equipment used for imaging. In PRIME and TREX studies, the same imaging and grading modalities, the Heidelberg Spectralis HRA+OCT software, is used in the same clinic. We provide extensive experiments in Appendix C.5 and C.6 to characterize possible domain differences on OLIVES. In Table 11, we show that the biomarker detection results when trained and tested on PRIME trial is lower than when trained with TREX and tested on PRIME. This is because a longer treatment period on TREX dataset provides more diverse data that is conducive for...\"}"}
{"id": "mZke5vYdF99", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"training ML algorithms. Intuitively, this suggests that treatment causes domain shift in data, which is illustrated in Table 12. Training and testing within the first week data provides the best results for biomarker detection. This analysis is further expanded in Fig. 11. Rather than showing domain difference, we adapt between the first and last visit domains. Specifically, we use a part of the last visit data to train with the first visit data and show that: a) adapting between OCT scans before and after treatment is possible, and b) the addition of biomarkers increases the results for diagnosis of DR/DME. Hence, OLIVES provides data modalities that promotes research in treatment-based domain difference and adaptation in medical data.\\n\\nDataset Limitations, Societal Impact, and Ethical Concerns\\n\\nThe OLIVES dataset is derived from two clinical studies conducted from only one U.S. clinic. While there is a range in the age, ethnicity and racial demographics within the cohorts, this range is only limited to one geographical location. Hence, an end-to-end system can be biased. To mitigate this limitation, we provide links to existing open access ophthalmic datasets in Appendix B.4 that are collected from other parts of the world. While none of these datasets are as rich as our own in terms of numbers, modalities, or labels, they can be used to modularly test algorithms. We present one such result in Table 13 and show that combining datasets allows for higher results. The PRIME and TREX trials are randomized clinical studies with the goal of comparing different treatment regimens. These studies aim to find the best practices for how and when they should treat patients to get the most optimal outcomes. However, there are no control groups within the studies that did not receive treatment. While this is common in clinical trials, it adds a new challenge to ML-focused research of time-series analysis. We list datasets that provide healthy images in Appendix B.4 to complement OLIVES. We believe that a combination of datasets taken over multiple geographical regions, times, and disease states is essential to construct generalizable and ethical ML models. ML models can potentially amplify existing inequalities within healthcare access. For instance, the data in OLIVES is collected from December 2013 to April 2021, which implies the participants had the time and means to be part of these trials. This may not always be the case for disadvantaged groups. Hence, any benefit that machine learning could provide will be restricted to small subsets of society unless thought is put into preventing this disparity. Hence, a careful analysis of potential concerns is required to use OLIVES and any other dataset to enrich the functionality and adaptability of machine learning algorithms in everyday lives.\\n\\nConclusion\\n\\nWe introduce the OLIVES dataset to bridge the gap between existing ophthalmic datasets and the clinical diagnosis and treatment process. OLIVES provides curated and contained data that can be used for clinical interpretation of biomarkers, clinical reasoning regarding disease prediction, multi-modal integration of ophthalmic data and treatment monitoring through time-series analysis. Also, we propose and benchmark medically-grounded contrastive learning strategies that are possible because of the presence of correlated multi-modal data within the introduced dataset. The OLIVES dataset opens new frontiers for training holistic and medically-relevant ML frameworks that mimic the clinical diagnosis pipeline for ophthalmic studies.\\n\\nReferences\\n\\n[1] Marzieh Golabbakhsh and Hossein Rabbani, \u201cVessel-based registration of fundus and optical coherence tomography projection images of retina using a quadratic registration model,\u201d IET Image Processing, vol. 7, no. 8, pp. 768\u2013776, 2013.\\n[2] J Yu Hannah, Justis P Ehlers, Duriye Damla Sevgi, Jenna Hach, Margaret O\u2019Connell, Jamie L Reese, Sunil K Srivastava, and Charles C Wykoff, \u201cReal-time photographic-and fluorescein angiographic-guided management of diabetic retinopathy: Randomized prime trial outcomes,\u201d American Journal of Ophthalmology, vol. 226, pp. 126\u2013136, 2021.\\n[3] John F Payne, Charles C Wykoff, W Lloyd Clark, Beau B Bruce, David S Boyer, David M Brown, TREX-DME study group, et al., \u201cRandomized trial of treat and extend ranibizumab with and without navigated laser for diabetic macular edema: Trex-dme 1 year outcomes,\u201d Ophthalmology, vol. 124, no. 1, pp. 74\u201381, 2017.\\n[4] John F Payne, Charles C Wykoff, W Lloyd Clark, Beau B Bruce, David S Boyer, David M Brown, John A Wells III, David L Johnson, Matthew Benz, Eric Chen, et al., \u201cRandomized trial of treat and extend ranibizumab with and without navigated laser versus monthly dosing for 10...\"}"}
{"id": "mZke5vYdF99", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. John F Payne, Charles C Wykoff, W Lloyd Clark, Beau B Bruce, David S Boyer, and David M Brown, \u201cLong-term outcomes of treat-and-extend ranibizumab with and without navigated laser for diabetic macular oedema: Trex-dme 3-year results,\u201d British Journal of Ophthalmology, vol. 105, no. 2, pp. 253\u2013257, 2021.\\n\\n2. Charles C Wykoff, Muneeswar G Nittala, Brenda Zhou, Wenying Fan, Swetha Bindu Velaga, Shaun IR Lampen, Alexander M Rusakevich, Justis P Ehlers, Amy Babiuch, David M Brown, et al., \u201cIntravitreal aflibercept for retinal nonperfusion in proliferative diabetic retinopathy: outcomes from the randomized recovery trial,\u201d Ophthalmology Retina, vol. 3, no. 12, pp. 1076\u20131086, 2019.\\n\\n3. Dogancan Temel, Gukyeong Kwon, Mohit Prabhushankar, and Ghassan AlRegib, \u201cCure-tsr: Challenging unreal and real environments for traffic sign recognition,\u201d arXiv preprint arXiv:1712.02463, 2017.\\n\\n4. Dogancan Temel, Jinsol Lee, and Ghassan AlRegib, \u201cCure-or: Challenging unreal and real environments for object recognition,\u201d in 2018 17th IEEE international conference on machine learning and applications (ICMLA). IEEE, 2018, pp. 137\u2013144.\\n\\n5. Min-Hung Chen, Baopu Li, Yingze Bao, and Ghassan AlRegib, \u201cAction segmentation with mixed temporal domain adaptation,\u201d in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2020, pp. 605\u2013614.\\n\\n6. Ching-Yu Cheng, Zhi Da Soh, Shivani Majithia, Sahil Thakur, Tyler Hyungtaek Rim, Yih Chung Tham, and Tien Yin Wong, \u201cBig data in ophthalmology,\u201d The Asia-Pacific Journal of Ophthalmology, vol. 9, no. 4, pp. 291\u2013298, 2020.\\n\\n7. Saad M Khan, Xiaoxuan Liu, Siddharth Nath, Edward Korot, Livia Faes, Siegfried K Wagner, Pearse A Keane, Neil J Sebire, Matthew J Burton, and Alastair K Denniston, \u201cA global review of publicly available datasets for ophthalmological imaging: barriers to access, usability, and generalisability,\u201d The Lancet Digital Health, vol. 3, no. 1, pp. e51\u2013e66, 2021.\\n\\n8. Tahereh Mahmudi, Rahele Kafieh, Hossein Rabbani, Mohammadreza Akhlagi, et al., \u201cComparison of macular octs in right and left eyes of normal people,\u201d in Medical Imaging 2014: Biomedical Applications in Molecular, Structural, and Functional Imaging. SPIE, 2014, vol. 9038, pp. 472\u2013477.\\n\\n9. Daniel Kermany, Kang Zhang, Michael Goldbaum, et al., \u201cLabeled optical coherence tomography (oct) and chest x-ray images for classification,\u201d Mendeley data, vol. 2, no. 2, 2018.\\n\\n10. Sina Farsiu, Stephanie J Chiu, Rachelle V O\u2019Connell, Francisco A Folgar, Eric Yuan, Joseph A Izatt, Cynthia A Toth, Age-Related Eye Disease Study 2 Ancillary Spectral Domain Optical Coherence Tomography Study Group, et al., \u201cQuantitative classification of eyes with and without intermediate age-related macular degeneration using optical coherence tomography,\u201d Ophthalmology, vol. 121, no. 1, pp. 162\u2013172, 2014.\\n\\n11. Martina Melin\u0161, Marin Radmilovi\u0107, Zoran Vatavuk, and Sven Lon\u010dari\u0107, \u201cAnnotated retinal optical coherence tomography images (aroi) database for joint retinal layer and fluid segmentation,\u201d Automatika: \u0159asopis za automatiku, mjerenje, elektroniku, ra\u010dunarstvo i komunikacije, vol. 62, no. 3-4, pp. 375\u2013385, 2021.\\n\\n12. Stephanie J Chiu, Michael J Allingham, Priyatham S Mettu, Scott W Cousins, Joseph A Izatt, and Sina Farsiu, \u201cKernel regression based segmentation of optical coherence tomography images with diabetic macular edema,\u201d Biomedical optics express, vol. 6, no. 4, pp. 1172\u20131194, 2015.\\n\\n13. Dogancan Temel, Melvin J Mathew, Ghassan AlRegib, and Yousuf M Khalifa, \u201cRelative afferent pupillary defect screening through transfer learning,\u201d IEEE Journal of Biomedical and Health Informatics, vol. 24, no. 3, pp. 788\u2013795, 2019.\"}"}
{"id": "mZke5vYdF99", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Daniel S Kermany, Michael Goldbaum, Wenjia Cai, Carolina CS Valentim, Huiying Liang, Sally L Baxter, Alex McKeown, Ge Yang, Xiaokang Wu, Fangbing Yan, et al., \u201cIdentifying medical diagnoses and treatable diseases by image-based deep learning,\u201d *Cell*, vol. 172, no. 5, pp. 1122\u20131131, 2018.\\n\\nYash Logan, Kiran Kokilepersaud, Gukyeong Kwon, Ghassan AlRegib, Charles Wykoff, and Hannah Yu, \u201cMulti-modal learning using physicians diagnostics for optical coherence tomography classification,\u201d *IEEE International Symposium on Biomedical Imaging (ISBI)*, 2022.\\n\\nJeffrey De Fauw, Joseph R Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, Xavier Glorot, Brendan O'Donoghue, Daniel Visentin, et al., \u201cClinically applicable deep learning for diagnosis and referral in retinal disease,\u201d *Nature medicine*, vol. 24, no. 9, pp. 1342\u20131350, 2018.\\n\\nMichael G Kawczynski, Thomas Bengtsson, Jian Dai, J Jill Hopkins, Simon S Gao, and Jeffrey R Willis, \u201cDevelopment of deep learning models to predict best-corrected visual acuity from optical coherence tomography,\u201d *Translational vision science & technology*, vol. 9, no. 2, pp. 51\u201351, 2020.\\n\\nFilippo Arcadu, Fethallah Benmansour, Andreas Maunz, John Michon, Zdenka Haskova, Dana McClintock, Anthony P Adamis, Jeffrey R Willis, and Marco Prunotto, \u201cDeep learning predicts oct measures of diabetic macular thickening from color fundus photographs,\u201d *Investigative ophthalmology & visual science*, vol. 60, no. 4, pp. 852\u2013857, 2019.\\n\\nMohit Prabhushankar and Ghassan AlRegib, \u201cExtracting causal visual features for limited label classification,\u201d in *2021 IEEE International Conference on Image Processing (ICIP)*. IEEE, 2021, pp. 3697\u20133701.\\n\\nGhassan AlRegib and Mohit Prabhushankar, \u201cExplanatory paradigms in neural networks,\u201d *arXiv preprint arXiv:2202.11838*, 2022.\\n\\nKrzysztof Chalupka, Pietro Perona, and Frederick Eberhardt, \u201cVisual causal feature learning,\u201d *arXiv preprint arXiv:1412.2309*, 2014.\\n\\nRosana Zacarias Hannouche, Marcos Pereira de \u00c1vila, David Leonardo Cruvinel Isaac, Alan Ricardo Rassi, et al., \u201cCorrelation between central subfield thickness, visual acuity and structural changes in diabetic macular edema,\u201d *Arquivos brasileiros de oftalmologia*, vol. 75, no. 3, pp. 183\u2013187, 2012.\\n\\nJennifer K Sun, Michael M Lin, Jan Lammer, Sonja Prager, Rutuparna Sarangi, Paolo S Silva, and Lloyd Paul Aiello, \u201cDisorganization of the retinal inner layers as a predictor of visual acuity in eyes with center-involved diabetic macular edema,\u201d *JAMA ophthalmology*, vol. 132, no. 11, pp. 1309\u20131316, 2014.\\n\\nJunnan Li, Pan Zhou, Caiming Xiong, and Steven CH Hoi, \u201cPrototypical contrastive learning of unsupervised representations,\u201d *arXiv preprint arXiv:2005.04966*, 2020.\\n\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in *International conference on machine learning*. PMLR, 2020, pp. 1597\u20131607.\\n\\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He, \u201cImproved baselines with momentum contrastive learning,\u201d *arXiv preprint arXiv:2003.04297*, 2020.\\n\\nPhuc H Le-Khac, Graham Healy, and Alan F Smeaton, \u201cContrastive representation learning: A framework and review,\u201d *IEEE Access*, 2020.\\n\\nMohit Prabhushankar and Ghassan AlRegib, \u201cContrastive reasoning in neural networks,\u201d *arXiv preprint arXiv:2103.12329*, 2021.\\n\\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan, \u201cSupervised contrastive learning,\u201d *arXiv preprint arXiv:2004.11362*, 2020.\"}"}
{"id": "mZke5vYdF99", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kiran P Kokilepersaud, Stephanie Trejo Corona, Mohit Prabhushankar, Ghassan Alregib, and Charles Wykoff, \u201cSupervised contrastive learning on clinical labels for biomarker classification in oct,\u201d *Journal of Biomedical And Health Informatics*, Under Review.\\n\\nKiran P Kokilepersaud, Stephanie Trejo Corona, Mohit Prabhushankar, Ghassan Alregib, and Charles Wykoff, \u201cGradient-based severity labeling for biomarker classification in oct,\u201d *IEEE International Conference in Image Processing*, 2022.\\n\\nKiran Kokilepersaud, Mohit Prabhushankar, and Ghassan AlRegib, \u201cV olumetric supervised contrastive learning for seismic semantic segmentation,\u201d in *Second International Meeting for Applied Geoscience & Energy*. Society of Exploration Geophysicists and American Association . . . , 2022, pp. 1699\u20131703.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, \u201cDeep residual learning for image recognition,\u201d in *Proceedings of the IEEE conference on computer vision and pattern recognition*, 2016, pp. 770\u2013778.\\n\\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger, \u201cDensely connected convolutional networks,\u201d in *Proceedings of the IEEE conference on computer vision and pattern recognition*, 2017, pp. 4700\u20134708.\\n\\nMingxing Tan and Quoc Le, \u201cEfficientnet: Rethinking model scaling for convolutional neural networks,\u201d in *International conference on machine learning*. PMLR, 2019, pp. 6105\u20136114.\\n\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at scale. arxiv 2020,\u201d *arXiv preprint arXiv:2010.11929*, 2010.\\n\\nIrene Y Chen, Emma Pierson, Sherri Rose, Shalmali Joshi, Kadija Ferryman, and Marzyeh Ghassemi, \u201cEthical machine learning in healthcare,\u201d *Annual review of biomedical data science*, vol. 4, pp. 123\u2013144, 2021.\\n\\nPratul P Srinivasan, Leo A Kim, Priyatham S Mettu, Scott W Cousins, Grant M Comer, Joseph A Izatt, and Sina Farsiu, \u201cFully automated detection of diabetic macular edema and dry age-related macular degeneration from optical coherence tomography images,\u201d *Biomedical optics express*, vol. 5, no. 10, pp. 3568\u20133577, 2014.\\n\\nStefan Maetschke, Bhavna Antony, Hiroshi Ishikawa, Gadi Wollstein, Joel Schuman, and Rahil Garnavi, \u201cA feature agnostic approach for glaucoma detection in oct volumes,\u201d *PloS one*, vol. 14, no. 7, pp. e0219126, 2019.\\n\\nCalifornia Healthcare Foundation, \u201cDiabetic retinopathy detection identify signs of diabetic retinopathy in eye images,\u201d *https://www.kaggle.com/competitions/diabetic-retinopathy-detection/overview*, 2015, Accessed: 2022-06-08.\\n\\nLiu Li, Mai Xu, Xiaofei Wang, Lai Jiang, and Hanruo Liu, \u201cAttention based glaucoma detection: a large-scale database and cnn model,\u201d in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019, pp. 10571\u201310580.\\n\\nNing Li, Tao Li, Chunyu Hu, Kai Wang, and Hong Kang, \u201cA benchmark of ocular disease intelligent recognition: One shot for multi-disease detection,\u201d in *International Symposium on Benchmarking, Measuring and Optimization*. Springer, 2020, pp. 177\u2013193.\\n\\nRuhan Liu, Xiangning Wang, Qiang Wu, Ling Dai, Xi Fang, Tao Yan, Jaemin Son, Shiqi Tang, Jiang Li, Zijian Gao, et al., \u201cDeepdrid: Diabetic retinopathy\u2014grading and image quality estimation challenge,\u201d *Patterns*, p. 100512, 2022.\\n\\nChi Liu, Xiaotong Han, Zhixi Li, Jason Ha, Guankai Peng, Wei Meng, and Mingguang He, \u201cA self-adaptive deep learning method for automated eye laterality detection based on color fundus photography,\u201d *Plos one*, vol. 14, no. 9, pp. e0222025, 2019.\"}"}
{"id": "mZke5vYdF99", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Michael D Abr\u00e0moff, James C Folk, Dennis P Han, Jonathan D Walker, David F Williams, Stephen R Russell, Pascale Massin, Beatrice Cochener, Philippe Gain, Li Tang, et al., \\\"Automated analysis of retinal images for detection of referable diabetic retinopathy,\\\" *JAMA Ophthalmology*, vol. 131, no. 3, pp. 351\u2013357, 2013.\\n\\nKedir M Adal, Peter G van Etten, Jose P Martinez, Lucas J van Vliet, and Koenraad A Vermeer, \\\"Accuracy assessment of intra- and intervisit fundus image registration for diabetic retinopathy screening,\\\" *Investigative Ophthalmology & Visual Science*, vol. 56, no. 3, pp. 1805\u20131812, 2015.\\n\\nAntoine Rivail, Ursula Schmidt-Erfurth, Wolf-Dieter Vogl, Sebastian M Waldstein, Sophie Riedl, Christoph Grechenig, Zhichao Wu, and Hrvoje Bogunovic, \\\"Modeling disease progression in retinal OCTs with longitudinal self-supervised learning,\\\" in *International Workshop on Predictive Intelligence in Medicine*. Springer, 2019, pp. 44\u201352.\\n\\nYuting Hu, Zhiling Long, Anirudha Sundaresan, Motaz Alfarraj, Ghassan AlRegib, Sungmee Park, and Sundaresan Jayaraman, \\\"Fabric surface characterization: assessment of deep learning-based texture representations using a challenging dataset,\\\" *The Journal of the Textile Institute*, vol. 112, no. 2, pp. 293\u2013305, 2021.\\n\\nYazeed Alaudah, Patrycja Michalowicz, Motaz Alfarraj, and Ghassan AlRegib, \\\"A machine-learning benchmark for facies classification,\\\" *Interpretation*, vol. 7, no. 3, pp. SE175\u2013SE187, 2019.\\n\\nYuhan Zhang, Mingchao Li, Zexuan Ji, Wen Fan, Songtao Yuan, Qinghuai Liu, and Qiang Chen, \\\"Twin self-supervision based semi-supervised learning (ts-ssl): Retinal anomaly classification in SD-OCT images,\\\" *Neurocomputing*, 2021.\\n\\nJiaming Qiu and Yankui Sun, \\\"Self-supervised iterative refinement learning for macular OCT volumetric data classification,\\\" *Computers in Biology and Medicine*, vol. 111, pp. 103327, 2019.\\n\\nNicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R Roth, Shadi Albarqouni, Spyridon Bakas, Mathieu N Galtier, Bennett A Landman, Klaus Maier-Hein, et al., \\\"The future of digital health with federated learning,\\\" *NPJ Digital Medicine*, vol. 3, no. 1, pp. 1\u20137, 2020.\\n\\nSven Holm, Greg Russell, Vincent Nourrit, and Niall McLoughlin, \\\"Dr Hagis\u2014a fundus image database for the automatic extraction of retinal surface vessels from diabetic patients,\\\" *Journal of Medical Imaging*, vol. 4, no. 1, pp. 014503, 2017.\\n\\nAshish Markan, Aniruddha Agarwal, Atul Arora, Krinjeela Bazgain, Vipin Rana, and Vishali Gupta, \\\"Novel imaging biomarkers in diabetic retinopathy and diabetic macular edema,\\\" *Therapeutic Advances in Ophthalmology*, vol. 12, pp. 2515841420950513, 2020.\\n\\nDominick A Rizzi and Stig Andur Pedersen, \\\"Causality in medicine: towards a theory and terminology,\\\" *Theoretical Medicine*, vol. 13, no. 3, pp. 233\u2013254, 1992.\\n\\nArpan Guha Mazumder, Swarnadip Chatterjee, Saunak Chatterjee, Juan Jose Gonzalez, Swarnendu Bag, Sambuddha Ghosh, Anirban Mukherjee, and Jyotirmoy Chatterjee, \\\"Spectropathology-corroborated multimodal quantitative imaging biomarkers for neuroretinal degeneration in diabetic retinopathy,\\\" *Clinical Ophthalmology (Auckland, NZ)*, vol. 11, pp. 2073, 2017.\\n\\nHui-Zhuo Xu, Zhiming Song, Shuhua Fu, Meili Zhu, and Yun-Zheng Le, \\\"RPE barrier breakdown in diabetic retinopathy: seeing is believing,\\\" *Journal of Ocular Biology, Diseases, and Informatics*, vol. 4, no. 1, pp. 83\u201392, 2011.\\n\\nLaxmi Gella, Rajiv Raman, Padmaja Kumari Rani, and Tarun Sharma, \\\"Spectral domain optical coherence tomography characteristics in diabetic retinopathy,\\\" *Oman Journal of Ophthalmology*, vol. 7, no. 3, pp. 126, 2014.\\n\\nYuji Itoh, Ashleigh L Levison, Peter K Kaiser, Sunil K Srivastava, Rishi P Singh, and Justis P Ehlers, \\\"Prevalence and characteristics of hyporeflective preretinal tissue in vitreomacular interface disorders,\\\" *British Journal of Ophthalmology*, vol. 100, no. 3, pp. 399\u2013404, 2016.\"}"}
{"id": "mZke5vYdF99", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The international vitreomacular traction study group classification of vitreomacular adhesion, traction, and macular hole,\" in *Ophthalmology*, vol. 120, no. 12, pp. 2611\u20132619, 2013.\\n\\nGeorge Trichonas and Peter K Kaiser, \u201cOptical coherence tomography imaging of macular oedema,\u201d in *British Journal of Ophthalmology*, vol. 98, no. Suppl 2, pp. ii24\u2013ii29, 2014.\\n\\nMohit Prabhushankar, Gukyeong Kwon, Dogancan Temel, and Ghassan AlRegib, \u201cContrastive explanations in neural networks,\u201d in *2020 IEEE International Conference on Image Processing (ICIP)*. IEEE, 2020, pp. 3289\u20133293.\\n\\nYash-yee Logan, Mohit Prabhushankar, and Ghassan AlRegib, \u201cDecal: Deployable clinical active learning,\u201d in *International Conference on Machine Learning (ICML) Workshop on Adaptive Experimental Design and Active Learning in the Real World*, 2022.\\n\\nYash-yee Logan, Ryan Benkert, Ahmad Mustafa, and Ghassan AlRegib, \u201cPatient aware active learning for fine-grained oct classification,\u201d in *International Conference on Image Processing (ICIP)*. 2022, IEEE.\"}"}
{"id": "mZke5vYdF99", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]. Please see Section 3.2, Line 173.\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes]. The clinical procedure for both trials is discussed in Section D.\\n\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [Yes]. Please see Section 3.1, Line 139 and Section D.2, Line 1086.\\n\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A].\"}"}
