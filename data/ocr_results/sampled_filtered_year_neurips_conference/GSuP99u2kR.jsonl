{"id": "GSuP99u2kR", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Chest X-ray. Cardiomegaly with diffuse bilateral interstitial infiltrates and a right-sided pleural effusion.\\n\\nUser: Which side of the lung has a pleural effusion?\\nAssistant: The right side of the lung has a pleural effusion.\\n\\nUser: According to the CT image, does the patient have cardiomegaly?\\nAssistant: The image appears to be a chest x-ray. However, it indicates that the patient has cardiomegaly, which means an enlarged heart. Additionally, there are diffuse bilateral interstitial infiltrates and a right-sided pleural effusion present in the image.\\n\\nUser: Additional Info: The patient's respiratory status continued to worsen and on hospital day number 14, requiring intubation and mechanical ventilation for hypoxemic respiratory failure.\\nAssistant: Thank you for providing additional information about the patient's condition. Based on the information you provided, it appears that the patient's respiratory status continued to worsen and on hospital day number 14, they required intubation and mechanical ventilation for hypoxemic respiratory failure. This suggests that the patient's lung function has significantly declined, and they may be experiencing respiratory distress. It is important for the medical team to closely monitor the patient's condition and provide appropriate treatment to manage their symptoms and improve their lung function.\\n\\nB More Discussions of LLaV A-Med\\nB.1 Limitations of LLaV A-Med\\nUsers of LLaV A-Med.\\nThe penitential users are individuals and professionals within the biomedical domain who seek assistance in understanding, analyzing, and discussing biomedical images. For example, (1) Researchers and Scientists: Biomedical researchers working on various topics, such as CT, Chest X-Ray, MRI, and histology, can use LLaV A-Med to analyze complex biomedical images, identify patterns, and derive insights from them. For these creative scenarios, models can generalize easily to provide many new insights based on the large amount of observed samples, while it is time-consuming for humans to do so. However, models can hallucinate, and humans can select and revise the model response. (2) Medical Practitioners: Doctors, nurses, and other healthcare professionals can use LLaV A-Med to improve the working efficiency, as LLaV A-Med can quickly provide initial answers by understanding diagnostic images, based on which medical practitioners...\"}"}
{"id": "GSuP99u2kR", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"can improve its factuality without repetitively drafting report from scratch every time. (3) Medical Students and Educators: LLaV A-Med can serve as an educational tool for medical students and educators, helping them learn and teach topics related to biomedical images. With the expert approved simple cases, the AI can help FQA, assisting in explaining concepts, clarifying doubts, and providing additional context for various imaging techniques and findings.\\n\\nLimitations of LLaV A-Med. Precaution is required when utilizing the LLaV A-Med model in practice: (1) Domain specificity: LLaV A-Med is designed for the biomedical domain, and its performance may not be as effective in other domains. When testing on other domains, LLaV A-Med tends to respond with biomedical background knowledge. (2) Reliability: Like other AI models, LLaV A-Med might inherit biases from the data it was trained on, which could affect its responses. While LLaV A-Med shows promise in answering open-ended research questions about biomedical images, its reliability is still subject to the quality and quantity of the training data. The model's performance on biomedical questions can be improved by fine-tuning, but there is always a possibility that it may not generalize well to certain types of questions or images not covered in the training data. Therefore, model hallucination still exists. We strongly suggest users to double-check the responses, and consider them as the preliminary responses that can revised with expert knowledge. (3) Dependency on input quality: The quality of LLaV A-Med's responses depends on the quality of the input data (biomedical images and captions). Inaccurate or incomplete input data can lead to suboptimal assistance. For example, the current image resolution of the system is $224 \\\\times 224$, which could be too low for the model to see the important details. Without due precautions in practice, the potential negative societal impact of deploying LLaV A-Med can appear. Despite these limitations, LLaV A-Med demonstrates strong potential in assisting with inquiries about biomedical images, encouraging more future research to improve the system.\\n\\nB.2 On the use of GPT-4 API\\n\\nNote that our use of GPT-4 is to convert the text associated with the image into conversational QA format. This is not the typical model distillation as in the language domain, where both teacher and student are text-to-text models. Instead, we trained a (image, text)-to-text model, while the GPT-4 is used as a text-to-text model for data annotation. We discuss both ethical and legal concerns when using GPT-4 or similar LLM to generate self-instructional data.\\n\\nEthical issues: (1) Accuracy and Misinformation: Generated content may not always be accurate, leading to misinformation being spread, though we have designed a comprehensive filtering script as a post-process to improve the quality. (2) Bias and Fairness: Since we do not have access to the training data of GPT4, the generated instruct data might reflect those biases, reinforcing social or cultural inequalities in the base model training. (3) Deception: In our self-instruct data creation pipeline, the GPT-4 API call can be replaced with human annotators, if more budget is available. Since GPT-4's strong annotation ability is close to humans, if the content is generated without disclosure, it might deceive users into thinking a human produced it. Legal issues: In terms of data usage, we explicitly state that the OpenAI terms should be compiled, and the data can only be used for research purposes. To partially address this concern, we believe that the recently released LLaMA-2-70B-Chat appears to have narrowed the gap. We find that LLaMA-2-70B-Chat can start to follow complex instructions like creating multimodal instructions. However, LLaMA-2-70B-Chat is not correctly following the conversation format. This may be potentially fixed with more sophisticated prompt tuning.\\n\\nLimitations of data pipeline and the resulting dataset. Our data pipeline inherits the aforemen\u00adtioned limitations of utilizing GPT-4 API. We considered a comprehensive filtering approach for quality control. Initially, we found that the resulting dataset contains many hallucinated examples. Based on key words of hallucinated examples, we gradually expand our rules to filter out the those examples. Eventually, a comprehensive list of key words are constructed for filtering to increase the data quality. While the low quality samples probably still exist, we believe our filtering approach is effective given a limited budget, evidenced by the improved performance using LLaV A-Med on medical VQA datasets. As future directions, one can have experts to revise or filter the generated samples for higher quality (if more budget is allowed), or get the real-world medical visual conversational data from clinics.\"}"}
{"id": "GSuP99u2kR", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### C More Experiment Details\\n\\n#### C.1 Established Benchmarks\\n\\nThe three established medical VQA datasets are described as below:\\n\\n| Dataset     | Train | Test | Val | Test |\\n|-------------|-------|------|-----|------|\\n| VQA-RAD     | 313   | 203  | 450 | 96   |\\n| SLAKE       | 2599  | 858  | 858 |      |\\n| PathVQA     | 1797  | 451  | 4919| 1053 |\\n\\n| # Images     | Train | Val | Test |\\n|--------------|-------|-----|------|\\n| VQA-RAD      | 450   | 96  | 96   |\\n| SLAKE        | 858   | 858 |      |\\n| PathVQA      | 19,755| 6279| 6761 |\\n\\n| # QA Pairs   | Train | Val | Test |\\n|--------------|-------|-----|------|\\n| VQA-RAD      | 770   | 179 | 2976 |\\n| SLAKE        | 1943  | 422 | 416  |\\n| PathVQA      | 9949  | 3144| 3370 |\\n\\n| # Open       | Train | Val | Test |\\n|--------------|-------|-----|------|\\n| VQA-RAD      | 1027  | 272 | 1943 |\\n| SLAKE        | 9806  | 3135| 3391 |\\n| PathVQA      | 9949  | 3144| 3370 |\\n\\n| # Closed     | Train | Val | Test |\\n|--------------|-------|-----|------|\\n| VQA-RAD      | 1027  | 272 | 1943 |\\n| SLAKE        | 9806  | 3135| 3391 |\\n| PathVQA      | 9949  | 3144| 3370 |\\n\\nTable 8: Dataset statistics. For SLAKE, only the English subset is considered for head-to-head comparison with existing methods.\\n\\n- **VQA-RAD** [17] contains 3515 QA pairs generated by clinicians and 315 radiology images that are evenly distributed over the head, chest, and abdomen. Each image is associated with multiple questions. Questions are categorized into 11 categories: abnormality, attribute, modality, organ system, color, counting, object/condition presence, size, plane, positional reasoning, and other. Half of the answers are closed-ended (i.e., yes/no type), while the rest are open-ended with either one-word or short phrase answers.\\n\\n- **SLAKE** [26] is a Semantically-Labeled Knowledge-Enhanced dataset for medical VQA. It consists of 642 radiology images and over 7000 diverse QA pairs annotated by experienced physicians, where the questions may involve external medical knowledge (solved by provided medical knowledge graph), and the images are associated with rich visual annotations, including semantic segmentation masks and object detection bounding boxes. Besides, SLAKE includes richer modalities and covers more human body parts than the currently available dataset, including brain, neck, chest, abdomen, and pelvic cavity. Note SLAKE is bilingual dataset with English and Chinese. When compared with existing methods, we only consider the English subset.\\n\\n- **PathVQA** [14] is a dataset of pathology images. It contains a total of 4998 pathology images with 32,799 QA pairs. Every image has several questions that relate to multiple aspects such as location, shape, color, appearance, etc. The questions are categorized into two types, with several varieties: open-ended questions such as why, what, how, where, etc., and closed-ended questions.\\n\\n#### C.2 Ablation Studies\\n\\n**Impact of Stage-1 training.** We note that it is not always possible to train LLaV A-Med from LLaV A. For example, we could leverage customized vision encoder (e.g., BioMed CLIP) or LLM (e.g., Vicuna) to directly train LLaV A-Med using the proposed two-stage training process. The results are reported in Table 9. The customized pre-trained models can provide better performance, e.g., LLaV A-Med trained from BioMed CLIP is better than LLaV A-Med initialized from LLaV A. In the main text, we consider the strategy to train Stage-1 with the linear projection layer only. We now ablate three alternative schemes to study the impact of Stage-1:\\n\\n- **Training without Stage-1.** We skip Stage-1, and directly perform medical instruction tuning from LLaV A.\\n\\n- **Training Stage-1 with full-model fine-tuning, and including input instruction text.** We keep the same Stage-1 data, but tune the full LLM weights and the linear projection layer.\"}"}
{"id": "GSuP99u2kR", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Training Stage-1 with full-model fine-tuning, and removing input instruct text. For Stage-1 data, we only consider images as the input, and remove the description-related instruct in Table 6 and Table 7. We tune the full LLM weights and the linear projection layer. The results are reported in Table 10 (a). It yields higher average performance at the early epochs such as epoch 1, when training LLaV A-Med from LLaV A using Stage-2 only, without Stage-1. As the training continues to epoch 3 or more, all training methods perform similarly measured by the average scores. However, training with Stage-1 consistently provides higher performance than training without Stage-1 on the PathVQA dataset (see the comparisons in green cells), which indicates the Stage-1 can benefit certain biomedical domains, when related additional knowledge is learned.\\n\\nRemoving the instruct text in Stage-1 that concentrates image description generally improves the performance. This is because LLaV A-Med can smoothly transfer the knowledge of LLaV A in dealing with diverse instruct, without over-fitting to complete the image description tasks.\\n\\nOur suggestions on the necessity of Stage-1 training are:\\n\\n(i) If LLaV A-Med is trained with a customized vision encoder or LLM that are not included in LLaV A (i.e., no LLaV A checkpoint is available), Stage-1 is critical in aligning the multimodal feature space, and yield good performance.\\n\\n(ii) If LLaV A-Med is trained by initializing from LLaV A, the Stage-1 training is optional. In this case, it is more cost-efficient to skip Stage-1 and train Stage-2 only, which can quickly provide good performance on the vertical domains with less cost. However, for scenarios with a large number of in-domain image-text pairs that pre-trained LLaV A does not have much related knowledge, we suggest adding the Stage-1 training on the in-domain pairs: The best strategy in this case is full-model fine-tuning of the LLM, and removing the instruction text of describing the image.\\n\\nImpact of experiment variance.\\n\\nIn Table 10 (b), we reported multiple experiment run of the same configuration for above Stage-1 training schemes. It turns out the standard derivation of average score is very small. This statistical stability suggest we could use one single run to represent the given experimental configurations. Given the large number of ablation experiments we have performed in this paper, we choose to run the job once for most experiments.\\n\\nQuality-cost trade-off.\\n\\nIn Table 11 (a), increasing the number of instruct tuning epochs does not improve zero-shot medical VQA performance. Increasing the data size from 10K to 60K improves the average performance by an absolute 1.65% gain, but training cost increases more than four times. The performance gain is limited compared with the additional cost. That's why we stop further scaling up data size. Instead, we switch to improve the data quality. By comparing 60K-IM and 60K datasets, with almost the same training cost, the performance is increased by an absolute 0.86% gain. To achieve a quality-cost trade-off, We suggest more effort devoted to improving the instruction data quality rather than quantity. In Table 11 (b), for fine-tuning stage, we increase the number of fine-tuning epochs on the 60K-IM instruction dataset, and find that the best trade-off is 9 epochs.\\n\\nC.3 More LLaV A-Med Biomedical Chat Results\\n\\nWe show more multimodal conversation examples in Table 13, 14, 15.\"}"}
{"id": "GSuP99u2kR", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model Configuration | Stage 1 | Stage 2 | FT | Open | Closed |\\n|---------------------|---------|---------|----|------|--------|\\n| LLaV A Init.        | 0       | 3       | 0  | 0    | 0      |\\n| CLIP Vision Encoder | 0       | 1       | 0  | 15.33| 15.44  |\\n|                      | 0       | 3       | 0  | 23.61| 15.38  |\\n|                      | 0       | 6       | 0  | 6.35 | 14.74  |\\n|                      | 60K-IM  | 0       | 1  | 28.93| 54.41  |\\n|                      | 60K-IM  | 0       | 3  | 39.96| 55.29  |\\n|                      | 60K-IM  | 0       | 6  | 11.56| 54.26  |\\n|                      | 60K-IM  | 1       | 3  | 40.73|        |\\n|                      | 60K-IM  | 1       | 3  | 31.03| 61.76  |\\n|                      | 60K-IM  | 1       | 6  | 27.44| 59.93  |\\n|                      | 60K-IM  | 1       | 3  | 28.23| 61.40  |\\n|                      | 60K-IM  | 1       | 3  | 66.26| 80.88  |\\n| No LLaV A Init.     | 1       | 0       | 0  | 16.15| 21.32  |\\n|                      | 1       | 0       | 9  | 59.35| 76.84  |\\n|                      | 1       | 3       | 0  | 31.71| 59.93  |\\n|                      | 1       | 3       | 9  | 64.39| 81.99  |\\n| BioMed CLIP Vision Encoder | 1 | 3 | 0 | 37.84 | 60.66 |\\n|                      | 1       | 3       | 9  | 64.75| 83.09  |\\n| LLaV A 0 0 0        | 20.74   | 59.19   | 26.82 | 50.24 | 8.74 | 45.65 | 35.23 |\\n\\nTable 9: Ablation studies of initializing from LLaV A.\"}"}
{"id": "GSuP99u2kR", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Ablation studies of Stage-1 training and experience variance. Zero-shot results on three established biomedical VQA datasets are reported.\\n\\n(a) The impact of Stage-1 training. All jobs are initialized with LLaV A. It yields higher average performance at the early epochs such as epoch 1, when training LLaV A-Med from LLaV A using Stage-2 only, without Stage-1. As the training continues to epoch 3 or more, all training methods perform similarly measured by the average scores. However, training with Stage-1 consistently provides higher performance than training without Stage-1 on the PathVQA dataset (see the comparisons in green cells), which indicates the knowledge learned in Stage-1 can benefit certain biomedical domains, when related domain data is added.\\n\\n(b) The ablation study with averaged results from multiple experimental runs. This is experiment setting that initializing from Stage-1 from different schemes, and running multiple jobs of training with 10K instruct samples for 3 epochs. The standard derivation is small, indicating that a single run can represent the experiment configuration setting.\"}"}
{"id": "GSuP99u2kR", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11: The quality-cost trade-off. Training hours are reported with batch size 128 on eight A100 GPUs.\\n\\n(a) The performance of Stage-2 to tune LLaV A-Med with varied epochs on different instruct datasets.\\n\\n| Epochs | Performance | Training Hours |\\n|--------|-------------|----------------|\\n| 1      | 39.08       | 0.6            |\\n| 3      | 40.73       | 2.6            |\\n| 1      | 40.72       | 8.0            |\\n\\n(b) The performance to fine-tune LLaV A-Med with varied epochs.\\n\\n| Epochs | Performance | Training Hours |\\n|--------|-------------|----------------|\\n| 0      | 41.22       | 0.63           |\\n| 1      | 46.73       | 1.37           |\\n| 3      | 65.30       | 2.74           |\\n| 9      | 73.90       | 4.25           |\\n| 15     | 73.88       | 4.93           |\\n| 18     | 73.22       |                |\\n\\nTable 12: Examples on zero-shot on Chinese questions. Though LLaV A-Med training data itself does not include Chinese, it is able to correctly understand the Chinese questions.\\n\\nUser: \u8fd9\u5f20\u56fe\u7247\u7684\u6210\u50cf\u65b9\u5f0f\u662f\u4ec0\u4e48?\\nLLaV A-Med: The image is an abdominal computed tomography (CT) scan, which is a cross-sectional view of the patient's abdomen. The image is taken in the portal phase, which means it is captured after the administration of contrast material to enhance the visibility of certain structures within the abdomen.\\n\\nUser: \u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u54ea\u79cd\u7c7b\u578b\u7684\u6838\u78c1\u5171\u632f?\\nLLaV A: The image shows a typical appearance of a nodular lesion on T1-weighted MR images. The lesion appears hyperintense, which means it is brighter than the surrounding tissue. This can be indicative of various types of lesions, and further evaluation would be needed to determine the specific nature and cause of the lesion.\\n\\nUser: \u8fd9\u4e2a\u9885\u9aa8\u7684\u7279\u5f81\u662f?\\nLLaV A: As a visual AI assistant, I cannot determine the specific abnormality on the chest X-ray based on the provided image description. The image description mainly focuses on the organs and their colors...\\n\\nTable 13: Example comparison of medical visual chat and reasoning capabilities.\"}"}
{"id": "GSuP99u2kR", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"User: What is the white arrow pointing to?\\n\\nLLaV A-Med: The white arrow in the MRI image is pointing to a cystic lesion in the pancreatic head.\\n\\nUser: Could this be a mucinous cystic neoplasm?\\n\\nLLaV A-Med: Yes, it is possible that the cystic lesion in the pancreatic head could be a mucinous cystic neoplasm. Mucinous cystic neoplasms are fluid-filled cysts that can develop in the pancreas. They are usually benign, but in some cases, they can be malignant. Further evaluation and tests would be needed to determine the exact nature of the lesion and the appropriate course of action.\\n\\nUser: Describe the appearance of the mucinous cystic neoplasm.\\n\\nLLaV A-Med: A mucinous cystic neoplasm typically appears as a well-defined, round or oval-shaped cyst with a smooth outer wall. The interior of the cyst is filled with a thick, gel-like fluid called mucin, which gives the cyst its characteristic appearance. The size of the cyst can vary, and in some cases, they can be quite large. It is important to note that the appearance of a mucinous cystic neoplasm on imaging studies, such as MRI or CT scans, can be quite similar to other types of cysts or lesions, so further diagnostic tests and evaluation by a healthcare professional are necessary to confirm the diagnosis.\\n\\nUser: What is the problem with the stomach in this image?\\n\\nLLaV A-Med: The chest X-ray shows a dilated stomach, which means that the stomach appears to be enlarged or abnormally expanded. This could be due to various reasons, such as an obstruction, motility issues, or other underlying conditions affecting the stomach.\\n\\nUser: How is it affecting the diaphragm?\\n\\nLLaV A-Med: The dilated stomach is displacing the diaphragm, which means that it is pushing the diaphragm to the contralateral side. This can potentially affect the normal functioning of the diaphragm and may lead to respiratory issues or other complications.\"}"}
{"id": "GSuP99u2kR", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LLaV A-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day\\n\\nChunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, Jianfeng Gao\\n\\nMicrosoft\\n\\nhttps://aka.ms/llava-med\\n\\nAbstract\\n\\nConversational generative AI has demonstrated remarkable promise for empowering biomedical practitioners, but current investigations focus on unimodal text. Multimodal conversational AI has seen rapid progress by leveraging billions of image-text pairs from the public web, but such general-domain vision-language models still lack sophistication in understanding and conversing about biomedical images. In this paper, we propose a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images. The key idea is to leverage a large-scale, broad-coverage biomedical figure-caption dataset extracted from PubMed Central, use GPT-4 to self-instruct open-ended instruction-following data from the captions, and then fine-tune a large general-domain vision-language model using a novel curriculum learning method. Specifically, the model first learns to align biomedical vocabulary using the figure-caption pairs as is, then learns to master open-ended conversational semantics using GPT-4 generated instruction-following data, broadly mimicking how a layperson gradually acquires biomedical knowledge. This enables us to train a Large Language and Vision Assistant for Biomedicine (LLaV A-Med) in less than 15 hours (with eight A100s). LLaV A-Med exhibits excellent multimodal conversational capability and can follow open-ended instruction to assist with inquiries about a biomedical image. On three standard biomedical visual question answering datasets, fine-tuning LLaV A-Med outperforms previous supervised state-of-the-art on certain metrics. To facilitate biomedical multimodal research, we will release our instruction-following data and the LLaV A-Med model.\\n\\n1 Introduction\\n\\nParallel image-text data is abundantly available in the general domain, such as web images and their associated captions. Generative pretraining has proven effective to leverage this parallel data for self-supervised vision-language modeling, as demonstrated by multimodal GPT-4 [35] and open-sourced efforts such as LLaV A [27]. By instruction-tuning models to align with human intents based on multimodal inputs, the resulting large multimodal models (LMMs) exhibit strong zero-shot task completion performance on a variety of user-oriented vision-language tasks such as image understanding and reasoning, paving the way to develop general-purpose multimodal conversational assistants [22, 2, 23, 10].\\n\\nWhile successful in the general domains, such LMMs are less effective for biomedical scenarios because biomedical image-text pairs are drastically different from general web content. As a result, general-domain visual assistants may behave like a layperson, who would refrain from answering...\"}"}
{"id": "GSuP99u2kR", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"biomedical questions, or worse, produce incorrect responses or complete hallucinations. Much progress has been made in biomedical visual question answering (VQA), but prior methods typically formulate the problem as classification (e.g., among distinct answers observed in the training set) and are not well equipped for open-ended instruction-following. Consequently, although conversational generative AI has demonstrated great potential for biomedical applications [20, 33, 19], current investigations are often limited to unimodal text.\\n\\nIn this paper, we present L\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\nL\\n"}
{"id": "GSuP99u2kR", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"While Visual Med-Alpaca is trained on 54K samples from limited biomedical subject domains, LLaV A-Med is trained a more diverse set.\\n\\nAn automated approach to building models that can answer questions based on biomedical images stands to support clinicians and patients. To describe existing biomedical VQA methods, we make a distinction between discriminative and generative methods. For discriminative methods, VQA is treated a classification problem: models make predictions from a predefined set of answers. While discriminative methods yield good performance, they deal with closed-set predictions \\\\cite{14}, and require mitigation when a customized answer set is provided in at inference \\\\cite{24, 51, 9}. The discriminative formulation is suboptimal towards the goal of developing a general-purpose biomedical assistant that can answer open questions in the wild. To this end, generative methods have been developed to predict answers as a free-form text sequence \\\\cite{5, 30, 44}. Generative methods are more versatile because they naturally cast the close-set questions as a special case where candidate answers are formulated as language instructions.\\n\\nData-Centric Paradigms.\\n\\nLLaV A-Med is similar to prefix tuning of language models (LMs) in \\\\cite{44} in that a new trainable module connects frozen image encoder and causal LM. In \\\\cite{44}, a three-layer MLP network is used to map the visual features into a visual prefix, and the pre-trained LM are GPT2-XL \\\\cite{40}, BioMedLM \\\\cite{45}, and BioGPT \\\\cite{31}, with size varying from 1.5B to 2.7B. By contrast, LLaV A-Med uses a linear projection and a 7BLM \\\\cite{8, 43}. Most importantly, we undertake data-centric paradigm, while all existing methods are model-centric \\\\cite{44}. \\\\cite{44} focuses efforts on exploring various modeling choices. Our main contributions instead comprise proposing a novel data generation method that uses GPT-4 to self-instruct biomedical multimodal instruction-following data using freely-available broad-coverage biomedical image-text pairs extracted from PubMed Central \\\\cite{51}.\\n\\n3 Biomedical Visual Instruction-Following Data\\n\\nThere are a lack of multimodal biomedical datasets to train an instruction-following assistant. To fill this gap, we create the first dataset of its kind from widely existing biomedical image-text pairs, through a machine-human co-curation procedure. It consists of two sets, concept alignment and instruction-following, which are used at different training stages, described in Section 4.\\n\\nBiomedical Concept Alignment Data.\\n\\nFor a biomedical image $X_v$ and its associated caption $X_c$, we sample a question $X_q$, which asks to describe the biomedical image. With $(X_v, X_c, X_q)$, we create a single-round instruction-following example:\\n\\n\\\\begin{quote}\\nHuman: $X_q$\\nAssistant: $X_c$\\n\\\\end{quote}\\n\\nDepending on the length of caption, the question that is sampled either asks to describe the image concisely or in detail. Two lists of questions are provided in Appendix A. In practice, 25% of captions have length less than 30 words in PMC-15M \\\\cite{51}, and thus 30 words is used as the cutoff point to determine which list to choose. We sample 600K image-text pairs from PMC-15M. Though this dataset only presents one-single task instructions, i.e., image captioning, it contains a diverse and representative set of biomedical concept samples from the original PMC-15M \\\\cite{51}.\\n\\nBiomedical Instruction-Tuning Data.\\n\\nTo align the model to follow a variety of instructions, we present and curate diverse instruction-following data with multi-round conversations about the provided biomedical images, by prompting language-only GPT-4. Specifically, given an image caption, we design instructions in a prompt that asks GPT-4 to generate multi-round questions and answers in a tone as if it could see the image (even though it only has access to the text). Sometimes the image caption is too short for GPT-4 to generate meaningful questions and answers. To provide more context regarding the image, we also create a prompt that includes not only captions but also sentences from the original PubMed paper that mentions the image. We also manually curate few-shot examples in the prompt to demonstrate how to generate high-quality conversations based on the provided caption and context. See Appendix A.2 for the prompt and few-shot examples. To collect image captions and their context, we filter PMC-15M to retain the images that only contain a single plot. From them, we sample 60K image-text pairs from the five most common imaging modalities: CXR (chest X-ray), CT (computed tomography), MRI (magnetic resonance imaging), histopathology, and gross (i.e., macroscopic) pathology. We then extract sentences that mention the image from the...\"}"}
{"id": "GSuP99u2kR", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Contrast-enhanced CT scan of the chest for patient #1. A large cavitating lesion was identified in the right upper lobe with pleural involvement and likely extension into the chest wall. In addition, right hilar lymphadenopathy was suspected. This was the primary malignant lesion in this patient.\\n\\nUser: Where is the lesion located?\\nAssistant: The lesion is located in the posterior aspect of the right upper lobe of the lung.\\n\\nUser: Is it extending into the chest wall?\\nAssistant: Yes, the CT scan suggests that the large cavitating lesion in the right upper lobe is likely extending into the chest wall.\\n\\nUser: Can you explain more about the cavitating lesion?\\nAssistant: Certainly. A cavitating lesion refers to an abnormal mass or growth within the lung tissue that has a cavity or hollow space inside it. This cavity can be formed due to the necrosis (death) of the tissue inside the lesion, leading to the breakdown and disintegration of the affected tissue. Cavitating lesions can be caused by various factors, such as infections (like tuberculosis or fungal infections), autoimmune diseases, or malignancies (cancer). In the context of the CT scan provided, the large cavitating lesion in the right upper lobe of the lung is suggestive of a malignant (cancerous) growth. The cavity within the lesion may be a result of the rapid and uncontrolled growth of cancer cells, which can lead to the destruction of surrounding healthy tissue. It's important to note that further diagnostic tests, such as a biopsy, would be required to confirm the malignancy and determine the specific type of cancer involved.\\n\\nFigure 1: An instance of our GPT-4 generated instruction-following data. Top: The figure and caption were extracted from a PubMed Central full-text article [38], along with the corresponding citances (mentions of the given figure in the article). Bottom: The instruction-following data generated by GPT-4 using the text only (caption and citances). Note that the image is not used to prompt GPT-4; we only show it here as a reference.\\n\\nAn example of instruction-following data is shown in Figure 1 shows, and the data statistics is shown Figure 2. We have produced three versions of instruct data when iteratively improving the data quality:\\n\\n(i) 60K-IM. The aforemenioned dataset that considers inline mentions (IM) as the context.\\n(ii) 60K. A dataset of similar size (60K samples) without IM in self-instruct generation.\\n(iii) 10K. A smaller dataset (10 samples) without IM. They are used to ablate our data generation strategies and their impact on trained LLaVA-Med in experiments.\\n\\n4 Adapting Multimodal Conversational Models to the Biomedical Domain\\n\\nWe employ LLaVA, a general-domain multimodal conversation model [27], as the initial general-domain LM, and continuously train the model to the biomedical domain. The same network architecture is utilized, where a linear projection layer connects the vision encoder and the language model.\\n\\nFor LLaVA-Med model training, we use a two-stage procedure, illustrated in Figure 3.\"}"}
{"id": "GSuP99u2kR", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Domain            | Images | QA Pairs |\\n|-------------------|--------|----------|\\n| Chest X-Ray        |        |          |\\n| MRI               |        |          |\\n| Histology         |        |          |\\n| Gross pathology   |        |          |\\n| CT                |        |          |\\n\\n(a) Instruction (b) Responses\\n\\n(c) Frequencies of images and QA pairs on the five domains.\"}"}
{"id": "GSuP99u2kR", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: LLaVA-Med was initialized with the general-domain LLaVA and then continuously trained in a curriculum learning fashion (first biomedical concept alignment then full-blown instruction-tuning). We evaluated LLaVA-Med on standard visual conversation and question answering tasks. Though this paper focuses on biomedical domains, the proposed adaptation procedure is generalizable to other vertical domains such as gaming and education, where novel concepts and domain knowledge are needed to build a helpful assistant. Similar to the 'don't stop pre-training' argument in [12], we consider a scalable pipeline to create domain-specific instruct data from large unlabelled data, and advocate 'don't stop instruction-tuning' to build customized LMM.\\n\\n(iii) Low serving cost. While the model size of general LMM can be giant and serving cost can be prohibitively high, customized LMM has its unique advantages in low serving cost.\\n\\n(iv) Smooth Model Adaptation. Alternatively, the network architecture allows us to initialize the vision encoder from BioMedCLIP [51], or initialize the language model from Vicuna [8], which may lead to higher performance. However, adapting from LLaVA smooth adaptation as a chatbot, where model's behaviors transit from layperson to a professional assistant that is able to provide helpful domain-specific response.\\n\\n5 Experiments\\n\\nWe conduct experiments to study two key components, the quality of the produced multimodal biomedical instruction-following data, and performance of LLaVA-Med. We consider two research evaluation settings: (1) What is the performance of LLaVA-Med as an open-ended biomedcal visual chatbot? (2) How does LLaVA-Med compare to existing methods on standard benchmarks? To clarify, throughout the entire experiments, we only utilize the language-only GPT-4.\\n\\n5.1 Biomedical Visual Chatbot\\n\\nTo evaluate the performance of LLaVA-Med on biomedical multimodal conversation, we construct an evaluation dataset with 193 novel questions. For this test dataset, we randomly selected 50 unseen image and caption pairs from PMC-15M, and generate two types of questions: conversation and detailed description. The conversation data is collected using the same self-instruct data generation pipeline as for the 2nd stage. Detailed description questions were randomly selected from a fixed set of questions to elicit detailed description responses.\\n\\nWe leverage GPT-4 to quantify the correctness of the model answer to a question when given the image context and caption. GPT-4 makes a reference prediction, setting the upper bound answer for the teacher model. We then generate response to the same question from another LMM. Given responses from the two assistants (the candidate LMM and GPT-4), the question, figure caption, and figure context, we ask GPT-4 to score the helpfulness, relevance, accuracy, and level of details of the responses from the two assistants, and give an overall score on a scale of 1 to 10, where a higher score indicates better overall performance. GPT-4 is also asked to provide a comprehensive explanation the evaluation, for us to better understand the models. We then compute the relative score using GPT-4 reference score for normalization.\\n\\nThe results are reported in Table 1. LLaVA-Med with Stage-1 training alone is insufficient as a chatbot, as it loses its ability to follow diverse instructions, though biomedical concept coverage is improved. LLaVA-Med with the full two-stage training consistently outperforms the general domain LLaVA, and training with larger instruct data (from 10K to 60K samples) leads to higher performance. When inline mentions are considered in self-instruct, the generated data 60K-IM slightly improves the chat ability. The results demonstrate the effectiveness of the strategies in biomedical instruction-following data collection as well as the value of dataset assets. Overall, for the best LLaVA-Med, it matches the 50.2% performance of GPT-4. Note that GPT-4 generates response by considering\"}"}
{"id": "GSuP99u2kR", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Performance comparison of multimodal chat instruction-following abilities, measured by the relative score via language GPT-4 evaluation.\\n\\n| Source: | https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7398722/ |\\n\\n**User** What is shown in this image?\\n\\n**LLaV A-Med** The image is a chest X-ray (CXR) that shows bilateral patchy infiltrates, which are areas of increased opacity in the lungs. These infiltrates can be indicative of various lung conditions, such as infections, inflammation, or other lung diseases.\\n\\n**User** Are there any devices implanted in this patient?\\n\\n**LLaV A-Med** Yes, it appears that the patient has several devices implanted. These devices may include endotracheal tubes, central venous catheters, and/or pulmonary artery catheters. These devices are typically used for monitoring and managing various medical conditions, as well as for providing support during surgical procedures.\\n\\n**User** What is shown in this image?\\n\\n**LLaV A** The image shows a ribcage with wires coming out of it, indicating that it is likely a medical illustration or diagram.\\n\\n**User** Are there any devices implanted in this patient?\\n\\n**LLaV A** Yes, there are several devices protruding from the patient's chest, which are likely medical devices used for treatment or monitoring.\\n\\n**User** What is shown in this image?\\n\\n**Language GPT-4** The image shows a chest x-ray taken on Day 2 of a patient's admission after intubation. You can see the endotracheal tube marked by a yellow line, and rapidly worsening ground-glass opacities throughout the lungs. There is also relative subpleural sparing, which is indicated by red arrows.\\n\\n**User** Are there any devices implanted in this patient?\\n\\n**Language GPT-4** Yes, there is an endotracheal tube implanted in the patient, as indicated by the yellow line in the image.\\n\\nTable 2: Example comparison of medical visual chat and reasoning capabilities. The language-only GPT-4 is considered as the performance upper bound, as the golden captions and inline mentions are fed into GPT-4 as the context, without requiring the model to understand the raw image.\\n\\nRecent studies \\\\[25, 29, 52\\\\] raise the self-enhancement bias issue of LLM evaluation, i.e., favoring its own generations. While we agree with the existence of the bias, we believe that GPT-4 is a meaningful and consistent measurement in our settings. We always compare the answers from the candidate model against the GPT-4's answers when computing the relative score, and the ranking of the resulting numbers is consistent, though the numbers themselves might be biased to GPT's answers. The conclusions of our ablation study can be obtained by considering the ranking. Further, by taking the self-enhancement bias into consideration for fairness, we expect that LLaV A-Med actually performs even closer to GPT-4 than the current numbers indicate.\\n\\nIn Table 2, we provide examples on the biomed visual conversations of different chatbots. LLaV A-Med precisely answers the questions with biomedical knowledge, while LLaV A behaves like a...\"}"}
{"id": "GSuP99u2kR", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Comparison with prior state-of-the-art supervised methods. For open-set questions, we report the recall for our free-form text generation method in column \\\\( \\\\text{Open} \\\\). For closed-set questions, we report the accuracy in column \\\\( \\\\text{Closed} \\\\). Bold indicates LLaV A-Med achieves new SoTA. For open-ended questions, prior methods still formulate the problem as classification among distinct answers in the training set, which may overestimate their generalizability as these datasets are unusual in that the test answers are almost always present in training.\\n\\nlayperson, who hallucinate based on commonsense. Since the multimodal GPT-4 is not publicly available, we resort to language-only GPT-4 for comparison. We feed golden captions and inline mentions into GPT-4 as the context, it generates knowledgeable response through re-organizing the information in the conversational manner.\\n\\n5.2 Performance on Established Benchmarks\\n\\nDatasets and Evaluation Metrics. We train and evaluate LLaV A-Med on three biomedical VQA datasets. The data statistics are summarized in Table 8 in Appendix. For the closed-set questions, we report the accuracy/percentage of the ground-truth tokens that appear in the generated sequences. For open-set questions, we use recall to evaluate the ratio that ground-truth tokens appear in the generated sequences. In the literature, the open-set problem is formulated as an closed-set setting, where the unique training answers are considered as the answer candidates, from which the models can select to predict answers for testing questions. Since we do not provide any constraint for the responses to open-set questions, our formulation is closer to open-set nature, but is intrinsically harder.\\n\\nComparisons with SoTA. We compare LLaV A-Med with the general domain LLaV A and existing representative methods in Table 3. First, All LLaV A-Med variants outperform LLaV A. While the difference of language model initialization from LLaV A or Vicuna is minor, the initialization of vision encoder from BioMed CLIP is slightly better than from general-domain CLIP. Second, the fine-tuning performance of LLaV A-Med is higher than supervised SoTA on the closed-set questions on VQA-RAD and PathVQA. This validates LLaV A-Med's strong ability in following instruction to complete biomedical tasks, when clear instructions are provided (e.g., yes or no). Third, for open-set questions, LLaV A-Med achieves SoTA on SLAKE, while its performance is limited on other datasets, especially compared with existing methods. This is perhaps because the open-set biomedical questions can be ambiguous without constraining their excepted answer options. Meanwhile, evaluation of free-form text prediction for medical VQA remains as an open research problem that we are actively exploring.\\n\\nAblation Studies. To study the impact of our curated instruction data and hyper-parameters in the training pipeline, we report the performance of different model variants in Table 4 (a). Several findings are confirmed: (i) LLaV A-Med consistently outperforms LLaV A by a large margin, indicating the effectiveness of our biomedical domain-specific adaptation. The performance gaps on zero-shot are larger than that in fine-tuned settings, showing that LLaV A-Med is clearly a better option than LLaV A when deploying one model for various scenarios in the wild.\"}"}
{"id": "GSuP99u2kR", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| LLaV A-Med Model Variants | VQA-RAD | SLAKE | PathVQA | Average |\\n|--------------------------|---------|-------|---------|---------|\\n|                         | Stage 1 | Stage 2 | FT      | Open Closed | Open Closed | Open Closed | Open Closed |\\n| CLIP Vision Encoder [39], 7B Language Model | 0 1 0 0 | 15.27 12.50 | 18.55 13.46 | 6.26 13.51 |\\n| CLIP Vision Encoder [39], 13B Language Model | 0 3 0 0 | 15.33 15.44 | 23.61 15.38 | 6.35 14.74 |\\n| 10K                     | 1 3 0   | 25.79 57.35 | 31.50 51.68 | 8.49 59.66 |\\n| 10K                     | 3 3 0   | 28.44 59.56 | 22.63 43.99 | 5.40 52.67 |\\n| 10K                     | 1 3 1   | 36.39 55.88 | 71.64 56.49 | 25.50 82.87 |\\n| 10K                     | 1 3 3   | 18.59 55.51 | 78.60 63.46 | 34.02 86.94 |\\n| 60K                     | 1 1 0   | 29.80 55.15 | 38.08 50.00 | 11.70 59.66 |\\n| 60K                     | 1 3 0   | 29.67 60.29 | 35.53 53.85 | 11.76 53.20 |\\n| 60K                     | 1 3 1   | 22.63 58.09 | 72.75 54.33 | 24.19 71.60 |\\n| 60K                     | 1 3 3   | 54.12 64.71 | 79.33 64.90 | 17.18 71.37 |\\n| 60K-IM                  | 1 3 0   | 28.23 61.40 | 39.17 52.16 | 12.30 54.05 |\\n| 60K-IM                  | 1 3 1   | 28.61 56.25 | 70.58 54.57 | 11.17 59.19 |\\n| 60K-IM                  | 1 3 3   | 55.50 66.54 | 80.57 64.18 | 35.88 89.15 |\\n| 60K-IM                  | 1 3 9   | 66.26 80.88 | 82.30 84.86 | 37.59 91.54 |\\n| 60K-IM                  | 1 3 18  | 61.37 81.25 | 84.24 83.17 | 37.88 91.39 |\\n| 60K-IM                  | 1 3 15  | 61.53 84.19 | 83.08 85.34 | 37.95 91.21 |\\n| 60K-IM                  | 1 3 18  | 61.37 81.25 | 84.24 83.17 | 37.88 91.39 |\\n\\n(a) Ablation studies with varying number of training epochs at different stages. 60K-IM indicates the instruct data generated with inline mentions. The gray rows are zero-shot performance of LLaV A-Med trained with different instruct data, they are selected to show in Table 3.\\n\\n(b) The impact of Stage-1 training. All jobs are initialized with LLaV A. \u201cDescription\u201d indicates whether the description text is included in input in Stage-1. \u201cModule\u201d indicates the trainable module in Stage-1. The numbers in green cells shows the effectiveness of Stage-1 training, though the average scores are similar.\\n\\nTable 4: Quantitative results on three established biomedical VQA datasets. \u201cInstruct\u201d is the instruct dataset, \u201cStage 1\u201d and \u201cStage 2\u201d provides the number of training epochs for each stage, \u201cFT\u201d is Fine-Tuning.\\n\\n| Stage 1 | Stage 2 | VQA-RAD | SLAKE | PathVQA |\\n|---------|---------|---------|-------|---------|\\n| 1 3     | Instruct 1 3 | 6.8 19.4 | 10K 0.6 1.8 | 0.3 0.6 | 0.6 1.0 | 1.0 2.5 |\\n| 60K     | 2.6 8.0 |\\n\\nTable 5: Running time (hours) for 1 and 3-epoch training, with batch size 128 on eight A100 GPUs.\"}"}
{"id": "GSuP99u2kR", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Instruction in Stage 1 may encourage the model to lose its ability in following diverse instructions. Instruction-following data in Stage 2 is critical, and the performance is generally improved when the instruction data amount increases from 10K to 60K. The 60K-IM data provides the best averaged zero-shot and fine-tuned performance, respectively, validating the effectiveness of considering inline mention as external knowledge in data creation.\\n\\nFine-tuning longer on downstream datasets till 9 epochs benefits the performance, especially on checkpoints with 3-epoch training in Stage 2. Increasing language model size from 7B to 13B improves the overall zero-shot performance and fine-tuned performance. We suggest practitioners to choose the appropriate quality-cost trade-off, by referring to the running time in Table 5.\\n\\nWhen downstream samples are available, fine-tuning itself provides the largest performance gain (54.57-35.23=19.34). However, by training with high-quality instruction data such as 60K-IM in the Stage-2, we can further boost performance significantly (65.30-54.57=10.73). Stage-2 itself is not as effective as direct fine-tuning on downstream tasks, meanwhile Stage 1, 2, and fine-tuning are all required for the best performance.\\n\\nWe study the confidence interval (CI) by running the same experiment configuration three times, and report the standard derivation (std) in Table 10 (b). Note it is infeasible to provide CIs for all experiments due to the large number of jobs. Though the std of averaged results of three datasets are small, there is some evidence that results on one single dataset might be statistically significant. We suggest the users with the resource to run multiple jobs using the released code to draw more rigorous conclusions for experiments of interest & importance, to alleviate the limitation from the lack of CIs.\\n\\nImpact of Stage-1. We consider more strategies to train Stage-1 in addition to tuning the linear projection layer only. Please see the representative results in Table 4 (b), and the detailed discussion in Section C.2. It yields higher average performance at the early epochs such as epoch 1, when training LLaV A-Med from LLaV A using Stage-2 only, without Stage-1. As the training continues to epoch 3 or more, all training methods perform similarly measured by the average scores. However, training with Stage-1 consistently provides higher performance than training without Stage-1 on the PathVQA dataset, which indicates the Stage-1 can benefit certain biomedical domains, when related additional knowledge is learned. Our suggestions on the necessity of Stage-1 training are\\n\\n1. If LLaV A-Med is trained with a customized vision encoder or LLM that are not included in LLaV A (i.e., no LLaV A checkpoint is available), Stage-1 is critical in aligning the multimodal feature space, and yield good performance.\\n2. If LLaV A-Med is trained by initializing from LLaV A, the Stage-1 training is optional. In this case, it is more cost-efficient to skip Stage-1 and train Stage-2 only, which can quickly provide good performance on the vertical domains with less cost. However, for scenarios with a large number of in-domain image-text pairs that pre-trained LLaV A does not have much related knowledge, we suggest adding the Stage-1 training on the in-domain pairs: The best strategy in this case is full-model fine-tuning of the LLM, and removing the instruction text of describing the image.\\n\\n6 Conclusions\\n\\nWe present LLaV A-Med, a large language-and-vision model for the biomedical domain. To create this model, we create high-quality biomedical language-image instruction-following dataset using a self-instruct approach to build a data curation pipeline using language-only GPT-4 and external knowledge. LLaV A-Med demonstrates strong excellent chat abilities with domain knowledge, and outperforms previous supervised SoTA on three VQA datasets on certain metrics with subsequent fine-tuning.\\n\\nWhile we believe that LLaV A-Med represents a significant step towards building a useful biomedical visual assistant, we note that LLaV A-Med is limited by hallucinations and weak in-depth reasoning common to many LMMs. We discuss the limitations of LLaV A-Med and the utilization of GPT-4 API for data generation pipeline in Section B. Future work is directed toward improving quality and reliability of LLaV A-Med. We hope the LLaV A-Med recipe can inspire the applications of training large language-and-vision assistant to more vertical domains.\\n\\nReferences\\n\\n[1] Clinical Camel. [https://wanglab.ml/clinical_camel.html](https://wanglab.ml/clinical_camel.html), 2023.\"}"}
{"id": "GSuP99u2kR", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.\\n\\nMalek Ayoub, Megan Quamme, Abdul-Rahman K Abdel-Reheem, Poe Lwin, and Megan K Quamme. Covid or not covid? a great mimicker behind the smoke screen. Cureus, 13(11), 2021.\\n\\nBappy Basak, Alexander Haragan, Michael Shackcloth, and Joyce Thekkudan. Chondromyxoid fibroma of the rib: A rare benign tumor with potential for local recurrence. Cureus, 13(10), 2021.\\n\\nYakoub Bazi, Mohamad Mahmoud Al Rahhal, Laila Bashmal, and Mansour Zuair. Vision\u2013language model for visual question answering in medical imagery. Bioengineering, 2023, 8.\\n\\nAnchit Bharat, Nikita Jain, Belaal Sheikh, Hafiz Jeelani, and Maryna Shayuk. Vaping-induced lung injury: An uncharted territory. Cureus, 12, 07 2020.\\n\\nBenedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel C Castro, Anton Schwaighofer, Stephanie Hyland, Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle, et al. Making the most of text semantics to improve biomedical vision\u2013language processing. In ECCV. Springer, 2022.\\n\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\\n\\nSedigheh Eslami, Christoph Meinel, and Gerard De Melo. Pubmedclip: How much does clip benefit visual question answering in the medical domain? In Findings of the Association for Computational Linguistics: EACL 2023, pages 1151\u20131163, 2023.\\n\\nZhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al. Vision-language pre-training: Basics, recent advances, and future trends. Foundations and Trends\u00ae in Computer Graphics and Vision, 2022.\\n\\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1):1\u201323, 2021.\\n\\nSuchin Gururangan, Ana Marasovi\u00b4c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith. Don't stop pretraining: Adapt language models to domains and tasks. arXiv preprint arXiv:2004.10964, 2020.\\n\\nTianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander L\u00f6ser, Daniel Truhn, and Keno K Bressem. Medalpaca\u2013an open-source collection of medical conversational ai models and training data. arXiv preprint arXiv:2304.08247, 2023.\\n\\nXuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286, 2020.\\n\\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clinicalbert: Modeling clinical notes and predicting hospital readmission. arXiv preprint arXiv:1904.05342, 2019.\\n\\nAlistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports. Scientific data, page 317, 2019.\\n\\nJason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. A dataset of clinically generated visual questions and answers about radiology images. Scientific data, 2018.\"}"}
{"id": "GSuP99u2kR", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. *Bioinformatics*, 36(4):1234\u20131240, 2020.\\n\\nPeter Lee, Sebastien Bubeck, and Joseph Petro. Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine. *New England Journal of Medicine*, 388(13):1233\u20131239, 2023.\\n\\nPeter Lee, Carey Goldberg, and Isaac Kohane. The ai revolution in medicine: Gpt-4 and beyond. 2023.\\n\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive NLP tasks. *NeurIPS*, 2020.\\n\\nChunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. *arXiv* preprint arXiv:2309.10020, 2023.\\n\\nChunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, and Jianfeng Gao. ELEV ATER: A benchmark and toolkit for evaluating language-augmented visual models. In *NeurIPS Track on Datasets and Benchmarks*, 2022.\\n\\nPengfei Li, Gang Liu, Lin Tan, Jinying Liao, and Shenjun Zhong. Self-supervised vision-language pretraining for medical visual question answering. *arXiv preprint arXiv:2211.13594*, 2022.\\n\\nRuosen Li, Teerth Patel, and Xinya Du. Prd: Peer rank and discussion improve large language model based evaluations. *arXiv preprint arXiv:2307.02762*, 2023.\\n\\nBo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering. In *International Symposium on Biomedical Imaging (ISBI)*. IEEE, 2021.\\n\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. *arXiv* preprint arXiv:2304.08485, 2023.\\n\\nHaotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng Gao, Yong Jae Lee, and Chunyuan Li. Learning customized visual models with retrieval-augmented knowledge. *arXiv preprint arXiv:2301.07094*, 2023.\\n\\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg evaluation using gpt-4 with better human alignment. *arXiv preprint arXiv:2303.16634*, 2023.\\n\\nYunyi Liu, Zhanyu Wang, Dong Xu, and Luping Zhou. Q2atransformer: Improving medical vqa via an answer querying decoder. *arXiv preprint arXiv:2304.01611*, 2023.\\n\\nRenqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt: generative pre-trained transformer for biomedical text generation and mining. *Briefings in Bioinformatics*, 2022.\\n\\nHassan Mirmohammad Sadeghi, Abbas Karimi, Samira Derakhshan, Pouyan Aminishakib, and Kiarash Parchami. Conventional osteosarcoma of the mandible: Report of a rare case. *Clinical Case Reports*, 9(9):e04843, 2021.\\n\\nHarsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems. *arXiv preprint arXiv:2303.13375*, 2023.\\n\\nOpenAI. ChatGPT. [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/), 2022.\\n\\nOpenAI. GPT-4 technical report. [https://arxiv.org/abs/2303.08774](https://arxiv.org/abs/2303.08774), 2023.\"}"}
{"id": "GSuP99u2kR", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kyriakos A Papavasiliou, Dimitrios Stamiris, Stavros Stamiris, Antonia Bintoudi, and Eleftherios Tsiridis. Quadratus femoris partial tear secondary to occult ischiofemoral impingement. Journal of Orthopaedic Case Reports, 11(9):7, 2021.\\n\\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with GPT-4. arXiv preprint arXiv:2304.03277, 2023.\\n\\nRoger Kevin Pringle and Lawrence H Wyatt. The appropriate use of radiography in clinical practice: a report of two cases of biomechanical versus malignant spine pain. Chiropractic & Osteopathy, 14(1):1\u20138, 2006.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019.\\n\\nGeorge Shih, Carol C Wu, Safwan S Halabi, Marc D Kohli, Luciano M Prevedello, Tessa S Cook, Arjun Sharma, Judith K Amorosa, Veronica Arteaga, Maya Galperin-Aizenberg, et al. Augmenting the national institutes of health chest radiograph dataset with expert annotations of possible pneumonia. Radiology: Artificial Intelligence, 2019.\\n\\nChang Shu, Baian Chen, Fangyu Liu, Zihao Fu, Ehsan Shareghi, and Nigel Collier. Visual med-alpaca: A parameter-efficient biomedical llm with visual capabilities. 2023.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\\n\\nTom van Sonsbeek, Mohammad Mahdi Derakhshani, Ivona Najdenkoska, Cees GM Snoek, and Marcel Worring. Open-ended medical visual question answering through prefix tuning of language models. arXiv preprint arXiv:2303.05977, 2023.\\n\\nA Venigalla, J Frankle, and M Carbin. BiomedLM: a domain-specific large language model for biomedical text. MosaicML. Accessed: Dec 23, 2022.\\n\\nHaochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo: Tuning llama model with chinese medical knowledge, 2023.\\n\\nChaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-llama: Further finetuning llama on medical papers. arXiv preprint arXiv:2304.14454, 2023.\\n\\nHonglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yuxiao Liu, Qian Wang, and Dinggang Shen. Doctorglm: Fine-tuning your chinese doctor is not a herculean task. arXiv preprint arXiv:2304.01097, 2023.\\n\\nLi Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You. Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. arXiv preprint arXiv:2303.14070, 2023.\\n\\nMansoor Zafar, Abdul Wahab Paracha, Muteeb Ashraf, Tila Muhammad, Mark Whitehead, Muhammad Toqeer, and Abdul Paracha. Delayed spontaneous regression of metastatic gastric cancer: A case report of a rare finding. Cureus, 13(12), 2021.\\n\\nSheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, Cliff Wong, et al. Large-scale domain-specific pretraining for biomedical vision-language processing. arXiv preprint arXiv:2303.00915, 2023.\\n\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.\"}"}
{"id": "GSuP99u2kR", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Checklist\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] We build a large-scale multi-modal biomedical instruction tuning dataset.\\n   (b) Did you describe the limitations of your work? [Yes] See \u201cComparisons with SoTA\u201d and \u201cConclusions\u201d.\\n   (c) Did you discuss any potential negative societal impacts of your work? [Yes] Yes we discussed the safety and ethical filtering policy of our data generation process in section 3.\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] We read it and our dataset does not contain sensitive items.\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n   (b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See project website https://aka.ms/llava-med and Appendix for dataset links\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Method & Experiment\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] Error bars are not most applicable in our case, instead we report comprehensive ablation results in section 5.2.\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See section 5.2\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes]\\n   (b) Did you mention the license of the assets? [Yes] See https://aka.ms/llava-med.\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] See project website https://aka.ms/llava-med and Appendix for dataset links.\\n   (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] We follow the copyright of the original data source from PubMedCentral.\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] We respect the personally identifiable in the images such as the right to be portrayed, which is the responsibility of the copyright owner.\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\"}"}
{"id": "GSuP99u2kR", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 Open-sourced Medical Visual Instruction-Following Datasets\\nTraining.\\nThe Stage-1 data follows CC BY NC 4.0 license. As described in Section 3, we create three versions of datasets for our biomedical visual instruction tuning in the 2nd stage.\\n\\n- **10K**: https://hanoverprod.blob.core.windows.net/public/med_llava/finetune_pmc/finetune_postprocess_caption_10k.json\\n- **60K**: https://hanoverprod.blob.core.windows.net/public/med_llava/finetune_pmc/finetune_postprocess_caption_cleaned_60k.json\\n- **60K-IM**: https://hanoverprod.blob.core.windows.net/public/med_llava/finetune_pmc/finetune_postprocess_caption_im_cleaned_60k.json\\n\\nEvaluation.\\nAs described in Section 5.1, to evaluate the biomedical chat ability, we create an evaluation set.\\n\\nhttps://hanoverprod.blob.core.windows.net/public/med_llava/multimodal_chat_eval/qa_50_images.jsonl\\n\\nImages.\\nThe image url paths can be seen in the files:\\n\\n- **Training**: https://hanoverprod.blob.core.windows.net/public/med_llava/images/finetune_image_urls.jsonl\\n- **Evaluation**: https://hanoverprod.blob.core.windows.net/public/med_llava/images/eval_image_urls.jsonl\\n\\nA.2 Prompts\\nInstructions for brief image description.\\nThe list of instructions used to briefly describe the image content are shown in Table 6. They present the same meaning with natural language variance.\\n\\n- \u201cDescribe the image concisely.\u201d\\n- \u201cProvide a brief description of the given image.\u201d\\n- \u201cOffer a succinct explanation of the picture presented.\u201d\\n- \u201cSummarize the visual content of the image.\u201d\\n- \u201cGive a short and clear explanation of the subsequent image.\u201d\\n- \u201cShare a concise interpretation of the image provided.\u201d\\n- \u201cPresent a compact description of the photo\u2019s key features.\u201d\\n- \u201cRelay a brief, clear account of the picture shown.\u201d\\n- \u201cRender a clear and concise summary of the photo.\u201d\\n- \u201cWrite a terse but informative summary of the picture.\u201d\\n- \u201cCreate a compact narrative representing the image presented.\u201d\\n\\nTable 6: The list of instructions for brief image description.\\n\\nInstructions for detailed image description.\\nThe list of instructions used to describe the image content in detail are shown in Table 7. They present the same meaning with natural language variance.\\n\\nSelf-instruct prompts.\\nThe prompts used to generate medical instruction following data are shown in Figure 4 and Figure 5.\"}"}
{"id": "GSuP99u2kR", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: The list of instructions for detailed image description.\\n\\nPromoting GPT-4 to generate medical visual instruction-following messages.\\n\\nMessages:\\n\\n```\\n{\"role\":\"system\", \"content\":\"You are an AI assistant specialized in biomedical topics. You are provided with a text description (Figure Caption) of a figure image from a biomedical research paper. In some cases, you may have additional text (Figure Context) that mentions the image. Unfortunately, you don't have access to the actual image.\\n\\nBelow are requirements for generating the questions and answers in the conversation:\\n\\n- Avoid quoting or referring to specific facts, terms, abbreviations, dates, numbers, or names, as these may reveal the conversation is based on the text information, rather than the visual aspects of the image.\\n- Do not use phrases like \\\"mentioned\\\", \\\"caption\\\", \\\"context\\\" in the conversation. Instead, refer to the information as being \\\"in the image.\\\"\\n- Ensure that questions are diverse and cover a range of visual aspects of the image.\\n- The conversation should include at least 2-3 turns of questions and answers about the visual aspects of the image.\\n- Answer responsibly, avoiding overconfidence, and do not provide medical advice or diagnostic information. Encourage the user to consult a healthcare professional for advice.\"}\"}\\n```\\n\\nFigure 4: We use to prompt GPT-4 to generate medical visual instruction-following data. Manually curated few-shot examples are included in the prompt, where each example has input `sample['context']` and output `sample['response']`. Please see Figure 5 for one of the few-shot examples.\"}"}
