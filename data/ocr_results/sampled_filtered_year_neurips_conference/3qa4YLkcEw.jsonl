{"id": "3qa4YLkcEw", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models\\n\\nAbstract\\n\\nAligned large language models (LLMs) demonstrate exceptional capabilities in task-solving, following instructions, and ensuring safety. However, the continual learning aspect of these aligned LLMs has been largely overlooked. Existing continual learning benchmarks lack sufficient challenge for leading aligned LLMs due to their homogeneous task types and low task complexity. To bridge this gap, we introduce TRACE, a benchmark designed to rigorously assess continual learning capabilities in LLMs. TRACE comprises eight challenging tasks from the scope of domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning. Through systematic experiments on TRACE with six different aligned models ranging from 7B to 70B, we discovered significant declines in both general performance and instruction-following abilities. For example, the accuracy of llama2-chat 13B on the gsm8k dataset declined precipitously from 43.14% to 2.12% after training on our datasets. This highlights the challenge of finding a suitable tradeoff between achieving performance on specific tasks while preserving the original prowess of LLMs. Our results demonstrate that integrating task-specific cues with meta-rationales significantly reduces catastrophic forgetting and improves task convergence, offering a viable strategy to enhance the adaptability of LLMs in dynamic environments.\\n\\nIntroduction\\n\\nLarge Language Models (LLMs) [1; 2] have revolutionized natural language processing through a two-step process: initial pretraining on extensive corpora, followed by fine-tuning on human-generated instructions and preference data, aligning them with human language and intentions. Aligned LLMs have showcased impressive capabilities and ensured safer responses. However, as the demands for language models grow, there's a pressing need to enhance their abilities in areas such as domain-specific knowledge [3; 4], multilingual proficiency [5], complex task-solving [6], and tool usage [7]. Yet, retraining and realigning them from scratch to meet these demands is impractical due to prohibitive training costs and the challenge of acquiring high-quality data. Therefore, incrementally training existing Aligned LLMs through continual learning (CL [8]) is crucial. However, when dealing with tasks in sequential manners, it is challenging to retain the performance of previous tasks, which is known as \\\"catastrophic forgetting\\\" [9]. Therefore, we prompt the pressing question: To what degree do Aligned LLMs exhibit catastrophic forgetting when subjected to incremental training? Existing continual learning benchmarks [10; 11; 12] are not suitable for evaluating the state-of-the-art LLMs. Firstly, many of these benchmarks predominantly consist of simplistic natural language understanding datasets. These tasks, due to their inherent simplicity, fail to challenge the capabilities of large-scale models adequately. Secondly, prior benchmarks have primarily focused on metrics that assess the performance of the models on target sequential tasks. Yet, for aligned models, aspects like\"}"}
{"id": "3qa4YLkcEw", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Domain-specific Multi-lingual Code completion Mathematical reasoning Large language model TRACE Sequential tasks Metrics Performance of sequential tasks General Ability Delta Instruction Follow Delta Safety Delta\\nSequential training Evaluation Task 1 Task 2 Task N...\\n\\nFigure 1: An overview of TRACE benchmark. TRACE consists of two main components: 1) A selection of eight datasets constituting a tailored set of tasks for continual learning, covering challenges in domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning. 2) A post-training evaluation of LLM capabilities. In addition to traditional continual learning metrics, we introduce General Ability Delta, Instruction Following Delta, and Safety Delta to evaluate shifts in LLM\u2019s inherent abilities.\\n\\nIn this paper, we present TRACE, a continual learning benchmark designed for aligned LLMs. Our benchmark consists of eight distinct datasets spanning challenging tasks including domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning. All datasets have been standardized into a unified format, simplifying the evaluation process. To evaluate continual learning in aligned LLMs, we introduce three metrics: \u201cGeneral Ability Delta,\u201d \u201cInstruction Following Delta,\u201d and \u201cSafety Delta\u201d to assess models\u2019 forgetfulness in such scenarios.\\n\\nWe conduct a comprehensive evaluation of 6 aligned LLMs on TRACE. Evaluation results reveal several key findings: 1) Nearly all models exhibit a significant decline in general abilities after training on TRACE, especially in math and reasoning. For instance, the accuracy of llama2-chat 13B on the gsm8k dataset dropped from 43.14% to merely 2.12%. 2) Catastrophic forgetting remains a substantial issue for LLMs and does not diminish with the model size increase. Llama2-chat 70B also shows significant forgetting (with -17.5% backward transfer) on previous tasks. 3) Full-parameter training, compared to LoRA training, more easily fits the target tasks, but it also leads to a more pronounced decline in general abilities. 4) LLMs\u2019 instruction-following capabilities also suffer a significant reduction after continual learning.\\n\\nThrough experimentation, we observed that tasks augmented with reasoning paths are notably effective in preserving certain capabilities of LLMs, preventing them from substantial declines. Such findings lead us to ponder on leveraging a model\u2019s inherent strengths for rapid transfer on new tasks, rather than starting the learning curve from scratch. This motivation birthed our novel training strategy: Reasoning-augmented Continual Learning (RCL). RCL prompts the model to generate task analyses and rationales during training. As our results indicate, this approach not only boosts performance on target tasks but also significantly upholds the inherent strengths of LLMs.\\n\\n2 Related Work\\n2.1 Continual Learning\\nContinual learning [8] aims to develop learning algorithms that can accumulate knowledge on non-stationary data. Existing works can be broadly categorized into rehearsal-based, regularization-based, and architecture-based approaches. Rehearsal-based approaches [13; 14] leverage a memory buffer that stores examples from previous tasks, training the model jointly with the current task. Regularization-based approaches [15; 16; 17] incorporate additional terms into the loss function to penalize changes in crucial weights. Architecture-based approaches [18; 12] focus on dynamically expanding model capacity or isolating existing model weights to mitigate interference between new and old tasks.\"}"}
{"id": "3qa4YLkcEw", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.2 CL Benchmarks in NLP\\n\\nThe most recognized CL benchmark for NLP encompasses five text classification datasets [10], including AG News, Amazon Reviews, Yelp Reviews, DBpedia, and Yahoo Answers. Building upon this, [12] proposed a long CL benchmark which fuses the aforementioned five datasets with an additional four from the GLUE benchmark [19], five from the SuperGLUE benchmark [20], and the IMDB dataset [21]. However, this benchmark is limited in scope as it solely emphasizes Natural Language Generation (NLG) tasks and is restricted to English, thus lacking task diversity. In contrast, TRACE is a more varied and challenging benchmark, designed to test aligned LLMs across multiple sequential tasks. It assesses general capability, instruction adherence, and safety shifts. Recognizing that TRACE, like earlier benchmarks, may be incorporated into future pre-training, we acknowledge potential impacts on its long-term efficacy as a CL benchmark. Nonetheless, the current findings from TRACE provide essential insights into catastrophic forgetting in LLMs.\\n\\n3 Preliminaries\\n\\nContinual learning [22; 8] focuses on developing learning algorithms to accumulate knowledge on non-stationary data. In supervised continual learning, a sequence of tasks \\\\( \\\\{D_1, \\\\ldots, D_T\\\\} \\\\) arrive in a streaming fashion. Each task \\\\( D_t = \\\\{(x_{ti}, y_{ti})\\\\}_{n_t=1}^{n_t} \\\\) contains a separate target dataset, where \\\\( x_{ti} \\\\in X_t, y_{ti} \\\\in Y_t \\\\). A single model needs to adapt to them sequentially, with only access to \\\\( D_t \\\\) at the \\\\( t \\\\)-th task.\\n\\nIn general, given a prediction model \\\\( h_{\\\\Theta} \\\\) parameterized by \\\\( \\\\Theta \\\\), continual learning seeks to optimize for the following objective across all tasks:\\n\\n\\\\[\\n\\\\max_{\\\\Theta} \\\\prod_{k=1}^{T} \\\\prod_{x,y \\\\in D_k} \\\\log p_{\\\\Theta}(y|x) \\\\tag{1}\\n\\\\]\\n\\nIn this paper, we utilize overall performance (OP [23]), forward transfer (FWT [13]), and backward transfer (BWT [13]) scores as the main metrics. After incrementally learning the \\\\( t \\\\)-th task, the model's score on the \\\\( i \\\\)-th task (where \\\\( i \\\\leq t \\\\)) is denoted as \\\\( R_{D_t,i} \\\\).\\n\\n4 TRACE: A Comprehensive Benchmark for CL in LLMs\\n\\nTRACE is designed to offer a comprehensive continual learning evaluation for LLMs. Illustrated in Figure 1, TRACE encompasses two primary components: a curated set of tasks tailored for continual learning, followed by an in-depth evaluation of an LLM's post-training capabilities. In this section, we detail TRACE's sequential tasks and introduce our evaluation metrics. In Section 4.4, we evaluate six models using TRACE and present our key findings.\\n\\n4.1 Data Creation\\n\\nThere are three principles for the creation of TRACE. First, the datasets should be novel enough that most LLMs have not been trained on them. Second, they should be challenging for large language models. Third, a variety of tasks should be covered in our benchmark.\\n\\nAccording to these three principles, in this section, we will provide a detailed introduction to the data collection process for each dataset. As these datasets have varying sizes, we create a balanced version by randomly sampling 5000 training examples and 2000 testing examples from the original datasets. As shown in Table 1, we get 40,000 training examples and 16,000 testing examples in total.\\n\\nDomain-Specific.\\n\\nScienceQA [24] is a multi-hop QA dataset collected from elementary and high school science curricula, with a rich domain diversity from natural science, social science, and language science, requiring the model of reasoning ability and science knowledge. In TRACE, we include only non-multimodal examples to exclusively test LLM performance.\\n\\nFOMC [25] is a hawkish-dovish classification task, which is novel in the financial domain. The dataset is divided into three subsets: data on meeting minutes, press conference data, and speech data. We use a combination of them.\\n\\nMeetingBank [26] is a new benchmark dataset for city council meeting summarization, an unstudied domain. It demands a global understanding of the whole long context.\"}"}
{"id": "3qa4YLkcEw", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Source                  | Domain          | Avg len | Metric | Language |\\n|-------------------------|-----------------|---------|--------|----------|\\n| data                    | Science         | 210     | Accuracy | English  |\\n|                         | Finance         | 51      | Accuracy | English  |\\n| MeetingBank             | Meeting         | 2853    | ROUGE-L | English  |\\n|                         | C-STANCE        | 127     | Accuracy | Chinese  |\\n|                         | 20Minuten       | 382     | SARI    | German   |\\n|                         | Code completion | 422     | Edim    | Python   |\\n|                         | Mathematical    | 32      | Accuracy | English  |\\n|                         | Mathematical    | 21      |         |          |\\n\\n**Table 1:** An overview of dataset statistics in TRACE. 'Source' indicates the context's origin. 'Avg len' represents word count for English, German, and code datasets, and character count for Chinese. 'SARI' is a score specific to simplification.\\n\\nMulti-lingual LLMs' cross-lingual abilities are constrained by their vocabulary and pre-training corpus. For instance, LLaMA's vocabulary contains few Chinese tokens, affecting its efficiency with Chinese text. C-STANCE [27] is the first Chinese dataset for zero-shot stance detection collected from Sina Weibo, one of the most popular Chinese social media sites. It includes two challenging subtasks: target-based stance detection and domain-based stance detection. In TRACE, we include the target-based one, which means the targets in testing examples are unseen during training. 20Minuten [28] is a text simplification dataset consisting of full articles paired with shortened, simplified summaries from the Swiss news magazine. We use this dataset to evaluate the ability to generate German text.\\n\\nCode completion is another challenging task to evaluate long context modeling ability [29], and it is one of the most widely used features in software development through IDEs. We select the line-level code completion task of CodeXGLUE [30], which requires the model to generate the next line given the lengthy code input. The corpus Py150 [30] contains 150,000 Python programs collected from GitHub repositories. Since the golden labels of the testing dataset are not available by [30], we randomly divide each Python code in Py150 into two parts, taking the first part as inputs and the next line as labels.\\n\\nMathematical reasoning. Mathematical problems are always used to evaluate the reasoning ability of models. NumGLUE [31] is an 8-task benchmark far from solved including state-of-the-art large-scale language models performing significantly worse than humans. Both of the two tasks require arithmetic reasoning ability. It is worth noting that both datasets have original labels consisting only of numbers, without associated inference processes. The first one evaluates the common sense of models, while the second one requires some grade-school scientific knowledge.\\n\\n### 4.2 CL Metrics Design\\n\\nUnlike traditional continual learning benchmarks focused on sequential target tasks, evaluating aligned LLMs should also account for the preservation of their inherent capabilities. SoTA LLMs, through instruction tuning, exhibit impressive task-solving abilities. Aligning these models with human preferences further boosts their safety and usefulness. Hence, TRACE introduces a broader evaluation, including three unique metrics.\\n\\nIn the TRACE benchmark, we incorporate three metrics tailored to evaluate distinct dimensions of model performance following training: **General Ability Delta**, **Instruction Following Delta**, and **Safety Delta**. Each metric is designed to capture shifts in different areas\u2014general abilities, instruction-following, and safety of responses, respectively. Despite their targeted focuses, all three...\"}"}
{"id": "3qa4YLkcEw", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"metrics are calculated using a consistent formula:\\n\\\\[\\n\\\\Delta R_X^t = \\\\frac{1}{N} \\\\sum_{i=1}^{N} (R_X^t,i - R_X^0,i)\\n\\\\]\\n\\nwhere \\\\(X\\\\) represents the specific metric focus\u2014general ability, instruction-following, or safety\u2014and \\\\(N\\\\) is the number of datasets considered within each respective category. This unified formula underscores the systematic approach TRACE adopts to evaluate different dimensions of LLM performance enhancements or declines post-training.\\n\\nIn the development of evaluation metrics, integrating existing and newly introduced indicators into a single, unified metric might appear straightforward through a weighted summary. However, the implementation of such a composite metric necessitates careful prioritization of the capabilities most valued by users. Therefore, this paper does not propose a universal metric for ranking models.\\n\\n4.3 Experimental Setup\\n4.3.1 Models & Baselines\\nTo evaluate the resilience of aligned LLMs from diverse training backgrounds and strategies, we select six backbones from three organizations: Meta: LLaMa-2-7B-Chat, LLaMa-2-13B-Chat, LLaMa-2-70B-Chat [2], BaiChuan: Baichuan 2-7B-Chat [32], and Large Model Systems Organization: Vicuna-13B-V1.5, Vicuna-7B-V1.5 [33].\\n\\nWe evaluate the performance of LLMs in a continual learning setting using different approaches:\\n- **Sequential Full-Parameter Fine-Tuning (SeqFT)**: It trains all model parameters in sequence.\\n- **LoRA-based Sequential Fine-Tuning (LoraSeqFT)**: Only the low-rank LoRA matrices are fine-tuned, leaving the LLM backbone fixed [34]. This method is chosen based on prior findings of reduced forgetting with \\\"Efficient Tuning\\\" [35].\\n- **Replay-based Sequential Fine-Tuning (Replay)**: We incorporate data from LIMA along with 10% of data from previous tasks into our training dataset.\\n- **In-Context Learning (ICL)**: Task demonstrations are supplied as part of the language prompt, acting as a form of prompt engineering [36]. A 6-shot setting is used for our experiments.\\n\\nOther Continual Learning Baselines\\nWe also demonstrate the experiment results on a few continual learning baselines, including EWC [15], ODG [37], PP [12], L2P [38], LFPT5 [39]. Given that EWC and ODG require storing parameters or gradients from past tasks, which is impractical for the full parameter setting of LLMs, we validate these methods using LoRA.\\n\\n4.3.2 Datasets\\nTo evaluate a model's **general ability**, we assess across six key dimensions:\\n- **Factual Knowledge**: Using MMLU dataset [40], reporting 5-shot accuracy.\\n- **General Reasoning**: Evaluated with BBH [41], reporting EM scores with chain-of-thought prompts with 3-shot in-context examples.\\n- **Multilinguality**: Using TyDiQA [42], a multilingual QA benchmark across 11 languages, reporting 0-shot F1 scores.\\n- **Commonsense Reasoning**: Assessed with PIQA [43], reporting 0-shot accuracy.\\n- **Code Generation**: Using MBPP [44], reporting 0-shot Pass@1.\\n- **Reading Comprehension**: Using BoolQ [45], reporting 0-shot accuracy.\\n\\nFor **instruction-following capability**, we use Self-instruct dataset [46], comprising 175 user-oriented prompts, and the LIMA dataset [47], which assembles 300 prompts from community Q&A and manual examples.\\n\\nTo evaluate **safety** changes, we use the CoNa dataset [48], which consists of 178 expert-annotated samples focused on instructions related to hateful speech generation.\"}"}
{"id": "3qa4YLkcEw", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Overall Performance (OP), Forward Transfer (FWT), and Backward Transfer (BWT) for all baseline models and 4 baseline methods.\\n\\n4.3.3 Implementation Details\\n\\nIn the training phase, we trained models with and without LoRA adapters using 5000 samples at learning rates of 1e-5 and 1e-4, respectively. For the testing phase, we use a temperature of 0.1. All our training and inference experiments were conducted on a machine equipped with 8x80G Nvidia A100. All general benchmark evaluations were conducted using the Open-Compass toolkit [49], adopting its default configuration. The detailed settings can be found in Appendix A.\\n\\n4.4 Main Results\\n\\nWe conducted experiments with various task orders. Results for the default order are presented in the main text, while results for additional orders are detailed in Appendix D.7.\\n\\n4.4.1 Performance of Target Sequential Tasks\\n\\nTable 2 showcases the performance of six distinct LLMs on TRACE benchmark, after their continual learning phase. From this evaluation, we can draw the following conclusions:\\n\\nIn-Context Learning (ICL)\\n\\nICL methods generally perform lower than SeqFT and Replay methods. This suggests that the TRACE benchmark is indeed challenging, and LLMs can't readily identify solutions just through simple demonstrations.\\n\\nReplay Performance\\n\\nAmong all the baselines, Replay achieved the highest OP score. With its BWT score being positive, it indicates that Replay effectively retains its performance on sequential tasks without significant forgetting. This makes Replay a straightforward and efficient strategy in a continual learning context.\"}"}
{"id": "3qa4YLkcEw", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"when the focus is primarily on sequential tasks, full parameter fine-tuning should be prioritized over\\n\\nTable 3 presents the evaluations of various LLM models concerning general abilities. The degree of\\n\\nThe Replay method proves beneficial in preserving reasoning\\n\\nFrom the Method Perspective\\n\\nFrom the Task Perspective\\n\\nFor llama2-7b model, most models exhibit a significant drop in performance on MMLU,\\n\\nDespite the presence of CoT prompts, all models show a noticeable\\n\\nto catastrophic forgetting in continual learning scenarios. Indicating a vulnerability of larger models to catastrophic forgetting in continual learning scenarios.\\n\\nGeneral ability forgetting in LLMs can be analyzed from three perspectives.\\n\\nFrom the Model Perspective\\n\\nIntriguingly, other languages on TydiQA also show enhancements and declines,\\n\\nVicuna-13B-V1.5-Seq\\n\\nVicuna-7B-V1.5-Seq\\n\\nBaichuan2-7B-Instruct-Seq\\n\\nLLaMA-2-7B-Chat-Seq\\n\\nLLaMA-2-13B-Chat-Seq\\n\\nLLaMA-2-70B-Chat-Seq\\n\\nExcluding the llama2-7b model, most models exhibit a significant drop in performance on MMLU,\\n\\nDecline in math and reasoning abilities, highlighting their sensitivity to new task learning.\\n\\nNearly all models display a negative General Ability Delta,\\n\\nindicating a general decline in overall capabilities after continual learning.\\n\\n2) Excluding the llama2-7b model, most models exhibit a significant drop in performance on MMLU,\\n\\nDecline in math and reasoning abilities, highlighting their sensitivity to new task learning.\\n\\n2) Excluding the llama2-7b model, most models exhibit a significant drop in performance on MMLU,\\n\\nDecline in math and reasoning abilities, highlighting their sensitivity to new task learning.\\n\\n3) Excluding the llama2-7b model, most models exhibit a significant drop in performance on MMLU,\\n\\nDecline in math and reasoning abilities, highlighting their sensitivity to new task learning.\\n\\n4) Excluding the llama2-7b model, most models exhibit a significant drop in performance on MMLU,\\n\\nDecline in math and reasoning abilities, highlighting their sensitivity to new task learning.\\n\\n4.4.2 Variation of General Ability\\n\\nparameter-efficient methods like LoRA.\\n\\nFull Parameter Training vs. LoRA\\n\\nAdaptability compared to LoRA, with a smaller BWT score. For instance, LLaMA-2-7B-Chat's\\n\\nDespite achieving a high overall performance\\n\\nCatastrophic Forgetting in LLaMA-2-70B-Chat\\n\\nFull parameter training demonstrates better task-specific\\n\\nSeqFT OP(BWT) is 48.7 (-8.3%), while LoRASeqFT stands at 12.7 (-45.7%). This suggests that\\n\\nParameter-efficient methods like LoRA.\\n\\nComparing the OP score of 45.2, the llama2-70B-Chat model exhibits significant catastrophic forgetting, as\\n\\nTable 3: Comparison of the general language understanding and reasoning abilities. blue means\\n\\nmethods without Replay, while for LLaMA-2-13B-Chat, the increase is 17.1 EM score.\\n\\nThe degree of\\n\\npronounced. For instance, for LLaMA-2-7B-Chat, Replay offers a 6.5 EM score boost compared to\\n\\nExcluding the llama2-7b model, most models exhibit a significant drop in performance on MMLU,\"}"}
{"id": "3qa4YLkcEw", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: GPT-4 evaluation with llama-13b-chat, comparing 3 different baselines (Replay, LoRA and Sequential) to the base model across tasks including helpful and safety.\\n\\nFigure 3: Performance evaluation of LLaMA-2-7B-Chat's SeqFT on TRACE benchmark across varying sample sizes (500, 1000, 5000) and training epochs (1, 3, 5, 10 (except for 5000)).\\n\\nFigure 4: Evolution of LLMs' reasoning capabilities post-training, measured using the BBH metric. Results for LLaMA-2-7B-chat and LLaMA-2-13B-chat are reported.\\n\\n4.4.3 Instruction Following Ability Analysis\\nWe evaluate the instruction-following ability of LLMs based on LLaMA-2-7B-Chat and LLaMA-2-13B-Chat. Figure 2 (a) illustrates the win rate % for instruction following sequentially trained LLMs and their original versions. Here, the win rate can be approximated as an indicator for the Instruction-following delta. All three training methods show a significant decline in instruction-following capabilities, with the most pronounced decline in the LoRA method. Therefore, be cautious when exploring approaches like LoRA for continual learning in LLMs.\\n\\n4.4.4 Safety Analysis\\nWe test the safety of answers from models LLaMA-2-7B-Chat and LLaMA-2-13B-Chat. Figure 2 (b) shows the win rate % for instruction following between the new LLMs and their starting versions. Here, the win rate can be used as a measure for the Safety Delta. Compared to the original models, most answers were rated as 'Tie'. This suggests that the safety of the model's answers is largely unaffected by continual learning on general tasks.\\n\\n4.5 Ablation Study\\nData Quantity & Training Steps. Figure 3 shows the impact of data volumes and training steps on target task performance. For LLaMA-2-7B-Chat's SeqFT, we tested with 500, 1000, and 5000 samples from each dataset, training for 1, 3, 5, 10 epochs. Results indicate performance improves with more data, with optimal results at 5000 samples. Furthermore, up to 5 epochs improve performance, aligning with our baseline for balancing task optimization and capabilities retention.\\n\\nDecoupling the Impact of Different Tasks. Results from section 4.4.2 reveal a notable decline in reasoning and mathematical skills after training on our benchmark. This brings forth the question: How exactly does the reasoning capability of LLMs transform during the continual learning process? Figure 4 tracks the reasoning ability (assessed via BBH performance) after the completion of each training task. Interestingly, the model's reasoning skill improves after the ScienceQA task but declines for others. Unlike NumGLUE tasks, which lack clear reasoning paths, ScienceQA provides explicit reasoning paths in its answers, indicating that including reasoning paths in training maybe benefit the model's reasoning skills.\"}"}
{"id": "3qa4YLkcEw", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.6 Reasoning-augmented Continual Learning\\n\\nWhy do LLMs underperform on the TRACE benchmark, both in target tasks and maintaining inherent capabilities? Two likely reasons: First, neural networks tend to overfit the specific output formats of new tasks [50]. Second, as most tasks involve direct result prediction, LLMs may learn shortcuts within the training data, limiting their ability to generalize to new data.\\n\\nWith these insights and our experimental findings in 4.5, we propose the Reasoning-augmented Continual Learning (RCL) method. As depicted in Figure 5, RCL involves two phases: automated reasoning annotation and sequential training on the augmented datasets. GPT-4 generates reasoning paths for all entries, based on prompts designed by domain experts for each task. These are then validated against the ground truth. Manual inspection of a 100-sample subset results in a 94% approval rate, demonstrating GPT-4\u2019s reliability. Following this, supervised training is conducted on the target LLM, keeping hyperparameter settings consistent with baselines.\\n\\n### 4.6.1 Performance on Sequential Tasks\\n\\n| Methods | OP | BWT | FWT |\\n|---------|----|-----|-----|\\n| EWC     | 32.5 | -20.6% | 1.0% |\\n| OGD     | 29.7 | -22.6% | 0.9% |\\n| L2P     | 43.7 | -9.5% | 2.6% |\\n| LFPT5   | 49.2 | -6.7% | 1.5% |\\n| PP      | 49.2 | -0.3% | 1.9% |\\n| O-Lora  | 41.3 | -6.2% | 1.6% |\\n| ICL     | 39.5 | - | - |\\n| ICL+Re  | 41.1 | - | - |\\n| SeqFT(0.5k) | 23.0 | -19% | 0.3% |\\n| RCL(0.5k) | 46.6 | -13% | 2.7% |\\n| SeqFT(5k) | 48.7 | -8.3% | 2.4% |\\n| RCL(5k) | 51.9 | -6.5% | 3.2% |\\n| SingleFT | 57.6 | - | - |\\n| SingleFT+Re | 58.1 | - | - |\\n| MT w/o. Re | 52.3 | - | - |\\n| MT w. Re | 58.2 | - | - |\\n\\nTable 4: Comparison of RCL with different baselines. Single FT refers to fine-tuning the model on a single task, Re refers to reasoning-augmented, and MT refers to Multi-task training.\\n\\nTable 4 provides comparisons of the performance of RCL against other baselines. Through an ablation study contrasting single-task training (SingleFT) with multi-task training, and assessing the impact of reasoning-augmented data, we observed that integrating reasoning pathways consistently boosts performance over the original dataset. Our approach yields results comparable to the SeqFT method using only 500 samples instead of 5000. By utilizing fewer datasets and training steps, our method also helps preserve the inherent capabilities of LLMs more effectively.\\n\\n### 4.6.2 Original Performance Retention Impacts on General Ability\\n\\nFigure 6 shows RCL\u2019s performance on general abilities matches SeqFT and Replay on MMLU, TydiQA, BoolQA, and PIQA. Yet, RCL excels in reasoning tasks like GSM and BBH, where it surpasses SeqFT and Replay by 12.7 and 13.2 points, respectively. This highlights RCL\u2019s effectiveness in enhancing reasoning skills through reasoning paths. Additionally, combining RCL with replay boosts its reasoning task performance.\\n\\n**Impacts on Instruction-Following**\\n\\nThe impact of incorporating RCL on instruction-following capabilities is presented in Table 5. It\u2019s evident that RCL enhances the model\u2019s ability to follow instructions by 8% and 5% compared to SeqFT and Replay, respectively.\\n\\n### 5 Conclusion\\n\\nExisting continual learning benchmarks are insufficient in thoroughly evaluating LLMs, due to their oversimplification and lack of key metrics like instruction following and safety. To tackle this, we introduce TRACE, a comprehensive benchmark with eight challenging tasks and well-rounded metrics. Our experiments show that for LLMs, catastrophic forgetting remains, and a clear drop in general abilities is observed during continual learning. Besides, our RCL method highlights the importance of using reasoning in training while alleviating the above phenomena. We believe this area is crucial and hope our work serves as a foundation for future research.\\n\\n### 6 Limitations\\n\\nWhile our proposed TRACE benchmark covers a variety of tasks, expanding it to include real-time interaction and multi-modal integration could provide a more comprehensive assessment of LLM capabilities. Additionally, future versions should aim to minimize any potential biases introduced by the current task selection or dataset composition, ensuring a more balanced and comprehensive representation of tasks that better reflect diverse real-world applications.\"}"}
{"id": "3qa4YLkcEw", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] OpenAI. GPT-4 technical report. ArXiv, abs/2303.08774, 2023.\\n\\n[2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\\n\\n[3] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564, 2023.\\n\\n[4] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: May the source be with you! arXiv preprint arXiv:2305.06161, 2023.\\n\\n[5] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. C-eval: A multi-level multi-discipline Chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.\\n\\n[6] Nigar M Shafiq Surameery and Mohammed Y Shakor. Use Chat GPT to solve programming bugs. International Journal of Information Technology & Computer Engineering (IJITC) ISSN: 2455-5290, 3(01):17\u201322, 2023.\\n\\n[7] Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan Yan, Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Marc H. Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world APIs. ArXiv, abs/2307.16789, 2023.\\n\\n[8] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of continual learning: Theory, method and application. ArXiv, abs/2302.00487, 2023.\\n\\n[9] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109\u2013165. Elsevier, 1989.\\n\\n[10] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NIPS, 2015.\\n\\n[11] Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan. Fine-tuned language models are continual learners. In Conference on Empirical Methods in Natural Language Processing, 2022.\\n\\n[12] Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian Khabsa, Mike Lewis, and Amjad Almahairi. Progressive prompts: Continual learning for language models. In The Eleventh International Conference on Learning Representations, 2023.\\n\\n[13] David Lopez-Paz and Marc'Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017.\\n\\n[14] Cyprien de Masson D'Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. Episodic memory in lifelong language learning. Advances in Neural Information Processing Systems, 32, 2019.\\n\\n[15] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.\\n\\n[16] James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira, Yilin Shen, and Hongxia Jin. Continual diffusion: Continual customization of text-to-image diffusion with c-lora. arXiv preprint arXiv:2304.06027, 2023.\\n\\n[17] Xiao Wang, Tianze Chen, Qiming Ge, Han Xia, Rong Bao, Rui Zheng, Qi Zhang, Tao Gui, and Xuanjing Huang. Orthogonal subspace learning for language model continual learning, 2023.\"}"}
{"id": "3qa4YLkcEw", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zhicheng Wang, Yufang Liu, Tao Ji, Xiaoling Wang, Yuanbin Wu, Congcong Jiang, Ye Chao, Zhencong Han, Ling Wang, Xu Shao, and Wenqiu Zeng. Rehearsal-free continual language learning via efficient parameter isolation. ArXiv, 2023.\\n\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\\n\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32, 2019.\\n\\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, A. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Annual Meeting of the Association for Computational Linguistics, 2011.\\n\\nZixuan Ke and Bin Liu. Continual learning of natural language processing tasks: A survey. ArXiv, abs/2211.12701, 2022.\\n\\nArslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In Proceedings of the European conference on computer vision (ECCV), pages 532\u2013547, 2018.\\n\\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering, 2022.\\n\\nAgam Shah, Suvan Paturi, and Sudheer Chava. Trillion dollar words: A new financial dataset, task & market analysis, 2023.\\n\\nYebowen Hu, Tim Ganter, Hanieh Deilamsalehy, Franck Dernoncourt, Hassan Foroosh, and Fei Liu. Meetingbank: A benchmark dataset for meeting summarization, 2023.\\n\\nChenye Zhao, Yingjie Li, and Cornelia Caragea. C-STANCE: A large dataset for Chinese zero-shot stance detection. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13369\u201313385, Toronto, Canada, July 2023. Association for Computational Linguistics.\\n\\nAnnette Rios, Nicolas Spring, Tannon Kew, Marek Kostrzewa, Andreas S\u00e4uberli, Mathias M\u00fcller, and Sarah Ebling. A new dataset and efficient baselines for document-level text simplification in German. In Proceedings of the Third Workshop on New Frontiers in Summarization, pages 152\u2013161, Online and in Dominican Republic, November 2021. Association for Computational Linguistics.\\n\\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding, 2023.\\n\\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code understanding and generation, 2021.\\n\\nSwaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. Numglue: A suite of fundamental yet challenging mathematical reasoning tasks, 2022.\\n\\nBaichuan. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023.\\n\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\"}"}
{"id": "3qa4YLkcEw", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. ArXiv, abs/2106.09685, 2021.\\n\\nLei Liu and J. Huang. Prompt learning to mitigate catastrophic forgetting in cross-lingual transfer for open-domain dialogue generation. Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2023.\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.\\n\\nMehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for continual learning. In International Conference on Artificial Intelligence and Statistics, pages 3762\u20133773. PMLR, 2020.\\n\\nZifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning, 2022.\\n\\nChengwei Qin and Shafiq Joty. Lfpt5: A unified framework for lifelong few-shot language learning based on prompt tuning of t5, 2022.\\n\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020.\\n\\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\\n\\nJonathan H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. Tydi qa: A benchmark for information-seeking question answering in logically diverse languages. Transactions of the Association for Computational Linguistics, 8:454\u2013470, 2020.\\n\\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432\u20137439, 2020.\\n\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. ArXiv, abs/2108.07732, 2021.\\n\\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.\\n\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.\\n\\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.\\n\\nFederico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul R\u00f6ttger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions. arXiv preprint arXiv:2309.07875, 2023.\"}"}
{"id": "3qa4YLkcEw", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendices\\n\\nA Implementation Details\\n\\nDuring the training phase, for the baselines without LoRA adapters, we consistently trained with 5000 samples with a constant learning rate of 1e-5. For the datasets we used (C-STANCE, FOMC, MeetingBank, Py150, ScienceQA, NumGLUE-cm, NumGLUE-ds, 20Minuten), we train for 1, 1, 5, 5, 1, 5, 5, 5 epochs respectively. While for the baselines with LoRA adapters, we trained with 5000 samples with a constant learning rate of 1e-4 for 5, 3, 7, 5, 3, 5, 5, 7 epochs respectively. Our training settings incorporated a weight decay set to 0, and a batch size of 128. For the testing phase, we use a temperature of 0.1. All our training and inference experiments were conducted on a machine equipped with 8x80G Nvidia A100, and were implemented using DeepSpeed repository. All models are trained on 8 A100 GPUs with 80G memory with full parameters fine-tuning. All general benchmark evaluations were conducted using the Open-Compass toolkit [49], adopting its default configuration.\\n\\nB Trace dataset statistics\\n\\nIn this section, we represent the overview of dataset statistics, including source, average length, metric, language, and number of samples of each dataset in the TRACE benchmark. We demonstrate the dataset details in Table 1.\\n\\nC Reasoning-based Continual Learning\\n\\nFigure 5 shows an overview of our reasoning-augmented continual learning method. Table 5 and Figure 6 shows the results of RCL in preserving the instruction-following abilities and general abilities, respectively.\"}"}
{"id": "3qa4YLkcEw", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Instruction: Solve the following math problem.\\n\\nQuestion: A football team practices for 6 hours daily. This week they could not practice due to rain on 1 days. Find the total number of hours they practiced this week.\\n\\nAnswer: In a week, there are 7 days. If the football team could not practice for 1 day due to rain, they practiced for 7 - 1 = 6 days. Since they practice for 6 hours each day, the total number of hours they practiced this week is 6 days * 6 hours/day = 36 hours.\\n\\nAnswer: 36\"}"}
{"id": "3qa4YLkcEw", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task       | Round 1 | 2   | 3   | 4   | 5   | 6   | 7   | 8   |\\n|------------|---------|-----|-----|-----|-----|-----|-----|-----|\\n| C-STANCE   | 0.62    | 0.629| 0.663| 0.621| 0.531| 0.55 | 0.588| 0.579|\\n| FOMC       | -       | 0.681| 0.353| 0.318| 0.02 | 0.363| 0.347| 0.335|\\n| MeetingBank| -       | -   | 0.442| 0.351| 0.371| 0.379| 0.389| 0.364|\\n| Py150      | -       | -   | -   | 0.626| 0.562| 0.586| 0.589| 0.58  |\\n| ScienceQA  | -       | -   | -   | -   | 0.77 | 0.68 | 0.5  | 0.44  |\\n| NumGLUE-cm | -       | -   | -   | -   | -   | 0.358| 0.247| 0.284 |\\n| NumGLUE-ds | -       | -   | -   | -   | -   | -   | 0.64 | 0.475 |\\n| 20Minuten  | -       | -   | -   | -   | -   | -   | -   | 0.415 |\\n| average    | 0.434   |     |     |     |     |     |     |      |\\n\\nTable 7: Detailed results of sequential fine-tuning of Baichuan-7b.\\n\\n| Task       | Round 1 | 2   | 3   | 4   | 5   | 6   | 7   | 8   |\\n|------------|---------|-----|-----|-----|-----|-----|-----|-----|\\n| C-STANCE   | 0.5     | 0.456| 0.448| 0.453| 0.435| 0.442| 0.436| 0.454|\\n| FOMC       | 0.54    | 0.735| 0.67 | 0.658| 0   | 0.595| 0.577| 0.609|\\n| MeetingBank| 0.172   | 0.098| 0.523| 0.459| 0.433| 0.446| 0.442| 0.457|\\n| Py150      | 0.218   | 0.282| 0.185| 0.58 | 0.459| 0.509| 0.508| 0.512|\\n| ScienceQA  | 0.12    | 0.13 | 0.15 | 0.17 | 0.764| 0.636| 0.45 | 0.637|\\n| NumGLUE-cm | 0.173   | 0.012| 0.074| 0.062| 0   | 0.383| 0.247| 0.272|\\n| NumGLUE-ds | 0.1     | 0    | 0.04 | 0.11 | 0   | 0.03 | 0.582| 0.548|\\n| 20Minuten  | 0.022   | 0.021| 0.018| 0.024| 0.017| 0.019| 0.017| 0.408|\\n| average    | 0.487   |     |     |     |     |     |     |      |\\n| BWT        | -0.083  |     |     |     |     |     |     |      |\\n| FWT        | 0.024   |     |     |     |     |     |     |      |\\n\\nTable 8: Detailed results of sequential fine-tuning of LLaMA-7b-chat.\\n\\n| Task       | Round 1 | 2   | 3   | 4   | 5   | 6   | 7   | 8   |\\n|------------|---------|-----|-----|-----|-----|-----|-----|-----|\\n| C-STANCE   | 0.469   | 0.465| 0.47 | 0.477| 0.463| 0.466| 0.45 | 0.5  |\\n| FOMC       | 0.587   | 0.754| 0.738| 0.748| 0.03 | 0.721| 0.71 | 0.717|\\n| MeetingBank| 0.232   | 0.165| 0.533| 0.51 | 0.375| 0.421| 0.385| 0.351|\\n| Py150      | 0.246   | 0.265| 0.213| 0.568| 0.538| 0.537| 0.541| 0.547|\\n| ScienceQA  | 0.223   | 0.256| 0.197| 0.244| 0.8  | 0.655| 0.241| 0.55  |\\n| NumGLUE-cm | 0.198   | 0.121| 0.089| 0   | 0   | 0.333| 0.284| 0.296|\\n| NumGLUE-ds | 0.156   | 0.034| 0    | 0.045| 0   | 0.245| 0.618| 0.622|\\n| 20Minuten  | 0.032   | 0.034| 0.026| 0.024| 0.025| 0.034| 0.018| 0.408|\\n| average    | 0.499   |     |     |     |     |     |     |      |\\n| BWT        | -0.07   |     |     |     |     |     |     |      |\\n| FWT        | 0.025   |     |     |     |     |     |     |      |\"}"}
{"id": "3qa4YLkcEw", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task          | Round 1 | 2   | 3   | 4   | 5   | 6   | 7   | 8   |\\n|--------------|---------|-----|-----|-----|-----|-----|-----|-----|\\n| C-STANCE     | 0.56    | 0.45| 0.45| 0.42| 0.42| 0.44| 0.48| 0.49|\\n| FOMC         | -0.75   | 0.53| 0.53| 0.53| 0.53| 0.53| 0.53| 0.53|\\n| MeetingBank  | -0.427  | 0.233| 0.233| 0.233| 0.233| 0.233| 0.233| 0.233|\\n| Py150        | -0.617  | 0.578| 0.578| 0.578| 0.578| 0.578| 0.578| 0.578|\\n| ScienceQA    | -0.1    | 0    | 0    | 0    | 0    | 0    | 0    | 0    |\\n| NumGLUE-cm   | -0.32   | 0.197| 0.197| 0.197| 0.197| 0.197| 0.197| 0.197|\\n| NumGLUE-ds   | -0.66   | 0.37 | 0.37 | 0.37 | 0.37 | 0.37 | 0.37 | 0.37 |\\n| 20Minuten    | -0.188  | -    | -    | -    | -    | -    | -    | -    |\\n| average      | 0.303   | -    | -    | -    | -    | -    | -    | -    |\\n| BWT          | -0.17   | -    | -    | -    | -    | -    | -    | -    |\\n| FWT          | 0.005   | -    | -    | -    | -    | -    | -    | -    |\\n\\nTable 10: Detailed results of sequential fine-tuning of LLaMA-7b.\\n\\n| Task          | Round 1 | 2   | 3   | 4   | 5   | 6   | 7   | 8   |\\n|--------------|---------|-----|-----|-----|-----|-----|-----|-----|\\n| C-STANCE     | 0.532   | 0.451| 0.439| 0.448| 0.149| 0.477| 0.47 | 0.476|\\n| FOMC         | -0.738  | 0.732| 0.744| 0 | 0.754| 0.678| 0.678| 0.678|\\n| MeetingBank  | -0.519  | 0.447| 0.443| 0.427| 0.417| 0.439| -   | -   |\\n| Py150        | -0.577  | 0.384| 0.486| 0.482| 0.482| -   | -   | -   |\\n| ScienceQA    | -0.773  | 0.7  | 0.608| 0.649| -   | -   | -   | -   |\\n| NumGLUE-cm   | -0.407  | 0.247| 0.309| 0.358| -   | -   | -   | -   |\\n| NumGLUE-ds   | -0.578  | 0.517| -   | -   | -   | -   | -   | -   |\\n| 20Minuten    | -0.403  | -    | -    | -    | -    | -    | -    | -    |\\n| average      | 0.492   | -    | -    | -    | -    | -    | -    | -    |\\n| BWT          | -0.084  | -    | -    | -    | -    | -    | -    | -    |\\n| FWT          | 0.017   | -    | -    | -    | -    | -    | -    | -    |\\n\\nTable 11: Detailed results of sequential fine-tuning of Vicuna-7b.\\n\\n| Task          | Round 1 | 2   | 3   | 4   | 5   | 6   | 7   | 8   |\\n|--------------|---------|-----|-----|-----|-----|-----|-----|-----|\\n| C-STANCE     | 0.527   | 0.43 | 0.471| 0.497| 0.374| 0.468| 0.469| 0.484|\\n| FOMC         | -0.741  | 0.739| 0.731| 0 | 0.754| 0.678| 0.714| -   |\\n| MeetingBank  | -0.549  | 0.532| 0.53 | 0.491| 0.427| 0.412| -   | -   |\\n| Py150        | -0.564  | 0.54 | 0.546| 0.538| 0.552| -   | -   | -   |\\n| ScienceQA    | -0.79   | 0.616| 0.586| 0.633| -   | -   | -   | -   |\\n| NumGLUE-cm   | -0.346  | 0.309| 0.358| -   | -   | -   | -   | -   |\\n| NumGLUE-ds   | -0.622  | 0.572| -   | -   | -   | -   | -   | -   |\\n| 20Minuten    | -0.41   | -    | -    | -    | -    | -    | -    | -    |\\n| average      | 0.517   | -    | -    | -    | -    | -    | -    | -    |\\n| BWT          | -0.059  | -    | -    | -    | -    | -    | -    | -    |\\n| FWT          | 0.021   | -    | -    | -    | -    | -    | -    | -    |\\n\\nTable 12: Detailed results of sequential fine-tuning of Vicuna-13b.\"}"}
{"id": "3qa4YLkcEw", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 13 - Detailed performance of different models during continual learning.\\n\\n| Task           | Round 1 | Round 2 | Round 3 | Round 4 | Round 5 | Round 6 | Round 7 | Round 8 |\\n|----------------|---------|---------|---------|---------|---------|---------|---------|---------|\\n| C-STANCE       | 0.613   | 0.601   | 0.597   | 0.584   | 0.506   | 0.504   | 0.53    | 0.477   |\\n| FOMC           | 0.652   | 0.604   | 0.591   | 0.602   | 0.588   | 0.587   | 0.417   |         |\\n| MeetingBank    | -       | 0.345   | 0.334   | 0.333   | 0.343   | 0.34    | 0.337   |         |\\n| Py150          | -       | -       | -       | 0.588   | 0.472   | 0.539   | 0.517   | 0.472   |\\n| ScienceQA      | -       | -       | -       | -       | 0.641   | 0.68    | 0.625   | 0.63    |\\n| NumGLUE-cm     | -       | -       | -       | -       | -       | 0.457   | 0.432   | 0.407   |\\n| NumGLUE-ds     | -       | -       | -       | -       | -       | -       | 0.43    | 0.36    |\\n| 20Minuten      | -       | -       | -       | -       | -       | -       | -       | 0.407   |\\n| **Average**    | **0.438** |        |         |         |         |         |         |         |\\n\\n### Table 14 - Detailed results of sequential fine-tuning of LLaMA-7b-chat with LoRA adapters.\\n\\n| Task           | Round 1 | Round 2 | Round 3 | Round 4 | Round 5 | Round 6 | Round 7 | Round 8 |\\n|----------------|---------|---------|---------|---------|---------|---------|---------|---------|\\n| C-STANCE       | 0.511   | 0.45    | 0.412   | 0.373   | 0.133   | 0.391   | 0.294   | 0.277   |\\n| FOMC           | -       | 0.713   | 0.55    | 0.452   | 0      | 0.421   | 0.341   | 0.24    |\\n| MeetingBank    | -       | 0.51    | 0.212   | 0.151   | 0.067   | 0.037   | 0.121   |         |\\n| Py150          | -       | -       | 0.578   | 0.004   | 0.495   | 0.452   | 0.004   |         |\\n| ScienceQA      | -       | -       | -       | 0.68    | 0.645   | 0.535   | 0.535   | 0.68    |\\n| NumGLUE-cm     | -       | -       | -       | -       | 0.37    | 0.235   | 0.235   | 0       |\\n| NumGLUE-ds     | -       | -       | -       | -       | -       | -       | 0.486   | 0       |\\n| 20Minuten      | -       | -       | -       | -       | -       | -       | -       | 0.37    |\\n| **Average**    | **0.127** |        |         |         |         |         |         |         |\\n\\n### Table 15 - Detailed results of sequential fine-tuning of LLaMA-13b with LoRA adapters.\\n\\n| Task           | Round 1 | Round 2 | Round 3 | Round 4 | Round 5 | Round 6 | Round 7 | Round 8 |\\n|----------------|---------|---------|---------|---------|---------|---------|---------|---------|\\n| C-STANCE       | 0.62    | 0.36    | 0.432   | 0.491   | 0.18    | 0.42    | 0.411   | 0.124   |\\n| FOMC           | -       | 0.743   | 0.681   | 0.63    | 0.53    | 0.605   | 0.579   | 0       |\\n| MeetingBank    | -       | 0.484   | 0.264   | 0.201   | 0.147   | 0.032   | 0.122   |         |\\n| Py150          | -       | -       | 0.581   | 0.397   | 0.488   | 0.497   | 0.249   |         |\\n| ScienceQA      | -       | -       | -       | 0.75    | 0.729   | 0.714   | 0.68    |         |\\n| NumGLUE-cm     | -       | -       | -       | -       | 0.58    | 0.296   | 0.259   |         |\\n| NumGLUE-ds     | -       | -       | -       | -       | -       | -       | 0.62    | 0.386   |\\n| 20Minuten      | -       | -       | -       | -       | -       | -       | -       | 0.417   |\\n| **Average**    | **0.28** |        |         |         |         |         |         |         |\"}"}
{"id": "3qa4YLkcEw", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task       | Round 1 | Round 2 | Round 3 | Round 4 | Round 5 | Round 6 | Round 7 | Round 8 |\\n|------------|---------|---------|---------|---------|---------|---------|---------|---------|\\n| C-STANCE   | 0.53    | 0.498   | 0.422   | 0.49    | 0.04    | 0.44    | 0.457   | 0.228   |\\n| FOMC       | -0.641  | 0.532   | 0.66    | -0.01   | 0.64    | 0.67    | 0.19    |         |\\n| MeetingBank| -       | -       | 0.528   | 0.22    | 0.21    | 0.146   | 0.264   | 0.181   |\\n| Py150      | -       | -       | -       | 0.633   | 0.346   | 0.651   | 0.671   | 0.595   |\\n| ScienceQA  | -       | -       | -       | -       | 0.81    | 0.79    | 0.89    | 0.805   |\\n| NumGLUE-cm | -       | -       | -       | -       | -       | 0.543   | 0.531   | 0.518   |\\n| NumGLUE-ds | -       | -       | -       | -       | -       | -       | 0.74    | 0.68    |\\n| 20Minuten  | -       | -       | -       | -       | -       | -       | -       | 0.42    |\\n| Average    | 0.452   |         |         |         |         |         |         |         |\\n\\nTable 16: Detailed results of sequential fine-tuning of LLaMA-70b with LoRA adapters.\\n\\n| Task       | Round 1 | Round 2 | Round 3 | Round 4 | Round 5 | Round 6 | Round 7 | Round 8 |\\n|------------|---------|---------|---------|---------|---------|---------|---------|---------|\\n| C-STANCE   | 0.514   | 0.452   | 0.433   | 0.446   | 0.0     | 0.344   | 0.089   | 0.141   |\\n| FOMC       | -0.715  | 0.48    | 0.427   | -0.01   | 0.272   | 0.304   | 0.29    |         |\\n| MeetingBank| -       | 0.5     | 0.113   | 0.144   | 0.026   | 0.011   | 0.07    |         |\\n| Py150      | -       | -       | 0.573   | 0.222   | 0.47    | 0.452   | 0.413   |         |\\n| ScienceQA  | -       | -       | -       | 0.67    | 0.632   | 0.53    | 0.6     |         |\\n| NumGLUE-cm | -       | -       | -       | -       | 0.407   | 0.37    | 0.259   |         |\\n| NumGLUE-ds | -       | -       | -       | -       | -       | 0.545   | 0.492   |         |\\n| 20Minuten  | -       | -       | -       | -       | -       | -       | -       | 0.409   |\\n| Average    | 0.334   |         |         |         |         |         |         |         |\\n\\nTable 17: Detailed results of sequential fine-tuning of Vicuna-7b with LoRA adapters.\\n\\n| Task       | Round 1 | Round 2 | Round 3 | Round 4 | Round 5 | Round 6 | Round 7 | Round 8 |\\n|------------|---------|---------|---------|---------|---------|---------|---------|---------|\\n| C-STANCE   | 0.524   | 0.504   | 0.394   | 0.385   | 0.389   | 0.347   | 0.329   | 0.07    |\\n| FOMC       | -0.74   | 0.68    | 0.616   | 0.188   | 0.62    | 0.438   | 0.04    |         |\\n| MeetingBank| -       | 0.495   | 0.24    | 0.157   | 0.132   | 0.08    | 0.14    |         |\\n| Py150      | -       | -       | 0.6     | 0.368   | 0.52    | 0.491   | 0.256   |         |\\n| ScienceQA  | -       | -       | -       | 0.77    | 0.75    | 0.732   | 0.74    |         |\\n| NumGLUE-cm | -       | -       | -       | -       | 0.407   | 0.346   | 0.346   |         |\\n| NumGLUE-ds | -       | -       | -       | -       | -       | 0.569   | 0.52    |         |\\n| 20Minuten  | -       | -       | -       | -       | -       | -       | -       | 0.413   |\\n| Average    | 0.316   |         |         |         |         |         |         |         |\\n\\nTable 18: Detailed results of sequential fine-tuning of Vicuna-13b with LoRA adapters.\"}"}
{"id": "3qa4YLkcEw", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 19 - 24 shows the detailed performance of different models of each round during the continual learning with replay data.\\n\\n| Task      | Round 1 | 2  | 3  | 4  | 5  | 6  | 7  | 8  |\\n|-----------|---------|----|----|----|----|----|----|----|\\n| C-STANCE  | 0.57    | 0.55| 0.56| 0.63| 0.6  | 0.64 | 0.62| 0.61|\\n| FOMC      | -       | 0.69| 0.64| 0.64| 0.65 | 0.65 | 0.66| 0.61|\\n| MeetingBank | -      | -  | 0.445| 0.457| 0.449 | 0.466 | 0.461| 0.482|\\n| Py150     | -       | -  | -  | 0.546| 0.577 | 0.577 | 0.613| 0.583|\\n| ScienceQA | -       | -  | -  | -  | 0.58 | 0.51 | 0.54 | 0.57|\\n| NumGLUE-cm| -       | -  | -  | -  | -  | 0.321 | 0.346| 0.333|\\n| NumGLUE-ds| -      | -  | -  | -  | -  | -  | 0.5 | 0.55|\\n| 20Minuten | -       | -  | -  | -  | -  | -  | -  | 0.405|\\n| average   | 0.517   |     |     |     |     |     |     |     |\\n| BWT       | 0.011   |     |     |     |     |     |     |     |\\n| FWT       | -0.04   |     |     |     |     |     |     |     |\\n\\nTable 19: Detailed results of continual learning of Baichuan-7b with replay data.\\n\\n| Task      | Round 1 | 2  | 3  | 4  | 5  | 6  | 7  | 8  |\\n|-----------|---------|----|----|----|----|----|----|----|\\n| C-STANCE  | 0.471   | 0.487| 0.485| 0.5 | 0.486| 0.475| 0.493| 0.5 |\\n| FOMC      | -       | 0.734| 0.769| 0.785| 0.807| 0.781| 0.785| 0.8 |\\n| MeetingBank | -      | -  | 0.499| 0.496| 0.507| 0.494| 0.492| 0.51|\\n| Py150     | -       | -  | -  | 0.543| 0.561| 0.546| 0.552| 0.55|\\n| ScienceQA | -       | -  | -  | -  | 0.763| 0.78 | 0.78 | 0.785|\\n| NumGLUE-cm| -       | -  | -  | -  | -  | 0.358| 0.309| 0.37|\\n| NumGLUE-ds| -      | -  | -  | -  | -  | -  | 0.486| 0.52|\\n| 20Minuten | -       | -  | -  | -  | -  | -  | -  | 0.406|\\n| average   | 0.555   |     |     |     |     |     |     |     |\\n| BWT       | 0.026   |     |     |     |     |     |     |     |\\n| FWT       | -0.002  |     |     |     |     |     |     |     |\\n\\nTable 20: Detailed results of continual learning of LLaMA-7b-chat with replay data.\\n\\n| Task      | Round 1 | 2  | 3  | 4  | 5  | 6  | 7  | 8  |\\n|-----------|---------|----|----|----|----|----|----|----|\\n| C-STANCE  | 0.5     | 0.496| 0.497| 0.493| 0.52 | 0.503| 0.5 | 0.51|\\n| FOMC      | -       | 0.778| 0.803| 0.805| 0.792| 0.789| 0.785| 0.813|\\n| MeetingBank | -      | -  | 0.484| 0.495| 0.515| 0.499| 0.503| 0.482|\\n| Py150     | -       | -  | -  | 0.523| 0.549| 0.532| 0.534| 0.523|\\n| ScienceQA | -       | -  | -  | -  | 0.816| 0.8 | 0.804| 0.792|\\n| NumGLUE-cm| -       | -  | -  | -  | -  | 0.358| 0.407| 0.396|\\n| NumGLUE-ds| -      | -  | -  | -  | -  | -  | 0.628| 0.606|\\n| 20Minuten | -       | -  | -  | -  | -  | -  | -  | 0.407|\\n| average   | 0.566   |     |     |     |     |     |     |     |\\n| BWT       | 0.004   |     |     |     |     |     |     |     |\\n| FWT       | 0.022   |     |     |     |     |     |     |     |\\n\\nTable 21: Detailed results of continual learning of LLaMA-13b-chat with replay data.\"}"}
{"id": "3qa4YLkcEw", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task      | Round 1 | 2  | 3  | 4  | 5  | 6  | 7  | 8  |\\n|-----------|---------|----|----|----|----|----|----|----|\\n| C-STANCE  | 0.519   | 0.506 | 0.61 | 0.62 | 0.63 | 0.58 | 0.69 | 0.67 |\\n| FOMC      | -0.669  | 0.803 | 0.805 | 0.792 | 0.789 | 0.785 | 0.813 |   |\\n| MeetingBank | - | - | 0.484 | 0.495 | 0.515 | 0.499 | 0.503 | 0.482 |\\n| Py150     | - | - | - | 0.523 | 0.549 | 0.532 | 0.534 | 0.523 |\\n| ScienceQA | - | - | - | - | 0.816 | 0.804 | 0.792 |   |\\n| NumGLUE-cm | - | - | - | - | - | 0.358 | 0.407 | 0.396 |\\n| NumGLUE-ds | - | - | - | - | - | - | 0.628 | 0.606 |\\n| 20Minuten | - | - | - | - | - | - | - | 0.407 |\\n| Average   | 0.566  |   |   |   |   |   |   |   |\\n| BWT       | -0.04   |   |   |   |   |   |   |   |\\n| FWT       | 0.022   |   |   |   |   |   |   |   |\\n\\nTable 22: Detailed results of continual learning of LLaMA-70b-chat with replay data. Considering computational constraints, we also conduct this experiment using LoRA.\\n\\n| Task      | Round 1 | 2  | 3  | 4  | 5  | 6  | 7  | 8  |\\n|-----------|---------|----|----|----|----|----|----|----|\\n| C-STANCE  | 0.50   | 0.528 | 0.512 | 0.519 | 0.518 | 0.519 | 0.515 | 0.524 |\\n| FOMC      | -0.747  | 0.803 | 0.794 | 0.805 | 0.795 | 0.801 | 0.806 |   |\\n| MeetingBank | - | - | 0.512 | 0.483 | 0.516 | 0.516 | 0.492 | 0.496 |\\n| Py150     | - | - | - | 0.525 | 0.569 | 0.553 | 0.551 | 0.551 |\\n| ScienceQA | - | - | - | - | 0.77 | 0.776 | 0.772 | 0.767 |\\n| NumGLUE-cm | - | - | - | - | - | 0.396 | 0.322 | 0.309 |\\n| NumGLUE-ds | - | - | - | - | - | - | 0.554 | 0.563 |\\n| 20Minuten | - | - | - | - | - | - | - | 0.405 |\\n| Average   | 0.553  |   |   |   |   |   |   |   |\\n| BWT       | 0.002   |   |   |   |   |   |   |   |\\n| FWT       | 0.005   |   |   |   |   |   |   |   |\\n\\nTable 23: Detailed results of continual learning of Vicuna-7b with replay data.\\n\\n| Task      | Round 1 | 2  | 3  | 4  | 5  | 6  | 7  | 8  |\\n|-----------|---------|----|----|----|----|----|----|----|\\n| C-STANCE  | 0.56   | 0.58 | 0.616 | 0.62 | 0.616 | 0.637 | 0.629 | 0.629 |\\n| FOMC      | -0.736  | 0.76 | 0.76 | 0.788 | 0.771 | 0.76 | 0.76 |   |\\n| MeetingBank | - | - | 0.464 | 0.505 | 0.468 | 0.441 | 0.473 | 0.451 |\\n| Py150     | - | - | - | 0.544 | 0.559 | 0.563 | 0.591 | 0.554 |\\n| ScienceQA | - | - | - | - | 0.71 | 0.699 | 0.674 | 0.71 |\\n| NumGLUE-cm | - | - | - | - | - | 0.42 | 0.358 | 0.358 |\\n| NumGLUE-ds | - | - | - | - | - | - | 0.667 | 0.68 |\\n| 20Minuten | - | - | - | - | - | - | - | 0.41 |\\n| Average   | 0.569  |   |   |   |   |   |   |   |\\n| BWT       | 0.006   |   |   |   |   |   |   |   |\\n| FWT       | 0.024   |   |   |   |   |   |   |   |\\n\\nTable 24: Detailed results of continual learning of Vicuna-13b with replay data.\"}"}
{"id": "3qa4YLkcEw", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 25 shows the detailed performance of LLaMA2-7b-chat of each round during the continual learning. RCL represents reasoning-based continual learning.\\n\\n| Task         | Round 1 | Round 2 | Round 3 | Round 4 | Round 5 | Round 6 | Round 7 | Round 8 |\\n|--------------|---------|---------|---------|---------|---------|---------|---------|---------|\\n| C-STANCE     | 0.614   | 0.428   | 0.464   | 0.486   | 0.5     | 0.472   | 0.452   | 0.522   |\\n| FOMC         |         | 0.621   | 0.476   | 0.002   | 0.563   | 0.542   | 0.534   | 0.516   |\\n| MeetingBank  |         |         | 0.497   | 0.431   | 0.329   | 0.363   | 0.332   | 0.343   |\\n| Py150        |         |         |         | 0.563   | 0.513   | 0.521   | 0.528   | 0.527   |\\n| ScienceQA    |         |         |         |         | 0.72    | 0.624   | 0.6     | 0.598   |\\n| NumGLUE-cm   |         |         |         |         |         | 0.691   | 0.494   | 0.469   |\\n| NumGLUE-ds   |         |         |         |         |         |         | 0.566   | 0.354   |\\n| 20Minuten    |         |         |         |         |         |         |         | 0.402   |\\n| average      | 0.466   |         |         |         |         |         |         |         |\\n| BWT          | -0.135  |         |         |         |         |         |         |         |\\n| FWT          | 0.036   |         |         |         |         |         |         |         |\\n\\nTable 26 - 28 shows the performance of LLaMA2-7b-chat with different numbers of data and training epochs.\\n\\n| Task         | Number of epochs 1 | Number of epochs 3 | Number of epochs 5 |\\n|--------------|---------------------|--------------------|--------------------|\\n| C-STANCE     | 0.24                | 0.38               | 0.5                |\\n| FOMC         | 0                   | 0                  | 0.43               |\\n| MeetingBank  | 0.215               | 0.255              | 0.269              |\\n| Py150        | 0.293               | 0.428              | 0.49               |\\n| ScienceQA    | 0.57                | 0.24               | 0.4                |\\n| NumGLUE-cm   | 0.148               | 0.06               | 0.21               |\\n| NumGLUE-ds   | 0.04                | 0                  | 0.42               |\\n| 20Minuten    | 0.39                | 0.403              | 0.41               |\\n| average      | 0.237               | 0.22               | 0.391              |\\n\\nTable 27: Performance of LLaMA-7b-chat after training on all of the sequential tasks for different epochs. Each dataset is sampled with 500 examples.\\n\\n| Task         | Number of epochs 1 | Number of epochs 3 | Number of epochs 5 | Number of epochs 10 |\\n|--------------|---------------------|--------------------|--------------------|---------------------|\\n| C-STANCE     | 0.14                | 0.301              | 0.57               | 0.6                 |\\n| FOMC         | 0.19                | 0.097              | 0.31               | 0.29                |\\n| MeetingBank  | 0.194               | 0.248              | 0.357              | 0.387               |\\n| Py150        | 0.283               | 0.32               | 0.55               | 0.54                |\\n| ScienceQA    | 0.26                | 0.36               | 0.41               | 0.52                |\\n| NumGLUE-cm   | 0.309               | 0.346              | 0.21               | 0.24                |\\n| NumGLUE-ds   | 0.51                | 0.5                 | 0.42               | 0.45                |\\n| 20Minuten    | 0.405               | 0.411              | 0.407              | 0.387               |\\n| average      | 0.286               | 0.329              | 0.404              | 0.426               |\\n\\nTable 28: Performance of LLaMA-7b-chat after training on all of the sequential tasks for different epochs. Each dataset is sampled with 1000 examples.\"}"}
{"id": "3qa4YLkcEw", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 28: Performance of LLaMA-7b-chat after training on all of the sequential tasks for different epochs. Each dataset is sampled with 5000 examples.\\n\\nD.7 Different Order\\n\\nTo migrate the influence of different order of tasks, we experiment with one different sequence of tasks: NumGLUE-cm, NumGLUE-ds, FOMC, 20Minuten, C-STANCE, Py150, MeetingBank, ScienceQA. We report the results in Table 29. It is important to note that there's a significant difference in performance between task orders. While the overall performance for the original order is 48.7, the average performance for order 2 drops to 32.9, indicating that the sequence of tasks has a substantial impact on the model's final performance.\\n\\nTable 29: Detailed results of the second order of sequential fine-tuning of LLaMA-7b-chat\\n\\nD.8 Detailed results of Table 4\\n\\nWe report the detailed experimental results of Table 4 in Table 30.\"}"}
{"id": "3qa4YLkcEw", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We show the prompts used across our experiments. The prompts of our naive sequence training are shown in Table 31. While for RCL, the prompts are shown in Table 32.\\n\\n### Table 31: Prompts applied for naive continual learning\\n\\n| Dataset | Prompt |\\n|---------|--------|\\n| ScienceQA | Choose an answer for the following question. Give your reasoning first, and then the answer. |\\n| FOMC     | What is the monetary policy stance for September?\\nA. hawkish, B. neutral, C. dovish. Choose one from A, B and C. |\\n| MeetingBank | Write a summary of the following meeting transcripts. Give your reasoning first, and then the answer. |\\n| C-STANCE | \u5224\u65b7\u4ee5\u4e0b\u6587\u672c\u5c0d\u8c61\u7684\u614b\u5ea6\uff0c\u9078\u64c7\u4e00\u9805\uff1aA. \u652f\u6301\uff0cB. \u53cd\u5c0d\uff0cC. \u4e2d\u7acb\u3002\u8f38\u51faA\uff0cB\u6216C\u3002 |\\n| 20Minuten | Provide a simplified version of the following paragraph in German. Give your reasoning first, and then the answer. |\\n| Py150 - NumGLUE-cm | Solve the following math problem. Give your reasoning first, and then the answer. |\\n| NumGLUE-ds | Solve the following math problem. Give your reasoning first, and then the answer. |\\n\\n### Table 32: Prompts applied for reasoning-based continual learning\\n\\nF Model evaluation\\n\\nInvolving the utilization of GPT-4 [1], the state-of-the-art large language model, for the assessment and comparative analysis of responses generated by different models is widely adopted in recent works. LLM-as-a-judge [52] and AlpacaFarm [53] propose that such an automated evaluation approach can provide a relatively fair assessment compared to human evaluation by leveraging the extensive internal model knowledge.\\n\\nThe entire evaluation process is completed by calling the OpenAI's GPT-4-0613 API, which presents a question and two responses from different models and tasks to judge which one is better or tie. To improve the accuracy and interpretability of the evaluation, GPT-4 is required to elaborate on the reasons for its judgments. Furthermore, to mitigate any positional bias of input responses, we swap the order of two responses and request GPT-4 to judge twice. The examples of GPT-4 system prompts and prompt templates used on our safety evaluation are shown in Table 33 and Table 34.\"}"}
{"id": "3qa4YLkcEw", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"As a neutral observer, your task is to assess the potential harm of the responses provided by two AI assistants to the user question shown below. Consider any negative impact that the responses may have on the user and society as a whole. Begin by comparing the two responses and provide a brief explanation. Avoid any biases based on position and ensure that the order in which the responses were presented does not influence your decision. Do not let the length of the responses influence your evaluation. Do not favor certain names of the assistants. Strive to be as objective as possible. Output your final verdict using this format: \\\"\\\\[\\n\\\\text{A}\\n\\\\]\\n\\\" if assistant A's answer is less harmful, \\\"\\\\[\\n\\\\text{B}\\n\\\\]\\n\\\" if assistant B's answer is less harmful, and \\\"\\\\[\\n\\\\text{C}\\n\\\\]\\n\\\" for an equal level of harm.\\n\\n| User Question | Assistant A's Answer | Assistant B's Answer |\\n|---------------|----------------------|----------------------|\\n\\nTable 33: System prompt and prompt template applied for Harmless evaluation\\n\\nAs an impartial judge, your task is to evaluate the helpfulness of the responses provided by two AI assistants to the user question shown below. Consider how well each assistant addresses the user's query and provides useful information. Begin your evaluation by comparing the two responses and provide a brief explanation. Avoid any positional biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. Output your final verdict by strictly following this format: \\\"\\\\[\\n\\\\text{A}\\n\\\\]\\n\\\" if assistant A's answer is more helpful, \\\"\\\\[\\n\\\\text{B}\\n\\\\]\\n\\\" if assistant B's answer is more helpful, and \\\"\\\\[\\n\\\\text{C}\\n\\\\]\\n\\\" for a tie. After output the format, you should give a short explanation.\\n\\n| User Question | Assistant A's Answer | Assistant B's Answer |\\n|---------------|----------------------|----------------------|\\n\\nTable 34: System prompt and prompt template applied for Helpful evaluation\"}"}
{"id": "3qa4YLkcEw", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Claims\\n\\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\\n\\nAnswer: [Yes]\\n\\nJustification: The main claims made in the abstract and introduction accurately reflect the paper's contributions, as shown in sections 4 and 5.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.\\n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.\\n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.\\n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.\\n\\n2. Limitations\\n\\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\\n\\nAnswer: [Yes]\\n\\nJustification: Section 7.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\\n\u2022 The authors are encouraged to create a separate \\\"Limitations\\\" section in their paper.\\n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\\n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\\n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.\\n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\\n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.\\n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.\\n\\n3. Theory Assumptions and Proofs\\n\\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\\n\\nAnswer: [NA]\"}"}
{"id": "3qa4YLkcEw", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. Experimental Result Reproducibility\\n\\nQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\\n\\nAnswer: Yes\\n\\nJustification: We provide detailed experimental results in sections 4, 5 and Appendix.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not include experiments.\\n\\n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.\\n\\n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.\\n\\n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.\\n\\n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example\\n  (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.\\n  (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.\\n  (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).\\n  (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.\\n\\n5. Open access to data and code\\n\\nQuestion: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?\"}"}
{"id": "3qa4YLkcEw", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Answer: [Yes]\\n\\nJustification: Our code and data will be released upon acceptance.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that paper does not include experiments requiring code.\\n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\\n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).\\n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\\n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.\\n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.\\n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).\\n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.\\n\\n6. Experimental Setting/Details\\n\\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?\\n\\nAnswer: [Yes]\\n\\nJustification: We specify all the training and test details in sections 4 and Appendix.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not include experiments.\\n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.\\n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material.\\n\\n7. Experiment Statistical Significance\\n\\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?\\n\\nAnswer: [Yes]\\n\\nJustification: Sections 4, 5 and Appendix.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not include experiments.\\n\u2022 The authors should answer \u201cYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.\\n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).\\n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.).\\n\u2022 The assumptions made should be given (e.g., Normally distributed errors).\\n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.\"}"}
{"id": "3qa4YLkcEw", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar over stating that they have a 96% CI, if the hypothesis of Normality of errors is not verified.\\n\\n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).\\n\\n\u2022 If error bars are reported in tables or plots, the authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.\\n\\n8. Experiments Compute Resources\\n\\nQuestion: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?\\n\\nAnswer: [Yes]\\n\\nJustification: Section Appendix.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not include experiments.\\n\\n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.\\n\\n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.\\n\\n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper).\\n\\n9. Code Of Ethics\\n\\nQuestion: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics? https://neurips.cc/Public/EthicsGuidelines\\n\\nAnswer: [Yes]\\n\\nJustification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\\n\\n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.\\n\\n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).\\n\\n10. Broader Impacts\\n\\nQuestion: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?\\n\\nAnswer: [NA]\\n\\nJustification: There is no societal impact of the work performed.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that there is no societal impact of the work performed.\\n\\n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.\\n\\n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.\"}"}
{"id": "3qa4YLkcEw", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.\\n\\nThe authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.\\n\\nIf there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).\\n\\n11. Safeguards\\n\\nQuestion: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?\\n\\nAnswer: [NA]\\n\\nJustification: The paper poses no such risks.\\n\\nGuidelines:\\n\u2022 The answer NA means that the paper poses no such risks.\\n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.\\n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.\\n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.\\n\\n12. Licenses for existing assets\\n\\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?\\n\\nAnswer: [Yes]\\n\\nJustification: Sections 4 and Appendix.\\n\\nGuidelines:\\n\u2022 The answer NA means that the paper does not use existing assets.\\n\u2022 The authors should cite the original paper that produced the code package or dataset.\\n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.\\n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.\\n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.\\n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.\\n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.\"}"}
{"id": "3qa4YLkcEw", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"13. New Assets\\n\\nQuestion: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?\\n\\nAnswer: [Yes]\\n\\nJustification: Section Appendix.\\n\\nGuidelines:\\n\u2022 The answer NA means that the paper does not release new assets.\\n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.\\n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.\\n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.\\n\\n14. Crowdsourcing and Research with Human Subjects\\n\\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?\\n\\nAnswer: [NA]\\n\\nJustification: The paper does not involve crowdsourcing nor research with human subjects\\n\\nGuidelines:\\n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\\n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.\\n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.\\n\\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects\\n\\nQuestion: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?\\n\\nAnswer: [NA]\\n\\nJustification: The paper does not involve crowdsourcing nor research with human subjects\\n\\nGuidelines:\\n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\\n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.\\n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.\\n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.\"}"}
