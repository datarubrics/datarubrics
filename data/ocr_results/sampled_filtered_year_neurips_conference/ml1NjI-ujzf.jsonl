{"id": "ml1NjI-ujzf", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Active-Passive SimStereo \u2013 Benchmarking the Cross-Generalization Capabilities of Deep Learning-based Stereo Methods\\n\\nLaurent Jospin\\n\\n Allen Antony\\n\\n Lian Xu\\n\\n Hamid Laga\\n\\n Farid Boussaid\\n\\n Mohammed Bennamoun\\n\\n1 University of Western Australia\\n2 Murdoch University\\n\\n{laurent.jospin,lian.xu,farid.boussaid,mohammed.bennamoun}@uwa.edu.au\\nH.Laga@murdoch.edu.au\\n\\nAbstract\\n\\nIn stereo vision, self-similar or bland regions can make it difficult to match patches between two images. Active stereo-based methods mitigate this problem by projecting a pseudo-random pattern on the scene so that each patch of an image pair can be identified without ambiguity. However, the projected pattern significantly alters the appearance of the image. If this pattern acts as a form of adversarial noise, it could negatively impact the performance of deep learning-based methods, which are now the de-facto standard for dense stereo vision. In this paper, we propose the Active-Passive SimStereo dataset and a corresponding benchmark to evaluate the performance gap between passive and active stereo images for stereo matching algorithms. Using the proposed benchmark and an additional ablation study, we show that the feature extraction and matching modules of a selection of twenty selected deep learning-based stereo matching methods generalize to active stereo without a problem. However, the disparity refinement modules of three of the twenty architectures (ACVNet, CascadeStereo, and StereoNet) are negatively affected by the active stereo patterns due to their reliance on the appearance of the input images.\\n\\n1 Introduction\\n\\nStereo vision is used by many artificial or natural vision systems to acquire depth information from a pair of 2D projective views of the 3D world. In the context of computer vision, stereo matching operates in a multi-step pipeline (Fig. 2) composed of: (i) a feature volume construction from the left and right views, (ii) a cost volume computation, which may be coupled with a regularization module, (iii) a disparity extraction from the cost volume, which is done using the argmin function, and (iv) a disparity refinement module, which may also use the cost volume and/or the image features as additional cues. The central step in this pipeline is the construction of the cost volume, which is a function $C(x, y, d)$ that measures how unlikely a pixel of spatial coordinates $(x, y)$ is to be assigned a disparity value $d$. Textureless and repetitive patterns in images can produce flat or periodic cost curves in the cost volume, leading to erroneous disparity maps in passive stereo systems, where only a pair of cameras is used. To address this issue, active stereo-based methods [13] project a pseudo-random light pattern on the scene to remove the textureless or self-similar areas in the stereo images (Fig. 1). Active stereo is now a critical component in many applications such as augmented reality [22] and robotics [3]. They are also part of consumer electronics devices such as smartphones [26].\\n\\nTraditional stereo matching pipelines rely solely on closed-form formulations [29]. However, in recent years, learning-based methods have led to a series of breakthroughs in the field. Early learning-based...\"}"}
{"id": "ml1NjI-ujzf", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 1: A sample from our dataset, with realistic (a) passive and (b) active stereo images along with (c) their corresponding perfect ground truth disparities. The proposed dataset allows the comparison of the relative performance of stereo vision methods when used for passive or active stereo matching.\\n\\nFigure 2: The typical stereo matching pipeline. Based methods focused on replacing one or more blocks in the traditional pipeline with a deep neural network. The latest methods, however, address the problem in an end-to-end fashion; see [15] for a detailed survey. Due to the lack of public active stereo datasets and the fact that passive stereo was perceived as more challenging, most of these models have been trained for the passive stereo problem. An important property of the closed-form formulae used in traditional stereo matching methods is that when non-self similar texture is added to the scene, their performance monotonically increases. This key feature is at the core of active stereo systems [21]. If one can determine that the latest deep learning methods can also leverage active pseudo random noise to improve their prediction, this would show that these methods are indeed learning to match similar regions of the images rather than fitting some bias into the data. Additionally, it provides some insight into the models' generalization ability, which is important for their safe deployment in their intended application (e.g., autonomous driving).\\n\\nUnder ideal conditions, deep learning-based methods are expected to behave in a similar fashion to their non learning counterpart and exhibit improved performance when additional pseudo-random texture is added to the scene. Yet, many large-scale deep learning models see a degradation of their performances when used on datasets that are only slightly different from their original training datasets [43]. They often require an adaptation procedure to generalize to new unseen domains [33]. Furthermore, they can be severely affected by even little adversarial noise under certain circumstances [32], as they are prone to overfitting on small biases present in their training data [23]. However, these flaws are not everywhere. For example, it has already been shown that once simulated images are close enough to real images, deep learning stereo systems generalize without issues [34]. Also, unlike adversarial noise, the pseudo random patterns used in active stereo have not been learned specifically to cause failure for deep learning models. This means that existing deep learning methods might generalize to the active stereo domain without any form of fine-tuning.\\n\\nIn this work, we investigate how different state-of-the-art deep learning-based stereo matching architectures are impacted when presented with active, instead of passive, stereo images. To make the evaluation of the generalization ability of stereo vision models easier, we propose Active-Passive SimStereo, a novel dedicated dataset composed of computer-generated images rendered using a physically-based rendering engine. The proposed dataset provides both active and passive frames for each given scene. This allows to evaluate and compare the performance of each algorithm on active and passive stereo using exactly the same scenes. The data set is publicly available at https://dx.doi.org/10.21227/gf1e-t452.\\n\\nThe remaining parts of the paper are organized as follows. Section 2 reviews the related work. Section 3 describes the proposed dataset. Section 4 presents the proposed benchmark used for\"}"}
{"id": "ml1NjI-ujzf", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2 Related Work\\n\\nMany datasets and benchmarks have been proposed for passive stereo vision including the popular Middleburry dataset [29, 11], whose latest version uses a precise but expensive reconstruction pipeline to acquire the ground truth [30]. The corresponding Middleburry Stereo Evaluation benchmark is widely used to evaluate stereo vision algorithms. Due to the challenges associated with the 3D ground-truth acquisition, the aforementioned dataset only contains a small amount of labelled data, which is not sufficient to train large-scale deep architectures. Subsequently, the Scene Flow datasets have been proposed [19]. They contain a large number of simulated image pairs with ground-truth optical flows and disparities generated from open source motion graphics short movies or randomized virtual 3D objects. However, the appearance of these simulated scenes is not realistic. Thus, most deep learning-based models for stereo vision need to be fine-tuned after being trained on the Scene Flow datasets. The UnrealStereo4K simulated dataset [34] was later proposed to provide higher resolution and more realistic images, taken from video games scenes.\\n\\nOne of the most popular applications of stereo vision is autonomous driving, since vision based systems offer a cost-effective alternative or complement to LIDAR-based systems for depth measurement. Thus, many datasets and benchmarks have been specifically developed for this application. Examples include the KITTI Vision suite [20, 7], which is currently the most popular stereo vision benchmark for autonomous driving, DrivingStereo [40], which is a large dataset commonly used for training rather than evaluation, and ApolloScape [12], which provides a benchmark suite for different challenges related to autonomous driving, including stereo vision. The ground truth of these datasets was obtained using a LIDAR-based system. Occasionally, a recognition system was also used to detect and categorize cars in images before aligning a CAD model onto the LIDAR depth map [20, 12]. The inherent noise associated with these various processing steps implies that the ground truth cannot be trusted for very precise reconstructions. However, given that autonomous driving scenarios do accommodate a disparity error of one or two pixels, this is not a problem for the intended use of those datasets.\\n\\nFor active stereo vision, there are far fewer public datasets, none of which has become popular for training or evaluating deep learning-based stereo matching methods. The few end-to-end methods trained for active stereo use soft labels, i.e., labels with associated uncertainty, such as the depth generated by stereo cameras [44, 41]. Other methods did also use self-supervision, e.g., by using the information conserved when compressing a given image patch as supervisory signal [28]. Simulation techniques have also been proposed to generate semi-realistic images from CAD models [25] based on screen space projection of texture. This approach has been used on multiple occasions [28, 44], but none of the produced datasets has been made public. In this work, we use a similar approach but with a physically-based rendering pipeline to improve the realism of the scenes, making our dataset more suitable for evaluation.\"}"}
{"id": "ml1NjI-ujzf", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Illustration of the benefits of a physically-based rendering pipeline. In our images (c), (d), indirect lighting creates soft and realist shadows and specularities, which is not the case with existing simulated datasets (a), (b).\\n\\nFor each 3D scene, we designed two lighting setups. The first corresponds to a passive stereo acquisition scenario without any pseudo-random pattern while the second one corresponds to an active stereo acquisition scenario with a collection of lights projecting a pseudo-random pattern. We used the Cycles path tracing engine integrated into Blender for rendering. We used standard shaders for the non-textured light sources. For the pseudo-random pattern light projectors, we used a custom programmed shader to generate a pattern resembling the one from the RealSense cameras. The light intensity \\\\( I(d) \\\\), is a function of the incoming direction \\\\( d \\\\):\\n\\n\\\\[\\nI(d) = p_1^2 \\\\left(1 - W_{s1} \\\\left(d + t \\\\right)^2 \\\\right) + c_1^2 \\\\left(1 - W_{s2} \\\\left(d + t \\\\right)^2 \\\\right) p_2^2.\\n\\\\]\\n\\nHere, \\\\( W_{s1} \\\\) and \\\\( W_{s2} \\\\) are Whorley noise patterns; \\\\( s_1 \\\\) and \\\\( s_2 \\\\) are the scale factors of their respective Whorley noise pattern with \\\\( s_1 < s_2 \\\\); \\\\( p_1 \\\\) and \\\\( p_2 \\\\) are two light intensity correction factors with \\\\( p_1 \\\\ll p_2 \\\\); \\\\( c \\\\) is the minimal power of the \\\\( W_{s2} \\\\) pattern; \\\\( t \\\\) is a random translation of the texture space to generate different patterns for different lights; and \\\\( p \\\\) is the power gain of the lamp, expressed in Watts.\\n\\nThe ground truth is then extracted from the depth pass \\\\( z \\\\) generated by the rendering software. \\\\( z \\\\) measures the distance between the visible point in the 3D scene and the optical center of the camera. The ground truth disparity \\\\( \\\\hat{d} \\\\) can be computed as:\\n\\n\\\\[\\n\\\\hat{d} = Bf z \\\\quad (2)\\n\\\\]\\n\\nwhere \\\\( B \\\\) is the stereo camera baseline and \\\\( f \\\\) is the camera focal length in pixel. We used a Baseline of 0.16 m (where m here refers to the unit used internally by Blender, not the actual meters in the real world), and cameras with a focal length of 48.61 mm, which amounts to 888.89 pixels at standard resolution for a 35 mm film equivalent sensor.\"}"}
{"id": "ml1NjI-ujzf", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: The two different light setups to simulate (a) passive and (b) active stereo acquisition in a given 3D scene.\\n\\nRGB frameNIR frame\\n(a) abstract bowls (b) dining room (c) floating cubes (d) fruits (e) office (f) park (g) plant vintage (h) round\u2019n\u2019twist (i) bedroom (j) space station\\n\\nFigure 5: Sample images from the proposed test set.\\n\\n3.2 The Dataset\\nThe proposed dataset contains 515 image pairs split into a training set (80% of the images) and a test set (20% of the images) used for benchmarking. The test set contains 103 image pairs (Fig. 5), comprising different shapes (e.g., large flat surfaces such as floors or small areas with depth discontinuities such as plant leaves), depth ranges, and styles (i.e., realistic scenes or abstract compositions). The remaining 412 image pairs are contributed as a training set. We use the standard resolution (640 \u00d7 480 pixels) for the benchmark as it matches or approaches the resolution of most stereo cameras. It also guarantees that the images can be processed by most methods even on memory-constrained hardware. Despite the small number of images, the test set is shown to be large enough to evaluate deep learning methods. Detailed experiments to demonstrate this are provided in the Supplementary Material. Our dataset is also large enough for fine-tuning (see Section 7).\\n\\nIn addition to the benchmark, we provide a simulation Blender file with the specific shader, as well as the python code to post-process the images.\\n\\nTable 1: Comparison of the resolution and disparity ranges of different datasets.\\n\\n| Dataset                | Generation method | # images pairs | Resolution [px] | Min | Mean | Max   |\\n|------------------------|-------------------|----------------|-----------------|-----|------|-------|\\n| Ours (train)           | Ray tracing rendering | 412            | 640 \u00d7 480       | 0.00 | 21.77| 212.67|\\n| Ours (test)            | Ray tracing rendering | 103            | 640 \u00d7 480       | 0.00 | 25.12| 129.67|\\n| Middlebury 2014 [30]   | Large baseline active stereo | 33             | 2850 \u00d7 1900     | 1   | 28.94| 148.36|\\n| Kitti 2012 [7]         | Real images + Laser scanner | 389            | 1240 \u00d7 380      | 1   | 4.11 | 38.32 |\\n| Kitti 2015 [20]        | Real images + Laser scanner + 3D cad object alignment | 400            | 1240 \u00d7 380      | 1   | 4.46 | 227.99|\\n| Sceneflow [19]         | Screen space rasterization | 39049         | 960 \u00d7 540       | 1.12| 39.87| 940.75|\\n| UnrealStereo4k [34]    | Screen space rasterization | 7200          | 3840 \u00d7 2160     | 0.01| 173.20|1515.60|\\n\\nThe images in these datasets have variable sizes. Thus, the values given here are an approximation.\\n\\n4 The Benchmark\\nWe use five different scores to compare the generalisation abilities of different methods. The most important metric in stereo vision is the BAD-N, which measures the proportion of pixels above a given error threshold $N$. It is computed over a test set $T$ as:\\n\\n$$\\\\text{BAD}_N = \\\\frac{1}{\\\\|T\\\\|} \\\\sum_{I \\\\in T} \\\\sum_{h I_i = 0} \\\\sum_{w I_j = 0} \\\\frac{1}{|\\\\Delta d_{i,j}| > N_{hw}}$$\"}"}
{"id": "ml1NjI-ujzf", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $h$ and $w$, respectively, are the height and the width of the image $I$. $A$ is the indicator function of $A$ and $\\\\Delta_d$ is the disparity error for the image $I$. We report results for $N \\\\in \\\\{0.5, 1, 2, 4\\\\}$. Lower BAD$_N$ scores indicate a better accuracy in reconstructing the disparity. In the rest of the paper, we will indicate this by appending a down arrow ($\\\\downarrow$) after the name of these metrics.\\n\\nWe also use the Mean Absolute Error (MAE) and the Root Mean Square Error (RMSE). The former is a good estimate of the expected amplitude of the error of a given method, and the latter, especially when compared to the MAE, is a good indicator of the presence of outlier points with large errors.\\n\\nThe RMSE and MAE scores over $T$ are computed as:\\n\\n$$\\n\\\\text{RMSE}_T = \\\\frac{1}{\\\\|T\\\\|} \\\\sum_{I \\\\in T} \\\\sum_{h_i=0}^{h} \\\\sum_{w_j=0}^{w} \\\\Delta_d_{i,j}^2 \\\\cdot hw,\\n$$\\n\\n$$\\n\\\\text{MAE}_T = \\\\frac{1}{\\\\|T\\\\|} \\\\sum_{I \\\\in T} \\\\sum_{h_i=0}^{h} \\\\sum_{w_j=0}^{w} |\\\\Delta_d_{i,j}| \\\\cdot hw.\\n$$\\n\\nThese scores are the average of the corresponding metric over the test set. Lower MAE and RMSE indicate a better accuracy in reconstructing the disparity. In the rest of the paper, we will indicate this by appending a down arrow ($\\\\downarrow$) after the name of these metrics.\\n\\nA consequence of using the MAE and RMSE is that if a method exhibits low performances on a specific image, the overall score of the method will be greatly influenced by this single image. For an absolute performance evaluation benchmark, this is not a problem. However, this is an issue when evaluating the relative performance variation resulting from a domain change (e.g., from passive stereo to active stereo). To mitigate this use, we measure the mean relative score variation across all images. Given a metric $M$, the relative score variation $R_M$ is computed as:\\n\\n$$\\nR_M = \\\\frac{1}{\\\\|T\\\\|} \\\\sum_{I \\\\in T} M_{IP} - M_{IA}.\\n$$\\n\\nwhere $M_{IP}$ and $M_{IA}$ are the metric scores evaluated on the passive stereo results and active stereo results of an image $I$, respectively. In this paper, we focus on $R_{\\\\text{MAE}}$ and $R_{\\\\text{BAD}_2}$.\\n\\nWe also report the proportion $P_M$ of the testing images in which the active stereo results outperform their passive stereo counterparts in terms of the metric $M$. It is formulated as:\\n\\n$$\\nP_M = \\\\frac{1}{\\\\|T\\\\|} \\\\sum_{I \\\\in T} 1_{M_{IA} < M_{IP}}.\\n$$\\n\\nIn this paper, we focus on $P_{\\\\text{MAE}}$ and $P_{\\\\text{BAD}_2}$. We report in the supplementary material other variants of those metrics.\\n\\nHigher $P$ and $R$ scores indicate a better generalization of the method from passive stereo to active stereo. In the rest of the paper, we will indicate this by appending an up arrow ($\\\\uparrow$) after the name of these metrics.\\n\\nFinally, discontinuities in the depth image are an important parameter in evaluating stereo reconstruction methods. Discussing the detailed performances of existing methods would be beyond the scope of this paper, which focuses on the generalization capabilities from passive to active stereo and vice versa. Nonetheless, we also included, in the supplementary material, versions of all the metrics presented above, restricted either on the edge regions of the image or on its flat regions.\\n\\n5 Results on Existing Methods\\n\\nWe used the proposed benchmark to evaluate state-of-the-art, end-to-end deep neural networks for stereo-matching. We considered 20 methods for which the source code and the pre-trained models are available. In the supplementary material, we also evaluate a selection of traditional non-learning methods. Those methods are listed in Table 2, along with the datasets used to train the models. We used mainly the models weights trained on SceneFlow [19] and fine-tuned on KITTI 2015 [20] as this approach is the standard for deep stereo models [15]. For each method, we list the reported D1 score in the KITTI 2015 benchmark [20]. This is the proportion of pixels with an error greater than 3px or 5% of the disparity. We also report, if available, the BAD$_2$ and MAE scores of the method in the Middlebury benchmark [30].\"}"}
{"id": "ml1NjI-ujzf", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method                      | Stereo type | Train set | Fine-tuned set | D1-all |\\n|----------------------------|-------------|-----------|----------------|--------|\\n| Bad                        |             |           |                |        |\\n| MAE                        |             |           |                |        |\\n| AANet [38]                 | Passive     | SceneFlow | KITTI 2015     | 2.55%  | 25.20% | 8.88px |\\n| ACVNet [37]                | Passive     | SceneFlow | KITTI 2015     | 1.65%  | 13.60% | 8.24px |\\n| AnyNet [35]                | Passive     | SceneFlow | KITTI 2015     | 6.20%  | -      | -      |\\n| CascadeStereo [9]          | Passive     | SceneFlow | KITTI 2015     | 2.00%  | 18.80% | 4.50px |\\n| CREStereo [16]             | Passive     | CREStereo | ETH3D           | 1.69%  | 3.71%  | 1.15px |\\n| Deep-Pruner (best) [6]     | Passive     | SceneFlow | KITTI 2015     | 2.15%  | 30.10% | 4.80px |\\n| Deep-Pruner (fast) [6]     | Passive     | SceneFlow | KITTI 2015     | 2.59%  | -      | -      |\\n| GA-Net [42]                | Passive     | SceneFlow | KITTI 2015     | 2.59%  | 18.90% | 12.20px|\\n| GwcNet [10]                | Passive     | SceneFlow | KITTI 2015     | 2.11%  | -      | -      |\\n| HighResStereo [39]         | Passive     | SceneFlow | KITTI 2015     | 2.14%  | 10.20% | 2.07px |\\n| Lac-GwcNet [18]            | Passive     | SceneFlow | KITTI 2015     | 1.77%  | -      | -      |\\n| MobileStereoNet3D [31]     | Passive     | SceneFlow | KITTI 2015     | 2.10%  | -      | -      |\\n| MobileStereoNet2D [31]     | Passive     | SceneFlow | KITTI 2015     | -      | -      | -      |\\n| PSM-Net [1]                | Passive     | SceneFlow | KITTI 2015     | 2.32%  | 42.10% | 6.68px |\\n| RAFT-Stereo [17]           | Passive     | SceneFlow | -              | -      | -      | 4.74%  |\\n| RealTimeStereo [2]         | Passive     | SceneFlow | KITTI 2015     | 7.54%  | -      | -      |\\n| SMD-Nets [34]              | Passive     | UnrealStereo | 4K | 2.08%  | -      | -      |\\n| SRH-Net [5]                | Passive     | SceneFlow | KITTI 2015     | -      | -      | -      |\\n| StereoNet [14]             | Passive     | SceneFlow | -              | -      | -      | -      |\\n| ActiveStereoNet [44]       | Active      | Self-supervised | - | -      | -      | -      |\\n\\nAs we are more interested in the generalization abilities of the different methods than their peak performances, we did not fine-tune any of these methods to our dataset. This is to ensure the measured generalization performances are not biased by fine-tuning for one specific modality in our dataset. On the other hand, fine-tuning for both modalities would not give the same insights on the generalization abilities of each method. However, this means that the reported performances should not be considered as the best achievable performances of the studied methods. In a second step, we fine-tune the methods which failed to generalize on our dataset to check if it can be used to fine-tune passive stereo methods on active stereo (Section 7).\\n\\nIn this paper, we focus on the main aggregate results as well as on our general observations when analysing the error maps. The detailed scores per image are provided in the supplementary material.\\n\\nTable 3 reports results on passive and active stereo images. We observed that, when evaluated on active stereo images, all considered methods show an improvement in their performance for all considered metrics. Please keep in mind that not all methods have been trained on the same dataset or even domain; our benchmark aims to evaluate the relative performances of the different methods rather than their absolute performances. ActiveStereoNet [44] stands out as the worst performing method for passive stereo but its performance drastically improves when presented with active stereo images. This is due to the fact that it is the only one trained on active stereo images. A model trained for active stereo is not expected to generalize well on passive stereo without adaptation since the domain shift makes matching much harder. ActiveStereoNet is far from being the best performing method.\"}"}
{"id": "ml1NjI-ujzf", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method                  | MAE  | BAD  | R  | MAE  |\\n|------------------------|------|------|---|------|\\n| AANet [38]             | 87%  | 99%  | 35%| 55%  |\\n| ACVNet [37]            | 71%  | 74%  | 30%| 37%  |\\n| AnyNet [35]            | 93%  | 98%  | 29%| 29%  |\\n| Cascade-Stereo [9]     | 65%  | 71%  | 25%| 28%  |\\n| CREStereo [16]         | 80%  | 62%  | 41%| 34%  |\\n| Deep-Pruner (best) [6] | 94%  | 100% | 52%| 62%  |\\n| Deep-Pruner (fast) [6] | 95%  | 98%  | 47%| 56%  |\\n| GA-Net [42]            | 87%  | 96%  | 43%| 59%  |\\n| GwcNet [10]            | 99%  | 99%  | 55%| 63%  |\\n| High-Res-Stereo [39]   | 84%  | 85%  | 36%| 49%  |\\n| Lac-GwcNet [18]        | 96%  | 96%  | 55%| 63%  |\\n| MobileStereoNet3D [31] | 96%  | 99%  | 45%| 59%  |\\n| MobileStereoNet2D [31] | 96%  | 99%  | 52%| 65%  |\\n| PSM-Net [1]            | 90%  | 93%  | 30%| 38%  |\\n| RAFT-Stereo [17]       | 81%  | 73%  | 37%| 39%  |\\n| RealTimeStereo [2]     | 92%  | 97%  | 34%| 35%  |\\n| SMD-Nets [34]          | 95%  | 97%  | 54%| 60%  |\\n| SRH-Net [5]            | 97%  | 98%  | 48%| 64%  |\\n| StereoNet [14]         | 84%  | 85%  | 38%| 45%  |\\n| ActiveStereoNet [44]   | 99%  | 99%  | 68%| 58%  |\\n\\nThe neural network architecture is probably at play here. ActiveStereoNet uses the same architecture as StereoNet [14], albeit a different training method. StereoNet is not the best performing method on passive stereo and is outperformed by ActiveStereoNet on active stereo images, which demonstrates the benefits of the training strategy proposed by the authors of ActiveStereoNet.\\n\\nThe relative score improvements, reported in Table 4, also show that, overall, existing methods have a good capability to generalize to active stereo. ACVNet [37], Cascade Stereo [9] as well as CREStereo [16] and RAFT-Stereo [17] are the only methods that seem to have issues generalizing, as their P\\\\textsuperscript{MAE} and P\\\\textsuperscript{BAD\\\\textsuperscript{2}} scores are way lower than the other methods. For CREStereo [16] and RAFT-Stereo [17], it turns out this is more due to the fact that these methods perform so well on passive stereo, see Table 3, that they have less room for improvement when moving to active stereo. For ACVNet [37] and Cascade Stereo [9], this is because moving to the active stereo domain causes artifacts in the reconstructed disparities, see Section 6 for more details.\\n\\nThese results are encouraging and show that, when trained on passive stereo, current state-of-the-art deep learning methods are able to generalize to active stereo. Generalizing to passive stereo for a method trained on active stereo, seems to be much harder. However, the results are not very conclusive, since only one method is covered, and it has been trained in a self-supervised fashion. The details of these results, as well as the performance of each method on each image, are provided in the results Excel sheet included in the supplementary material.\\n\\n6 Ablation study\\n\\nThe visual inspection of the results shows that many visual artifacts are difficult to capture by aggregate metrics; see for example Figure 7. The refinement module at the end of the stereo matching pipeline is the one most likely to be impacted by a switch from passive to active stereo. Consequently, we chose to test all methods a second time, but with their final refinement module deactivated. Figure 6 shows error maps for methods which did generalize to active stereo while Figure 7 shows error maps of the methods which had issues generalizing (ACVNet [37], Cascade Stereo [9] and StereoNet [14]).\\n\\nLooking into more details at the error maps of Fig. 7, we can notice different effects that may explain these observations. In certain rare situations, artifacts will appear in the depth map reconstructed using active stereo; see Fig. 7a and Fig. 7b. This indicates that under specific circumstances, the pseudo random pattern used for active stereo can act like an adversarial noise. ACVNet has such artifacts around small objects edges, like for example the handle of the chair in Fig. 7a. ACVNet uses...\"}"}
{"id": "ml1NjI-ujzf", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Detailed error maps comparison between the unrefined and refined disparity maps for methods without apparent problems in active stereo compared to passive stereo.\\n\\n(a) Office of Fig. 5e with ACVNet [37].\\n\\n(b) Bedroom of Fig. 5i with CascadeStereo [9].\\n\\n(c) Fruits of Fig. 5d with StereoNet [14].\\n\\nFigure 7: Detailed error maps comparison shows some artifacts not present in the unrefined active disparity appearing in the refined active disparity of certain methods, showing that their refinement module is negatively impacted by the active stereo pseudo random pattern.\\n\\nan attention based mechanism to post-process the matching cost volume. This tends to generate edge artifacts in active stereo images. The refinement process is negatively impacted by these artifacts, which further degrade the quality of the final disparity map.\\n\\nCascade Stereo has some of the most visible examples of such effects, where large error patches seem to appear in the active stereo disparity map, while they are not present in the passive disparity map. If refinement is turned off, the error maps for active and passive stereo are quite similar; see Fig. 7b. Note that the errors between unrefined and refined disparities can vary quite a lot for CascadeStereo compared to the other methods in Figure 6 and Figure 7. This shows that CascadeStereo relies extensively on its final refinement module to improve its results, especially compared to other methods. This, in turn, explains why it does not generalize as well as other architectures; see Table 4.\\n\\nStereoNet also exhibits a strange behaviour; see Fig. 7c. While the network is able to use the pseudo random noise to reconstruct the disparity in uniform regions, the disparity reconstruction appears to be noisy in these areas. Once again, deactivating the refinement module removes the problem. This is not surprising since the StereoNet architecture is an hourglass hierarchical architecture. Other hierarchical methods aggregate their cost volume at different resolution (e.g. [42], [1]) before making the disparity prediction on the upsampled cost volume. StereoNet makes the prediction on a downsampled cost volume and then uses a module guided only by the initial image to upsample the disparity [14]. This approach makes StereoNet fast, but also very reliant on the appearance of the input images. This makes generalization to active stereo difficult.\\n\\n7 Fine-tuning the models which failed to generalize\\n\\nCan the problems some methods encountered when trying to generalize their predictions from passive to active stereo image pairs be eliminated by fine-tuning the aforementioned architectures on the active stereo images of our dataset? To test this, we fine-tuned ACVNet [37], Cascade-Stereo [9] and StereoNet [14] on our test set for 10 epochs. To avoid any problem of catastrophic forgetting, we ensured that the learning rate is kept low, at $5 \\\\times 10^{-5}$ per mini-batch. Each mini-batch is made of four images and each epoch contained 103 mini-batches. The loss used for training the initial model was used for fine-tuning along with all hyperparameters specified for the original model.\"}"}
{"id": "ml1NjI-ujzf", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5: Evaluation of the three fine-tuned methods, ACVNet [37], Cascade-Stereo [9] and StereoNet [14].\\n\\n| Method                  | RMSE (px) | MAE (px) | T\u2193 | BAD 0.5 | BAD 1  | BAD 2  |\\n|-------------------------|-----------|----------|----|--------|--------|--------|\\n| Active stereo images    |           |          |    |        |        |        |\\n| ACVNet [37] (original)  | 3.49      | 1.31     | 23%| 13%    | 8%     | 5%     |\\n| ACVNet [37] (active stereo fine-tuned) | 2.16 | 0.66 | 17% | 7% | 4% | 3% |\\n| Cascade-Stereo [9] (original) | 4.48 | 2.07 | 64% | 33% | 14% | 8% |\\n| Cascade-Stereo [9] (active stereo fine-tuned) | 2.11 | 0.66 | 20% | 7% | 4% | 2% |\\n| StereoNet [14] (original)  | 7.74      | 1.79     | 44%| 23%    | 12%    | 7%     |\\n| StereoNet [14] (active stereo fine-tuned) | 7.50 | 1.78 | 56% | 29% | 13% | 7% |\\n\\n(a) Office of Fig. 5e with ACVNet [37].\\n\\n(b) Bedroom of Fig. 5i with CascadeStereo [9].\\n\\n(c) Fruits of Fig. 5d with StereoNet [14].\\n\\nFigure 8: Error maps of ACVNet, CascadeStereo and StereoNet when fine-tuned on our dataset.\\n\\nThe results after fine-tuning both methods are reported in Table 5. One can observe that ACVNet and Cascade-Stereo exhibit a drastic improvement in their performance. The visual inspection of the results reveals that for most image regions, Cascade-Stereo error is now mostly below one pixel, showing no large visible artifacts. ACVNet sees similar improvements to Cascade-Stereo. This shows that the methods were able to adapt to active stereo after fine-tuning.\\n\\nFor StereoNet, the RMSE improved, the BAD 0.5 and BAD 1 metric degraded, while the other metrics stagnated. No more artifacts correlated to the active pattern points distribution are visible. However, new edge artifacts appear in certain images. This indicates that the fine-tuning is turning the refinement module off instead of adapting it to the projected dot pattern. This, in turn, shows that an architecture largely reliant on the smoothness of the input images is ill-suited for active stereo vision. ActiveStereoNet solved this issue by decoupling the image convolutions from the disparity convolutions in their refinement module [44], which gave it more flexibility to distinguish the active pattern from real objects.\\n\\n8 Conclusion and Perspectives\\n\\nWe proposed the first dataset and associated benchmark that enables the comparison of the relative performance of stereo vision algorithms when applied to active and passive stereo. Using this dataset, we undertook extensive experiments to evaluate the performance of twenty state-of-the-art end-to-end deep learning models. The reported results show that it is possible, to a certain extent, to use methods trained for passive stereo for active stereo vision. This work also shows that the weak point of those architectures is the final refinement layers. Using our training set, we were able to improve the performances of StereoNet [14] and CascadeStereo [9], which had difficulty generalizing to active stereo. StereoNet, the architecture reliant on the smoothness of the input image pair for refinement, still had very poor results, indicating that models that favor shapes prior over appearance priors are more robust.\\n\\nActive stereo is an important subfield of stereo-vision. By using our proposed dataset, we were able to examine the generalization ability of current deep learning models. The dataset can also be used to fine-tune these deep learning stereo models for active stereo vision. In the future, we expect to see a growing number of ever larger deep neural networks for stereo vision. Thus, being able to evaluate their generalization ability will become more and more important, and our dataset will prove invaluable in this regard.\"}"}
{"id": "ml1NjI-ujzf", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgments and Disclosure of Funding\\n\\nWe thank the reviewers for their helpful comments. The authors have no competing interests to disclose. This research is supported by the Australian Research Council grants ARC DP220102197 and ARC DP210101682.\\n\\nReferences\\n\\n[1] Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo matching network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5410\u20135418, 2018.\\n\\n[2] Jia-Ren Chang, Pei-Chun Chang, and Yong-Sheng Chen. Attention-aware feature aggregation for real-time stereo matching on edge devices. In Proceedings of the Asian Conference on Computer Vision (ACCV), November 2020.\\n\\n[3] Shengyong Chen, Youfu Li, and Ngai Ming Kwok. Active vision in robotic systems: A survey of recent developments. The International Journal of Robotics Research, 30(11):1343\u20131377, 2011.\\n\\n[4] Blender Online Community. Blender - a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. URL http://www.blender.org.\\n\\n[5] Hongzhi Du, Yanyan Li, Yanbiao Sun, Jigui Zhu, and Federico Tombari. Srh-net: Stacked recurrent hourglass network for stereo matching. IEEE Robotics and Automation Letters, 6(4):8005\u20138012, 2021.\\n\\n[6] Shivam Duggal, Shenlong Wang, Wei-Chiu Ma, Rui Hu, and Raquel Urtasun. Deeppruner: Learning efficient stereo matching via differentiable patchmatch. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 4383\u20134392, 2019.\\n\\n[7] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\\n\\n[8] Gabriel F. Giralt. The interchangeability of vfx and live action and its implications for realism. Journal of Film and Video, 69(1):3\u201317, 2017. ISSN 07424671, 19346018.\\n\\n[9] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\\n\\n[10] Xiaoyang Guo, Kai Yang, Wukui Yang, Xiaogang Wang, and Hongsheng Li. Group-wise correlation stereo network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3273\u20133282, 2019.\\n\\n[11] Heiko Hirschmuller and Daniel Scharstein. Evaluation of cost functions for stereo matching. In 2007 IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20138. IEEE, 2007.\\n\\n[12] Xinyu Huang, Peng Wang, Xinjing Cheng, Dingfu Zhou, Qichuan Geng, and Ruigang Yang. The apolloscape open dataset for autonomous driving and its application. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(10):2702\u20132719, 2020.\\n\\n[13] Leonid Keselman, John Iselin Woodfill, Anders Grunnet-Jepsen, and Achintya Bhowmik. Intel realsense stereoscopic depth cameras. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, July 2017.\\n\\n[14] Sameh Khamis, Sean Fanello, Christoph Rhemann, Adarsh Kowdle, Julien Valentin, and Shahram Izadi. Stereonet: Guided hierarchical refinement for real-time edge-aware depth prediction. In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany, pages 8\u201314, 2018.\\n\\n[15] Hamid Laga, Laurent Valentin Jospin, Farid Boussaid, and Mohammed Bennamoun. A survey on deep learning techniques for stereo-based depth estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.\"}"}
{"id": "ml1NjI-ujzf", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[16] Jiankun Li, Peisen Wang, Pengfei Xiong, Tao Cai, Ziwei Yan, Lei Yang, Jiangyu Liu, Hao-qiang Fan, and Shuaicheng Liu. Practical stereo matching via cascaded recurrent network with adaptive correlation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16263\u201316272, 2022.\\n\\n[17] Lahav Lipson, Zachary Teed, and Jia Deng. Raft-stereo: Multilevel recurrent field transforms for stereo matching. In International Conference on 3D Vision (3DV), 2021.\\n\\n[18] Biyang Liu, Huimin Yu, and Yangqi Long. Local similarity pattern and cost self-reassembling for deep stereo matching networks. Proceedings of the AAAI Conference on Artificial Intelligence, 36(2):1647\u20131655, Jun. 2022.\\n\\n[19] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4040\u20134048, 2016.\\n\\n[20] Moritz Menze and Andreas Geiger. Object scene flow for autonomous vehicles. In Conference on Computer Vision and Pattern Recognition (CVPR), 2015.\\n\\n[21] H. K. Nishihara. Practical Real-Time Imaging Stereo Matcher. Optical Engineering, 23(5):536 \u2013 545, 1984.\\n\\n[22] Sergio Orts-Escolano, Christoph Rhemann, Sean Fanello, Wayne Chang, Adarsh Kowdle, Yury Degtyarev, David Kim, Philip L. Davidson, Sameh Khamis, Mingsong Dou, Vladimir Tankovich, Charles Loop, Qin Cai, Philip A. Chou, Sarah Mennicken, Julien Valentin, Vivek Pradeep, Shenlong Wang, Sing Bing Kang, Pushmeet Kohli, Yuliya Lutchyn, Cem Keskin, and Shahram Izadi. Holoportation: Virtual 3d teleportation in real-time. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, UIST '16, page 741\u2013754, New York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450341899.\\n\\n[23] Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, and Alex Hanna. Data and its (dis)contents: A survey of dataset development and use in machine learning research. Patterns, 2(11):100336, 2021. ISSN 2666\u20133899.\\n\\n[24] Matt Pharr, Wenzel Jakob, and Greg Humphreys. Physically based rendering: From theory to implementation. Morgan Kaufmann, 2016.\\n\\n[25] Benjamin Planche, Ziyan Wu, Kai Ma, Shanhui Sun, Stefan Kluckner, Oliver Lehmann, Terrence Chen, Andreas Hutter, Sergey Zakharov, Harald Kosch, and Jan Ernst. Depthsynth: Real-time realistic synthetic data generation from cad models for 2.5d recognition. In 2017 International Conference on 3D Vision (3DV), pages 1\u201310, 2017.\\n\\n[26] Tomislav Pribani\u0107, Tomislav Petkovi\u0107, Matea Djonli\u0107, Vincent Angladon, and Simone Gasparini. 3d structured light scanner on the smartphone. In Aur\u00e9lio Campilho and Fakhri Karray, editors, Image Analysis and Recognition, pages 443\u2013450, Cham, 2016. Springer International Publishing. ISBN 978-3-319-41501-7.\\n\\n[27] Gernot Riegler, Yiyi Liao, Simon Donne, Vladlen Koltun, and Andreas Geiger. Connecting the dots: Learning representations for active monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\\n\\n[28] Sean Ryan Fanello, Julien Valentin, Christoph Rhemann, Adarsh Kowdle, Vladimir Tankovich, Philip Davidson, and Shahram Izadi. Ultrastereo: Efficient learning-based matching for active stereo systems. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.\\n\\n[29] Daniel Scharstein and Richard Szeliski. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. International journal of computer vision, 47(1):7\u201342, 2002.\\n\\n[30] Daniel Scharstein, Heiko Hirschm\u00fcller, York Kitajima, Greg Krathwohl, Nera Ne\u0161i\u0107, Xi Wang, and Porter Westling. High-resolution stereo datasets with subpixel-accurate ground truth. In German conference on pattern recognition, pages 31\u201342. Springer, 2014.\"}"}
{"id": "ml1NjI-ujzf", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Faranak Shamsafar, Samuel Woerz, Rafia Rahim, and Andreas Zell. Mobilestereonet: Towards lightweight deep networks for stereo matching. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2417\u20132426, 2022.\\n\\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\\n\\nAlessio Tonioni, Matteo Poggi, Stefano Mattoccia, and Luigi Di Stefano. Unsupervised adaptation for deep stereo. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.\\n\\nFabio Tosi, Yiyi Liao, Carolin Schmitt, and Andreas Geiger. Smd-nets: Stereo mixture density networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nYan Wang, Zihang Lai, Gao Huang, Brian H. Wang, Laurens van der Maaten, Mark Campbell, and Kilian Q. Weinberger. Anytime stereo image depth estimation on mobile devices. In 2019 International Conference on Robotics and Automation (ICRA), pages 5893\u20135900, 2019.\\n\\nSteven Worley. A cellular texture basis function. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 291\u2013294, 1996.\\n\\nGangwei Xu, Junda Cheng, Peng Guo, and Xin Yang. Attention concatenation volume for accurate and efficient stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12981\u201312990, 2022.\\n\\nHaofei Xu and Juyong Zhang. Aanet: Adaptive aggregation network for efficient stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1959\u20131968, 2020.\\n\\nGengshan Yang, Joshua Manela, Michael Happold, and Deva Ramanan. Hierarchical deep stereo matching on high-resolution images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\\n\\nGuorun Yang, Xiao Song, Chaoqin Huang, Zhidong Deng, Jianping Shi, and Bolei Zhou. Drivingstereo: A large-scale dataset for stereo matching in autonomous driving scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\\n\\nHaojie Zeng, Bin Wang, Xiaoping Zhou, Xiaojing Sun, Longxiang Huang, Qian Zhang, and Yang Wang. Tsfe-net: Two-stream feature extraction networks for active stereo matching. IEEE Access, 9:33954\u201333962, 2021.\\n\\nFeihu Zhang, Victor Prisacariu, Ruigang Yang, and Philip HS Torr. Ga-net: Guided aggregation net for end-to-end stereo matching. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 185\u2013194, 2019.\\n\\nFeihu Zhang, Xiaojuan Qi, Ruigang Yang, Victor Prisacariu, Benjamin Wah, and Philip Torr. Domain-invariant stereo matching networks. In Europe Conference on Computer Vision (ECCV), 2020.\\n\\nYinda Zhang, Sameh Khamis, Christoph Rhemann, Julien Valentin, Adarsh Kowdle, Vladimir Tankovich, Michael Schoenberg, Shahram Izadi, Thomas Funkhouser, and Sean Fanello. Activestereonet: End-to-end self-supervised learning for active stereo systems. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.\"}"}
