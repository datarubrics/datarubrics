{"id": "9CKx9SsSSc", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. REPresentations for a random nEarest Neighbor distance-based method (REPEN) [56]. A neural network-based model that leverages transformed low-dimensional embedding for random distance-based detectors. The hidden size of REPEN is set to 20, and the margin of triplet loss is set to 1000. REPEN is trained for 1000 epochs with early stopping. The batch size is set to 256, where the total number of steps (batches of samples) is set to 50. Adadelta [82] optimizer with 0.001 learning rate and $0.95^\\\\rho$ is applied to update network parameters.\\n\\n3. Deep Semi-supervised Anomaly Detection (DeepSAD) [69]. DeepSAD is a deep one-class method that improves its unsupervised version DeepSVDD [68] by penalizing the inverse distances of anomaly representation such that anomalies must be mapped further away from the hypersphere center. The hyperparameter $\\\\eta$ in the loss function is set to 1.0, where DeepSAD is trained for 50 epochs with 128 batch size. Adam optimizer with 0.001 learning rate and $10^{-6}$ weight decay is applied for updating the network parameters. DeepSAD additionally employs an autoencoder for calculating the initial center of the hypersphere, where the autoencoder is trained for 100 epochs with 128 batch size, and optimized by Adam optimizer with learning rate 0.001 and $10^{-6}$ weight decay.\\n\\n4. Deviation Networks (DevNet) [59]. A neural network-based model uses a prior Gaussian distribution to enforce a statistical deviation score of input instances. The margin hyperparameter $a$ in the deviation loss is set to 5. DevNet is trained for 50 epochs with 512 batch size, where the total number of steps is set to 20. RMSprop [66] optimizer with 0.001 learning rate and $0.95^\\\\rho$ is applied to update network parameters.\\n\\n5. Feature Encoding With Autoencoders for Weakly Supervised Anomaly Detection (FEAWAD) [90]. A neural network-based model that integrates the network architecture of DAGMM [91] with the deviation loss in DevNet [59]. FEAWAD is trained for 30 epochs with 512 batch size, where the total number of steps is set to 20. Adam optimizer with 0.0001 learning rate is applied to update network parameters.\\n\\n6. Residual Nets (ResNet) [24]. This method introduces a ResNet-like architecture [27] for tabular data. ResNet is trained for 100 epochs with 64 batch size. AdamW [51] optimizer with 0.001 learning rate is applied to update network parameters.\\n\\n7. Feature Tokenizer + Transformer (FTTransformer) [24]. FTTransformer is an effective adaptation of the Transformer architecture [75] for tabular data. FTTransformer is trained for 100 epochs with 64 batch size. AdamW optimizer with 0.0001 learning rate and $10^{-5}$ weight decay is applied to update network parameters.\\n\\nC.1 Design Choices Specification\\n\\nData Handling\\n\\nData augmentation aims to enhance the quality of training data by generating synthetic anomalies, mitigating the class-imbalance problem in AD tasks. For a dataset $D = \\\\{x_u^1, \\\\ldots, x_u^k, (x_{a^k+1}, y_{a^k+1}), \\\\ldots, (x_{a^k+m}, y_{a^k+m}), \\\\ldots, (x_{a^k+m+j}, y_{a^k+m+j}), \\\\ldots, (x_{a^k+m+j}, y_{a^k+m+j})\\\\}$ defined in the main paper, the output after a specific data augmentation method is $D' = x_u^1, \\\\ldots, x_u^k, (x_{a^k+1}, y_{a^k+1}), \\\\ldots, (x_{a^k+m}, y_{a^k+m}), \\\\ldots, (x_{a^k+m+j}, y_{a^k+m+j}), \\\\ldots, x_{a^k+m+j}, y_{a^k+m+j}$, where $m \\\\ll k$ and $m+j \\\\approx k$. This will exhibit a more balanced distribution between the normal and abnormal ones. Except for the default setting (maintaining the original class distribution of the dataset), we present four additional choices in this dimension: Oversampling, SMOTE [14], Mixup [39, 74, 84], and GAN [6].\\n\\nData preprocessing includes two commonly used methods, MinMaxScaler and Normalization. Given the input data $D \\\\in \\\\mathbb{R}^{n \\\\times d}$, for each feature vector $F \\\\in \\\\mathbb{R}^n$, MinMax scales $F$ to the range $[a, b]$: $F' = (F - F_{\\\\text{min}}) \\\\ast (b - a) / (F_{\\\\text{max}} - F_{\\\\text{min}}) + a$, where $F_{\\\\text{min}}$ and $F_{\\\\text{max}}$ represent the minimum and maximum of $F$. For each sample vector $S \\\\in \\\\mathbb{R}^d$, Normalization compute its $L_2$ norm and scale $S$ to $S' = S / \\\\|S\\\\|_2$ if $\\\\|S\\\\|_2 \\\\neq 0$.\\n\\nNetwork Construction\\n\\nNetwork Construction is often regarded as an important part in the CV or NLP domain, while it is often neglected in the AD problem. In fact, some design choices like AutoEncoder [90] and Transformer [24] could facilitate the detection performance of downstream tabular AD tasks. We comprehensively present the entire network construction process from various aspects, such as network architecture, hidden layers, activation functions, dropout rate, and parameter initialization, and provide numerous design choices.\\n\\nNetwork Training\\n\\nMany researchers regard the loss function as the key component of AD models. Binary cross entropy (BCE) loss is the conventionally accepted choice for classification tasks but is often considered inappropriate for AD. However, its value as a robust baseline may be overlooked by the AD community, as we have pointed out in the...\"}"}
{"id": "9CKx9SsSSc", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In ADGym, we explore the potential of diverse loss functions under different combinations of design choices. As an overview of these loss functions, Focal loss [45] increases the weighting of abnormal data in cross-entropy loss, enabling the model to focus more on the classification of anomalies. Minus loss [18] offers divergent update directions for anomaly scores amidst normal and abnormal data, and circumvents the issue of loss explosion with an upper bound. Resembling minus loss in approach, Inverse loss [69] is inherently self-limiting. Hinge loss [56, 59] leverages a pre-established hyperparameter, $M$, to maintain an anomaly score margin of a minimum of $M$. Deviation loss [59] mandates that the anomaly scores of normal data align with a standard Gaussian distribution, simultaneously ensuring a minimal $M$ margin for anomaly scores. Other dimensions here are consistent with common DL.\\n\\nC.2 Meta-features and Meta-predictors\\n\\nDetails and the selected list of meta-features\\n\\nData characterization measures which represent task information are fundamental for meta-learning [65]. We extract multiple categories of them as meta-features in the reference of [88], including:\\n\\n1. **Statistical features**\\n   - These depict the dataset from multiple statistical indicators, including but not limited to characteristics such as the minimum value, maximum value, mean, and variance, as well as combinations of these features. These characteristics are widely used in the previous AutoML studies, demonstrating their feasibility for application.\\n\\n2. **Landmarker features**\\n   - These are constructed by a series of AD models, including iForest, HBOS, LODA, and PCA (anomalies are reconstructed harder), which represent more AD task-specific information. For each of the four different unsupervised AD models trained by a given dataset, we extract their characteristics to engineer a series of meta-features (See Table C3 for complete landmaker features).\\n\\n   Overall, the experiment on the proposed two-stage method has proven that we can construct similar models for datasets with similar landmarker meta-features.\\n\\n| Name | Formula | Rationale | Variants |\\n|------|---------|-----------|----------|\\n| Nr instances | $n$ | Speed, Scalability | $p_n$, $\\\\log(n)$, $\\\\log(n^p)$ |\\n| Nr features | $p$ | Curse of dimensionality | $\\\\log(p)$, $\\\\%$ categorical |\\n| Sample mean | $\\\\mu$ | Concentration |\\n| Sample median | $\\\\tilde{X}$ | Concentration |\\n| Sample var | $\\\\sigma^2$ | Dispersion |\\n| Sample min | $\\\\min X$ | Data range |\\n| Sample max | $\\\\max X$ | Data range |\\n| Sample std | $\\\\sigma$ | Dispersion |\\n| $P_i$ | | Dispersion |\\n| Interquartile Range | $q_{75} - q_{25}$ | Dispersion |\\n| Normalized mean | $\\\\mu_{\\\\max X}$ | Data range |\\n| Normalized median | $\\\\tilde{X}_{\\\\max X}$ | Data range |\\n| Sample range | $\\\\max X - \\\\min X$ | Data range |\\n| Median absolute deviation | $\\\\text{median} (X - \\\\tilde{X})$ | Variability and dispersion |\\n| Average absolute deviation | $\\\\text{avg} (X - \\\\tilde{X})$ | Variability and dispersion |\\n| Quantile Coefficient | $(q_{75} - q_{25})/(q_{75} + q_{25})$ | Dispersion |\\n| Coefficient of variance | $\\\\text{Dispersion}$ |\\n| Outlier outside 1 & 99 % samples | outside 1% or 99% | Basic outlying patterns |\\n| Outlier 3 STD % samples | outside $3\\\\sigma$ | Basic outlying patterns |\\n| Normal test | If a sample differs from a normal dist. | Feature normality |\\n| $k$th moments | 5th to 10th moments | Feature skewness |\\n| Skewness | $\\\\text{Feature skewness}$ | Feature normality |\\n| Kurtosis | $\\\\mu_4/\\\\sigma_4^4$ | Feature normality |\\n| Correlation | $\\\\rho$ | Feature interdependence |\\n| Covariance | $\\\\text{Cov}$ | Feature interdependence |\\n| Sparsity | $\\\\#$ Unique values | Degree of discreteness |\\n| ANOV A p-value | | Feature redundancy |\\n| Coeff of variation | $\\\\text{Dispersion}$ |\\n| Norm. entropy | $H(X)/\\\\log_2 n$ | Feature informativeness |\\n| Landmarker (HBOS) | See Table C3 | Outlying patterns |\\n| Landmarker (LODA) | See Table C3 | Outlying patterns |\\n| Landmarker (PCA) | See Table C3 | Outlying patterns |\\n| Landmarker (iForest) | See Table C3 | Outlying patterns |\\n\\nDetailed and Complete List of Meta-predictors\\n\\nWe represent each design choice with LabelEncoder class from the scikit-learn library. These encoded data, together with the meta-features and the number of labeled anomalies $n_a$, are considered as the input data of the following three kinds of meta-predictors:\\n\\n1. **Two-stage meta-predictor**\\n   - This meta-predictor uses the meta-features described above. It firstly extracts meta-features $E_{\\\\text{meta}}$ from the given training datasets as an essential input, and then combines these meta-features with design choice embeddings and $n_a$, learning to predict the relative performance rank.\"}"}
{"id": "9CKx9SsSSc", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table C3: Landmarker features. An example of feature is the minimization of tree depth from the model iForest.\\n\\n| Model Perspective Variants | iForest | HBOS | LODA |\\n|---------------------------|---------|------|------|\\n| **Model Perspective**     |         |      |      |\\n| **Feature importance**    |         |      |      |\\n| Tree depth                | min, max, mean, std, skewness, kurtosis | | |\\n| Number of leaves          |         |      |      |\\n| Mean of base tree feature importance | | | |\\n| Max of base tree feature importance | | | |\\n| **Histogram**             |         |      |      |\\n| Mean of each histogram per feature | min, max, mean, std, skewness, kurtosis | | |\\n| Max of each histogram per feature | | | |\\n| **Random projection**     |         |      |      |\\n| Mean of each random projection per feature | min, max, mean, std, skewness, kurtosis | | |\\n| Max of each random projection per feature | | | |\\n\\n**End-to-end meta-predictor.** This meta-predictor learns meta-features implicitly through an end-to-end fashion that is inspired by Dataset2Vec[30].\\n\\n**Ensembled version** of the above two types of meta-predictors. It has been proved that ensemble models, such as XGBoost and CatBoost, perform well in AD tasks, which motivates us to explore whether the ensemble meta-predictor exhibits superior performance in experiments. From another perspective, this is also a validation of whether our proposed meta-predictor possesses high robustness. Specifically, for each AD task (dataset), we select the top-k (default k = 5 in our experiments) best-performance design pipelines predicted by meta-predictor, and utilize the average voting strategy to output the anomaly scores.\\n\\n**Details on training strategies of meta-predictors**\\n\\nWe experimented with different loss functions to better guide the meta-predictor in selecting the top-k design pipelines. During the training process of meta-predictors, weighted MSE loss assigns greater weights for those design choices with both higher real and predicted performance. The former aims to help the model focus on accurately predicting high-performing design pipelines, while the latter avoids mistakenly predicting poor-performing pipelines as part of the top-k. Pearson loss emphasizes the linear correlation between the predicted performances of different design choices and their actual ones. Ranknet loss [11], a commonly used pairwise algorithm in Learning to Rank, aims to model the probability that one design pipeline is superior to another in a probabilistic manner.\\n\\n**Additional Experimental Results**\\n\\n**D.1 Additional Results of Large Evaluations on AD Design Choices**\\n\\n**D.1.1 Using AUCPR as Metrics**\\n\\nFor AUCPR performances, we evaluate different design dimensions via the box plots, as shown in Figure D1, D2, and D3, respectively. Overall, we find that the conclusions drawn based on either AUCROC (shown in the main paper) or AUCPR are consistent.\"}"}
{"id": "9CKx9SsSSc", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) Loss function\\n(b) Optimizer\\n(c) Epochs\\n(d) Batch size\\n(e) Learning rate\\n(f) Weight decay\\n\\nTo prevent overfitting on limited labeled data and thus achieve better performance.\\n\\nloss function. Both Adam and RMSprop optimizers outperform classical SGD. Smaller epochs could work for AD tasks, where Tanh and LeakyReLU activation functions are generally better than ReLU.\\n\\n\\\\[ \\\\text{loss} = 5 \\\\]\"}"}
{"id": "9CKx9SsSSc", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"D.1.2 Using Relative Rank as Metrics\\n\\nConsidering the varying degrees of difficulty across different datasets, we also demonstrate the relative rankings of both AUCROC (as shown in figure D4~D6) and AUCPR (as shown in figure D7~D9) to measure the performance of different design choices. Specifically, we rank 1000 sampled design choices on each dataset. For a more intuitive comparison with our previous results, we calculate\\n\\\\[ 1 - \\\\text{normalized rank}(\\\\text{design choice}) \\\\]\\nwhere higher values indicate better performance. We found that such a metric further amplifies the differences between various design choices and generally aligns with the previous experimental findings. Notably, under the metric of relative ranking, we observe a significant decline in the performance of ResNet, which is inferior to AE. Drawing from our previous experiments, it becomes evident that ResNet's stability across various design pipelines did not manifest under this metric, which, however, is a crucial advantage in practical scenarios.\\n\\n---\\n\\n(a) Data augmentation\\n(b) Data preprocessing\\n(c) Hidden layers\\n(d) Activation function\\n(e) Network initialization\\n\\nFigure D4: AUCROC relative rank performance of data handling designs.\\nFigure D5: AUCROC relative rank performance of network construction designs.\"}"}
{"id": "9CKx9SsSSc", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Optimizer | Data preprocessing | Weight decay | Network initialization | Activation function | AUCROC relative rank | AUCPR relative rank \\\\\\\\\\n--- | --- | --- | --- | --- | --- | --- \\\\\\\\\\nNone | minmax | MLP | SGD | tanh | 0.0 | 0.0 \\\\\\\\\\nNone | minmax | MLP | SGD | ReLU | 0.0 | 0.0 \\\\\\\\\\nOver | SMOTE | Mixup | Adam | xavier | 0.0 | 0.0 \\\\\\\\\\nNone | normalize | MLP | RMSprop | Adam | 0.0 | 0.0\\n\\nFigure D6: AUCROC relative rank performance of network training designs. Hinge loss is generally better than other loss functions.\\n\\nFigure D7: AUCPR relative rank performance of data handling designs.\\n\\nFigure D8: AUCPR relative rank performance of network construction designs.\\n\\nOver-Stratification\"}"}
{"id": "9CKx9SsSSc", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D.1.3 Unsupervised Network Pre-training\\n\\nIn addition to the existing network initialization methods introduced in Table 1, we also investigate whether deep learning based AD models can benefit from unsupervised pre-training tasks. For tabular AD problems, we follow [68, 69] to first train an AutoEncoder that has an encoder with the same architecture as the corresponding network, e.g., constructing encoder-decoder transformer block in FTTransformer, and then utilize the reconstruction loss (mean squared error between the input data and its reconstructed ones) to perform model training. After that, we initialize corresponding models with the converged parameters of the encoder and fine-tune them on the downstream tasks, which contain only limited labeled data.\\n\\nPre-training has been widely used for NLP [15] and CV [52] tasks and verified to significantly enhance the performance of downstream tasks. However, for tabular AD problems, we did not observe a significant advantage of pre-training over other network initialization methods, as shown in Figure D10. This may be due to the reason that compared to the NLP and CV tasks that contain rich textual semantics and visual patterns, the inherent structure of tabular data is more rigid and constrained, resulting in the model struggling to learn general features without label guidance.\"}"}
{"id": "9CKx9SsSSc", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"representative design dimensions: data augmentation, network architecture, and loss function. As shown in figure D11, data augmentation techniques tend to have counterproductive effects on single-anomaly datasets in the vast majority of cases. Figure D12 shows the performance over different network architectures, where ResNet consistently excels over other architectures when dealing with dependency anomalies. However, its performance diminishes with datasets that have a global anomalies context and fewer anomalies. Compared to the two dimensions mentioned above, the variance between different loss functions is significantly greater, making the selection of an appropriate loss function crucial, as shown in figure D13. The deviation loss consistently demonstrates competitive results across various anomaly types and numbers of anomalies, and the performance of hinge loss also improves rapidly as the number of anomalies increases.\"}"}
{"id": "9CKx9SsSSc", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D.2.2 Additional Results of ADGym over datasets with global anomalies\\n\\nIn this section, we investigate whether meta-predictor can select the best design choices based on dataset meta-features. We chose the meta-predictor trained with two shallow model methods and focused on datasets with global anomalies. These datasets have previously been shown to favor competitive design choices, such as the AE architecture and deviation loss function. Figures D14 and D15 respectively display our results concerning the network architecture and loss function design dimensions, both of which have been shown to significantly impact model performance in earlier experiments. We represent the results using scatter plots, where all results are averaged over three seeds and the number of anomalies; the closer to the top-right corner indicates better performance of the design choice. Empirical evidence shows that the meta-predictor can effectively select the best design choice (AE and deviation loss) in both dimensions, with the prediction results for network architecture almost perfectly aligning with the actual results.\\n\\nFigure D14: performance of meta-predictors on network architecture over global anomalies. Figures (a) and (b) show meta-predictors output trained by CatBoost and XGBoost, and (c) shows result of previous benchmark experiments on global anomalies (as ground truth baseline). For a more intuitive comparison, we subtract the output from 1 to ensure that the best design choices are positioned closer to the top-right corner.\\n\\nFigure D15: performance of meta-predictors on loss function over global anomalies.\\n\\nD.3 Additional Results for Different Domains of Datasets\\n\\nIn order to investigate the performance of the AD design choices across different domains, we categorize 29 datasets into 14 distinct domains and aggregate the experimental results based on the domain of each dataset. In Table D4 and D5, each data point represents the average performance rank of the design choices (as indicated by the rows) on the datasets within a specific domain (as indicated by the columns). We observe that no design choices are universally superior or inferior. A case in point is the GAN-based data augmentation method, which generally underperforms on most datasets but shows a clear advantage in the financial domain. A similar situation also occurs with the performance of FTTransformer on datasets in the Sociology domain. Meanwhile, some designs that have performed well in past experiments, such as the deviation loss function, rank last on datasets in the financial domain. These results underscore the necessity of automated selection of AD designs and align with our previous conclusions.\"}"}
{"id": "9CKx9SsSSc", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table D4: AUCROC performance rank of design choices across different domains. First column shows different design dimensions, where aug, na, af, ls are abbreviations for data augmentation, network architecture, activate function, and loss function, respectively.\\n\\n| Design Method  | Astronautics | Biology | Botany | Chemistry | Document | Finance | Healthcare | Image | Linguistics | Network | Physical | Physics | Sociology | Web |\\n|----------------|--------------|---------|--------|-----------|----------|---------|------------|-------|-------------|---------|----------|---------|-----------|-----|\\n| aug GAN        | 5            | 5       | 5      | 5         | 5        | 2       | 5          | 5      | 5           | 5       | 5        | 4       | 5         |     |\\n| aug Mixup      | 2            | 2       | 1      | 3         | 4        | 4       | 2          | 1      | 4           | 3       | 4        | 2       | 2         |     |\\n| aug Origin     | 4            | 3       | 3      | 4         | 3        | 5       | 3          | 4      | 2           | 3       | 2        | 5       | 3         |     |\\n| aug Oversampling | 3          | 4       | 2      | 2         | 2        | 3       | 2          | 3      | 3           | 2       | 4        | 3       | 3         |     |\\n| aug SMOTE      | 1            | 1       | 4      | 1         | 1        | 1       | 1          | 4      | 1           | 1       | 1        | 1       | 1         |     |\\n| na AE          | 2            | 3       | 2      | 2         | 1        | 2       | 2          | 3      | 4           | 2       | 3        | 4       | 3         |     |\\n| na FTT         | 4            | 4       | 4      | 4         | 4        | 4       | 4          | 4      | 1           | 4       | 4        | 1       | 4         |     |\\n| na MLP         | 1            | 1       | 3      | 1         | 2        | 1       | 1          | 3      | 1           | 1       | 1        | 1       | 2         |     |\\n| na ResNet      | 3            | 2       | 1      | 3         | 3        | 3       | 3          | 2      | 3           | 3       | 3        | 3       | 4         |     |\\n| af LeakyReLU   | 1            | 2       | 1      | 1         | 2        | 1       | 2          | 2      | 3           | 1       | 2        | 1       | 3         |     |\\n| af ReLU        | 3            | 3       | 2      | 3         | 3        | 3       | 3          | 3      | 3           | 3       | 3        | 3       | 3         |     |\\n| af Tanh        | 2            | 1       | 3      | 2         | 1        | 2       | 1          | 1      | 1           | 2       | 1        | 2       | 1         |     |\\n| ls bce         | 3            | 4       | 3      | 1         | 2        | 4       | 4          | 1      | 4           | 2       | 1        | 2       | 1         |     |\\n| ls deviation   | 4            | 3       | 2      | 2         | 4        | 6       | 3          | 3      | 2           | 3       | 3        | 4       | 4         |     |\\n| ls focal       | 5            | 6       | 5      | 4         | 6        | 2       | 5          | 5      | 6           | 5       | 6        | 5       | 5         |     |\\n| ls hinge       | 2            | 1       | 1      | 5         | 3        | 3       | 1          | 2      | 4           | 1       | 2        | 4       | 1         |     |\\n| ls inverse     | 6            | 5       | 6      | 6         | 5        | 5       | 6          | 6      | 5           | 6       | 5        | 6       | 6         |     |\\n| ls minus       | 1            | 2       | 4      | 3         | 1        | 1       | 3          | 4      | 4           | 4       | 4        | 4       | 4         |     |\"}"}
{"id": "9CKx9SsSSc", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table D8: AUCPR performance of meta-predictors trained on other loss functions. Different loss functions do not yield significant performance improvement for the meta-predictor.\\n\\n| Weighted MSE Pearson Ranknet | DL-single | DL-ensemble | DL-single | DL-ensemble | DL-single | DL-ensemble | DL-single | DL-ensemble | DL-single | DL-ensemble | DL-single | DL-ensemble |\\n|-----------------------------|-----------|-------------|-----------|-------------|-----------|-------------|-----------|-------------|-----------|-------------|-----------|-------------|\\n| 2-stage end2end             | 0.495     | 0.500       | 0.502     | 0.501       | 0.524     | 0.525       | 0.527     | 0.533       | 0.525     | 0.527       | 0.529     | 0.553       |\\n| 10                           | 0.558     | 0.533       | 0.571     | 0.543       | 0.585     | 0.581       | 0.603     | 0.599       | 0.583     | 0.581       | 0.592     | 0.599       |\\n| 20                           | 0.592     | 0.573       | 0.614     | 0.560       | 0.631     | 0.602       | 0.640     | 0.609       | 0.635     | 0.624       | 0.642     | 0.623       |\\n\\nTable D9: AUCPR performance of meta-predictors trained on large scale design space. Larger design space provides potentially more effective design choices for newcoming datasets.\\n\\n| RS | SS | GT | DL-single | DL-ensemble | ML-single | ML-ensemble |\\n|----|----|----|-----------|-------------|-----------|-------------|\\n| 2-stage end2end | XGB | CatB | XGB | CatB |\\n\\n| 5  | 0.449 | 0.373 | 0.716 | 0.539 | 0.537 | 0.547 |\\n|----|-------|-------|-------|-------|-------|-------|\\n| 10 | 0.491 | 0.455 | 0.742 | 0.590 | 0.583 | 0.613 |\\n|----|-------|-------|-------|-------|-------|-------|\\n| 20 | 0.528 | 0.557 | 0.762 | 0.641 | 0.617 | 0.656 |\\n\\nTable D10: AUCPR performance of meta-predictors trained on refined design space. No significant improvement of meta-predictor is observed since not only potentially better design choices are discarded, but also results in a reduction of training data in meta-predictors.\\n\\n| RS | SS | GT | DL-single | DL-ensemble | ML-single | ML-ensemble |\\n|----|----|----|-----------|-------------|-----------|-------------|\\n| 2-stage end2end | XGB | CatB | XGB | CatB |\\n\\n| 5  | 0.467 | 0.370 | 0.706 | 0.528 | 0.509 | 0.531 |\\n|----|-------|-------|-------|-------|-------|-------|\\n| 10 | 0.532 | 0.458 | 0.737 | 0.573 | 0.550 | 0.593 |\\n|----|-------|-------|-------|-------|-------|-------|\\n| 20 | 0.564 | 0.540 | 0.759 | 0.614 | 0.580 | 0.656 |\\n\\nD.4.2 Using Relative Rank as Metrics\\n\\nIn this subsection, we present results using the relative rankings based on AUCROC and AUCPR as metrics to mitigate the impact of variations in dataset difficulty. Specifically, we compute the relative rankings for each datasets across all models, including design pipelines generated by various versions of ADGym meta-predictor. We then subtract these normalized results from 1, ensuring that larger values indicate better performance. Table D11 and Table D12 depict the relative rankings of various SOTA models for AUCROC and AUCPR, respectively, while Table D13 and Table D14 compare the performance of different meta-predictor versions. It's important to note that performance cannot be compared across tables using this relative ranking metric, so we include the average result of the SOTA models as a baseline for ranking purposes. Our findings are consistent with previous conclusions, indicating that the performance of design pipelines from the meta predictor surpasses that of the existing SOTA models.\\n\\nTable D11: Baseline of SOTA AD methods with AUCROC relative rank.\\n\\n| n  |\\n|----|\\n| 5  | 0.383 |\\n| 10 | 0.331 |\\n| 20 | 0.301 |\\n\\nTable D12: Baseline of SOTA AD methods with AUCPR relative rank.\\n\\n| n  |\\n|----|\\n| 5  | 0.370 |\\n| 10 | 0.310 |\\n| 20 | 0.271 |\"}"}
{"id": "9CKx9SsSSc", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table D13: Relative Rank AUCROC of auto-selected pipelines by ADGym.\\n\\n| n  | SOTA RS | RS | SS | GT | DL-single | DL-ensemble | ML-single | ML-ensemble |\\n|----|---------|----|----|----|-----------|-------------|-----------|-------------|\\n| 5  | 0.289   | 0.275 | 0.231 | 0.986 | 0.598     | 0.551       | 0.609     | 0.573        | 0.582 | 0.562 | 0.605 | 0.633 |\\n| 10 | 0.243   | 0.216 | 0.202 | 0.982 | 0.594     | 0.530       | 0.635     | 0.588        | 0.610 | 0.588 | 0.659 |\\n| 20 | 0.225   | 0.215 | 0.244 | 0.978 | 0.601     | 0.530       | 0.621     | 0.581        | 0.586 | 0.587 | 0.640 | 0.685 |\\n\\nTable D14: Relative Rank AUCPR of auto-selected pipelines by ADGym.\\n\\n| n  | SOTA RS | RS | SS | GT | DL-single | DL-ensemble | ML-single | ML-ensemble |\\n|----|---------|----|----|----|-----------|-------------|-----------|-------------|\\n| 5  | 0.353   | 0.284 | 0.276 | 0.984 | 0.572     | 0.545       | 0.600     | 0.552        | 0.557 | 0.566 | 0.608 | 0.596 |\\n| 10 | 0.275   | 0.297 | 0.270 | 0.982 | 0.563     | 0.527       | 0.599     | 0.569        | 0.547 | 0.569 | 0.628 | 0.668 |\\n| 20 | 0.260   | 0.319 | 0.313 | 0.973 | 0.549     | 0.514       | 0.614     | 0.539        | 0.536 | 0.600 | 0.617 | 0.659 |\\n\\nD.5 Additional Results of Evaluations under Real-world Issues\\n\\nIn real-world applications, AD tasks frequently encounter complex scenarios characterized by noisy and corrupted input data. In this section, we assess the performance and robustness of various AD design choices under three prevalent but imperfect real-world conditions. All experiments are conducted within a weakly-supervised framework, wherein only a sparse set of ten anomaly samples are labeled.\\n\\n- **Duplicated Anomalies.** Abnormal data are likely to recur multiple times due to various factors. These repeated anomalies often impact many density-based AD algorithms. To simulate this scenario, we duplicated each abnormal sample three times in all datasets for our experiments.\\n\\n- **Irrelevant Features.** In real-world tabular data, there often exist irrelevant features (columns). Even when feature selection techniques such as Random Forests or Logistic Regression are employed, their efficacy in AD applications is not guaranteed. Therefore, assessing the robustness of AD design choices in the presence of such irrelevant features is crucial. To examine this, we add an additional 30% irrelevant features to all datasets by generating uniform noise.\\n\\n- **Annotation Errors.** Annotation errors are among the most common forms of noise encountered in real-world scenarios, especially in AD tasks where the data is extremely imbalanced. To assess the robustness of the design choices against label noise, we inverted 10% of the labels in each dataset, limiting this modification to the training sets.\\n\\nWe present the experimental results in Figure D16 and Figure D17 employing both AUCROC and AUCPR metrics. It is evident from the figures that the performance of various design choices is significantly impacted by Irrelevant Features and Annotation Errors, manifesting as a noticeable decline in both metrics. The conclusion variances among different design choices are small under the influence of Irrelevant Features, trending towards convergence. Surprisingly, we find that when Duplicated Anomalies are present, the performance of all design choices tends to improve, and the variance also increases, possibly mitigating the issue of data imbalance. What is particularly encouraging in our experiments is that the ResNet architecture not only shows a substantial and stable advantage in scenarios with Duplicated Anomalies but also exhibits less impact in the other two settings.\\n\\nWe also conduct experiments with an Irrelevant Features noisy ratio of 0.1 and an Annotation Errors ratio of 0.05, where only a slight mitigation in the impact on design choices, and the conclusions remained consistent with our current findings.\"}"}
{"id": "9CKx9SsSSc", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(c) Activation function\\n\\n(e) Optimizer\\n\\n(b) Network architecture\\n\\n(a) Data augmentation\"}"}
{"id": "9CKx9SsSSc", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ADGym: Design Choices for Deep Anomaly Detection\\n\\nMinqi Jiang,\u2217 Chaochuan Hou,\u2217 Ao Zheng,\u2217 Songqiao Han,\u2020 Hailiang Huang,2 Qingsong Wen, Xiyang Hu,\u2020 Yue Zhao\\n\\n1 AI Lab, Shanghai University of Finance and Economics\\n2 MoE Key Laboratory of Interdisciplinary Research of Computation and Economics\\n3 DAMO Academy, Alibaba Group\\n4 Carnegie Mellon University\\n\\njiangmq95@163.com, houchaochuan@foxmail.com, zheng-ao@outlook.com, {han.songqiao,hlhuang}@shufe.edu.cn, qingsongedu@gmail.com, {xiyanghu,zhaoy}@cmu.edu\\n\\nAbstract\\n\\nDeep learning (DL) techniques have recently found success in anomaly detection (AD) across various fields such as finance, medical services, and cloud computing. However, most of the current research tends to view deep AD algorithms as a whole, without dissecting the contributions of individual design choices like loss functions and network architectures. This view tends to diminish the value of preliminary steps like data preprocessing, as more attention is given to newly designed loss functions, network architectures, and learning paradigms. In this paper, we aim to bridge this gap by asking two key questions:\\n\\n(i) Which design choices in deep AD methods are crucial for detecting anomalies?\\n(ii) How can we automatically select the optimal design choices for a given AD dataset, instead of relying on generic, pre-existing solutions?\\n\\nTo address these questions, we introduce ADGym, a platform specifically crafted for comprehensive evaluation and automatic selection of AD design elements in deep methods. Our extensive experiments reveal that relying solely on existing leading methods is not sufficient. In contrast, models developed using ADGym significantly surpass current state-of-the-art techniques.\\n\\n1 Introduction\\n\\nAnomaly detection (AD) aims to identify data objects that significantly deviate from the majority of samples, with numerous successful applications in intrusion detection [36, 44], fault detection [22, 85], medical diagnosis [13, 38], fraud detection [2, 8, 9], social media analysis [81, 86], etc. Recently, deep neural networks have become the primary techniques in AD due to their powerful representation learning capacity [57]. Among all, weakly-supervised AD (WSAD) methods [56, 58, 59, 69, 83, 90], which leverage imperfect ground truth anomaly labels (e.g., those that are incomplete, inaccurate, or inexact) in deep neural networks for AD, have gained attention in this new frontier [29]. A comprehensive study by [26] showcases WSAD's superiority over unsupervised AD techniques, especially for real-world conditions where the ground truth labels are never complete and accurate.\\n\\nThus, our work introduces ADGym, a platform for understanding and designing deep WSAD methods, with the potential to be extended to unsupervised and supervised deep AD methods.\\n\\nGoal I: Understanding Design Choices of Deep AD.\\n\\nMany WSAD methods attribute their improvements to novel network architectures or loss functions, based on the authors' understanding of anomalies. However, these choices represent only a fraction of the design considerations, as there are many other factors to consider, such as data preprocessing and model training techniques.\\n\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\"}"}
{"id": "9CKx9SsSSc", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: ADGym supports a comprehensive list of design choices for deep AD methods.\\n\\n| Pipeline Design Dimensions | Design Choices |\\n|----------------------------|----------------|\\n| **Data Handling**          |                |\\n| Data Augmentation          | Oversampling, SMOTE, Mixup, GAN |\\n| Data Preprocessing         | MinMax, Normalization |\\n| **Network Construction**   |                |\\n| Network Architecture       | MLP, AutoEncoder, ResNet, FTTransformer |\\n| Hidden Layers              | [2], [100, 20], [100, 50, 20] |\\n| Activation                 | Tanh, ReLU, LeakyReLU |\\n| Dropout                    | [0.0, 0.1, 0.3] |\\n| Initialization             | default, Xavier (normal), Kaiming (normal) |\\n| **Network Training**       |                |\\n| Loss Function              | BCE, Focal, Minus, Inverse, Hinge, Deviation, Ordinal |\\n| Optimizer                  | SGD, Adam, RMSprop |\\n| Epochs                     | [20, 50, 100] |\\n| Batch Size                 | [16, 64, 256] |\\n| Learning Rate              | [1e-2, 1e-3] |\\n| Weight Decay               | [1e-2, 1e-4] |\\n\\nNote: Bolded design dimensions are those of greater impact on the AD task (discussed in detail in \u00a74).\\n\\n1. We list specific design choices for deep AD models and group them as design dimensions. Our experiments reveal that some of these dimensions (in bold) significantly impact the performance.\\n\\nMany questions remain unanswered in current AD studies, such as the interaction between components within an AD method, the relative importance of each component in performance, and the potential of new data-centric techniques to improve AD model performance. Thus, in the first part of the study (\u00a73.2), we have evaluated various design combinations on large benchmark datasets. Interestingly, the optimal model, comprising different design choices by combination, varies by datasets and notably outperforms existing state-of-the-art (SOTA) AD models. This raises a key question: how can we automatically design AD models for datasets other than using existing models?\\n\\n**Goal II: Constructing AD Algorithms Automatically via ADGym.**\\n\\nIndeed, our prior research also shows there is no one-size-fits-all AD model for every dataset\u2014we must select AD models based on the underlying dataset. Our prior work focuses on model selection from a pre-defined list, mainly for non-neural-network AD methods with limited design choices. In today's deep learning era, a pre-defined list is not sufficient, given the large number of design choices in Table 1 and \u00a73.2. There could be infinite deep AD models with different design combinations. In the second part of this study, we develop a design-choice-level selection approach that utilizes meta-learning, called ADGym (see \u00a73.3). In a nutshell, our approach leverages the supervision signals from historical datasets to guide selecting the best design choices for constructing AD models tailored to a new dataset. We aim to enhance the existing AD model selection process, ensuring the best combination of design choices for a given application or dataset. What sets ADGym apart from prior works is our focus on granular design choice selection tailored for deep AD, which has a much larger space than a pre-defined model list for existing methods only. See Fig. 1 for a comparison.\\n\\n**What Do We Learn from the Experiments (\u00a74)?**\\n\\nADGym helps us better understand and design deep AD methods, with key observations from our experiments: (1) No single design choice consistently outperforms others across all datasets, justifying the need for an automated approach to collectively select the most effective design choices. (2) Employing a meta-predictor to automate design choice selection yields notable improvements over static SOTA methods. (3) We can make meta-predictors better by using ensemble techniques or increasing the range of design choices.\\n\\nTo sum up, our work makes the following technical contributions:\\n\\n1. **Understanding AD Design Choices via Benchmarking.** We present the first benchmark that breaks down and compares diverse deep AD design choices across 29 real-world datasets and 4 groups of synthetic datasets, which leads to a few interesting observations.\\n\\n2. **Design Choice Automation.** We introduce ADGym, the first automated framework for selecting design choices for weakly-supervised AD, which significantly outperforms SOTA AD methods.\\n\\n3. **Accessibility and Extensibility.** We have made ADGym publicly available so that practitioners can build better-than-SOTA AD methods. It is also easy to include new design choices.\"}"}
{"id": "9CKx9SsSSc", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2 Related Work\\n\\n2.1 Weakly-supervised Anomaly Detection (WSAD)\\n\\nDue to the cost and difficulties in data annotation, previous studies [42, 43, 47, 68, 91] mainly focus on developing unsupervised AD methods with different assumptions of data distribution [1], while they are shown to have no statistically significant difference from each other in a recent benchmark [26]. In practice, however, there could exist at least a handful of labeled instances that are either identified by domain experts or the bad cases occurred during the deployment of AD methods. Therefore, recent studies [56, 58, 59, 69, 90] propose weakly-supervised AD methods to effectively leverage the valuable knowledge in labeled data and thus facilitate anomaly identification. Nevertheless, existing WSAD methods are often used as is, without detailed evaluation of each design choice like network architectures and loss functions. A more granular analysis of specific components of WSAD methods could be helpful for a deeper understanding of the AD methods.\\n\\n2.2 Benchmarks for Anomaly Detection\\n\\nAnomaly detection benchmarks mainly perform large comparisons and evaluations on the detection performance of different AD methods under unified and controlled conditions. While previous studies [12, 17, 23, 71, 72] focus on benchmarking classical machine learning AD methods, there has been an increasing trend towards benchmarking deep learning AD methods as well. [67] reviews both classic and deep AD models and points to the connections between classic shallow algorithms and deep AD models. Our previous work [26] performs the largest-scale AD benchmark so far, evaluating 30 shallow and deep AD algorithms across 57 benchmark datasets under different levels of supervision. Besides, some benchmark works focus on AD tasks with different data modalities, including time-series [46, 60, 35], graph [49, 48], CV [78] and NLP [63]. All of these benchmarks focus on discovering which AD algorithms (as a whole) are more effective. Differently, we focus on understanding the effectiveness of design choices in deep AD methods, complementing these works.\\n\\n2.3 Automatic Model Selection for AD\\n\\n2.3.1 Unsupervised Anomaly Model Selection\\n\\nDeveloping automatic model selection schemes for unsupervised AD algorithms faces a major challenge in lacking evaluation criteria. A recent survey [53] provides a comprehensive survey of using internal model evaluation strategies that solely rely on input features and outlier scores for model selection. Their extensive experiment shows that none of the existing and adapted evaluation strategies would be practically useful for unsupervised AD model selection.\\n\\nAnother existing work stream leverages meta-learning, based on the similarity between the new task and historical datasets where model performance is available. Some notable work [88, 89, 87] assume that model performance can be generalized across similar datasets, thus selecting the model for a new task based on similar historical tasks. Specifically, [89, 87] introduce the idea of building a meta-predictor (which is a regressor) to predict the model performance on a new dataset by training it on historical model performances. In ADGym, we take the idea of meta-learning to train a specialized meta-predictor for deep AD methods, where existing works [88, 89] only focus on non-neural-network-based AD models with a relatively small model pool, where we extend to more complex deep AD design spaces. See \u00a73.3 for details.\\n\\n2.3.2 Supervised Anomaly Model Selection, HP Optimization, and Neural Arch. Search\\n\\nSupervised AD model selection and HP optimization (HPO) aim to train a set of models, evaluate their performance using ground truth labels, and ultimately select the best model. However, as previously mentioned, ground truth labels in AD are scarce, which prevents supervised selection from being widely explored. PyODDS [41] optimizes an AD pipeline, while TODS [34] creates a selection system for time series data. Both, however, focus primarily on shallow models, excluding most deep models. Recently, AutoOD [40] conducts neural architecture search (NAS) for deep AD using labels. However, it only focuses on AutoEncoder models over image data. AutoPatch [31] is another two NAS works for AD, but they still do not consider networks beyond CNNs.\"}"}
{"id": "9CKx9SsSSc", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: The framework of ADGym. Existing AD model selection focuses on selecting the best model from a small, fixed pool. Our proposed method is more flexible to build a good model by choosing different parts, including data handling, network construction, and network training. Here, we highlight the difference between ADGym and the above works. First, ADGym aims to select from a large, comprehensive design pool, as opposed to existing model selection, which is often restricted to a small pre-defined model pool; meanwhile, ADGym also brings more granularity than general HPO and NAS, which only focus on a small set of HPs (given neural architectures can also be considered as HPs). Second, ADGym supports more scenarios other than focusing on a single family of AD methods, e.g., autoencoders, or just network architectures. Third, different from the SOTA method AutoOD, ADGym is a zero-shot algorithm that does not require any model building for a new dataset, thereby significantly reducing the online time; see \u00a73.3 for details.\\n\\n3 ADGym: Benchmarking and Automating Design Choices in Deep AD\\n\\n3.1 Problem Definition\\n\\nIn this paper, we focus on the common weakly supervised AD scenario, where given a training dataset \\\\(D = \\\\{x_{u1}, \\\\ldots, x_{uk}, x_{ak+1}, y_{ak+1}, \\\\ldots, x_{ak+m}, y_{ak+m}\\\\}\\\\) contains both unlabeled samples \\\\(D_u = \\\\{x_{ui}\\\\}_{i=1}^k\\\\) and a handful of labeled anomalies \\\\(D_a = \\\\{x_{aj}, y_{aj}\\\\}_{j=1}^m\\\\), where \\\\(x \\\\in \\\\mathbb{R}^d\\\\) represents the input feature and \\\\(y_{aj}\\\\) is the label of identified anomalies. Usually, we have \\\\(m \\\\ll k\\\\), since only limited prior knowledge of anomalies is available. Such data assumption is more practical for AD problems, and has been studied in recent deep AD methods [56, 58, 59, 69, 90]. The goal of a weakly-supervised AD model \\\\(M\\\\) is to assign higher anomaly scores to anomalies.\\n\\n3.2 Goal I: Understanding Design Choices of Deep AD\\n\\nThe first primary goal of this study is to investigate the large design space of deep AD solutions, which should cover as many design choices as possible, e.g., different data augmentation methods, network architectures, etc. Following the taxonomy of the previous study [80] and practical experiences in industrial applications [2, 22, 78], we decouple the standard process of deep AD methods, starting from the input data and ending with model training. The Pipeline includes: data handling \\\\(\\\\rightarrow\\\\) network construction \\\\(\\\\rightarrow\\\\) network training, as shown in Table 1. For each step of the pipeline, we further categorize it as different Design Dimensions, where diverse Design Choices can be specified to instantiate an AD method. More details are shown in Appx. C.1\\n\\nWith the comprehensive design space above, we not only evaluate the possible designs of weakly-supervised AD methods, but also investigate several interesting design dimensions often overlooked in previous AD research. For example, previous AD studies often perform model training on the raw input data (instead of augmented data for mitigating class imbalance problems), and simple network architectures like multi-layer perceptron (MLP) (instead of more recent network architectures like ResNet [27] and Transformer [75]), where the other cutting-edge techniques have already been widely used in other domains like NLP and CV. To sum up, we pair the combination of comprehensive design choices in Table 1 with benchmark AD datasets to unlock new insights. See results in \u00a74.2.\\n\\n3.3 Goal II: Constructing AD Algorithms Automatically via ADGym\\n\\nFrom Model Selection to Pipeline Selection. With the large AD design space illustrated in the last section, we investigate how to construct effective AD methods given the downstream applications...\"}"}
{"id": "9CKx9SsSSc", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"automatically. Given a pre-defined model set $M = \\\\{M_1, ..., M_m\\\\}$ that includes $m$ combinations of applicable design choices, model selection picks a model $M \\\\in M$ to train on a dataset $D_{test}$ and output the anomaly score $O := M(X_{test})$ to achieve the highest detection performance. In our case, we construct a pipeline $P$ from our design space to generate each model $M$. As shown in Fig. 1, our design (right) brings more flexibility by choosing from details than existing model selection works that only choose from a fixed, small set of AD models.\\n\\nMeta-learning for Pipeline Selection\\n\\nMeta learning has been recently used in unsupervised AD model selection [88, 89], where the core idea is to transfer knowledge from model performance information on historical datasets to the selection on a new dataset. Intuitively, if two datasets are similar, their best models should also resemble. Under the meta-learning framework, we assume there are $n$ historical AD datasets (i.e., detection tasks) $D_{train} = \\\\{D_1, \\\\ldots, D_n\\\\}$; each meta-train dataset is accompanied with ground truth labels for performance evaluation. To leverage prior knowledge for pipeline selection on a new dataset, we first conduct experiments on $n$ historical datasets to evaluate and collect the performance of $m$ possible designed pipelines (as the combination of all applicable design choices), by two widely used metrics: AUCROC (Area Under Receiver Operating Characteristic Curve) and AUCPR (Area Under Precision-Recall Curve).\\n\\nFor each metric, we acquire the corresponding performance matrix $P \\\\in \\\\mathbb{R}^{n \\\\times m}$, where $P_{i,j}$ corresponds to the $j$-th constructed AD model's performance on the $i$-th historical dataset. Meanwhile, historical datasets vary in task difficulty, resulting in variations in the numerical range of the constructed AD models' performance. Therefore, we convert the value of the performance into their relative/normalized ranking, where $P_{i,j} = \\\\text{rank}(P_{i,j})/m \\\\in [0, 1]$. Smaller ranking values indicate better performance on the corresponding dataset.\\n\\nTo predict the performance of a given pipeline on a new dataset, we propose to train a meta-predictor as a regression problem: the input of meta-predictor is $\\\\{E_{meta}^i, E_{comp}^j\\\\}$, corresponding to the meta-feature [88] (i.e., the unified representations of a dataset) of $i$-th dataset and the embedding of $j$-th AD component (i.e., the representation of a pipeline/components). We defer the specific details into Appx. C.2. Given the meta-predictor $f(\\\\cdot)$, we train it to map dataset and pipeline characteristics to their corresponding performance ranking across all historical datasets, as shown in Eq. (1). Refer to our open-sourced code for details of embeddings and the choice of regression models.\\n\\n$$f: E_{meta}^i, E_{comp}^j \\\\rightarrow P_{i,j}, i \\\\in \\\\{1, \\\\ldots, n\\\\}, j \\\\in \\\\{1, \\\\ldots, m\\\\}$$\\n\\nFor a new coming dataset (i.e., test dataset $X_{test}$), we acquire the predicted relative ranking of different AD components using the trained $f(\\\\cdot)$, and select top-1 ($k$) to construct AD model(s). Note this procedure is zero-shot without needing any neural network training on $X_{test}$ but only extracting meta-features and pipeline embeddings. We show the effectiveness of the meta-predictor in \u00a74.3.\\n\\n4 Experiments\\n\\n4.1 Experiment Settings\\n\\nAD Datasets and Baselines. In ADGym, we gather datasets from the previous AD studies and benchmarks [12, 19, 26, 57, 64] for evaluating existing AD models and different AD designs. Datasets with sample sizes smaller than 1000, as well as those with problematic model results, are removed, resulting in a total of 29 remaining datasets. These datasets cover many diverse application domains, such as healthcare, audio and language processing, image processing, finance, etc. Furthermore, based on our previous work[26], we categorize anomalies into four types, local, global, cluster and dependency anomalies, and generate synthetic datasets for each individual anomaly type. For each dataset, 70% data is split as the training set and the remaining 30% is used as the test set. We use stratified sampling to keep the anomaly ratio consistent. For each experiment, we repeat 3 times and report the average. Further details of datasets and baselines are presented in Appx. A and B.\\n\\nMeta-predictor in ADGym. We introduce two types of meta-predictors in ADGym, namely DL-based and ML-based. The DL-based meta-predictor is instantiated as a two-layer MLP and trained for 100 epochs with early stopping. The training process utilizes the Adam optimizer [32] with a\"}"}
{"id": "9CKx9SsSSc", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning rate of 0.001 and batch size of 512. For the ML-based meta-predictor, we instantiate the meta-predictor with XGBoost and CatBoost and their default hyperparameter settings. Considering the expensive computational cost of training meta-predictors on the entire design space, we randomly sample 1,000 design choices for each experiment illustrated below. See details in Appx. C.2.\\n\\nEvaluation Metrics\\n\\nWe perform evaluations with two widely used metrics: AUCROC (Area Under Receiver Operating Characteristic Curve), AUCPR (Area Under Precision-Recall Curve) value, and the relative rankings corresponding to these two metrics.\\n\\n4.2 Large Evaluation on AD Design Choices\\n\\nIn this work, we perform large evaluations on the decoupled pipelines according to the standard procedure of AD methods. Such analysis is often overlooked in previous AD studies, and we investigate each design dimension of decoupled pipelines by fixing its corresponding design choice (e.g., Focal loss), and randomly sampling other dimensional design choices to construct AD models, e.g.,\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\text{DateAugmentation:} & \\\\quad \\\\text{Mixup, } \\\\\\\\\\n\\\\text{NetworkArchitecture:} & \\\\quad \\\\text{FTT, } \\\\\\\\\\n\\\\text{LossFunction:} & \\\\quad \\\\text{Focal, } \\\\\\\\\\n\\\\end{align*}\\n\\\\]\\n\\nIn other words, we investigate each design dimension by controlling other variables. Different design choices are compared w.r.t. \\\\( n_a = 5, 10, 20 \\\\) and demonstrated with box plots, where the number of comparisons in each box is ensured to be the same. In the following subsections, we analyze the benchmark results on each of the design dimensions, namely data handling, network construction, and network training.\\n\\n4.2.1 Data Handling\\n\\nFor the design dimension of data augmentation methods (e.g., SMOTE and Mixup methods) shown in Figure 2, we find that almost no method has brought significant performance improvements. This could be explained by the fact that, unlike time series [76], NLP [70], or CV tasks [20], data augmentation is rarely incorporated into the design of existing AD methods tailored for tabular data [57]. Besides, our results indicate that GAN-based augmentation method is even worse than simpler methods like oversampling, probably due to the difficulty of modeling and generating realistic anomalies for the tabular data. The same trend is observed in the synthetic datasets with individual anomalies. In the vast majority of cases, the results from data augmentation are inferior to those from the original data. (See Appx.D.2.) For data preprocessing methods, we do not observe a significant difference between the minmax scaling and normalizing methods.\"}"}
{"id": "9CKx9SsSSc", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this subsection, we verify the effectiveness of the proposed meta-predictors by answering the questions illustrated below. Beyond the fixed SOTA AD methods, we provide three model construction plans for a thorough baseline comparison. The random selection plan (RS) randomly generates a pipeline from all design choices. The supervised selection plan (SS) selects the pipeline according to the ground-truth labels. In contrast, the unsupervised selection plan (US) relies on the anomalies to guide the training process of such a complicated model. Furthermore, compared to our earlier work on ADBench[26] where the FTTransformer shows competitive performance, it can be reasonably assumed that the FTTransformer would also work well on other datasets with similar characteristics.\\n\\nSignificant differences in the design dimensions of hidden layers, dropout, and network initialization. For the activation function, Tanh and LeakyReLU appear to be more effective than the ReLU function when the specific distribution that can be more effectively reconstructed.\\n\\nFor model optimization, we observe that Adam and RMSprop optimizers are better than the classical SGD, where large training epochs (e.g., epochs=100) lead to overfitting on limited labeled data, resulting in a relatively inferior performance. For the design of loss functions, we compare the classical BCE loss, hinge loss, and deviation loss in all synthetic datasets with a significant similarity across terms of the AUCROC metric.\\n\\nFigure 3: AUCROC performance of network construction designs. MLP is still an effective architecture for AD tasks. We also notice that for the adaptive selection of the model's dimension, the batch size, learning rate, and weight decay, they do not show obvious differences in the design dimensions of hidden layers, dropout, and network initialization. (details in Appx. D). For model optimization, we observe that Adam and RMSprop optimizers are better than the classical SGD, where large training epochs (e.g., epochs=100) lead to overfitting on limited labeled data, resulting in a relatively inferior performance. For the design of loss functions, we compare the classical BCE loss, hinge loss, and deviation loss in all synthetic datasets with a significant similarity across terms of the AUCROC metric.\"}"}
{"id": "9CKx9SsSSc", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Is the model constructed by meta-predictor better than existing SOTA methods?\\n\\nAUCROC performance of meta-predictors shows a relative improvement over the best SOTA models. RS, SS, and GT refer to the random REPEN 2.6%/3.7% w.r.t. those SOTA methods, where they achieve better AUCROC performance in all settings, and this Our results show that the automatic construction of AD models is indeed capable of surpassing meta-predictors. \u201c2-stage\u201d and \u201cend2end\u201d correspond to the two-stage and end-to-end for extracting tree-based methods like XGBoost. The suffix -ensemble refers to ensemble multiple predictions of respectively. DL or ML corresponds to the meta-predictor that is either instantiated with MLP or\\n\\nIn the subsequent paragraphs, we present a comparison between the AD models constructed by the pipeline in a sample of 1000. We iteratively leave one dataset as the testing dataset, and leverage all the remaining datasets to train the meta-predictor(s). The central experimental results of ADGym\u2019s performance are provided in Table 3. We report the average AUCROC performance across real-world the ground truth plan (GT) is the best-performing plan, and we select the best-performing plan for each dataset. The suffix -ensemble refers to ensemble multiple predictions of\\n\\nTo prevent overfitting on limited labeled data and thus achieve better performance. Both Adam and RMSprop optimizers outperform classical SGD. Smaller epochs could\\n\\nTable 2: Performance of the baseline SOTA AD methods.\\n\\nTable 3: Performance of auto-selected pipelines by ADGym. RS, SS, and GT refer to the random REPEN 2.6%/3.7% w.r.t. those SOTA methods, where they achieve better AUCROC performance in all settings, and this Our results show that the automatic construction of AD models is indeed capable of surpassing meta-predictors. \u201c2-stage\u201d and \u201cend2end\u201d correspond to the two-stage and end-to-end for extracting tree-based methods like XGBoost. The suffix -ensemble refers to ensemble multiple predictions of respectively. DL or ML corresponds to the meta-predictor that is either instantiated with MLP or\\n\\nIn the subsequent paragraphs, we present a comparison between the AD models constructed by the pipeline in a sample of 1000. We iteratively leave one dataset as the testing dataset, and leverage all the remaining datasets to train the meta-predictor(s). The central experimental results of ADGym\u2019s performance are provided in Table 3. We report the average AUCROC performance across real-world the ground truth plan (GT) is the best-performing plan, and we select the best-performing plan for each dataset. The suffix -ensemble refers to ensemble multiple predictions of\\n\\nTo prevent overfitting on limited labeled data and thus achieve better performance. Both Adam and RMSprop optimizers outperform classical SGD. Smaller epochs could\\n\\nTable 2: Performance of the baseline SOTA AD methods.\\n\\nTable 3: Performance of auto-selected pipelines by ADGym. RS, SS, and GT refer to the random REPEN 2.6%/3.7% w.r.t. those SOTA methods, where they achieve better AUCROC performance in all settings, and this Our results show that the automatic construction of AD models is indeed capable of surpassing meta-predictors. \u201c2-stage\u201d and \u201cend2end\u201d correspond to the two-stage and end-to-end for extracting tree-based methods like XGBoost. The suffix -ensemble refers to ensemble multiple predictions of respectively. DL or ML corresponds to the meta-predictor that is either instantiated with MLP or\\n\\nIn the subsequent paragraphs, we present a comparison between the AD models constructed by the pipeline in a sample of 1000. We iteratively leave one dataset as the testing dataset, and leverage all the remaining datasets to train the meta-predictor(s). The central experimental results of ADGym\u2019s performance are provided in Table 3. We report the average AUCROC performance across real-world the ground truth plan (GT) is the best-performing plan, and we select the best-performing plan for each dataset. The suffix -ensemble refers to ensemble multiple predictions of\\n\\nTo prevent overfitting on limited labeled data and thus achieve better performance. Both Adam and RMSprop optimizers outperform classical SGD. Smaller epochs could\\n\\nTable 2: Performance of the baseline SOTA AD methods.\\n\\nTable 3: Performance of auto-selected pipelines by ADGym. RS, SS, and GT refer to the random REPEN 2.6%/3.7% w.r.t. those SOTA methods, where they achieve better AUCROC performance in all settings, and this Our results show that the automatic construction of AD models is indeed capable of surpassing meta-predictors. \u201c2-stage\u201d and \u201cend2end\u201d correspond to the two-stage and end-to-end for extracting tree-based methods like XGBoost. The suffix -ensemble refers to ensemble multiple predictions of respectively. DL or ML corresponds to the meta-predictor that is either instantiated with MLP or\\n\\nIn the subsequent paragraphs, we present a comparison between the AD models constructed by the pipeline in a sample of 1000. We iteratively leave one dataset as the testing dataset, and leverage all the remaining datasets to train the meta-predictor(s). The central experimental results of ADGym\u2019s performance are provided in Table 3. We report the average AUCROC performance across real-world the ground truth plan (GT) is the best-performing plan, and we select the best-performing plan for each dataset. The suffix -ensemble refers to ensemble multiple predictions of\\n\\nTo prevent overfitting on limited labeled data and thus achieve better performance. Both Adam and RMSprop optimizers outperform classical SGD. Smaller epochs could\\n\\nTable 2: Performance of the baseline SOTA AD methods.\\n\\nTable 3: Performance of auto-selected pipelines by ADGym. RS, SS, and GT refer to the random REPEN 2.6%/3.7% w.r.t. those SOTA methods, where they achieve better AUCROC performance in all settings, and this Our results show that the automatic construction of AD models is indeed capable of surpassing meta-predictors. \u201c2-stage\u201d and \u201cend2end\u201d correspond to the two-stage and end-to-end for extracting tree-based methods like XGBoost. The suffix -ensemble refers to ensemble multiple predictions of respectively. DL or ML corresponds to the meta-predictor that is either instantiated with MLP or\\n\\nIn the subsequent paragraphs, we present a comparison between the AD models constructed by the pipeline in a sample of 1000. We iteratively leave one dataset as the testing dataset, and leverage all the remaining datasets to train the meta-predictor(s). The central experimental results of ADGym\u2019s performance are provided in Table 3. We report the average AUCROC performance across real-world the ground truth plan (GT) is the best-performing plan, and we select the best-performing plan for each dataset. The suffix -ensemble refers to ensemble multiple predictions of\\n\\nTo prevent overfitting on limited labeled data and thus achieve better performance. Both Adam and RMSprop optimizers outperform classical SGD. Smaller epochs could\\n\\nTable 2: Performance of the baseline SOTA AD methods.\\n\\nTable 3: Performance of auto-selected pipelines by ADGym. RS, SS, and GT refer to the random REPEN 2.6%/3.7% w.r.t. those SOTA methods, where they achieve better AUCROC performance in all settings, and this Our results show that the automatic construction of AD models is indeed capable of surpassing meta-predictors. \u201c2-stage\u201d and \u201cend2end\u201d correspond to the two-stage and end-to-end for extracting tree-based methods like XGBoost. The suffix -ensemble refers to ensemble multiple predictions of respectively. DL or ML corresponds to the meta-predictor that is either instantiated with MLP or\\n\\nIn the subsequent paragraphs, we present a comparison between the AD models constructed by the pipeline in a sample of 1000. We iteratively leave one dataset as the testing dataset, and leverage all the remaining datasets to train the meta-predictor(s). The central experimental results of ADGym\u2019s performance are provided in Table 3. We report the average AUCROC performance across real-world the ground truth plan (GT) is the best-performing plan, and we select the best-performing plan for each dataset. The suffix -ensemble refers to ensemble multiple predictions of\\n\\nTo prevent overfitting on limited labeled data and thus achieve better performance. Both Adam and RMSprop optimizers outperform classical SGD. Smaller epochs could\\n\\nTable 2: Performance of the baseline SOTA AD methods.\\n\\nTable 3: Performance of auto-selected pipelines by ADGym. RS, SS, and GT refer to the random REPEN 2.6%/3.7% w.r.t. those SOTA methods, where they achieve better AUCROC performance in all settings, and this Our results show that the automatic construction of AD models is indeed capable of surpassing meta-predictors. \u201c2-stage\u201d and \u201cend2end\u201d correspond to the two-stage and end-to-end for extracting tree-based methods like XGBoost. The suffix -ensemble refers to ensemble multiple predictions of respectively. DL or ML corresponds to the meta-predictor that is either instantiated with MLP or\\n\\nIn the subsequent paragraphs, we present a comparison between the AD models constructed by the pipeline in a sample of 1000. We iteratively leave one dataset as the testing dataset, and leverage all the remaining datasets to train the meta-predictor(s). The central experimental results of ADGym\u2019s performance are provided in Table 3. We report the average AUCROC performance across real-world the ground truth plan (GT) is the best-performing plan, and we select the best-performing plan for each dataset. The suffix -ensemble refers to ensemble multiple predictions of\\n\\nTo prevent overfitting on limited labeled data and thus achieve better performance. Both Adam and RMSprop optimizers outperform classical SGD. Smaller epochs could\\n\\nTable 2: Performance of the baseline SOTA AD methods.\\n\\nTable 3: Performance of auto-selected pipelines by ADGym. RS, SS, and GT refer to the random REPEN 2.6%/3.7% w.r.t. those SOTA methods, where they achieve better AUCROC performance in all settings, and this Our results show that the automatic construction of AD models is indeed capable of surpassing meta-predictors. \u201c2-stage\u201d and \u201cend2end\u201d correspond to the two-stage and end-to-end for extracting tree-based methods like XGBoost. The suffix -ensemble refers to ensemble multiple predictions of respectively. DL or ML corresponds to the meta-predictor that is either instantiated with MLP or\\n\\nIn the subsequent paragraphs, we present a comparison between the AD models constructed by the pipeline in a sample of 1000. We iteratively leave one dataset as the testing dataset, and leverage all the remaining datasets to train the meta-predictor(s). The central experimental results of ADGym\u2019s performance are provided in Table 3. We report the average AUCROC performance across real-world the ground truth plan (GT) is the best-performing plan, and we select the best-performing plan for each dataset. The suffix -ensemble refers to ensemble multiple predictions of\\n\\nTo prevent overfitting on limited labeled data and thus achieve better performance. Both Adam and RMSprop optimizers outperform classical SGD. Smaller epochs could\\n\\nTable 2: Performance of the baseline SOTA AD methods.\\n\\nTable 3: Performance of auto-selected pipelines by ADGym. RS, SS, and GT refer to the random REPEN 2.6%/3.7% w.r.t. those SOTA methods, where they achieve better AUCROC performance in all settings, and this Our results show that the automatic construction of AD models is indeed capable of surpassing meta-predictors. \u201c2-stage\u201d and \u201cend2end\u201d correspond to the two-stage and end-to-end for extracting tree-based methods like XGBoost. The suffix -ensemble refers to ensemble multiple predictions of respectively. DL or ML corresponds to the meta-predictor that is either instantiated with MLP or\\n\\nIn the subsequent paragraphs, we present a comparison between the AD models constructed by the pipeline in a sample of 1000. We iteratively leave one dataset as the testing dataset, and leverage all the remaining datasets to train the meta-predictor(s). The central experimental results of ADGym\u2019s performance are provided in Table 3. We report the average AUCROC performance across real-world the ground truth plan (GT) is the best-performing plan, and we select the best-performing plan for each dataset. The suffix -ensemble refers to ensemble multiple predictions of\\n\\nTo prevent overfitting on limited labeled data and thus achieve better performance. Both Adam and RMSprop optimizers outperform classical SGD. Smaller epochs could\\n\\nTable 2: Performance of the baseline SOTA AD methods.\\n\\nTable 3: Performance of auto-selected pipelines by ADGym. RS, SS, and GT refer to the random REPEN 2.6%/3.7% w.r.t. those SOTA methods, where they achieve better AUCROC performance in all settings, and this Our results show that the automatic construction of AD models is indeed capable of surpassing meta-predictors. \u201c2-stage\u201d and \u201cend2end\u201d correspond to the two-stage and end-to-end for extracting tree-based methods like XGBoost. The suffix -ensemble refers to ensemble multiple predictions of respectively. DL or ML corresponds to the meta-predictor that is either instantiated with MLP or\\n\\nIn the subsequent paragraphs, we present a comparison between the AD models constructed by the pipeline in a sample of 1000. We iteratively leave one dataset as the testing dataset, and leverage all the remaining datasets to train the meta-predictor(s). The central experimental results of ADGym\u2019s performance are provided in Table 3. We report the average AUCROC performance across real-world the ground truth plan (GT) is the best-performing plan, and we select the best-performing plan for each dataset. The suffix -ensemble refers to ensemble multiple predictions of\\n\\nTo prevent overfitting on limited labeled data and thus achieve better performance. Both Adam and RMSprop optimizers outperform classical SGD. Smaller epochs could\\n\\nTable 2: Performance of the baseline SOTA AD methods.\\n\\nTable 3: Performance of auto-selected pipelines by ADGym. RS, SS, and GT refer to the random REPEN 2.6%/3.7% w.r.t. those SOTA methods, where they achieve better AUCROC performance in all settings, and this Our results show that the automatic construction of AD models is indeed capable of surpassing meta-predictors. \u201c2-stage\u201d and \u201cend2end\u201d correspond to the two-stage and end-to-end for extracting tree-based methods like XGBoost. The suffix -ensemble refers to ensemble multiple predictions of respectively. DL or ML corresponds to the meta-predictor that is either instantiated with MLP or\\n\\nIn the subsequent paragraphs, we present a comparison between the AD models constructed by the pipeline in a sample of 1000. We iteratively leave one dataset as the testing dataset, and leverage all the remaining datasets to train the meta-predictor(s). The central experimental results of ADGym\u2019s performance are provided in Table 3. We report the average AUCROC performance across real-world the ground truth plan (GT) is the best-performing plan, and we select the best-performing plan for each dataset. The suffix -ensemble refers to ensemble multiple predictions of\\n\\nTo prevent overfitting on limited labeled data and thus achieve better performance. Both Adam and RMSprop optimizers outperform classical SGD. Smaller epochs could\\n\\nTable 2: Performance of the baseline SOTA AD methods.\\n\\nTable 3: Performance of auto-selected pipelines by ADGym. RS, SS, and GT refer to the random REPEN 2.6%/3.7% w.r.t. those SOTA methods, where they achieve better AUCROC performance in all settings, and this Our results show that the automatic construction of AD models is indeed capable of surpassing meta-predictors. \u201c2-stage\u201d and \u201cend2end\u201d correspond to the two-stage and end-to-end for extracting tree-based methods like XGBoost. The suffix -ensemble refers to ensemble multiple predictions of respectively. DL or ML corresponds to the meta-predictor that is either instantiated with MLP or\\n\\nIn the subsequent paragraphs, we present a comparison between the AD models constructed by the pipeline in a sample of 1000. We iteratively leave one dataset as the testing dataset, and leverage all the remaining datasets to train the meta-predictor(s). The central experimental results of ADGym\u2019s performance are provided in Table 3. We report the average AUCROC performance across real-world the ground truth plan (GT) is the best-performing plan, and we select the best-performing plan for each dataset. The suffix -ensemble refers to ensemble multiple predictions of\\n\\nTo prevent overfitting on limited labeled data and thus achieve better performance. Both Adam and RMSprop optimizers outperform classical SGD. Smaller epochs could\\n\\nTable 2: Performance of the baseline SOTA AD methods.\\n\\nTable 3: Performance of auto-selected pipelines by ADGym. RS, SS, and GT refer to the random REPEN 2.6%/3.7% w.r.t. those SOTA methods, where they achieve better AUCROC performance in all settings, and this Our results show that the automatic construction of AD models is indeed capable of surpassing meta-predictors. \u201c2-stage\u201d and \u201cend2end\u201d correspond to the two-stage and end-to-end for extracting tree-based methods like XGBoost. The suffix -ensemble refers to ensemble multiple predictions of respectively. DL or ML corresponds to the meta-predictor that is either instantiated with MLP or\\n\\nIn the subsequent paragraphs, we present a comparison between the AD models constructed by the pipeline in a sample of 1000. We iteratively leave one dataset as the testing dataset, and leverage all the remaining datasets to train the meta-predictor(s). The central experimental results of ADGym\u2019s performance are provided in Table 3. We report the average AUCROC performance across real-world the ground truth plan (GT) is the best-performing plan, and we select the best-performing plan for each dataset. The suffix -ensemble refers to ensemble multiple predictions of\\n\\nTo prevent overfitting on limited labeled data and thus achieve better performance. Both Adam and RMSprop optimizers outperform classical SGD. Smaller epochs could\\n\\nTable 2: Performance of the baseline SOTA AD methods.\\n\\nTable 3: Performance of auto-selected pipelines by ADGym. RS, SS, and GT refer to the random REPEN 2.6%/3.7% w.r.t. those SOTA methods, where they achieve better AUCROC performance in all settings, and this Our results show"}
{"id": "9CKx9SsSSc", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Moreover, the average AUCROC performance of meta-predictors is 9.4%, 9.4%, and 8.5% higher than that of SOTA models w.r.t. $n_a = 5, 10, \\\\text{ and } 20$, respectively. Moreover, the clear advantage of GT to the current SOTA methods indicates that existing AD solutions could be further improved through exploring the design space or automatic selection techniques, which also validates the value of ADGym. Besides, compared to random selection (RS) and supervised selection (SS) which select design choices based on the performance of limited labeled data, automatic selection via meta-predictor(s) is significantly more effective. However, we find that the SS method is even inferior to the RS method. A reasonable explanation is that the scarcity of labeled data makes it difficult for the selected design choices to generalize well on newcoming data.\\n\\n2. Is it more helpful for meta-predictors to transfer knowledge across different datasets by end-to-end trained meta-features or extraction-based meta-features?\\n\\nHere, we explore the impact of different methods for extracting meta-features on meta-predictor performance, as shown in Table 3. This includes:\\n\\n(i) Two-stage method, where the meta-features of a specific dataset are first extracted by the method proposed in MetaOD [88], i.e., extracting both statistical features like min, max, variance, skewness, covariance, etc., and landmarker features that depend on the unsupervised AD algorithms to capture outlying characteristics of a dataset. The extracted meta-features $E_{\\\\text{meta}}$ are then concatenated with $n_a$ and $E_{\\\\text{comp}}$ as inputs to the meta-predictor.\\n\\n(ii) End-to-end method, where the meta-features are directly extracted by the meta-predictor [30] and optimized with the learning process of the downstream task.\\n\\nGenerally, we observe close performances between the two-stage and end-to-end methods of extracting meta-features in the meta-predictors. This conclusion holds valid not only for the default MSE loss, but also for the Ranknet loss [11] used in the meta-predictors.\\n\\n3. Is the tree-based ensemble meta-predictor effective in tabular AD? Can meta-predictors further benefit from model ensembling?\\n\\nConsistent with the findings in the previous studies [24, 25, 26], we still find that the tree-based ensemble model(s) are better solutions for tabular-based AD tasks, where the performance of meta-predictors are improved when the MLP trainer used in meta-predictor is replaced by the ensemble models like XGBoost and CatBoost. Moreover, DL-based meta-predictor can also benefit from the model ensembling strategy, as we observe that both two-stage and end-to-end meta-predictors improve when we ensemble the anomaly scores of predicted top-$k$ combinations of design choices.\\n\\n4. Do advanced loss functions bring performance gains to the meta-predictor?\\n\\nIn addition to the default MSE loss function, we also investigate several other losses (as shown in Table 4), including:\\n\\n(i) Weighted MSE imposes a stronger penalty on errors in predicting the top and bottom design choices.\\n\\n(ii) Pearson correlation between the predictions and ground-truth targets.\\n\\n(iii) Ranknet [11] learns to rank different design choices, which is widely used in recommendation systems. However, compared to the results of MSE loss shown in Table 3, we do not find significant improvement when more complicated loss functions are implemented to train the meta-predictors.\\n\\nTable 4: AUCROC performance of meta-predictors trained on other loss functions. Different loss functions do not yield significant performance improvement for the meta-predictor.\\n\\n| $n_a$  | Weighted MSE | Pearson | Ranknet |\\n|-------|-------------|---------|---------|\\n|       | DL-single   | DL-ensemble | DL-single | DL-ensemble | DL-single | DL-ensemble | DL-single | DL-ensemble |\\n|      5 | 0.801       | 0.748    | 0.811   | 0.774     | 0.778     | 0.808     | 0.804     | 0.815     | 0.800     | 0.809 |\\n|      10| 0.830       | 0.770    | 0.842   | 0.786     | 0.825     | 0.833     | 0.834     | 0.843     | 0.835     | 0.836 | 0.841 |\\n|      20| 0.844       | 0.786    | 0.862   | 0.801     | 0.839     | 0.840     | 0.852     | 0.854     | 0.853     | 0.853 | 0.862 |\\n\\n5. Does larger AD design space bring performance gains to the meta-predictors?\\n\\nIn order to explore whether the meta-predictors can uncover more promising AD design choices beyond those bolded design dimensions highlighted in Table 1, we include more possible design dimensions like hidden layers, dropout, and initialization methods in network construction, and epochs, batch size, and weight decay in network training, while maintaining the scale of design space (i.e., the number of Cartesian products of design choices) at 1,000. Table 5 shows that the meta-predictors generally benefit from a larger design space, where the AUCROC performances of both DL- and ML-based meta-predictors achieve improvements compared to that of small design space.\"}"}
{"id": "9CKx9SsSSc", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"shown in Table 3. It's worth mentioning that DL meta-predictors seem to surpass ML meta-predictors in a larger space. This may be due to a raised complexity and lower probability of over-fitting in a larger selection pool, both benefiting DL meta-predictors. However, this performance gap is still trivial (0.002-0.006 in absolute value). Considering a better efficiency, we still present the ML meta-predictors as the formal recommendation.\\n\\nTable 5: AUCROC performance of meta-predictors trained on large scale design space. Larger design space provides potentially more effective design choices for newcoming datasets.\\n\\n| n  | RS   | SS   | GT   | DL-single | DL-ensemble | ML-single | ML-ensemble |\\n|----|------|------|------|-----------|-------------|-----------|-------------|\\n| 5  | 0.738| 0.657| 0.904| 0.824     | 0.808       | 0.829     |\\n| 10 | 0.767| 0.731| 0.912| 0.842     | 0.830       | 0.853     |\\n| 20 | 0.791| 0.750| 0.922| 0.863     | 0.846       | 0.876     |\\n\\n6. Does refining AD design space bring performance gains to the meta-predictor? We further refined the design space in ADGym and excluded the design choices whose average performances on the training datasets are below the median, resulting in better yet fewer design choices that can be utilized for training meta-predictors. The refined results are reported in Table 6, compared to Table 3. We have not found that refining the design space brings any gains to the meta-predictor, possibly because such approach loses many potential design choices that could perform well on newcoming datasets, or loses (half of) the training data of meta-predictors.\\n\\nTable 6: AUCROC performance of meta-predictors trained on refined design space. No significant improvement of meta-predictor is observed since not only potentially better design choices are discarded, but also results in a reduction of training data in meta-predictors.\\n\\n| n  | RS   | SS   | GT   | DL-single | DL-ensemble | ML-single | ML-ensemble |\\n|----|------|------|------|-----------|-------------|-----------|-------------|\\n| 5  | 0.766| 0.653| 0.902| 0.797     | 0.809       | 0.812     |\\n| 10 | 0.803| 0.700| 0.912| 0.824     | 0.833       | 0.838     | 0.848       |\\n| 20 | 0.833| 0.754| 0.921| 0.851     | 0.830       | 0.867     | 0.874       | 0.867       | 0.877 |\\n\\n5 Conclusions, Limitations, and Future Directions\\nIn this paper, we introduce ADGym, a novel platform designed for benchmarking and automating AD design choices. We break down AD algorithms into granular components and systematically assess each one\u2019s effectiveness through extensive experiments. Furthermore, we develop an automated method for selecting optimal design choices, enabling the creation of AD models that surpass current SOTA algorithms. Our results highlight the crucial role design choices play and offer a structured approach to optimizing them in model development. With the broader AD community in mind, we have made ADGym openly available. We believe its comprehensive design choice evaluation capabilities will significantly contribute to future advancements in automated AD model generation.\\n\\nLooking ahead, we see several opportunities to enhance ADGym and broaden its application. Firstly, we plan to incorporate time-series AD tasks that handle data with temporal variations. By automating the design choice selection, AD models will be better equipped to adapt to these changing distributions, thereby improving anomaly detection in dynamic time-series contexts. Currently, ADGym is geared towards weakly supervised AD. In the future, we aim to include unsupervised neural network-based AD algorithms as well. Additionally, it is worth noting that non-neural-network-based techniques, such as ensemble-tree methods, have shown great promise in practical scenarios. Therefore, exploring automatic pipeline formulation in these areas is an exciting and valuable direction for future research.\\n\\n6 Acknowledgement\\nWe thank anonymous reviewers for their insightful feedback and comments. M.J., C.H., A.Z., S.H., and H.H. are supported by the National Natural Science Foundation of China under Grant No. 72271151, and the National Key Research and Development Program of China under Grant No. 2022YFC3303301. M.J., C.H., A.Z., S.H., and H.H. thank the financial support provided by FlagInfo-SHUFE Joint Laboratory. X.H. and Y.Z. are independently supported by CMU.\"}"}
{"id": "9CKx9SsSSc", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] C. C. Aggarwal and C. C. Aggarwal. An introduction to outlier analysis. Springer, 2017.\\n\\n[2] M. Ahmed, A. N. Mahmood, and M. R. Islam. A survey of anomaly detection techniques in financial domain. Future Generation Computer Systems, 55:278\u2013288, 2016.\\n\\n[3] S. Akcay, A. Atapour-Abarghouei, and T. P. Breckon. Ganomaly: Semi-supervised anomaly detection via adversarial training. In ACCV, pages 622\u2013637, 2018.\\n\\n[4] F. Alimoglu and E. Alpaydin. Methods of combining multiple classifiers based on different representations for pen-based handwritten digit recognition. In TAINN. Citeseer, 1996.\\n\\n[5] E. Alpaydin and C. Kaynak. Cascading classifiers. Kybernetika, 34(4):369\u2013374, 1998.\\n\\n[6] I. Ashrapov. Tabular gans for uneven distribution, 2020.\\n\\n[7] D. Ayres-de Campos, J. Bernardes, A. Garrido, J. Marques-de Sa, and L. Pereira-Leite. Sisporto 2.0: a program for automated analysis of cardiotocograms. Journal of Maternal-Fetal Medicine, 2000.\\n\\n[8] S. Bhattacharyya, S. Jha, K. Tharakunnel, and J. C. Westland. Data mining for credit card fraud: A comparative study. Decision support systems, 50(3):602\u2013613, 2011.\\n\\n[9] R. J. Bolton and D. J. Hand. Statistical fraud detection: A review. Statistical science, 17(3):235\u2013255, 2002.\\n\\n[10] N. Br\u00fcmmer, S. Cumani, O. Glembek, M. Karafi\u00e1t, P. Mat\u011bjka, J. Pe\u0161\u00e1n, O. Plchot, M. Soufifar, E. d. Villiers, and J. H. \u010cernock\u00fd. Description and analysis of the brno276 system for lre2011. In Odyssey 2012-the speaker and language recognition workshop, 2012.\\n\\n[11] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning, pages 89\u201396, 2005.\\n\\n[12] G. O. Campos, A. Zimek, J. Sander, R. J. Campello, B. Micenkov\u00e1, E. Schubert, I. Assent, and M. E. Houle. On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study. Data mining and knowledge discovery, 30:891\u2013927, 2016.\\n\\n[13] S. Chauhan and L. Vig. Anomaly detection in ecg time signals via deep long short-term memory networks. In 2015 IEEE international conference on data science and advanced analytics (DSAA), pages 1\u20137. IEEE, 2015.\\n\\n[14] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321\u2013357, 2002.\\n\\n[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n\\n[16] T. Dietterich, A. Jain, R. Lathrop, and T. Lozano-Perez. A comparison of dynamic reposing and tangent distance for drug activity prediction. NeurIPS, 6, 1993.\\n\\n[17] R. Domingues, M. Filippone, P. Michiardi, and J. Zouaoui. A comparative evaluation of outlier detection algorithms: Experiments and analyses. Pattern recognition, 74:406\u2013421, 2018.\\n\\n[18] M. Du, Z. Chen, C. Liu, R. Oak, and D. Song. Lifelong anomaly detection through unlearning. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, pages 1283\u20131297, 2019.\\n\\n[19] A. Emmott, S. Das, T. Dietterich, A. Fern, and W.-K. Wong. A meta-analysis of the anomaly detection problem. ArXiv, 1503.01158, 2015.\\n\\n[20] S. Y. Feng, V. Gangal, J. Wei, S. Chandar, S. Vosoughi, T. Mitamura, and E. Hovy. A survey of data augmentation approaches for nlp. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 968\u2013988, 2021.\\n\\n[21] P. W. Frey and D. J. Slate. Letter recognition using holland-style adaptive classifiers. Machine learning, 6(2):161\u2013182, 1991.\\n\\n[22] J. Gao, X. Song, Q. Wen, P. Wang, L. Sun, and H. Xu. RobustTAD: Robust time series anomaly detection via decomposition and convolutional neural networks. KDD Workshop on Mining and Learning from Time Series (KDD-MileTS\u201920), 2020.\"}"}
{"id": "9CKx9SsSSc", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"M. Goldstein and S. Uchida. A comparative evaluation of unsupervised anomaly detection algorithms for multivariate data. PloS one, 11(4):e0152173, 2016.\\n\\nY. Gorishniy, I. Rubachev, V. Khrulkov, and A. Babenko. Revisiting deep learning models for tabular data. Advances in Neural Information Processing Systems, 34:18932\u201318943, 2021.\\n\\nL. Grinsztajn, E. Oyallon, and G. Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.\\n\\nS. Han, X. Hu, H. Huang, M. Jiang, and Y. Zhao. ADBench: Anomaly detection benchmark. Advances in Neural Information Processing Systems (NeurIPS), 35:32142\u201332159, 2022.\\n\\nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778, 2016.\\n\\nP. Horton and K. Nakai. A probabilistic classification system for predicting the cellular localization sites of proteins. In Ismb, volume 4, pages 109\u2013115, 1996.\\n\\nM. Jiang, C. Hou, A. Zheng, X. Hu, S. Han, H. Huang, X. He, P. S. Yu, and Y. Zhao. Weakly supervised anomaly detection: A survey. arXiv preprint arXiv:2302.04549, 2023.\\n\\nH. S. Jomaa, L. Schmidt-Thieme, and J. Grabocka. Dataset2vec: Learning dataset meta-features. Data Mining and Knowledge Discovery, 35:964\u2013985, 2021.\\n\\nT. Kerssies. Neural architecture search for visual anomaly segmentation. arXiv preprint arXiv:2304.08975, 2023.\\n\\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\nM. Kudo, J. Toyama, and M. Shimbo. Multidimensional curve classification using passing-through regions. Pattern Recognition Letters, 20(11-13):1103\u20131111, 1999.\\n\\nK.-H. Lai, D. Zha, G. Wang, J. Xu, Y. Zhao, D. Kumar, Y. Chen, P. Zumkhawaka, M. Wan, D. Martinez, et al. Tods: An automated time series outlier detection system. In Proceedings of the aaai conference on artificial intelligence, volume 35, pages 16060\u201316062, 2021.\\n\\nK.-H. Lai, D. Zha, J. Xu, Y. Zhao, G. Wang, and X. Hu. Revisiting time series outlier detection: Definitions and benchmarks. In Neural Information Processing Systems (NeurIPS), 2021.\\n\\nA. Lazarevic, L. Ertoz, V. Kumar, A. Ozgur, and J. Srivastava. A comparative study of anomaly detection schemes in network intrusion detection. In SDM, pages 25\u201336. SIAM, 2003.\\n\\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\\n\\nC. Leibig, V. Allken, M. S. Ayhan, P. Berens, and S. Wahl. Leveraging uncertainty information from deep neural networks for disease detection. Scientific reports, 7(1):1\u201314, 2017.\\n\\nC. Li, X. Li, L. Feng, and J. Ouyang. Who is your right mixup partner in positive and unlabeled learning. In International Conference on Learning Representations, 2022.\\n\\nY. Li, Z. Chen, D. Zha, K. Zhou, H. Jin, H. Chen, and X. Hu. Autood: Neural architecture search for outlier detection. In 2021 IEEE 37th International Conference on Data Engineering (ICDE), pages 2117\u20132122. IEEE, 2021.\\n\\nY. Li, D. Zha, P. Venugopal, N. Zou, and X. Hu. Pyodds: An end-to-end outlier detection system with automated machine learning. In Companion Proceedings of the Web Conference 2020, pages 153\u2013157, 2020.\\n\\nZ. Li, Y. Zhao, N. Botta, C. Ionescu, and X. Hu. Copod: copula-based outlier detection. In 2020 IEEE international conference on data mining (ICDM), pages 1118\u20131123. IEEE, 2020.\\n\\nZ. Li, Y. Zhao, X. Hu, N. Botta, C. Ionescu, and G. Chen. Ecod: Unsupervised outlier detection using empirical cumulative distribution functions. IEEE Transactions on Knowledge and Data Engineering, 2022.\"}"}
{"id": "9CKx9SsSSc", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[46] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.\\n\\n[46] F. Liu, C. Zeng, L. Zhang, Y. Zhou, Q. Mu, Y. Zhang, L. Zhang, and C. Zhu. Fedtadbench: Federated time-series anomaly detection benchmark. arXiv preprint arXiv:2212.09518, 2022.\\n\\n[47] F. T. Liu, K. M. Ting, and Z.-H. Zhou. Isolation forest. In 2008 eighth ieee international conference on data mining, pages 413\u2013422. IEEE, 2008.\\n\\n[48] K. Liu, Y. Dou, Y. Zhao, X. Ding, X. Hu, R. Zhang, K. Ding, C. Chen, H. Peng, K. Shu, et al. Bond: Benchmarking unsupervised outlier node detection on static attributed graphs. Advances in Neural Information Processing Systems, 35:27021\u201327035, 2022.\\n\\n[49] K. Liu, Y. Dou, Y. Zhao, X. Ding, X. Hu, R. Zhang, K. Ding, C. Chen, H. Peng, K. Shu, L. Sun, J. Li, G. H. Chen, Z. Jia, and P. S. Yu. Bond: Benchmarking unsupervised outlier node detection on static attributed graphs. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 27021\u201327035. Curran Associates, Inc., 2022.\\n\\n[50] W.-Y. Loh. Classification and regression trees. WIREs Data Mining and Knowledge Discovery, 1, 2011.\\n\\n[51] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. ArXiv, 1711.05101, 2017.\\n\\n[52] J. Lu, D. Batra, D. Parikh, and S. Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019.\\n\\n[53] M. Q. Ma, Y. Zhao, X. Zhang, and L. Akoglu. The need for unsupervised outlier model selection: A review and evaluation of internal evaluation strategies. ACM SIGKDD Explorations Newsletter, 25(1), 2023.\\n\\n[54] D. Malerba, F. Esposito, and G. Semeraro. A further comparison of simplification methods for decision-tree induction. In Learning from data, pages 365\u2013374. Springer, 1996.\\n\\n[55] N. Moustafa and J. Slay. Unsw-nb15: a comprehensive data set for network intrusion detection systems (unsw-nb15 network data set). In 2015 military communications and information systems conference (MilCIS), pages 1\u20136. IEEE, 2015.\\n\\n[56] G. Pang, L. Cao, L. Chen, and H. Liu. Learning representations of ultrahigh-dimensional data for random distance-based outlier detection. In KDD, pages 2041\u20132050, 2018.\\n\\n[57] G. Pang, C. Shen, L. Cao, and A. V. D. Hengel. Deep learning for anomaly detection: A review. ACM computing surveys (CSUR), 54(2):1\u201338, 2021.\\n\\n[58] G. Pang, C. Shen, H. Jin, and A. van den Hengel. Deep weakly-supervised anomaly detection. ArXiv, 1910.13601, 2019.\\n\\n[59] G. Pang, C. Shen, and A. van den Hengel. Deep anomaly detection with deviation networks. In KDD, pages 353\u2013362, 2019.\\n\\n[60] J. Paparrizos, Y. Kang, P. Boniol, R. S. Tsay, T. Palpanas, and M. J. Franklin. Tsb-uad: an end-to-end benchmark suite for univariate time-series anomaly detection. Proceedings of the VLDB Endowment, 15(8):1697\u20131711, 2022.\\n\\n[61] J. R. Quinlan. Induction of decision trees. Machine learning, 1(1):81\u2013106, 1986.\\n\\n[62] J. R. Quinlan, P. J. Compton, K. Horn, and L. Lazarus. Inductive knowledge acquisition: a case study. In Australian Conference on Applications of expert systems, pages 137\u2013156, 1987.\\n\\n[63] M. M. Rahman, D. Balakrishnan, D. Murthy, M. Kutlu, and M. Lease. An information retrieval approach to building datasets for hate speech detection. arXiv preprint arXiv:2106.09775, 2021.\\n\\n[64] S. Rayana. ODDS library, 2016.\\n\\n[65] A. Rivolli, L. P. Garcia, C. Soares, J. Vanschoren, and A. C. de Carvalho. Meta-features for meta-learning. Knowledge-Based Systems, 240:108101, 2022.\\n\\n[66] S. Ruder. An overview of gradient descent optimization algorithms. ArXiv, 1609.04747, 2016.\\n\\n[67] L. Ruff, J. R. Kauffmann, R. A. Vandermeulen, G. Montavon, W. Samek, M. Kloft, T. G. Dietterich, and K.-R. M\u00fcller. A unifying review of deep and shallow anomaly detection. Proceedings of the IEEE, 109(5):756\u2013795, 2021.\"}"}
{"id": "9CKx9SsSSc", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"L. Ruff, R. Vandermeulen, N. Goernitz, L. Deecke, S. A. Siddiqui, A. Binder, E. M\u00fcller, and M. Kloft. Deep one-class classification. In *ICML*, pages 4393\u20134402, 2018.\\n\\nL. Ruff, R. A. Vandermeulen, N. G\u00f6rnitz, A. Binder, E. M\u00fcller, K. M\u00fcller, and M. Kloft. Deep semi-supervised anomaly detection. In *ICLR*. OpenReview.net, 2020.\\n\\nC. Shorten and T. M. Khoshgoftaar. A survey on image data augmentation for deep learning. *Journal of big data*, 6(1):1\u201348, 2019.\\n\\nJ. Soenen, E. Van Wolputte, L. Perini, V. Vercruyssen, W. Meert, J. Davis, and H. Blockeel. The effect of hyperparameter tuning on the comparative evaluation of unsupervised anomaly detection methods. In *Proceedings of the KDD\u201921 Workshop on Outlier Detection and Description*, pages 1\u20139. Outlier Detection and Description Organising Committee, 2021.\\n\\nG. Steinbuss and K. B\u00f6hm. Benchmarking unsupervised outlier detection with realistic synthetic data. *ACM Transactions on Knowledge Discovery from Data (TKDD)*, 15(4):1\u201320, 2021.\\n\\nC. Termritthikun, L. Xu, Y. Liu, and I. Lee. Neural architecture search and multi-objective evolutionary algorithms for anomaly detection. In *2021 International Conference on Data Mining Workshops (ICDMW)*, pages 1001\u20131008. IEEE, 2021.\\n\\nS. Thulasidasan, G. Chennupati, J. A. Bilmes, T. Bhattacharya, and S. Michalak. On mixup training: Improved calibration and predictive uncertainty for deep neural networks. *Advances in Neural Information Processing Systems*, 32, 2019.\\n\\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. *Advances in neural information processing systems*, 30, 2017.\\n\\nQ. Wen, L. Sun, F. Yang, X. Song, J. Gao, X. Wang, and H. Xu. Time series data augmentation for deep learning: A survey. In *Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)*, pages 4653\u20134660, 2021.\\n\\nK. S. Woods, J. L. Solka, C. E. Priebe, W. P. Kegelmeyer Jr, C. C. Doss, and K. W. Bowyer. Comparative evaluation of pattern recognition techniques for detection of microcalcifications in mammography. In *State of The Art in Digital Mammographic Image Analysis*, pages 213\u2013231. World Scientific, 1994.\\n\\nG. Xie, J. Wang, J. Liu, J. Lyu, Y. Liu, C. Wang, F. Zheng, and Y. Jin. Im-iad: Industrial image anomaly detection benchmark in manufacturing. *arXiv preprint arXiv:2301.13359*, 2023.\\n\\nP. Xu, L. Zhang, X. Liu, J. Sun, Y. Zhao, H. Yang, and B. Yu. Do not train it: A linear neural architecture search of graph neural networks. In *International Conference on Machine Learning*. PMLR, 2023.\\n\\nJ. You, Z. Ying, and J. Leskovec. Design space for graph neural networks. *Advances in Neural Information Processing Systems*, 33:17009\u201317021, 2020.\\n\\nW. Yu, J. Li, M. Z. A. Bhuiyan, R. Zhang, and J. Huai. Ring: Real-time emerging anomaly monitoring system over text streams. *IEEE Transactions on Big Data*, 5(4):506\u2013519, 2017.\\n\\nM. D. Zeiler. Adadelta: an adaptive learning rate method. *ArXiv*, 1212.5701, 2012.\\n\\nC. Zhang, T. Zhou, Q. Wen, and L. Sun. TFAD: A decomposition time series anomaly detection architecture with time-frequency analysis. In *Proceedings of the 31st ACM International Conference on Information & Knowledge Management*, pages 2497\u20132507, 2022.\\n\\nH. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. *arXiv preprint arXiv:1710.09412*, 2017.\\n\\nY. Zhang, Z. Guan, H. Qian, L. Xu, H. Liu, Q. Wen, L. Sun, J. Jiang, L. Fan, and M. Ke. Cloudrca: a root cause analysis framework for cloud computing platforms. In *Proceedings of the 30th ACM International Conference on Information & Knowledge Management*, pages 4373\u20134382, 2021.\\n\\nJ. Zhao, X. Liu, Q. Yan, B. Li, M. Shao, and H. Peng. Multi-attributed heterogeneous graph convolutional network for bot detection. *Information Sciences*, 537:380\u2013393, 2020.\\n\\nY. Zhao and L. Akoglu. Towards unsupervised hpo for outlier detection. *arXiv preprint arXiv:2208.11727*, 2022.\\n\\nY. Zhao, R. Rossi, and L. Akoglu. Automatic unsupervised outlier model selection. *Advances in Neural Information Processing Systems*, 34:4489\u20134502, 2021.\"}"}
{"id": "9CKx9SsSSc", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Y. Zhao, S. Zhang, and L. Akoglu. Toward unsupervised outlier model selection. In IEEE International Conference on Data Mining (ICDM). IEEE, 2022.\\n\\nY. Zhou, X. Song, Y. Zhang, F. Liu, C. Zhu, and L. Liu. Feature encoding with autoencoders for weakly supervised anomaly detection. TNNLS, 2021.\\n\\nB. Zong, Q. Song, M. R. Min, W. Cheng, C. Lumezanu, D. Cho, and H. Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In ICLR, 2018.\"}"}
{"id": "9CKx9SsSSc", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We provide more details of the evaluated 29 datasets (\u00a7A), compared baselines (\u00a7B), the proposed ADGym (\u00a7C), and additional experimental results (\u00a7D).\\n\\n### A Dataset List\\n\\nMost of the datasets used for model evaluation are derived from our previous work [26], and we drop those datasets smaller than 1,000, and use the subsets of 3,000 for those datasets greater than 3,000 due to the computational cost. We also remove those datasets that cause errors for the compared baseline models. This results in a total of 29 datasets, as is described in Table A1. These datasets cover many application domains, including healthcare (e.g., disease diagnosis), audio and language processing (e.g., speech recognition), image processing (e.g., object identification), finance (e.g., financial fraud detection), and more, where we show this information in the last column.\\n\\n| Data      | # Samples | # Features | Anomaly % | Anomaly Category | Reference |\\n|-----------|-----------|------------|-----------|------------------|-----------|\\n| ALOI      | 49,534    | 27         | 1508      | Image            | [19]      |\\n| annthyroid| 7,200     | 6          | 534       | Healthcare       | [61]      |\\n| backdoor  | 93,329    | 196        | 2,329     | Network          | [55]      |\\n| campaign  | 41,188    | 62         | 4640      | Finance          | [59]      |\\n| Cardiotocography | 2,141 | 21 | 466 | Healthcare | [7] |\\n| celeba    | 202,599   | 39         | 4,547     | Image            | [59]      |\\n| census    | 299,285   | 500        | 18,568    | Sociology        | [59]      |\\n| donors    | 619,326   | 10         | 36,710    | Sociology        | [59]      |\\n| fault     | 1,941     | 27         | 673       | Physical         | [19]      |\\n| landsat   | 6,435     | 36         | 1,333     | Astronautics     | [19]      |\\n| letter    | 1,600     | 32         | 100       | Image            | [21]      |\\n| magic.gamma | 19,020 | 10   | 6,688    | Physical         | [19]      |\\n| mammography | 11,183 | 6  | 260 | Healthcare | [77] |\\n| mnist     | 7,603     | 100        | 700       | Image            | [37]      |\\n| musk      | 3,062     | 166        | 97        | Chemistry        | [16]      |\\n| optdigits | 5,216     | 64         | 150       | Image            | [5]       |\\n| PageBlocks| 5,393     | 10         | 510       | Document         | [54]      |\\n| pendigits | 6,870     | 16         | 156       | Image            | [4]       |\\n| satellite | 6,435     | 36         | 2,036     | Astronautics     | [64]      |\\n| satimage-2| 5,803   | 36 | 71 | Astronautics | [64] |\\n| shuttle   | 49,097    | 9          | 3,511     | Astronautics     | [64]      |\\n| skin      | 245,057   | 3          | 50,859    | Image            | [19]      |\\n| SpamBase  | 4,207     | 57         | 1,679     | Document         | [12]      |\\n| speech    | 3,686     | 400        | 61        | Linguistics      | [10]      |\\n| thyroid   | 3,772     | 6          | 93        | Healthcare       | [62]      |\\n| vowels    | 1,456     | 12         | 50        | Linguistics      | [33]      |\\n| Waveform  | 3,443     | 21         | 100       | Physics          | [50]      |\\n| Wilt      | 4,819     | 5          | 257       | Botany           | [12]      |\\n| yeast     | 1,484     | 8          | 507       | Biology          | [28]      |\\n\\n### B Compared Baselines\\n\\nWe compare the automatically selected AD models via ADGym with the following weakly- and fully-supervised baselines, which are considered as effective AD solutions in the previous work [26].\\n\\n1. Semi-Supervised Anomaly Detection via Adversarial Training (GANomaly) [3]. A GAN-based method defines the reconstruction error of the input data as the anomaly score. We replace the convolutional layer in its original version with the dense layer for the tabular AD task, where the hidden size of the encoder-decoder-encoder structure of GANomaly is set to half of the input dimension. We train the GANomaly for 50 epochs with 64 batch size, where the SGD [66] optimizer with 0.01 learning rate and 0.7 momentum is applied for both the generator and discriminator.\"}"}
