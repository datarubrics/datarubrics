{"id": "W5If9P1xqO", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nModern climate projections lack adequate spatial and temporal resolution due to computational constraints. A consequence is inaccurate and imprecise predictions of critical processes such as storms. Hybrid methods that combine physics with machine learning (ML) have introduced a new generation of higher fidelity climate simulators that can sidestep Moore's Law by outsourcing compute-hungry, short, high-resolution simulations to ML emulators. However, this hybrid ML-physics simulation approach requires domain-specific treatment and has been inaccessible to ML experts because of lack of training data and relevant, easy-to-use workflows.\\n\\nWe present ClimSim, the largest-ever dataset designed for hybrid ML-physics research. It comprises multi-scale climate simulations, developed by a consortium of climate scientists and ML researchers. It consists of 5.7 billion pairs of multivariate input and output vectors that isolate the influence of locally-nested, high-resolution, high-fidelity physics on a host climate simulator's macro-scale physical state. The dataset is global in coverage, spans multiple years at high sampling frequency, and is designed such that resulting emulators are compatible with downstream coupling into operational climate simulators. We implement a range of deterministic and stochastic regression baselines to highlight the ML challenges and their scoring. The data ([https://huggingface.co/datasets/LEAP/ClimSim_high-res](https://huggingface.co/datasets/LEAP/ClimSim_high-res)) and code ([https://leap-stc.github.io/ClimSim](https://leap-stc.github.io/ClimSim)) are released openly to support the development of hybrid ML-physics and high-fidelity climate simulations for the benefit of science and society.\\n\\n\u2217 Corresponding author: sungduk@uci.edu\\n\\nAlso available in a low-resolution version ([https://huggingface.co/datasets/LEAP/ClimSim_low-res](https://huggingface.co/datasets/LEAP/ClimSim_low-res)) and an aquaplanet version ([https://huggingface.co/datasets/LEAP/ClimSim_low-res_aqua-planet](https://huggingface.co/datasets/LEAP/ClimSim_low-res_aqua-planet)).\"}"}
{"id": "W5If9P1xqO", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1 Introduction\\n\\n1.1 Overview\\n\\nPredictions from numerical physical simulations are the primary tool informing policy on climate change. However, current climate simulators poorly represent cloud and extreme rainfall physics [1, 2] despite stretching the limits of the world's most powerful supercomputers. The complexity of the Earth system imposes significant restrictions on the spatial resolution we can use in these simulations [3]. Physics occurring on scales smaller than the temporal and/or spatial resolutions of climate simulations are commonly represented using empirical mathematical representations called \u201cparameterizations\u201d. Unfortunately, assumptions in these parameterizations often lead to errors that can grow into inaccuracies in the future predicted climate.\\n\\nMachine learning (ML) is an attractive approach to emulate the complex nonlinear sub-resolution physics\u2014processes occurring on scales smaller than the resolution of the climate simulator\u2014at a lower computational complexity. Their implementation has the exciting possibility of resulting in climate simulations that are both cheaper and more accurate than they currently are [4, 5]. Current climate simulators have a typical smallest resolvable scale of 80\u2013200 km, equivalent to the size of a typical U.S. county. However, accurately representing cloud formation requires a resolution of 100 m or finer, demanding six orders of magnitude increase in computational intensity. Exploiting ML remains a conceivable solution to sidestep the limitations of classical computing [5]: resulting hybrid-ML climate simulators combine traditional numerical methods\u2014which solve the equations governing large-scale fluid motions of Earth's atmosphere\u2014with ML emulators of the macro-scale effects of small-scale physics. Instead of relying on heuristic assumptions about these small-scale processes, the emulators learn directly from data generated by short-duration, high-resolution simulations [4, 6\u201318].\\n\\nThe task is essentially a regression problem: in the climate simulation, an ML parameterization emulator returns the large-scale outputs\u2014changes in wind, moisture, or temperature\u2014that occur due to unresolved small-scale (sub-resolution) physics, given large-scale resolved inputs (e.g., temperature, wind velocity; see Section 4).\\n\\nWhile several proofs of concept have emerged in recent years, hybrid-ML climate simulators have yet to be advanced to operational use. Obtaining sufficient training data is a major challenge impeding interest from the ML community. This data must contain all macro-scale variables that regulate the behavior of sub-resolution physics and be compatible with downstream hybrid ML-climate simulations. Addressing this using training data from uniformly high-resolution simulations has proven to be very expensive and can lead to issues when coupled to a host climate simulation. A promising solution is to utilize multi-scale climate simulation methods to generate training data. Crucially, these provide a clean interface between the emulated high-resolution physics and the host climate simulator's planetary-scale dynamics [19]. In theory, this makes downstream hybrid coupled simulation approachable and tractable. In practice, the full potential of multi-scale methods remains largely untapped due to a scarcity of existing datasets, exacerbated by the combination of operational simulation code complexity and the need for domain expertise in choosing variables.\\n\\nWe introduce ClimSim, the largest and most physically comprehensive dataset for training ML emulators of atmospheric storms, clouds, turbulence, rainfall, and radiation for use in hybrid-ML climate simulations. ClimSim is a comprehensive collection of inputs and outputs from physical climate simulations using the multi-scale method. ClimSim was prepared by atmospheric scientists and climate simulator developers to lower the barriers to entry for ML experts on this important problem. Our benchmark dataset serves as a foundation for developing robust frameworks that emulate parameterizations for cloud and extreme rainfall physics, and their interaction with other sub-resolution processes. These frameworks enable online coupling within the host coarse-resolution climate simulator, ultimately improving the performance and accuracy of climate simulators used for long-term projections.\\n\\n1.2 Concepts and Terminology from Earth Science\\n\\nConvective Parameterization:\\n\\nIn atmospheric science, \u201cconvection\u201d refers to storm cloud and rain development, as well as the associated turbulent air motions. Convective parameterizations represent the integrated effects of these processes, such as the vertical transport of heat, moisture, and momentum within the atmosphere, and condensational heating and drying, on the temporal and spatial\"}"}
{"id": "W5If9P1xqO", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Stochastic parameterizations represent sub-resolution effects as stochastic processes, dependent on grid-scale variable inputs to capture variations arising from sub-grid scale dynamics.\\n\\nMulti-scale climate simulation is a technique that represents convection without a convective parameterization, by deploying a smaller-scale, high-resolution cloud-resolving simulator nested within each host grid column of a climate simulator. The smaller-scale simulator explicitly resolves the detailed behavior of clouds and their turbulent motions at both a higher spatial and temporal resolution (but with a smaller domain) than the host simulator. This improves the accuracy of the host simulations, but comes at a high computational cost. The time-integrated and horizontally averaged influence of the resolved convection is fed upscale to the host climate simulator, and is the target of hybrid ML-climate simulation approaches.\\n\\nSignificance of Precipitation Processes for Climate Impacts:\\n\\nIn climate simulations, changes in precipitation with warming is a particularly important issue. The frequency of extreme precipitation events increases with warming, with corresponding societal impacts. Current climate simulators agree on the direction of this change, but exhibit large spread in the quantitative rate of increase with warming.\\n\\n**Related Work**\\n\\nThere have been several recent efforts to produce hybrid-ML emulators using multi-scale climate simulations, analogous to ClimSim. Most of these focused on simple aquaplanets and those that included real geography did not include enough variables for complete land-surface coupling, to our knowledge. Most examine simple multi-layer perceptrons except for those who used a ResNet architecture, and those who used a variational encoder-decoder that accounts for stochasticity. Although downstream hybrid testing in real-geography settings is error-prone, some hybrid stability has been demonstrated. Compressing input data to avoid causal confounders may improve downstream accuracy, and methods have been proven to enforce physical constraints.\\n\\nCompared to the training data used above, ClimSim's comprehensive variable coverage is unprecedented, including all variables needed to couple to and from a land system simulator and enforce physical constraints. Its availability across coarse-resolution, high-resolution, aquaplanet and real-geography use cases is also new to the community. Successful ML innovations with ClimSim can have a downstream impact since it is based on a state-of-the-art multi-scale climate simulator that is actively supported by a mission agency (U.S. Department of Energy).\\n\\nIn non-multi-scale settings, an important body of related work has made exciting progress on using analogous hybrid ML approaches to reduce biases in uniform resolution climate simulations, including in an operational climate code with land coupling and downstream hybrid stability (see Supplementary Information; SI). Other related work includes full model emulation (FME) for short-term weather prediction. Whether this approach is possible for climate simulation using the high-frequency output of its state variables remains an open question. For instance, it has recently been shown that incorporating spherical geometry and resolution invariance through spherical Fourier neural operators leads to stability of long rollouts. While ClimSim is focused on hybrid-ML climate simulation and we do not demonstrate FME baselines, ClimSim contains full atmospheric state variable sampling well suited for the task.\\n\\n**ClimSim Dataset Construction**\\n\\nExperiment Outline:\\n\\nClimSim presents a regression problem with mapping from a multivariate input vector, with inputs $x \\\\in \\\\mathbb{R}^{d_i}$ of size $d_i = 124$ and targets $y \\\\in \\\\mathbb{R}^{d_o}$ of size $d_o = 128$ (Figure 1). The input represents the local vertical structure (in horizontal location and time) of macro-scale state variables in a multi-scale physical climate simulator before any adjustments from sub-grid scale convection and radiation are made. The input also includes concatenated scalars containing boundary conditions of incoming radiation at the top of the atmospheric column, and land surface model constraints at its base. The target vector contains the tendencies of the same state variables representing the redistribution of mass and water, microphysical water species conversions, and radiative heating feedbacks associated with explicitly resolved convection. This brackets the change...\"}"}
{"id": "W5If9P1xqO", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: The spatially-local version of ClimSim that our baselines are scored on. A spatially-global version of the problem that expands to the full list of variables would be useful to try.\\n\\nIn atmospheric state after tens of thousands of computationally intensive, spatially nested simulators of explicit cloud physics have completed a temporally-nested integration. The ultimate goal is to outsource these physics to ML by mapping inputs to targets at comparable fidelity. The target vector includes scalar fields and fluxes from the bottom of the atmospheric column expected by the land surface model component that it must couple to; land-atmosphere coupling is important to predicting regional water cycle dynamics [45, 46]. Importantly, ClimSim also includes the option for expanded inputs $x \\\\in \\\\mathbb{R}^{d_i}$ of size $d_i = 617$ and targets $y \\\\in \\\\mathbb{R}^{d_o}$ of size $d_o = 368$, which we demonstrate in one of our experiments.\\n\\nLocality vs. Nonlocality: A spatially-global version of the problem could be of practical use for improving ML via helpful spatial context [47, 48]. In such a case, the problem becomes 2D\u21922D regression, and would encompass inputs $x \\\\in \\\\mathbb{R}^{d_i}$ of maximum size $d_i = 617 \\\\times 21,600$ (grid columns) and targets, $y \\\\in \\\\mathbb{R}^{d_o}$, of maximum size $d_o = 368 \\\\times 21,600$. Here the second dimension represents the unstructured \\\"cube-sphere\\\" computational mesh used by the climate model, which is a list of grid cell locations that span the surface of the sphere [49]. In contrast to typical image-to-image translation or spatio-temporal prediction problems in ML that involve data on a structured grid (i.e. rectilinear), the task at hand is of lower dimensionality. Further details about the climate simulator configuration, simulations, and data, including complete variable lists, can be found in SI.\\n\\nDataset Collection: We ran the E3SM-MMF multi-scale climate simulator [28, 29, 49, 50], using multiple NVIDIA A100 GPUs for a total of $\\\\sim 9,800$ GPU-hours. We saved global instantaneous values of the atmospheric state before and after high-resolution calculations occurred, isolating state updates due to explicitly-resolved moist convection, boundary layer turbulence, and radiation; details of the climate simulator configuration can be found in SI. These data were saved at 20-minute intervals (i.e. the time step of the climate model) for 10 simulated years, resulting in 5.7 billion samples for the high-resolution simulation that uses an unstructured \\\"cube-sphere\\\" horizontal grid with 21,600 grid columns spanning the globe. This grid yields an approximate horizontal grid spacing of 1.5\u00b0, but unlike a traditional climate model that maps points across the sphere using two dimensions aligned with cardinal north/south and east/west directions, unstructured grids use a single dimension to organize the horizontal location of points. The atmospheric columns at each location and time are...\"}"}
{"id": "W5If9P1xqO", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"treated as independent samples. Thus, the total number of samples can be understood by considering\\n5.7 billion\\n\u2248\\n21,600 horizontal locations per time step\\n\u00d7\\n72-time steps per simulated day\\n\u00d7\\n3,650\\n(simulated days). It is important to note that each sample retains a 1D structure corresponding to\\nthe vertical variation across 60 levels. We also ran two additional simulations with approximately\\nten times less horizontal resolution, with only 384 grid columns spanning the globe, resulting in\\n100 million samples for each simulation. These low-resolution options allow for fast prototyping\\nof ML models, due to smaller training data volumes and less geographic complexity. One low-\\nresolution simulation uses an \\\"aquaplanet\\\" configuration, i.e., a lower boundary condition of specified\\nsea surface temperature, invariant in the longitudinal dimension with no seasonal cycle. This is\\nthe simplest prototyping dataset, removing variance associated with continents and time-varying\\nboundary conditions. The total data volume is 41.2TB for the high-resolution dataset and 744GB for\\neach of the low-resolution datasets.\\n\\nDataset Interface:\\nRaw model outputs emerge from the climate simulator as standard NetCDF files\\nwhich can be easily parsed in any language. Each timestep yields files containing input and target\\nvectors separately, resulting in a total of 525,600 files for each of the three datasets. To prevent\\nredundancy, variable metadata and grid information was saved separately.\\n\\nThe raw tensors from the climate simulations are initially either 2D or 3D, depending on the variable.\\nFor 2D tensors, the dimensions represent time and horizontal location. While these variables actually\\ndepend on three physical dimensions (time and 2D space), since each location on the sphere is\\nindexed along a single axis due to the climate model's unstructured horizontal grid, the apparent\\ndimensionality is lower. Such variables include solar insolation, snow depth over land, surface energy\\nfluxes, and surface precipitation rate. 3D tensors include the additional dimension representing\\naltitude relative to the Earth's surface, for height-varying state variables like temperature, humidity,\\nand wind vector components. Separate files are used to store each timestep and variable. ClimSim\\nincludes a total of 24 2D variables and 10 3D variables (see Table 1 in SI).\\n\\nDataset Split:\\nThe 10-year datasets are divided into: (a) a training and validation spanning the\\nfirst 8 years (0001-02 to 0009-01; YYYY-MM), excluding the first simulated month for numerical\\nspin-up, and (b) a test set spanning the remaining two years (i.e., 0009-03 to 0011-02). A one-month\\ngap is intentionally introduced between the two sets to prevent test set contamination via temporal\\ncorrelation. Both sets are stored separately in our data repositories.\\n\\nEnergy use:\\nThe computing and energy costs of generating ClimSim could be viewed as wasteful and\\nhaving a negative consequence for society through associated emissions. We emphasize that while it\\ncan appear large, the compute used is actually orders of magnitude less than what is consumed by\\noperational climate prediction. Associated emissions are minimized given that our integrations were\\nperformed on energy-efficient GPU hardware. The cost must also be weighed against the potential\\nsocial benefit of mitigating future energy consumption by eliminating end users' need for costly\\nphysics-based MMF simulations. Meanwhile, a large consortium of interested parties have helped\\nagree on this dataset, to help ensure it is not wasted.\\n\\n4 Experiments\\nTo guide ML practitioners using ClimSim, we provide an example ML workflow using the low-\\nresolution, real-geography dataset for the task described in Section 1. All but one of our baselines\\nfocus on emulating the subset of total available input and target variables illustrated in Figure 1,\\nwith the following inputs\\n\\nx \u2208 Rd\\n\\nof size\\n\\ndi = 124\\n(Figure 1, Table 1), chosen for its similarity to recent attempts in the literature.\\n\\nTraining/Validation Split:\\nWe divide the 8-year training/validation set into the first 7 years (i.e.,\\n0001-02 to 0008-01 in the raw filenames' \\\"year-month\\\" notation) for training and the subsequent 1\\nyear (0008-02 to 0009-01) for validation.\\n\\nPreprocessing Workflow:\\nOur preprocessing steps were (1) downsample in time by using every\\n7th sample, (2) collapse horizontal location and time into a single sample dimension, (3) normalize\\nvariables by subtracting the mean and dividing by the range, with these statistics calculated separately\\nat each of the 60 vertical levels for the four variables with vertical dependence, and (4) concatenate\\nvariables into multi-variate input and output vectors for each sample (Figure 1). The heating tendency\"}"}
{"id": "W5If9P1xqO", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: The subset of input and target variables used in most of our experiments (Figure 1).\\n\\n- **Input**: Size\\n- **Target**: dT/dt\\n\\nThe temperature tendency, dT/dt, was calculated from the raw climate simulator output as \\n\\\\[\\n\\\\frac{T_{\\\\text{after}} - T_{\\\\text{before}}}{\\\\Delta t}\\n\\\\]\\nwhere \\\\(\\\\Delta t = 1200\\\\) s is the climate simulator's known macro-scale timestep. Likewise, the moisture tendency was calculated via taking the difference of humidity state variables recorded before versus after the convection and radiation calculations. This target variable transformation is done so that we can compare the performance of our baseline models to that of previously published models that reported errors of emulated tendencies [14, 39].\\n\\nAdditionally, this transformation implicitly normalizes the target variables leading to better convergence properties for ML algorithms. Given the domain-specific nature of the preprocessing workflow, we provide scripts in the GitHub repository for workflow reproduction.\\n\\n### 4.1 Baseline Architectures\\n\\nSix baseline models used in our experiment are briefly described here. Refer to SI for further details.\\n\\n- **Convolutional Neural Network (CNN)** uses a 1D ResNet-style network. Each ResNet block contains two 1D convolutional layers and a skip connection. CNNs can learn spatial structure and have outperformed MLP and graph-based networks in [51]. The inputs and outputs for the CNN are stacked in the channel dimensions, such that the mapping is 60 \u00d7 6 \u2192 60 \u00d7 10. Accordingly, global variables have been repeated along the vertical dimension.\\n\\n- **Encoder-Decoder (ED)** consists of an Encoder and a Decoder with 6 fully-connected hidden layers each [39]. The Encoder of ED condenses the original dimensionality of the input variables down to only 5 nodes inside the latent space. This enhances the interpretability of ED and makes the model beneficial for advanced postprocessing of multivariate climate data [39].\\n\\n- **Heteroskedastic Regression (HSR)** [52] predicts a separate mean and standard deviation for each output variable, using a regularized MLP.\\n\\n- **Multi-layer Perceptron (MLP)** is a fully connected, feed-forward neural network. The MLP architecture used for our experiments is optimized via an extensive hyperparameter search with 8,257 trials.\\n\\n- **Randomized Prior Network (RPN)** is an ensemble model [53]. Each member of the RPN is built as the sum of a trainable and a non-trainable (so-called \u201cprior\u201d) surrogate model; we used MLP for simplicity. Multiple replicas of the networks are constructed by independent and random sampling of both trainable and non-trainable parameters [54, 55]. RPNs also resort to data bootstrapping (e.g., subsampling and randomization) in order to mitigate the uncertainty collapse of the ensemble method when tested beyond the training data points [55].\\n\\n- **Conditional Variational Autoencoder (cVAE)** uses amortized variational inference to fit a deep generative model that is conditioned on the input and can produce samples from a complex predictive distribution.\"}"}
{"id": "W5If9P1xqO", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: MAE and $R^2$ for target variables averaged globally and temporally (from 0009-03 to 0011-02). Variables include heating tendency ($dT/dt$), moistening tendency ($dq/dt$), net surface shortwave flux (NETSW), downward surface longwave flux (FLWDS), snow rate (PRECSC), rain rate (PRECC), visible direct solar flux (SOLS), near-IR direct solar flux (SOLL), visible diffused solar flux (SOLSD), and near-IR diffused solar flux (SOLLD). Units of non-energy flux variables are converted to a common energy unit, $W/m^2$. Best model performance for each variable is bolded.\\n\\n4.2 Skill Boost from Expanding Features and Targets\\n\\nWe performed an ablation of our best performing MLP baseline to demonstrate the added value of the expanded inputs and targets available in ClimSim, i.e. using inputs $x$ of size $d_i = 617$ and targets $y \\\\in \\\\mathbb{R}^{d_o}$ of size $d_o = 368$; see Table 1 in SI for the full list of variables. We use the same transformation described in our preprocessing workflow to compute and add condensate (cloud liquid and cloud ice) and momentum (zonal and meridional winds) tendencies to the target vector. We conducted this ablation study with both the low-resolution and the high-resolution datasets (see Section 3.1 in SI for further details regarding these MLP variants). For common elements of the target vector, using all available variables leads to a uniform improvement in prediction accuracy, especially for precipitation, in both resolutions (Figures SI7, SI8 and Table SI4). The larger errors (e.g., MAE and RMSE) observed in the high-resolution emulators are anticipated due to the increased variance of higher-resolution data. Nevertheless, the similarity of their $R^2$ values to those of the corresponding low-resolution emulators confirms their adequate performance.\\n\\n4.3 Evaluation Metrics\\n\\nOur evaluation metrics are computed separately for each variable in the output vector. Mean absolute error (MAE) and the coefficient of determination ($R^2$) are calculated independently at each horizontal and vertical location, and then averaged horizontally and vertically to produce the summary statistics in Figure 2. For the vertically-varying fields, we first form a mass-weighting and then convert moistening and heating tendencies into common energy units in Watts per square meter as in [56]. We also report continuous ranked probability scores (CRPS) for all considered models in SI.\\n\\n4.4 Baseline Model Results\\n\\nFigure 2 summarizes the error characteristics. Whereas heating and moistening rates have comparable global mean MAE, behind a common background vertical structure (Figure 2 b,c) the coefficient of determination $R^2$ (d,e) reveals that certain architectures (RPN, HSR, cVAE, CNN) consistently perform better in the upper atmosphere (model level < 30) whereas the highly optimized MLP model outperforms in the lower atmosphere (model level > 30) and therefore the global mean (Table 2). For the global mean MAE we see the largest averaged errors for PRECC and NETSW (mean MAE > 15 $W/m^2$, Figure 2 and Table 2), where MLP clearly has the best the best skill compared to all other benchmark models. For the other variables, the global mean MAE is considerably smaller and the skill of the benchmarks model appears to be more similar in absolute numbers. While for the global mean $R^2$ we find the lowest measurable performance for $dT/dt$ and PRECC ($mean R^2 < 0.7$) and in these cases, CNN gives the most skillful predictions. The other variables have larger $R^2$ of order 0.8 or higher, which suggests that these quantities are easier to deep-learn (Table 2). For $dq/dt$ and PRECSC global mean $R^2$ is not an ideal evaluation metric due to negligible variability in $dq/dt$ in the upper atmosphere and for PRECSC in the tropics in the dataset (Table 2).\"}"}
{"id": "W5If9P1xqO", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Additional tables and figures that reveal the geographic and vertical structure of these errors, fit quality, and analysis of stochastic metrics, are included in SI (Sections 4.3, 8.1, and 8.2 in SI).\\n\\nFigure 2: (a) Summary, where \\\\( \\\\frac{dT}{dt} \\\\) and \\\\( \\\\frac{dq}{dt} \\\\) are the tendencies of temperature and specific humidity, respectively, and were vertically integrated with mass weighting. (b,c) retain the vertical structure of MAE and (d,e) R2. Error bars and grey shadings show the the 5- to 95-percentile range of MLP. Refer to Table 1 for variable definitions.\\n\\n4.5 Physics-Informed Guidance to Improve Generalizability and Coupled Performance\\n\\nPhysical Constraints: Mass and energy conservation are important criteria for Earth system simulation. If these terms are not conserved, errors in estimating sea level rise or temperature change over time may become as large as the signals we hope to measure. Enforcing conservation on emulated results helps constrain results to be physically plausible and reduce the potential for errors accumulating over long time scales. We discuss how to do this and enforce additional constraints, such as non-negativity for precipitation, condensate, and moisture variables in the Supporting Information.\\n\\nStochasticity and Memory: The results of the embedded convection calculations regulating d\\\\( \\\\text{o} \\\\) are chaotic, and thus worthy of stochastic architectures, as in our RPN, HSR, and cVAE baselines. These solutions are likewise sensitive to sub-grid initial state variables from an interior nested spatial dimension that has not been included in our data.\\n\\nTemporal Locality: Incorporating the previous timesteps' target or feature in the input vector inflation could be beneficial as it captures some information about this convective memory and utilizes temporal autocorrelations present in atmospheric data.\\n\\nCausal Pruning: A systematic and quantitative pruning of the input vector based on objectively assessed causal relationships to subsets of the target vector has been proposed as an attractive preprocessing strategy, as it helps remove spurious correlations due to confounding variables and optimize the ML algorithm [16].\\n\\nNormalization: Normalization that goes beyond removing vertical structure could be strategic, such as removing the geographic mean (e.g., latitudinal, land/sea structure) or composite seasonal variances (e.g., local smoothed annual cycle) present in the data. For variables exhibiting exponential variation and approaching zero at the highest level (e.g., metrics of moisture), log-normalization might be beneficial.\"}"}
{"id": "W5If9P1xqO", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our baseline models have focused on the low-resolution dataset, for ease of data volume, and using only a subset of the available inputs and outputs. This illustrates the essence of the ML challenge. However, we show in our ablation study, using MLPs, that including all input variables yields generally an improved reproduction of the target variables in both the low-resolution and the high-resolution dataset (Figures SI7 and SI8 and Table SI4). Accordingly, we encourage users who discover competitive fits in this approachable limit to expand to all inputs/outputs in the high-resolution, real-geography dataset, for which successful fits become operationally relevant.\\n\\nFurther ML Approaches:\\nRecent methods to capture multi-scale processes using neural operators that learn in a discretization-invariant manner and can predict at higher resolutions than available during training time [57] may be attractive. Their performance can be further enhanced by incorporating physics-informed losses at a higher resolution than available training data [58]. Ideas on ML modeling for sub-grid closures from adjacent fields like turbulent flow physics and reactive flows can also be leveraged for developing architectures with an inductive bias for known priors [59], easing prediction of stiff non-linear behavior [60\u201362], generative modeling with physical constraints [63, 64] and for interpretability of the final trained models [60].\\n\\nLimitations and Other Applications\\nIdealizations:\\nA limitation of the multi-scale climate simulator used to produce ClimSim (E3SM-MMF) is that it assumes scale separation, i.e., that convection can be represented as laterally periodic within the grid size of the host simulator, and neglects sub-grid scale representations of topographic and land-surface variability. Despite these simplifications, the data adequately captures many essential aspects of the ML problem, such as stochasticity, and interactions across radiation, microphysics, and turbulence.\\n\\nHybrid testing:\\nInclusion of a natural path for downstream testing of learned physics emulators as fully coupled components of a hybrid-ML climate simulator is vital. However, such a workflow is not yet included in ClimSim, since there is no easy way for the ML community to run many hybridized variants of the E3SM-MMF in a distributed high-performance GPU computing infrastructure via a lightweight API. It is our eventual goal to tackle the software engineering needed to enable such a protocol, since, in the long term, it is in this downstream environment where ML researchers should expect to have their maximum impact on the field of hybrid-ML climate simulation. Meanwhile, ClimSim provides the first step.\\n\\nStochasticity:\\nOne open problem that the dataset may allow assessing is understanding the role of stochasticity in hybrid-ML simulation. While primarily used as a dataset for regression, it would be also interesting to assess and understand the degree to which different variables are better modeled as stochastic or deterministic, or if the dataset gives rise to heavy-tailed or even multi-modal conditional distributions that are important to capture. To date, these questions have been raised based on physical conjectures [e.g., 65] but remain to be addressed in the ML-based parameterization literature. For instance, precipitation distributions have long tails that are projected to lengthen under global warming [34, 66]\u2014and will thus tend to generate out-of-sample extremes. ClimSim could help construct optimal architectures to capture precipitation tails and other impactful climate variables such as surface temperature.\\n\\nInterpretability:\\nThis dataset could also be utilized to discover physically interpretable models for atmospheric convection, radiation, and microphysics. A possible workflow would apply dimensional-reduction techniques to identify dominant vertical variations, followed by symbolic regression to recover analytic expressions [67, 68].\\n\\nGeneralizability:\\nAlthough the impacts of global warming and inter-annual variability are absent in this initial version of ClimSim, important questions surrounding climate-convection interactions can begin to be addressed. One strategy would involve partitioning the data such that the emulator is trained on cold columns, but validated on warm columns, where warmth could be measured by surface temperatures, as in [56]. However, the results from this approach may also reflect the dependence of convection on the geographical distribution of surface temperatures in the current climate and should be interpreted with caution. To optimally engage ML researchers in solving the climate generalization problem, a multi-climate extension of ClimSim should be developed that includes physical simulations that samples future climate states and more internal variability.\"}"}
{"id": "W5If9P1xqO", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Relevance determination and active learning: While the climate simulator code offers data generation flexibility, guidance on ideal regimes to target for improved learning would benefit the domain scientists able to run it. This question can be addressed with the current data and metrics of interest provided.\\n\\nConclusion and Future Work\\n\\nWe introduce ClimSim, the most physically comprehensive dataset yet published for training ML emulators of atmospheric storms, clouds, turbulence, rainfall, and radiation for use in hybrid-ML climate simulation. It contains all inputs and outputs necessary for downstream coupling in a full-complexity multi-scale climate simulator. We conduct a series of experiments on a subset of these variables that demonstrate the degree to which climate data scientists have been able to fit their deterministic and stochastic components.\\n\\nWe hope ML community engagement in ClimSim will advance fundamental ML methodology and clarify the path to producing increasingly skillful sub-grid physics emulators that can be reliably used for operational climate simulation. To facilitate two-way communication between ML practitioners and climate scientists, we incorporate many desired characteristics for an ideal benchmark dataset suggested in [69]. Such interdisciplinary collaboration will open up an exciting future in which the computational limits that currently constrain climate simulation can be reconsidered.\\n\\nWe plan to soon extend ClimSim to include, first, a sampling of multiple future climate states. Second, we aim to provide a protocol for downstream hybrid simulation testing. We hope lessons learned in our chosen limit of multi-scale atmospheric simulation will have applicability in other sub-fields of Earth System Science where computational constraints are currently a barrier to including explicit representations of more systems of nested complexity.\"}"}
{"id": "W5If9P1xqO", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nThis work is broadly supported across countries and agencies. Primary support is by the National Science Foundation (NSF) Science and Technology Center (STC) Learning the Earth with Artificial Intelligence and Physics (LEAP), Award # 2019625-STC and the Exascale Computing Project (17-SC-20-SC), a collaborative effort of the U.S. Department of Energy (DOE) Office of Science (SC), the National Nuclear Security Administration, and the Energy Exascale Earth System Model project, funded by DOE grant DE-SC0022331. M.S.P, S.Y ., L.P., A.M.J., J.L., N.L., and G.M. further acknowledge support from the DOE (DE-SC0023368) and NSF (AGS-1912134). R.Y , S.M, P.G, M.P. acknowledge funding from the DOE Advanced Scientific Computing Research (ASCR) program (DE-SC0022255). V .E., P.G., G.B., and F.I.-S. acknowledge funding from the European Research Council Synergy Grant (Agreement No. 855187) under the Horizon 2020 Research and Innovation Programme. E.A.B. was supported, in part, by NSF grant AGS-2210068. S.J. acknowledges funding from DOE ASRC under an Amalie Emmy Noether Fellowship Award in Applied Mathematics (B&R #KJ0401010). M.A.B acknowledges NSF funding from an AGS-PRF Fellowship Award (AGS-2218197). R.G. acknowledges funding from the NSF (DGE-2125913) and the U.S. Department of Defense (DOD). S.M. acknowledges support from an NSF CAREER Award and NSF grant IIS-2007719. L.Z. and N.L. received M2LInES research funding by the generosity of Eric and Wendy Schmidt by recommendation of the Schmidt Futures program. This research used resources of the National Energy Research Scientific Computing Center (NERSC), a DOE SC User Facility operated under Contract No. DE-AC02-05CH11231. The Pacific Northwest National Laboratory is operated by Battelle for the DOE under Contract DE-AC05-76RL01830. This work was performed under the auspices of the DOE by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. This work used Bridges2 at the Pittsburgh Supercomputing Center through allocation ATM190002 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. This work also utilized the DOD High Performance Computing Modernization Program (HPCMP).\\n\\nReferences\\n\\n[1] IPCC, Climate Change 2021: The Physical Science Basis. Contribution of Working Group I to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change. 2021.\\n\\n[2] S. Sherwood, M. J. Webb, J. D. Annan, K. C. Armour, P. M. Forster, J. C. Hargreaves, G. Hegerl, S. A. Klein, K. D. Marvel, E. J. Rohling, et al., \u201cAn assessment of earth\u2019s climate sensitivity using multiple lines of evidence,\u201d Rev. Geophys., vol. 58, no. 4, p. e2019RG000678, 2020.\\n\\n[3] T. Schneider, J. Teixeira, C. S. Bretherton, F. Brient, K. G. Pressel, C. Sch\u00e4r, and A. P. Siebesma, \u201cClimate goals and computing the future of clouds,\u201d Nat. Clim. Change, vol. 7, no. 1, pp. 3\u20135, 2017.\\n\\n[4] P. Gentine, M. Pritchard, S. Rasp, G. Reinaudi, and G. Yacalis, \u201cCould machine learning break the convection parameterization deadlock?,\u201d Geophys. Res. Lett., vol. 45, no. 11, pp. 5742\u20135751, 2018.\\n\\n[5] V . Eyring, V . Mishra, G. P. Griffith, L. Chen, T. Keenan, M. R. Turetsky, S. Brown, F. Jotzo, F. C. Moore, and S. van der Linden, \u201cReflections and projections on a decade of climate science,\u201d Nat. Clim. Change, vol. 11, no. 4, pp. 279\u2013285, 2021.\\n\\n[6] C. S. Bretherton, B. Henn, A. Kwa, N. D. Brenowitz, O. Watt-Meyer, J. McGibbon, W. A. Perkins, S. K. Clark, and L. Harris, \u201cCorrecting coarse-grid weather and climate models by machine learning from global storm-resolving simulations,\u201d J. Adv. Model. Earth Syst., vol. 14, no. 2, p. e2021MS002794, 2022.\\n\\n[7] S. K. Clark, N. D. Brenowitz, B. Henn, A. Kwa, J. McGibbon, W. A. Perkins, O. Watt-Meyer, C. S. Bretherton, and L. M. Harris, \u201cCorrecting a 200 km resolution climate model in multiple climates by machine learning from 25 km resolution simulations,\u201d Journal of Advances in Modeling Earth Systems, vol. 14, no. 9, p. e2022MS003219, 2022.\"}"}
{"id": "W5If9P1xqO", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Kwa, S. K. Clark, B. Henn, N. D. Brenowitz, J. McGibbon, O. Watt-Meyer, W. A. Perkins, L. Harris, and C. S. Bretherton, \\\"Machine-learned climate model corrections from a global storm-resolving model: Performance across the annual cycle,\\\" *J. Adv. Model. Earth Syst.*, vol. 15, no. 5, p. e2022MS003400, 2023.\\n\\nC. H. Sanford, A. Kwa, O. Watt-Meyer, S. K. Clark, N. D. Brenowitz, and C. S. Bretherton, \\\"Improving the reliability of ml-corrected climate models with novelty detection,\\\" *Authorea Preprints*, 2023.\\n\\nS. Rasp, M. S. Pritchard, and P. Gentine, \\\"Deep learning to represent subgrid processes in climate models,\\\" *Proc. Natl. Acad. Sci. USA*, vol. 115, no. 39, pp. 9684\u20139689, 2018.\\n\\nN. D. Brenowitz, T. Beucler, M. Pritchard, and C. S. Bretherton, \\\"Interpreting and stabilizing machine-learning parameterizations of convection,\\\" *J. Atmos. Sci.*, vol. 77, no. 12, pp. 4357\u20134375, 2020.\\n\\nY. Han, G. J. Zhang, X. Huang, and Y. Wang, \\\"A moist physics parameterization based on deep learning,\\\" *J. Adv. Model. Earth Syst.*, vol. 12, no. 9, p. e2020MS002076, 2020.\\n\\nJ. Ott, M. Pritchard, N. Best, E. Linstead, M. Curcic, and P. Baldi, \\\"A fortran-keras deep learning bridge for scientific computing,\\\" 2020. arxiv:2004.10652.\\n\\nG. Mooers, M. Pritchard, T. Beucler, J. Ott, G. Yacalis, P. Baldi, and P. Gentine, \\\"Assessing the potential of deep learning for emulating cloud superparameterization in climate models with real-geography boundary conditions,\\\" *J. Adv. Model. Earth Syst.*, vol. 13, no. 5, p. e2020MS002385, 2021.\\n\\nX. Wang, Y. Han, W. Xue, G. Yang, and G. J. Zhang, \\\"Stable climate simulations using a realistic general circulation model with neural network parameterizations for atmospheric moist physics and radiation processes,\\\" *Geosci. Model Dev.*, vol. 15, no. 9, pp. 3923\u20133940, 2022.\\n\\nF. Iglesias-Suarez, P. Gentine, B. Solino-Fernandez, T. Beucler, M. Pritchard, J. Runge, and V. Eyring, \\\"Causally-informed deep learning to improve climate models and projections,\\\" 2023. arxiv:2304.12952.\\n\\nJ. Yuval and P. A. O'Gorman, \\\"Stable machine-learning parameterization of subgrid processes for climate modeling at a range of resolutions,\\\" *Nature Comm.*, vol. 11, no. 1, p. 3295, 2020.\\n\\nJ. Yuval, P. A. O'Gorman, and C. N. Hill, \\\"Use of neural networks for stable, accurate and physically consistent parameterization of subgrid atmospheric processes with good performance at reduced precision,\\\" *Geophys. Res. Lett.*, vol. 48, no. 6, p. e2020GL091363, 2021.\\n\\nS. Rasp, \\\"Coupled online learning as a way to tackle instabilities and biases in neural network parameterizations: general algorithms and lorenz 96 case study (v1.0),\\\" *Geosci. Model Dev.*, vol. 13, no. 5, pp. 2185\u20132196, 2020.\\n\\nK. A. Emanuel, *Atmospheric convection*. 1994.\\n\\nD. Randall, *Atmosphere, clouds, and climate*, vol. 6. 2012.\\n\\nA. P. Siebesma, S. Bony, C. Jakob, and B. Stevens, *Clouds and climate: Climate science's greatest challenge*. 2020.\\n\\nJ. W.-B. Lin and J. D. Neelin, \\\"Influence of a stochastic moist convective parameterization on tropical climate variability,\\\" *Geophys. Res. Lett.*, vol. 27, no. 22, pp. 3691\u20133694, 2000.\\n\\nJ. D. Neelin, O. Peters, J. W.-B. Lin, K. Hales, and C. E. Holloway, \\\"Rethinking convective quasi-equilibrium: observational constraints for stochastic convective schemes in climate models,\\\" *Phil. Trans. Royal Soc. A*, vol. 366, no. 1875, pp. 2581\u20132604, 2008.\\n\\nW. W. Grabowski and P. K. Smolarkiewicz, \\\"Crcp: A cloud resolving convection parameterization for modeling the tropical convecting atmosphere,\\\" *Phys. D: Nonlinear Phenom.*, vol. 133, no. 1-4, pp. 171\u2013178, 1999.\"}"}
{"id": "W5If9P1xqO", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[26] J. J. Benedict and D. A. Randall, \u201cStructure of the madden\u2013julian oscillation in the superparameterized cam,\u201d J. Atmos. Sci., vol. 66, no. 11, pp. 3277\u20133296, 2009.\\n\\n[27] D. A. Randall, \u201cBeyond deadlock,\u201d Geophys. Res. Lett., vol. 40, no. 22, pp. 5970\u20135976, 2013.\\n\\n[28] W. M. Hannah, C. R. Jones, B. R. Hillman, M. R. Norman, D. C. Bader, M. A. Taylor, L. Leung, M. S. Pritchard, M. D. Branson, G. Lin, et al., \u201cInitial results from the super-parameterized e3sm,\u201d Journal of Advances in Modeling Earth Systems, vol. 12, no. 1, p. e2019MS001863, 2020.\\n\\n[29] M. R. Norman, D. C. Bader, C. Eldred, W. M. Hannah, B. R. Hillman, C. R. Jones, J. M. Lee, L. Leung, I. Lyngaas, K. G. Pressel, et al., \u201cUnprecedented cloud resolution in a gpu-enabled full-physics atmospheric climate simulation on olcf\u2019s summit supercomputer,\u201d Int. J. High Perform. Compu. Appl., vol. 36, no. 1, pp. 93\u2013105, 2022.\\n\\n[30] D. Randall, M. Khairoutdinov, A. Arakawa, and W. Grabowski, \u201cBreaking the cloud parameterization deadlock,\u201d Bull. Am. Meteorol. Soc., vol. 84, no. 11, pp. 1547\u20131564, 2003.\\n\\n[31] M. Khairoutdinov, C. DeMott, and D. Randall, \u201cEvaluation of the simulated interannual and sub-seasonal variability in an amip-style simulation using the csu multiscale modeling framework,\u201d J. Clim., vol. 21, no. 3, pp. 413\u2013431, 2008.\\n\\n[32] P. Pall, M. R. Allen, and D. A. Stone, \u201cTesting the clausius \u2013 clapeyron constraint on changes in extreme precipitation under co2 warming,\u201d Clim. Dyn., vol. 28, no. 4, pp. 351\u2013363, 2007.\\n\\n[33] S. B. Guerreiro, H. J. Fowler, R. Barbero, S. Westra, G. Lenderink, S. Blenkinsop, E. Lewis, and X. F. Li, \u201cDetection of continental-scale intensification of hourly rainfall extremes,\u201d Nat. Clim. Change, vol. 8, no. 9, pp. 803\u2013807, 2018.\\n\\n[34] J. D. Neelin, C. Martinez-Villalobos, S. N. Stechmann, F. Ahmed, G. Chen, J. M. Norris, Y.-H. Kuo, and G. Lenderink, \u201cPrecipitation extremes and water vapor: Relationships in current climate and implications for climate change,\u201d Current Clim. Change Rep., vol. 8, no. 1, pp. 17\u201333, 2022.\\n\\n[35] F. V. Davenport, M. Burke, and N. S. Diffenbaugh, \u201cContribution of historical precipitation change to us flood damages,\u201d Proc. Natl. Acad. Sci. USA, vol. 118, no. 4, p. e2017524118, 2021.\\n\\n[36] A. G. Pendergrass and D. L. Hartmann, \u201cTwo modes of change of the distribution of rain,\u201d J. Clim., vol. 27, no. 22, pp. 8357\u20138371, 2014.\\n\\n[37] C. Martinez-Villalobos and J. D. Neelin, \u201cRegionally high risk increase for precipitation extreme events under global warming,\u201d Sci. Rep., vol. 13, p. 5579, 2023.\\n\\n[38] J. Lin, S. Yu, T. Beucler, P. Gentine, D. Walling, and M. Pritchard, \u201cSystematic sampling and validation of machine Learning-Parameterizations in climate models,\u201d Sept. 2023.\\n\\n[39] G. Behrens, T. Beucler, P. Gentine, F. Iglesias-Suarez, M. Pritchard, and V. Eyring, \u201cNon-linear dimensionality reduction with a variational encoder decoder to understand convective processes in climate models,\u201d J. Adv. Model. Earth Syst., vol. 14, no. 8, p. e2022MS003130, 2022.\\n\\n[40] T. Beucler, M. Pritchard, S. Rasp, J. Ott, P. Baldi, and P. Gentine, \u201cEnforcing analytic constraints in neural networks emulating physical systems,\u201d Phys. Rev. Lett., vol. 126, no. 9, p. 098302, 2021.\\n\\n[41] C. J. Reed, R. Gupta, S. Li, S. Brockman, C. Funk, B. Clipp, K. Keutzer, S. Candido, M. Uyttenhove, and T. Darrell, \u201cScale-mae: A scale-aware masked autoencoder for multiscale geospatial representation learning,\u201d 2023. arxiv:2212.14532.\\n\\n[42] J. Pathak, S. Subramanian, P. Harrington, S. Raja, A. Chattopadhyay, M. Mardani, T. Kurth, D. Hall, Z. Li, K. Azizzadenesheli, P. Hassanzadeh, K. Kashinath, and A. Anandkumar, \u201cFourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators,\u201d 2022. arxiv:2202.11214.\\n\\n[43] B. Bonev, T. Kurth, C. Hundt, J. Pathak, M. Baust, K. Kashinath, and A. Anandkumar, \u201cSpherical fourier neural operators: Learning stable dynamics on the sphere,\u201d in Proc. ICLR, 2023.\"}"}
{"id": "W5If9P1xqO", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"R. Lam, A. Sanchez-Gonzalez, M. Willson, P. Wirnsberger, M. Fortunato, A. Pritzel, S. Ravuri, T. Ewalds, F. Alet, Z. Eaton-Rosen, W. Hu, A. Merose, S. Hoyer, G. Holland, J. Stott, O. Vinyals, S. Mohamed, and P. Battaglia, \u201cGraphcast: Learning skillful medium-range global weather forecasting,\u201d 2022. arxiv:2212.12794.\\n\\nE. M. Fischer, S. I. Seneviratne, P. L. Vidale, D. L\u00fcthi, and C. Sch\u00e4r, \u201cSoil moisture\u2013atmosphere interactions during the 2003 European summer heat wave,\u201d J. Clim., vol. 20, no. 20, pp. 5081\u20135099, 2007.\\n\\nS. I. Seneviratne, T. Corti, E. L. Davin, M. Hirschi, E. B. Jaeger, I. Lehner, B. Orlowsky, and A. J. Teuling, \u201cInvestigating soil moisture\u2013climate interactions in a changing climate: A review,\u201d Earth-Sci. Rev., vol. 99, no. 3-4, pp. 125\u2013161, 2010.\\n\\nP. Wang, J. Yuval, and P. A. O\u2019Gorman, \u201cNon-local parameterization of atmospheric subgrid processes with neural networks,\u201d J. Adv. Model. Earth Syst., vol. 14, no. 10, p. e2022MS002984, 2022.\\n\\nB. L\u00fctjens, C. H. Crawford, C. D. Watson, C. Hill, and D. Newman, \u201cMultiscale neural operator: Learning fast and grid-independent PDE solvers,\u201d 2022. arxiv:2207.11417.\\n\\nW. M. Hannah, K. G. Pressel, M. Ovchinnikov, and G. S. Elsaesser, \u201cCheckerboard patterns in e3smv2 and e3sm-mmfv2,\u201d Geosci. Model Dev., vol. 15, no. 9, pp. 6243\u20136257, 2022.\\n\\nW. M. Hannah, A. M. Bradley, O. Guba, Q. Tang, J.-C. Golaz, and J. Wolfe, \u201cSeparating physics and dynamics grids for improved computational efficiency in spectral element Earth system models,\u201d J. Adv. Model. Earth Syst., vol. 13, no. 7, p. e2020MS002419, 2021.\\n\\nS. R. Cachay, V. Ramesh, J. N. S. Cole, H. Barker, and D. Rolnick, \u201cClimart: A benchmark dataset for emulating atmospheric radiative transfer in weather and climate models,\u201d 2021. arxiv:2111.14671.\\n\\nE. Wong-Toi, A. Boyd, V. Fortuin, and S. Mandt, \u201cUnderstanding pathologies of deep het-skedastic regression,\u201d 2023. arxiv:2306.16717.\\n\\nI. Osband, J. Aslanides, and A. Cassirer, \u201cRandomized prior functions for deep reinforcement learning,\u201d 2018. arxiv:1806.03335.\\n\\nY. Yang, G. Kissas, and P. Perdikaris, \u201cScalable uncertainty quantification for deep operator networks using randomized priors,\u201d Comput. Methods Appl. Mech. Eng., vol. 399, p. 115399, 2022.\\n\\nM. A. Bhouri, M. Joly, R. Yu, S. Sarkar, and P. Perdikaris, \u201cScalable Bayesian optimization with high-dimensional outputs using randomized prior networks,\u201d 2023. arxiv:2302.07260.\\n\\nT. Beucler, M. Pritchard, J. Yuval, A. Gupta, L. Peng, S. Rasp, F. Ahmed, P. A. O\u2019Gorman, J. D. Neelin, N. J. Lutsko, and P. Gentine, \u201cClimate-invariant machine learning,\u201d 2021. arxiv:2112.08440.\\n\\nZ. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar, \u201cFourier neural operator for parametric partial differential equations,\u201d 2021. arxiv:2010.08895.\\n\\nZ. Li, H. Zheng, N. Kovachki, D. Jin, H. Chen, B. Liu, K. Azizzadenesheli, and A. Anandkumar, \u201cPhysics-informed neural operator for learning partial differential equations,\u201d 2023. arxiv:2111.03794.\\n\\nJ. Ling, A. Kurzawski, and J. Templeton, \u201cReynolds averaged turbulence modelling using deep neural networks with embedded invariance,\u201d J. Fluid Mech., vol. 807, pp. 155\u2013166, 2016.\\n\\nJ. F. MacArt, J. Sirignano, and J. B. Freund, \u201cEmbedded training of neural-network subgrid-scale turbulence models,\u201d Phys. Rev. Fluids, vol. 6, no. 5, p. 050502, 2021.\\n\\nV. Xing, C. Lapeyre, T. Jaravel, and T. Poinsot, \u201cGeneralization capability of convolutional neural networks for progress variable variance and reaction rate subgrid-scale modeling,\u201d Energies, vol. 14, no. 16, p. 5096, 2021.\"}"}
{"id": "W5If9P1xqO", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"M. P. Brenner, J. D. Eldredge, and J. B. Freund, \\\"Perspective on machine learning for advancing fluid mechanics,\\\" Phys. Rev. Fluids, vol. 4, p. 100501, 2019.\\n\\nA. Subramaniam, M. L. Wong, R. D. Borker, S. Nimmagadda, and S. K. Lele, \\\"Turbulence enrichment using physics-informed generative adversarial networks,\\\" 2020. arxiv:2003.01907.\\n\\nB. Kim, V. C. Azevedo, N. Thuerey, T. Kim, M. Gross, and B. Solenthaler, \\\"Deep fluids: A generative network for parameterized fluid simulations,\\\" Comput. Graph. Forum, vol. 38, no. 2, pp. 59\u201370, 2019.\\n\\nJ. W.-B. Lin and J. D. Neelin, \\\"Toward stochastic moist convective parameterization in general circulation models,\\\" Geophys. Res. Lett., vol. 30 (4), p. 1162, 2003.\\n\\nP. A. O'Gorman, \\\"Precipitation extremes under climate change,\\\" Current Clim. Change Rep., vol. 1, pp. 49\u201359, 2015.\\n\\nL. Zanna and T. Bolton, \\\"Data-driven equation discovery of ocean mesoscale closures,\\\" Geophys. Res. Lett., vol. 47, no. 17, p. e2020GL088376, 2020.\\n\\nA. Grundner, T. Beucler, P. Gentine, and V. Eyring, \\\"Data-driven equation discovery of a cloud cover parameterization,\\\" 2023. arxiv:2304.08063.\\n\\nI. Ebert-Uphoff, D. R. Thompson, I. Demir, Y. R. Gel, A. Karpatne, M. Guereque, V. Kumar, E. Cabral-Cano, and P. Smyth, \\\"A vision for the development of benchmarks to bridge geoscience and data science,\\\" in 17th International Workshop on Climate Informatics, 2017.\"}"}
