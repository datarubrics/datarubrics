{"id": "bxwWikAXSy", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Device Type | Samples |\\n|-------------|---------|\\n| Ink         |         |\\n| Google PixelBook | 51k     |\\n| Google Nexus 5X | 28k     |\\n| Coolpad Mega 2.5D | 14k   |\\n| OnePlus One   | 13k     |\\n| Google Nexus 5 | 11k     |\\n| Google Nexus 6 | 11k     |\\n| Google Nexus 6P | 11k     |\\n| Coolpad Mega 3  | 8k       |\\n| LG Optimus L9 | 8k       |\\n| Galaxy Grand Duos | 7k     |\\n| Google Pixel XL | 6k      |\\n| Samsung Galaxy S7 | 5k     |\\n\\nTable 7: Top-12 devices used, with the number of samples obtained from each device. The bias towards Google devices simply reflects the conditions in which inks were collected.\\n\\nFigure 14: Examples of various writing orders found in the training set. Red arrows show the movement of the pen between strokes. Top left: most common writing order (top-down, fraction bar drawn left-to-right), top right: fraction bar written first, bottom left: fraction bar drawn right-to-left, bottom right: fraction written bottom-up.\\n\\nH Sources of Noise\\n\\nThe result of any task performed by humans will contain mistakes, and MathWriting is no exception. We've done our best to remove most of the mistakes, but we know that some remain.\\n\\nStray strokes\\nThese do not carry any meaning and should be ignored by any recognizer. Since they also appear in real applications, there could be some benefit in having some in the dataset to teach the model about them. That said, it being usually easier to add noise rather than to remove it, we made the choice of discarding as many inks containing stray strokes as possible. Not all inks with stray strokes have been found and removed though (e.g. train/9e64be65cb874902.inkml that was discovered post-publication). The fraction of inks containing stray strokes is significantly lower than 1%, and should not be an issue for training a model.\\n\\nIncorrect ground truth\\nContributors did not always copy the prompt perfectly, leading to a variety of differences. In most of the cases we spotted, we were able to fix the label to match what had actually been written. A short manual review once the dataset was in its final state showed the rate of incorrect ground truth to be between 1% and 2%. Most of the mistakes are very minor, usually a single token added, missing or incorrect. Errors here also come from ambiguities or misuse of...\"}"}
{"id": "bxwWikAXSy", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the LATEX notation: expressions coming from Wikipedia contain some misuse like using $\\\\Sigma$ where $\\\\sum$ was more appropriate, $\\\\triangle$ instead of $\\\\Delta$, $\\\\triangledown$ instead of $\\\\nabla$, $\\\\begin{matrix}\\\\end{matrix}$ instead of $\\\\binom$, and also some handwriting-specific ambiguities like $\\\\dagger$ vs $\\\\top$ vs $T$. There are also some instances where reference numbers or extra punctuation are included.\\n\\nAggressive normalization\\n\\nWhile the above sources of noise are unavoidable, normalization is a postprocessing operation that can in theory be tweaked to perfection. In practice, it's a compromise between reducing accidental ambiguities (i.e. removing synonyms), and removing information.\\n\\nExamples: we made the choice of treating $\\\\binom$ as a synonym for a 2-element matrix. While it does improve recognition accuracy by making the problem easier, it also moves the burden of distinguishing between the two cases to downstream steps in the recognition pipeline. Similar things can be said about removing all commands that indicate that their content is text instead of math (e.g. $\\\\mbox$), dropping size modifiers, rewriting function commands (e.g. $\\\\sin$, $\\\\cos$), etc. Using a different normalization could prove beneficial depending on the context the recognizer is used in practice. However, for the purpose of a benchmark any reasonable compromise is adequate.\\n\\nI Tokenization Code\\n\\nPython code used in this work to tokenize LATEX mathematical expressions.\\n\\n```python\\nimport re\\n\\n_COMMAND_RE = re.compile(r'\\\\(mathbb\\\\{[a-zA-Z]\\\\}|begin{[a-z]+}|end{[a-z]+}|operatorname*|[a-zA-Z]+\\\\.)')\\n\\ndef tokenize_expression(s: str) -> list[str]:\\n    tokens = []\\n    while s:\\n        if s[0] == '\\\\':\\n            tokens.append(_COMMAND_RE.match(s).group(0))\\n        else:\\n            tokens.append(s[0])\\n        s = s[len(tokens[-1]):]\\n    return tokens\\n```\\n\\nJ Tokens\\n\\nUsing the above code to compute tokens, the set of all samples in the dataset (human-written, synthetic, from all splits) contain the following after normalization:\\n\\n- Syntactic tokens: _ ^ { } & \\\\ space\\n- Latin letters and numbers: a-z A-Z 0-9\\n- Blackboard capital letters $\\\\mathbb{A}$ - $\\\\mathbb{Z}$\\n- Latin punctuation and symbols: , ; : ! ? . ( ) [ ] \\\\{ \\\\} * / + - \\\\_ \\\\& \\\\# \\\\%\\n- Greek letters: $\\\\alpha \\\\beta \\\\delta \\\\Delta \\\\epsilon \\\\eta \\\\chi \\\\gamma \\\\Gamma \\\\iota \\\\kappa \\\\lambda \\\\Lambda \\\\nu \\\\mu \\\\omega \\\\Omega \\\\phi \\\\Phi \\\\pi \\\\Pi \\\\psi \\\\Psi \\\\rho \\\\sigma \\\\Sigma \\\\tau \\\\theta \\\\Theta \\\\upsilon \\\\Upsilon \\\\varphi \\\\varpi \\\\varsigma \\\\vartheta \\\\xi \\\\Xi \\\\zeta$\\n- Mathematical constructs: $\\\\frac \\\\sqrt \\\\prod \\\\sum \\\\iint \\\\int \\\\oint$\\n- Diacritics and modifiers - Note the absence of the single-quote character, which is normalized to $^\\\\prime$: $\\\\hat \\\\tilde \\\\vec \\\\overline \\\\underline \\\\prime \\\\dot \\\\not$\\n- Matrix environment: $\\\\begin{matrix}\\\\end{matrix}$\"}"}
{"id": "bxwWikAXSy", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This section shows a few examples of rendered inks, so that the reader can get a feel for the kind of data that is in MathWriting. All samples are from the training set. They have been manually picked to show a variety of sizes, characters and structures.\\n\\nK.1 Human-Written Samples\"}"}
{"id": "bxwWikAXSy", "page_num": 20, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "bxwWikAXSy", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "bxwWikAXSy", "page_num": 22, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "bxwWikAXSy", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "bxwWikAXSy", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MathWriting: A Dataset For Handwritten Mathematical Expression Recognition\\n\\nPhilippe Gervais\\npgervais@acm.org\\nAsya Fadeeva\\nfadeich@google.com\\nAndrii Maksai\\namaksai@google.com\\n\\nAbstract\\nRecognition of handwritten mathematical expressions allows to transfer scientific notes into their digital form. It facilitates the sharing, searching, and preservation of scientific information. We introduce MathWriting, the largest online handwritten mathematical expression dataset to date. It consists of 230k human-written samples and an additional 400k synthetic ones. This dataset can also be used in its rendered form for offline HME recognition. One MathWriting sample consists of a formula written on a touch screen and a corresponding LATEX expression. We also provide a normalized version of LATEX expression to simplify the recognition task and enhance the result quality. We provide baseline performance of standard models like OCR and CTC Transformer as well as Vision-Language Models like PaLI on the dataset. The dataset together with an example colab is accessible on Github.\\n\\n1 Introduction\\n\\nThree examples of HME from MathWriting. More examples can be found in Appendix K. Each ink is accompanied by a unique identifier that matches a corresponding filename in the dataset.\\n\\nMathWriting dataset (2.9 GB):\\nhttps://storage.googleapis.com/mathwriting_data/mathwriting-2024.tgz\\nAssociated code:\\nhttps://github.com/google-research/google-research/tree/master/mathwriting\\n\\nOnline text recognition models have improved a lot over the past years, because of improvements in model structure [1, 2, 3] and also because of an increase in the amount of training data [4, 5, 6].\\n\\nMathematical expression (ME) recognition is a challenging task that has received less attention than regular recognition of words and characters [7]. ME recognition is different from regular text recognition in a number of interesting ways which can prevent improvements from transferring from one to the other. Though MEs share with text most of their symbols, they follow a more rigid structure which is also two-dimensional, see Figure 1. Where text can be treated to some extent as a one-dimensional problem amenable to sequence modeling, MEs cannot because the relative position...\"}"}
{"id": "bxwWikAXSy", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of symbols in space is meaningful. It is also different from symbol segmentation or object detection because the output of a recognizer has to contain the relationship between symbols, serialized in some form (LaTeX, a graph, InkML, etc.). Similarly to the case of text, handwritten MEs (HME) are more difficult to recognize than printed ones as they are more ambiguous and less training data is available.\\n\\nHandwritten data is costly to obtain as it must be written by hand, which is compounded in the case of online representation (ink) by the necessity to use dedicated hardware (touchscreen, digital pen, etc.). By publishing the MathWriting dataset, we hope to alleviate some of the needs for data for research purposes. Samples include a large number of human-written inks, as well as synthetic ones. MathWriting can readily be used with other online datasets like CROHME [8] or Detexify [9] - we publish the data in InkML format to facilitate this. It can also be used for offline ME recognition simply by rasterizing the inks, using code provided on the Github page. MathWriting is the largest set of online HME published so far - both human-written and synthetic. It significantly expands the set of symbols covered by CROHME [8], enabling more sophisticated recognition capabilities. Since inks can be rasterized, MathWriting can also been seen as larger than existing offline HME datasets [10, 11, 12]. For these reasons we introduce a new benchmark, applicable to both online and offline ME recognition.\\n\\nThis work's main contributions are:\\n\\n\u2022 a large dataset of Handwritten Mathematical Expressions under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\\n\\n\u2022 LaTeX ground truth expressions in normalized form to simplify training and to make evaluation more robust.\\n\\n\u2022 Evaluation of different models like CTC Transformer and PaLI on the dataset to show what recognition quality could be achieved with the provided data.\\n\\nThe paper focuses on the high-level description of the dataset: creation process, postprocessing, train/test split, ground truth normalization, statistics, and a general discussion of the dataset content to help practitioners understand what can and cannot be achieved with it. All the low-level technical information like file formats can be found in the readme.md file present at the root of the dataset archive linked above. We also provide code examples on Github, to show how to read the various files, process and rasterize the inks, and tokenize the LaTeX ground truth.\\n\\n2 Dataset Creation\\n\\nMathWriting dataset primarily consists of LaTeX expressions from Wikipedia, more details about the acquisition of expressions are provided in Appendix B. These expressions were used for both ink collection from human contributors Section 2.1 as well as synthetic data generation Section 2.2. We did a very limited filtering of very noisy human-written examples (described in Appendix C).\\n\\n2.1 Ink Collection\\n\\nInks were obtained from human contributors through an in-house Android app. Participants agreed to the standard Google terms of use and privacy policy. The task consisted in copying a rendered mathematical expression (prompt) shown on the device's screen using either a digital pen or a finger on a touch screen. Mathematical expressions used as prompt were first obtained in LaTeX format, then rendered into a bitmap through the LaTeX compiler (see Appendix A for the template used). 95% of MathWriting expressions were obtained from Wikipedia. The remaining ones were generated to cover underrepresented cases in Wikipedia, like isolated letters with nested sub/superscripts or complicated fractions (see Section B). Contributors were hired internally at Google. 6 collection campaigns were run between 2016 and 2019, each lasting between 2 to 3 weeks. Collected data contains only inks and labels, so no personally identifiable information is present in the dataset. Offensive content is highly unlikely because LaTeX expressions were taken from Wikipedia and we conducted a filtering of noisy data (described in Appendix C).\"}"}
{"id": "bxwWikAXSy", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.2 Synthetic Samples and Isolated Symbols\\n\\nWe created synthetic samples in order to further increase the label diversity for training. This also enabled compensating for limitations of the human collection like the maximum length of the expressions, which were limited by the size of the screen they were written on. We used LaTeX expressions from Wikipedia that were not used in the data collection. The resulting synthetic data has a 90th percentile of expression length of 68 characters, compared to 51 in train. This is especially important as deep neural nets often fail to generalize to inputs longer than their training data [13, 14]. Using synthetic long inks together with the original human-written inks can help to eliminate that problem as shown in [15, 16]. The synthesis technique is as follows: starting from a raw LaTeX mathematical expression, we computed a DVI file using the LaTeX compiler, from which we extracted bounding boxes. We then used those bounding boxes to place handwritten individual symbols, resulting in a complete expression. See Figure 1 for an example of extracted bounding boxes and the resulting synthetic example.\\n\\nFigure 1: An example of a synthetic ink created from bounding boxes with label \\\\(((p+q)+(p-q))/2=q\\\\). Inks for individual symbols are all from the symbols split. They have been manually extracted from inks in train. For each symbol that we wanted to support, we manually selected strokes corresponding to it for 20-30 distinct occurrences in train, and used that information to generate a set of individual inks. Similar synthesis techniques have been used by [8] with inks, [10] and [12] with raster images.\\n\\nA significant difference between synthetic and human-written inks is the stroke order. For synthetic inks, stroke order follows the order of the bounding boxes in the DVI file, which can be different from the usual order of writing for mathematical expressions. However, the writing order within a given symbol is consistent with human writing.\\n\\n2.3 Dataset Split\\n\\nMathWriting is composed of five different sets of samples, which we call 'splits': train, valid, test, symbols, and synthetic. The splits train, valid, and test consist only of human-written examples. The split symbols is provided for synthetic data generation and is not used in training. The split of human-written samples between train, valid, and test was partially done based on writers, partially based on labels. More details are provided in Appendix D. Experiments have shown that a more important factor than the handwriting style was whether the label had already been seen during training. This fact is also supported by research in the area of compositional generalization [17]. In the published version, valid has a 55% (8.5k samples) intersection with train based on unique normalized labels, and test has an 8% intersection (647 samples). We chose to have a low intersection between train and test in order to correctly measure generalization of trained models to unseen labels.\\n\\n2.4 Label Normalization\\n\\nAll samples in the dataset come with two labels: the LaTeX expression that was used during the data collection (annotation label in the InkML files), and a normalized version of it meant for model training, which is free from a few sources of confusions for an ML model (annotation normalizedLabel). An example with original and normalized labels is provided in Figure 2. Label normalization covers three main categories (details are provided in Appendix E):\"}"}
{"id": "bxwWikAXSy", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"51b364eb9ba2185a\\n\\nFigure 2: An example from the train split, with its labels:\\n\\nRaw: \\\\[ f(x) = \\\\frac{1}{e} \\\\cdot \\\\sum_{n=0}^{\\\\infty} \\\\frac{n^x}{n!} \\\\]\\n\\nNormalized: \\\\[ f(x) = \\\\frac{1}{e} \\\\cdot \\\\sum_{n=0}^{\\\\infty} \\\\frac{n^x}{n!} \\\\]\\n\\n\u2022 variations used in print that can't be reproduced in handwriting - e.g. bold, italic - or that haven't been reproduced consistently by contributors.\\n\\n\u2022 non-uniqueness of the \\\\text{TEx} syntax. e.g. \\\\[ \\\\frac{1}{2} \\\\] and \\\\[ \\\\frac{1}{2} \\\\] are equivalent.\\n\\n\u2022 visual variations that can be reproduced in handwriting but can't reliably be inferred by a model. This includes size modifiers like \\\\( \\\\left, \\\\right \\\\).\\n\\nWe provide the raw labels to make it possible to experiment with alternative normalization schemes, which could lead to better outcomes for different applications.\\n\\n2.4.1 Limitations of normalization\\n\\nThe normalization process is purely syntactic, and cannot cover cases where the meaning of the expression has to be taken into account. For example, a lot of expressions from Wikipedia use \\\\( \\\\cos \\\\) instead of \\\\( \\\\cos \\\\). It is often clear to a human reader whether the sequence of characters c,o,s represents the \\\\( \\\\cos \\\\) command or simply three letters. However, this cannot be reliably inferred by a syntactic parser, for example in \\\\( \\\\text{ta} \\\\cos \\\\) vs \\\\( \\\\text{tacos} \\\\). An alternative would be to update the raw labels, which we didn't do because we wanted to keep the information that was used during the collection as untouched as possible. Similarly, cases like \\\\( 10^{-1} \\\\) usually mean \\\\( \\\\{10\\\\}^{-1} \\\\), though they render exactly the same. We made the choice to normalize to the former because it's the only option with a purely syntactic normalizer. It's also better than not removing these extra braces because it gives more consistent label structures, which simplifies the model training problem.\\n\\n3 Dataset Statistics\\n\\nIn this section we describe the key characteristics of MathWriting and compare it to CROHME23 [8]. In Table 1 we provide the information about the volume of the dataset splits both in terms of examples (inks) and unique labels.\\n\\n|            | train | synthetic | valid | test |\\n|------------|-------|-----------|-------|------|\\n| # distinct inks | 230k  | 396k      | 16k   | 8k   |\\n| # distinct labels | 53k   | 396k      | 8k    | 4k   |\\n\\n3.1 Label Statistics\\n\\nMathWriting contains 457k unique labels after normalization (see Section 2.4). From Table 1 we see that most unique expressions are covered by the synthetic portion of the dataset. However, the absolute number of unique expressions in human-written part is still high \u2013 61k. This underlines the importance of synthetic data as it allows models to see a much bigger variety of expressions. It is important to note that the synthetic split has essentially no repeated expressions. On the other hand, in real data multiple different writings of the same expression are quite common (see Figure 1 in Appendix F). This fact allows us to separately evaluate model's quality on expressions that were\"}"}
{"id": "bxwWikAXSy", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"observed during training and that those that hadn\u2019t. As seen in Table 2 the biggest intersection in\\nexpressions is between \\\\textit{valid} and \\\\textit{train}. The minimal overlap between \\\\textit{test} and \\\\textit{train} splits is\\nbeneficial for assessing a model\u2019s ability to generalize to expressions that were not seen in \\\\textit{train}.\\n\\nTable 2: Counts of unique labels shared between MathWriting splits\\n\\n|              | train | synthetic | valid | test |\\n|--------------|-------|-----------|-------|------|\\n| train        | 0     | 3.6k      | 355   | -    |\\n| synthetic    | 0     | -         | 0     | -    |\\n| valid        | 3.6k  | 0         | -     | 239  |\\n| test         | 355   | 0         | 239   | -    |\\n\\nThe median length of expressions in characters is 26 which is comparable to one of the most popular\\nEnglish recognition datasets IAMonDB \\\\cite{18} which has median of 29 characters. However, it is\\nimportant to note that LATEX expressions have tokens that span multiple characters like \\\\( \\\\frac{}{} \\\\). The\\nmedian length of expressions in tokens (provided in Appendix J) is 17, thus making training a model\\non tokens rather than characters easier due to shorter target lengths \\\\cite{19,20}. We want to emphasize\\nthat MathWriting can be used with a different tokenization scheme and token vocabulary from what\\nwe propose in Appendix J. In Figure 3 we show the number of occurrences for the most frequent\\ntokens. Tokens \\\\{\\\\textit{\\\\_\\\\_\\\\_} and \\\\textit{\\\\_\\\\_\\\\_}\\\\} are by far the most frequent as they are integral to the LATEX syntax.\\n\\nFigure 3: Histogram of the top-100 most frequent tokens in MathWriting.\\n\\n3.2 Ink Statistics\\n\\nEach ink in MathWriting dataset is a sequence of strokes \\\\( I = [s_0, \\\\ldots, s_n] \\\\), each stroke \\\\( s_i \\\\) consisting\\nof points. A point is represented as a triplet \\\\((x, y, t)\\\\) where \\\\( x \\\\) and \\\\( y \\\\) are coordinates on the screen and \\\\( t \\\\)\\nis a timestamp. In Table 3 we provide statistics on number of strokes, points, and duration of writing.\\nIt\u2019s important to note that as inks were collected on different devices, the absolute coordinate values\\ncan vary a lot. In human-written data the time information \\\\( t \\\\) always starts from 0 but it is not always\\nthe case in the \\\\textit{synthetic} split. Different samples often have different sampling rates (number of\\npoints written in one second) due to the use of different devices (see Figure 4). More details in\\nSection 3.3. Consequently, the same ink written on two different devices can result in inks with a\\ndifferent number of points. For human-written inks, the sampling rate is consistent between strokes,\\nbut it is not the case for synthetic ones. In order to accommodate a model and make sequences shorter,\\ninks can be resampled in time (see example in Figure 13, Appendix F).\\n\\nTable 3: Ink statistics for MathWriting.\\n\\n|            | 10th percentile | median | 90th percentile |\\n|------------|-----------------|--------|-----------------|\\n| # strokes  | 5               | 14     | 39              |\\n| # points   | 131             | 350    | 1069            |\\n| writing time (sec) | 1.88    | 6.03    | 16.42           |\\n| aspect ratio | 1.32             | 3.53    | 9.85            |\"}"}
{"id": "bxwWikAXSy", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Left: an ink with very low sampling rate (9.4 points per second) Right: an ink with very high sampling rate (260 points per second)\\n\\nTable 4: Counts of inks, distinct labels and distinct tokens used in MathWriting and CROHME23. The single token present in CROHME23 but not in MathWriting is the literal dollar sign $.\\n\\n|          | MathWriting | CROHME23 | Common |\\n|----------|-------------|----------|--------|\\n| Inks     | 650k        | 164k     | 0      |\\n| Labels   | 457k        | 102k     | 47k    |\\n| Vocab    | 254         | 105      | 104    |\\n\\nTable 5: Count of human-written and synthetic inks for MathWriting and CROHME23. Human-written inks represent 38% of the total for MathWriting, and 10% for CROHME23.\\n\\n|          | MathWriting | CROHME23 |\\n|----------|-------------|----------|\\n| Human    | 253k        | 17k      |\\n| Synthetic| 396k        | 147k     |\\n\\n3.3 Devices Used\\n\\nAround 150 distinct device types have been used by contributors. In most cases inks were written on smartphones using a finger on a touchscreen. However, there are cases where tablets with styluses were used. The main device used in this case is Google Pixelbook, which accounted for 51k inks total (see Table 7, Appendix F). Out of all device types, 37 contributed more than 1000 inks. Note that writing on a touchscreen with a finger or a stylus results in different low-level artifacts. All devices were running the same Android application for ink collection, regardless of whether their operating system was Android or ChromeOS.\\n\\n3.4 Comparison With CROHME23\\n\\nIn this section we compare main dataset statistics of MathWriting and CROHME23 as it is a popular publicly available dataset for HME recognition. In terms of overall size, MathWriting has nearly 3.9 times as many samples and 4.5 times as many distinct labels after normalization, see Table 4. A significant number of labels can be found in both datasets (47k), but the majority is dataset-specific. This suggests that combining both datasets during training could yield improved HME recognition quality. MathWriting has more human-written inks than CROHME23 as seen in Table 5, and contains a much larger variety of tokens. It has 254 distinct tokens including all Latin capital letters and almost the entire Greek alphabet. It also contains matrices, which are not included in CROHME23. Therefore, more scientific fields like quantum mechanics, differential calculus, and linear algebra can be represented using MathWriting.\\n\\n4 Experiments\\n\\n4.1 Evaluation setup\\n\\nWe propose the following evaluation setup based on MathWriting for the quality of handwriting math expression recognition.\\n\\n- **evaluation samples**: the test split of MathWriting.\\n- **metric**: character error rate (CER) [21], where a \u201ccharacter\u201d is a \\\\( \\\\LaTeX \\\\) token as defined by the code in Appendix I.\"}"}
{"id": "bxwWikAXSy", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We provide a reference implementation of the evaluation metric at the Github page [2]. We propose the use of CER as a metric to make results comparable to other recognition tasks like text recognition [22, 23], and the use of LaTeX tokens instead of ASCII characters so that an error on a single non-latin letter (e.g. \\\\alpha recognized as a) counts as one instead of many.\\n\\n4.2 Baseline Recognition Models\\n\\nIn Table 6 we provide results for different models. All models are trained exclusively on the MathWriting dataset (train and synthetic), except for the OCR API that was trained on other datasets as well. The following models represent different approaches to handwriting recognition \u2013 offline [23], online [24] and mixed [25].\\n\\nOCR\\nThis is a publicly available Document AI OCR API [26], which processes bitmap images. It has been trained partly on samples from MathWriting. We sent inks rendered with black ink on a white background and searched for optimal image size and stroke width to get the best evaluation result from the model.\\n\\nCTC Transformer\\nThis model is a transformer base with a Connectionist Temporal Classification loss on top (CTC) [27]. It contains 11 transformer layers with an embedding size of 512. We used swish activation function and dropout of 0.15 as those parameters performed best on valid. We train with an Adam optimizer, learning rate of 1e-3, batch size 256 for 100k steps. One training run took 4 hours on 4 TPU v2. We trained from scratch and exclusively on MathWriting (train and synthetic). The model is similar to [24], replacing LSTM layers by Transformer layers and not using any external language model on top.\\n\\nVLM\\nWe fine-tuned a large Vision-Language Model PaLI [28] on MathWriting (train and synthetic). We used the representation proposed in [25] where an ink is represented as both a sequence of points (similar to CTC Transformer) and its rasterized version (similar to OCR). We train three models with different data shuffling for 200k steps with batch size 128, learning rate 0.3 and dropout 0.2. One training run took 14 hours on 16 TPU v5p. Models were finetuned exclusively on train and synthetic MathWriting data. Overall, it took 2 TPU v2 days and 28 TPU v5p days to run the experiments.\\n\\n| Model          | Input Parameters | CER on valid | CER on test |\\n|----------------|------------------|--------------|-------------|\\n| OCR [26]       | Image            | 6.50         | 7.17        |\\n| CTC Transformer [25] | Ink 35M          | 4.52 (0.08)  | 5.49 (0.05) |\\n| PaLI [25]      | Image+Ink        | 700M         | 4.47 (0.08) | 5.95 (0.06) |\\n\\nTable 6: Recognition results for different models. The evaluation metric is reported on both the valid and test splits.\\n\\nTable 6 shows the evaluation comparison between the three models. The OCR model has no information about the order of writing and speed (offline recognition), which explains its lower performance than methods that take time information into account (online recognition). The two other methods \u2013 PaLI and CTC Transformer perform significantly better than OCR. These results show that our dataset can be used to train classical recognition models like CTC transformer as well as more recent architectures like VLM.\\n\\nFigure 5 shows examples of model mistakes. Two of the main causes of mistakes are confusing similar-looking characters like \u201cz\u201d and \u201c2\u201d, and errors in the structural arrangement of the characters, for instance not placing a sub-expression in a subscript or superscript.\\n\\n5 Discussion\\n\\n5.1 Differences in Writing Style\\n\\nThe number of contributors was large enough that a variety of writing styles are represented in the dataset. An example for different ways of writing letter \u2018r\u2019 can be seen in Figure 6. Additional\"}"}
{"id": "bxwWikAXSy", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Examples of recognition mistakes from the CTC Transformer model. We observe similar mistakes from the other models.\\n\\nFigure 6: Three ways of writing a lowercase \u2018r\u2019.\\n\\nExamples are provided in Figure 7. Similar though less obvious differences exist for other letters.\\n\\n5.2 Recognition Challenges\\n\\nMathWriting presents some inherent recognition challenges, which are typical of handwritten representations. For example, it\u2019s not really possible to distinguish these pairs from the ink alone: \\\\( \\\\frac{\\\\underline{a}}{b} \\\\) vs \\\\( \\\\frac{a}{\\\\overline{b}} \\\\), and \\\\( \\\\overline{\\\\omega} \\\\) vs \\\\( \\\\varpi \\\\).\\n\\nWe\u2019d like to point out that these ambiguities are not an issue for humans in practice, because they rely on contextual information to disambiguate: a particular writing idiosyncrasy, consistency with nearby expressions, knowledge of the scientific domain, etc. See Figures 8 and 9 for more examples.\\n\\n5.3 Dataset Applications and Future Work\\n\\nMathWriting can be used to train recognizers for a large variety of scientific fields, and is also large enough to enable synthesis of mathematical expressions. Combining it with other large datasets like CROHME would increase the variety of samples even further, both in terms of writing style and number of expressions, likely improving the performance of a model.\\n\\nBounding box information for synthetic samples together with individual symbols are provided to enable experimentation with synthetic ink generation. Synthetic samples were generated through the straightforward process of pasting inks of individual symbols (symbols) exactly where bounding boxes were located. This gives synthetic samples a very regular structure, see Figure 1. It is possible...\"}"}
{"id": "bxwWikAXSy", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Left: character ambiguity. Is it $1 \\\\leq x_n < x_{n+1}$ or $1 \\\\leq n_\\\\eta < n_{\\\\eta+1}$? Right: what is the fraction nesting order?\\n\\nFigure 9:\\nLeft: $\\\\binom{\\\\binom{2}{2}}{2}$ or 2-element matrix? Right: $p_{na}^n = a$ or $p_{na}^n = a$?\\n\\nto improve this process by modifying the location, size or orientation of bounding boxes prior to generating the synthetic inks. This would soften L\\\\textsc{A}\\\\TeX's rigid structure and make synthetic data closer to human handwriting. Another application of these bounding boxes would be to bootstrap a recognizer that would also return character segmentation information. This kind of output is critical for some UI features - for example, editing an handwritten expression.\\n\\nMathWriting can also be improved by varying the label normalization. Changing it can have different benefits depending on the application, as mentioned above. We provide the source L\\\\textsc{A}\\\\TeX string for that reason. Another possible improvement in recognition can come from additional contextual information, for instance the scientific field [29] that can be added post-hoc. Combining recognizers with a language model [24] trained on a large set of mathematical expressions would be a step in a similar direction.\\n\\n6 Limitations\\nA single sample in MathWriting dataset has one handwritten L\\\\textsc{A}\\\\TeX formula, see Figure 2. As a result, models that are trained on this dataset would probably perform poorly on complete handwritten documents, such as the IAMonDo dataset [30]. Also, as the dataset contains only L\\\\textsc{A}\\\\TeX expressions, it is unlikely that models trained on it will accurately recognize handwritten text in English or other languages. As shown in Figure 3, some L\\\\textsc{A}\\\\TeX tokens are way more frequent than others. Some infrequent tokens like $\\\\ni$ could be hard to recognise.\\n\\n7 Conclusion\\nWe introduced MathWriting, the largest dataset of online handwritten mathematical expressions to date, together with the experimental results of three different types of models. We hope this dataset will help advance research in both online and offline mathematical expression recognition. Additionally, we invite data practitioners to build on the dataset. We intentionally chose a file format for MathWriting close to the one used by CROHME to facilitate their combined use. We also provided original or intermediate representations (raw L\\\\textsc{A}\\\\TeX strings, bounding boxes) to enable experimentation with the data itself, and suggested a few directions (Section 5.3).\"}"}
{"id": "bxwWikAXSy", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Alex Graves, Marcus Liwicki, Santiago Fern\u00e1ndez, Roman Bertolami, Horst Bunke, and J\u00fcrgen Schmidhuber. A novel connectionist system for unconstrained handwriting recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31:855\u2013868, 2009.\\n\\n[2] Johannes Michael, Roger Labahn, Tobias Gr\u00fcning, and Jochen Z\u00f6llner. Evaluating sequence-to-sequence models for handwritten text recognition. 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1286\u20131293, 2019.\\n\\n[3] Yutian Liu, Wenjun Ke, and Jianguo Wei. Attention guidance mechanism for handwritten mathematical expression recognition, 2024.\\n\\n[4] Emre Aksan, Fabrizio Pece, and Otmar Hilliges. Deepwriting: Making digital ink editable via deep generative modeling, 2018.\\n\\n[5] Harold Mouch\u00e8re, Christian Viard-Gaudin, Richard Zanibbi, and Utpal Garain. Icfhr 2014 competition on recognition of on-line handwritten mathematical expressions (crohme 2014). volume 2014, 09 2014.\\n\\n[6] Harold Mouch\u00e8re, Christian Viard-Gaudin, Richard Zanibbi, and U. Garain. Icfhr2016 crohme: Competition on recognition of online handwritten mathematical expressions. In 2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR), pages 607\u2013612, 2016.\\n\\n[7] May Mowaffaq AL-Taee, Sonia Ben Hassen Neji, and Mondher Frikha. Handwritten recognition: A survey. 2020 IEEE 4th International Conference on Image Processing, Applications and Systems (IPAS), pages 199\u2013205, 2020.\\n\\n[8] Yejing Xie, Harold Mouch\u00e8re, Foteini Liwicki, Sumit Rakesh, Rajkumar Saini, Masaki Nakagawa, Cuong Nguyen, and Thanh-Nghia Truong. ICDAR 2023 CROHME: Competition on Recognition of Handwritten Mathematical Expressions, pages 553\u2013565. 08 2023.\\n\\n[9] Detexify data. https://github.com/kirel/detexify-data.\\n\\n[10] Yuntian Deng, Anssi Kanervisto, Jeffrey Ling, and Alexander M. Rush. Image-to-markup generation with coarse-to-fine attention, 2017.\\n\\n[11] Ye Yuan, Xiao Liu, Wondimu Dikubab, Hui Liu, Zhilong Ji, Zhongqin Wu, and Xiang Bai. Syntax-aware network for handwritten mathematical expression recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4553\u20134562, 2022.\\n\\n[12] Aida calculus math handwriting recognition dataset. https://www.kaggle.com/datasets/aidapearson/ocr-data.\\n\\n[13] Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models, 2022.\\n\\n[14] Dusan Varis and Ond\u0159ej Bojar. Sequence length is a domain: Length-based overfitting in transformer models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2021.\\n\\n[15] Aleksandr Timofeev, Anastasiia Fadeeva, Andrei Afonin, Claudiu Musat, and Andrii Maksai. DSS: Synthesizing Long Digital Ink Using Data Augmentation, Style Encoding and Split Generation, page 217\u2013235. Springer Nature Switzerland, 2023.\\n\\n[16] Arun Narayanan, Rohit Prabhavalkar, Chung-Cheng Chiu, David Rybach, Tara N. Sainath, and Trevor Strohman. Recognizing long-form speech using streaming end-to-end models, 2019.\\n\\n[17] Daniel Keysers, Nathanael Sch\u00e4rli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, and Olivier Bousquet. Measuring compositional generalization: A comprehensive method on realistic data, 2020.\"}"}
{"id": "bxwWikAXSy", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"M. Liwicki and H. Bunke. IAM-OnDB\u2014an on-line English sentence database acquired from handwritten text on a whiteboard. In ICDAR\u201905. IEEE, 2005.\\n\\nDusan Varis and Ond\u0159ej Bojar. Sequence length is a domain: Length-based overfitting in transformer models. pages 8246\u20138257, 01 2021.\\n\\nMasato Neishi and Naoki Yoshinaga. On the relation between position information and sentence length in neural machine translation. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 328\u2013338, Hong Kong, China, November 2019. Association for Computational Linguistics.\\n\\nJohannes Michael, Roger Labahn, Tobias Gr\u00fcning, and Jochen Z\u00f6llner. Evaluating sequence-to-sequence models for handwritten text recognition, 2019.\\n\\nGeorge Retsinas, Giorgos Sfikas, Basilis Gatos, and Christophoros Nikou. Best practices for a handwritten text recognition system, 2024.\\n\\nDmitrijs Kass and Ekta Vats. Attentionhtr: Handwritten text recognition based on attention encoder-decoder networks, 2022.\\n\\nVictor Carbune, Pedro Gonnet, Thomas Deselaers, Henry A. Rowley, Alexander Daryin, Marcos Calvo, Li-Lun Wang, Daniel Keysers, Sandro Feuz, and Philippe Gervais. Fast multi-language lstm-based online handwriting recognition, 2020.\\n\\nAnastasiia Fadeeva, Philippe Schlattner, Andrii Maksai, Mark Collier, Efi Kokiopoulou, Jesse Berent, and Claudiu Musat. Representing online handwriting for recognition in large vision-language models, 2024.\\n\\nGoogle Cloud. Detect handwriting in image, 2023.\\n\\nAlex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, and J\u00fcrgen Schmidhuber. Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd International Conference on Machine Learning, ICML \u201906, page 369\u2013376, New York, NY, USA, 2006. Association for Computing Machinery.\\n\\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali: A jointly-scaled multilingual language-image model, 2023.\\n\\nMark Collier, Rodolphe Jenatton, Efi Kokiopoulou, and Jesse Berent. Transfer and marginalize: Explaining away label noise with privileged information. In International Conference on Machine Learning, 2022.\\n\\nEmanuel Inderm\u00fchle, Marcus Liwicki, and Horst Bunke. IAMOnDAn: An online handwritten document database with non-uniform contents. pages 97\u2013104, 06 2010.\\n\\nChecklist\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? [Yes] We claim to provide a large dataset of HME \u2013 see Section 1, together with normalized labels \u2013 the process is described in Section 2.4 and example is provided in Figure 2. Experimental results on this dataset are presented in Section 4.2.\\n   (b) Did you describe the limitations of your work? [Yes] We described general limitations of MathWriting dataset in Section 6, limitations of label normalization in Section 2.4.1, recognition challenges of mathematical expressions in Section 5.2 and sources of noise in the dataset in Section H.\"}"}
{"id": "bxwWikAXSy", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(c) Did you discuss any potential negative societal impacts of your work? [N/A] The type of the dataset we are publishing is not new, there are similar datasets like CROHME23[8]. Given the widespread use of handwriting recognition, we don\u2019t see any potential negative impacts of our work.\\n\\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] All the participants were paid the minimum hourly rate as discussed in Section 2.1. The dataset doesn\u2019t include any personal information about contributors apart from their handwriting samples that they agreed to share.\\n\\n2. If you are including theoretical results...\\n\\n(a) Did you state the full set of assumptions of all theoretical results? [N/A] Our paper doesn\u2019t include any theoretical results.\\n\\n(b) Did you include complete proofs of all theoretical results? [N/A] Our paper doesn\u2019t include any theoretical results.\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n\\n(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [No] All experiments in this paper are conducted using publicly available datasets. We provide code in Github for ink rasterisation, CER computation and expression tokenization. The Visual-Language Model PaLI used in the experiments is non-open-sourced, so full results from Table 6 cannot be reproduced.\\n\\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] We specify the training details like number of parameters, learning rate, dropout, training data, etc. for the models \u2013 CTC transformer and PaLI in Section 4.2.\\n\\n(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] In Section 4.2 we report average and variance of three training runs with different shuffling of training data.\\n\\n(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] In Section 4.2 we provide the total number of TPU days it took to run our experiments.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n\\n(a) If your work uses existing assets, did you cite the creators? [Yes] We used pretrained PaLI model for finetuning and cited [28].\\n\\n(b) Did you mention the license of the assets? [N/A] As the PaLI model is non-open-sourced there is no license for it.\\n\\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes] URLs to the dataset and code are provided in Section 1.\\n\\n(d) Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? [Yes] We discussed the conditions of data collection in Section 2.1.\\n\\n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] We mention in Section 2.1 that there is no personally identifiable information present in the dataset and offensive content is highly unlikely given the nature of the dataset.\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] We describe the instructions of the data campaigns in Section 2.1 as they are quite simple \u2013 to write a rendered expression provided on the screen.\\n\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] The paper does not involve research with human subjects.\\n\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes] We write about the employment of participants in Section 2.1, we don\u2019t disclose the exact amount of compensation as it is confidential information.\"}"}
{"id": "bxwWikAXSy", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix\\n\\nA LaTeX template for label rendering\\n\\nAll the packages and definitions that are required to compile all the normalized and raw labels:\\n\\n\\\\usepackage{amsmath}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\newcommand{\\\\R}{\\\\mathbb{R}}\\n\\\\newcommand{\\\\C}{\\\\mathbb{C}}\\n\\\\newcommand{\\\\Q}{\\\\mathbb{Q}}\\n\\\\newcommand{\\\\Z}{\\\\mathbb{Z}}\\n\\nB Acquisition of LaTeX Expressions\\n\\nThe labels we publish mostly come from Wikipedia (95% of all samples have labels from Wikipedia). A small part were generated, to cover deeply nested fractions, number-heavy expressions, and isolated letters with nested superscripts and subscripts, which are rare in Wikipedia.\\n\\nThe extraction process from Wikipedia followed these steps:\\n\\n\u2022 download an XML Wikipedia dump which provides Wikipedia's raw textual content. enwiki-20231101-pages-articles.xml was used for synthetic samples, older dumps for human-written ones\\n\u2022 extract all LaTeX expressions from that file. This gives the list of all mathematical expressions in LaTeX notation from Wikipedia\\n\u2022 keep those which could be compiled using the packages listed in Appendix A. Wikipedia contains a significant number of expressions that are not accepted by the LaTeX compiler, because of syntax errors or other reasons\\n\u2022 keep only those which can be processed by our normalizer which only supports a subset of all LaTeX commands and structures\\n\\nFor expressions used for synthesis, the following extra steps were performed:\\n\\n\u2022 keep only the expressions whose normalized form contains more than a single LaTeX token. Example: \\\\( \\\\alpha \\\\) is rejected but \\\\( \\\\alpha^{2} \\\\) is kept. This step is useful to eliminate trivial expressions that wouldn't add any useful information\\n\u2022 de-duplicate expressions based on their normalized form. e.g. \\\\( \\\\frac{1}{2} \\\\) and \\\\( \\\\frac{1}{2} \\\\) normalize to the same thing, we kept only one of them in raw form\\n\u2022 restrict the list of expressions to the same set of tokens used in the train split: if the normalized form of an expression contained at least one token that was not also present somewhere in train, it was discarded.\\n\\nC Postprocessing of MathWriting dataset\\n\\nWe applied no postprocessing to the collected inks other than dropping entirely those that were completely unreadable or had stray marks. Inks are provided in their original form, as they were recorded with the collection app. What we did not do was to discard samples that were very hard to read or ambiguous, because we believe this type of sample to be essential in training a high-quality model.\\n\\nSome cleanup was performed on the labels (ground truths). The goal was to make the dataset better suited to training an ML model, and eliminate unavoidable issues that occurred during the collection.\\n\\nAfter training some initial models, we manually reviewed samples for which they performed poorly. This helped identify a lot of unusable inks (near-blank, lots of stray strokes, scribbles, etc.), and...\"}"}
{"id": "bxwWikAXSy", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a lot of ink/label discrepancies. A fairly common occurrence was a contributor forgetting to copy part of the prompted expression. We adjusted the label to what was actually written unless the ink contained a partially-drawn symbol, in which case we discarded the sample entirely. In this process we eliminated or fixed around 20k samples.\\n\\nThe most important postprocessing step was to normalize the labels: there are many different ways to write a mathematical expression in LATEX format that will render to images that are equivalent in handwritten form. We applied a series of transformations to eliminate as many variations as possible while retaining the same semantic. This greatly improved the performance of models and made their evaluation more precise. We publish both the normalized and raw (unnormalized) labels, to enable people to experiment with other normalization procedures.\\n\\nThis normalization is similar to what [10] did, but pushed further because of the specifics of handwritten MEs. See Section 2.4 for more detail.\\n\\nD Dataset split\\n\\nThe valid and test splits are the result of multiple operations performed between 2016 and 2019. The first split operation, performed on the data available in 2016, was based on the contributor id: any given contributor's samples would not appear in more than one split (either train, valid, test).\\n\\nThis is common practice for handwriting recognition systems, to test how the recognizer performs on unseen handwriting styles.\\n\\nExperiments then showed that a more important factor than the handwriting style was whether the label had already been seen during training. Subsequent data collection campaigns focused on increasing label variety, and new samples were added to valid and test, this time split by label: a given normalized mathematical expression would not appear in more than one split.\\n\\nE Label Normalization\\n\\nE.1 Syntactic Variations\\n\\nThere are several ways to change a LATEX string without changing the rendered output significantly. The normalization we implemented does the following:\\n\\n\u2022 all unnecessary space is dropped\\n\u2022 all command arguments are consistently put in curly braces\\n\u2022 superscripts and subscripts are put in curly braces and their order is normalized. e.g. \\\\(a^2_1\\\\) becomes \\\\(a_{1}^{2}\\\\).\\n\u2022 redundant braces are dropped\\n\u2022 infix commands are replaced by their prefix versions. e.g. \\\\(\\\\over\\\\) is replaced by \\\\(\\\\frac\\\\)\\n\u2022 a lot of synonyms are collapsed. e.g. \\\\(\\\\le\\\\) and \\\\(\\\\leq\\\\), \\\\(\\\\longrightarrow\\\\) and \\\\(\\\\rightarrow\\\\), etc. Some of the synonyms are only synonyms in handwriting. For example \\\\(\\\\star\\\\) (\u22c6) and \\\\(\\\\ast\\\\) are different in print (5-prong and 6-prong stars), but the difference was not expressed in handwriting by our contributors.\\n\u2022 functions commands like \\\\(\\\\sin\\\\) are replaced by the sequence of letters of the function name (e.g. \\\\(\\\\sin\\\\) is replaced by \\\\(\\\\text{sin}\\\\)). This reduces the output vocabulary, and eliminates a source of confusion because we found that LATEX expressions from Wikipedia come with a mix of function commands and sequences of letters.\\n\u2022 expansion of abbreviations. e.g. \\\\(\\\\cdots\\\\), \\\\(\\\\ldots\\\\), etc. have been replaced by the corresponding sequence of characters.\\n\u2022 matrix environments are normalized to use only the 'matrix' environment surrounded by the proper delimiters like brackets or parentheses.\\n\u2022 \\\\(\\\\binom\\\\) is turned into a 2-element column matrix. Expressions from Wikipedia did not use those consistently, so we made the choice to normalize \\\\(\\\\binom\\\\) away.\"}"}
{"id": "bxwWikAXSy", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sub/superscript are put in braces, $\\\\overline{u^2}$ + $\\\\frac{1}{2}k_{ap}g_zh^2$\\n\\nSubscripts are put before superscripts, extra space is dropped\\n\\\\[\\n\\\\int_{-a}^{a}f(x)dx=0\\n\\\\]\\n\\nSingle quotes are replaced by a superscript\\n\\\\[\\nf'(\\\\overline{x})\\n\\\\]\\n\\nThe following characteristics can not be represented in handwriting and have been normalized away:\\n\\n- color\\n- accurate spacing: e.g. ~, \\\\quad\\n- font style and size: e.g. \\\\mathrm, \\\\mathit, \\\\mathbf, \\\\scriptstyle\\n\\nThere are others that can be represented in handwriting, but that are not consistent enough in MathWriting to be preserved:\\n\\n- font families: Fraktur, Calligraphic. In practice, only Blackboard (\\\\mathbb) has been written consistently enough by contributors that we were able to keep it: \\\\mathcal and \\\\mathfrak are dropped.\\n- some variations like \\\\rightarrow \\\\rightarrow and \\\\longrightarrow\\n- some character variations. e.g. \\\\varrho, \\\\varepsilon\\n- size modifiers like \\\\left, \\\\right, \\\\big. Similarly, variable-width diacritics like \\\\widehat.\\n\\nIn this section we show additional graphs that illustrate dataset statistics that are described in Section 3. The frequencies of normalized L_A TEX expressions are presented in Figure 11. Figure 12 illustrates the...\"}"}
{"id": "bxwWikAXSy", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11: Counts of inks corresponding to the same normalized expression, ordered by increasing count. Each position on the horizontal corresponds to a unique normalized expression. Almost 5k unique expressions have been written 10 times or more by contributors.\\n\\nDistribution of sampling rates within human-written data. Results of resampling points in time are presented in Figure 13.\\n\\nFigure 12: Histogram of sampling rates in human-written data of MathWriting dataset.\\n\\nFigure 13: Examples of time resampling with different time periods. Larger periods result in shorter sequences of points.\\n\\nG Variety of Writing Styles\\n\\nIn this section we provide additional examples of differences in the writing order of fractions \u2013 Figure 14. These examples show that MathWriting dataset contains a variety of writing styles.\"}"}
