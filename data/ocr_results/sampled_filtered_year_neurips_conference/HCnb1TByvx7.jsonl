{"id": "HCnb1TByvx7", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nSocial media platforms were conceived to act as online \u2018town squares\u2019 where people could get together, share information and communicate with each other peacefully. However, harmful content borne out of bad actors are constantly plaguing these platforms slowly converting them into \u2018mosh pits\u2019 where the bad actors take the liberty to extensively abuse various marginalised groups. Accurate and timely detection of abusive content on social media platforms is therefore very important for facilitating safe interactions between users. However, due to the small scale and sparse linguistic coverage of Indic abusive speech datasets, development of such algorithms for Indic social media users (one-sixth of global population) is severely impeded. To facilitate and encourage research in this important direction, we contribute for the first time MACD\u2014a large-scale (150K), human-annotated, multilingual (5 languages), balanced (49% abusive content) and diverse (70K users) abuse detection dataset of user comments, sourced from a popular social media platform\u2014ShareChat. We also release AbuseXLMR, an abusive content detection model pretrained on large number of social media comments in 15+ Indic languages which outperforms XLM-R and MuRIL on multiple Indic datasets. Along with the annotations, we also release the mapping between comment, post and user id's to facilitate modelling the relationship between them. We share competitive monolingual, cross-lingual and few-shot baselines so that MACD can be used as a dataset benchmark for future research. Dataset, code and AbuseXLMR are available at: https://github.com/ShareChatAI/MACD\\n\\n1 Introduction\\n\\nAdoption of social media platforms has increased dramatically in recent times. Unfortunately, this rapid adoption is often accompanied with an increase in the frequency of abusive interactions like cyber-bullying, abusive language, hate speeches etc. [67] towards individuals and groups which can trigger violent real-world situations [13], and result in devaluation and exclusion of minority members [36, 51]. Repeated exposure to these types of harmful content could lead to psychological trauma, radicalization and even self-harm [69]. In addition, several incidents in India, such as smearing campaigns against famous political leaders, celebrities, and social media personalities,\"}"}
{"id": "HCnb1TByvx7", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abusive examples along with their translations and descriptions are boxed in red color and Non-abusive in green. Online anti-religious propaganda and cyber harassment observed on social media platforms further encourages one to tackle this alarming problem. To counter, social media platforms employ human content moderators to filter such content, so that end-users are not exposed to these. However, content moderators have been reported to suffer from burnout, depression, and PTSD as a result of viewing these harmful content day in and day out. Thus, an effective solution to combat online abuse would be to develop automatic abuse detection systems which can identify abusive content in a timely manner and could at least partially alleviate the burden from the moderators and facilitate safe and healthy interactions on social media platforms.\\n\\nHowever, social media interactions are not structured formally and contain spelling mistakes, slangs, grammatical errors, emoticons etc. Moreover, the content is expressed in multiple languages and can even be code-mixed, which makes detection extremely challenging, especially for resource impoverished languages. To solve this, abusive speech detection datasets have been developed for various languages and have been primarily sourced from social media platforms like Twitter, Facebook, Gab, YouTube etc. Similarly, abusive speech datasets for Indic languages have also been contributed. However, the scale and linguistic coverage of these datasets is sparse (see Table 1). Large-scale annotated datasets for abusive speech research in Indic languages is a need of the hour. Considering that one-sixth of the global population speaks Indic languages, such dataset would have a huge impact. While combining smaller datasets into one large-scale dataset is possible, differences in annotation guidelines and dataset sources (Gab, YouTube, Reddit etc.) can introduce inconsistency, which can impact the studies. In such scenarios, a large-scale dataset curated from a single source and annotated based on a consistent set of guidelines is more helpful.\\n\\nTo reduce this gap and foster abusive speech detection in Indic languages, we contribute a novel, large-scale, human-annotated, well-balanced, diverse and multilingual abuse detection dataset - Multilingual Abusive Comment Detection dataset (MACD), sourced from a popular social media platform - ShareChat, which supports over 15 Indic languages. MACD comprises of 150K textual comments posted on 92881 posts by 70453 users with 74K abusive and 77K non-abusive comments (49% abuse ratio) from five Indic languages - Hindi (Hi), Tamil (Ta), Telugu (Te), Malayalam (Ml) and Kannada (Kn). We select these languages as they witness maximum engagement on the platform. To the best of our knowledge, MACD is one of the largest abusive speech datasets for Indic languages. Comments containing abusive speech towards individuals/religions/race/political group, sexual references, profane language, violent intentions etc. are annotated as abusive by language-specific team of expert annotators. Along with MACD, we further contribute AbuseXLMR, which has been pretrained using the XLM-R model from 5M+ social media comments. AbuseXLMR bridges the domain gap which exists in XLM-R, mBERT and MuRIL as they have not been pretrained over social media datasets. We show that AbuseXLMR excels over these contextual models on MACD as well as many other popular Indic abuse detection datasets like HASOC, MOLD, and Bengali datasets. AbuseXLMR also triumphs over XLM-R and MuRIL under zero-shot cross-lingual and few-shot performance settings.\"}"}
{"id": "HCnb1TByvx7", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison of MACD with abusive speech datasets (> 4K samples)\\n\\n| Dataset    | Language | Samples | Abuse% |\\n|------------|----------|---------|--------|\\n| Indic [48] | Hindi    | 6K      | 51%    |\\n| Indic [47] | Hindi    | 5K      | 50%    |\\n| Indic [42] | Hindi    | 4.5K    | 32%    |\\n| Indic [70] | Hindi    | 4.5K    | 32%    |\\n| Indic [59] | Bengali  | 30K     | 33%    |\\n| MACD       | Hindi    | 33K     | 52%    |\\n| MACD       | Tamil    | 30K     | 46%    |\\n| MACD       | Telugu   | 30K     | 52%    |\\n| MACD       | Kannada  | 33K     | 49%    |\\n| MACD       | Malayalam| 25K     | 45%    |\\n\\nTable 2: MACD statistics.\\n\\n| Data description | Value |\\n|------------------|-------|\\n| # Total samples  | 152422|\\n| # Abusive samples| 74550 |\\n| # Non-abusive samples | 77872 |\\n| # Abuse %        | 49%   |\\n| # Posts          | 92881 |\\n| # Users          | 70453 |\\n| # Average comments length | 85 chars |\\n| # Shortest comment | 2 chars |\\n| # Longest comment | 6621 chars |\\n\\nHighlighting the improved low-data capabilities. Thus AbuseXLMR positions itself as a domain-adapted, data-efficient and accurate abuse detection model for Indic languages.\\n\\nThe scale, linguistic coverage and consistent expert level annotations of MACD for resource-impoverished Indic languages would enable detailed study of abusive speech in these under-explored languages. Owing to the large scale of MACD could facilitate both pretraining and end-to-end training of deep models as we show using a series of competitive baselines here.\\n\\n- We release MACD, a large-scale (150K), well-balanced (49% abuse ratio), human-annotated, multilingual (5 Indic languages) and diverse (70K users) abuse detection dataset.\\n- We release AbuseXLMR which is a pretrained abuse detection model for social media content in Indic languages. AbuseXLMR outperforms XLM-R and MuRIL on four Indic datasets which can be used for future research.\\n- We contribute monolingual, cross-lingual and few-shot baselines for benchmarking and future work on our dataset.\\n\\nFigure 2: MACD across multiple dimensions: (a) number of samples for abusive and non-abusive categories for all the languages (b) distribution of length (number of characters) of comments in each language (c) distribution of number of users for each language (d) distribution of sub-categories on a subset of abusive comments from Hindi language. [Best viewed in color].\\n\\n2 Related work\\nAbusive speech detection has received lot of attention from the research community across textual [25, 23, 28, 22], audio [37, 33] and visual [35, 31, 4] domains. Abuse detection datasets in non-Indic languages [34, 28, 73, 52, 40, 22, 24, 72, 47, 29, 7, 52, 54, 41, 50, 52, 3, 2, 8, 55] have been instrumental in pushing the state-of-the-art for these languages. We summarize abuse detection text datasets in the non-Indic language subsection of Appendix Table 10. In contrast to non-Indic datasets, we observe that Indic datasets are substantially small in scale. [48, 47] proposed dataset for Hindi language consisting of 5K and 6K samples respectively. While the dataset is well balanced with 50% abusive content, the number of samples are insufficient for large-scale study. [62] proposed a dataset for Hindi language containing 2K posts sourced from Twitter and Facebook. Similarly, [70] contribute a dataset for Hindi (4.5K posts) and Marathi (2K posts). [9] proposed a dataset for...\"}"}
{"id": "HCnb1TByvx7", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bengali language containing 4K posts. Contribute a large (30K) dataset for Bengali but the abuse ratio is lower (33%). In Table 1, we note the statistics of Indic datasets containing more than 4K samples and observe that majority of these datasets are relatively small. We also compare MACD with code-mixed datasets in Appendix Table 13. The code-mixed dataset repository is one of the largest of its kind comprising more than 10 Indic languages and 740K comments. However, it is not available publicly for research purposes. Unlike code-mixed datasets, MACD is focussed towards studying Indic languages and the vocabulary contains less than 4% code-mixing (see Table 12). Overall, MACD is large-scale, publicly available and encompasses five popular Indic languages - Hindi, Tamil, Telugu, Kannada and Malayalam with consistent annotation for Indic languages.\\n\\nExisting methods: Initial investigations in abusive speech detection leveraged lexicon [63], hand-crafted features [22, 71, 41] and metadata [68, 18, 57]. Recently, transformer based models have shown state-of-the-art performance for various hate speech detection [46, 64, 14, 49]. Multilingual variants like mBERT [26], XLM-R [20] have been proposed for addressing semantic understanding across multilingual and resource-impoverished settings. For Indic languages, MuRIL [44] and IndicBERT [43] have been proposed. MuRIL has been trained over 16 Indic languages and English language datasets using MLM [65] and translation language modelling (TLM) [21]. IndicBERT is a multilingual ALBERT [45] model trained over 12 Indian languages. For all these models, the pretraining is not done on social media data; in order to bridge this gap we release AbuseXLMR, our own pretrained model on 5M+ social media data. Besides a limited number of studies have been explored on the effect of transfer learning [56]. Therefore we explore this gap by studying various transfer schemes, i.e., zero-shot learning and few-shot learning.\\n\\nTable 3: Monolingual: Accuracy (Acc) and F1-macro score (F1) for different models on MACD dataset. Best results and second best results are shown in bold and underline respectively.\\n\\n| Model         | Hindi Acc | Hindi F1 | Tamil Acc | Tamil F1 | Telugu Acc | Telugu F1 | Kannada Acc | Kannada F1 | Malayalam Acc | Malayalam F1 |\\n|---------------|-----------|----------|-----------|----------|------------|-----------|-------------|------------|----------------|---------------|\\n| TF-IDF (LR)   | 81.23     | 81.19    | 83.57     | 83.41    | 86.10      | 86.08     | 82.12       | 81.94      | 81.84          | 81.52         |\\n| TF-IDF (SVM)  | 82.36     | 82.34    | 84.43     | 84.33    | 86.52      | 86.49     | 83.13       | 83.04      | 83.66          | 83.42         |\\n| mBERT         | 84.32     | 84.31    | 87.42     | 87.37    | 89.08      | 89.07     | 86.64       | 86.58      | 84.33          | 84.18         |\\n| XLM-R         | 86.12     | 86.11    | 87.92     | 87.87    | 89.50      | 89.44     | 86.75       | 86.71      | 85.55          | 85.42         |\\n| MuRIL         | 85.72     | 85.68    | 88.35     | 88.33    | 89.47      | 89.42     | 87.20       | 87.12      | 85.49          | 85.32         |\\n| AbuseXLMR     | 87.96     | 87.93    | 88.62     | 88.60    | 91.40      | 91.37     | 88.14       | 88.12      | 88.14          | 88.04         |\\n\\nTable 4: Alternate Splits: Accuracy (Acc) and F1-macro score (F1) for different splits using AbuseXLMR.\\n\\n| Splits          | Hindi Acc | Hindi F1 | Tamil Acc | Tamil F1 | Telugu Acc | Telugu F1 | Kannada Acc | Kannada F1 | Malayalam Acc | Malayalam F1 |\\n|-----------------|-----------|----------|-----------|----------|------------|-----------|-------------|------------|----------------|---------------|\\n| Random (80:10:10) | 88.53     | 88.52    | 88.83     | 88.81    | 91.63      | 91.60     | 88.00       | 88.00      | 88.23          | 88.14         |\\n| Chronological   | 87.43     | 86.78    | 87.47     | 86.99    | 90.42      | 90.38     | 87.16       | 87.16      | 83.17          | 81.82         |\\n| Unbalanced      | 93.05     | 85.69    | 92.19     | 84.48    | 94.65      | 88.86     | 93.33       | 85.74      | 92.27          | 82.81         |\\n\\nComments have been sourced from a popular social media platform - ShareChat. Since abusive comments are rare, we sample textual comments which have been reported as abusive by other users on the platform. These comments have higher probability of being abusive. We further enhance this set by using keyword matching (lexicon of 15K trigger words) for identifying comments containing frequently used abusive words. However, due to the contextual nature of abuse, presence of these words is not sufficient and manual annotation is required for assigning the ground-truth. In order to obtain the language of the comment, we used the language specified by the user in her profile.\"}"}
{"id": "HCnb1TByvx7", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 5: Zero-shot cross-lingual evaluation.\\n\\nThe models are trained on the source language (row) and evaluated on the test set of target languages (column). Best cross-lingual results are marked in bold and monolingual results are highlighted by underline.\\n\\n| Target \u2192 | Hindi | Tamil | Telugu | Kannada | Malayalam |\\n|----------|-------|-------|--------|---------|-----------|\\n| Source \u2193 |       |       |        |         |           |\\n| Acc      | 85.72 | 70.20 | 75.99  | 63.33   | 68.25     |\\n| F1       | 85.68 | 68.84 | 75.85  | 59.68   | 66.63     |\\n| Acc      | 75.35 | 88.35 | 78.67  | 66.81   | 73.62     |\\n| F1       | 75.12 | 88.33 | 78.50  | 62.12   | 72.43     |\\n| Acc      | 70.30 | 69.55 | 89.47  | 56.72   | 66.17     |\\n| F1       | 70.19 | 68.61 | 89.42  | 50.84   | 64.74     |\\n| Acc      | 72.82 | 68.36 | 73.33  | 87.20   | 68.53     |\\n| F1       | 72.79 | 66.34 | 72.90  | 57.47   | 67.59     |\\n\\n**XLM-R**\\n\\n| Target \u2192 | Hindi | Tamil | Telugu | Kannada | Malayalam |\\n|----------|-------|-------|--------|---------|-----------|\\n| Source \u2193 |       |       |        |         |           |\\n| Acc      | 86.12 | 58.78 | 61.59  | 60.97   | 67.20     |\\n| F1       | 86.11 | 53.59 | 58.30  | 57.02   | 65.98     |\\n| Acc      | 72.35 | 87.92 | 67.83  | 60.90   | 69.78     |\\n| F1       | 71.82 | 87.87 | 64.83  | 53.29   | 68.26     |\\n| Acc      | 70.28 | 54.62 | 89.50  | 56.60   | 66.35     |\\n| F1       | 70.16 | 47.57 | 89.44  | 50.84   | 65.41     |\\n| Acc      | 71.07 | 57.57 | 68.47  | 86.75   | 66.92     |\\n| F1       | 71.07 | 48.40 | 67.11  | 86.71   | 65.97     |\\n\\n**AbuseXLMR**\\n\\n| Target \u2192 | Hindi | Tamil | Telugu | Kannada | Malayalam |\\n|----------|-------|-------|--------|---------|-----------|\\n| Source \u2193 |       |       |        |         |           |\\n| Acc      | 87.96 | 81.21 | 85.75  | 69.93   | 84.78     |\\n| F1       | 87.93 | 81.02 | 85.74  | 67.87   | 84.78     |\\n| Acc      | 86.73 | 88.62 | 87.08  | 83.13   | 85.95     |\\n| F1       | 86.70 | 88.60 | 87.05  | 82.55   | 85.94     |\\n| Acc      | 81.50 | 79.83 | 91.40  | 77.45   | 82.48     |\\n| F1       | 81.49 | 79.74 | 91.37  | 77.01   | 82.48     |\\n| Acc      | 82.05 | 84.24 | 91.37  | 88.14   | 82.75     |\\n| F1       | 82.01 | 84.15 | 91.37  | 88.12   | 82.72     |\\n| Acc      | 84.60 | 80.77 | 84.35  | 74.87   | 88.14     |\\n| F1       | 84.15 | 79.51 | 83.93  | 71.57   | 88.04     |\\n\\n5 We use character set of these languages to identify the dominant language of the comment.\"}"}
{"id": "HCnb1TByvx7", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: Accuracy (Acc) and F1-macro (F1) under few-shot (5, 25, 100, 250 and 500 samples) settings. We run experiments for five seeds and report the average performance.\\n\\n| Language | Few-shot Settings | XLM-R | MuRIL | AbuseXLMR |\\n|----------|------------------|-------|-------|-----------|\\n|          | 5                |       |       |           |\\n| Hindi    | 50.36            | 46.95 | 52.40 |           |\\n| Tamil    | 49.45            | 51.46 | 53.47 |           |\\n| Telugu   | 51.98            | 48.06 | 52.63 |           |\\n| Kannada  | 50.20            | 51.18 | 52.17 |           |\\n| Malayalam| 46.91            | 52.21 | 48.07 |           |\\n|          | 25               |       |       |           |\\n| Hindi    | 51.97            | 50.32 | 60.22 |           |\\n| Tamil    | 52.06            | 52.05 | 61.80 |           |\\n| Telugu   | 49.36            | 49.36 | 55.01 |           |\\n| Kannada  | 51.88            | 56.85 | 56.85 |           |\\n| Malayalam| 53.22            | 57.33 | 57.33 |           |\\n|          | 100              |       |       |           |\\n| Hindi    | 55.78            | 55.71 | 77.31 |           |\\n| Tamil    | 58.18            | 54.38 | 81.12 |           |\\n| Telugu   | 53.92            | 55.98 | 82.96 |           |\\n| Kannada  | 54.91            | 57.13 | 80.43 |           |\\n| Malayalam| 55.25            | 55.97 | 78.24 |           |\\n|          | 250              |       |       |           |\\n| Hindi    | 69.57            | 60.99 | 77.69 |           |\\n| Tamil    | 72.18            | 61.57 | 84.25 |           |\\n| Telugu   | 77.09            | 77.37 | 87.65 |           |\\n| Kannada  | 70.51            | 62.66 | 80.26 |           |\\n| Malayalam| 70.68            | 67.69 | 83.15 |           |\\n|          | 500              |       |       |           |\\n| Hindi    | 75.88            | 78.17 | 82.29 |           |\\n| Tamil    | 78.26            | 81.12 | 84.25 |           |\\n| Telugu   | 80.32            | 82.96 | 87.65 |           |\\n| Kannada  | 75.41            | 80.43 | 84.29 |           |\\n| Malayalam| 77.21            | 78.24 | 83.15 |           |\\n\\nReferences, (c) Personal beliefs and practices (HI): Comments in which the dressing sense, choice of content, choice of language etc. are targeted, (d) Gender discrimination (HG): Comments in which the person is attacked on basis of gender, (e) Religious beliefs and practices (HR): Comments in which the person is attacked on basis of religious beliefs and practices. For example, comments questioning wearing of head-scarf, (f) Hate towards political views (HP): Comments in which the political views of person are attacked. For example, ridiculing people for supporting political party, (g) Violent intent (VI): Comments in which threat or call for violence is raised. In Figure 1, we share examples from Hindi language.\\n\\nAnnotator agreement: We measure the annotator agreement for all the languages using Cohen\u2019s Kappa and observe $\\\\kappa = 0.73$, $\\\\kappa = 0.72$, $\\\\kappa = 0.71$, and $\\\\kappa = 0.70$ for Hindi, Tamil, Telugu, Malayalam and Kannada respectively.\\n\\nMetadata: We also include the identifier of user who made the comment and identifier of the original content on which the comment was expressed to further enrich MACD dataset with social graph information. We masked both these identifiers for respecting privacy of the users.\\n\\n3.3 Dataset analysis\\nWe summarize the key statistics of MACD in Table 2.\\n\\nLinguistically diverse: MACD has been sourced from five Indic languages - Hindi, Tamil, Telugu, Malayalam and Kannada providing a highly diverse multilingual abuse detection dataset.\\n\\nLarge-scale: MACD dataset contains 150K samples with more than 25K samples for each languages which makes it one of the largest abuse detection dataset for Indic languages.\\n\\nBalanced: MACD is balanced across both categories with 74K abusive and 77K non-abusive comments (49% abusive samples). Ratio of abusive comments range from 52%, 46%, 52%, 48% and 49% for Hindi, Tamil, Telugu, Malayalam and Kannada respectively as shown in Figure 2(a).\\n\\nComment length: We plot the distribution of number of characters present in the comments for all the languages in Figure 2 (b). We note that majority of comments have an average of 85 characters per.\"}"}
{"id": "HCnb1TByvx7", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"comment which reflects the spontaneous and conversational nature of these comments. The shortest comment in our dataset is 2 characters and longest is 6621 characters.\\n\\nUser distribution: In Figure 2 (c), we plot the distribution of users across languages. We note that all the languages have more than 10K different users with Hindi having more than 20K users. This shows that MACD is rich in diversity because it captures wide range of nuances like spelling and grammatical mistakes, use of abbreviations and emojis along with various social sensitivities and beliefs of large number of social media users.\\n\\nAbuse types: We also investigate the diversity of MACD by having some crude annotations of the different abuse types for a small subset of Hindi abusive comments. As observed in Figure 2 (d), HI forms the dominant subcategory where comments are being made toward personal beliefs and practices. This is followed by profanity (PR) where comments containing cuss, swear words are being made. We would like to point out that this is an ongoing work and we are in the process of refining the type labels and extending it to the full dataset. In a future work, we shall release this fine-grained data once it has been satisfactorily tested for quality.\\n\\nDataset splits: We provide different MACD splits for analyzing various aspects of MACD:\\n\\n(a) Random splits: We randomly split MACD in 60:20:20 ratio to form the train, validation and test set and use this as default split for most of the study. We also release 80:10:10 ratio splits of MACD to increase the amount of training data for improved performance.\\n\\n(b) Chronological splits: Abusive behaviour evolves over time inspired by real-world events. For modelling these trends, we split MACD, chronologically into 60:20:20 ratio. We ordered all the comments using their date of creation before splitting them.\\n\\n(c) Unbalanced splits: Abusive content is rarely balanced and is rather sparse in natural settings. To simulate this scenario, we also provide an unbalanced split for MACD where we sample abusive and non-abusive comments in 1:5 ratio to represent near-natural settings.\\n\\n4 AbuseXLMR: Contextual models are trained on large-scale multilingual datasets but are not adapted for social media domain. This creates a domain gap and results in inferior performance, especially in low-data scenarios. To tackle this, we pretrain XLM-R on large-scale dataset extracted from ShareChat. We select XLM-R for pretraining because it showed good performance (Table 3). Moreover, unlike MuRIL and mBERT, XLM-R does not require consecutive sentences corpora because it does not use Next Sentence Prediction (NSP) task during pretraining. Since, MACD comments are primarily single-sentence and consecutive comments are not coherent, it is not useful to formulate MACD for NSP task. We extract large amounts of unlabelled comments which have been reported as abusive by ShareChat users or matched one of the trigger word for a duration of one year (Apr 2021 - Apr 2022) from the platform. We randomly sample 5M comments out of the complete corpora and use these sampled comments for continued pretraining of the XLM-R model using masked language modelling (MLM) loss. These comments belonged to 15+ Indic languages extending to Bengali, Marathi, etc. which makes the pretraining corpus linguistically diverse and qualifies AbuseXLMR as a suitable model for multiple Indic languages. Language set by the user was used as proxy since accurate determination of language is not required as we are pretraining with the complete corpus. Pretraining on this corpus adapts AbuseXLMR to the social media nuances like spelling mistakes, grammatical mistakes, emoticons etc. and thus enhances its capabilities as compared to MuRIL and XLM-R. Our experiments demonstrate the efficacy of AbuseXLMR over other models highlighting the importance of bridging the domain-gap across MACD and other popular Indic datasets.\\n\\n5 Experiment and results: We consider the task of classifying textual comment into abusive and non-abusive categories. Unless specified, we use the random 60:20:20 splits for our experiments. We compute the performance of TF-IDF based model and transformer-based multilingual contextual models like XLM-R [20],...\"}"}
{"id": "HCnb1TByvx7", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 7: Accuracy and macro-F1 in monolingual (mono), joint (joint) and pretraining (pr-mono) settings.\\n\\n| Model        | Hindi  | Tamil | Telugu | Kannada | Malayalam |\\n|--------------|--------|-------|--------|---------|-----------|\\n| mBERT (mono) | 84.32  | 87.42 | 89.08  | 86.64   | 84.33     |\\n| mBERT (joint)| 85.46  | 87.92 | 90.05  | 87.16   | 85.28     |\\n| XLM-R (mono) | 86.12  | 87.92 | 89.50  | 86.75   | 85.55     |\\n| XLM-R (joint)| 85.27  | 88.15 | 89.08  | 87.48   | 86.09     |\\n| XLM-R (pr-mono)| 86.34 | 87.90 | 90.05  | 87.20   | 86.54     |\\n\\n### Table 8: Cross dataset: Accuracy and macro-F1 on HASOC-2019 using MACD with XLM-R.\\n\\n| Model          | Hindi | Tamil | Telugu | Kannada | Malayalam |\\n|----------------|-------|-------|--------|---------|-----------|\\n| Baseline       | 83.08 | 82.82 |        |         |           |\\n| Pretrained-MACD| 83.68 | 83.99 |        |         |           |\\n| Hindi-MACD     | 83.58 | 83.76 |        |         |           |\\n| Joint-MACD     | 84.37 | 84.26 |        |         |           |\\n| Zero-Shot-MACD | 76.56 | 76.39 |        |         |           |\\n\\n### Table 9: Cross dataset: Accuracy and macro-F1 score on MOLD [30], Bengali [59] and HASOC [48] dataset.\\n\\n| Model       | Marathi | Bengali | Hindi |\\n|-------------|---------|---------|-------|\\n| MuRIL       | 89.60   | 90.63   | 84.45 |\\n| XLM-R       | 88.96   | 90.47   | 82.85 |\\n| AbuseXLMR   | 90.72   | 90.78   | 84.98 |\\n\\nAll the five languages included in MACD are covered in the training corpus of the contextual models making them suitable for our study. We discuss more details about the models and training in the appendix section. We report accuracy and macro-F1 score on the test split of MACD for measuring the model's performance.  \\n\\n**Monolingual experiments:** We show the performance of monolingual experiments (training and testing on the same language) in Table 3. We observe that contextual models improve upon the TF-IDF results by nearly 4%. This shows the advantages of modeling context in our dataset. The superior performance of contextual models can also be attributed to the fact that all the five languages of MACD were also used in the pretraining stages of these models. Comparing the contextual models, MuRIL performs better than XLM-R and mBERT for Tamil and Kannada, while XLM-R shows best performance for Hindi, Telugu and Malayalam. Improved performance of XLM-R can be attributed to the fact that it was pretrained on more than 100 languages including the Indian languages while MuRIL was trained specifically on 17 Indic languages. Finally, we note that AbuseXLMR outperforms all the other models for all the languages highlighting the importance of pretraining on domain-aligned data.\\n\\n**Other splits:** We further perform monolingual experiments with AbuseXLMR in Table 4 for the other three splits of MACD. We observe that the F1 scores drop, while accuracy improves on unbalanced splits. This could be due to the high class imbalance. We also note that the model performance drops when we use chronological splits highlighting the impact of evolving trends. In future, we would like to investigate how additional signals from the social network could be used to push up the performance back for both the above scenarios.\\n\\n**Zero-shot cross-lingual experiments:** We compute the performance for each language in a zero-shot cross-lingual setting. We train the model on the source language and measure the performance against the test set of the target languages. From Table 5, we observe cross-lingual performance is substantially lower (more than 10% drop in F1) than monolingual performance for XLM-R and mBERT model. This shows that both these models do not generalize so well across languages. However, the cross-lingual performance of AbuseXLMR is substantially higher than both XLM-R and MuRIL. These results show that domain-adaption of AbuseXLMR improves the zero shot performance drastically.\\n\\n**Few-shot experiments:** In Table 6 we finetune models on 5, 25, 100, 250 and 500 samples for five random seeds and report the average performance. We observe that using less than 100 samples with XLM-R and MuRIL results in macro-F1 score of less than 50%, highlighting that 100 and fewer examples are not sufficient. However, with 250 and more examples, the performance starts to improve. We note that AbuseXLMR demonstrates improved few-shot capabilities. Even with 100 examples.\"}"}
{"id": "HCnb1TByvx7", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"AbuseXLMR is able to demonstrate impressive results, almost comparable to performance shown by MuRIL and XLM-R with 500 samples.\\n\\nJoint training experiments: In Table 7, we study the impact of using all the languages present in MACD to together for training the models to investigate if cross-learning between languages happens. We combine the training splits of all the languages to form a combined training set. We repeat the same for the validation set. We then train the model using these combined sets and test them on the same held out test set of the respective languages. We note that joint training (joint) using mBERT improves the performance for all the languages over the monolingual (mono) setting, while XLM-R improves the results for three languages. These results show that there is cross-learning between the languages, which can be leveraged. Moreover, since social media content contains code-switching and emojis, there could be an overlap of tokens between languages that further benefits the joint training.\\n\\nPretraining experiments: We study the impact of self-supervised pretraining using training set of all the languages using Masked Language Modelling (MLM) loss. We continue pretraining of the XLM-R 7 checkpoint for 10 more epochs using the training data for all the 5 languages of MACD. We then finetune this checkpoint with monolingual supervised dataset for each language. We observe that this two-stage process of pretraining followed by finetuning shows performance gains for all the languages over their monolingual (mono) settings as shown by the pr-mono row in Table 7. These highlight the efficacy of using MACD for pretraining models for other datasets also.\\n\\nCross-dataset experiments: We evaluate the performance of models trained using MACD on HASOC-2019 binary classification task using the Hindi subset. Since the samples on HASOC are collected from Twitter and Facebook, this also allows to understand the cross-platform learning since MACD has not been sourced from Twitter/Facebook. We finetune XLM-R under different settings and evaluate performance on the HASOC test set.\\n\\n- Baseline: Finetune XLM-R on HASOC train set\\n- Pretrained-MACD: We pretrain XLM-R over MACD dataset and finetune this checkpoint on train set of HASOC\\n- Hindi-MACD: We finetune the XLM-R model in supervised manner on Hindi subset of MACD dataset and finetune it on HASOC train set\\n- Joint-MACD: We select the XLM-R model trained in supervised manner on combined MACD dataset and finetune it on HASOC train set\\n- Zero-shot-MACD: Evaluate joint model on HASOC test set in zero-shot setting\\n\\nFrom Table 8, we observe that self-supervised pretraining XLM-R using MACD dataset improves F1-macro score from 82.82 to 83.99. Finetuning XLM-R using supervision of the Hindi set from MACD also improves upon the baseline results by more than 1%. However, finetuning the XLM-R trained over combination of all the languages from MACD on the HASOC train show the best performance by improving the baseline by 1.7% (82.82 to 84.26). This shows that combination of all the languages of MACD are able to improve the performance in a cross-platform setup also.\\n\\nAbuseXLMR Experiments: In Table 9, we compare the performance of finetuning contextual models with three different datasets sourced from different languages and platforms to benchmark the portability and generalization of AbuseXLMR. We use Bengali [59] dataset, Marathi [30] dataset (MOLD) and HASOC [48] for this study. For all three datasets, we note that AbuseXLMR outperforms both MuRIL and XLM-R, highlighting the strength of AbuseXLMR in abusive content detection.\\n\\nWe also report results for translation based experiments A.4) to motivate the need for large-scale abusive speech datasets in Indic languages. We observe that models trained over English abuse detection datasets do not transfer well on English translated version of Indic comments.\\n\\nError analysis: XLM-R Errors: We analyze the failure cases of XLM-R since it obtains the best results for majority of the languages (Table 3). We randomly sample few error cases and analyse them to understand the scenarios where contextual model fails. Based on our analysis we divide the error cases into the following categories: (a) Implicit Abuse: These cases do not contain explicitly abusive words and require higher order reasoning. For example, (Hindu festivals cause environmental harm but sacrificing animals on Eid does not?) does not have any explicit abusive word but is aimed towards spreading hatred against a religion, (b) Trigger Words: While...\"}"}
{"id": "HCnb1TByvx7", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"XLM-R models context well, but we still observe scenarios where it fails due to presence of concepts which dominate abusive comments. For example, a comment like (Basanti, don\u2019t dance in front of these dogs), is a famous movie dialogue which is non-abusive but since the word dog is used in abusive context, the model fails, and,\\n\\nAnnotator Confusion: These are ambiguous instances like \u201cYour figure is sexy\u201d, where the model prediction cannot be deemed incorrect as the comment could be interpreted in both ways depending on the cultural sensitivities.\\n\\nXLM-R vs TF-IDF: We compare the error cases of XLM-R and TF-IDF to understand the impact of modelling the context for MACD. (a) Contextual Abuse: Some words appear more frequently in the abusive samples. However, depending upon the context, the meaning changes. For example, (this is cloth shop), the word cloth is used in abusive comments which question the lifestyle choices. Contextual model XLM-R predicts the correct label while TF-IDF fails, reiterating the importance of modelling the context, (b) Spelling Mistakes: Profane words are often misspelled, either intentionally to escape moderation algorithm or unintentionally due to informal nature of social media. For example, Hindi translation of word a**hole (translated to English for better explanation) is spelled differently as ashole, assh0le, a**hole etc. Token based approaches fail to capture all the variations and hence, language models pretrained with subword tokenization can detect such cases.\\n\\n7 Broader impacts, limitations and ethical considerations\\n\\nDeveloping large-scale, multilingual and human-annotated datasets for modeling Indic languages remains an open challenge. Absence of large-scale Indic datasets for abuse detection have severely impeded the research in these languages which are spoken by large number of people across the world. We hope MACD and AbuseXLMR will motivate and enable the research community to study and arrest the ill-effects of social media abuse and foster healthy, inclusive and safe social media interactions between users from all religions, genders, and ethnicity.\\n\\nLimitation: Considering that MACD covers only 5 Indic languages and does not represent the entire population, more parallel efforts need to continue for further narrowing this gap.\\n\\nExplicit warning: We request the community to be mindful that MACD contains comments which express abusive behaviour towards religion, region, gender etc. that might be abusive and depressing to the researchers. We did not censor such harmful words/phrases because that would defeat the purpose of the study. Kindly use your discretion while following up on our work.\\n\\nUser privacy: Protecting the privacy of users is a core value for ShareChat and we took measures for ensuring that no Personally Identifiable Information (PII) is made public. We will also provide an opt-out form for users to request explicit deletion of comments. We do not store the raw data used for this study. Only the anonymized data will be made available for future research.\\n\\nInformed consent: The comments in MACD are publicly available on the ShareChat application. These comments are published by users of the platform for public consumption and informed consent is requested by the platform for broadcasting them.\"}"}
{"id": "HCnb1TByvx7", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Muhammad Pervez Akhter, Zheng Jiangbin, Irfan Raza Naqvi, Mohammed Abdelmajeed, and Muhammad Tariq Sadiq. Automatic detection of offensive language for Urdu and roman Urdu. IEEE Access, 8:91213\u201391226, 2020.\\n\\n[2] Monirah A Al-Ajlan and Mourad Ykhlef. Optimized Twitter cyberbullying detection based on deep learning. In 2018 21st Saudi Computer Society National Computer Conference (NCC), pages 1\u20135. IEEE, 2018.\\n\\n[3] Nuha Albadi, Maram Kurdi, and Shivakant Mishra. Are they our brothers? analysis and detection of religious hate speech in the Arabic Twitterosphere. In 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), pages 69\u201376. IEEE, 2018.\\n\\n[4] Cleber Alc\u00e2ntara, Viviane Moreira, and Diego Feijo. Offensive video detection: dataset and baseline results. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 4309\u20134319, 2020.\\n\\n[5] Ika Alfina, Rio Mulia, Mohamad Ivan Fanany, and Yudo Ekanata. Hate speech detection in the Indonesian language: A dataset and preliminary study. In 2017 International Conference on Advanced Computer Science and Information Systems (ICACSIS), pages 233\u2013238. IEEE, 2017.\\n\\n[6] Andrew Arsht and Daniel Etcovitch. The human cost of online content moderation. Harvard Law Review Online, Harvard University, Cambridge, MA, USA. Retrieved from https://jolt.law.harvard.edu/digest/the-human-cost-of-online-content-moderation, 2018.\\n\\n[7] Valerio Basile, Cristina Bosco, Elisabetta Fersini, Nozza Debora, Viviana Patti, Francisco Manuel Rangel Pardo, Paolo Rosso, Manuela Sanguinetti, et al. Semeval-2019 task 5: Multilingual detection of hate speech against immigrants and women in Twitter. In 13th International Workshop on Semantic Evaluation, pages 54\u201363. Association for Computational Linguistics, 2019.\\n\\n[8] Valerio Basile, Cristina Bosco, Elisabetta Fersini, Debora Nozza, Viviana Patti, Francisco Manuel Rangel Pardo, Paolo Rosso, and Manuela Sanguinetti. SemEval-2019 task 5: Multilingual detection of hate speech against immigrants and women in Twitter. In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 54\u201363, Minneapolis, Minnesota, USA, June 2019. Association for Computational Linguistics.\\n\\n[9] Shiladitya Bhattacharya, Siddharth Singh, Ritesh Kumar, Akanksha Bansal, Akash Bhagat, Yogesh Dawer, Bornini Lahiri, and Atul Kr. Ojha. Developing a multilingual annotated corpus of misogyny and aggression. In Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying, pages 158\u2013168, Marseille, France, May 2020. European Language Resources Association (ELRA).\\n\\n[10] Aditya Bohra, Deepanshu Vijay, Vinay Singh, Syed Sarfaraz Akhtar, and Manish Shrivastava. A dataset of Hindi-English code-mixed social media text for hate speech detection. In Proceedings of the second workshop on computational modeling of people's opinions, personality, and emotions in social media, pages 36\u201341, 2018.\\n\\n[11] Cristina Bosco, Dell'Orletta Felice, Fabio Poletto, Manuela Sanguinetti, and Tesconi Maurizio. Overview of the Evalita 2018 hate speech detection task. In EVALITA 2018-Sixth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian, volume 2263, pages 1\u20139. CEUR, 2018.\\n\\n[12] U. Bretschneider and R. Peters. Detecting offensive statements towards foreigners in social media. In In: Proceedings of the 50th Hawaii International Conference on System Sciences (HICSS), Hawaii, USA, 2017.\\n\\n[13] Daniel L Byman. How hateful rhetoric connects to real-world violence, 2021.\\n\\n[14] Tommaso Caselli, Valerio Basile, Jelena Mitrovi\u0107, and Michael Granitzer. HateBERT: Retraining BERT for abusive language detection in English. In Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021), pages 17\u201325, Online, August 2021. Association for Computational Linguistics.\"}"}
{"id": "HCnb1TByvx7", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bharathi Raja Chakravarthi, Vigneshwaran Muralidaran, Ruba Priyadharshini, and John Philip McCrae. Corpus creation for sentiment analysis in code-mixed Tamil-English text. In Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL), pages 202\u2013210, Marseille, France, May 2020. European Language Resources association.\\n\\nBharathi Raja Chakravarthi, Ruba Priyadharshini, Navya Jose, Thomas Mandl, Prasanna Kumar Kumaresan, Rahul Ponnusamy, RL Hariharan, John Philip McCrae, Elizabeth Sherly, et al. Findings of the shared task on offensive language identification in tamil, malayalam, and kannada. In Proceedings of the first workshop on speech and language technologies for Dravidian languages, pages 133\u2013145, 2021.\\n\\nBharathi Raja Chakravarthi, Ruba Priyadharshini, Vigneshwaran Muralidaran, Navya Jose, Shardul Suryawanshi, Elizabeth Sherly, and John P McCrae. Dravidiancodemix: Sentiment analysis and offensive language identification dataset for dravidian languages in code-mixed text. Language Resources and Evaluation, pages 1\u201342, 2022.\\n\\nDespoina Chatzakou, Nicolas Kourtellis, Jeremy Blackburn, Emiliano De Cristofaro, Gianluca Stringhini, and Athena Vakali. Mean birds: Detecting aggression and bullying on twitter. In Proceedings of the 2017 ACM on web science conference, pages 13\u201322, 2017.\\n\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, \u00c9douard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online, July 2020. Association for Computational Linguistics.\\n\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, \u00c9douard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, 2020.\\n\\nAlexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\\n\\nThomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. Automated hate speech detection and the problem of offensive language. In Proceedings of the International AAAI Conference on Web and Social Media, volume 11, pages 512\u2013515, 2017.\\n\\nThomas Davidson, Dana Warmsley, Michael W. Macy, and Ingmar Weber. Automated hate speech detection and the problem of offensive language. In ICWSM, 2017.\\n\\nOna de Gibert, Naiara P\u00e9rez, Aitor Garc\u00eda-Pablos, and Montse Cuadros. Hate speech dataset from a white supremacy forum. In Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pages 11\u201320, 2018.\\n\\nOna de Gibert, Naiara Perez, Aitor Garc\u00eda-Pablos, and Montse Cuadros. Hate speech dataset from a white supremacy forum. In Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pages 11\u201320, Brussels, Belgium, October 2018. Association for Computational Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\\n\\nPaula Fortuna, Jo\u00e3o Rocha da Silva, Juan Soler-Company, Leo Wanner, and S\u00e9rgio Nunes. A hierarchically-labeled portuguese hate speech dataset. In Proceedings of the 3rd Workshop on Abusive Language Online (ALW3), 2019.\"}"}
{"id": "HCnb1TByvx7", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Antigoni Founta, Constantinos Djouvas, Despoina Chatzakou, Ilias Leontiadis, Jeremy Blackburn, Gianluca Stringhini, Athena Vakali, Michael Sirivianos, and Nicolas Kourtellis. Large scale crowdsourcing and characterization of twitter abusive behavior. ICWSM, 2018.\\n\\nAntigoni Maria Founta, Constantinos Djouvas, Despoina Chatzakou, Ilias Leontiadis, Jeremy Blackburn, Gianluca Stringhini, Athena Vakali, Michael Sirivianos, and Nicolas Kourtellis. Large scale crowdsourcing and characterization of twitter abusive behavior. In Twelfth International AAAI Conference on Web and Social Media, 2018.\\n\\nSaurabh Sampatrao Gaikwad, Tharindu Ranasinghe, Marcos Zampieri, and Christopher Homan. Cross-lingual offensive language identification for low resource languages: The case of Marathi. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021), pages 437\u2013443, Held Online, September 2021. INCOMA Ltd.\\n\\nZhiwei Gao, Shuntaro Yada, Shoko Wakamiya, and Eiji Aramaki. Offensive language detection on video live streaming chat. In Proceedings of the 28th International Conference on Computational Linguistics, pages 1936\u20131940, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics.\\n\\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64(12):86\u201392, 2021.\\n\\nSreyan Ghosh, Samden Lepcha, S Sakshi, and Rajiv Ratn Shah. Speech toxicity analysis: A new spoken language processing task. arXiv preprint arXiv:2110.07592, 2021.\\n\\nJennifer Golbeck, Zahra Ashktorab, Rashad O Banjo, Alexandra Berlinger, Siddharth Bhagwan, Cody Buntain, Paul Cheakalos, Alicia A Geller, Rajesh Kumar Gnanasekaran, Raja Rajan Gunasekaran, et al. A large labeled corpus for online harassment research. In Proceedings of the 2017 ACM on web science conference, pages 229\u2013233, 2017.\\n\\nRaul Gomez, Jaume Gibert, Lluis Gomez, and Dimosthenis Karatzas. Exploring hate speech detection in multimodal publications. In 2020 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1459\u20131467. IEEE, 2020.\\n\\nJeff Greenberg and Tom Pyszczynski. The effect of an overheard ethnic slur on evaluations of the target: How to spread a social disease. Journal of Experimental Social Psychology, 21(1):61\u201372, 1985.\\n\\nVikram Gupta, Rini Sharon, Ramit Sawhney, and Debdoot Mukherjee. Adima: Abuse detection in multilingual audio. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6172\u20136176, 2022.\\n\\nRishav Hada, Sohi Sudhir, Pushkar Mishra, Helen Yannakoudakis, Saif M. Mohammad, and Ekaterina Shutova. Ruddit: Norms of offensiveness for English Reddit comments. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2700\u20132717, Online, August 2021. Association for Computational Linguistics.\\n\\nAdeep Hande, Ruba Priyadharshini, and Bharathi Raja Chakravarthi. Kancmd: Kannada codemixed dataset for sentiment analysis and offensive language detection. In Proceedings of the Third Workshop on Computational Modeling of People\u2019s Opinions, Personality, and Emotion\u2019s in Social Media, pages 54\u201363, 2020.\\n\\n\u00d3scar Garibo i Orts. Multilingual detection of hate speech against immigrants and women in twitter at semeval-2019 task 5: Frequency analysis interpolation for hate in speech detection. In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 460\u2013463, 2019.\\n\\nMuhammad Okky Ibrohim and Indra Budi. Multi-label hate speech and abusive language detection in indonesian twitter. In Proceedings of the Third Workshop on Abusive Language Online, pages 46\u201357, 2019.\"}"}
{"id": "HCnb1TByvx7", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Aditya Kadam, Anmol Goel, Jivitesh Jain, Jushaan Singh Kalra, Mallika Subramanian, Manvith Reddy, Prashant Kodali, T. H. Arjun, Manish Shrivastava, and Ponnurangam Kumaraguru. Battling hateful content in indic languages hasoc '21. In FIRE, 2021.\\n\\nDivyanshu Kakwani, Anoop Kunchukuttan, Satish Golla, Gokul N.C., Avik Bhattacharyya, Mitesh M. Khapra, and Pratyush Kumar. IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages. In Findings of EMNLP, 2020.\\n\\nSimran Khanuja, Diksha Bansal, Sarvesh Mehtani, Savya Khosla, Atreyee Dey, Balaji Gopalan, Dilip Kumar Margam, Pooja Aggarwal, Rajiv Teja Nagipogu, Shachi Dave, et al. Muril: Multilingual representations for indian languages. arXiv preprint arXiv:2103.10730, 2021.\\n\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. ALBERT: A lite BERT for self-supervised learning of language representations. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\\n\\nPing Liu, Wen Li, and Liang Zou. Nuli at semeval-2019 task 6: Transfer learning for offensive language detection using bidirectional transformers. In SemEval@ NAACL-HLT, pages 87\u201391, 2019.\\n\\nThomas Mandl, Sandip Modha, Anand Kumar M, and Bharathi Raja Chakravarthi. Overview of the hasoc track at fire 2020: Hate speech and offensive language identification in tamil, malayalam, hindi, english and german. In Forum for Information Retrieval Evaluation, pages 29\u201332, 2020.\\n\\nThomas Mandl, Sandip Modha, Prasenjit Majumder, Daksh Patel, Mohana Dave, Chintak Mandlia, and Aditya Patel. Overview of the hasoc track at fire 2019: Hate speech and offensive content identification in indo-european languages. In Proceedings of the 11th forum for information retrieval evaluation, pages 14\u201317, 2019.\\n\\nBinny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee. Hatexplain: A benchmark dataset for explainable hate speech detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14867\u201314875, 2021.\\n\\nHala Mulki, Hatem Haddad, Chedi Bechikh Ali, and Halima Alshabani. L-hsab: A levantine twitter dataset for hate speech and abusive language. In Proceedings of the Third Workshop on Abusive Language Online, pages 111\u2013118, 2019.\\n\\nBrian Mullen and Diana R Rice. Ethnophaulisms and exclusion: The behavioral consequences of cognitive representation of ethnic immigrant groups. Personality and Social Psychology Bulletin, 29(8):1056\u20131067, 2003.\\n\\nNedjma Ousidhoum, Zizheng Lin, Hongming Zhang, Yangqiu Song, and Dit-Yan Yeung. Multilingual and multi-aspect hate speech analysis. In Proceedings of EMNLP. Association for Computational Linguistics, 2019.\\n\\nJohn Pavlopoulos, Prodromos Malakasiotis, Juli Bakagianni, and Ion Androutsopoulos. Improved abusive comment moderation with user embeddings. In Proceedings of the 2017 EMNLP Workshop: Natural Language Processing meets Journalism, pages 51\u201355, Copenhagen, Denmark, September 2017. Association for Computational Linguistics.\\n\\nMichal Ptaszynski, Agata Pieciukiewicz, and Pawe\u0142 Dyba\u0142a. Results of the poleval 2019 shared task 6: First dataset and open shared task for automatic cyberbullying detection in polish twitter. 2019.\\n\\nLaraQuijano-Sanchez, Juan Carlos Pereira Kohatsu, Federico Liberatore, and Miguel Camacho-Collados. Haternet a system for detecting and analyzing hate speech in twitter. In Zenodo, 2019.\\n\\nTharindu Ranasinghe and Marcos Zampieri. An evaluation of multilingual offensive language identification methods for the languages of India. Information, 12(8):306, 2021.\"}"}
{"id": "HCnb1TByvx7", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Manoel Horta Ribeiro, Pedro H Calais, Yuri A Santos, Virg\u00edlio AF Almeida, and Wagner Meira Jr. Characterizing and detecting hateful users on Twitter. In Twelfth international AAAI conference on web and social media, 2018.\\n\\nJulian Risch, Philipp Schmidt, and Ralf Krestel. Data integration for toxic comment classification: Making more than 40 datasets easily accessible in one unified format. In Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021), pages 157\u2013163, 2021.\\n\\nNauros Romim, Mosahed Ahmed, Hriteshwar Talukder, Saiful Islam, et al. Hate speech detection in the Bengali language: A dataset and its baseline evaluation. In Proceedings of International Joint Conference on Advances in Computational Intelligence, pages 457\u2013468. Springer, 2021.\\n\\nBj\u00f6rn Ross, Michael Rist, Guillermo Carbonell, Benjamin Cabrera, Nils Kurowsky, and Michael Wojatzki. Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis. In Michael Bei\u00dfwenger, Michael Wojatzki, and Torsten Zesch, editors, Proceedings of NLP4CMC III: 3rd Workshop on Natural Language Processing for Computer-Mediated Communication, volume 17 of Bochumer Linguistische Arbeitsberichte, pages 6\u20139, Bochum, sep 2016.\\n\\nManuela Sanguinetti, Fabio Poletto, Cristina Bosco, Viviana Patti, and Marco Stranisci. An Italian Twitter corpus of hate speech against immigrants. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), 2018.\\n\\nAnita Saroj and Sukomal Pal. An Indian language social media collection for hate and offensive speech. In Proceedings of the Workshop on Resources and Techniques for User and Author Profiling in Abusive Language, pages 2\u20138, 2020.\\n\\nSara Sood, Judd Antin, and Elizabeth Churchill. Profanity use in online communities. In Proceedings of the SIGCHI conference on human factors in computing systems, pages 1481\u20131490, 2012.\\n\\nSteve Durairaj Swamy, Anupam Jamatia, and Bj\u00f6rn Gamb\u00e4ck. Studying generalisability across abusive language detection datasets. In Proceedings of the 23rd conference on computational natural language learning (CoNLL), pages 940\u2013950, 2019.\\n\\nWilson L Taylor. \u201cCLOZE procedure\u201d: A new tool for measuring readability. Journalism Quarterly, 30(4):415\u2013433, 1953.\\n\\nCagri Toraman, Furkan \u015eahinu\u00e7, and Eyup Yilmaz. Large-scale hate speech detection with cross-domain transfer. In Proceedings of the Language Resources and Evaluation Conference, pages 2215\u20132225, Marseille, France, June 2022. European Language Resources Association.\\n\\nTwitter. Twitter transparency report - https://transparency.twitter.com/en/reports/rules-enforcement.html#2020-jul-dec, 2021.\\n\\nElise Fehn Unsv\u00e5g and Bj\u00f6rn Gamb\u00e4ck. The effects of user features on Twitter hate speech detection. In Proceedings of the 2nd workshop on abusive language online (ALW2), pages 75\u201385, 2018.\\n\\nMitch Van Geel, Paul Vedder, and Jenny Tanilon. Relationship between peer victimization, cyberbullying, and suicide in children and adolescents: A meta-analysis. JAMA Pediatrics, 168(5):435\u2013442, 2014.\\n\\nAbhishek Velankar, Hrushikesh Patil, Amol Gore, Shubham Salunke, and Raviraj Joshi. Hate and offensive speech detection in Hindi and Marathi. In FIRE, 2021.\\n\\nZeerak Waseem. Are you a racist or am I seeing things? Annotator influence on hate speech detection on Twitter. In Proceedings of the first workshop on NLP and computational social science, pages 138\u2013142, 2016.\\n\\nZeerak Waseem and Dirk Hovy. Hateful symbols or hateful people? Predictive features for hate speech detection on Twitter. In Proceedings of the NAACL student research workshop, pages 88\u201393, 2016.\"}"}
{"id": "HCnb1TByvx7", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. Predicting the type and target of offensive posts in social media. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1415\u20131420, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\\n\\nChecklist\\n\\n\u2022 Did you include the license to the code and datasets? [Yes] See Section B.5.1\\n\\n1. For all authors...\\n\\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n\\n(b) Did you describe the limitations of your work? [Yes] See Limitation section in Section 8 and A.8\\n\\n(c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section 7\\n\\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n\\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n\\n(b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments...\\n\\n(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Code and dataset will be available at https://github.com/ShareChatAI/MACD\\n\\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See section A.5\\n\\n(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No]\\n\\n(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See section A.5\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n\\n(a) If your work uses existing assets, did you cite the creators? [Yes]\\n\\n(b) Did you mention the license of the assets? [Yes] See Section B.5.1\\n\\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes] Code and dataset will be available at https://github.com/ShareChatAI/MACD\\n\\n(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] A.2\\n\\n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] A.2\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] See section 3.2\\n\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [No] The wages for annotation team are confidential as per the employment policies of ShareChat.\"}"}
