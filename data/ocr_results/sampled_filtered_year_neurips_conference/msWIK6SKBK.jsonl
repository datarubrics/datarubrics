{"id": "msWIK6SKBK", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 16: Histograms of energy weighted averages\\n\\nFigure 17: Histograms of lateral widths\\n\\n(a) Point dist. for Energy feature\\n(b) Energy Mean\\n(c) Energy Variance\\n(d) Cell Energy Ratio\\n(e) Layer Energy\\n\\nFigure 18: Histograms of various shower shape variables\"}"}
{"id": "msWIK6SKBK", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figs. 19 - 24 show the histograms of various shower shape variables for SUPAv2 and samples generated with PointFlow, SetV AE, and Transformer.\\n\\nFigure 19: Histograms of point distributions for $\\\\eta$, $\\\\phi$, and $r$.\\n\\nFigure 20: Histograms of sample means for different features.\\n\\nFigure 21: Histograms of sample variance for different features.\\n\\nFigure 22: Histograms of energy weighted averages.\"}"}
{"id": "msWIK6SKBK", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 23: Histograms of lateral widths\\n\\n(a) Point dist. for Energy feature\\n\\n(b) Energy Mean\\n\\n(c) Energy Variance\\n\\n(d) Cell Energy Ratio\\n\\n(e) Layer Energy\\n\\nFigure 24: Histograms of various shower shape variables\"}"}
{"id": "msWIK6SKBK", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figs. 25 - 30 show the histograms of various shower shape variables for SUPAv3 and samples generated with PointFlow, SetV AE, and Transformer.\\n\\nFigure 25: Histograms of point distributions for $\\\\eta$, $\\\\phi$, and $r$.\\n\\nFigure 26: Histograms of sample means for different features.\\n\\nFigure 27: Histograms of sample variance for different features.\\n\\nFigure 28: Histograms of energy weighted averages.\"}"}
{"id": "msWIK6SKBK", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 29: Histograms of lateral widths\\n(a) Point dist. for Energy feature\\n(b) Energy Mean\\n(c) Energy Variance\\n(d) Cell Energy Ratio\\n(e) Layer Energy\\n\\nFigure 30: Histograms of various shower shape variables\"}"}
{"id": "msWIK6SKBK", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figs. 31 - 36 show the histograms of various shower shape variables for SUPAv4 and samples generated with PointFlow, SetVAE, and Transformer.\"}"}
{"id": "msWIK6SKBK", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 35: Histograms of lateral widths\\n\\n(a) Point dist. for Energy feature\\n(b) Energy Mean\\n(c) Energy Variance\\n(d) Cell Energy Ratio\\n(e) Layer Energy\\n\\nFigure 36: Histograms of various shower shape variables\"}"}
{"id": "msWIK6SKBK", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"A.4.5 SUPAv5\\n\\nWe only consider layer 0 for SUPAv5. Figs. 37 - 42 show the histograms of various shower shape variables for SUPAv5 and samples generated with PointFlow, SetV AE, and Transflowmer.\\n\\n**Figure 37**: Histograms of point distributions for $\\\\eta$, $\\\\phi$, and $r$.\\n\\n**Figure 38**: Histograms of sample means for different features.\\n\\n**Figure 39**: Histograms of sample variance for different features.\\n\\n**Figure 40**: Histograms of energy weighted averages.\"}"}
{"id": "msWIK6SKBK", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"In this section we will present some studies on generative modeling with the grid representation of data from SUPA. We discuss about how to downsample the point clouds below. For these studies, we generated another version of the dataset with SUPA such that it is similar to the CALO GAN dataset, i.e., with three layers and downsampled to a resolution in the multiples of $3 \\\\times 96$, $12 \\\\times 12$, and $12 \\\\times 6$, for layer 0, 1, and 2, respectively.\\n\\n**Downsampling.**\\nFor comparison, we downsample the point clouds to their corresponding image representation (see Figure 1) by first defining the region of interest i.e. a rectangular region for each layer and the number of bins/cells/pixels in both the horizontal (or $\\\\eta$) and vertical (or $\\\\phi$) directions.\\n\\nFinally, for each cell, we sum the energy of all the points falling within it to get the pixel intensity. We can increase the number of cells in order to get higher resolutions. Figure 1b, 1c, and 1d show the downsampled image representations at resolutions of 3x, 2x and 1x respectively for the shower shown in Figure 1a. We choose 1x to be the same resolution as used in CaloGAN [Paganini et al., 2018] (i.e. $12 \\\\times 12$ for Layer 1).\\n\\n**A.5.1 Validity of SUPA as a benchmark with grid representation**\\nWe show the comparison of performance of generative models trained over data generated with SUPA and Geant4 in \u00a7 5.3. In this section, we extend those studies with more analysis and plots. Figure 43 shows the scatter plot of the average ranks of those models. The average rank for a model on a dataset is obtained by first ranking them with respect to each marginal's discrepancy and then averaging over all the marginals.\\n\\nFurther, in Figures 44-49, we show a subset of the marginals (see \u00a7 5 for a detailed explanation on the marginals and Paganini et al. [2018] for the grid representation based marginals) for Geant4 and 25.\"}"}
{"id": "msWIK6SKBK", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 43: Scatter Plot for ranks over different models. Ranking of the models are consistent over both, SUPA and GEANT 4, showing the validity of SUPA as a benchmark.\\n\\nFigure 44: Histogram for various marginals for GEANT 4 e+ (top) and SUPA (bottom) vs. showers generated from different trained models.\\n\\nFurther, the distributions of the generated showers from different models behave similarly on both datasets, reinstating the proposition that a better model on SUPA implies a better model on the detailed GEANT 4.\"}"}
{"id": "msWIK6SKBK", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.5.2 High-resolution experiments\\n\\nIn this section, we show the utility of SUPA beyond using it for training at low resolution (similar to the resolution used in CaloGAN, which we call 1x), as well as the limitation of the current models.\\n\\nTable 4: Mean discrepancy metric (see \u00a7 A.5.1) for CaloFlow model when trained and tested over different resolutions. Columns correspond to the training resolution and rows to the test resolution.\\n\\nThe results on the diagonal show that CaloFlow's performance degrades when resolution increases, and the top row shows that it is not simply due to the sheer dimensionality of the signal since the model does not leverage structure at high resolution to perform better at low resolution.\\n\\nWe train CaloFlow [Krause and Shih, 2021] with SUPA by downsampling the point clouds at the higher resolutions of 2x and 3x. Table 4 shows the mean discrepancy metric (see \u00a7 A.5.1) for the models. We observe the trend that training at higher resolutions result in poorer performance (diagonal terms) in general. Further, when the generated samples from the trained models are downsampled to 1x, the performance deteriorates as compared to samples generated from models trained directly with data at 1x resolution.\"}"}
{"id": "msWIK6SKBK", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 48: Histogram for $E_{\\\\text{ratio}}$ for Geant $e^+$ (top) and SUPA (bottom) vs. showers generated with different trained models.\\n\\nFigure 49: Histogram for Layer sparsity for Geant $e^+$ (top) and SUPA (bottom) vs. showers generated with different trained models.\"}"}
{"id": "msWIK6SKBK", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SUPA: A Lightweight Diagnostic Simulator for Machine Learning in Particle Physics\\n\\nAtul Kumar Sinha\\nUniversity of Geneva\\natul.sinha@unige.ch\\n\\nDaniele Paliotta\\nUniversity of Geneva\\ndaniele.paliotta@unige.ch\\n\\nB\u00e1lint M\u00e1t\u00e9\\nUniversity of Geneva\\nbalint.mate@unige.ch\\n\\nJohn A. Raine\\nUniversity of Geneva, CERN\\njohnny.raine@cern.ch\\n\\nTobias Golling\\nUniversity of Geneva, CERN\\ntobias.golling@cern.ch\\n\\nFran\u00e7ois Fleuret\\nUniversity of Geneva\\nfrancois.fleuret@unige.ch\\n\\nAbstract\\nDeep learning methods have gained popularity in high energy physics for fast modeling of particle showers in detectors. Detailed simulation frameworks such as the gold standard GEANT4 are computationally intensive, and current deep generative architectures work on discretized, lower resolution versions of the detailed simulation. The development of models that work at higher spatial resolutions is currently hindered by the complexity of the full simulation data, and by the lack of simpler, more interpretable benchmarks. Our contribution is SUPA, the SUrrogate PArticle propagation simulator, an algorithm and software package for generating data by simulating simplified particle propagation, scattering and shower development in matter. The generation is extremely fast and easy to use compared to GEANT4, but still exhibits the key characteristics and challenges of the detailed simulation. The proposed simulator generates thousands of particle showers per second on a desktop machine, a speed up of up to 6 orders of magnitude over GEANT4, and stores detailed geometric information about the shower propagation. SUPA provides much greater flexibility for setting initial conditions and defining multiple benchmarks for the development of models. Moreover, interpreting particle showers as point clouds creates a connection to geometric machine learning and provides challenging and fundamentally new datasets for the field.\\n\\n1 Introduction\\nIn order to understand the fundamental building blocks of nature, High Energy Physics (HEP) experiments involve highly energetic particle collisions. These collisions cause particles to decay, and the identification of the resulting decay particles is of key importance to develop and confirm new physics/theories. This is enabled through an electromagnetic or hadronic calorimeter. As particles interact with the calorimeter, they split into multiple other particles, forming a particle shower. The nature of the generated shower depends on the specific material and geometry of the calorimeter, which are carefully selected and driven by physics theory and practical considerations.\\n\\nThe generated shower deposits energy in active layers of the calorimeters, and provides energy and location measurements from the particles produced in the cascade.\"}"}
{"id": "msWIK6SKBK", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Precise computer simulation is central to a better understanding of experimental results in HEP. Simulation is especially useful for the task of event reconstruction, where the deposited energy in the calorimeter is used to identify the particle that originated a given shower. The pattern of the deposited energy depends on the specific type of the (intermediate) particles, the initial energy and incidence angle, as well as the specific geometry and shape of the detector material, among other factors. The state-of-the-art for this kind of simulations is GEANT4 [Agostinelli et al., 2003], a Monte Carlo toolkit for modeling the propagation of particles through matter.\\n\\nWhile detailed simulation through GEANT4 provides fine-grained shower generation and captures the underlying distributions accurately, this software consists of more than 3.5 million lines of C++, is computationally very expensive, and requires specific domain knowledge to be set up and tuned. This makes it cumbersome and expensive to produce the amount and diversity of data needed to speed up machine learning research on these topics. Table 1 compares GEANT4 with our proposed simulator SUPA.\\n\\nOur key contributions to ease these issues are:\\n\\n\u2022 We introduce SUPA: a fast, easy to use simulator for simple particle propagation, scattering and shower development in matter (see \u00a7 3).\\n\u2022 We structure the simulator in a way that allows to easily change the complexity of the underlying shower development as well as the properties of the showers in terms of multiplicity, energy range, structure (see \u00a7 4.1).\\n\u2022 We use SUPA to generate data that highlights the limitations of current machine learning approaches for fast shower simulation (see \u00a7 5.2). We experimentally demonstrate that SUPA can be used as a proxy for detailed simulation, while being easy to tune and up to 6 orders of magnitude faster than GEANT4.\\n\\n1.1 Image Representation\\n\\nExisting approaches for generating particle showers [Paganini et al., 2018, Krause and Shih, 2021] focus on a quantized representation of the calorimeter hits, binning them into discrete pixels, or cells, and setting the pixel intensity to the sum of the energy deposited. The upside of this approach is that, due to the structure of existing calorimeters, the data takes the form of low resolution images and standard computer vision models can be directly applied. However, as we increase the resolution of the quantized representations in order to preserve more information, the resulting images become exceedingly sparse, leading to difficulties in training existing machine learning architectures.\\n\\nFigure 1b, 1c, and 1d show the downsampled image representations at resolutions of 3x, 2x and 1x respectively for the shower shown in Figure 1a. We choose 1x to be the same resolution as used in CaloGAN [Paganini et al., 2018] (i.e. $12 \\\\times 12$ for Layer 1).\\n\\nIt is also important to notice that, when decreasing the resolution of the data and looking at showers, while the signal to model is of lower dimension and less sparse, the underlying structure and relationships between hits are lost. This requires models to implicitly learn equivariances from multiple examples, while also introducing artificial structure arising from the chosen detector geometry. For example, with a shower incident on the surface of a detector, a slight translation in either direction does not change the properties of the shower. However it still has an impact on the resulting datasets, where for small shifts it is not translationally equivariant. Having access to the underlying energy depositions would therefore greatly reduce the dependence on the geometry of the detectors in the models, and allow generative models to learn the underlying structure from the physical process directly.\\n\\n1.2 Point Cloud Representation\\n\\nMany popular point-cloud datasets [Wu et al., 2015, Chang et al., 2015] in the machine learning community contain shape representations of physical objects, such as points sampled uniformly from the surfaces of chairs, tables, etc. These datasets enjoy many nice properties, e.g. locality and redundancy, that calorimetry datasets do not. Moreover, existing generative models for point clouds rely on these simplifying assumptions and therefore their performance on more diverse datasets is still relatively unexplored. In the case of particle showers in calorimeters, dealing directly with the point cloud representation provides an optimal description of the underlying physics processes. However,\"}"}
{"id": "msWIK6SKBK", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 1: An example shower (single layer) shown at different resolutions due to the detailed simulation resulting from \\\\textsc{geant4}, the number of points per shower represents a huge computational challenge. A key objective of our synthetic simulator is to bridge this gap by allowing for quicker ground truth data generation for different initial settings, thus providing a way of modulating the complexity of the dataset, leading to more interpretable benchmarks. This would also make it possible to take gradual steps in developing the models and work out which of the many complexities in the \\\\textsc{geant4} datasets are hardest to model.\\n\\n2 Related Work\\n\\nShower simulation with \\\\textsc{geant4} requires a complete description of the geometry and material of the detector and simulates all interactions between the initial particle, any subsequently produced particles through decays or emissions, and the detector material. The upside of using \\\\textsc{geant4} for simulation is that it is very accurate and represents the underlying physical phenomena precisely, but it comes at the cost of being computationally very expensive. Moreover, it requires expertise to define the configuration and a multitude of expert hours to set it up for new detector geometries. To reduce the computational resources required to simulate particle physics collisions and their energy depositions in detectors, deep generative models have been applied to generate particle showers. Most of these models \\\\cite{Abhishek2021, Krause2021, Paganini2018} employ the dataset first introduced in CaloGAN \\\\cite{Paganini2018} which is based on a simplified calorimeter inspired by the ATLAS Liquid Argon (LAr) electromagnetic calorimeter.\\n\\n2.1 Datasets and calorimeter structure\\n\\nFor the CaloGAN dataset, the calorimeter is cubic in shape with each dimension being 480 mm and no material in front of it. The volume is divided into three layers along the radial (z) direction with varying thicknesses of 90 mm, 347 mm, and 43 mm, respectively. These layers are further segmented into discrete cells, with different sizes for each layer: 160 mm $\\\\times$ 5 mm (first layer), 40 mm $\\\\times$ 40 mm (second layer) and 40 mm $\\\\times$ 80 mm (third layer). Each layer can be represented as a single-channel two-dimensional image with pixel intensities representing the energy deposited in the region. The final read-out has the resolution of 3 $\\\\times$ 96, 12 $\\\\times$ 12, and 12 $\\\\times$ 6. For this dataset, three different particle types were considered, namely, positrons, charged pions and photons. Further, the particles were configured to be incident perpendicular to the calorimeter with initial energies uniformly distributed in the range between 1 GeV and 100 GeV.\\n\\nSeveral other calorimeter geometries and configurations are employed across the literature. Erdmann et al. \\\\cite{Erdmann2019} consider a calorimeter configuration motivated by the CMS High Granularity Calorimeter (HGCAL) prototype, while Belayneh et al. \\\\cite{Belayneh2020} study a calorimeter based on the geometric layout of the proposed Linear Collider Detector (LCD) for the CLIC accelerator. Buhmann et al. \\\\cite{Buhmann2021a, Buhmann2021b} investigate the prototype calorimeter for the International Large Detector (ILD) which is one of the two proposed detector concepts for the International Linear Collider (ILC). The Fast Calorimeter Simulation Challenge 2022 (CaloChallenge2022) \\\\cite{FaucciGiannelli2022a, FaucciGiannelli2022b, FaucciGiannelli2022c} was proposed to spur the development and benchmarking of fast and high-fidelity calorimeter shower generation using deep learning methods. The challenge proposed three datasets, ranging in difficulty from easy to hard.\\n\\nThe ATLAS calorimeter has e.g. a more complex geometry, with the cells having accordion shaped electrodes to maximise the active volume.\"}"}
{"id": "msWIK6SKBK", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the difficulty is determined by the dimensionality of the calorimeter showers i.e. number of layers and number of voxels in each layer. In a different application setting, Erdmann et al. [2018] model a calorimeter response for cosmic ray-induced air showers in the Earth\u2019s atmosphere, producing signals in ground based detector stations. In contrast to the CaloGAN calorimeter, it has a single readout layer with $9 \\\\times 9$ cells, each cell corresponds to a detector unit placed with a spacing of $1500$ m. Although many different detector technologies are considered in the various models, the common aspect between the models and the corresponding datasets is the projection of the spatial signal onto discretized cells in 2D planes or a generalized 3D volume. The projection is done either directly while designing the calorimeter geometry, or as a post processing step. Further, the number of cells and thus the resolution of the final read-out is usually kept small. This is done either to simplify the data for training the models, or in order to speed up the simulations. Due to the different geometries used for generating the datasets, it is not a straightforward task to compare the performance of the various models. As the developments of each shower are completely dependent on the detailed model of the detector in GEANT4, although the same initial particles can be simulated, it is very difficult to draw parallels between two different datasets, as it is not possible simply to change the representation of the data from one model to look like that of another.\\n\\n2.2 Deep Generative Models\\n\\nThe applications of deep generative modeling to calorimeter simulation have so far almost entirely focused on Generative Adversarial Networks (GANs, Goodfellow et al. 2014) and Normalizing Flows [Rezende and Mohamed, 2016]. CaloGAN [Paganini et al., 2018] was the first application of GANs to a longitudinally segmented calorimeter. The approach is based on LAGAN [de Oliveira et al., 2017], a DCGAN [Radford et al., 2016]-like architecture, that is able to synthetize the shower images. The generator outputs a gray-scale image for each layer in the calorimeter, with each output pixel representing the energy pattern collected at that location. The architecture is also complemented with an auxiliary classifier tasked with reconstructing the initial energy $E$.\\n\\nKrause and Shih [2021] improve on CaloGAN with CaloFlow, a normalizing flow architecture to generate shower images. CaloFlow is able to generate better samples than previous approaches based on GANs and VAEs, while also providing more stable training. Inductive CaloFlow (iCaloFlow) Buckley et al. [2023] extends the CaloFlow model to higher granularity detector geometries. It utilizes a teacher-student distillation to increase sampling speed without loss of expressivity. Previous work by ATLAS collaboration [2018] proposes to use a Variational Autoencoder on the flattened images [Kingma and Welling, 2014]. Both the encoder and decoder consist of an MLP conditioned on the energy of the initial particle. In addition to the reconstruction and KL-terms, the authors also optimize for the overall energy deposit in both in the individual layers and in the overall system.\\n\\nBuhmann et al. [2023] proposes a diffusion-based generative model representing one of the initial methods devised for effectively processing the point cloud representation of calorimeter showers.\\n\\n3 Propagation Model\\n\\nSUPA generates data that imitates the propagation and splitting/scattering of point particles. To generate an event, a point particle is initialized at the origin of the 3D space with an initial energy value and a velocity parallel to the z-axis. We then define a sequence of 2D planes orthogonal to the z-axis at \\\\{z_0, ..., z_N\\\\} that we call \u201cslices\u201d (or \u201csub-layers\u201d, interchangeably). We assume that the slices are evenly spaced along the z-axis, with a gap of $\\\\Delta z$ between consecutive slices. As the initial particle reaches the first predefined z-slice at $z_0$, it has 3 options (see Figure 2): stop and deposit its energy with stopping probability $p_{\\\\text{stop}}$, split into two with splitting probability $p_{\\\\text{split}}$, or simply continue moving with the same velocity with pass-through probability $p_{\\\\text{pass}} = 1 - (p_{\\\\text{stop}} + p_{\\\\text{split}})$.\\n\\nThen:\\n\\n- If the particle splits, two particles are generated at the splitting location with energy and velocity features that ensure momentum conservation, i.e., that the sum of momentum is conserved. As such, we refer to this property as the energy conservation. A continuous attribute obeying simple conservation laws at each splitting, which can be interpreted as an equivalent for the energy of the particles in a shower.\"}"}
{"id": "msWIK6SKBK", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Layer 2\\nLayer 3\\nLayer 1\\n\\nFigure 2: Schematic summary of a propagation event generated by SUPA. (left) Splitting events can happen at the black vertical lines, called slices. See \u00a7 3 for a detailed explanation of the process. (right) Each time a particle crosses a pre-defined \\\"slice\\\", it either splits in two, stops and deposits energy, or passes through unperturbed.\\n\\nThe vectors of the new particles is equal to the decaying one. The original particle is removed from the propagation model and the two new ones continue propagating with their respective velocities. The splitting angle $\\\\theta$ and deviation angle $\\\\epsilon$ are sampled from distributions $p_{\\\\theta}(\\\\cdot)$ and $p_{\\\\epsilon}(\\\\cdot)$, respectively.\\n\\n- If the particle stops, its energy is deposited and the energy value and the location of the hit are recorded.\\n- If the particle passes through, it simply continues with its original velocity.\\n\\nSee Algorithm 1 for the pseudocode of this process. We can think of the process as a detector layer being positioned at the $z_0$-slice recording energy deposits. After the slice at $z_0$, the model either contains 0, 1 or 2 particles still propagating. As they reach the additional slices at $\\\\{z_1, \\\\ldots, z_N\\\\}$ the above procedure is repeated: whenever a particle is at a $z$-slice, it either passes, deposits its energy or decays.\\n\\nAfter all particles crossed the last slice at $z_N$ we collect all energy deposits (energy value and 3D location). Instead of building a dataset from the exact 3D location of these deposits, we partition the slices into subsets we call layers. We refer to the partitioning scheme as layer configuration.\\n\\nWe then build an event from the energy deposit $E$ and the $(x, y)$-coordinate of all hits for all slices within a given layer $L_i$, effectively projecting the $z$-axis into a single point. An event then consists of $N_2D$ point clouds, one for each layer, with a single scalar feature per point. Intuitively, the slices $z_i$ are the positions where the particle can split and stop (granularity of the simulation), and the layers $L_i$ correspond to the detector layers, where the readout happens. Figure 2 contains a schematic summary for the propagation process.\\n\\nHaving complete control over the algorithm, we can increase/decrease the complexity of the generated data by adjusting the distribution of the properties of the initial particle, the number and location of the slices $z_i$, the probabilities $p_{\\\\text{stop}}$, $p_{\\\\text{pass}}$, $p_{\\\\text{split}}$ and the distribution of the properties of the children after a splitting event. This is especially useful to debug and understand the behaviour of machine learning models at different levels for complexity.\\n\\nTable 1: Comparison of GEANT4 and the proposed model (SUPA)\\n\\n|               | GEANT4 | SUPA |\\n|---------------|--------|------|\\n| codebase      | 3.6m lines in C++ | few hundred lines in Python |\\n| target usage  | detailed physics simulations | fast data generation |\\n| speed         | 1772 ms/shower | 0.1-100 ms/shower |\"}"}
{"id": "msWIK6SKBK", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1 SUPA - Particle Splitting\\n\\nParameters:\\n- Stopping probability: \\\\( p_{\\\\text{stop}} \\\\)\\n- Splitting probability: \\\\( p_{\\\\text{split}} \\\\)\\n- Layer configuration:\\n  \\\\[ L_1 = \\\\{ z_0, \\\\ldots, z_{i_1} \\\\} \\\\]\\n  \\\\[ L_2 = \\\\{ z_{i_1} + 1, \\\\ldots, z_{i_2} \\\\} \\\\]\\n  \\\\[ \\\\ldots \\\\]\\n  \\\\[ L_n = \\\\{ z_{i_n - 1} + 1, \\\\ldots, z_N \\\\} \\\\]\\n- Slice Gap: \\\\( \\\\Delta z \\\\)\\n- Splitting angle: \\\\( \\\\theta \\\\)\\n- Deviation angle: \\\\( \\\\epsilon \\\\)\\n\\nInput:\\n- Tree \\\\( T \\\\)\\n- Position \\\\( p = (\\\\Delta \\\\eta, \\\\Delta \\\\phi) \\\\)\\n- Energy \\\\( E \\\\)\\n- Layer \\\\( l \\\\)\\n\\nOutput:\\n- A collection of \\\\( \\\\{ p_i, E_i \\\\} \\\\)\\n\\n\\\\( N \\\\) of generated particles\\n\\n1: \\\\((p, E, l) \\\\leftarrow \\\\text{Input} \\\\), \\\\( N \\\\leftarrow p(\\\\delta(n = 2)) \\\\)\\n\\n2: \\\\( \\\\theta \\\\sim p_\\\\theta(\\\\cdot), \\\\epsilon \\\\sim p_\\\\epsilon(\\\\cdot) \\\\), \\\\( \\\\alpha \\\\sim U(0, 1) \\\\)\\n\\n3: if \\\\( \\\\alpha < p_{\\\\text{stop}}[l] \\\\) then\\n\\n4: Active \\\\( \\\\leftarrow \\\\text{False} \\\\)\\n\\n5: else if \\\\( p_{\\\\text{stop}}[l] < \\\\alpha < p_{\\\\text{stop}}[l] + p_{\\\\text{split}} \\\\) then\\n\\n6: \\\\( \\\\omega \\\\leftarrow U(0, 2\\\\pi) \\\\)\\n\\n7: \\\\( p_0 \\\\leftarrow p + \\\\Delta z \\\\ast (\\\\sin(\\\\theta/2 + \\\\epsilon), \\\\cos(\\\\theta/2 + \\\\epsilon)) \\\\)\\n\\n8: \\\\( p_1 \\\\leftarrow p - \\\\Delta z \\\\ast (\\\\sin(\\\\theta/2 + \\\\epsilon), \\\\cos(\\\\theta/2 + \\\\epsilon)) \\\\)\\n\\n9: \\\\( E\\\\{0, 1\\\\} \\\\leftarrow E \\\\ast (\\\\theta/2 \\\\pm \\\\epsilon)/\\\\theta \\\\)\\n\\n10: \\\\( T\\\\).add\\\\((p_0, l + 1, E_0)\\\\), \\\\( T\\\\).add\\\\((p_1, l + 1, E_1)\\\\)\\n\\n11: else\\n\\n12: \\\\( T\\\\).add\\\\((p, l + 1, E)\\\\)\\n\\n13: end if\\n\\n14: Return \\\\( T \\\\)\\n\\nNotation.\\n\\nWe denote a single shower as \\\\( x = \\\\{ x_{L_1}, x_{L_2}, \\\\ldots, x_{L_N} \\\\} \\\\), where \\\\( x_{L_i} \\\\) denotes the \\\\( i \\\\)th layer and \\\\( N \\\\) is the total number of layers considered. For the point cloud representation, \\\\( x_{L_i} \\\\) can be further expanded as \\\\( x_{L_i} = \\\\{ x_{i_1}, x_{i_2}, \\\\ldots, x_{i_N_i} \\\\} \\\\), where \\\\( N_i \\\\) denotes the total number of points in layer \\\\( i \\\\) of the shower (can vary across showers). Further, each point is a vector \\\\( x_{i_j} = [\\\\eta_{i_j}, \\\\phi_{i_j}, E_{i_j}] \\\\), where \\\\( \\\\eta \\\\) and \\\\( \\\\phi \\\\) are the coordinates and \\\\( E \\\\) denotes the energy.\\n\\n4 Using SUPA\\n\\nSUPA provides various meta-parameters which affect the data generation and various characteristics of the generated data. This allows to generate shower data with varying levels of complexity and thus enables incremental benchmarking. For instance, \\\\( p_{\\\\text{split}} \\\\) affects the number of emerging secondary particles and thus implicitly the number of hits (or points), while \\\\( p_{\\\\text{stop}} \\\\) controls the position (in depth) of the observed hits. \\\\( \\\\theta \\\\) controls the overall spread of the hits in the lateral plane and \\\\( \\\\alpha \\\\) controls the dynamic range of the observed energy values. The different configurations for these parameters is analogous to different detector geometry/material or perhaps a different particle.\\n\\n4.1 SUPA Datasets\\n\\nWe propose five standard configurations and the resulting datasets SUPAv1-5 for our analysis and benchmarking, summarized in Table 2. All configurations have the same initial energy of \\\\( 65 \\\\) GeV , initial angle of \\\\( \\\\pi/2 \\\\) and initial impact position \\\\((0, 0)\\\\), i.e. particles are incident perpendicular and at the center of the calorimeter. Section A.2 in the appendix gives more details and also shows the interpretation of these variations with respect to different summary variables. SUPAv1 is deterministic\"}"}
{"id": "msWIK6SKBK", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"with respect to splitting and stopping probabilities as well as in \\\\( \\\\theta \\\\) and \\\\( \\\\alpha \\\\). It has the same number of points across all events and all the hits have the same energy value. SUPAv2-4 have the same configuration for splitting and stopping probabilities (see Sec. A.2.1), while SUPAv2 is deterministic in \\\\( \\\\theta \\\\) and \\\\( \\\\alpha \\\\). SUPAv3 is naturally more spread out than SUPAv2 (see Fig. 6b vs. Fig. 6c, and also distributions for \\\\( \\\\eta \\\\), \\\\( \\\\phi \\\\), \\\\( r \\\\) in Fig. 7, and \\\\( \\\\langle \\\\eta \\\\rangle \\\\), \\\\( \\\\langle \\\\phi \\\\rangle \\\\), \\\\( \\\\langle r \\\\rangle \\\\) in Fig. 8). SUPAv4 is similar to SUPAv3, except for the layer configuration. SUPAv4 only has a subset of the sub-layers by dropping the last 8 sub-layers from SUPAv3. While SUPAv3 captures the total initial energy in Layer 0, SUPAv4 has some energy leakage which is apparent from the distribution of layer energy \\\\( \\\\bar{E} \\\\) in Fig. 10f. Another interesting structure in shower data is the dynamic range of the energy values in a given shower. A high dynamic range of energy makes learning generative models difficult [Krause and Shih, 2021]. With SUPA, we can control the dynamic range via the parameter \\\\( \\\\alpha \\\\). SUPAv1 and SUPAv2 have fixed \\\\( \\\\alpha \\\\) (= 0), thus all the splits are symmetric leading to a low dynamic range (see Fig. 10d for histogram of variance in energy values). The other factor which affects the dynamic range is the layer configuration, more number of slices in a layer would lead to more dynamic range. SUPAv5 has multiple layers and has a higher \\\\( p_{\\\\text{split}} \\\\) in the initial slices, thus it is more complex in terms of multiplicity or total number of hits. Please refer to the appendix Sec. A.2 for more details and other canonical SUPA datasets.\\n\\n4.2 Extensions\\nSUPA is very flexible and can be easily extended. The slices can be further divided into different regions each with their own meta-parameters providing more granular control over the scattering and splitting process. Another variation requires having some slices that are not merged into layers to mimic dead material. These extensions can possibly bring it closer to more realistic detectors, and lead to unique structures in the dataset and thus present challenges for model development and evaluation. It is also possible to condition the data generation on various attributes, for example: initial impact angle, impact energy, impact position, etc. This encourages interesting studies and benchmarks regarding extrapolation and generalization capabilities of generative models for unseen conditioning variables during training.\\n\\n5 Evaluation and Analysis\\nThe evaluation of generative models for high dimensional data is a non-trivial problem. While reconstruction losses and/or data likelihood are available for some model architectures (e.g. auto-encoders or flows), judging the sample quality is still difficult. Moreover, the average log-likelihood is difficult to evaluate or even approximate for many interesting models. For instance, in computer vision, evaluating generative models for images is done via proxy measures such as the Frechet Inception Distances (FID, Heusel et al., 2017). Point cloud generative models are evaluated [Yang et al., 2019] using metrics such as Minimum matching distance (MMD), Coverage (COV) and 1-Nearest Neighbour Accuracy (1-NNA), where similarity between a set of reference point clouds and generated point clouds are measured using a distance metric such as the Chamfer distance (CD) or Earth mover's distance (EMD) based on optimal matching.\\n\\n5.1 Shower Shape Variables\\nFor calorimeter simulation, we cannot judge performance using individual examples due to the stochastic nature of shower development. The standard practice to estimate the quality of generative models for calorimetry data (shower simulations) is to use histograms of different metrics called shower shape variables [Paganini et al., 2018] in addition to images of calorimeter showers. These shower shape variables are domain-specific and physically motivated, and matching densities over these marginals is indicative of matching salient aspects of the data. These marginals capture various aspects of how energy is distributed within individual layers and across different layers.\\n\\nWe extend the shower shape variables for the point clouds representation of showers. A summary of all the shower shape variables considered are present in the appendix.\"}"}
{"id": "msWIK6SKBK", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Feature means $\\\\langle \\\\eta_i \\\\rangle$, $\\\\langle \\\\phi_i \\\\rangle$, $\\\\langle r_i \\\\rangle$, $\\\\langle E_i \\\\rangle$. Mean of each feature.\\n\\n$\\\\langle \\\\eta_i \\\\rangle = P_j \\\\eta_{ij} P_j 1$,\\n$\\\\langle \\\\phi_i \\\\rangle = P_j \\\\phi_{ij} P_j 1$,\\n$\\\\langle r_i \\\\rangle = P_j r_{ij} P_j 1$\\n$\\\\langle E_i \\\\rangle = P_j E_{ij} P_j 1$\\n\\nwhere $r_{ij} = q(\\\\eta_{ij})^2 + (\\\\phi_{ij})^2$ denotes the distance of the point in the lateral plane from the center.\\n\\nFeature variances $\\\\sigma \\\\langle \\\\eta_i \\\\rangle$, $\\\\sigma \\\\langle \\\\phi_i \\\\rangle$, $\\\\sigma \\\\langle r_i \\\\rangle$, $\\\\sigma \\\\langle E_i \\\\rangle$. Variance of each feature.\\n\\n$\\\\sigma \\\\langle \\\\eta_i \\\\rangle = r P_j \\\\eta_{ij}^2 P_j 1 - \\\\langle \\\\eta_i \\\\rangle^2$\\n\\nLayer Energy $\\\\bar{E}_i$. Denotes the total energy deposited in layer $i$ of the shower.\\n\\n$\\\\bar{E}_i = P_j \\\\in N_i E_{ij}$.\\n\\nTotal Energy $E_{\\\\text{tot}}$. Total energy across all layers of the shower.\\n\\n$E_{\\\\text{tot}} = P_i \\\\leq N \\\\bar{E}_i$.\\n\\nLayer Centroids $\\\\langle \\\\eta_i \\\\rangle_E$, $\\\\langle \\\\phi_i \\\\rangle_E$, $\\\\langle r_i \\\\rangle_E$. Energy weighted mean of the features ($\\\\eta$, $\\\\phi$, or $r$).\\n\\n$\\\\langle \\\\eta_i \\\\rangle = P_j E_{ij} \\\\eta_{ij} E_{ij}$,\\n$\\\\langle \\\\phi_i \\\\rangle = P_j E_{ij} \\\\phi_{ij} E_{ij}$,\\n$\\\\langle r_i \\\\rangle = P_j E_{ij} r_{ij} E_{ij}$\\n\\nThe layer centroids can be interpreted as the center of energy in the lateral plane in respective dimensions.\\n\\nLayer Lateral Width $\\\\sigma \\\\langle \\\\eta_i \\\\rangle_E$, $\\\\sigma \\\\langle \\\\phi_i \\\\rangle_E$, $\\\\sigma \\\\langle r_i \\\\rangle_E$. Denotes the standard deviation of the layer centroids.\\n\\n$\\\\sigma \\\\langle \\\\eta_i \\\\rangle_E = s P_j E_{ij} (\\\\eta_{ij})^2 E_{ij} - \\\\langle \\\\eta_i \\\\rangle^2 E_{ij}$\\n\\nThe layer lateral widths can be interpreted as the spread around the center of energy in the lateral plane in respective dimensions. We drop the layer notation $i$ from the above metrics when working with a single layer for brevity.\\n\\n5.2 Performance Analysis of Generative Models\\n\\nIn order to summarise the discrepancy into a scalar value, we consider the Wasserstein-1 distance between the histograms of the ground truth and generated sample's marginals. Further, we compute the mean discrepancy over different groups of marginals to summarize the performance of models.\\n\\nWe train point cloud generative models, PointFlow [Yang et al., 2019], SetVAE [Kim et al., 2021], and a transformer-based flow model (which we call Transflowmer, see Sec. A.3), on SUPA data. Table 3 summarizes the performance of different models across different variations of SUPA datasets.\\n\\nPointflow performs better than SetVAE for all marginals not involving energy feature such as point level marginals, feature means and variances for location parameters, etc., and worse for marginals which depend on energy such as, layer centroids, and layer energy, etc. (with the exception of layer lateral widths and feature variance $\\\\sigma \\\\langle E \\\\rangle$, which are both second order moments), across all datasets.\\n\\nRoughly speaking, PointFlow struggles at modeling the energy feature while it is competitive for the location features, and SetVAE behaves opposite. It is interesting to observe that Transflowmer is either the best performing or very close to best performing model (either SetVAE or PointFlow) across all datasets and marginals, indicating relatively greater flexibility at modeling a variety of features. The instances with relatively high values in Table 3 (feature means for SUPA v1, and layer energy for SUPA v2 and SUPA v3) are where the marginals had a very narrow spread, indicating the difficulty in modeling discrete distributions. The models also have the tendency to predict points with very high energy values unseen during training (see Fig. 24a). We provide more details in the Appendix Section A.4 on the training, results and analysis, along with plots of various shower shape variables to visually compare the goodness of fit.\\n\\nThese observations highlight the need for adapting point cloud generative models such that they are able to model points with special features (different from location parameters), such as physical features like energy, momentum, etc. Although the location parameters are correlated with these special features, the overall structure of this correlation can be very different from domain to domain and also from how they are correlated with other location features.\\n\\nFor computing the Wasserstein distance, we normalize the shower shape variables for the ground truth and the generated sample according to the mean and standard deviation of the ground truth.\"}"}
{"id": "msWIK6SKBK", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"Table 3: Performance benchmarks across different datasets with SetVAE, PointFlow and Transformer. The distance metric is Wasserstein-1. The reported numbers are averages over a group of marginals as indicated in the top row. Lower numbers are better. See \u00a7 5.1 for more details on the metrics.\\n\\n| Dataset   | SV   | PF   | TF   | SV   | PF   | TF   | SV   | PF   | TF   |\\n|-----------|------|------|------|------|------|------|------|------|------|\\n| SUPAv1    | 0.044| 0.037| 0.028| 0.001| 0.000| 0.000| 16.254| 21.921| 5.789 |\\n| SUPAv2    | 0.290| 0.167| 0.029| 0.253| 0.462| 0.145| 0.517| 0.256| 0.091 |\\n| SUPAv3    | 0.510| 0.130| 0.044| 0.304| 0.484| 0.048| 0.769| 0.075| 0.105 |\\n| SUPAv4    | 0.269| 0.122| 0.029| 0.111| 0.459| 0.006| 0.460| 0.094| 0.073 |\\n| SUPAv5    | 0.166| 0.028| 0.042| 0.078| 0.090| 0.151| 0.592| 0.031| 0.189 |\\n\\n\u00a7 5.3 SUPA as a benchmark\\nWe train the models of \u00a7 2.2 on both the events generated by GEANT and SUPA. For these studies, we generated another version of the dataset with SUPA and downsample the point clouds to grid representation such that it is similar to the CALOGAN dataset (see \u00a7 A.5 for details on data generation). The training loss of the flow model is a log-likelihood which meaningfully represents the quality of the captured distribution. Figure 3 displays the correlation of the log-likelihood values of different variations of the CaloFlow architecture trained both on GEANT and SUPA-generated events. Figure 4 shows the scatter plot of the mean discrepancy (across all marginals) obtained by different models on GEANT and SUPA-generated events. We show more plots in the Appendix \u00a7 A.5.\"}"}
{"id": "msWIK6SKBK", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We can observe that all these correlation plots are roughly monotonic, showing that benchmarking on SUPA provides a good estimate of relative performance on the detailed GEANT4.\\n\\nConclusion\\n\\nWe introduced SUPA, a lightweight pseudo-particle simulator that is inspired by the physics governed development of particle showers and qualitatively resembles the data generated by the gold standard GEANT4. By allowing to easily change the underlying parameters of the propagation model, we introduce the freedom to define new benchmark datasets of varying complexity with a fidelity and simplicity not available using the GEANT4 toolkit.\\n\\nWe believe that the ease of use, flexibility and speed of this simulator could be a MNIST moment for the field, allowing the rapid exploration of models in various regime of functioning as discussed in \u00a7 4.1. We also highlight the gaps in the ability of existing point cloud generative models on calorimetry data in \u00a7 5.2 and highlight the need for more powerful point cloud generative models. Additionally, we showed in \u00a7 5.3 and \u00a7 A.5.1 that with grid representation of the shower data, performance of deep generative models estimated on data produced with this simulator is a good proxy for their performance on the standard data used historically in machine learning for particle physics. We highlight flexibility of SUPA in \u00a7 A.5.2, allowing us to train/evaluate at different resolutions.\\n\\nMoreover, by giving easy access to the fully recorded event, we open the way to applying methods from point clouds and geometric deep learning research, with the goal of possibly devising end-to-end architectures able to reconstruct a full event from the energy deposits by solving the complete inverse problem, something currently impossible with data from GEANT4.\\n\\nReferences\\n\\nAbhishek Abhishek, Eric Drechsler, Wojciech Fedorko, and Bernd Stelzer. CaloDV AE: Discrete variational autoencoders for fast calorimeter shower simulation. 2021.\\n\\nS. Agostinelli, J. Allison, K. Amako, J. Apostolakis, H. Araujo, P. Arce, M. Asai, D. Axen, S. Banerjee, G. Barrand, F. Behner, L. Bellagamba, J. Boudreau, L. Broglia, A. Brunengo, H. Burkhardt, S. Chauvie, J. Chuma, R. Chytracek, G. Cooperman, G. Cosmo, P. Degtyarenko, A. Dell'Acqua, G. Depaola, D. Dietrich, R. Enami, A. Feliciello, C. Ferguson, H. Fesefeldt, G. Folger, F. Foppiano, A. Forti, S. Garelli, S. Giani, R. Giannitrapani, D. Gibin, J.J. G\u00f3mez Cadenas, I. Gonz\u00e1lez, G. Gracia Abril, G. Greeniaus, W. Greiner, V. Grichine, A. Grossheim, S. Guatelli, P. Gumplinger, R. Hamatsu, K. Hashimoto, H. Hasui, A. Heikkinen, A. Howard, V. Ivanchenko, A. Johnson, F.W. Jones, J. Kallenbach, N. Kanaya, M. Kawabata, Y. Kawabata, M. Kawaguti, S. Kelner, P. Kent, A. Kimura, T. Kodama, R. Kokoulin, M. Kossov, H. Kurashige, E. Lamanna, T. Lamp\u00e9n, V. Lara, V. Lefebure, F. Lei, M. Liendl, W. Lockman, F. Longo, S. Magni, M. Maire, E. Medernach, K. Minamimoto, P. Mora de Freitas, Y. Morita, K. Murakami, M. Nagamatu, R. Nartallo, P. Nieminen, T. Nishimura, K. Ohtsubo, M. Okamura, S. O'Neale, Y. Oohata, K. Paech, J. Perl, A. Pfeiffer, M.G. Pia, F. Ranjard, A. Rybin, S. Sadilov, E. Di Salvo, G. Santin, T. Sasaki, N. Savvas, Y. Sawada, S. Scherer, S. Sei, V. Sirotenko, D. Smith, N. Starkov, H. Stoecker, J. Sulkimo, M. Takahata, S. Tanaka, E. Tcherniaev, E. Safai Tehrani, M. Tropeano, P. Truscott, H. Uno, L. Urban, P. Urban, M. Verderi, A. Walkden, W. Wander, H. Weber, J.P. Wellisch, T. Wenaus, D.C. Williams, D. Wright, T. Yamada, H. Yoshida, and D. Zschiesche. Geant4\u2014a simulation toolkit. Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment, 506(3):250\u2013303, 2003. ISSN 0168-9002. doi: https://doi.org/10.1016/S0168-9002(03)01368-8. URL https://www.sciencedirect.com/science/article/pii/S0168900203013688.\\n\\nATLAS collaboration. Deep generative models for fast shower simulation in atlas. http://cdsweb.cern.ch/record/2630433/files/ATL-SOFT-PUB-2018-001.pdf, 2018. Accessed: 2022-01-26.\\n\\nDawit Belayneh, Federico Carminati, Amir Farbin, Benjamin Hooberman, Gulrukh Khattak, Miaoyuan Liu, Junze Liu, Dominick Olivito, Vit\u00f3ria Barin Pacela, Maurizio Pierini, et al. Calorimeter-10\"}"}
{"id": "msWIK6SKBK", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"try with deep learning: particle simulation and reconstruction for collider physics.\\n\\nMatthew R Buckley, Claudius Krause, Ian Pang, and David Shih. Inductive caloflow. arXiv preprint arXiv:2305.11934, 2023.\\n\\nErik Buhmann, Sascha Diefenbacher, Engin Eren, Frank Gaede, Gregor Kasieczka, Anatolii Korol, and Katja Kr\u00fcger. Decoding photons: Physics in the latent space of a bib-ae generative network. arXiv preprint arXiv:2102.12491, 2021a.\\n\\nErik Buhmann, Sascha Diefenbacher, Engin Eren, Frank Gaede, Gregor Kasieczka, Anatolii Korol, and Katja Kr\u00fcger. Getting high: high fidelity simulation of high granularity calorimeters with high speed. Computing and Software for Big Science, 5(1):1\u201317, 2021b.\\n\\nErik Buhmann, Sascha Diefenbacher, Engin Eren, Frank Gaede, Gregor Kasieczka, Anatolii Korol, William Korcari, Katja Kr\u00fcger, and Peter McKeown. Caloclouds: Fast geometry-independent highly-granular calorimeter simulation. arXiv preprint arXiv:2305.04847, 2023.\\n\\nAngel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University \u2014 Princeton University \u2014 Toyota Technological Institute at Chicago, 2015.\\n\\nLuke de Oliveira, Michela Paganini, and Benjamin Nachman. Learning particle physics by example: Location-aware generative adversarial networks for physics synthesis. Computing and Software for Big Science, 1(1), Sep 2017. ISSN 2510-2044. doi: 10.1007/s41781-017-0004-6. URL http://dx.doi.org/10.1007/s41781-017-0004-6.\\n\\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. CoRR, abs/1605.08803, 2016. URL http://arxiv.org/abs/1605.08803.\\n\\nMartin Erdmann, Lukas Geiger, Jonas Glombitza, and David Schmidt. Generating and refining particle detector simulations using the wasserstein distance in adversarial networks. Computing and Software for Big Science, 2(1):1\u20139, 2018.\\n\\nMartin Erdmann, Jonas Glombitza, and Thorben Quast. Precise simulation of electromagnetic calorimeter showers using a wasserstein generative adversarial network. Computing and Software for Big Science, 3(1):1\u201313, 2019.\\n\\nMichele Faucci Giannelli, Gregor Kasieczka, Claudius Krause, Ben Nachman, Dalila Salamani, David Shih, and Anna Zaborowska. Fast Calorimeter Simulation Challenge 2022 - Dataset 2, March 2022a. URL https://doi.org/10.5281/zenodo.6366271.\\n\\nMichele Faucci Giannelli, Gregor Kasieczka, Claudius Krause, Ben Nachman, Dalila Salamani, David Shih, and Anna Zaborowska. Fast Calorimeter Simulation Challenge 2022 - Dataset 3, March 2022b. URL https://doi.org/10.5281/zenodo.6366324.\\n\\nMichele Faucci Giannelli, Gregor Kasieczka, Claudius Krause, Ben Nachman, Dalila Salamani, David Shih, and Anna Zaborowska. Fast Calorimeter Simulation Challenge 2022 - Dataset 1, March 2022c. URL https://doi.org/10.5281/zenodo.6368338.\\n\\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf.\\n\\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems, 2017.\"}"}
{"id": "msWIK6SKBK", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "msWIK6SKBK", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Appendix\\n\\nA.1 Shower shape variables\\n\\nWe extend the list of shower shape variables described in Sec. 5.1:\\n\\nLayer Energy Fraction $f_i$. Fraction of the total energy deposited in layer $i$ of the shower.\\n\\n$$f_i = \\\\frac{\\\\bar{E}_i}{E_{tot}}.$$  \\n\\nEnergy Ratio $E_{ratio,i}$. Ratio of the difference between highest and second highest energy intensity point or cell in layer $i$ and their difference.\\n\\n$$E_{ratio,i} = \\\\frac{E_{i[1]} - E_{i[2]}}{E_{i[1]} + E_{i[2]}}.$$  \\n\\nDepth $d$. Deepest layer in the shower with non-zero energy deposit.\\n\\n$$d = \\\\max_i \\\\{i : \\\\max_j (E_{i,j}) > 0\\\\}.$$  \\n\\nLayer/Depth Weighted Total Energy $l_d$. Sum of the layer energies weighted by the layer number.\\n\\n$$l_d = \\\\sum_{i \\\\leq N_i} \\\\bar{E}_i.$$  \\n\\nShower Depth $s_d$. Depth weighted total energy normalized by the total energy in the shower.\\n\\n$$s_d = \\\\frac{l_d}{E_{tot}}.$$  \\n\\nShower Depth Width $\\\\sigma_{s_d}$. Standard deviation of $s_d$ in units of layer number.\\n\\n$$\\\\sigma_{s_d} = \\\\sqrt{\\\\frac{\\\\sum_{i \\\\leq N_i} \\\\bar{E}_i^2}{E_{tot}} - \\\\left(\\\\frac{l_d}{E_{tot}}\\\\right)^2}.$$  \\n\\nBrightest Voxels $E_{k_{brightest\\\\_layer}}$. The $k$th brightest voxel in layer $i$ normalized by the total layer energy. $E_{k_{brightest\\\\_layer}} = \\\\frac{E_{i[k]}}{E_i}.$\\n\\nLayer Sparsity. The ratio of the number of cells with non-zero energy to the total number of cells in layer $i$. This is only valid for the image representation of showers.\\n\\nA.2 Details on different variations of SUPA datasets\\n\\nA.2.1 Parameters\\n\\nFig. 5 shows the remaining parameters used for generating SUPA variations (see Table 2 for details on other parameters). SUPAv1 is most deterministic as particles always split in the first six sub-layers with no deposits ($p_{split} = 1$ and $p_{stop} = 0$ for all sub-layers $< 7$), further since $p_{stop} = 1$ at sub-layer 7, all the particles get deposited. Thus each event/example in SUPAv1 has exactly $128 (= 2^7)$ points.\\n\\nFurther, since $\\\\alpha$ is fixed to 0, all splits are symmetric and energy is always halved at each split, thus all deposits have the same energy value. SUPAv5 has higher $p_{split}$ in the initial sub-layers ($< 7$) than SUPAv2-4, while $p_{stop}$ is the same for all of them, thus SUPAv5 has more number of hits/points than SUPAv2-4 in the respective sub-layers or layers.\\n\\n![Probability Value](image1.png)\\n\\n(a) SUPAv1\\n\\n![Probability Value](image2.png)\\n\\n(b) SUPAv2, SUPAv3, SUPAv4\\n\\n![Probability Value](image3.png)\\n\\n(c) SUPAv5\\n\\nFigure 5: Parameters $p_{split}$, $p_{stop}$, $p_{pass}$ for SUPA variations\\n\\nA.2.2 Shower Shape Variables\\n\\nFig. 6 shows the average events for different variations of SUPA datasets and Figs. Fig. 7 - 12 shows the histograms of the various shower shape variables for all SUPA datasets.\"}"}
{"id": "msWIK6SKBK", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 6: Average event representation for different variations of SUPA datasets\\n\\nFigure 7: Histograms of point level distributions\\n\\nFigure 8: Histograms of feature means\\n\\nFigure 9: Histograms of feature variances\"}"}
{"id": "msWIK6SKBK", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"**A.3 Point Cloud Generative Models**\\n\\nPointFlow [Yang et al., 2019] is a flow-based model with a PointNet-like encoder and a continuous normalizing flow (CNF) decoder. Additionally, the latents (encoder outputs) are modeled with another CNF to enable sampling. We adapted the PointFlow code to handle variable number of points with masking and masked batch norm. The encoder consists of 1D convolutions with filter sizes $128, 128, 256,$ and $512$, followed by a three-layer MLP with $256$ and $128$ hidden dimensions to convert the point cloud into its latent representation of size $128$. The CNF decoder has four conditional concat squash layers with a hidden dimension of $128$ and the latent CNF has three concat squash layers with a hidden dimension of $64$. The overall architecture has $0.7$M trainable parameters.\\n\\nSetV AE Kim et al. [2021] is a transformer-based hierarchical VAE for set-structured data which learns latent variables at multiple scales, capturing coarse-to-fine dependency of the set elements while achieving permutation invariance. We set the number of heads to $4$, the dimension of the initial set to $64$, the hidden dimension to $64$, the number of mixtures for the initial set to $4$, and $15$. \\n\\n---\\n\\n**Figure 10:** Histograms of various shower shape variables\\n\\n**Figure 11:** Histograms of layer centroids\\n\\n**Figure 12:** Histograms of layer widths\"}"}
{"id": "msWIK6SKBK", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the number of inducing points in the hierarchical setup to \\\\[2, 4, 8, 16, 32\\\\]. The overall architecture has \\\\(0.5\\\\)M trainable parameters. The Transflowmer is a flow-architecture using Real NVP layers [Dinh et al., 2016].\\n\\nAs the events are point clouds of varying cardinality, the coupling layers of the flow are required to be permutation equivariant and able to process a varying number of inputs. To satisfy these constraints, we use transformers [Vaswani et al., 2017] without positional encoding in the coupling layers. The overall architecture consists of 16 coupling layers, each of them is parametrised by a 3-layer transformer with \\\\(d_{model} = 32\\\\). The overall architecture has \\\\(2.1\\\\)M parameters.\\n\\nWe train all the models with \\\\(100K\\\\) training examples.\\n\\nA.4 Experiments on SUPA datasets\\n\\nWe train point cloud generative models, PointFlow [Yang et al., 2019], SetVAE [Kim et al., 2021], and Transflowmer on SUPA datasets. In this section, we show histogram plots to compare the generative performance across different shower shape variables. For all these plots, the axes limits are chosen according to the ground truth data and generated samples can have probability mass outside the shown range.\\n\\nA.4.1 SUPAv1\\n\\nFigs. 13 - 18 show the histograms of various shower shape variables for SUPAv1 and samples generated with PointFlow, SetVAE, and Transflowmer. Figure 13: Histograms of point distributions for \\\\(\\\\eta, \\\\phi,\\\\) and \\\\(r\\\\). Figure 14: Histograms of sample means for different features. Figure 15: Histograms of sample variance for different features.\"}"}
