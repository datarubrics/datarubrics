{"id": "0SMhqvgHST", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.\\n\\n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.\\n\\n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).\\n\\n\u2022 If error bars are reported in tables or plots, the authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.\\n\\n8. Experiments Compute Resources\\n\\nQuestion: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?\\n\\nAnswer: [TODO]\\n\\nJustification: [TODO]\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not include experiments.\\n\\n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.\\n\\n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.\\n\\n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper).\\n\\n9. Code Of Ethics\\n\\nQuestion: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics? https://neurips.cc/public/EthicsGuidelines\\n\\nAnswer: [Yes]\\n\\nJustification: Yes it does abide by the code of ethics to our best of our understanding.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\\n\\n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.\\n\\n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).\\n\\n10. Broader Impacts\\n\\nQuestion: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?\\n\\nAnswer: [No]\\n\\nJustification: It\u2019s a method for finding optimal subsets of benchmarks from a large pool and a framework that automates model encoder evaluation. Societal impacts relate to improved research efficiency and hopefully compute usage, however this is too far from what one would consider strongly tied societal impacts.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that there is no societal impact of the work performed.\\n\\n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.\\n\\n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.\"}"}
{"id": "0SMhqvgHST", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.\\n\\n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.\\n\\n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).\\n\\n11. Safeguards\\n\\nQuestion: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?\\n\\nAnswer: [NA]\\n\\nJustification: It's a benchmark with datasets that are already public and previously published in other papers.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper poses no such risks.\\n\\n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.\\n\\n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.\\n\\n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.\\n\\n12. Licenses for existing assets\\n\\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?\\n\\nAnswer: [Yes]\\n\\nJustification: All datasets used have appropriate licenses, and the code packages used in implementing our software framework have appropriate licenses as well.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not use existing assets.\\n\\n\u2022 The authors should cite the original paper that produced the code package or dataset.\\n\\n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.\\n\\n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.\\n\\n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.\\n\\n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.\"}"}
{"id": "0SMhqvgHST", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.\\n\\n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset's creators.\\n\\n13. New Assets\\n\\nQuestion: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?\\n\\nAnswer: Yes\\n\\nJustification: Our codebase is fully documented.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not release new assets.\\n\\n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.\\n\\n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.\\n\\n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.\\n\\n14. Crowdsourcing and Research with Human Subjects\\n\\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?\\n\\nAnswer: NA\\n\\nJustification: No crowdsourcing with humans\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\\n\\n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.\\n\\n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.\\n\\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects\\n\\nQuestion: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?\\n\\nAnswer: NA\\n\\nJustification: Same as previous answer.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\\n\\n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.\\n\\n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.\\n\\n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.\"}"}
{"id": "0SMhqvgHST", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Model evaluation is a cornerstone of machine learning, guiding model design and progress measurement. Designing generalizable evaluation processes remains a challenge, however, partly due to the vast number of possible domain, task and modality combinations and lack of knowledge of how informative they are. In this paper, we propose EEVEE (Efficient Evaluation process Evolution Engine), a method that frames evaluation process design as a learning problem. By analyzing a large number of evaluation metrics from diverse benchmarks and models, EEVEE identifies a smaller subset of tasks with high predictive power over the full set of evaluation metrics, reducing evaluation time. To find the optimal subset maximizing signal while minimizing GPU hours, EEVEE evaluates pre-trained models of various architectures, pretraining schemes, and modalities on diverse downstream tasks and datasets including image classification, segmentation, relational reasoning, zero-shot image-to-text tasks, medical classification and segmentation, video classification, and regression. Our results identify three subsets of benchmarks, with 8, 15 and 21 tasks, providing high quality signal for model generalization. Key benchmarks selected include iWildCam, CLEVR-Math, ACDC, WinoGround, CIFAR100, Fungi, and ADE20K. We structure the subsets into three tiers for 12, 24, and 36 GPU-hour budgets and package them into a unified, efficient, and user-friendly Python framework that we built with the researcher in mind \u2013 which we refer to as the GATE engine. Our experiments reveal ConvNextV2, SigLIP and CLIP as top-performing model encoders, with EfficientNetV2 and ResNext50 excelling in medical tasks and challenging image classification, in particular in Happy Whale Individual classification, ConvNet based models seem to outperform transformer models by a factor of 2.5x, which is surprising. The top performing encoder being ConvNextV2 followed by CLIP seems to agree with other recent large scale evaluations. We also demonstrate the framework's versatility in fine-tuning models from text and audio modalities, paving the way for future cross-modal evaluations.\\n\\nIntroduction\\n\\nIncreasing Complexities of Benchmarking:\\n\\nAs we create benchmarks for expanding model capability evaluation, the growing number and complexity of these benchmarks inadvertently complicates evaluation, requiring more resources like engineering, computation, and research time. Consequently, prioritizing which benchmarks to use becomes challenging. The high costs and longer wait times of newer, complex benchmarks often deter their adoption, leading researchers to rely on older, simpler benchmarks. This risks missing valuable insights from innovative ideas that may underperform on older benchmarks.\"}"}
{"id": "0SMhqvgHST", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"simpler benchmarks but have broader applicability, while promoting incremental improvements that\\noverfit to simpler benchmarks but underperform in comprehensive evaluations.\\n\\nTo illustrate the mounting increase in available benchmarks, we can look at the historical benchmarks\\nin deep learning. Few benchmarks have had as much impact as ImageNet [29], which remains a\\nrich resource for model training and evaluation, particularly in visuo-linguistic models. As key\\ncapabilities for deep neural networks were discovered, more benchmarks were generated to measure\\nand stimulate progress in those areas. In natural language processing, the GLUE benchmark [65],\\nSQuAD [45], and CoNLL-2003 [48] have been instrumental. In audio processing, LibriSpeech [39],\\nTIMIT [15], and VCTK [68] are widely used. For machine translation, WMT [3], IWSLT [22], and\\nEuroparl [25] have driven advancements. Relational reasoning has been advanced by benchmarks\\nsuch as CLEVR [23], bAbI [66], and RA VEN [71]. In segmentation, PASCAL VOC [14], Cityscapes\\n[8], and COCO [33] remain crucial. Large language models are often evaluated using benchmarks\\nlike SuperGLUE [64], LAMBADA [40], and MMLU [19]. Vision-language models are typically\\nevaluated using benchmarks such as VQA [1], Visual7W [76], and Flickr30k [42].\\n\\nAs a result, a researcher has to choose from all these options, and even more, and then find a\\nway to unify and experiment with their models across all of them. The lack of unification, and\\nthe lack of guarantees for their generalization signal, quickly becomes a kind of \u201cevaluation hell\u201d,\\nwhere researchers waste a lot of time just doing redudant things like fixing the same bugs to\\ndownload datasets, preprocess them etc, while at the same time not having any real signal as to which\\nbenchmarks are more informative, other than just knowing what has been used the most \u2013 which is\\nusually a function of popularity, and not real informativeness. To elaborate, the adoption of complex\\nevaluation processes that could enhance research efficiency and impact is often hindered by the\\nengineering effort required to evaluate machine learning models. Researchers must create involved\\npipelines across multiple datasets demanding high data engineering efforts, develop task-specific\\nadapters, and derive nuanced training recipes, which is time-consuming. As a result, researchers\\noften revert to simpler evaluation strategies instead of comprehensive assessments.\\n\\nA good benchmark should alleviate these burdens by automating dataset handling, integrating task\\nadapters, optimizers, schedulers, and logging mechanisms seamlessly. It should provide broad and\\nmeaningful signals with minimal GPU time, accommodating various computational budgets, ensuring\\ninclusivity. Furthermore, an increasingly important factor for a robust modern benchmark engine\\nis its support for multi-modal learning and early fusion techniques. AI systems must seamlessly\\nintegrate and reason across multiple modalities, such as text, images, audio, and more. Multi-modal\\nlearning enhances self-supervised learning opportunities and provides inherent supervision through\\nnatural alignments, like audio-visual synchronization in videos. Early fusion, where data from\\ndifferent modalities is combined at the initial stages of processing, allows models to leverage shared\\nrepresentations, improving generalization and reasoning capabilities across varied tasks and domains.\\nThese key desiderata are what motivates the production of this work.\\n\\nWith the desiderata in mind, we next introduce EEVEE, a methodology developed for building\\nhigh-signal low-cost evaluation routines, and GATE, the resulting benchmark that is designed to\\nbe extensible, readable, flexible, modular and robust, supported by a new efficient, easy to use\\nframework.\\n\\nEEVEE, Learning Optimal Benchmarks:\\nThe ability to find which benchmarks offer the most\\nsignal with respect to a given goal, such that we can optimize our compute time, research iteration\\nspeed, and engineering time is increasingly crucial. In this work, rather than just manually designing\\na new set of benchmarks, we propose a methodology, called EEVEE (Empirical Evaluation process\\nEvolution Engine) that frames evaluation design as a learning problem and then leverages machine\\nlearning to automate the discovery and refinement of evaluation processes.\\n\\nMore specifically, EEVEE operates by taking in a large set of performance metrics from diverse\\nmodels applied across various benchmarks and identifies a smaller subset of benchmarks with high\\npredictive power over the entire set. EEVEE achieves this through two main components: (a) an\\nevolutionary algorithm to optimize the selection of benchmark combinations based on a computed\\nscore, and (b) a meta-model trained to predict a model's performance on the full set of benchmarks\\nusing performance metrics from a chosen subset. We parameterize the meta-model as a small\\nneural network.\"}"}
{"id": "0SMhqvgHST", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The meta-model receives input performance metrics from a subset of benchmarks and predicts performance on the full set of performance metrics. Through careful k-fold cross-validation and leveraging a diverse set of models and benchmarks, EEVEE iteratively evolves benchmark combinations that offer high information content with respect to the entire spectrum of benchmarks, ensuring robust, efficient and comprehensive evaluation that can be targeted to computational budgets ranging from more \\\"GPU Poor\\\" users to high-budget organizations.\\n\\nTaking the desiderata explained above and the resulting understanding of what a good evaluation engine should look like, we demonstrate the effectiveness of EEVEE by tasking it with the discovery of benchmark combinations that offer good signal-to-GPU-time ratio, for the evaluation of model encoders \u2013 also referred to as backbones, on their ability to adapt to new tasks, domains, and modalities. For this purpose, we choose a pool of 20 models, varying in their pretraining schemes (e.g CLIP, DINO, ImageNet Classification), architectures (e.g. ResNets, ViTs, ConvNext) and even their source modalities (e.g. Whisper, BERT), which we adapt on 31 benchmarks ranging from image classification, segmentation, relational reasoning, zero-shot image-to-text tasks, medical classification and segmentation, video classification, and regression, using robust fine tuning recipes, and training for 10K iterations, ensuring that the signal we get is about models that are adaptable, generalizable and efficient in their adaptation.\\n\\nBy applying 20 models on 31 benchmarks and employing EEVEE on their resulting metrics, we identify three subsets of benchmarks, each targeted to a specific computational budget range. Some of the key benchmarks that have been selected include iWildCam, CLEVR-Math, ACDC, WinoGround, mini-ImageNet, Fungi, ADE20K, and dtextures. We refer to the discovered subsets as Tiers, and assign to them identifiers for their sizes, specifically, small (n=8, 12 GPU hours), base (n=15, 24 GPU hours) and big (n=31, 36 GPU hours). We package these tiers into our comprehensive benchmarking suite and software framework (called GATE) designed for domain, task and modality transferability evaluation, which facilitates the transfer of neural network encoders to different modalities, domains, and tasks. GATE's architecture caters to the research community, enabling straightforward replacement of these transferable encoders with minimal effort. With these innovations, GATE seeks to evolve the landscape of model encoder evaluation, championing a deeper understanding of transfer learning and model adaptability.\\n\\nContributions:\\n1. We introduce EEVEE, a machine learning approach for selecting subsets of benchmarks optimized to offer maximal predictive power over a larger benchmark set. 2. We conduct a comprehensive investigation of diverse benchmarks within the space of image, image+text and video modalities, pinpointing those with the highest predictive value for a model's performance in downstream tasks. We apply EEVEE to model encoder evaluation by training 20 models on 31 benchmarks, identifying subsets of 8, 15 and 21 benchmarks that offer high signal-to-GPU-hour ratios. 3. We pack the EEVEE-discovered subsets (of 8, 15 and 21 benchmarks out of 31 benchmarks) into targeted benchmark packs, referred to as tiers, designed for specific compute budgets (of 12, 24 and 36 GPU hours) and project phases, and establish standard experimental settings for these tiers. We call these collectively as the GATE Benchmarks. 4. We develop the GATE engine, a unified benchmark suite and software framework that automates dataset downloading, preprocessing, and pipelining for fine tuning and evaluation. GATE facilitates the incorporation of new model encoders, adapts input modalities, fine-tunes with robust recipes, and logs critical information such as training and evaluation metrics, power, energy, computational usage, task visualizations, and model gradients per layer. 5. Through our extensive investigation, we identify foundation models demonstrating superior transferability across diverse tasks. 6. We advocate for the inclusion of modality-shifting transfer experiments in the standard evaluation process for ML researchers, supported by our experimental results on the performance of existing foundation models in these benchmarks.\\n\\n2 Related Work\\nOn the Diversity of Benchmarks:\\nThere is a vast array of benchmark suites in machine learning. To the best of our knowledge, the benchmark suites relating strongly to GATE are ImageNet [9], VTAB [70], VLMBench [73] and WILDS [26]. ImageNet has been of tremendous importance and interest to the transfer learning community. Nevertheless, there has been skepticism about overfitting to such datasets resulting from implicitly qualifying models using the test set performance over the years [46, 6] or the test set not being challenging enough to gauge model generalization power [47]. Although ImageNet pre-training helps transfer performance to the many-shot classification setting [13], it provides minimal to no gains on more challenging datasets such as fine-grained\"}"}
{"id": "0SMhqvgHST", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Desiderata\\n\\nBenchmark\\n\\nTable 1: Our Desiderata (first column) VS Benchmarks (first row)\\n\\nclassification [27]. Similarly, with a larger distribution shift, ImageNet pre-trained models was\\n146 found to offer limited benefits for medical imaging tasks due to large distribution shifts induced by\\n147 fundamental differences in data sizes, features, and task specifications; that is, lightweight models\\n148 perform comparably to standard architectures [44]. To make matters worse, ImageNet performance\\n149 is less correlated with and less predictive of downstream performance on diverse tasks beyond\\n150 classification such as object detection, few-shot classification, and segmentation [13]. On top of it all,\\n151 when ImageNet is extended with a perturbed temporal dimension, models performance significantly\\n152 worsen [52].\\n\\nOn the Usability of Benchmarks:\\n\\nBeyond ImageNet, VTAB introduced a benchmark with a wider\\n154 diversity of tasks and domains [70]. Nevertheless, it does not offer task and domain shifts offered\\n155 in GATE, such as medical segmentation and video classification and regression that are known to\\n156 be ill-measured and gauged by ImageNet alone [44, 52]. That said, VTAB offers satellite imaging\\n157 and 3D tasks which GATE does not. Nevertheless, GATE as a software framework was optimized to\\n158 minimise usage friction, to take no more than 12 GPU hours on our smallest tier, and, to only require\\n159 approximately 1 hour of adding the new encoder and wrapping it into GATE wrappers for GATE to be\\n160 able to go away and take care of everything, including dataset downloading, task adapter integration\\n161 and full train/val and test cycles with logging of various key metrics. VTAB, in our experience,\\n162 requires a lot more manual work in getting the datasets, and integrating new models to be adapted.\\n163 Similarly, VLMBench [73] and WILDS [26] offer more diverse datasets beyond previous work but\\n164 neither offer a tiered approach that enables iterative development of models during pre-training, nor\\n165 produce extensible and flexible benchmarks that can be easily glued into researchers experimentation\\n166 code without friction.\\n\\nOn the Systematic Selection of Benchmarks:\\n\\nPrevious work investigated the properties inherit\\n168 in multi-task benchmarks that trade-off diversity and sensitivity where the latter is how robust a\\n169 benchmark ranking is to the inclusion of irrelevant models or minute changes in the tasks themselves\\n170 [72]. It was found that multi-task benchmark are unstable to irrelevant changes in tasks design.\\n171 Nevertheless, this is related to how the benchmark ranks models; whether it compares how model often\\n172 ranks higher than another in cardinal benchmarks or if the performance across tasks is averaged to\\n173 produce a single rank in cardinal ones. Meanwhile, our benchmark produces fine-grained information\\n174 to model performances across diverse tasks rather than producing specific ranking which is delegated\\n175 to the user analysis. Another complementary thread of work investigates dynamic benchmarks where\\n176 model training and data collection is interleaved to continually challenge model knowledge [53]. To\\n177 the best of our knowledge, this is the first work that studies the selection of multi-task, multi-domain\\n178 benchmarks that satisfy limited compute budgets while maximizing research signal.\\n\\nIn summary, Table 1 shows the desiderata that we believe a good evaluation suite and framework\\n180 should have such that they can both offer the community useful signal, and also balance that with\\n181 being practical so that people can adopt it.\\n\\n3 EEVEE Methodology\\n\\nEEVEE is our proposed method for automating the selection of Pareto-optimal benchmark subsets.\\n184 By analyzing benchmark performance metrics, EEVEE identifies a small, highly informative subset\\n185 that maximizes information relative to the entire benchmark pool. This ensures that, as machine\\n186 learning benchmark breadth and depth increases, we will always be able to identify and select few that\\n187 offer high information about the whole. We strike a balance between providing rich evaluation signals\\n188 and maintaining simplicity, reducing computational costs and human efforts required for adopting\\n189 new benchmarks. EEVEE enables the production of a tiered evaluation engine accommodating\\n190 various computational budgets, fostering an inclusive and accessible research environment, and\\n191 improving the quality of insights derived from machine learning research while addressing reluctance\\n304x42\"}"}
{"id": "0SMhqvgHST", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"towards resource-intensive evaluation processes. This balance between efficiency, simplicity, and signal richness presents EEVE\u2019s value proposition for advancing machine learning research.\\n\\nWorking Principle of EEVEE:\\n\\nEEVEE works by building a meta-model over the performance metrics of models sufficient both in number and diversity, on the full benchmark pool from which we want to choose our subset. With the term benchmark in this paper we refer to a dataset + task pairs.\\n\\nFormally, given a large benchmark pool $B = \\\\{b_0, b_1, \\\\ldots, b_K\\\\}$, where $B$ is the full set of benchmarks, and $b_i$ are individual benchmarks therein, we have a sufficiently large and diverse pool of model performance metrics $M = \\\\{m_{00}, m_{01}, \\\\ldots, m_{NK}\\\\}$. Here, $m_{ji}$ is the performance metric of model $j$ on benchmark $b_i$. We aim to discover a subset of $B$ of size $k$. This means $k$ total benchmarks make up the subset. If we build a meta-model $g(M_{\\\\text{selected}}, \\\\theta)$ to predict all of $M$ given only the selected subset $M_{\\\\text{selected}}$, it should minimize the following loss:\\n\\n$$L_{\\\\text{EEVE}} = \\\\text{MSE}(M, g(M_{\\\\text{selected}}, \\\\theta))$$\\n\\nIn this equation, MSE is the mean squared error. $M$ represents the full set of performance metrics of all our models on the full benchmark pool $B$. The term $g(M_{\\\\text{selected}}, \\\\theta)$ represents the predictions of the meta-model $g$ with parameters $\\\\theta$ when it is given the performance metrics of all models from the selected subset of benchmarks $B_{\\\\text{selected}}$, referred to as $M_{\\\\text{selected}}$.\\n\\nHowever, our main focus lies in the selected combination of performance metrics $M_{\\\\text{selected}}$ that can generalize well on previously unseen models. To that end, we must split $M$ into train, validation and test sets, each consisting of performance metrics acquired from different models (e.g. train $\\\\rightarrow$ ResNet50, ViT-Base, CLIP, and val $\\\\rightarrow$ ResNext50, DINO, DeIT), and explicitly optimize the inner loop test loss rather than the training loss, while we use the validation loss to select the best meta-model for test. Hence the loss we wish to minimize is:\\n\\n$$L_{\\\\text{test}} = \\\\text{MSE}(M_{\\\\text{test}}, g(M_{\\\\text{test}}_{\\\\text{selected}}, \\\\theta))$$\\n\\nWe need a non-differentiable method for choosing the $k$ benchmarks in $M_{\\\\text{selected}}$, since brute force becomes intractable very quickly, so we employ evolutionary methods to learn the $k_{\\\\text{selected}}$ benchmarks.\\n\\nThis results in a bi-level optimization, with an evolutionary method on the outer loop $e(B_{\\\\text{selected}})$, where $e$ is the evolutionary method, and $B_{\\\\text{selected}}$ are the benchmarks being selected \u2013 or indeed, the genes being optimized, and a small meta-model parameterized as a neural network $g(\\\\theta)$ that receives a train/val split from $B_{\\\\text{selected}}$ and trains itself to do the task described in Equation 1, after which process it is scored using the val set using the loss in Equation 2. Then, once a given candidate of benchmarks $B_{\\\\text{selected}}$ is scored, in this way, the outer loop performs a tournament selection where only the top 50 candidates are preserved and mutated by removing one benchmark at random, and adding another at random. Each winning candidate mutates into 10 children, and the parent is also preserved in the gene pool, producing a gene pool with 550 candidates for every cycle. At initialization, we sample 1000 random combinations. We have found that 1000 is a good starting population that is both tractable to score and facilitates the necessary diversity that enables limited variation in results across several runs, showcasing convergent behaviour. diversity that our results across runs have little variation from one another, pointing to a convergent behaviour. We include full pseudocode showcasing all the details related to how we performed EEVEE for our experiments in Algorithm 1, 2 and 3 in Figure 1.\\n\\nApplying EEVEE on Model Encoder Generalization\\n\\nWhy Model Encoder Evaluation?\\n\\nA common practice across machine learning applications involves augmenting general model encoders with task-oriented heads. The adaption of this paradigm can be attributed to the computational efficiency associated with training model encoders, over more expensive setups. Much of computer vision, as well as vision to text search and retrieval happen using model encoders. Similarly, various applications requiring translation from one domain/modality/task to another require an encoder of some sort. Even the \u201cdecoder-only\u201d LLM models that have demonstrated incredible capabilities in the last few years, internally can be seen as a series of representation encoders, a series of refinement before they reach the decoding stage.\"}"}
{"id": "0SMhqvgHST", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"A. Adaptation Process\\n\\nPreparations: Choosing Models, Benchmarks and Adaptation Processes:\\n\\n1. Model to Evaluate\\n2. Dataset\\n3. Task\\n\\n| Decoder Model | Task Head | or | Model to Evaluate |\\n|---------------|-----------|---|-------------------|\\n\\n- **Big:** Each having 8, 15 and 21 benchmarks within it,\\n- **Small:**...\"}"}
{"id": "0SMhqvgHST", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"modifications, described in Figure 2, we use a fine tuning scheme \u2013 this decision was informed by preliminary experiments on both full fine tuning and linear probe with a frozen backbone, in which we found that there was a clear superiority of fine tuning over linear probing for the benchmarks we chose in our pool. Full details of these preliminary experiments can be found in Appendix 8.1. In our preliminary experiments we were able to identify three recipes, one for ConvNet-style architectures, one for ViT-style architectures and one for Hybrid architectures such as ConvNext and ResNext that worked well for all tasks, details in 8.1.\\n\\nB. Model Pool\\n\\nWe wanted the space of models used to cover many important pretraining schemes, architectures, and source modalities. The details of these choices are provided next:\\n\\n1. Pretraining Task and Dataset Variation: With a consistent architecture, models were subjected to various pretraining tasks and datasets. Model instances representing this category include CLIPViT [43], ConvNextV2 [35], Siglip, FlexViT [7], LaionViT, ImageNet1K ViT [11] with Random Augment, SAM-ViT, DiNoViT, EfficientFormerV2 [32] and DeiT3 [59]. Further to these, we include models initialized from scratch, specifically, ViT, ResNet50 [18], EfficientNetV2, and fine-tuned on the GATE tasks.\\n\\n2. Architectural Variation: We explored models having the same pretraining dataset (ImageNet), but differing in their architecture. This group encompassed a mix of standard CNN models such as EffNetV2, ResNet50, ResNext50 [67], ConvNextV2_Base [35] and transformer-based models like EfficientFormer [32] and FlexViT [7].\\n\\n3. Modality and Dataset Variation: This axis comprised models trained on modalities other than vision such as Whisper, coming from an audio to text task and Bert [10], Bart [31] and Mpnet [55] coming from various text-based tasks. These models had their original input processing systems replaced by a Vision Transformer style embedding and were subsequently fine-tuned on the GATE tasks. A more comprehensive account of these models, including their selection rationale and unique characteristics, is provided in the Appendix Section 13.\\n\\nC. Benchmark Pool\\n\\nThe benchmark pool, detailed in the Appendix, includes Image Classification (ImageNet1k [9], CIFAR100 [28], Places365 [74], Food101 [36], HappyWhale [17]), Few Shot Image Classification (Aircraft [37], Fungi [50], MiniImageNet [62], CUB200 [63], Describable Features [69]), Zero Shot Text-Image Classification (Flickr30K [41], New Yorker Caption Context [20], Winoground [58]), Visual Relational Reasoning (CLEVR [23], CLEVRMath [34]), Image Semantic Segmentation (ADE20K [75], COCO10K [33], COCO164K [33], NYU-Depth-v2 [54], PascalContext [38], Cityscapes [8]), Medical Image Classification (Chexpert [21], Diabetic Retinopathy [16], HAM10000 [60]), Medical Segmentation (ACDC [5]), Video Classification (HMDB51 [30], UCF-101 [56], Kinetics400 [24]) and Video Regression (iWildcam [4]).\\n\\nProducing Diverse Model Performance Metrics: We apply our adaptation process on each and every model chosen, on every benchmark in the benchmark pool. To acquire test results we ensemble by averaging logits of the top 1, 3 and 5 validation models to produce three separate ensemble results.\\n\\nD. Experimental Approach\\n\\nWe wanted our research environment to reflect the end user, so we can properly understand their needs, and to offer a pragmatic experimental setup of in-the-wild researchers with little time to hyperparameter optimize, and which have to make decisions on small amounts of preliminary experiments \u2013 someone choosing a model encoder off the shelf and adapting it to downstream setting. For that reason, we kept any hyperparameter tuning, or human attention when it came to specific models to a minimum. Instead, we relied on existing good recipes, and did some preliminary experiments as explained in detail in 8.1. Briefly, we discovered specific adjustments for each architecture type: for Convolutional Architectures, we used AdamW with a learning rate of 1e-3, and 6e-4 for segmentation tasks; for Vision Transformer Architectures, AdamW with a learning rate of 1e-5; and for Convolutional + Transformer Hybrid Architectures, AdamW with a learning rate of 2e-5. A plateau learning rate scheduler was configured with parameters like mode \u201cmin\u201d, factor 0.5, patience 1000, and threshold 1e-4, allowing models to effectively choose their own schedules based on their learning progress. This adaptive scheduling facilitated \u201cgood enough\u201d learning rates and enhanced performance across different architectures.\\n\\n4 Results\\n\\nSingle Benchmark Predictiveness: As demonstrated in Figure 3, using EEVEE we quantified the predictive power of each benchmark on its own, when not in a combination with others. We have found that ADE20K, Flickr30K, and the New York Caption Competition lead in their predictive power, with few-shot tasks, and relational reasoning, being very close to the best in predictive power. ImageNet1K sits squarely in the middle of the competition. Furthermore, some of the most \u201cnovel\u201d\"}"}
{"id": "0SMhqvgHST", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: The EEVEE MSE Loss (k=1) shows \u201cpredictiveness over the whole,\u201d with lower values being better. Benchmarks like iWildcam, HappyWhale, and WinoGround test unique capabilities and may not predict all tasks, yet EEVEE often includes at least two of these in its top combinations along with a \u201cnatural-image representative\u201d such as CIFAR100, ADE20K or Flickr30K.\\n\\nPerformance Loss (Higher means more important)\\n\\nDataset Name\\n\\nFigure 4: Degradation of predictive power when a given benchmark is removed and the meta-model trained from scratch, for different GATE tiers. Benchmarks like iWildcam, HappyWhale, ACDC, NYU and Winoground are the least predictive tasks, WinoGround being magnitudes less predictive. We argue that this is mainly due to the tasks being \u201charder,\u201d and our models being less designed for those. The results in WinoGround were barely better than chance for example. However, when once we move to combinations of benchmarks, these \u2018less\u2019 predictive benchmarks become key contributors to better predictive power, as they represent edge cases, as can be seen in Figures 6g, 7c, 7i, where these have the highest importance when removed from a given set.\\n\\nPredictiveness of Discovered Combinations\\n\\nIn Figure 5, we can see how the top-50 performing candidate combinations perform as we vary the number of benchmarks per combination from 1 to 26. We can see that there is a point of diminishing returns around the $k=8$ point, after which there appears to be some \u201coverfitting\u201d occurring. We verified that the overfitting was a result of having a small sample number of 20 models, to train, val and test our meta-models with, as well as the 2-layer MLP we used to model Few-to-All metric predictions. We tried our level best to find the best architecture and regularization schemes for our meta-model, and this was the best we could do given available compute and (human) time. We chose 8, 15, and 21 as the combination-threshold to make our packs out of as they satisfied the computational budgets we set for ourselves, and they have very diverse and predictive tasks, as can be seen in Figures 6g, 7c, 7i. For full details on all the discovered top-k combinations please look at Appendix Section 16.1.\\n\\nBest Models based on GATE: As can be seen in Table 2, or the Appendix extended Table 3, the best overall models are ConvNextV2, SigLIP and CLIP in that order, with SigLIP and CLIP often exchanging ranks between themselves. However, it is worth noting that EfficientNetV2 demonstrated exceptional performance/compute across all tasks, and even outperformed all models in many medical tasks. Finally, ConvNet based models, and particularly ResNext50 seem to have done exceptionally well in the edge-case scenarios of ACDC, Happy Whale Individual identification, and...\"}"}
{"id": "0SMhqvgHST", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"exceptional, efficient alternative for the medical domain and for benchmarks, and offers a researcher-designed environment in which one can easily port their own models prefixed with 's' refer to 'from scratch' trained models,\\n\\n| Task Mean 61.9 | Flickr30K Img2Txt | Flickr30K Txt2Img | ImageNet VQA | ImageNetV6 | ImageNetV6b | ImageNetV6c | CLEVR Colour |\\n|---------------|------------------|------------------|--------------|-----------|------------|------------|-------------|\\n| Base GATE Mean | 60               | 32               | 60           | 55        | 61         | 68         | 60          |\\n| Small GATE Mean| 63.1             | 36.4             | 61           | 61.4      | 60.5       | 55.9       | 61          |\\n\\n| Task Mean 84.4 | Video Class | UCF-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|-------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 61.9 | Base GATE Rank | Task Mean 21   | Task Mean | Task Mean 61.9 | Task Mean 7       |\\n| 89            | 61          | 21            | 61.9                  | 75               | 96                  |\\n\\n| Task Mean 99.1 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 89  | 99.1          | 96                     | 99.1            | 99.1                |\\n\\n| Task Mean 96 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 96 | 96            | 99                     | 96              | 96                  |\\n\\n| Task Mean 95 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 95 | 95            | 99                     | 95              | 95                  |\\n\\nTask Mean 93 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 93 | 93            | 99                     | 93              | 99                  |\\n\\n| Task Mean 91 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 91 | 91            | 99                     | 91              | 99                  |\\n\\n| Task Mean 90 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 90 | 90            | 99                     | 90              | 99                  |\\n\\n| Task Mean 89 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 89 | 89            | 99                     | 89              | 99                  |\\n\\n| Task Mean 88 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 88 | 88            | 99                     | 88              | 99                  |\\n\\n| Task Mean 87 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 87 | 87            | 99                     | 87              | 99                  |\\n\\n| Task Mean 86 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 86 | 86            | 99                     | 86              | 99                  |\\n\\n| Task Mean 85 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 85 | 85            | 99                     | 85              | 99                  |\\n\\n| Task Mean 84 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 84 | 84            | 99                     | 84              | 99                  |\\n\\n| Task Mean 83 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 83 | 83            | 99                     | 83              | 99                  |\\n\\n| Task Mean 82 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 82 | 82            | 99                     | 82              | 99                  |\\n\\n| Task Mean 81 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 81 | 81            | 99                     | 81              | 99                  |\\n\\n| Task Mean 80 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 80 | 80            | 99                     | 80              | 99                  |\\n\\n| Task Mean 79 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 79 | 79            | 99                     | 79              | 99                  |\\n\\n| Task Mean 78 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 78 | 78            | 99                     | 78              | 99                  |\\n\\n| Task Mean 77 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 77 | 77            | 99                     | 77              | 99                  |\\n\\n| Task Mean 76 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 76 | 76            | 99                     | 76              | 99                  |\\n\\n| Task Mean 75 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 75 | 75            | 99                     | 75              | 99                  |\\n\\n| Task Mean 74 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 74 | 74            | 99                     | 74              | 99                  |\\n\\n| Task Mean 73 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 73 | 73            | 99                     | 73              | 99                  |\\n\\n| Task Mean 72 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 72 | 72            | 99                     | 72              | 99                  |\\n\\n| Task Mean 71 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 71 | 71            | 99                     | 71              | 99                  |\\n\\n| Task Mean 70 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 70 | 70            | 99                     | 70              | 99                  |\\n\\n| Task Mean 69 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 69 | 69            | 99                     | 69              | 99                  |\\n\\n| Task Mean 68 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 68 | 68            | 99                     | 68              | 99                  |\\n\\n| Task Mean 67 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 67 | 67            | 99                     | 67              | 99                  |\\n\\n| Task Mean 66 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 66 | 66            | 99                     | 66              | 99                  |\\n\\n| Task Mean 65 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 65 | 65            | 99                     | 65              | 99                  |\\n\\n| Task Mean 64 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 64 | 64            | 99                     | 64              | 99                  |\\n\\n| Task Mean 63 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 63 | 63            | 99                     | 63              | 99                  |\\n\\n| Task Mean 62 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 62 | 62            | 99                     | 62              | 99                  |\\n\\n| Task Mean 61 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 61 | 61            | 99                     | 61              | 99                  |\\n\\n| Task Mean 60 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 60 | 60            | 99                     | 60              | 99                  |\\n\\n| Task Mean 59 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 59 | 59            | 99                     | 59              | 99                  |\\n\\n| Task Mean 58 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 58 | 58            | 99                     | 58              | 99                  |\\n\\n| Task Mean 57 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 57 | 57            | 99                     | 57              | 99                  |\\n\\n| Task Mean 56 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 56 | 56            | 99                     | 56              | 99                  |\\n\\n| Task Mean 55 | Food-101 Acc@1 | PASCAL VOC 2012 Acc@10 | Cityscapes mIoU | HWhale Species Acc@1 |\\n|---------------|---------------|------------------------|-----------------|---------------------|\\n| Task Mean 55 | 55            | 99                     | 55              | 99                  |\\n\\n| Task Mean 54 | Food-101"}
{"id": "0SMhqvgHST", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. VQA: Visual question answering. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 2425\u20132433, 2015.\\n\\n[2] Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas. Revisiting feature prediction for learning visual representations from video, 2024.\\n\\n[3] Loic Barrault, Ondrej Bojar, Marta R Costa-jussa, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, et al. Findings of the 2019 conference on machine translation (WMT19). In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 1\u201361, 2019.\\n\\n[4] Sara Beery, Grant Van Horn, and Pietro Perona. The iwildcam 2018 challenge dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 54\u201360, 2018.\\n\\n[5] Olivier Bernard, Alain Lalande, Caio Zotti, Florence Cervenansky, Xin Yang, Pheng-Ann Heng, Ismail Cetin, Karim Lekadir, Oscar Camara, Miguel A Gonzalez Ballester, et al. Deep learning techniques for automatic MRI cardiac multi-structures segmentation and diagnosis: Is the problem solved? IEEE Transactions on Medical Imaging, 37(11):2514\u20132525, 2018.\\n\\n[6] Lucas Beyer, Olivier J. H\u00e9naff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00e4ron van den Oord. Are we done with ImageNet?, 2020.\\n\\n[7] Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim M. Alabdulmohsin, and Filip Pavetic. Flexivit: One model for all patch sizes. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14496\u201314506, 2022.\\n\\n[8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Tobias Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3213\u20133223, 2016.\\n\\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Li Kai, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255. IEEE, 2009.\\n\\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n\\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n\\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021.\\n\\n[13] Linus Ericsson, Henry Gouk, and Timothy M. Hospedales. How well do self-supervised models transfer? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5414\u20135423, June 2021.\\n\\n[14] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International Journal of Computer Vision, 88(2):303\u2013338, 2010.\"}"}
{"id": "0SMhqvgHST", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "0SMhqvgHST", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "0SMhqvgHST", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In Proceedings of the 36th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 2019.\\n\\nErik F Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003, pages 142\u2013147, 2003.\\n\\nAdam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. In Advances in Neural Information Processing Systems (NeurIPS), pages 4967\u20134976, 2017.\\n\\nDirk Schroeder, Yin Cui, Yang Chai, Daniel Kristensen, Evangelos Kalogerakis, and Serge Belongie. The fgvcx fungi classification challenge. In CVPR Workshops, 2018.\\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, 2016.\\n\\nVaishaal Shankar, Achal Dave, Rebecca Roelofs, Deva Ramanan, Benjamin Recht, and Ludwig Schmidt. Do image classifiers generalize across time? In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 9661\u20139669, October 2021.\\n\\nAli Shirali, Rediet Abebe, and Moritz Hardt. A theory of dynamic benchmarks, 2023.\\n\\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In Proceedings of the European Conference on Computer Vision (ECCV), pages 746\u2013760, 2012.\\n\\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-training for language understanding. arXiv preprint arXiv:2004.09297, 2020.\\n\\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. In arXiv preprint arXiv:1212.0402, 2012.\\n\\nMingxing Tan and Quoc V Le. Efficientnetv2: Smaller models and faster training. arXiv preprint arXiv:2104.00298, 2021.\\n\\nTristan Thrush, Hongyu Jiang, Goutham Prasad, and Jacob Andreas. Winoground: Probning vision and language models for visio-linguistic compositionality. arXiv preprint arXiv:2204.03162, 2022.\\n\\nHugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard Grave, Armand Joulin, Gabriel Synnaeve, and Jakob Verbeek. Deit iii: Revenge of the vit. arXiv preprint arXiv:2204.07118, 2022.\\n\\nPhilipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Scientific Data, 5:180161, 2018.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), pages 5998\u20136008, 2017.\\n\\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In Advances in Neural Information Processing Systems (NeurIPS), pages 3630\u20133638, 2016.\\n\\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.\"}"}
{"id": "0SMhqvgHST", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 3266\u20133280, 2019.\\n\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\u2013355, 2018.\\n\\nJason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merrienboer, Armand Joulin, and Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.\\n\\nSaining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1492\u20131500. IEEE, 2017.\\n\\nJunichi Yamagishi, Christophe Veaux, and Kirsten MacDonald. Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit (version 0.92). University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2019.\\n\\nFisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Fine-grained visual comparisons with local learning. In 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 192\u2013199. IEEE, 2014.\\n\\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. In International Conference on Learning Representations, 2020.\\n\\nChi Zhang, Feng Gao, Baoxiong Jia, Song-Chun Zhu, and Yixin Zhu. Raven: A dataset for relational and analogical visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5317\u20135327, 2019.\\n\\nGuanhua Zhang and Moritz Hardt. Inherent trade-offs between diversity and stability in multi-task benchmarks, 2024.\\n\\nKaizhi Zheng, Xiaotong Chen, Odest Chadwicke Jenkins, and Xin Eric Wang. VLMbench: A compositional benchmark for vision-and-language manipulation. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks, 2022.\\n\\nBolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. In IEEE Transactions on Pattern Analysis and Machine Intelligence, volume 40, pages 1452\u20131464. IEEE, 2017.\\n\\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5122\u20135130, 2017.\\n\\nYuke Zhu, Olaf Groth, Michael S Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4995\u20135004, 2016.\"}"}
{"id": "0SMhqvgHST", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For an end-user to use GATE, they need to:\\n\\n1. Install the GATE framework python package, as described in the Github repo's readme page.\\n\\n2. Choose a path for implementing the new foundation model encoder they wish to evaluate. This is either cloning the full GATE repo and modifying existing components directly, or, importing the `GATEncoder` and `GATEModel` classes from GATE, and wrapping up their model within it. Doing so requires the researcher to implement a relevant forward function that can take in the modalities their model needs to process, as well as defining a configuration that tells GATE what modalities a model can receive and output features on, as well as any transforms needed for a batch to be ready for their model.\\n\\n3. The user chooses a GATE tier to use (from `smallGATE`, `baseGATE` and `bigGATE`). Based on the configuration defined by the user in step 2.\\n\\n4. GATE generates a list of commands, each representing an experiment that needs to be run, and can then run these commands on your local GPU box, parallelizing the tasks, one on each available GPU, or, can provide a list of commands or json file that one can use to run these commands on a GPU cluster, or other hardware.\\n\\n5. GATE emits a wandb project, with metrics, visualizations and other measures, allowing easy tracking of experiments, and sharing thereof, as well as huggingface model weights for each model being trained \u2013 which is also used to achieve a stateless execution.\\n\\n6. Once the experiments are completed, one can invoke the `produce-analysis.py` file within GATE to get tables and figures that analyse the data, similar to what appears in this paper. Those results can then be used to report results in a paper, or, be used to make decisions for production models.\\n\\nThis process ensures the GATE framework is aware of what a model's supported modalities are, as well as how to produce modality-specific features, given the model. Once this is completed, the user, with a single line of code, can select a GATE tier, and launch all jobs needed to produce results for that tier. Importantly, GATE is made to facilitate and encourage foundation models that are diverse in their capabilities, and allow the researchers to focus on what matters \u2013 that is, designing and training their foundation model \u2013 rather than spending the majority of their time building and optimizing evaluation boilerplate. Furthermore, the diversity of signal that GATE provides allows better understanding of a given model's strengths and weaknesses, which as a result makes the research, review and iteration process of the field as a whole more efficient. This is because there is a consistent boilerplate that runs all models, with broad signal that reduces probability of making erroneous conclusions \u2013 both in the overly optimistic, or overly pessimistic side of things.\\n\\n### 6.1 Principal Use Cases\\n\\n1. **Model Development and Iteration**: GATE serves as a valuable tool during the model research and development phase. By integrating the model into GATE and running either the `smallGATE` or `baseGATE` tiers, developers can obtain a comprehensive and robust performance evaluation of their model across diverse domains, tasks, and modalities. Worth noting that GATE allows easy inclusion of foundation models pretrained on images, video, audio, text, etc., to be fine-tuned on pixel-based tasks. It achieves this by replacing a model's root layer / embedding layer, with one appropriate for a given task's modality, and adding on top a relevant task adapter head.\\n\\n2. **Model Evaluation for Machine Learning Research**: GATE enhances the communication of research findings and their potential applications, a vital aspect of scientific collaboration. By using GATE as a benchmark, even at the most cost-efficient GPU hour level of `smallGATE`, the clarity and depth of future ML papers can be significantly improved. GATE's explicit evaluation of modality, domain, and task shifts in a given foundation model provides a nuanced and informative perspective on a model's true capabilities, offering a more detailed understanding of a model's strengths and weaknesses than optimizing a single metric, such as ImageNet validation error.\"}"}
{"id": "0SMhqvgHST", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The results were logged in WandB, and then further processed after all experiments were completed to generate the tables and figures in this paper. Much of the logged information outside of testing metrics were not used for any of the figures and tables in this paper. The full set of experiments and all the logged results can be found at our wandb gate project repo.\\n\\n7.1 Result Processing\\n\\nOnce all experiments were completed, we queried our wandb project repository and returned test results from all our experiments. If an experiment name was duplicated, we used the latest entries, and, for each experiment type there existed three independent runs. We averaged the results of any metrics across such independent runs to acquire a better approximation to the true performance of those models.\\n\\n8 Preliminary Experiments Details\\n\\n8.1 Preliminary Experiments\\n\\nFirst, we trained models on ImageNet1k, CIFAR100, CLEVR, ADE20K, CityScapes, and ACDC for 5K iterations, using cosine annealing learning schedule or plateau annealing, with AdamW, weight decays varying from 0.1 - 0.0001, and applied models from each major architecture category \u2013 specifically, the CLIPViT, ImageNet pretrained ViT, ResNext, ResNet and ConvNextV2. The results from these experiments pointed to the fact that there exists one general and good recipe for each architecture style. The recipes that we discovered were as follows:\\n\\n8.1.1 Across Architecture Settings\\n\\nUnless otherwise stated, the settings here are applied universally in all experiments.\\n\\nOptimizer: AdamW, weight decay 0.01, plateau annealing with patience 1000, relative scaling and scale factor 0.5, and, threshold 0.0001.\\n\\nTraining Details: Training iterations: 10K, validate every 500 iterations.\\n\\nTest Details: Top-3 validation models (across all validated checkpoints) are ensembled by prediction averaging.\\n\\n8.1.2 Architecture Specific Settings\\n\\nConvolutional Architectures: Optimizer: AdamW, learning rate 1e-3, and for segmentation tasks only, we used learning rate 6e-4\\n\\nVision Transformer Architectures: Optimizer: AdamW, learning rate 1e-5\\n\\nConvolutional + Transformer Hybrid Architectures: Optimizer: AdamW, learning rate 2e-5\\n\\nThe above recipes were what we used throughout all our experiments unless otherwise stated.\\n\\n9 GATE Guiding Principles\\n\\nThe fundamental values driving the design decisions behind GATE are the following:\\n\\n1. Maximizing Generalization Signal: GATE is designed to provide a high signal-to-noise ratio concerning a model's ability to generalize in diverse downstream contexts, that vary in domain, task and modality. This allows for a more robust assessment of a model's capacity for adaptation and versatility. By noise here we refer to how clear a given signal response is. For example, an image classification test accuracy signal on ImageNet, would provide clear\"}"}
{"id": "0SMhqvgHST", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"signal with respect to the natural domain and the classification task, but would be blurry for more compositional, object disentanglement and relational tasks, such as segmentation, or, visual question answering.\\n\\n2. Time Efficiency: Acknowledging the importance of computational resources and time, GATE operates within set benchmarks of 12, 24, and 36 GPU hours (established on A100 @ 40GB). These set timeframes ensure GATE's assessments are both thorough and expedient.\\n\\n3. Minimizing Usage Friction: The framework supporting GATE is designed to be user-friendly, enabling easy integration of new backbones and facilitating smooth experimentation. This low-friction approach ensures a streamlined experience when using GATE, making the process of evaluation more efficient.\\n\\nWe argue that a good balance of the above can generate a pragmatic, yet thorough foundation model evaluation suite, that will, importantly, be of real use to most researchers in the field.\\n\\n10 Defining the GATE Benchmark\\n\\nGATE is a comprehensive evaluation engine designed to advance the development of more general machine learning models. It improves on existing benchmarks by enabling the evaluation of models across diverse modalities, domains, and tasks.\\n\\nGATE is composed of three key components. The first is a benchmark pool, a broad collection of datasets, tasks, and processes that measure a model's performance across various domains, tasks, and modalities. The second component is a set of benchmark tiers, which are meticulously curated subsets from the GATE benchmark pool, tailored to specific compute budgets and project phases. The final tier is a software framework, designed to seamlessly integrate new foundation models and execute the GATE tiers, thereby enabling efficient performance evaluation across a diverse range of downstream modalities, domains, and tasks. Practically, GATE is directed towards machine learning researchers and developers as a means to efficiently, and with little friction, get broad signal about how their model performs after transfer in diverse contexts, specifically selected for their empirically evaluated high signal-to-noise ratio with respect to predictive power in how a model performs in previously unseen contexts.\\n\\nBuilding GATE was a careful balancing act. We needed to respect specific time budgets while also aiming for a wide variety of evaluation scenarios. Our approach was as follows:\\n\\n1. Select a diverse set of learning contexts, spanning multiple domains, tasks and modalities. We refer this as the Benchmark Pool.\\n\\n2. Select a broad set of key foundation models, varying in their architecture, pretraining scheme and source modality. We refer to this as the Model Pool.\\n\\n3. Fine tune each of the models in the model pool, on each of the contexts in the benchmark pool. Evaluate trained models on each context's test sets.\\n\\n4. Use the test set results acquired to quantify the predictive power each benchmark holds with respect to previously unseen benchmarks, both at the individual level and the collection level. We call this measure, the downstream generalization predictability measure (DGPM).\\n\\n5. Use the DGPM values of the various combinations of benchmarks to build the three GATE tiers, selecting combinations of benchmarks that can provide the most information within a target time budget.\\n\\nWe elaborate on each of the above steps in the following subsections.\\n\\n11 Benchmark Pool Selection Details\\n\\nMedical Image Classification: Medical data are known to present a substantial shift in both domain and even modality depending on their format. We have selected datasets that not only pose significant challenges for foundation models but also align with the broader imperative to deliver real-world benefits downstream.\"}"}
{"id": "0SMhqvgHST", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chexpert: A dataset comprising a challenging array of chest x-rays annotated with findings critical to diagnosing thoracic diseases. It tests models on their ability to navigate complex, multi-label medical data, encapsulating the kind of nuanced decision-making that AI must augment in clinical settings.\\n\\nDiabetic Retinopathy Classification: Early detection of diabetic retinopathy from retinal images is a public health priority; models fine-tuned on this dataset can have immediate implications for preventing vision loss on a global scale. This dataset requires models to decipher fine-grained, progressive changes indicative of the disease, reflecting the precision necessary for medical AI applications.\\n\\nHAM10000 (Human Against Machine with 10000 dermatoscopic images): The dataset provides a diverse spectrum of skin lesion images vital for differentiating between benign and malignant conditions. Incorporating this dataset not only challenges the pattern recognition prowess of AI but also contributes to the advancement of dermatology through machine learning technologies.\\n\\nMetrics: We collect Average Precision Score (APS), Area Under the Receiver Operating Characteristics Curve (AUC), and Brier Score (BS) both overall (i.e. macro) as well as for individual pathologies/classes.\\n\\nMedical Segmentation: This category evaluates foundational models\u2019 ability to generalize from natural to medical image modalities and to perform domain-specific tasks that require precision and complex spatial understanding:\\n\\nACDC (Automated Cardiac Diagnosis Challenge): This dataset is aimed at assessing models\u2019 generalization to the medical domain, particularly the transferability of representations for segmenting anatomical structures in cardiac MRI images. By focusing on the heart\u2019s intricate anatomy, ACDC tests the models\u2019 ability to adapt to clinically relevant shapes and patterns\u2014a shift from common visual recognition tasks to precise medical delineation.\\n\\nMetrics: We collect dice loss, mIoU, mean accuracy and overall accuracy.\\n\\n12 Benchmark Pool Details: Having a set of diverse benchmarks ranging in challenge factor, as well as modality, task and domain shift was key. We explain in more detail why we consider these factors important in Appendix in more detail. We refer to this as our benchmark pool, and it consists of the following:\\n\\nImage Classification: We employ ImageNet1k [9], CIFAR100 [28], Places365 [74], and Food101 [36] to cover diverse natural image domains. Additionally, we include HappyWhale [17] for a more challenging domain shift, aiding in wildlife research and providing an interesting test case for model evaluation.\\n\\nFew Shot Image Classification: We use the MetaDataset task recipe on the Aircraft [37], Fungi [50], MiniImageNet [62], CUB200 [63], and Describable Features [69] datasets to evaluate task and domain shift robustness for an evaluation model.\\n\\nZero Shot Text-Image Classification: Another key setting is that of zero-shot text-image classification, on which many current key models were trained and evaluated [43]. We utilize Flickr30K, New Yorker Caption Context (a challenging humor task), and Winoground \u2013 a task requiring the model to match two texts with their corresponding images, focusing on compositional differences.\\n\\nVisual Relational Reasoning: A context where earlier models, such as ResNet50 [18] had low performance without layers with associative inductive biases (e.g., relational neural networks or transformers [49, 61]). This ensures we are aware of any trade-offs in relational compositional abilities in our models. We use CLEVR [23] and CLEVRMath [34].\\n\\nImage Semantic Segmentation: Essential for various real-world applications, serving as an indicator of a model\u2019s ability to retain spatial information and identify objects at a per-pixel level. ADE20K [75], COCO10K [33], COCO164K [33], NYU-Depth-v2 [54], PascalContext [38], and Cityscapes [8].\\n\\nMedical Image Classification: Medical data exhibit substantial domain and modality shifts, posing significant challenges for machine learning models while aligning with the imperative to deliver real-world benefits. Chexpert [21] (chest X-rays annotated for thoracic disease diagnosis), Diabetic Retinopathy Classification.\"}"}
{"id": "0SMhqvgHST", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Betic Retinopathy Classification (retinal images for early detection of diabetic retinopathy),\\n\\nHAM10000 (dermatoscopic images for differentiating skin lesions).\\n\\nMedical Segmentation \u2192 ACDC (Automated Cardiac Diagnosis Challenge): This dataset assesses models' generalization to the medical domain, particularly the transferability of representations for segmenting anatomical structures in cardiac MRI images. By focusing on the heart's intricate anatomy, ACDC tests the models' ability to adapt to clinically relevant shapes and patterns.\\n\\nVideo Classification: Video classification tasks test models on their temporal generalization abilities and require an understanding of not only individual frame content but also the transition and context between frames.\\n\\nHMDB51 (Human Motion Database), UCF-101 (University of Central Florida - 101 action categories), Kinetics400.\\n\\nVideo Regression: Where classification tasks gauge categorical distinctions, video regression tasks assess models' ability to make continuous numerical predictions from temporal data, serving as an indicator of a model's capability to process and quantify dynamic content.\\n\\niWildcam (International Wildlife Camera Trap Challenge): This dataset targets estimating animal species abundance from videos and is a direct test of modality and task shift, and showcases a model's potential impact on ecological monitoring and species conservation efforts.\\n\\n1. Modality shifting contexts: Contexts where the foundation model is asked to learn to do well at a task that requires understanding of a previously unseen modality. More specifically, assuming a foundation model has been trained on natural images, this would be transferring to medical imaging, video, audio and test contexts. This would shed light on the performance of a model's middle layers.\\n\\n2. Task shifting contexts: Contexts where a model is tasked with performing a previously unseen task, for example, transferring from classification to segmentation or relational reasoning.\\n\\n3. Domain shifting contexts: Contexts where a model is required to perform a task on a domain that is different from the one it was trained on. For example moving from natural images on ImageNet at 224x224 resolution to black and white Omniglot characters at 28x28 resolution, or, moving from ImageNet to images of fungi. More extreme domain shifts would be going from natural images to medical images for example.\\n\\n13 Model Pool Details\\n\\n14 Task Adapter Details\\n\\n15 Experimental Details\\n\\nExperimental Environment Details: GPUs: 4 x A6000 Ada @ 48GB, CPUs: 128 Core AMD EPYC 7713 64-Core Processor, RAM: 1 TB, HD: 15TB NVME. All experiments were done with BF16 precision.\\n\\n16 Additional Results\\n\\n16.1 Full details on discovered combinations\"}"}
{"id": "0SMhqvgHST", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Degradation of predictive power when a given benchmark is removed and the meta-model trained from scratch, for different best combinations in varying $k$. \\n\\n(a) Best $k=2$ discovered combination\\n\\n(b) Best $k=3$ discovered combination\\n\\n(c) Best $k=4$ discovered combination\\n\\n(d) Best $k=5$ discovered combination\\n\\n(e) Best $k=6$ discovered combination\\n\\n(f) Best $k=7$ discovered combination\\n\\n(g) Best $k=8$ discovered combination\\n\\n(h) Best $k=9$ discovered combination\\n\\n(i) Best $k=10$ discovered combination\\n\\n(j) Best $k=11$ discovered combination\\n\\n(k) Best $k=12$ discovered combination\\n\\n(l) Best $k=13$ discovered combination\"}"}
{"id": "0SMhqvgHST", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| CIFAR-100 Loss | 54.7 | 0.6 |\\n|---------------|------|-----|\\n| ImageNet-1K Acc@5 | 85.3 | 0.0 |\\n| HWhale Species Acc@1 | 0   |    |\\n| Fungi Acc@1 | 1   | 99  |\\n| HWhale Individual Acc@5 | 75  | 1.5 |\\n| ADE20K CE Loss | 0   |    |\\n| Img Seg | 0   | 99  |\\n| Few-Shot Img Class | 96.2 | 2.0 |\\n| CIFAR-100 Acc@1 | 96.3 | 0.1 |\\n| Img Class Metric | 97.4 | 0.7 |\\n| Task Mean | 60.8 |\\n| Pascal mIoU | 87.6 |\\n| Pascal Dice Loss | 78.0 |\\n| cvnxt | siglip | clip | flex | deit | laion | vit | dino | smvit | rnx50 | effv2 | r50a1 | effrmr | seffv2 | sflex | svit | whspr | sr50a1 | bert | bart | mpnet |\"}"}
{"id": "0SMhqvgHST", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset      | Chexpert Loss | NYCC Img2Txt Acc@5 | Flickr30K Txt2Img Loss | Chexpert | Diabetic | Winoground Img2Txt | HAM10K Loss | HAM10K BS Macro |\\n|--------------|---------------|-------------------|------------------------|----------|----------|-------------------|-------------|----------------|\\n|              | 0             |                   |                        | 15       | 0        |                   | 3           | 0              |\\n|              |               |                   |                        | 3        | 5        |                   | 3           | 2              |\\n|              |               |                   |                        | 1        | 4        |                   | 69.6        | 40             |\\n|              |               |                   |                        | 52.5     | 14       |                   | 96.7        | 100            |\\n|              |               |                   |                        | 96.5     | 0        |                   | 3.8         | 70             |\\n|              |               |                   |                        | 89       | 0        |                   | 89          | 75             |\\n|              |               |                   |                        | 99.3     | 0        |                   | 99.3        | 80             |\\n|              |               |                   |                        | 98       | 3        |                   | 98          | 75             |\\n|              |               |                   |                        | 96.1     | 3        |                   | 96.1        | 40             |\\n|              |               |                   |                        | 85       | 3        |                   | 85          | 30             |\\n|              |               |                   |                        | 99       | 4        |                   | 99          | 60             |\\n|              |               |                   |                        | 99       | 4        |                   | 99          | 60             |\"}"}
{"id": "0SMhqvgHST", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"|       | Small GATE Rank | Base GATE Rank | Big GATE Rank | Full GATE Rank |\\n|-------|-----------------|----------------|--------------|----------------|\\n| Task Mean | 68.3            | 74             | 76.6         | 69.0           |\\n| UCF-101 Loss | 0.6             | 95.4           | 0.2          | 1              |\\n| UCF-101 Acc@5 | 77.7            | 92             | 75           | 97.9           |\\n| UCF-101 Acc@1 | 48              | 48             | 51.4         | 48             |\\n| Kinetics Loss | 1              | 3              | 3            | 4              |\\n| Kinetics Acc@5 | 84.4            | 84.4           | 84.4         | 84.4           |\\n| Kinetics Acc@1 | 48              | 48             | 51.4         | 48             |\\n| IWildCam MSE Loss | 3.7            | 3.7            | 3.7          | 3.7            |\\n| IWildCam MAE Score | 1              | 1              | 1            | 1              |\\n\\nTable 3: Full experiments table: Black/Bold best model, Green second best, Blue third best, and red than pretrained. This table showcases the full set of data we use to evolve GATE using EEVEE. The worst performing model. Models prefixed with 's' refer to 'from scratch' trained models, rather than pretrained.\"}"}
{"id": "0SMhqvgHST", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset Name | Performance Loss |\\n|--------------|------------------|\\n| 0            | 0.02             |\\n| 0.01         | 0.02             |\\n| 0.02         | 0.02             |\\n| 0.03         | 0.02             |\\n| 0.04         |                  |\\n\\n(a) Best k=13 discovered combination\\n\\n| Dataset Name | Performance Loss |\\n|--------------|------------------|\\n| 0            | 0.01             |\\n| 0.02         | 0.02             |\\n| 0.03         | 0.02             |\\n| 0.04         |                  |\\n\\n(b) Best k=14 discovered combination\\n\\n| Dataset Name | Performance Loss |\\n|--------------|------------------|\\n| 0            | 0.02             |\\n| 0.01         | 0.02             |\\n| 0.02         | 0.02             |\\n| 0.03         |                  |\\n\\n(c) Best k=15 discovered combination\\n\\n| Dataset Name | Performance Loss |\\n|--------------|------------------|\\n| 0            | 0.02             |\\n| 0.01         | 0.02             |\\n| 0.02         | 0.02             |\\n| 0.03         |                  |\\n\\n(d) Best k=16 discovered combination\\n\\n| Dataset Name | Performance Loss |\\n|--------------|------------------|\\n| 0            | 0.01             |\\n| 0.02         | 0.02             |\\n| 0.03         | 0.02             |\\n| 0.04         |                  |\\n\\n(e) Best k=17 discovered combination\\n\\n| Dataset Name | Performance Loss |\\n|--------------|------------------|\\n| 0            | 0.01             |\\n| 0.02         | 0.02             |\\n| 0.03         | 0.02             |\\n| 0.04         |                  |\\n\\n(f) Best k=18 discovered combination\\n\\n| Dataset Name | Performance Loss |\\n|--------------|------------------|\\n| 0            | 0.01             |\\n| 0.02         | 0.02             |\\n| 0.03         | 0.02             |\\n| 0.04         |                  |\\n\\n(g) Best k=19 discovered combination\\n\\n| Dataset Name | Performance Loss |\\n|--------------|------------------|\\n| 0            | 0.01             |\\n| 0.02         | 0.02             |\\n| 0.03         | 0.02             |\\n| 0.04         |                  |\\n\\n(h) Best k=20 discovered combination\\n\\n| Dataset Name | Performance Loss |\\n|--------------|------------------|\\n| 0            | 0.01             |\\n| 0.02         | 0.02             |\\n| 0.03         | 0.02             |\\n| 0.04         |                  |\\n\\n(i) Best k=21 discovered combination\\n\\n| Dataset Name | Performance Loss |\\n|--------------|------------------|\\n| 0            | 0.01             |\\n| 0.02         | 0.02             |\\n| 0.03         | 0.02             |\\n| 0.04         |                  |\\n\\n(j) Best k=22 discovered combination\\n\\n| Dataset Name | Performance Loss |\\n|--------------|------------------|\\n| 0            | 0.01             |\\n| 0.02         | 0.02             |\\n| 0.03         | 0.02             |\\n| 0.04         |                  |\\n\\n(k) Best k=23 discovered combination\\n\\n| Dataset Name | Performance Loss |\\n|--------------|------------------|\\n| 0            | 0.01             |\\n| 0.02         | 0.02             |\\n| 0.03         | 0.02             |\\n| 0.04         |                  |\\n\\n(l) Best k=24 discovered combination\\n\\n| Dataset Name | Performance Loss |\\n|--------------|------------------|\\n| 0            |                  |\\n| 0.01         | 0.02             |\\n| 0.02         | 0.02             |\\n| 0.03         | 0.02             |\\n| 0.04         |                  |\\n\\nFigure 7: Degradation of predictive power when a given benchmark is removed and the meta-model trained from scratch, for different best combinations in varying $k$. \\n\\n24\\n\\n(Higher means more important)\"}"}
{"id": "0SMhqvgHST", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset Name | Performance Loss (Higher means more important) |\\n|--------------|-----------------------------------------------|\\n| places365    |                                               |\\n| omniglot     |                                               |\\n| food101      |                                               |\\n| acdc         |                                               |\\n| flickr30k    |                                               |\\n| ucf          |                                               |\\n| nycc         |                                               |\\n| imagenet1k   |                                               |\\n| cubirds      |                                               |\\n| dtextures    |                                               |\\n| mini         |                                               |\\n| iwildcam     |                                               |\\n\\n(a) Best k=24 discovered combination:\\n\\n| Dataset Name | Performance Loss (Higher means more important) |\\n|--------------|-----------------------------------------------|\\n| flickr30k    |                                               |\\n| dtextures    |                                               |\\n| pascal       |                                               |\\n| mini         |                                               |\\n| omniglot     |                                               |\\n| food101      |                                               |\\n| winoground   |                                               |\\n| nyy          |                                               |\\n| vgg          |                                               |\\n| acdc         |                                               |\\n| iwildcam     |                                               |\\n| cifar100     |                                               |\\n| chexpert     |                                               |\\n\\n(b) Best k=25 discovered combination:\\n\\n| Dataset Name | Performance Loss (Higher means more important) |\\n|--------------|-----------------------------------------------|\\n| kinetics     |                                               |\\n| ade20k       |                                               |\\n| clevr-math   |                                               |\\n| imagenet1k   |                                               |\\n| ucf          |                                               |\\n| hmdb51       |                                               |\\n| vgg          |                                               |\\n| omniglot     |                                               |\\n| acdc         |                                               |\\n| nycc         |                                               |\\n| pascal       |                                               |\\n| fungi        |                                               |\\n| winoground   |                                               |\\n\\n(c) Best k=26 discovered combination:\\n\\n| Dataset Name | Performance Loss (Higher means more important) |\\n|--------------|-----------------------------------------------|\\n| clevr-math   |                                               |\\n| happy        |                                               |\\n| diabetic     |                                               |\\n| winoground   |                                               |\\n| nyy          |                                               |\\n| hmdb51       |                                               |\\n| mini         |                                               |\\n| acdc         |                                               |\\n| coco-164k    |                                               |\\n| kinetics     |                                               |\\n| iwildcam     |                                               |\\n| ade20k       |                                               |\\n| vgg          |                                               |\\n| clevr        |                                               |\\n\\n(d) Best k=27 discovered combination:\\n\\n| Dataset Name | Performance Loss (Higher means more important) |\\n|--------------|-----------------------------------------------|\\n| clevr-math   |                                               |\\n| happy        |                                               |\\n| diabetic     |                                               |\\n| winoground   |                                               |\\n| nyy          |                                               |\\n| hmdb51       |                                               |\\n| mini         |                                               |\\n| acdc         |                                               |\\n| coco-164k    |                                               |\\n| kinetics     |                                               |\\n| iwildcam     |                                               |\\n| ade20k       |                                               |\\n| vgg          |                                               |\\n| clevr        |                                               |\\n\\nFigure 8: Degradation of predictive power when a given benchmark is removed and the meta-model trained from scratch, for different best combinations in varying k.\\n\\nFigure 9: Ranking Heatmap for bigGATE. We show how the various models on the y-axis rank on the metrics on the x-axis, where brighter is higher/better rank. From left to right we apply a spearman correlation sorting to capture tasks more similar to imagenet1k more towards the leftmost side, and, dissimilar ones towards the rightmost side. From top to bottom we rank models based on average rank.\"}"}
{"id": "0SMhqvgHST", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 10: Ranking Heatmap for baseGATE: We show how the various models on the y-axis rank on the metrics on the x-axis, where brighter is higher/better rank. From left to right we apply a spearman correlation sorting to capture tasks more similar to imagenet1k more towards the leftmost side, and, dissimilar ones towards the rightmost side. From top to bottom we rank models based on average rank.\"}"}
{"id": "0SMhqvgHST", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 11: Ranking Heatmap for smallGATE: We show how the various models on the y-axis rank on the metrics on the x-axis, where brighter is higher/better rank. From left to right we apply a spearman correlation sorting to capture tasks more similar to imagenet1k more towards the leftmost side, and, dissimilar ones towards the rightmost side. From top to bottom we rank models based on average rank.\"}"}
{"id": "0SMhqvgHST", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 12: Architecture Variation: Results of keeping the pretraining method the same as ImageNet1k classification and varying the architecture across various key task domains.\\n\\nFigure 13: Pretraining Scheme Variation: Results of varying the pretraining method and keeping the architecture as ViT B16 across various key task domains.\"}"}
{"id": "0SMhqvgHST", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 14: Modality Variation: Results of attempting modality shifting from audio and text to vision tasks.\\n\\nFigure 15: Modality Variation: Results of attempting modality shifting from audio and text to vision tasks.\"}"}
{"id": "0SMhqvgHST", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Claims\\n\\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\\n\\nAnswer: [Yes]\\n\\nJustification: All the claims made are substantiated with rigorous empirical results and communicated via tables and figures.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.\\n\\n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.\\n\\n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.\\n\\n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.\\n\\n2. Limitations\\n\\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\\n\\nAnswer: [Yes]\\n\\nJustification: We have an explicit limitations section.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\\n\\n\u2022 The authors are encouraged to create a separate \u201cLimitations\u201d section in their paper.\\n\\n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\\n\\n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\\n\\n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.\\n\\n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\\n\\n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.\\n\\n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.\\n\\n3. Theory Assumptions and Proofs\\n\\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\\n\\nAnswer: [NA]\"}"}
{"id": "0SMhqvgHST", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. Experimental Result Reproducibility\\n\\nQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\\nAnswer: Yes\\nJustification: We do so both in the main paper, and in more detail in the appendix, in addition to offering the codebase that reproduces all results.\\n\\nGuidelines:\\n\u2022 The answer NA means that the paper does not include experiments.\\n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.\\n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.\\n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.\\n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example:\\n  (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.\\n  (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.\\n  (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).\\n  (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.\\n\\n5. Open access to data and code\\n\\nQuestion: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?\"}"}
{"id": "0SMhqvgHST", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 The answer NA means that paper does not include experiments requiring code.\\n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\\n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \\\"No\\\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).\\n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\\n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.\\n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.\\n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).\\n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.\\n\\n6. Experimental Setting/Details\\n\\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?\\n\\nAnswer: [Yes]\\n\\nJustification: We describe these in the experiments section in summary, and in the appendix in detail.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not include experiments.\\n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.\\n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material.\\n\\n7. Experiment Statistical Significance\\n\\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?\\n\\nAnswer: [Yes]\\n\\nJustification: Where relevant our results include error bars.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not include experiments.\\n\u2022 The authors should answer \\\"Yes\\\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.\\n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).\\n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)\\n\u2022 The assumptions made should be given (e.g., Normally distributed errors).\"}"}
