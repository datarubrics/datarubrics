{"id": "UYgE9IfQIV", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"adding new workload traces to the data/Workload folder or specifying a path to existing traces in the sustaindc_env.py file under the workload_file configuration. Below is an example of modifying the workload configuration:\\n\\n```python\\nclass EnvConfig(dict):\\n    DEFAULT_CONFIG = {\\n        \\\"workload_file\\\": \\\"data/Workload/Alibaba_CPU_Data_Hourly_1.csv\\\",\\n        ...\\n    }\\n```\\n\\nThe workload file should contain one year of data with an hourly periodicity (365*24=8760 rows). The file structure should have two columns, where the first column does not have a name, and the second column should be named `cpu_load`.\\n\\nBelow is an example of the file structure:\\n\\n```\\n,cpu_load\\n1,0.380\\n2,0.434\\n3,0.402\\n4,0.485\\n...\\n```\\n\\nFigure 11 shows examples of different workload traces from Alibaba (v2017) and Google (v2011) data centers. Figure 12 provides a comparison between two workload traces of Alibaba (v2017) and Google (v2011).\\n\\nE.2 Weather\\n\\nThe Weather external variable in SustainDC captures the ambient environmental conditions impacting the data center's cooling requirements. By default, SustainDC includes weather data files in the .epw format from https://energyplus.net/weather for various locations where data centers are commonly situated. These locations include Arizona, California, Georgia, Illinois, New York, Texas, Virginia, and Washington. Users can customize this component by adding new weather files to the data/Weather folder or specifying a path to existing weather files in the sustaindc_env.py file under the weather_file configuration. Below is an example of modifying the weather configuration:\\n\\n```python\\nclass EnvConfig(dict):\\n    DEFAULT_CONFIG = {\\n        \\\"weather_file\\\": \\\"data/Weather/USA_NY_New.York-Kennedy.epw\\\",\\n        ...\\n    }\\n```\\n\\nEach .epw file contains hourly data for various weather parameters, but for our purposes, we focus on the ambient temperature. Figure 13 shows the typical average ambient temperature across different locations over one year. Figure 14 provides a comparison of external temperatures across the different selected locations.\\n\\nE.3 Carbon Intensity\\n\\nThe Carbon Intensity (CI) external variable in SustainDC represents the carbon emissions associated with electricity consumption. By default, SustainDC includes CI data files for various locations:\"}"}
{"id": "UYgE9IfQIV", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 12: Comparison between two workload traces of Alibaba trace (2017) and Google (2011).\\n\\nFigure 13: Typical average ambient temperature across different locations across one year.\"}"}
{"id": "UYgE9IfQIV", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 14: Comparison between external temperature of the different selected locations.\\n\\nArizona, California, Georgia, Illinois, New York, Texas, Virginia, and Washington. These files are located in the `data/CarbonIntensity` folder and are extracted from `https://api.eia.gov/bulk/EBA.zip`. Users can customize this component by adding new CI files to the `data/CarbonIntensity` folder or specifying a path to existing files in the `sustaindc_env.py` file under the `cintensity_file` configuration. Below is an example of modifying the CI configuration:\\n\\n```python\\nclass EnvConfig(dict):\\n    DEFAULT_CONFIG = {\\n        'cintensity_file': 'data/CarbonIntensity/NY_NG_&_avgCI.csv',\\n        ...\\n    }\\n```\\n\\nThe CI file should contain one year of data with an hourly periodicity (365*24=8760 rows). The file structure should have the following columns: `timestamp`, `WND`, `SUN`, `WAT`, `OIL`, `NG`, `COL`, `NUC`, and `OTH` and `avg_CI`. `WND`, `SUN`, `WAT`, `OIL`, `NG`, `COL`, `NUC`, and `OTH` represent the energy sources contributing to the carbon intensity. These sources include wind, solar, water, oil, natural gas, coal, nuclear, and other types of energy, respectively. Below is an example of the file structure:\\n\\n```\\ntimestamp,WND,SUN,WAT,OIL,NG,COL,NUC,OTH,avg_CI\\n2022-01-01 00:00:00+00:00,1251,0,3209,0,15117,2365,4992,337,367.450\\n2022-01-01 01:00:00+00:00,1270,0,3022,0,15035,2013,4993,311,363.434\\n2022-01-01 02:00:00+00:00,1315,0,2636,0,14304,2129,4990,312,367.225\\n2022-01-01 03:00:00+00:00,1349,0,2325,0,13840,2334,4986,320,373.228\\n...\\n```\\n\\nIn Figure 15, the average daily carbon intensity for each selected location is shown, highlighting the variations in carbon emissions associated with electricity consumption across different regions.\\n\\nIn Figure 16, a comparison of carbon intensity across all the selected locations is presented, providing a comprehensive overview of how carbon emissions vary between these areas.\\n\\nIn Figure 17, we show the average daily carbon intensity against the average daily coefficient of variation (CV) for various locations. This figure highlights an important perspective on the variability and magnitude of carbon intensity values across different regions. Locations with a high CV indicate greater fluctuation in carbon intensity, offering more \u201croom to play\u201d for DRL agents to effectively reduce carbon emissions through dynamic actions. Additionally, locations with a high average carbon intensity value present greater opportunities for achieving significant carbon emission reductions.\\n\\nThe selected locations are highlighted, while other U.S. locations are also plotted for comparison. Regions with both high CV and high average carbon intensity are identified as prime targets for DRL agents to maximize their impact on reducing carbon emissions.\\n\\nIn the table below (8) is the summarizing the selected locations, typical weather values, and carbon emissions characteristics:\"}"}
{"id": "UYgE9IfQIV", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"**Figure 15:** Typical average carbon intensity across different locations over one year.\\n\\n| Location | Typical Weather | Carbon Emissions |\\n|----------|-----------------|------------------|\\n| Arizona  | Hot, dry summers; mild winters | High avg CI, High variation |\\n| California | Mild, Mediterranean climate | Medium avg CI, Medium variation |\\n| Georgia  | Hot, humid summers; mild winters | High avg CI, Medium variation |\\n| Illinois | Cold winters; hot, humid summers | High avg CI, Medium variation |\\n| New York | Cold winters; hot, humid summers | Medium avg CI, Medium variation |\\n| Texas    | Hot summers; mild winters | Medium avg CI, High variation |\\n| Virginia | Mild climate, seasonal variations | Medium avg CI, Medium variation |\\n| Washington | Mild, temperate climate; wet winters | Low avg CI, Low variation |\"}"}
{"id": "UYgE9IfQIV", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 16: Comparison of carbon intensity across the different selected locations.\\n\\nFigure 17: Average daily carbon intensity versus average daily coefficient of variation (CV) for the grid energy provided from US. Selected locations are remarked. High CV indicates more fluctuation, providing more opportunities for DRL agents to reduce carbon emissions. High average carbon intensity values offer greater potential gains for DRL agents.\"}"}
{"id": "UYgE9IfQIV", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The states with the most significant number of data centers tend to be Virginia, Texas, California, and New York. Virginia, especially, is a major hub due to its proximity to Washington D.C. and the abundance of fiber optic cable networks. Texas and California are also prominent due to their size, economic output, and significant tech industries. New York, particularly around New York City, hosts numerous data centers that serve the financial sector and other industries.\\n\\nThe selection of these locations is justified by their significant number of data centers, which emphasizes the potential impact of DRL agents in these regions. By targeting areas with both high data center density and favorable carbon intensity characteristics, DRL agents can maximize their effectiveness in reducing carbon emissions.\\n\\nF Reward Evaluation and Customization\\n\\nF.1 Load Shifting Penalty ($LS_{\\\\text{Penalty}}$)\\n\\nThe Load Shifting Penalty ($LS_{\\\\text{Penalty}}$) is applied to the Load Shifting Agent ($\\\\text{Agent}_{LS}$) in the Workload Environment ($\\\\text{Env}_{LS}$) if it fails to reschedule flexible workloads within the same day. If $D_t$ (the amount of rescheduled workload left) is positive at the end of the day, $\\\\text{penalty}_{\\\\text{tasks}}$ is assigned. Additionally, we included a function that progressively increases the penalty as the hour of the day approaches 24h. This means the penalty increases linearly from hour 23h to hour 24h. Furthermore, there is a penalty for tasks that were dropped due to queue limits ($\\\\text{penalty}_{\\\\text{dropped}}_{\\\\text{tasks}}$). This penalty is added to discourage the agent from dropping tasks and ensure that workloads are managed efficiently.\\n\\nTherefore, the $LS_{\\\\text{Penalty}}$ is composed of $\\\\text{penalty}_{\\\\text{tasks}}_{\\\\text{queue}}$ and $\\\\text{penalty}_{\\\\text{dropped}}_{\\\\text{tasks}}$. Related work in this area include (27; 28; 29; 30; 31; 32; 33).\\n\\nF.2 Default Reward Function\\n\\nThe default reward function used in SustainDC for the Load Shifting Agent is implemented as follows:\\n\\n```python\\ndef default_ls_reward(params: dict) -> float:\\n    \\\"\\\"\\\"Calculate the reward value based on normalized load shifting and energy consumption.\\\"\\n    Parameters:\\n    params (dict): Dictionary containing parameters:\\n    - bat_total_energy_with_battery_KWh (float): Total energy consumption with battery.\\n    - norm_CI (float): Normalized carbon intensity.\\n    - bat_dcload_min (float): Minimum data center load.\\n```\\n\\n\"}"}
{"id": "UYgE9IfQIV", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Returns: \\n\\nfloat: Calculated reward value.\\n\\n```python\\n# Energy part of the reward\\ntotal_energy_with_battery = params['bat_total_energy_with_battery_KWh']\\nnorm_CI = params['norm_CI']\\ndcload_min = params['bat_dcload_min']\\ndcload_max = params['bat_dcload_max']\\n\\n# Calculate the reward associated with the energy consumption\\nnorm_net_dc_load = (total_energy_with_battery - dcload_min) / (dcload_max - dcload_min)\\nfootprint = -1.0 * norm_CI * norm_net_dc_load\\n\\n# Penalize the agent for each task that was dropped due to queue limit\\npenalty_per_dropped_task = -10 # Define the penalty value per dropped task\\ntasks_dropped = params['ls_tasks_dropped']\\npenalty_dropped_tasks = tasks_dropped * penalty_per_dropped_task\\n\\ntasks_in_queue = params['ls_tasks_in_queue']\\ncurrent_step = params['ls_current_hour']\\npenalty_tasks_queue = 0\\nif current_step % (24*4) >= (23*4): # Penalty for queued tasks at the end of the day\\n    factor_hour = (current_step % (24*4)) / 96 # min = 0.95833, max = 0.98953\\n    factor_hour = (factor_hour - 0.95833) / (0.98935 - 0.95833)\\n    penalty_tasks_queue = -1.0 * factor_hour * tasks_in_queue / 10 # Penalty for each task left in the queue\\n\\nLS_penalty = penalty_dropped_tasks + penalty_tasks_queue\\nreward = footprint + LS_penalty\\nreturn reward\\n```\\n\\nF.3 Customization of Reward Formulations\\n\\nUsers can choose to use any other reward formulation by defining custom reward functions inside `utils/reward_creator.py`. To create a custom reward function, you can define it as follows:\\n\\n```python\\ndef custom_reward(params: dict) -> float:\\n    # Custom reward calculation logic\\n    pass\\n```\\n\\nReplace the logic inside the `custom_reward` function with your custom reward logic.\\n\\nFor more examples of custom reward functions, users can check the file `utils/reward_creator.py`.\\n\\nTo use the custom reward function, you need to include it in the `utils/reward_creator.py` as follows:\\n\\n```python\\n# Other reward methods can be added here.\\nREWARD_METHOD_MAP = {\\n    'default_dc_reward' : default_dc_reward,\\n    'default_bat_reward': default_bat_reward,\\n    'default_ls_reward' : default_ls_reward,\\n    # Add custom reward methods here\\n    'custom_reward' : custom_reward,\\n}\\n```\\n\\nAdditionally, you need to specify the reward function in `harl/configs/envs_cfgs/dcrl.yaml`:\\n\\n```yaml\\nagents:\\n...\\nls_reward: default_ls_reward\\ndc_reward: default_dc_reward\\nbat_reward: default_bat_reward\\n...\\n```\"}"}
{"id": "UYgE9IfQIV", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This flexibility ensures that SustainDC can be adapted to a wide range of research and operational needs in sustainable data center management.\"}"}
{"id": "UYgE9IfQIV", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.\\n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.\\n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).\\n\u2022 If error bars are reported in tables or plots, the authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.\\n\\n8. Experiments Compute Resources\\nQuestion: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?\\nAnswer: [Yes]\\nJustification: This is provided in the Evaluation Metrics and Experimental Settings section\\n\\nGuidelines:\\n\u2022 The answer NA means that the paper does not include experiments.\\n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.\\n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.\\n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper).\\n\\n9. Code Of Ethics\\nQuestion: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\\nAnswer: [Yes]\\nJustification:\\nGuidelines:\\n\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\\n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.\\n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).\\n\\n10. Broader Impacts\\nQuestion: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?\\nAnswer: [Yes]\\nJustification: The paper is mainly focused on sustainable data center computing and as such aspects of this are discussed in the Introduction and Conclusion.\\n\\nGuidelines:\\n\u2022 The answer NA means that there is no societal impact of the work performed.\\n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.\\n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.\"}"}
{"id": "UYgE9IfQIV", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.\\n\\nThe authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.\\n\\nIf there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).\\n\\n11. Safeguards\\n\\nQuestion: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?\\n\\nAnswer: [NA]\\n\\nJustification: [NA]\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper poses no such risks.\\n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.\\n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.\\n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.\\n\\n12. Licenses for existing assets\\n\\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?\\n\\nAnswer: [Yes]\\n\\nJustification: We use open source datasets, certain repositories that are cited and our own models for developing the environment.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not use existing assets.\\n\u2022 The authors should cite the original paper that produced the code package or dataset.\\n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.\\n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.\\n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.\\n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.\\n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.\"}"}
{"id": "UYgE9IfQIV", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"If this information is not available online, the authors are encouraged to reach out to the asset's creators.\\n\\nNew Assets\\n\\nQuestion: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?\\n\\nAnswer: [Yes]\\n\\nJustification: We provide the documentation\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not release new assets.\\n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.\\n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.\\n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.\\n\\nCrowdsourcing and Research with Human Subjects\\n\\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?\\n\\nAnswer: [NA]\\n\\nJustification: [NA]\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\\n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.\\n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.\\n\\nInstitutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects\\n\\nQuestion: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?\\n\\nAnswer: [NA]\\n\\nJustification: [NA]\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\\n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.\\n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.\\n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.\"}"}
{"id": "UYgE9IfQIV", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"APPENDIX\\n\\nContents\\n\\n1 Introduction 1\\n\\n2 Related Work 2\\n\\n3 Data Center Operational Model 2\\n\\n4 SustainDC environment overview 3\\n\\n4.1 Workload Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n\\n4.2 Data Center Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n\\n4.3 Battery Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n\\n4.4 Heterogeneous Multi Agent Control Problem . . . . . . . . . . . . . . . . . . . . 5\\n\\n4.5 Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n\\n4.6 Extendable plug-n-play Data Center Simulation Platform . . . . . . . . . . . . . . 7\\n\\n5 Evaluation Metrics and Experimental Settings 7\\n\\n6 Benchmarking Algorithms on SustainDC 8\\n\\n6.1 Single vs multi-agent Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n\\n6.2 Reward Ablation on $\\\\alpha$ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n\\n6.3 Multiagent Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n\\n7 Limitations 10\\n\\n8 Next Steps 10\\n\\n9 Conclusion 10\\n\\nA Models A-2\\n\\nA.1 Workload Environment ($Env_{LS}$) . . . . . . . . . . . . . . . . . . . . . . . . A-2\\n\\nA.1.1 Actions ($A_{LS}$) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A-2\\n\\nA.1.2 Observations ($S_{LS}$) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A-3\\n\\nA.1.3 Mathematical Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A-3\\n\\nA.2 Data Center Environment ($Env_{DC}$) . . . . . . . . . . . . . . . . . . . . . . . A-4\\n\\nA.2.1 Data Center IT Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A-4\\n\\nA.2.2 HVAC Cooling Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A-5\\n\\nA.2.3 Actions ($A_{DC}$) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A-5\\n\\nA.2.4 Observations ($S_{DC}$) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A-6\\n\\nA.2.5 Chiller Sizing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A-6\\n\\nA.2.6 Water Consumption Model . . . . . . . . . . . . . . . . . . . . . . . . . . . A-6\\n\\nA.3 Battery Environment ($Env_{BAT}$) . . . . . . . . . . . . . . . . . . . . . . . . . A-7\"}"}
{"id": "UYgE9IfQIV", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.3.1 Battery Model\\n\\nA.3.2 Actions (\\\\(A_{\\\\text{Bat}}\\\\))\\n\\nA.3.3 Observations (\\\\(S_{\\\\text{Bat}}\\\\))\\n\\nA.3.4 Mathematical Model\\n\\nA.4 Interconnection of Environments and Agent Actions\\n\\nB Customization of \\\\(\\\\text{dc\\\\_config.json}\\\\)\\n\\nC Performance of RL agents on Evaluation Metrics\\n\\nD Agents/Env behavior\\n\\nD.1 Battery\\n\\nE External variables\\n\\nE.1 Workload\\n\\nE.2 Weather\\n\\nE.3 Carbon Intensity\\n\\nF Reward Evaluation and Customization\\n\\nF.1 Load Shifting Penalty (\\\\(\\\\text{LS\\\\_Penalty}\\\\))\\n\\nF.2 Default Reward Function\\n\\nF.3 Customization of Reward Formulations\\n\\ncode, licenses, and setup instructions for SustainDC are available at GitHub. The documentation can be accessed at.\\n\\nA Models\\n\\nA.1 Workload Environment (\\\\(\\\\text{Env}_{\\\\text{LS}}\\\\))\\n\\nThe Workload Environment (\\\\(\\\\text{Env}_{\\\\text{LS}}\\\\)) simulates the management and scheduling of data center (DC) workloads, allowing for dynamic adjustment of utilization to optimize energy consumption and carbon footprint. The environment is designed to evaluate the performance of reinforcement learning (RL) algorithms in rescheduling delay-capable workloads within the DC.\\n\\nLet \\\\(B_t\\\\) be the instantaneous DC workload trace at time \\\\(t\\\\), with \\\\(X\\\\%\\\\) of the load being rescheduled up to \\\\(N\\\\) simulation steps into the future. The goal of an RL agent (\\\\(\\\\text{Agent}_{\\\\text{LS}}\\\\)) is to observe the current time of day (\\\\(S_{\\\\text{C}}t\\\\)), the current and forecast grid CI data (\\\\(CI_t...t_{t+L}\\\\)), and the amount of rescheduled workload left (\\\\(D_t\\\\)). Based on these observations, the agent decides an action \\\\(A_{\\\\text{ls},t}\\\\) to reschedule the flexible component of \\\\(B_t\\\\) to create a modified workload \\\\(\\\\hat{B}_t\\\\), thus minimizing the net CFP = \\\\(\\\\sum_{n=0}^{N} CF_{t}\\\\) over \\\\(N\\\\) steps. Here \\\\(CF_{t}\\\\) will be calculated based on the sum of the DC IT load due to \\\\(\\\\hat{B}_t\\\\), the corresponding HV AC cooling load, and the charging and discharging of the battery at every time step.\\n\\nA.1.1 Actions (\\\\(A_{\\\\text{LS}}\\\\))\\n\\nThe action space for \\\\(\\\\text{Agent}_{\\\\text{LS}}\\\\) includes three discrete actions:\\n\\n4 GitHub repository: https://github.com/HewlettPackard/dc-rl.\\n5 Documentation: https://hewlettpackard.github.io/dc-rl.\"}"}
{"id": "UYgE9IfQIV", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 Action 0: Decrease Utilization\\n  - This action attempts to defer the flexible portion of the current workload \\\\( B_{\\\\text{nonflex}} \\\\) to a later time. The non-flexible \\\\( B_{\\\\text{flex}} \\\\) workload is processed immediately, while the flexible workload is added to a queue for future execution.\\n\\n\u2022 Action 1: Do Nothing\\n  - This action processes both the flexible \\\\( B_{\\\\text{flex}} \\\\) and non-flexible \\\\( B_{\\\\text{nonflex}} \\\\) portions of the current workload immediately, without any deferral.\\n\\n\u2022 Action 2: Increase Utilization\\n  - This action attempts to increase the current utilization by processing tasks from the queue, if available, in addition to the current workload.\\n\\nA.1.2 Observations \\\\( (S_{LS}) \\\\)\\n\\nThe state space observed by the RL agent consists of several features, including:\\n\\n\u2022 Time of Day\\n  - Represented using sine and cosine transformations of the hour of the day to capture cyclical patterns.\\n\\n\u2022 Day of the Year\\n  - Represented using sine and cosine transformations to capture seasonal variations.\\n\\n\u2022 Current Workload\\n  - The current workload level, which includes both flexible and non-flexible components.\\n\\n\u2022 Queue Status\\n  - The length of the task queue, normalized by the maximum queue length.\\n\\n\u2022 Grid Carbon Intensity (CI)\\n  - Current and forecasted CI values, capturing the environmental impact of electricity consumption.\\n\\n\u2022 Battery State of Charge (SoC)\\n  - The current state of charge of the battery, if available.\\n\\nThe observation space is a combination of these features, providing the agent with a comprehensive view of the current state of the environment.\\n\\nA.1.3 Mathematical Model\\n\\nWorkload Breakdown\\n\\nLet \\\\( B_t \\\\) be the total workload at time \\\\( t \\\\). This workload is divided into flexible \\\\( B_{\\\\text{flex},t} \\\\) and non-flexible \\\\( B_{\\\\text{nonflex},t} \\\\) components:\\n\\n\\\\[\\nB_t = B_{\\\\text{flex},t} + B_{\\\\text{nonflex},t}\\n\\\\]\\n\\nThe flexible workload \\\\( B_{\\\\text{flex},t} \\\\) is a fraction of the total workload:\\n\\n\\\\[\\nB_{\\\\text{flex},t} = \\\\alpha \\\\cdot B_t, \\\\quad 0 < \\\\alpha < 1\\n\\\\]\\n\\nwhere \\\\( \\\\alpha \\\\) is the flexible workload ratio.\\n\\nActions and Workload Management\\n\\nDepending on the action \\\\( A_{ls,t} \\\\) chosen by the RL agent, the workload is managed as follows:\\n\\n1. Action 0: Decrease Utilization (Queue Flexible Workload)\\n   \\\\[\\n   \\\\hat{B}_t = B_{\\\\text{nonflex},t}\\n   \\\\]\\n   The flexible workload \\\\( B_{\\\\text{flex},t} \\\\) is added to a task queue \\\\( Q_t \\\\) for future execution:\\n   \\\\[\\n   Q_{t+1} = Q_t + B_{\\\\text{flex},t}\\n   \\\\]\\n\\n2. Action 1: Do Nothing\\n   \\\\[\\n   \\\\hat{B}_t = B_t = B_{\\\\text{nonflex},t} + B_{\\\\text{flex},t}\\n   \\\\]\\n   There is no change in the task queue:\\n   \\\\[\\n   Q_{t+1} = Q_t\\n   \\\\]\\n\\n3. Action 2: Increase Utilization (Process Queue)\\n   \\\\[\\n   \\\\hat{B}_t = B_t + \\\\min(Q_t, C_{\\\\text{max}} - B_t)\\n   \\\\]\\n   where \\\\( C_{\\\\text{max}} \\\\) is the maximum processing capacity. The processed tasks are removed from the task queue:\\n   \\\\[\\n   Q_{t+1} = Q_t - \\\\min(Q_t, C_{\\\\text{max}} - B_t)\\n   \\\\]\"}"}
{"id": "UYgE9IfQIV", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Illustration of the modeled data center, showing the IT section (cabinets and servers) and the Cooling section (Cooling Tower, chiller, and CRAH). The airflow path is also depicted, with cool air supplied through the raised floor and hot air returning via the ceiling. Note: We use CRAH and CRAC interchangeably in the text, but they both represent the same device (CRAH).\\n\\nA.2 Data Center Environment (Env\\\\textsubscript{DC})\\n\\nThe Data Center Environment (Env\\\\textsubscript{DC}) simulates the IT and HVAC operations within a DC, enabling the evaluation of RL algorithms aimed at optimizing cooling setpoints to reduce energy consumption and carbon footprint.\\n\\nThe data center modeled is illustrated in Figure 6. The IT section includes the cabinets and servers, while the Cooling section comprises a Cooling Tower, a chiller, and the Computer Room Air Handler (CRAH). The setup also features a raised floor system that channels cool air from the CRAH to the cabinets. The hot air exits the cabinets and returns to the CRAH via the ceiling.\\n\\nA.2.1 Data Center IT Model\\n\\nLet \\\\( \\\\hat{\\\\mathbf{B}}_t \\\\) be the net DC workload at time instant \\\\( t \\\\) obtained from the Workload Manager. The spatial temperature difference, \\\\( \\\\Delta T_{\\\\text{supply}} \\\\), given the DC configuration, is obtained from Computational Fluid Dynamics (CFD). For a given rack, the inlet temperature \\\\( T_{\\\\text{inlet},i,t} \\\\) at CPU \\\\( i \\\\) is computed as:\\n\\n\\\\[\\nT_{\\\\text{inlet},i,t} = \\\\Delta T_{\\\\text{supply},i} + T_{\\\\text{CRAC supply},t}\\n\\\\]\\n\\nwhere \\\\( T_{\\\\text{CRAC supply},t} \\\\) is the CRAC unit supply air temperature. This value is chosen by the RL agent \\\\( A_{\\\\text{DC}} \\\\).\\n\\nNext, the CPU power curve \\\\( f_{\\\\text{cpu}}(\\\\text{inlet temp}, \\\\text{cpu load}) \\\\) and IT Fan power curve \\\\( f_{\\\\text{itfan}}(\\\\text{inlet temp}, \\\\text{cpu load}) \\\\) are implemented as linear equations based on (9). Given a server inlet temperature of \\\\( T_{\\\\text{inlet},i,t} \\\\) and a processing amount of \\\\( \\\\hat{\\\\mathbf{B}}_t \\\\) performed by CPU \\\\( i \\\\), the total rack power consumption for rack \\\\( k \\\\) across all CPUs from \\\\( i = 1 \\\\) to \\\\( K \\\\), and the total DC Power IT Consumption can be calculated as follows:\\n\\n\\\\[\\n\\\\begin{align*}\\nP_{\\\\text{CPU},t} &= \\\\sum_{i} f_{\\\\text{cpu}}(T_{\\\\text{inlet},i,t}, \\\\hat{\\\\mathbf{B}}_t) \\\\\\\\\\nP_{\\\\text{IT Fan},t} &= \\\\sum_{i} f_{\\\\text{itfan}}(T_{\\\\text{inlet},i,t}, \\\\hat{\\\\mathbf{B}}_t) \\\\\\\\\\nP_{\\\\text{rack},k,t} &= P_{\\\\text{CPU},t} + P_{\\\\text{IT Fan},t} \\\\\\\\\\nP_{\\\\text{datacenter},t} &= \\\\sum_{k} P_{\\\\text{rack},k,t}\\n\\\\end{align*}\\n\\\\]\"}"}
{"id": "UYgE9IfQIV", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.2.2 HV AC Cooling Model\\n\\nBased on the DC IT Load\\n\\nIn the datacenter, the IT fan airflow rate, $V_{sfan}$, air thermal capacity $C_{air}$, and air density $\\\\rho_{air}$, the rack outlet temperature $T_{outlet,i,t}$ is estimated from (9) using:\\n\\n$$T_{outlet,i,t} = T_{inlet,i,t} + \\\\frac{P_{rack,k,t}}{C_{air} \\\\cdot \\\\rho_{air} \\\\cdot V_{sfan}}$$\\n\\nIn conjunction with the return temperature gradient information $\\\\Delta T_{return}$ estimated from CFDs, the final CRAC return temperature is obtained as:\\n\\n$$T_{CRACreturn,t} = \\\\text{avg}(\\\\Delta T_{return,i} + T_{outlet,i,t})$$\\n\\nWe assume a fixed-speed CRAC Fan unit for circulating air through the IT Room. Hence, the total HV AC cooling load for a given CRAC setpoint $T_{CRACsupply,t}$, return temperature $T_{CRACreturn,t}$, and the mass flow rate $m_{crac,fan}$ is calculated as:\\n\\n$$P_{cool,t} = m_{crac,fan} \\\\cdot C_{air} \\\\cdot (T_{CRACreturn,t} - T_{CRACsupply,t})$$\\n\\nTo perform $P_{cool,t}$, the amount of cooling, the net chiller load for a chiller with Coefficient of Performance (COP) may be estimated as:\\n\\n$$P_{chiller,t} = P_{cool,t} \\\\left(1 + \\\\frac{1}{\\\\text{COP}}\\\\right)$$\\n\\nNext, this cooling load is passed on to the cooling tower. Assuming a cooling tower delta as a function of temperature $f_{\\\\_delta}(t_{db})$, (21), the required cooling tower air flow rate is calculated as:\\n\\n$$V_{ct,air,t} = P_{chiller,t} \\\\cdot C_{air} \\\\cdot \\\\rho_{air} \\\\cdot f_{\\\\_delta}(t_{db})$$\\n\\nFinally, the Cooling Tower Load at a flow rate of $V_{ct,air,t}$ is calculated with respect to a reference air flow rate $V_{ct,air,REF}$ and power consumption $P_{ct,REF}$ from the configuration object:\\n\\n$$P_{CT,t} = \\\\frac{P_{ct,REF}}{V_{ct,air,t} / V_{ct,air,REF}^3}$$\\n\\nThus, the total HV AC load includes the cooling tower and chiller loads:\\n\\n$$P_{HV AC,t} = P_{CT,t} + P_{chiller,t}$$\\n\\nBased on these power values, the IT and HV AC Cooling energy consumptions can be represented as:\\n\\n$$E_{hvac,t} = P_{HV AC,t} \\\\times \\\\text{step size (9)}$$\\n\\n$$E_{it,t} = P_{datacenter,t} \\\\times \\\\text{step size (10)}$$\\n\\nA.2.3 Actions ([A]DC)\\n\\nThe action space for Agent DC consists of discrete actions representing the adjustment of the CRAC unit's supply air temperature, limited to a range between 16\u00b0C to 23\u00b0C:\\n\\n- **Action 0**: Decrease Temperature - The agent decreases the CRAC supply air temperature, enhancing cooling performance but increasing energy consumption.\\n- **Action 1**: Maintain Temperature - The agent maintains the current CRAC supply air temperature.\\n- **Action 2**: Increase Temperature - The agent increases the CRAC supply air temperature, which can reduce cooling energy consumption but may increase the IT equipment temperature.\"}"}
{"id": "UYgE9IfQIV", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.2.4 Observations (SDC)\\n\\nThe state space observed by the RL agent consists of several features, including:\\n\\n- **Time of Day**: Represented using sine and cosine transformations of the hour of the day to capture cyclical patterns.\\n- **Day of the Year**: Represented using sine and cosine transformations to capture seasonal variations.\\n- **Ambient Weather**: Includes current temperature and other relevant weather conditions.\\n- **IT Room Temperature**: Average temperature in the IT room.\\n- **Energy Consumption**: Previous step cooling and IT energy consumptions.\\n- **Grid Carbon Intensity (CI)**: Current and forecasted CI values.\\n\\nThe observation space provides a comprehensive view of the current state of the environment to the agent.\\n\\nA.2.5 Chiller Sizing\\n\\nThe chiller power consumption is calculated based on the load and operating conditions using the following method:\\n\\n\\\\[\\nP_{\\\\text{chiller},t} = \\\\text{calculate_chiller_power}(\\\\text{max} \\\\_ \\\\text{cooling} \\\\_ \\\\text{cap}, \\\\text{load}, \\\\text{ambient} \\\\_ \\\\text{temp})\\n\\\\]\\n\\nCalculation of Average CRAC Return Temperature\\n\\n\\\\[\\nT_{\\\\text{CRACreturn},t} = \\\\text{avg}(\\\\Delta T_{\\\\text{return},i} + T_{\\\\text{outlet},i,t})\\n\\\\]\\n\\nCalculation of HVAC Power\\n\\n\\\\[\\nP_{\\\\text{cool},t} = m_{\\\\text{crac},\\\\text{fan}} \\\\cdot C_{\\\\text{air}} \\\\cdot (T_{\\\\text{CRACreturn},t} - T_{\\\\text{CRACsupply},t})\\n\\\\]\\n\\n\\\\[\\nP_{\\\\text{chiller},t} = P_{\\\\text{cool},t} \\\\frac{1 + 1}{COP}\\n\\\\]\\n\\n\\\\[\\nV_{\\\\text{ct,air},t} = P_{\\\\text{chiller},t} C_{\\\\text{air}} \\\\cdot \\\\rho_{\\\\text{air}} \\\\cdot f_{\\\\text{ct} \\\\_ \\\\delta}(t_{\\\\text{db}})\\n\\\\]\\n\\n\\\\[\\nP_{\\\\text{CT},t} = P_{\\\\text{ct,REF}} V_{\\\\text{ct,air},t} V_{\\\\text{ct,air,REF}}^3\\n\\\\]\\n\\n\\\\[\\nP_{\\\\text{HV AC},t} = P_{\\\\text{CT},t} + P_{\\\\text{chiller},t}\\n\\\\]\\n\\nA.2.6 Water Consumption Model\\n\\nThe water usage for the cooling tower is estimated using a model based on research findings from several key sources. The model accounts for the water loss due to evaporation, drift, and blowdown. The primary references used to develop this model include (22), (23), and guidelines from SPX Cooling Technologies (24).\\n\\nThe water usage model is formulated as follows:\\n\\n1. **Range Temperature Calculation**: The difference between the hot water temperature entering the cooling tower and the cold water temperature leaving the cooling tower:\\n\\n   \\\\[\\n   \\\\text{range_temp} = \\\\text{hot_water_temp} - \\\\text{cold_water_temp}\\n   \\\\]\\n\\n   where hot_water_temp is the \\\\(T_{\\\\text{CRACreturn},t}\\\\), and cold_water_temp is the current CRAC setpoint \\\\(T_{\\\\text{CRACsupply},t}\\\\).\\n\\n2. **Normalized Water Usage**: The baseline water usage per unit time, adjusted for the wet bulb temperature of the ambient air. This accounts for the environmental conditions affecting the cooling tower's efficiency:\\n\\n   \\\\[\\n   \\\\text{norm_water_usage} = 0.044 \\\\cdot \\\\text{wet_bulb_temp} + (0.35 \\\\cdot \\\\text{range_temp} + 0.1)\\n   \\\\]\"}"}
{"id": "UYgE9IfQIV", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. Total Water Usage: The normalized water usage is adjusted to ensure non-negativity and further adjusted for drift losses, which are a small percentage of the total water circulated in the cooling tower:\\n\\n\\\\[\\nwater_{usage} = \\\\max(0, \\\\text{norm}_{water}_{usage}) + \\\\text{norm}_{water}_{usage} \\\\cdot \\\\text{drift}_{rate}\\n\\\\]\\n\\n4. Water Usage Conversion: The total water usage is converted to liters per simulation timestep interval for ease of reporting and consistency with other metrics. Given that we use \\\\( N \\\\) timesteps per hour in our simulations, the conversion is as follows:\\n\\n\\\\[\\nwater_{usage}_{liters}_{per}_{timestep} = water_{usage} \\\\cdot \\\\frac{1000}{N}\\n\\\\]\\n\\nThis model incorporates both theoretical and empirical insights, providing a comprehensive estimation of the water consumption in a data center's cooling tower. By considering the specific operational parameters and environmental conditions, it ensures accurate and reliable water usage calculations, critical for sustainable data center management.\\n\\nA.3 Battery Environment (\\\\( \\\\text{Env}_{BAT} \\\\))\\n\\nThe Battery Environment (\\\\( \\\\text{Env}_{Bat} \\\\)) simulates the battery banks operations within the DC, enabling the evaluation of RL algorithms aimed at optimizing auxiliary battery usage to reduce energy costs and carbon footprint. This environment is a modified version of the battery model from (25).\\n\\nA.3.1 Battery Model\\n\\nThe battery model represents the energy storage system, considering its capacity, charging and discharging efficiency, and rate limits. The battery state of charge (SoC) evolves based on the actions taken by the RL agent.\\n\\nLet \\\\( E_{bat,t} \\\\) be the energy stored in the battery at time \\\\( t \\\\). The battery can perform three actions: charge, discharge, or remain idle. The maximum battery capacity is \\\\( C_{max} \\\\), and the current state of charge is \\\\( E_{bat,t} \\\\).\\n\\nA.3.2 Actions (\\\\( A_{Bat} \\\\))\\n\\nThe action space for \\\\( \\\\text{Agent}_{Bat} \\\\) includes three discrete actions:\\n\\n- Action 0: Charge - The battery is charged at a rate of \\\\( r_{charge} \\\\), consuming \\\\( E_{bat,t} \\\\) Wh of energy.\\n- Action 1: Idle - The battery do not consume energy.\\n- Action 2: Discharge - The battery discharges energy at a rate of \\\\( r_{discharge} \\\\), supplying \\\\( E_{bat,t} \\\\) Wh of energy.\\n\\nA.3.3 Observations (\\\\( S_{Bat} \\\\))\\n\\nThe state space observed by the RL agent consists of several features, including:\\n\\n- Data Center Load - The current power consumption of the data center.\\n- Battery SoC - The current state of charge of the battery.\\n- Grid Carbon Intensity (CI) - Current and forecasted CI values.\\n- Time of Day and Year - Represented using sine and cosine transformations to capture cyclical patterns.\\n\\nThe observation space is a combination of these features, providing the agent with a comprehensive view of the current state of the environment.\"}"}
{"id": "UYgE9IfQIV", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.3.4 Mathematical Model\\n\\nBattery Charging and Discharging\\n\\nThe energy stored in the battery evolves based on the action taken:\\n\\n\\\\[ E_{bat,t} = \\\\begin{cases} \\\\frac{r_{charge} \\\\cdot \\\\eta_{charge} \\\\cdot \\\\Delta t}{0} & \\\\text{if charging} \\\\\\\\ 0 & \\\\text{if idle} \\\\end{cases} \\\\]\\n\\nwhere \\\\( r_{charge} \\\\) and \\\\( r_{discharge} \\\\) are the rates of charging and discharging the battery, respectively. These rates determine the amount of energy added to or removed from the battery within a time step \\\\( \\\\Delta t \\\\).\\n\\nCharging Rate (\\\\( r_{charge} \\\\))\\n\\nThe charging rate \\\\( r_{charge} \\\\) is the rate at which energy is added to the battery during the charging process. It is defined as:\\n\\n\\\\[ r_{charge} = \\\\min \\\\left( C_{max} - E_{bat,t} \\\\cdot \\\\eta_{charge} \\\\cdot \\\\Delta t, P_{charge,max} \\\\right) \\\\]\\n\\nwhere \\\\( P_{charge,max} \\\\) is the maximum allowable charging power. This rate ensures that the battery does not exceed its maximum capacity \\\\( C_{max} \\\\) and that charging occurs efficiently.\\n\\nDischarging Rate (\\\\( r_{discharge} \\\\))\\n\\nThe discharging rate \\\\( r_{discharge} \\\\) is the rate at which energy is drawn from the battery during the discharging process. It is defined as:\\n\\n\\\\[ r_{discharge} = \\\\min \\\\left( E_{bat,t} \\\\cdot \\\\eta_{discharge} \\\\cdot \\\\Delta t, P_{discharge,max} \\\\right) \\\\]\\n\\nwhere \\\\( P_{discharge,max} \\\\) is the maximum allowable discharging power. This rate ensures that the battery does not discharge below zero and that discharging occurs efficiently.\\n\\nEnergy Constraints\\n\\nThe state of charge is bounded by the battery capacity:\\n\\n\\\\[ 0 \\\\leq E_{bat,t} \\\\leq C_{max} \\\\]\\n\\nBattery Power Constraints\\n\\nThe maximum power that the battery can charge or discharge is limited by:\\n\\n\\\\[ P_{charge,max} = u \\\\cdot P_{charge} + v \\\\]\\n\\\\[ P_{discharge,max} = u \\\\cdot P_{discharge} + v \\\\]\\n\\nSimple Reward Calculation\\n\\nThe goal of the three agents (Agent LS, Agent DC, and Agent BAT) is to minimize the cumulative carbon footprint (CFP) over a given horizon \\\\( N \\\\). The CFP at each time step \\\\( t \\\\) is computed as:\\n\\n\\\\[ CFP_t = (E_{it,t} + E_{hvac,t} + E_{bat,t}) \\\\cdot CI_t \\\\]\\n\\nwhere:\\n\\n- \\\\( E_{it,t} \\\\): Energy consumption by IT equipment due to \\\\( \\\\hat{B_t} \\\\)\\n- \\\\( E_{hvac,t} \\\\): Energy consumption by HVAC systems\\n- \\\\( E_{bat,t} \\\\): Energy contribution from the battery (positive when discharging, negative when charging)\\n- \\\\( CI_t \\\\): Grid carbon intensity at time \\\\( t \\\\)\\n\\nThe total reward is then:\\n\\n\\\\[ R = -\\\\sum_{t=0}^{N} CFP_t \\\\]\\n\\nThe reward could have other terms that may consider queue length, water usage, average task delay, etc.\"}"}
{"id": "UYgE9IfQIV", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.4 Interconnection of Environments and Agent Actions\\n\\nFigure 7 illustrates the interconnection of the different environments (EnvLS, EnvDC, and EnvBAT) and the actions of their respective RL agents. This diagram highlights how the decisions made by each agent impact the overall DC operations and contribute to the optimization of energy consumption and carbon footprint.\\n\\nIn the Workload Environment (EnvLS), the RL agent (AgentLS) reschedules flexible workloads to optimize utilization. This action will influence the IT load, which directly impacts the Data Center Environment (EnvDC). The RL agent (AgentDC) in the data center environment adjusts the CRAC setpoints to optimize cooling and IT operations, thus affecting the HVAC cooling load and overall energy consumption.\\n\\nThe Battery Environment (EnvBAT) is influenced by the energy demands of the data center environment. The RL agent (AgentBAT) manages the charging and discharging of the battery to optimize energy usage and reduce the carbon footprint. The interconnections between these environments ensure that the agents work together to minimize the cumulative CFP by considering the energy consumption of IT, HVAC, and battery systems.\\n\\nBy observing the current state and forecast data, each agent makes informed decisions that contribute to the overall sustainability and efficiency of the data center operations. This coordinated approach leverages the strengths of each environment to achieve significant reductions in energy consumption and carbon emissions.\\n\\nB Customization of dc_config.json\\n\\nThe customization of the DC is done through the dc_config.json file located in the utils folder. This file allows users to specify every aspect of the DC environment design. We show here a part of the configuration file to indicate the different configurable elements inside SustainDC. Additional elements can be added to this config either under an existing section or a new section, and utils/dc_config_reader.py will automatically import the new configurations. Inside the \\\"data_center_configuration\\\" SustainDC allows the user to configure the dimensions of the data center environment.\"}"}
{"id": "UYgE9IfQIV", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"center arrangement, the compiled CFD supply and approach temperature delta values and the maxi-\\nmum allowable CPUs per rack. There is an extensive set of parameters that can be configured under\\nthe \\\"hvac_configuration\\\" section including physical constants, parameters of the computer room\\nair-conditioning unit (CRAC), chiller, pumps and cooling towers. The \\\"server_characteristics\\\"\\nblock allows the user to specify the properties of individual servers in the data center, including their\\nidle power, full load fan frequency and power.\\n\\n```\\n{\\n\\\"data_center_configuration\\\": {\\n    \\\"NUM_ROWS\\\": 4,\\n    \\\"NUM_RACKS_PER_ROW\\\": 5,\\n    \\\"RACK_SUPPLY_APPROACH_TEMP_LIST\\\": [5.3, 5.3, 5.3, 5.3, 5.3, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.3, 5.3, 5.3, 5.3, 5.3],\\n    \\\"RACK_RETURN_APPROACH_TEMP_LIST\\\": [-3.7, -3.7, -3.7, -3.7, -3.7, -2.5, -2.5, -2.5, -2.5, -2.5, -2.5, -2.5, -2.5, -2.5, -2.5, -3.7, -3.7, -3.7, -3.7, -3.7],\\n    \\\"CPUS_PER_RACK\\\": 200\\n},\\n\\n\\\"hvac_configuration\\\": {\\n    \\\"C_AIR\\\": 1006,\\n    \\\"RHO_AIR\\\": 1.225,\\n    \\\"CRAC_SUPPLY_AIR_FLOW_RATE_pu\\\": 0.00005663,\\n    \\\"CRAC_REFRENCE_AIR_FLOW_RATE_pu\\\": 0.00009438,\\n    \\\"CRAC_FAN_REF_P\\\": 150,\\n    \\\"CHILLER_COP_BASE\\\": 5.0,\\n    \\\"CHILLER_COP_K\\\": 0.1,\\n    \\\"CHILLER_COP_T_NOMINAL\\\": 25.0,\\n    \\\"CT_FAN_REF_P\\\": 1000,\\n    \\\"CT_REFRENCE_AIR_FLOW_RATE\\\": 2.8315,\\n    \\\"CW_PRESSURE_DROP\\\": 300000,\\n    \\\"CW_WATER_FLOW_RATE\\\": 0.0011,\\n    \\\"CW_PUMP_EFFICIENCY\\\": 0.87,\\n    \\\"CT_PRESSURE_DROP\\\": 300000,\\n    \\\"CT_WATER_FLOW_RATE\\\": 0.0011,\\n    \\\"CT_PUMP_EFFICIENCY\\\": 0.87\\n},\\n\\n\\\"server_characteristics\\\": {\\n    \\\"CPU_POWER_RATIO_LB\\\": [0.01, 1.00],\\n    \\\"CPU_POWER_RATIO_UB\\\": [0.03, 1.02],\\n    \\\"IT_FAN_AIRFLOW_RATIO_LB\\\": [0.01, 0.225],\\n    \\\"IT_FAN_AIRFLOW_RATIO_UB\\\": [0.225, 1.0],\\n    \\\"IT_FAN_FULL_LOAD_V\\\": 0.051,\\n    \\\"ITFAN_REF_V_RATIO\\\": 1.0,\\n    \\\"ITFAN_REF_P\\\": 10.0,\\n    \\\"INLET_TEMP_RANGE\\\": [16, 28],\\n    \\\"DEFAULT_SERVER_POWER_CHARACTERISTICS\\\": \\n        [[170, 20], [120, 10], [130, 10], [130, 10], [130, 10], [130, 10], [130, 10], [130, 10], [130, 10], [130, 10], [130, 10], [130, 10], [130, 10], [130, 10], [170, 10], [130, 10], [130, 10], [110, 10], [170, 10], [170, 10], [170, 10], [170, 10], [170, 10], [170, 10]],\\n    \\\"HP_PROLIANT\\\": [110, 170]\\n}\"}"}
{"id": "UYgE9IfQIV", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C Performance of RL agents on Evaluation Metrics\\n\\nIn this section, we provide the numerical results we obtained from the main paper. The results are shown in Tables 2 (advantage of multiagent vs single agent), 3 (effects of reward sharing across agents), 4, 5, 6 and 7 (ablation across geographical locations with different weather, grid carbon intensity and server load pattern). We observed that there is not a single algorithm that works well across different metrics and geographical locations, and this is visually appreciated in the main paper.\\n\\n### Table 2: Performance with respect to evaluation metrics on single and multiple RL agent baselines.\\n\\n| Evaluation Metric | Algorithm | CFP (kgCO2) | HVAC Energy (kwh) | IT Energy (kwh) | Task Queue | Water Usage (litre) |\\n|-------------------|-----------|-------------|-------------------|-----------------|------------|-------------------|\\n| 1                 | A LS + B DC + B BAT | 167.61 | 391.6 | 1033.8 | 0.52 | 10433.46 |\\n| 2                 | B LS + A DC + B BAT | 153.56 | 372.9 | 944.5 | 0.0 | 10930.77 |\\n| 3                 | B LS + B DC + A BAT | 168.22 | 390.3 | 1029.8 | 0.0 | 10493.95 |\\n| 4                 | A LS + A DC + B BAT | 155.97 | 374.9 | 941.3 | 0.48 | 10883.73 |\\n| 5                 | A LS + B DC + A BAT | 168.64 | 391.1 | 1030.9 | 0.56 | 10470.43 |\\n| 6                 | B LS + A DC + A BAT | 155.44 | 374.8 | 942.5 | 0 | 10883.73 |\\n| 7                 | A LS + A DC + A BAT | 155.23 | 371.8 | 937.4 | 0.45 | 10826.61 |\\n\\n### Table 3: IPPO evaluated on SustainDC with different values of collaborative reward coefficient $\\\\alpha$ (Average result over 12 runs).\\n\\n| Evaluation Metric | Algorithm | CFP (kgCO2) | HVAC Energy (kwh) | IT Energy (kwh) | Task Queue | Water Usage (litre) |\\n|-------------------|-----------|-------------|-------------------|-----------------|------------|-------------------|\\n|                  | IPPO($\\\\alpha=1.0$) | 176.3 | 415.2 | 932.8 | 12.5 | 445.6 |\\n|                  | IPPO($\\\\alpha=0.8$) | 176.2 | 414.6 | 932.8 | 9.5 | 445.8 |\\n|                  | IPPO($\\\\alpha=0.1$) | 176.4 | 415.3 | 932.9 | 15.7 | 446.2 |\\n\\n### Table 4: Multiagent RL framework evaluated on SustainDC for a data center located in New York (Average result over 5 runs).\\n\\n| Evaluation Metric | Algorithm | CFP (kgCO2) | HVAC Energy (kwh) | IT Energy (kwh) | Task Queue | Water Usage (litre) |\\n|-------------------|-----------|-------------|-------------------|-----------------|------------|-------------------|\\n|                  | IPPO     | 179.6 | 417.1 | 945.9 | 20.9 | 446.2 |\\n|                  | MAPPO    | 176.4 | 417.0 | 932.7 | 19.6 | 446.2 |\\n|                  | HAPPO    | 177.3 | 414.8 | 930.9 | 12.8 | 441.9 |\\n|                  | HAA2C    | 177.5 | 419.0 | 934.8 | 25.2 | 14977.1 |\\n|                  | HAD3QN   | 178.4 | 420.5 | 940.4 | 28.0 | 14950.9 |\\n|                  | HASAC    | 181.7 | 424.2 | 960.8 | 79.7 | 14842.4 |\\n\\nD Agents/Env behavior\\n\\nD.1 Battery\\n\\nThe battery environment demonstrates how the battery's state of charge (SoC) and actions evolve over time under random behaviors. These figures illustrate two different examples generated using distinct random seeds.\\n\\nFigure 8 shows the battery's SoC and the actions taken (Charge, Discharge, Idle) over simulated days for two different random behaviors.\"}"}
{"id": "UYgE9IfQIV", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 5: Multiagent RL framework evaluated on SustainDC for a data center located in Georgia\\n\\n| Evaluation Metric | Algorithm  | CFP (kgCO2) | HVAC Energy (kwh) | IT Energy (kwh) | Task Queue Water Usage (litre) |\\n|-------------------|------------|-------------|------------------|----------------|-------------------------------|\\n|                   | IPPO       | 265.4       | 376.7            | 935.4          | 6.8                           |\\n|                   | MAPPO      | 263.4       | 370.3            | 935.9          | 0.35                          |\\n|                   | HAPPO      | 264.1       | 370.4            | 929.0          | 0.47                          |\\n|                   | HAA2C      | 262.7       | 367.1            | 928.3          | 6.6                           |\\n|                   | HAD3QN     | 262.8       | 370.7            | 935.1          | 0.0                           |\\n|                   | HASAC      | 263.0       | 367.4            | 932.4          | 0.0                           |\\n\\n### Table 6: Multiagent RL framework evaluated on SustainDC for a data center located in California\\n\\n| Evaluation Metric | Algorithm  | CFP (kgCO2) | HVAC Energy (kwh) | IT Energy (kwh) | Task Queue Water Usage (litre) |\\n|-------------------|------------|-------------|------------------|----------------|-------------------------------|\\n|                   | IPPO       | 170.0       | 384.3            | 933.8          | 12.9                          |\\n|                   | MAPPO      | 159.3       | 388.2            | 936.1          | 19.5                          |\\n|                   | HAPPO      | 159.1       | 376.3            | 935.8          | 74.9                          |\\n|                   | HAA2C      | 158.7       | 381.7            | 933.5          | 54.1                          |\\n|                   | HAD3QN     | 161.5       | 378.4            | 929.6          | 25.8                          |\\n|                   | HASAC      | 172.9       | 434.4            | 1027.0         | 43.8                          |\\n\\nFigure 9 compares the energy consumption with and without the battery over simulated days for two different random behaviors. This comparison illustrates the impact of battery usage on the overall energy consumption of the data center.\\n\\nFigure 10 shows the energy added to and removed from the battery over simulated days for two different random behaviors. These figures demonstrate how the battery charges and discharges energy, providing insights into its operational patterns.\\n\\n**E External variables**\\n\\nE.1 Workload\\n\\nThe Workload external variable in SustainDC represents the computational demand placed on the data center. Workload traces are provided in the form of FLOPs (floating-point operations) required by various jobs. By default, SustainDC includes a collection of open-source workload traces from Alibaba and Google data centers. Users can customize this component by...\\n\\n### Table 7: Multiagent RL framework evaluated on SustainDC for a data center located in Arizona\\n\\n| Evaluation Metric | Algorithm  | CFP (kgCO2) | HVAC Energy (kwh) | IT Energy (kwh) | Task Queue Water Usage (litre) |\\n|-------------------|------------|-------------|------------------|----------------|-------------------------------|\\n|                   | IPPO       | 408.7       | 380.8            | 934.8          | 0.60                          |\\n|                   | MAPPO      | 410.8       | 383.3            | 947.5          | 502.4                         |\\n|                   | HAPPO      | 405.5       | 381.9            | 936.6          | 0.26                          |\\n|                   | HAA2C      | 407.1       | 385.0            | 929.9          | 7.54                          |\\n|                   | HAD3QN     | 405.6       | 386.4            | 1094.0         | 0.0051                        |\\n|                   | HASAC      | 404.6       | 380.8            | 936.7          | 0.54                          |\"}"}
{"id": "UYgE9IfQIV", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8: Battery State of Charge (SoC) and actions taken over time under two different random behaviors. The actions are labeled as Charge, Discharge, and Idle.\\n\\n(b) Battery behavior example 2\\n\\nFigure 9: Energy consumption with and without the battery over time under two different random behaviors. The comparison illustrates the effect of battery usage on overall energy consumption.\\n\\n(b) Battery behavior example 2\\n\\nFigure 10: Energy added to and removed from the battery over time under two different random behaviors. The figures show how the battery charges and discharges energy throughout the simulated period.\"}"}
{"id": "UYgE9IfQIV", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SustainDC: Benchmarking for Sustainable Data Center Control\\n\\nAvisek Naug\u2020, Antonio Guillen\u2020, Ricardo Luna\u2020, Vineet Gundecha\u2020, Cullen Bash, Sahand Ghorbanpour, Sajad Mousavi, Ashwin Ramesh Babu, Dejan Markovikj, Lekhapriya D Kashyap, Desik Rengarajan, Soumyendu Sarkar\u2020\u2217\\n\\nHewlett Packard Enterprise (Hewlett Packard Labs)\\n{avisek.naug, antonio.guillen, rluna, vineet.gundecha, cullen.bash, sahand.ghorbanpour, sajad.mousavi, ashwin.ramesh-babu, dejan.markovikj, lekhapriya.dheeraj-kashyap, desik.rengarajan, soumyendu.sarkar}@hpe.com\\n\\nAbstract\\n\\nMachine learning has driven an exponential increase in computational demand, leading to massive data centers that consume significant energy and contribute to climate change. This makes sustainable data center control a priority. In this paper, we introduce SustainDC, a set of Python environments for benchmarking multi-agent reinforcement learning (MARL) algorithms for data centers (DC). SustainDC supports custom DC configurations and tasks such as workload scheduling, cooling optimization, and auxiliary battery management, with multiple agents managing these operations while accounting for the effects of each other. We evaluate various MARL algorithms on SustainDC, showing their performance across diverse DC designs, locations, weather conditions, grid carbon intensity, and workload requirements. Our results highlight significant opportunities to improve data center operations using MARL algorithms. Given the increasing use of DC due to AI, SustainDC provides a crucial platform for developing and benchmarking advanced algorithms essential for achieving sustainable computing and addressing other heterogeneous real-world challenges.\\n\\n1 Introduction\\n\\nOne of the growing areas of energy and carbon footprint (CFP) can be traced to cloud data centers (DCs). The increased use of cloud resources for batch workloads related to AI model training, multimodal data storage and processing, or interactive workloads like streaming services, hosting websites have prompted enterprise clients to construct numerous data centers. Governments and regulatory bodies are increasingly focusing on environmental sustainability and imposing stricter regulations to reduce carbon emissions. This has prompted industry-wide initiatives to adopt more intelligent DC control approaches. This paper presents SustainDC, a sustainable DC Multi-Agent Reinforcement Learning (MARL) set of environments. SustainDC helps promote and prioritize sustainability, and it serves as a platform that facilitates collaboration among AI researchers, enabling them to contribute to a more environmentally responsible DC.\\n\\nThe main contributions of this paper are the following:\\n\\n\u2022 A highly customizable suite of environments focused on Data Center (DC) operations, designed to benchmark energy consumption and carbon footprint across various DC configurations.\"}"}
{"id": "UYgE9IfQIV", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The framework supports the subclassing of models for different DC components ranging from workloads and individual server specifications to cooling systems, enabling users to test fine-grained design choices.\\n\\n- The environments are implemented using the Gymnasium `Env` class, facilitating the benchmarking of various control strategies to optimize energy use, reduce carbon footprint, and evaluate related performance metrics.\\n- Supports both homogeneous and heterogeneous multi-agent reinforcement learning (MARL) controllers and traditional non-ML controllers. Extensive studies within these environments demonstrate the advantages and limitations of various multi-agent approaches.\\n- SustainDC enables reward shaping, allowing users to conduct ablation studies on specific DC components to optimize performance in targeted areas.\\n- SustainDC serves as a comprehensive benchmark environment for heterogeneous, multi-agent, and multi-objective reinforcement learning algorithms, featuring diverse agent interactions, customizable reward structures, high-dimensional observations, and reproducibility.\\n\\nCode, licenses, and setup instructions for SustainDC are available at GitHub. The documentation can be accessed at.\\n\\n### Related Work\\n\\nRecent advancements in Reinforcement Learning (RL) have led to an increased focus on optimizing energy consumption in areas such as building and DC management. This has resulted in the development of several environments for RL applications.\\n\\n- **CityLearn** is an open-source platform that supports single and MARL strategies for energy coordination and demand response in urban environments.\\n- **Energym**, **RL-Testbed**, and **Sinergym** were developed as RL wrappers that facilitate communication between Python and EnergyPlus, enabling RL evaluation on the collection of buildings modeled in EnergyPlus.\\n- **SustainGym** is one of the latest suite of general purpose RL tasks for evaluation of sustainability, simulating electric vehicle charging scheduling and battery storage bid, which lends itself to benchmarking different control strategies for optimizing energy, carbon footprint, and related metrics in electricity markets.\\n\\nMost of the above-mentioned works use **EnergyPlus** or **Modelica**, which were primarily designed for modeling thermo-fluid interactions with traditional analytic control with little focus on Deep Learning applications. The APIs provided in these works only allow sampling actions in a model free manner, lacking a straightforward approach to customization or re-parameterization of system behavior. This is because most of the works have a set of pre-compiled binaries (e.g. FMUs in Modelica) or fine-tuned spline functions (in EnergyPlus) to simulate nominal behavior. Furthermore, there is a significant bottleneck in using these precompiled environments from Energyplus or Modelica for Python based RL applications due to latency associated with cross-platform interactions, versioning issues in traditional compilers for EnergyPlus and Modelica, unavailability of open source compilers and libraries for executing certain applications.\\n\\nSustainDC allows users to simulate the electrical and thermo-fluid behavior of large DCs directly in Python. Unlike other environments that rely on precompiled binaries or external tools, SustainDC is easily end-user customizable and fast. It enables the design, configuration, and control benchmarking of DCs with a focus on sustainability. This provides the ML community with a new benchmark environment specifically for Heterogeneous MARL in the context of DC operations, allowing for extensive goal-oriented customization of the MDP transition function, state space, actions space, and rewards.\\n\\n### 3 Data Center Operational Model\\n\\nFigure 1 illustrates the typical components of a DC operation as modeled in SustainDC. Workloads are uploaded to the DC from a proxy client. For non-interactive batch workloads, some of these jobs can be scheduled flexibly, allowing delays to different periods during the day for optimization.\"}"}
{"id": "UYgE9IfQIV", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Data Center\\nGrid Energy\\nEnergy Converter\\nBattery Banks\\nWorkload\\nGrid Carbon Intensity\\nWeather\\n\\nFigure 1: Operational Model of a SustainDC Data Center\\n\\nCooling Tower\\nserver\\nserver\\nserver\\nserver\\nserver\\nserver\\nserver\\nserver\\nserver\\nserver\\nChiller\\n\\nFigure 2: Model of the data center. The configuration allows customization of the number of cabinets per row, the number of rows, and the number of servers per cabinet. The cooling system, comprising the CRAH, chiller, and cooling tower, manages the heat generated by the IT system.\\n\\nAs the servers (IT systems) in the DC process these workloads, they generate heat that must be removed. A complex HV AC system with multiple components is used to cool the IT system. As shown in Figure 2, warm air rises from the servers via convection. Driven by the HV AC fan's forced draft, this warm air enters the Computer Room Air Handler (CRAH) (depicted by red arrows), where it is cooled to an optimal setpoint by a heat exchange process using a \\\"primary\\\" chilled water loop. The chilled air is then returned to the IT room through a plenum located beneath the DC (shown by blue arrows). The warmed water from this loop returns to the Chiller, where another heat exchange process transfers heat to a \\\"secondary\\\" chilled water loop, which carries the heat to a Cooling Tower.\\n\\nThe cooling tower fan, operating at variable speeds, rejects this heat to the external environment, with fan speed and energy consumption determined by factors such as the secondary loop's inlet temperature at the cooling tower, the desired outlet temperature setpoint, and external air temperature and humidity. Depending on the external Weather and processed Workload, the IT and cooling systems consume Grid Energy. Selecting the optimal cooling setpoint for the CRAH can reduce the DC's carbon footprint and also impacts the servers' energy efficiency.\\n\\nLarger DCs may include onsite Battery Banks that charge from the grid during low Grid Carbon Intensity periods and may optionally provide auxiliary energy during high Grid Carbon Intensity periods. This introduces a decision-making sustainability challenge to determine the optimal charging and discharging intervals for the batteries.\\n\\nThese three control problems are interrelated, motivating the development of testbeds and environments for evaluating multi-agent control approaches that collectively aim to minimize carbon footprint, energy and water usage, energy cost, and other sustainability metrics of interest.\\n\\n4 SustainDC environment overview\\n\\nA high-level overview of SustainDC is provided in Figure 3, outlining the three main environments developed in Python along with their individual components, customization options, and associated control challenges.\"}"}
{"id": "UYgE9IfQIV", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SustainDC\\n\\nComponents\\n\\nCustomization\\n\\nControl\\n\\nWorkload trace\\n\\n\u2022 Application specific workload traces\\n\u2022 Reward\\n\\nRL controlled flexible workload scheduling\\n\\nServers, IT Fans, Cabinets, Data Center Room Geometry, HVAC Cooling Units and Fans, Chillers, Pumps, Cooling Tower Fans, CFD Results\\n\\n\u2022 Location specific Weather and Grid Carbon Intensity (CI) patterns\\n\u2022 IT server and fan idle and full load characteristics, arrangement of IT Cabinets in the Room\\n\u2022 Cooling Unit, Chiller, Pump and Cooling Tower parameters\\n\u2022 Reward\\n\\nRL controlled IT Room cooling setpoint\\n\\nBanks of Batteries\\n\\n\u2022 Charging rate\\n\u2022 Discharging rate\\n\u2022 Battery capacity\\n\u2022 Reward\\n\\nRL controlled battery power supply\\n\\nFigure 3: SustainDC overview and RL loop\\n\\n(a) High-level overview of SustainDC, showing the three main environments (Workload Env, Data Center Cooling Env, and Battery Env) along with their customizable components and control actions.\\n\\n(b) RL loop in SustainDC, depicting how states and actions are formed from individual agents.\\n\\nFigure 3a further illustrates SustainDC, showing the Workload Environment, Data Center Environment, and Battery Environment along with their customizable parameters. Figure 3b depicts the RL loop in SustainDC, illustrating how agents' actions and states optimize DC operations, considering external variables like grid CI, workload, and weather.\\n\\n4.1 Workload Environment\\n\\nThe Workload Environment (Env LS) manages the execution and scheduling of delay tolerant workloads within the DC by streaming workload traces (measured in FLOPs) over a specified time period. SustainDC includes a set of open-source workload traces from Alibaba (10) and Google (11) data centers. Users can customize this component by adding new workload traces to the data/Workload folder or by specifying a path to existing traces in the dc_config.json file.\\n\\nSome workloads are flexible, meaning they can be rescheduled within an allowable time horizon. Tasks such as updates or backups do not need immediate execution and can be delayed based on urgency or Service-Level Agreements (SLA). This flexibility allows workloads to be shifted to periods of lower grid carbon intensity (CI), thereby reducing the DC's overall carbon footprint (CFP).\\n\\nUsers can also customize the CI data. By default, we provide a one-year CI dataset for the following states: Arizona, California, Georgia, Illinois, New York, Texas, Virginia, and Washington, locations selected due to their high data center density. The carbon intensity data files, sourced from eia.gov (https://api.eia.gov/bulk/EBA.zip), are located in the data/CarbonIntensity folder.\\n\\nLet $B_t$ be the instantaneous DC workload trace at time $t$, with $X\\\\%$ of the load reschedulable up to $N$ simulation steps into the future. The objective of an RL agent (Agent LS) is to observe the current time of day ($SC_t$), the current and forecast grid CI data ($CI_t...t+L$), and the remaining amount of\"}"}
{"id": "UYgE9IfQIV", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Based on these observations, the agent chooses an action $A_{ls,t}$ (as shown in Table 1) to reschedule the flexible portion of $Bt$, to minimize the net CFP over $N$ steps.\\n\\n### 4.2 Data Center Environment\\n\\nThe Data Center environment ($Env_{DC}$) provides a comprehensive set of configurable models and specifications. For IT-level design, SustainDC enables users to define IT Room dimensions, server cabinet arrangements (including the number of rows and cabinets per row), and both approach and return temperatures. Additionally, users can specify server and fan power characteristics, such as idle power, rated full load power, and rated full load frequency.\\n\\nOn the cooling side, SustainDC allows customization of the chiller reference power, cooling fan reference power, and the supply air setpoint temperature for IT Room cooling. It also includes specifications for the pump and cooling tower, such as rated full load power and rated full load frequency. All these parameters can be configured in the $dc_{-}config$.json file.\\n\\nOne of SustainDC's key features is its ability to automatically adjust HVAC cooling capacities based on workload demands and IT room configurations, a process known as \\\"sizing.\\\" This ensures that the data center remains adequately cooled without unnecessary energy expenditure. In contrast, previous environments often neglected this capability, resulting in inaccurate outcomes. For example, changing IT room configurations in other environments typically impacted only IT energy consumption without considering the overall cooling requirements, leading to inconsistent RL-based control results, as seen in RL-Testbed in (3). SustainDC addresses this by integrating custom supply and approach temperatures derived from Computational Fluid Dynamics (CFD) simulations, simplifying the complex calculations of temperature changes between the IT Room HVAC and the IT Cabinets (9).\\n\\nIn addition, SustainDC includes weather data (in $data/Weather$) in the .epw format for the same locations as the CI data. This data, sourced from https://energyplus.net/weather, represents typical weather conditions for these regions. Users can also specify their own weather files if needed.\\n\\nGiven $\\\\hat{B}t$ as the adjusted workload from the Workload Environment, the goal of the RL agent ($Agent_{DC}$) is to select an optimal cooling setpoint $A_{dc,t}$ (Table 1) to minimize the net carbon footprint CFP from combined cooling ($E_{hvac}$) and IT ($E_{it}$) energy consumption over an $N$-step horizon. In SustainDC, the agent's default state space includes the time of day and year ($SC_{t}$), ambient weather ($t_{db}$), IT Room temperature ($t_{room}$), previous step cooling ($E_{hvac}$) and IT ($E_{it}$) energy usage, and forecasted grid CI data ($CI_{t...t}+L$).\\n\\n### 4.3 Battery Environment\\n\\nThe Battery Environment ($Env_{BAT}$) is based on battery charging and discharging models, such as $f_{charging}(BatSoc, \\\\delta_{\\\\tau})$ from (12). Parameters for these components, including battery capacity, can be configured in the $dc_{-}config$.json file.\\n\\nThe objective of the RL agent ($Agent_{BAT}$) is to optimally manage the battery's state of charge ($BatSoc_{t}$). Using inputs such as the net energy consumption ($E_{hvac}+E_{it}$) from the Data Center environment, the time of day ($SC_{t}$), the current battery state of charge ($BatSoc_{t}$), and forecasted grid CI data ($CI_{t...t}+L$), the agent determines an action $A_{bat,t}$ (as outlined in Table 1). Actions include charging the battery from the grid, taking no action, or discharging to provide auxiliary energy to the data center, all aimed at minimizing the overall carbon footprint, energy consumption, etc.\\n\\n### 4.4 Heterogeneous Multi Agent Control Problem\\n\\nWhile SustainDC enables users to tackle the individual control problems for each of the three environments, the primary goal of this paper is to establish a multi-agent control benchmark that facilitates joint optimization of the CFP by considering the coordinated actions of all three agents ($Agent_{LS}$, $Agent_{DC}$, and $Agent_{BAT}$). The sequence of operations for the joint multi-agent and multi-environment functions can be represented as follows:\\n\\n5\"}"}
{"id": "UYgE9IfQIV", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Agent LS: \\\\((\\\\text{SCt} \\\\times \\\\text{CIt} \\\\times \\\\text{Dt} \\\\times \\\\text{Bt}) \\\\rightarrow A_{\\\\text{ls},t}(1)\\\\)\\n\\nAgent DC: \\\\((\\\\text{SCt} \\\\times \\\\text{t}_{\\\\text{db}} \\\\times \\\\text{t}_{\\\\text{room}} \\\\times \\\\text{Ehvac} \\\\times \\\\text{Eit} \\\\times \\\\text{CIt}) \\\\rightarrow A_{\\\\text{dc},t}(2)\\\\)\\n\\nAgent BAT: \\\\((\\\\text{SCt} \\\\times \\\\text{Bat}_\\\\text{SoC} \\\\times \\\\text{CIt}) \\\\rightarrow A_{\\\\text{bat},t}(3)\\\\)\\n\\nEnv LS: \\\\((\\\\text{Bt} \\\\times A_{\\\\text{ls},t}) \\\\rightarrow \\\\hat{\\\\text{Bt}}(4)\\\\)\\n\\nEnv DC: \\\\((\\\\hat{\\\\text{Bt}} \\\\times \\\\text{t}_{\\\\text{db}} \\\\times \\\\text{t}_{\\\\text{room}} \\\\times A_{\\\\text{dc},t}) \\\\rightarrow (\\\\text{Ehvac}, \\\\text{Eit})(5)\\\\)\\n\\nEnv BAT: \\\\((\\\\text{Bat}_\\\\text{SoC} \\\\times A_{\\\\text{bat},t}) \\\\rightarrow (\\\\text{Bat}_\\\\text{SoC}, \\\\text{E}_{\\\\text{bat}})(6)\\\\)\\n\\n\\\\[\\n\\\\text{CFP}_t = (\\\\text{Ehvac} + \\\\text{Eit} + \\\\text{E}_{\\\\text{bat}}) \\\\times \\\\text{CIt}(7)\\n\\\\]\\n\\nwhere \\\\(\\\\text{E}_{\\\\text{bat}}\\\\) represents the net discharge from the battery based on the change in battery state of charge (\\\\(\\\\text{Bat}_\\\\text{SoC}\\\\)), which can be positive or negative depending on the action \\\\(A_{\\\\text{bat},t}\\\\). If the battery provides auxiliary energy, \\\\(\\\\text{E}_{\\\\text{bat}}\\\\) is negative; if it charges from the grid, \\\\(\\\\text{E}_{\\\\text{bat}}\\\\) is positive.\\n\\nThe objective of the multi-agent problem is to determine \\\\(\\\\theta_{\\\\text{LS}}, \\\\theta_{\\\\text{DC}}, \\\\theta_{\\\\text{BAT}}\\\\), which parameterize the policies for Agent LS, Agent DC, and Agent BAT, respectively, such that the total CFP is minimized over a specified horizon \\\\(N\\\\). For this study, we set \\\\(N = 31 \\\\times 24 \\\\times 4\\\\), representing a 31-day horizon with a step duration of 15 minutes.\\n\\n4.5 Rewards\\n\\nWhile CFP reduction is the default objective in SustainDC, the reward formulation is highly customizable, allowing users to incorporate alternative objectives such as total energy consumption, operating costs across all DC components, and water usage.\\n\\nWe primarily consider the following default rewards for the three environments (Env LS, Env DC, Env BAT):\\n\\n\\\\[\\n(r_{\\\\text{LS}}, r_{\\\\text{DC}}, r_{\\\\text{BAT}}) = - (\\\\text{CFP}_t + \\\\text{LS Penalty}), - (\\\\text{Ehvac}_t + \\\\text{Eit}_t), - \\\\text{CFP}_t(8)\\n\\\\]\\n\\nHere, \\\\(\\\\text{LS Penalty}\\\\) is a penalty applied to the Load Shifting Agent (Agent LS) in the Workload Environment (Env LS) if it fails to reschedule flexible workloads within the designated time horizon \\\\(N\\\\). Specifically, if \\\\(\\\\text{Dt}\\\\) is positive at the end of a horizon \\\\(N\\\\), \\\\(\\\\text{LS Penalty}\\\\) is assigned. Details on calculating Agent Control Knob Actions Optimization Strategy\\n\\n**Figure**\\n\\n**Agent LS**\\n\\n- **Delay-tolerant workload scheduling**\\n  - 0: Store Delayable Tasks\\n  - 1: Compute All Immediate Tasks\\n  - 2: Maximize Throughput\\n\\n  Shift tasks to periods of lower CI/lower external temperature/other variables to reduce the CFP.\\n\\n**Table 1**: Overview of control choices in SustainDC: the tunable knobs, the respective action choices, optimization strategies, and visual representations.\"}"}
{"id": "UYgE9IfQIV", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Extendable and plug-and-play design of SustainDC for data center control to address the multi-agent holistic optimization of data centers for resolving multiple dependencies in real-time.\\n\\nLS Penalty are provided in the supplemental document. Users can opt for custom reward formulations by subclassing the base reward class in utils/reward_creator.py.\\n\\nBased on these individual rewards, we can formulate an independent or collaborative reward structure, where each agent receives partial feedback in the form of rewards from the other agent-environment pairs. The collaborative feedback reward formulation for each agent is formulated as:\\n\\n\\\\[\\nR_{LS} = \\\\alpha \\\\cdot r_{LS} + (1 - \\\\alpha) / 2 \\\\cdot r_{DC} + (1 - \\\\alpha) / 2 \\\\cdot r_{BAT}\\n\\\\]\\n\\n\\\\[\\nR_{DC} = (1 - \\\\alpha) / 2 \\\\cdot r_{LS} + \\\\alpha \\\\cdot r_{DC} + (1 - \\\\alpha) / 2 \\\\cdot r_{BAT}\\n\\\\]\\n\\n\\\\[\\nR_{BAT} = (1 - \\\\alpha) / 2 \\\\cdot r_{LS} + (1 - \\\\alpha) / 2 \\\\cdot r_{DC} + \\\\alpha \\\\cdot r_{BAT}\\n\\\\]\\n\\nHere, \\\\(\\\\alpha\\\\) is the weighting parameter. This reward-sharing mechanism enables agents to incorporate feedback from their actions across environments, making it suitable for independent critic multi-agent RL algorithms, such as IPPO (13). For instance, the adjusted CPU load \\\\(\\\\hat{B}_{t}\\\\) influences data center energy demand (\\\\(E_{\\\\text{cool}} + E_{\\\\text{it}}\\\\)), which subsequently affects the battery optimizer's charge-discharge decisions and ultimately impacts the net \\\\(\\\\text{CO}_2\\\\) footprint. Consequently, we explore a collaborative reward structure and conduct ablation experiments with varying \\\\(\\\\alpha\\\\) values to assess the effectiveness of reward sharing.\\n\\n4.6 Extendable plug-n-play Data Center Simulation Platform\\n\\nFigure 4 illustrates the extendable and plug-and-play design of SustainDC framework for data center control to address the multi-agent optimization of data centers for resolving multiple internal and external dependencies of agents in real-time. The three different controllers for Cooling Optimizer, Flexible Load Shifter and Battery Controller can be substituted with RL or non-RL controllers.\\n\\nSimilarly, the underlying models performing the simulation can be substituted easily using the Modules and Extendable Functions block. In the future, we plan to include the models for next generation of fanless direct liquid cooling for AI servers (14) for Energy HVAC Model Plug-in.\\n\\n5 Evaluation Metrics and Experimental Settings\\n\\nWe consider five metrics to evaluate various RL approaches on SustainDC. The \\\\(\\\\text{CO}_2\\\\) footprint (CFP) represents the cumulative carbon emissions associated with DC operations over the evaluation period. HVAC Energy refers to the energy consumed by cooling components, including the chiller, pumps, and cooling tower. IT Energy refers to the energy consumed by the servers within the DC.\\n\\nWater Usage, the volume of chilled water recirculated through the cooling system, is a critical metric in DCs where chilled water supply from a central plant is constrained, and efficient use of this resource helps minimize the DC's water footprint. Finally, Task Queue tracks the accumulation of compute FLOPs from workloads that are deferred for rescheduling under lower CI periods. Higher Task Queue values indicate poorer SLA performance within the DC.\\n\\nExperiments were conducted on an Intel\u00ae Xeon\u00ae Platinum 8470 server with 104 CPUs, utilizing 4 threads per training agent. All hyperparameter configurations for benchmark experiments are detailed in the supplemental document. The codebase and documentation are linked to the paper.\"}"}
{"id": "UYgE9IfQIV", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The purpose of SustainDC is to explore the benefits of jointly optimizing the Workload, Data Center, and Battery Environments to reduce the operating CFP of a DC. To investigate this, we can perform ablation studies in which we evaluate net operating CFP by running trained RL agents in only one or two of the SustainDC environments while employing baseline methods ($B^*$) in the other environments. For the Workload Environment ($Env_{LS}$), the baseline ($B_{LS}$) assumes no workload shifting over the horizon, which aligns with current standard practices in most data centers. For the Data Center Environment ($Env_{DC}$), we use the industry-standard ASHRAE Guideline 36 as the baseline ($B_{DC}$) (15). In the Battery Environment ($Env_{BAT}$), we adapt the method from (12) for real-time operation, reducing the original optimization horizon from 24 hours to 3 hours as our baseline ($B_{BAT}$). Future work will include further baseline comparisons using Model Predictive Control (MPC) and other non-ML control algorithms.\\n\\nNext, we perform ablations on the collaborative reward parameter $\\\\alpha$, followed by benchmarking various multi-agent RL approaches. This includes multi-agent PPO (16) with an independent critic for each actor (IPPO) (13) and a centralized critic with access to states and actions from other MDPs (MAPPO) (17). Given the heterogeneous nature of action and observation spaces in SustainDC, we also benchmark several heterogeneous multi-agent RL (HARL) methods (18), including HAPPO (Heterogeneous Agent PPO), HAA2C (Heterogeneous Agent Advantage Actor Critic), HAD3QN (Heterogeneous Agent Dueling Double Deep Q Network), and HASAC (Heterogeneous Agent Soft Actor Critic). MARL agents were trained on one location and evaluated across different locations.\\n\\nIn Figure 5, we compare the relative performance of different RL algorithms using a radar chart based on the evaluation metrics in Section 5. Since reporting absolute values may lack context, we instead plot relative performance differences, offering insights into the pros and cons of each approach. (Absolute values for these benchmark experiments are provided in the supplementary document in tabular format.) Metrics are normalized by their mean and standard deviation, with lower values positioned at the radar chart periphery and higher values toward the center. Hence, the larger the area for an approach on the radar chart, the better its performance across the evaluated metrics.\\n\\n6.1 Single vs multi-agent Benchmarks\\n\\nFigure 5a compares the relative performance of a single RL agent versus multi-agent RL benchmarks, highlighting the advantages of a MARL approach for sustainable DC operations. Among single RL agent approaches, the workload manager RL agent (Experiment 1) and the battery agent (Experiment 3) perform similarly in reducing water usage. The standalone DC (cooling) RL agent (Experiment 2) demonstrates strong performance in both energy and CFP reduction. Note that for Experiments 1 and 3, the Lowest Task Queue metric should be disregarded, as the baseline workload manager does not shift workloads and thus inherently has the lowest task queue.\\n\\nWhen we evaluate pairs of RL agents working simultaneously, the absence of a cooling optimization agent (e.g., Experiment 5) results in performance similar to single RL agent implementations (Experiments 1 and 3), where only $A_{LS}$ or $A_{BAT}$ are used with baseline agents. This indicates that the RL-based cooling optimizer significantly improves overall performance compared to the rule-based ASHRAE Guideline 36 controller (as seen in Experiments 2 and 4). Finally, when all three RL agents operate simultaneously without a shared critic (Experiment 7 using IPPO), they achieve better outcomes in energy consumption, water usage, and task queue management, with a CFP relatively similar to other experiments. The combined performance across all three agents highlights the benefits of a MARL approach for DC optimization.\\n\\n6.2 Reward Ablation on $\\\\alpha$\\n\\nFigure 5b shows the relative differences in performance when considering collaborative reward components. We considered 2 values of $\\\\alpha$ at the extremes to indicate no collaboration ($\\\\alpha = 1.0$) and relying only on the rewards of other agents ($\\\\alpha = 0.1$). An intermediate value of $\\\\alpha = 0.8$ was chosen based on similar work on reward-based collaborative approach in (19; 20). The improvement in setting $\\\\alpha = 0.8$ shows that considering rewards from other agents can improve performance w.r.t. no collaboration ($\\\\alpha = 1.0$) especially in a partially observable MDP.\"}"}
{"id": "UYgE9IfQIV", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. \\\\( A \\\\text{LS} + B \\\\text{DC} + \\\\ldots + B \\\\text{BAT} \\\\)\\n2. \\\\( B \\\\text{LS} + A \\\\text{DC} + B \\\\text{BAT} \\\\)\\n3. \\\\( B \\\\text{LS} + B \\\\text{DC} + A \\\\text{BAT} \\\\)\\n4. \\\\( A \\\\text{LS} + A \\\\text{DC} + B \\\\text{BAT} \\\\)\\n5. \\\\( A \\\\text{LS} + B \\\\text{DC} + A \\\\text{BAT} \\\\)\\n6. \\\\( B \\\\text{LS} + A \\\\text{DC} + A \\\\text{BAT} \\\\)\\n7. \\\\( A \\\\text{LS} + A \\\\text{DC} + A \\\\text{BAT} \\\\)\\n\\n**A*: PPO RL Agent,  \\n**B*: non-RL Baseline Agent\\n\\n(a) Single RL agent, two RL agents and three RL agents\\n\\nFor single agents, PPO was used (Average result over 5 runs)\\n\\n(b) IPPO with different values of collaborative reward coefficient \\\\( \\\\alpha \\\\) (Average result over 12 runs)\\n\\n(c) Multiagent RL frameworks for a data center located in New York (Average result over 5 runs)\\n\\n(d) Multiagent RL frameworks for a data center located in Georgia (Average result over 5 runs)\\n\\n(e) Multiagent RL frameworks for a data center located in California (Average result over 5 runs)\\n\\n(f) Multiagent RL frameworks for a data center located in Arizona (Average result over 5 runs)\\n\\nFigure 5: Benchmarking RL Algorithms on the Sustain DC environment\\n\\n9\"}"}
{"id": "UYgE9IfQIV", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6.3 Multiagent Benchmarks\\n\\nWe evaluated and compared the relative performances of various MARL approaches, including PPO with independent actor-critics (IPPO, $\\\\alpha = 0.8$), centralized critic PPO (MAPPO), heterogeneous multi-agent PPO (HAPPO), HAA2C, HAD3QN, and HASAC. Figures 5c, 5d, 5e, and 5f illustrate the relative performance of these methods for DCs located in New York, Georgia, California, and Arizona. Our results reveal a consistent trend where PPO-based shared actor-critic methods (MAPPO, HAPPO) outperform the independent agent counterpart, IPPO. On further analysis, we observed that while IPPO effectively reduces HVAC and IT energy, the battery agent struggles to optimally schedule charging and discharging from the grid to meet data center demand. Among MAPPO, HAPPO, and HAA2C, HAPPO consistently performs best (except in Georgia).\\n\\nFor the off-policy methods (HAD3QN and HASAC), performance varies significantly across regions, with HASAC achieving the highest performance in Arizona. The reasons for these regional performance variations are not fully understood and may be partially due to differences in weather and carbon intensity. We plan to further investigate these variations in future work.\\n\\n7 Limitations\\n\\nThe absence of an oracle that already knows the best results possible for the different environments makes it difficult to quantify the threshold for performance compared to simpler environments. For computational speed in RL, we used reduced order models for certain components like pumps and cooling towers. We could not exhaustably tune the hyperparameters for all the networks.\\n\\n8 Next Steps\\n\\nWe are planning to deploy the trained agents to real data centers and are working towards domain adaptation for deployment with safeguards. We will augment the codebase with these updates. In order to have a smooth integration with current systems where HVAC runs in isolation, we plan a phased deployment with recommendation to the data center operative followed by direct integration of the control agents with the HVAC system with safeguards. For real-world deployment, a trained model should be run on a production server using appropriate checkpoints within a containerized platform with necessary dependencies. Security measures must restrict the software to only read essential data, generate decision variables, and write them with limited access to secure memory for periodic reading by the data center's HVAC management system. To ensure robustness against communication loss, a backup mechanism for generating decision variables is essential.\\n\\n9 Conclusion\\n\\nThis paper introduced SustainDC, a fully Python-based benchmarking environment for multi-agent reinforcement learning (MARL) in sustainable, cost-effective, and energy-efficient data center operations. SustainDC provides comprehensive customization options for modeling multiple aspects of data centers, including a flexible RL reward design, an area we invite other researchers to explore further. We benchmarked an extensive collection of single-agent and multi-agent RL algorithms in SustainDC across multiple geographical locations, comparing their performance to guide researchers in sustainable data center management with reinforcement learning.\\n\\nAdditionally, we are collaborating with consortiums like ExaDigiT, which focuses on high-performance computing (HPC) and supercomputing, as well as with industry partners, to implement some of these approaches in real-world scenarios. SustainDC's complexity and constraints, rooted in realistic systems, make it a suitable platform for benchmarking hierarchical RL algorithms. We plan to implement continual reinforcement learning to accommodate dynamic data center environments and prevent out-of-distribution errors during equipment upgrades and accessory changes. Moreover, SustainDC features an extendable, plug-and-play architecture of data center modeling compatible with digital twin frameworks, supporting research into other aspects of data center optimization for joint and multi-objective goals.\"}"}
{"id": "UYgE9IfQIV", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgement\\n\\nWe would like to thank Paolo Faraboschi for sharing his expertise in machine learning and practical implementation approaches, and Torsten Wilde for his feedback on energy optimization and sustainability. Additionally, we extend our gratitude to Wes Brewer, Feiyi Wang, Vineet Kumar, Scott Greenwood, Matthias Maiterth, and Terry Jones of Oak Ridge National Laboratory for their feedback and leadership within the ExaDigiT consortium, which helped refine our solution.\\n\\nReferences\\n\\n[1] J. R. V\u00e1zquez-Canteli, J. K\u00e4mpf, G. Henze, and Z. Nagy, \u201cCitylearn v1.0: An openai gym environment for demand response with deep reinforcement learning,\u201d in Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, BuildSys \u201919, (New York, NY, USA), p. 356\u2013357, Association for Computing Machinery, 2019.\\n\\n[2] P. Scharnhorst, B. Schubnel, C. Fern\u00e1ndez Bandera, J. Salom, P. Taddeo, M. Boegli, T. Gorecki, Y. Stauffer, A. Peppas, and C. Politi, \u201cEnergym: A building model library for controller benchmarking,\u201d Applied Sciences, vol. 11, no. 8, 2021.\\n\\n[3] T. Moriyama, G. D. Magistris, M. Tatsubori, T. Pham, A. Munawar, and R. Tachibana, \u201cReinforcement learning testbed for power-consumption optimization,\u201d CoRR, vol. abs/1808.10427, 2018.\\n\\n[4] J. Jim\u00e9nez-Raboso, A. Campoy-Nieves, A. Manjavacas-Lucas, J. G\u00f3mez-Romero, and M. Molina-Solana, \u201cSinergym: A building simulation and control framework for training reinforcement learning agents,\u201d in Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, (New York, NY, USA), p. 319\u2013323, Association for Computing Machinery, 2021.\\n\\n[5] C. Yeh, V. Li, R. Datta, Y. Yue, and A. Wierman, \u201cSustaingym: A benchmark suite of reinforcement learning for sustainability applications,\u201d in Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. PMLR, p. 1, 2023.\\n\\n[6] D. B. Crawley, L. K. Lawrie, C. O. Pedersen, and F. C. Winkelmann, \u201cEnergy plus: energy simulation program,\u201d ASHRAE journal, vol. 42, no. 4, pp. 49\u201356, 2000.\\n\\n[7] M. Wetter, W. Zuo, T. S. Nouidui, and X. Pang, \u201cModelica buildings library,\u201d Journal of Building Performance Simulation, vol. 7, no. 4, pp. 253\u2013270, 2014.\\n\\n[8] W. Zuo, M. Wetter, J. VanGilder, X. Han, Y. Fu, C. Faulkner, J. Hu, W. Tian, and M.Condon, \u201cImproving Data Center Energy Efficiency Through End-to-End Cooling Modeling and Optimization. Final Report,\u201d Apr. 2021. [Online; accessed 14. Oct. 2024].\\n\\n[9] K. Sun, N. Luo, X. Luo, and T. Hong, \u201cPrototype energy models for data centers,\u201d Energy and Buildings, vol. 231, p. 110603, 2021.\\n\\n[10] Alibaba Group, \u201cAlibaba production cluster data.\u201d https://github.com/alibaba/clusterdata, 2017. Accessed: 2024-06-05.\\n\\n[11] Google, \u201cGoogle cluster workload traces.\u201d https://github.com/google/cluster-data, 2019. Accessed: 2024-06-05.\\n\\n[12] B. Acun, B. Lee, F. Kazhamiaka, K. Maeng, U. Gupta, M. Chakkaravarthy, D. Brooks, and C.-J. Wu, \u201cCarbon explorer: A holistic framework for designing carbon aware datacenters,\u201d in Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, ACM, Jan. 2023.\\n\\n[13] C. S. de Witt, T. Gupta, D. Makoviichuk, V. Makoviychuk, P. H. S. Torr, M. Sun, and S. Whiteson, \u201cIs independent learning all you need in the starcraft multi-agent challenge?,\u201d 2020.\"}"}
{"id": "UYgE9IfQIV", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"G. C. Team, \u201cHPE announces industry\u2019s first 100% fanless direct liquid cooling systems architecture,\u201d Hewlett Packard Enterprise, Oct. 2024.\\n\\nK. Zhang, D. Blum, H. Cheng, G. Paliaga, M. Wetter, and J. Granderson, \u201cEstimating ASHRAE Guideline 36 energy savings for multi-zone variable air volume systems using Spawn of EnergyPlus,\u201d Journal of Building Performance Simulation, vol. 15, no. 2, pp. 215\u2013236, 2022.\\n\\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, \u201cProximal policy optimization algorithms,\u201d arXiv preprint arXiv:1707.06347, 2017.\\n\\nC. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu, \u201cThe surprising effectiveness of PPO in cooperative multi-agent games,\u201d in Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.\\n\\nY. Zhong, J. G. Kuba, X. Feng, S. Hu, J. Ji, and Y. Yang, \u201cHeterogeneous-agent reinforcement learning,\u201d Journal of Machine Learning Research, vol. 25, no. 32, pp. 1\u201367, 2024.\\n\\nS. Sarkar, V. Gundecha, S. Ghorbanpour, A. Shmakov, A. R. Babu, A. Naug, A. Pichard, and M. Cocho, \u201cFunction approximation for reinforcement learning controller for energy from spread waves,\u201d in IJCAI \u201923: Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pp. 6201\u20136209, Unknown publishers, Aug. 2023.\\n\\nS. Sarkar, V. Gundecha, A. Shmakov, S. Ghorbanpour, A. R. Babu, P. Faraboschi, M. Cocho, A. Pichard, and J. Fievez, \u201cMulti-agent reinforcement learning controller to maximize energy efficiency for multi-generator industrial wave energy converter,\u201d Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, pp. 12135\u201312144, Jun. 2022.\\n\\nT. J. Breen, E. J. Walsh, J. Punch, A. J. Shah, and C. E. Bash, \u201cFrom chip to cooling tower data center modeling: Part I Influence of server inlet temperature and temperature rise across cabinet,\u201d in 2010 12th IEEE Intersociety Conference on Thermal and Thermomechanical Phenomena in Electronic Systems, pp. 1\u201310, IEEE, 2010.\\n\\nR. Sharma, A. Shah, C. Bash, T. Christian, and C. Patel, \u201cWater efficiency management in datacenters: Metrics and methodology,\u201d in 2009 IEEE International Symposium on Sustainable Systems and Technology, pp. 1\u20136, 2009.\\n\\nM. Shublaq and A. K. Sleiti, \u201cExperimental analysis of water evaporation losses in cooling towers using filters,\u201d Energy and Buildings, vol. 231, p. 110603, 2020.\\n\\nSPX Cooling Technologies, \u201cWater usage calculator,\u201d 2023. Accessed: 2024-06-11.\\n\\nB. Acun, B. Lee, F. Kazhamiaka, K. Maeng, M. Chakkaravarthy, U. Gupta, D. Brooks, and C.-J. Wu, \u201cCarbon explorer: A holistic approach for designing carbon aware datacenters,\u201d Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, 2023.\\n\\nData Center Map, \u201cData center map: Directory of data centers.\u201d https://www.datacentermap.com/usa/. Accessed: 2024-06-10.\\n\\nS. Sarkar, A. Naug, R. Luna, A. Guillen, V. Gundecha, S. Ghorbanpour, S. Mousavi, D. Markovikj, and A. Ramesh Babu, \u201cCarbon footprint reduction for sustainable data centers in real-time,\u201d Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, pp. 22322\u201322330, Mar. 2024.\\n\\nS. Sarkar, A. Naug, A. Guillen, R. Luna, V. Gundecha, A. Ramesh Babu, and S. Mousavi, \u201cSustainability of data center digital twins with reinforcement learning,\u201d Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, pp. 23832\u201323834, Mar. 2024.\\n\\nS. Sarkar, A. Naug, R. Luna Gutierrez, A. Guillen, V. Gundecha, A. Ramesh Babu, and C. Bash, \u201cReal-time carbon footprint minimization in sustainable data centers with reinforcement learning,\u201d in NeurIPS 2023 Workshop on Tackling Climate Change with Machine Learning, 2023.\"}"}
{"id": "UYgE9IfQIV", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"S. Sarkar, A. Naug, A. Guillen, R. Luna Gutierrez, V. Gundecha, S. Ghorbanpour, S. Mousavi, and A. Ramesh Babu, \u201cSustainable data center modeling: A multi-agent reinforcement learning benchmark,\u201d in NeurIPS 2023 Workshop on Tackling Climate Change with Machine Learning, 2023.\\n\\nA. Naug, A. Guillen, R. Luna Guti\u00e9rrez, V. Gundecha, S. Ghorbanpour, L. Dheeraj Kashyap, D. Markovikj, L. Krause, S. Mousavi, A. R. Babu, and S. Sarkar, \u201cPydcm: Custom data center models with reinforcement learning for sustainability,\u201d in Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation, BuildSys \u201923, (New York, NY, USA), p. 232\u2013235, Association for Computing Machinery, 2023.\\n\\nA. Naug, A. Guillen, R. Luna Gutierrez, V. Gundecha, S. Ghorbanpour, S. Mousavi, A. Ramesh Babu, and S. Sarkar, \u201cA configurable pythonic data center model for sustainable cooling and ml integration,\u201d in NeurIPS 2023 Workshop on Tackling Climate Change with Machine Learning, 2023.\\n\\nJ. Athavale, C. Bash, W. Brewer, M. Maiterth, D. Milojicic, H. Petty, and S. Sarkar, \u201cDigital twins for data centers,\u201d Computer, vol. 57, no. 10, pp. 151\u2013158, 2024.\"}"}
{"id": "UYgE9IfQIV", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. **Claims**\\n\\n   **Question:** Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\\n\\n   **Answer:** [Yes]\\n\\n   **Justification:** The claims in the introduction are shown in mainly across the SustainDC Overview and Benchmarking sections.\\n\\n   **Guidelines:**\\n   - The answer NA means that the abstract and introduction do not include the claims made in the paper.\\n   - The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.\\n   - The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.\\n   - It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.\\n\\n2. **Limitations**\\n\\n   **Question:** Does the paper discuss the limitations of the work performed by the authors?\\n\\n   **Answer:** [Yes]\\n\\n   **Justification:** Please see section Limitations.\\n\\n   **Guidelines:**\\n   - The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\\n   - The authors are encouraged to create a separate \u201cLimitations\u201d section in their paper.\\n   - The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\\n   - The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\\n   - The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.\\n   - The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\\n   - If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.\\n   - While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.\\n\\n3. **Theory Assumptions and Proofs**\\n\\n   **Question:** For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\\n\\n   **Answer:** [NA]\"}"}
{"id": "UYgE9IfQIV", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. Experimental Result Reproducibility\\n\\nQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\\n\\nAnswer: [Yes]\\n\\nJustification: Since this is an paper with an extensive set of benchmarking experiments, we provide the experimental details for reproducibility in the supplemental document.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not include experiments.\\n\\n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.\\n\\n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.\\n\\n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.\\n\\n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example\\n  (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.\\n  (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.\\n  (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).\\n  (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.\\n\\n5. Open access to data and code\\n\\nQuestion: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?\"}"}
{"id": "UYgE9IfQIV", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. Experimental Setting/Details\\n\\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?\\n\\nAnswer: [Yes]\\n\\nJustification: We provide the details to the experimental settings in the supplemental as well as the linked codebase.\\n\\nGuidelines:\\n\u2022 The answer NA means that the paper does not include experiments.\\n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.\\n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material.\\n\\n7. Experiment Statistical Significance\\n\\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?\\n\\nAnswer: [NA]\\n\\nJustification: In this paper we do not show any results that are worth statistical significance.\\n\\nGuidelines:\\n\u2022 The answer NA means that the paper does not include experiments.\\n\u2022 The authors should answer \u201cYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.\\n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).\\n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)\\n\u2022 The assumptions made should be given (e.g., Normally distributed errors).\"}"}
