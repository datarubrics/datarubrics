{"id": "4S8agvKjle", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents\\n\\nChang Ma\\nJunlei Zhang\\nZhihao Zhu\\nCheng Yang\\nYujiu Yang\\nYaohui Jin\\nZhenzhong Lan\\nLingpeng Kong\\nJunxian He\\n\\nThe University of Hong Kong\\nZhejiang University\\nShanghai Jiao Tong University\\nTsinghua University\\nWestlake University\\nHKUST\\nllmagentboard@gmail.com\\n\\nAbstract\\nEvaluating Large Language Models (LLMs) as general-purpose agents is essential for understanding their capabilities and facilitating their integration into practical applications. However, the evaluation process presents substantial challenges. A primary obstacle is the benchmarking of agent performance across diverse scenarios within a unified framework, especially in maintaining partially-observable environments and ensuring multi-round interactions. Moreover, current evaluation frameworks mostly focus on the final success rate, revealing few insights during the process and failing to provide a deep understanding of the model abilities. To address these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark and accompanied open-source evaluation framework tailored to analytical evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis. This not only sheds light on the capabilities and limitations of LLM agents but also propels the interpretability of their performance to the forefront. Ultimately, AgentBoard serves as a step towards demystifying agent behaviors and accelerating the development of stronger LLM agents.\\n\\n1 Introduction\\nGeneral-purpose agents that can autonomously perceive and act in various environments are considered significant milestones in Artificial Intelligence (Russell and Norvig, 2005). Recent advancements in large language models (OpenAI, 2023; Touvron et al., 2023) have demonstrated emergent agent abilities that enable them to understand diverse environments and perform step-by-step planning through multi-round interactions (Yao et al., 2023; Song et al., 2023). These advanced abilities contribute to the potential of LLMs to act as generalist agents for real-world problem-solving. A comprehensive evaluation of LLM agents is crucial for the progression of this emerging field. To start, task diversity is necessary to cover various agent tasks such as embodied, web, and tool agents. Additionally, multi-round interaction is critical to mimic realistic scenarios, in contrast to the single-round tasks commonly adopted in existing benchmarks (Xu et al., 2023b; Lin and Chen, 2023; Qin et al., 2023a). Furthermore, evaluating agents in partially-observable environments, where they must actively explore to understand their surroundings, is essential for practical assessments. This differs from the \u201csynthetic\u201d agent tasks (Wang et al., 2023b) derived from conventional benchmarks in fully-observable environments, such as MMLU (Lanham et al., 2023) and GSM8K (Cobbe et al., 2021). However, existing agent benchmarks fail to satisfy all of these criteria.\\n\\nEqual Contribution.\\n\\n1 Code and data are available at https://github.com/hkust-nlp/AgentBoard\\n\\n38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.\"}"}
{"id": "4S8agvKjle", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Analysis\\n\\nGoal: Find the exit of the maze\u2026\\n\\nAgent\\nMove forward!\\n\\nEnvironment\\nOops! There is no road in front of you. Please choose another action.\\n\\nProgress Rate\\n0.25\\n\\nInteraction\\nTask\\nAgentBoard\\nPartially- Observable Environment\\nProgress\\n1\\nMulti-round Interaction\\nTask\\nAnalysis\\n\\n1\\n\\nWeb\\nWebShop\\nWebArena\\n\\nTool\\nQuery\\nOperation\\nGame\\nJericho\\nPDDL\\nEmbodied AI\\nAlfWorld\\nScienceWorld\\nBabyAI\\n\\n0\\n20\\n40\\n60\\n80\\n\\n0\\n5\\n10\\n15\\n20\\n\\nGPT-4\\nCurrent Run\\n\\n0\\n10\\n20\\n30\\n40\\n50\\n\\nAll\\nEasy\\nHard\\n\\nSuccess Rate vs Progress Rate\\n\\nFigure 1: The illustrative overview of AgentBoard. AgentBoard consists of 9 diverse tasks. Agents interact in multi-rounds with partially-observable environments to achieve each subgoal. Furthermore, AgentBoard provides an open-source analytical evaluation framework, as shown in the figure.\\n\\nMoreover, the inherent complexity in agent tasks characterized by multi-round interactions distinguishes them significantly from other language tasks. Due to this complexity, there is a pressing need to delve into the details and gain a deeper understanding of how models function during the process. Nonetheless, most current evaluations predominantly rely on the final success rate as their metric, which provides limited insights into these intricate processes (Liu et al., 2023a; Wang et al., 2023b; Yao et al., 2023; Liu et al., 2023b; Mialon et al., 2023). This simplified evaluation is particularly inadequate in challenging environments where most models demonstrate nearly zero success rates, consequently blurring finer distinctions and obscuring underlying mechanisms (Liu et al., 2023a).\\n\\nTo address these issues, we introduce AgentBoard, a benchmark designed for multi-turn LLM agents, complemented by an analytical evaluation board for detailed model assessment beyond final success rates. AgentBoard encompasses a diverse set of 9 unique tasks and 1013 exemplary environments, covering a range from embodied AI and game agents to web and tool agents. Each environment, whether newly created or adapted from pre-existing ones, is carefully crafted and authenticated by humans to ensure multi-round and partially observable characteristics in a unified manner. Notably, we have defined or manually annotated subgoals for each data sample, introducing a unified progress rate metric to track the agents' detailed advancements. As we will demonstrate in \u00a74.2, this metric uncovers significant progress made by models that would otherwise appear trivial due to negligible differences in success rates.\\n\\nAlong with the benchmark, we develop the AgentBoard evaluation framework as an open-source toolkit that features an analytical web panel to examine various dimensions of agent abilities through interactive visualization. The toolkit offers a unified interface, providing users with easy access and effortless customization options. As shown in Figure 1, the AgentBoard toolkit currently supports analysis and visualization on fine-grained progress rates tracking, performance breakdown for hard and easy examples, detailed performance across various sub-skills, long-range interaction assessment, grounding accuracy, and trajectory. This detailed evaluation is crucial for acknowledging the progress of LLM agents and for guiding the development of more robust LLM agent models. The comparison between AgentBoard and previous works is shown in Table 1.\\n\\nWe evaluated a range of proprietary and open-weight LLM agents using AgentBoard, obtaining insights into the current landscape of LLMs as agents. Key findings include: (1) GPT-4, unsurprisingly, outperforms all other models by exhibiting extensive proficiency across distinct agentic abilities with Llama3 (Touvron et al., 2023) and DeepSeek LLM (DeepSeek-AI et al., 2024) taking the lead; (2) Strong LLM agents are characterized by their capability for multi-turn interaction with the environment, an ability that is notably lacking in most open-weight models; (3) Emergent agentic abilities are strongly dependent on basic abilities like grounding, world modeling, and self-reflection. Current proprietary models typically demonstrate comprehensive agentic abilities, while open-weight LLMs show varying deficiencies. Through AgentBoard, we highlight the importance of analytic evaluation of LLM agents. The detailed evaluations provided by AgentBoard and its open-source toolkit are expected to significantly contribute to the further development of LLM agents.\"}"}
{"id": "4S8agvKjle", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: A GENT BOARD differs from other LLM benchmarks by providing a comprehensive framework that integrates all four guiding principles within its evaluation system.\\n\\nNotably, AgentBench entails both single and multi-round tasks, with mainly the former differentiating open-sourced models. The GAIA benchmark focuses solely on question answering tasks. The MINT benchmark primarily includes fully-observable environments tasks derived from conventional evaluations such as HumanEval and GSM8K.\\n\\n| Benchmarks        | Task Diversity | Multi-round | Interaction | Partially-Observable Environments | Fine-grained Progress Metrics | Analytical Evaluation |\\n|-------------------|----------------|-------------|-------------|-----------------------------------|-------------------------------|-----------------------|\\n| AgentBench (Liu et al., 2023a) | \u2714              | \u2717           | \u2717           | \u2717                                 |                               | \u2717                     |\\n| GAIA (Mialon et al., 2023)         | \u2717              | \u2717           | \u2717           | \u2717                                 | \u2717                            | \u274c                     |\\n| MINT (Wang et al., 2023b)           | \u2714              | \u2714           | \u2717           | \u2714                                 | \u2717                            | \u2717                     |\\n| API-Bank (Li et al., 2023)          | \u2717              | \u2714           | \u2714           | \u2717                                 | \u2717                            | \u2717                     |\\n| ToolEval (Qin et al., 2023b)        | \u2717              | \u2714           | \u2714           | \u2717                                 | \u2717                            | \u2717                     |\\n| LLM-Eval (Lin and Chen, 2023)       | \u2714              | \u2717           | \u2717           | \u2717                                 | \u2717                            | \u2717                     |\\n| A GENT BOARD | \u2714              | \u2714           | \u2714           | \u2714                                 | \u2714                            | \u2714                     |\\n\\n2 A GENT BOARD \u2013 Overview\\n\\nA GENT BOARD is a unified, open-source benchmark for evaluating LLM agents that adheres to five key principles: task diversity, multi-round interaction, partially-observable environments, fine-grained metrics, and analytical evaluation, as shown in Table 1. Our commitment to these principles manifests in three key areas:\\n\\n\u2022 Task Diversity and Uniformity, where we carefully curate nine diverse environments across four scenarios, ensuring they require multi-round interactions, are fully text-based and primarily partially-observable. This contrasts with many existing LLM agent benchmarks, which are often solvable in a single round, derived from fully-observable tasks like MMLU, or focus on a specific type of task, as demonstrated in Table 1. Compared to AgentBench (Liu et al., 2023a), in particular, our choice of tasks makes A GENT BOARD planning-heavier, where the results on A GENT BOARD well-correlate with the scores on traditional reasoning and coding benchmarks, while the AgentBench scores strongly correlated with scores on the knowledge test MMLU, as illustrated in Ruan et al. (2024). We elaborate on the choice of tasks and their adaptation in \u00a73.\\n\\n\u2022 Fine-grained Progress Rate, where A GENT BOARD is the first to propose a fine-grained progress rate metric tracking the intermediate progress of different agents. This metric distinguishes our benchmark in tracking minimal improvement in LLM agent performances. Such a capability is crucial in current endeavors to develop stronger open-weight LLMs, providing detailed insights that are essential for incremental advancements in agent capabilities. We provide detailed introduction for this metric and its annotation process in \u00a72.2.\\n\\n\u2022 Comprehensive Analysis, where A GENT BOARD is the first LLM Agent benchmark to expand metrics beyond mere success rate and scores to include detailed analyses. As illustrated in Figure 1, such a comprehensive evaluation includes (1) fine-grained progress rates tracking different agents, (2) grounding accuracy, (3) performance breakdown for hard and easy examples, (4) long-range interactions, (5) analyses of performance across various sub-skills, and (6) trajectory with friendly visualization. We elaborate these analyses in our experiments at \u00a74. Additionally, A GENT BOARD provides an web interface through Wandb dashboard that offers interactive visualizations of these analyses during evaluation. We perform a case study on the panel in \u00a75.\\n\\n2.1 A Unified Multi-Round Reflex Agent\\n\\nPreliminaries: An LLM agent receives textual world descriptions, chooses a text action, and gets feedback detailing state changes and any action errors. Interaction with these environments can be modeled as a special case of Partially Observable Markov Decision Processes (POMDPs) defined by tuple \\\\( \\\\langle g, S, A, O, T \\\\rangle \\\\), with goal \\\\( g \\\\), state space \\\\( S \\\\), valid actions space \\\\( A \\\\), observation space (including environment feedback) \\\\( O \\\\), transition function \\\\( T: S \\\\times A \\\\rightarrow S \\\\). An agent with policy \\\\( \\\\pi \\\\) makes prediction at time step \\\\( t \\\\) based on goal \\\\( g \\\\) and memory \\\\( m_t = \\\\{ o_j, a_j, o_{j+1}, a_{j+1}, \\\\ldots o_t \\\\} \\\\), \\\\( 0 \\\\leq j < t \\\\), which is a sequence of actions and observations. This trajectory of the agent \\\\( \\\\tau = [s_0, a_0, s_1, a_1, \\\\ldots s_t] \\\\) is formulated by policy and environmental state transitions, such as \\\\( p_{\\\\pi}(\\\\tau) = p(s_0) \\\\prod_{t=0}^{T} \\\\pi(a_t|g, s_t, m_t) T(s_{t+1}|s_t, a_t) \\\\) (1).\"}"}
{"id": "4S8agvKjle", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You are an agent in a virtual science school environment, tasked to interact with various elements. Here are commands that you can use: open, close, look around...\\n\\nGoal: You should perform actions to accomplish the goal:\\n\\nboil some water.\\n\\nMemory: Observation: This room is called the workshop. In it, you see: the agent, a table, a door to the hallway...\\n\\nAction: go to kitchen\\n\\nObservation: You move to the kitchen.\\n\\nAction: open cupboard\\n\\nObservation: The cupboard is open. There is a mug, a thermometer, and a cloth.\\n\\nUser Action: pickup mug from the cupboard\\n\\nObservation: You move the mug to the inventory.\\n\\nState\\n\\nEnvironment\\n\\nLanguage\\n\\nModel\\n\\nGeneration\\n\\nWhat has my previous actions done?\\n\\nHow does the world evolve?\\n\\nGoal\\n\\nObservation\\n\\nState\\n\\nAction\\n\\nFigure 2: (Left) A structural overview of the reflex agent, which iteratively interacts with the environment and makes next step predictions based on the goal and history. (Right) An example of a prompt for our reflex agent.\\n\\nTable 2: Examples of goals for the 4 task categories in AgentBoard, along with a sampled step of the trajectory and progress rate. The trajectory is generated by GPT-4. Some lengthy observations are omitted with \\\"...\\\" for brevity. The task name in the table uses an abbreviation, the full name can be found in \u00a73.\\n\\n| Task | Goal & Trajectory | Progress Rate |\\n|------|-------------------|---------------|\\n| ALF  | Goal: put a clean egg in microwave. | 0.00 \u2192 0.25 |\\n|      | Step 02 Action: open fridge 1 Observation: You open the fridge 1. The fridge 1 is open. In it, you see a apple 2, a egg 1, a lettuce 1, a pan 2, a plate 1, and a tomato 1. |               |\\n| JC   | Goal: Get out of the house. Then escape the city without getting caught via driving. | 0.43 \u2192 0.57 |\\n|      | Step 29 Action: take a shower Observation: You step into the shower, turn on the water, and within a few moments you feel like a new man. But no time to dawdle - you hop back out again and dry off in record time ... |               |\\n| WA   | Goal: Display the list of issues in the kkroening/ffmpeg-python repository that have labels related to questions | 0.25 \u2192 0.50 |\\n|      | Step 05 Action: click [5398] Observation: Tab 0 (current): Issues \u00b7 Karl Kroening / ffmpeg-python \u00b7 GitLab [6573] RootWebArea 'Issues \u00b7 Karl Kroening / ffmpeg-python \u00b7 GitLab' focused: True [6620] link. ... |               |\\n| TO   | Goal: In \\\"Sheet17\\\", calculate and complete the \\\"Profit\\\" of the products in the table based on the sales information of the products. And then, sort the table in descending order by \\\"Profit\\\". | 0.47 \u2192 0.49 |\\n|      | Step 07 Action: update_cell_by_formula with Action Input: {\"operator\": \\\"PRODUCT\\\", \\\"start_position\\\": \\\"C8\\\", \\\"end_position\\\": \\\"D8\\\", \\\"result_position\\\": \\\"E8\\\"} Observation: ['Product', 'Category', ... |               |\\n\\nThe Unified Framework: AgentBoard unifies all tasks around a general framework where the agent receives observations and performs actions, causing deterministic state transitions based on real-world dynamics. A feedback function is also defined in the environment to derive feedback from each interaction round. This feedback includes: (1) list all valid actions when the agent uses help actions such as check valid actions; (2) execute valid action and return a description of the changed state; (3) issue errors when the agent performs an action outside of the action space.\\n\\nWe aim to use a simplistic agent framework to showcase LLM basic agentic abilities. As shown in Figure 2, our agent makes decisions based on its memory of past perceptions, similar to how humans learn from experience and adapt. The implementation of the reflex agent assessed in this paper adopts an act-only prompting strategy in line with recent studies (Liu et al., 2023b; Zhou et al., 2023; Xu et al., 2023b), detailed in the right part of Figure 2, while other prompting strategies can be easily incorporated into our open-source framework. Also, LLM agents tend to struggle with limited context lengths in long interactions, failing to retain full history. Following the \\\"sliding window\\\" method from LangChain (Chase, 2022), we focus on recent, more impactful interactions within context constraints. This differs from previous practices that stop the agent when context limits are surpassed (Liu et al., 2023a; Wang et al., 2023b), allowing for extended, intricate interactions in our approach. We provide ablation results such as ReAct prompting (Yao et al., 2023) and other long-context processing techniques to justify our framework in Appendix F.\\n\\n2.2 Fine-grained Progress Rate\\n\\nRecent studies highlight the predominant use of success rate as the main metric for agent evaluation, which fails to capture the nuances of partial task completion by language model agents (Liu et al., 2023a; Li et al., 2023). This approach does not differentiate between near-complete tasks and minimal task execution, treating both as equivalent failures. Alternative metrics like reward scores are available but lack standardization (Chevalier-Boisvert et al., 2019; Wang et al., 2022). To mitigate this issue, we introduce a progress rate metric to accurately reflect LM agents' goal attainment at various stages.\"}"}
{"id": "4S8agvKjle", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In each round of interaction, a progress rate, denoted as $r_t$, is assigned to evaluate the agent's advancement towards the goal state $g$. As the agent moves through the states $s = [s_0, \\\\ldots, s_t]$, we assess its progress using a matching score $f(\\\\cdot, g) \\\\rightarrow [0, 1]$ that quantifies the similarity between the current state and the goal state. The initial value of $r_t$ is set to 0, indicating no progress. The progress rate $r_t$ reflects the highest matching score achieved, reaching 1 when the task is completed. The progress rate is formulated as below:\\n\\n$$r_t = \\\\begin{cases} \\n    r_{\\\\text{match}} = \\\\max_{0 \\\\leq i \\\\leq t} f(s_i, g), & \\\\text{if } f(\\\\cdot, g) \\\\text{ is continuous} \\\\\\\\\\n    r_{\\\\text{subgoal}} = \\\\max_{0 \\\\leq i \\\\leq t} \\\\frac{1}{K} \\\\sum_{k=1}^{K} f(s_i, g_k), & \\\\text{otherwise}\\n\\\\end{cases}$$\\n\\nThe function $f(\\\\cdot, g)$ measures state similarity in tasks, such as comparing table states in manipulation activities. It works well for tasks with direct state comparisons but is less effective for tasks with ambiguous intermediate states, where progress is hard to measure. We mitigate this by introducing a discrete matching score to assess how closely intermediate states align with defined subgoals. We begin by decomposing the overall goal $g$ into a sequence of subgoals $g = [g_1, \\\\ldots, g_K]$, with each subgoal leading into the next. The authors manually label each subgoal, which is then checked and adjusted through a rigorous process described in \u00a73.2. Notably, we manually edit the problems for a simpler setup where each final goal aligns with a unique subgoal sequence, and this affects only 5% of the original problems (a detailed descriptions for our adaptations are in Appendix L.1). Note that while we maintain a unique subgoal sequence, this allows for a diverse set of trajectories, e.g. taking detours when accomplishing the task. As an example, for task \u201cclean an egg and put it in microwave\u201d, the necessary subgoals would be \u201copen the fridge\u201d $\\\\rightarrow$ \u201ctaking an egg from the fridge\u201d $\\\\rightarrow$ \u201cclean the egg with sinkbasin\u201d $\\\\rightarrow$ \u201cput the egg in the microwave\u201d. Each subgoal $g_i$ is associated with a labeled state that indicates its completion. To evaluate the match between an agent state and a subgoal, we employ a regular-expression-based matching function denoted as $f(\\\\cdot, g_i) \\\\rightarrow \\\\{0, 1\\\\}$ and the progress rate as $r_{\\\\text{subgoal}}$ in Equation 2.\\n\\nWe employ progress rate along with the commonly used success rate metric, which computes the proportion of tasks completed within $T$ interactions.\\n\\n### 3.1 Environments\\n\\n**Embodied**\\n- **AlfWorld (ALF) (Shridhar et al., 2021)**\\n  are household tasks that require agents to explore surroundings and perform commonsense tasks like \u201cput two soapbars in garbagecan\u201d. This task uses subgoal-based progress rates (Appendix L.2) and the original success rate of the environment as metrics.\\n- **ScienceWorld (SW) (Wang et al., 2022)**\\n  is a challenging interactive text environment testing scientific commonsense, e.g. \u201cmeasure the melting point of the orange juice\u201d. The current subgoals provided by SW do not accurately reflect a language model's performance due to their sparsity and uneven weighting, as further explained in Appendix L.3. To rectify this, we re-annotate the subgoals for calculating progress rate $r_{\\\\text{subgoal}}$. We also re-annotate instructions on tool usage and rooms to explore to ensure the uniqueness of subgoal sequence for task completion.\\n- **BabyAI (BA) (Chevalier-Boisvert et al., 2019)**\\n  is an interactive 20x20 grid environment where agents navigate and interact with objects within a limited sight range. The original setup uses image-based observations and tensor-based actions like \u201c0: move left\u201d. We adapted it to include a textual action space and descriptive textual observations. Furthermore, we re-annotate subgoals for progress rates to fix subgoal sparsity problem in the original environment (Appendix L.4).\\n\\n**Game**\\n- **Jericho (JC) (Hausknecht et al., 2020)**\\n  is a collection of text-based game environments staged in fictional worlds. This task is unique as it requires strong world modeling ability; agents could only gain information about the magic world through exploration and interaction. The original games are too long (need 50-300 steps to finish for LLM agents with fixed context length. Therefore we rewrite the goal of each adventure to restrict the games to be finished within 15 subgoals.\\n- **PDDL (PL) (Vallati et al., 2015)**\\n  is a set of strategic games defined with Planning Domain Definition Language (PDDL). We selected 4 representative games, **Gripper**, **Barman**, **Blocksworld**, **Tyreworld** to benchmark.\"}"}
{"id": "4S8agvKjle", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Human verification of AGENT progress rate. The Pearson correlation coefficients \\\\( \\\\rho \\\\) compare human evaluations with the progress rates on 60 different trajectories per task. The Fleiss' kappa \\\\( \\\\kappa \\\\) reflects inter-annotator agreement. The trajectories are generated by three models: GPT-4, GPT-3.5-Turbo, and DeepSeek-67b.\\n\\nLMM agents in diverse scenarios. We adapted the environment implementation (Silver and Chitnis, 2020) written in PDDL expressions to provide text-based observations for agents, enabling a natural language interface for LLM. We measure progress using a matching score, \\\\( r_{\\\\text{match}} \\\\), which assesses similarity between the current and goal states (Appendix L.6).\\n\\nWeb - WebShop (WS) (Yao et al., 2022) is a network-based simulation environment for e-commerce experiences. Based on the original implementation method (Yao et al., 2022; Shinn et al., 2023), we have improved the error feedback, including refining the observation for exceeding page limits and interacting with wrong objects. These enhancements contribute to the effective interaction of the LLM agent with the environment. We also measure the distance of the current state to the final goal as the progress rate and expand the product scoring rules from Yao et al. (2022) to derive the score (Appendix L.7).\\n\\nTool - Tool-Query (TQ) consists of three sub-environments: Weather, Movie and Academia Environment. This tasks primarily involves querying information from respective databases by planning the use of diverse query tools. We manually curate diverse problems for each environment. We also annotate subgoals to compute the progress rate \\\\( r_{\\\\text{subgoal}} \\\\) (Appendix L.9). While LLMs often provide direct answers in question answering, we only consider an answer correct if the model follows the appropriate trajectory to access the databases. To ensure this, we design questions that cannot be answered directly by state-of-the-art LLMs and provide in-context examples to guide the LLM in querying the databases effectively.\\n\\nTool - Tool-Operation (TO) includes two sub-environments: Todo list management and Google Sheet Operations. These tasks involve using tools to access and modify information. The progress rate in the Todo Environment is measured using \\\\( r_{\\\\text{subgoal}} \\\\), similar to Tool-Query Environments. In the Sheet Environment, progress is evaluated using \\\\( r_{\\\\text{match}} \\\\), which uses a matching score between the cells of the current and golden table (Appendix L.10).\\n\\n3.2 Annotation Verification and Metric Justification\\n\\nAfter human annotation, we manually verified our labeled subgoals through multiple verification stages to ensure its quality, as detailed in Appendix J. More importantly, we conduct a user study to justify our proposed progress rate metric, asking human annotators to assess the progress of model trajectories and then evaluating its correlation with our automatic progress rate metric. Specifically, we gather 60 model trajectories for each of the 8 tasks from three strong LLMs\u2014\u2014GPT-4, GPT-3.5-Turbo, and Deepseek-67b. Each trajectory is assessed by four authors of the paper. The individual human rater is asked to select progress score from \\\\{0\\\\%, 25\\\\%, 50\\\\%, 75\\\\%, 100\\\\%\\\\} given trajectories and task descriptions, without seeing the automatic score. Mean of four scores is taken as the final human score for every trajectory. We show the Pearson correlation between human progress score and the progress rate in Figure 3, and report Fleiss' kappa \\\\( \\\\kappa \\\\) to reflect inter-annotator agreement. Results show that progress rate highly correlates with human assessment on the progress where the Pearson correlation exceeds 0.95 on all tasks, and substantial agreement is reached among the annotators.\\n\\n4 Experiments\\n\\nWe conduct a comprehensive evaluation of popular LLMs, including proprietary and open-weight models. Firstly, we report the success rate and progress rate of these agents. Then, we perform detailed analysis of the...\"}"}
{"id": "4S8agvKjle", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"proprietary models, open-weight general LLMs, and agent LLMs. Confidence Intervals are used to quantify the uncertainty in the results.\\n\\nTable 3: Performance of different LLMs sorted by success rate. The \\\"Avg.\\\" represents the average calculated across all tasks. The success rate represents the percentage of times the agent succeeded in completing the task, while the progress rate represents the percentage of times the agent made progress towards completing the task. The performance of LLMs as agents overall demonstrates the capability of these models in various tasks.\\n\\n- **Notably**, proprietary models still outperform the best open-weight models: GPT-4 significantly surpasses other models in terms of success rate. Additionally, larger LLMs exhibit superior performance compared to the 7-13 billion parameter models like Mistral-7b. Larger LLMs outperform their smaller counterparts in most tasks. For instance, the 70 billion parameter models, such as Llama3-70b, follow the scaling law (Kaplan et al., 2020). Larger LLMs outperform their smaller counterparts in most tasks.\\n\\n- **Agent-specific features** as it reflects the overall ability of the agent at each step. Often, proprietary models exhibit higher success rates compared to open-weight models. For example, Llama2-13b leads the progress rate by 11.7% in success rate, while the significantly smaller CodeLlama-34b exhibits similar success rates \u2013 for instance, on the Embodied AI and Game categories, the success rates of most of the models are around 20-30%, but their progress rates differ significantly: Llama2-70b exhibits 13.2% progress rate, while Text-Davinci-003 shows 18.8% progress rate.\\n\\n- **Strong coding skills** help agent tasks. Notably, proprietary models outperform the open-weight ones. For the detailed prompt, please refer to Appendix N. We benchmark a series of strong LLMs as agents and measure the various abilities of LLM agents, as part of the A GENT OARD project.\\n\\nPlease refer to Appendix I for detailed setup.\\n\\n**4.1 Evaluation Setup**\\n\\n- **AgentLM-70b** exhibits 58.4/50.7 rates, demonstrating its superior performance.\\n- **GPT-3.5-Turbo** exhibits 35.6/17.2 rates, showing its competitive performance.\\n- **Llama3-70b** exhibits 29.6/12.7 rates, indicating its moderate performance.\\n- **Claude3-Haiku** exhibits 20.7/2.2 rates, showing its limited performance.\\n- **Mistral-7b** exhibits 9.8/0.0 rates, indicating its inferior performance.\\n- **Llama2-13b** exhibits 7.8/0.0 rates, showing its lower performance.\\n- **AgentLM-13b** exhibits 14.1/0.7 rates, indicating its better performance.\\n- **Text-Davinci-003** exhibits 18.8/9.0 rates, showing its moderate performance.\\n- **DeepSeek-67b** exhibits 34.5/20.9 rates, indicating its better performance.\\n- **xLAM-70b** exhibits 53.4/42.5 rates, showing its superior performance.\\n- **GPT-3.5-Turbo-16k** exhibits 25.2/4.5 rates, indicating its moderate performance.\\n- **Llama3-8b** exhibits 14.1/0.7 rates, showing its better performance.\\n- **AgentLM-70b** exhibits 58.4/50.7 rates, demonstrating its superior performance.\\n- **GPT-4** exhibits 60.0/70.0 rates, indicating its excellent performance.\\n\\n**4.2 Main Results**\\n\\n- **Clubhouse** exhibits 65.5/43.3 78.8 rates, indicating its superior performance.\\n- **DeepMind** exhibits 32.6/31.6 rates, showing its competitive performance.\\n- **Microsoft** exhibits 65.5/43.3 78.8 rates, indicating its superior performance.\\n- **GPT-4** exhibits 35.6/17.2 rates, showing its competitive performance.\\n- **Llama3-70b** exhibits 29.6/12.7 rates, indicating its moderate performance.\\n- **Claude3-Haiku** exhibits 20.7/2.2 rates, showing its limited performance.\\n- **Mistral-7b** exhibits 9.8/0.0 rates, indicating its inferior performance.\"}"}
{"id": "4S8agvKjle", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"While tuning improves models' ability to follow instructions, it doesn't necessarily boost overall performance. Despite lower main results, achieves a grounding score of 68.7%, comparable to Text-Davinci-003, which shows strong performance among LLMs. The grounding accuracy of Text-Davinci-003 is only 58.9% on average but performs comparably to GPT-3.5-Turbo in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly, as noted by Zheng et al. (2024), performs better in main results, indicating strengths in other areas. Specifically, it correctly,"}
{"id": "4S8agvKjle", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Performance breakdown for hard and easy examples. For each task, we divide environments into \u201chard\u201d or \u201ceasy\u201d based on the number of subgoals/conditions to meet, as shown in Table 14. The outcomes are presented in Table 5. Unsurprisingly, all models show a significant performance drop on hard examples, consistent with Dziri et al. (2023)\u2019s findings that even robust LLMs like GPT-4 struggle with task compositionality. The performance on hard examples, reflecting challenging multiple subgoals settings, could be more crucial than average metrics.\\n\\nLong-Range Interaction. We analyze their progress rate over interaction steps (Figure 4). Models like GPT-4, Claude2 show consistent progress over 30 steps in Alfworld and PDDL tasks. However, in WebArena and Tool tasks, their performance peaks early and then stagnates. This may be due to that later stages for these tasks are challenging. Open-weight models, except Llama3-70b and Deepseek-67b, peak early and generally stop progressing after about 6 steps, likely struggling with the increased complexity of long-range interactions and extended context requirements.\\n\\nSub-skill Analysis. We aim to assess LLMs across several facets: memory that measures incorporating long-range information in context, planning that assesses decomposing complex goals into manageable sub-goals, world modeling which tests knowledge necessary for task completion, self-reflection that captures the ability to use environmental feedback, grounding that focuses on competency in generating valid actions, and spatial navigation that represents efficiency in moving to a target location. We develop a sub-skill scoring system based on Table 11. As depicted in Figure 5, GPT-4 surpasses all other LLMs across all sub-skills.\\n\\nExploration Behavior. Analysis on the exploration behavior are available in Appendix E.\"}"}
{"id": "4S8agvKjle", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating LLM in Decision Making Problems\\n\\nSeveral benchmarks and toolkits for LLM agents have been established, focusing on various tasks such as web-browsing, games, and tool use (Yao et al., 2022; Zhou et al., 2023; Shridhar et al., 2021; Qin et al., 2023a; Wang et al., 2023a; Ye et al., 2024; Kinniment et al., 2023). A few other benchmarks provide a proof-of-concept study on specific LLM features, with Wang et al. (2023b) focusing on model interaction ability, and Liu et al. (2023b) examining agent structures. Recent works by Liu et al. (2023a); Wu et al. (2023); Mialon et al. (2023) present a generalist challenge for LLM agents, please refer to Table 1 for a comparison. Note that recent progress in multimodal LLMs has spurred research into multimodal LLM agents (Zheng et al., 2024; Yang et al., 2023). Our study focuses exclusively on text-based environments to assess LLM agent abilities via textual reasoning and actions in-depth.\\n\\nConclusion\\n\\nIn this work, we introduce AGENTBOARD as a benchmark for evaluating generalist LLM agents. In addition to being a benchmark, AGENTBOARD offers an open-source, analytical evaluation framework that facilitates easy customization, unified metrics, and comprehensive analysis from diverse aspects, in addition to an interactive visualization web panel. Such analytical evaluation is equipped with an interactive visualization web panel, allowing users to efficiently explore the evaluation and gain a deeper understanding of the agents of interest. Overall, AGENTBOARD aims to facilitate detailed evaluation and understanding of LLM agents, driving further advancements in the field.\\n\\nLimitations:\\n\\nLimitations of AGENTBOARD include reliance on human-annotated subgoals to calculate progress rate. Although using LLMs for annotation is considered, current models underperform on AGENTBOARD tasks and cannot accurately generate subgoals. Additionally, AGENTBOARD evaluates agents mainly in simulated environments to maintain standardization. However, real-world benchmarking is crucial for practical applications but presents challenges such as variable ground truth labels and security risks. We will address them in future work.\\n\\nAcknowledgement\\n\\nWe thank Tao Yu, Shuyan Zhou for providing valuable comments on research questions and experimental design. We thank Yiheng Xu, Haiteng Zhao and Hongjin Su for early stage beta testing. Zhihao Zhu and Yaohui Jin are with the MoE Key Lab of Artificial Intelligence, Al Institute, Shanghai Jiao Tong University, and Zhihao Zhu is supported by Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102) and the Fundamental Research Funds for the Central Universities. We thank wandb for free logging and backing the engine of AGENTBOARD.\\n\\nReferences\\n\\nAhn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., et al. (2022). Do as i can, not as i say: Grounding language in robotic affordances. ArXiv preprint, abs/2204.01691.\\n\\nAnthropic (2023). Introducing claude.\\n\\nChase, H. (2022). Langchain.\\n\\nChevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H., and Bengio, Y. (2019). Babyai: A platform to study the sample efficiency of grounded language learning. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.\\n\\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., et al. (2023). Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna.lmsys.org (accessed 14 April 2023).\\n\\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. (2021). Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.\\n\\nDeepSeek-AI, :, Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding, H., Dong, K., Du, Q., Fu, Z., Gao, H., Gao, K., Gao, W., Ge, R., Guan, K., Guo, D., Guo, J., Hao, G., Hao, Z., He, Y., Hu, W., Huang, P., Li, E., Li, G., Li, J., Li, Y., Li, Y. K., Liang, W., Lin, F., Liu, A. X., Liu, B., Liu, W., Liu, X., Liu, X., Liu, Y., 10.\"}"}
{"id": "4S8agvKjle", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lu, H., Lu, S., Luo, F., Ma, S., Nie, X., Pei, T., Piao, Y., Qiu, J., Qu, H., Ren, T., Ren, Z., Ruan, C., Sha, Z., Shao, Z., Song, J., Su, X., Sun, J., Sun, Y., Tang, M., Wang, B., Wang, P., Wang, S., Wang, Y., Wu, T., Wu, Y., Xie, X., Xie, Z., Xie, Z., Xiong, Y., Xu, H., Xu, R., Xu, Y., Yang, D., You, Y., Yu, S., Yu, X., Zhang, B., Zhang, H., Zhang, L., Zhang, L., Zhang, M., Zhang, M., Zhang, W., Zhang, Y., Zhao, C., Zhao, Y., Zhou, S., Zhou, S., Zhu, Q., and Zou, Y. (2024). DeepSeek LLM: Scaling open-source language models with longtermism.\\n\\nDeng, X., Gu, Y., Zheng, B., Chen, S., Stevens, S., Wang, B., Sun, H., and Su, Y. (2023). Mind2web: Towards a generalist agent for the web. ArXiv preprint, abs/2306.06070.\\n\\nDriess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et al. (2023). Palm-e: An embodied multimodal language model. ArXiv preprint, abs/2303.03378.\\n\\nDziri, N., Lu, X., Sclar, M., Li, X. L., Jian, L., Lin, B. Y., West, P., Bhagavatula, C., Bras, R. L., Hwang, J. D., et al. (2023). Faith and fate: Limits of transformers on compositionality. ArXiv preprint, abs/2305.18654.\\n\\nGu, Y., Deng, X., and Su, Y. (2022). Don't generate, discriminate: A proposal for grounding language models to real-world environments. ArXiv preprint, abs/2212.09736.\\n\\nHausknecht, M. J., Ammanabrolu, P., C\u00f4t\u00e9, M., and Yuan, X. (2020). Interactive fiction games: A colossal adventure. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7903\u20137910. AAAI Press.\\n\\nHong, S., Zheng, X., Chen, J., Cheng, Y., Wang, J., Zhang, C., Wang, Z., Yau, S. K. S., Lin, Z., Zhou, L., et al. (2023). Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352.\\n\\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. (2023). Mistral 7b. ArXiv preprint, abs/2310.06825.\\n\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.\\n\\nKinniment, M., Sato, L. J. K., Du, H., Goodrich, B., Hasin, M., Chan, L., Miles, L. H., Lin, T. R., Wijk, H., Burget, J., et al. (2023). Evaluating language-model agents on realistic autonomous tasks. ArXiv preprint, abs/2312.11671.\\n\\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. (2023). Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611\u2013626.\\n\\nLanham, T., Chen, A., Radhakrishnan, A., Steiner, B., Denison, C., Hernandez, D., Li, D., Durmus, E., Hubinger, E., Kernion, J., et al. (2023). Measuring faithfulness in chain-of-thought reasoning. ArXiv preprint, abs/2307.13702.\\n\\nLeCun, Y. (2022). A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62.\\n\\nLi, M., Song, F., Yu, B., Yu, H., Li, Z., Huang, F., and Li, Y. (2023). Api-bank: A benchmark for tool-augmented llms. ArXiv preprint, abs/2304.08244.\\n\\nLin, Y.-T. and Chen, Y.-N. (2023). Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. ArXiv preprint, abs/2305.13711.\\n\\nLiu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., et al. (2023a). Agentbench: Evaluating llms as agents. ArXiv preprint, abs/2308.03688.\\n\\nLiu, Z., Yao, W., Zhang, J., Xue, L., Heinecke, S., Murthy, R., Feng, Y., Chen, Z., Niebles, J. C., Arpit, D., et al. (2023b). Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents. ArXiv preprint, abs/2308.05960.\\n\\nMadaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. (2023). Self-refine: Iterative refinement with self-feedback. ArXiv preprint, abs/2303.17651.\\n\\nMialon, G., Fourrier, C., Swift, C., Wolf, T., LeCun, Y., and Scialom, T. (2023). Gaia: a benchmark for general ai assistants. ArXiv preprint, abs/2311.12983.\"}"}
{"id": "4S8agvKjle", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OpenAI (2022). Introducing chatgpt.\\n\\nOpenAI (2023). Gpt-4 technical report.\\n\\narXiv, pages 2303\u201308774.\\n\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744.\\n\\nPourchot, A. and Sigaud, O. (2019). CEM-RL: combining evolutionary and gradient-based methods for policy search. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.\\n\\nPuterman, M. L. (1990). Markov decision processes. Handbooks in operations research and management science, 2:331\u2013434.\\n\\nQin, Y., Hu, S., Lin, Y., Chen, W., Ding, N., Cui, G., Zeng, Z., Huang, Y., Xiao, C., Han, C., et al. (2023a). Tool learning with foundation models. ArXiv preprint, abs/2304.08354.\\n\\nQin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Cong, X., Tang, X., Qian, B., et al. (2023b). Toolllm: Facilitating large language models to master 16000+ real-world apis. ArXiv preprint, abs/2307.16789.\\n\\nReed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., et al. (2022). A generalist agent. ArXiv preprint, abs/2205.06175.\\n\\nRichards, T. B. (2023). Significant-gravitas/autogpt: An experimental open-source attempt to make gpt-4 fully autonomous.\\n\\nRozi\u00e8re, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. (2023). Code llama: Open foundation models for code. ArXiv preprint, abs/2308.12950.\\n\\nRuan, Y., Maddison, C. J., and Hashimoto, T. (2024). Observational scaling laws and the predictability of language model performance. arXiv preprint arXiv:2405.10938.\\n\\nRussell, S. and Norvig, P. (2005). Ai a modern approach.\\n\\nShi, T., Karpathy, A., Fan, L., Hernandez, J., and Liang, P. (2017a). World of bits: An open-domain platform for web-based agents. In Precup, D. and Teh, Y. W., editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 3135\u20133144. PMLR.\\n\\nShi, T., Karpathy, A., Fan, L., Hernandez, J., and Liang, P. (2017b). World of bits: An open-domain platform for web-based agents. In Precup, D. and Teh, Y. W., editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 3135\u20133144. PMLR.\\n\\nShinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R., and Yao, S. (2023). Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems.\\n\\nShridhar, M., Yuan, X., C\u00f4t\u00e9, M., Bisk, Y., Trischler, A., and Hausknecht, M. J. (2021). Alfworld: Aligning text and embodied environments for interactive learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.\\n\\nSilver, T. and Chitnis, R. (2020). Pddlgym: Gym environments from pddl problems. ArXiv preprint, abs/2002.06432.\\n\\nSong, C. H., Wu, J., Washington, C., Sadler, B. M., Chao, W.-L., and Su, Y. (2023). Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2998\u20133009.\\n\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient foundation language models. ArXiv preprint, abs/2302.13971.\\n\\nVallati, M., Chrpa, L., Grze\u0161, M., McCluskey, T. L., Roberts, M., Sanner, S., et al. (2015). The 2014 international planning competition: Progress and trends. Ai Magazine, 36(3):90\u201398.\\n\\nWang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. (2023a). V oyager: An open-ended embodied agent with large language models. ArXiv preprint, abs/2305.16291.\"}"}
{"id": "4S8agvKjle", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Wang, R., Jansen, P., C\u00f4t\u00e9, M.-A., and Ammanabrolu, P. (2022). ScienceWorld: Is your agent smarter than a 5th grader? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11279\u201311298, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nWang, X., Wang, Z., Liu, J., Chen, Y., Yuan, L., Peng, H., and Ji, H. (2023b). Mint: Evaluating llms in multi-turn interaction with tools and language feedback. ArXiv preprint, abs/2309.10691.\\n\\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al. (2022). Emergent abilities of large language models. ArXiv preprint, abs/2206.07682.\\n\\nWu, Y., Tang, X., Mitchell, T. M., and Li, Y. (2023). Smartplay: A benchmark for llms as intelligent agents. ArXiv preprint, abs/2310.01557.\\n\\nWu, Z., Han, C., Ding, Z., Weng, Z., Liu, Z., Yao, S., Yu, T., and Kong, L. (2024). Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456.\\n\\nXie, T., Zhou, F., Cheng, Z., Shi, P., Weng, L., Liu, Y., Hua, T. J., Zhao, J., Liu, Q., Liu, C., et al. (2023). Openagents: An open platform for language agents in the wild. ArXiv preprint, abs/2310.10634.\\n\\nXu, B., Liu, X., Shen, H., Han, Z., Li, Y., Yue, M., Peng, Z., Liu, Y., Yao, Z., and Xu, D. (2023a). Gentopia: A collaborative platform for tool-augmented llms. arXiv preprint arXiv:2308.04030.\\n\\nXu, Q., Hong, F., Li, B., Hu, C., Chen, Z., and Zhang, J. (2023b). On the tool manipulation capability of open-source large language models. ArXiv preprint, abs/2305.16504.\\n\\nXu, Y., Su, H., Xing, C., Mi, B., Liu, Q., Shi, W., Hui, B., Zhou, F., Liu, Y., Xie, T., et al. (2023c). Lemur: Harmonizing natural language and code for language agents. ArXiv preprint, abs/2310.06830.\\n\\nYang, Z., Liu, J., Han, Y., Chen, X., Huang, Z., Fu, B., and Yu, G. (2023). Appagent: Multimodal agents as smartphone users. ArXiv preprint, abs/2312.13771.\\n\\nYao, S., Chen, H., Yang, J., and Narasimhan, K. (2022). Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744\u201320757.\\n\\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. (2023). ReAct: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations.\\n\\nYe, J., Li, G., Gao, S., Huang, C., Wu, Y., Li, S., Fan, X., Dou, S., Zhang, Q., Gui, T., et al. (2024). Tooleyes: Fine-grained evaluation for tool learning capabilities of large language models in real-world scenarios. ArXiv preprint, abs/2401.00741.\\n\\nZheng, B., Gou, B., Kil, J., Sun, H., and Su, Y. (2024). Gpt-4v(ision) is a generalist web agent, if grounded. ArXiv preprint, abs/2401.01614.\\n\\nZhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Bisk, Y., Fried, D., Alon, U., et al. (2023). Webarena: A realistic web environment for building autonomous agents. ArXiv preprint, abs/2307.13854.\"}"}
{"id": "4S8agvKjle", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix\\n\\nA Author Contributions\\n\\nCode Implementation\\nChang Ma implemented the code base for AgentBoard framework. The code for different tasks is implemented by respective person in charge: Junlei Zhang (Alfworld, Scienceworld), Chang Ma (BabyAI, Jericho and PDDL), Zhihao Zhu (WebShop and WebArena), Cheng Yang (Tool-Query and Tool-Operation). The website was implemented by Zhihao Zhu and the visualization panel was implemented by Chang Ma. The code of Alfworld, ScienceWorld, PDDLGym, WebShop, WebArena and Mint sped up the implementation.\\n\\nTask Unification\\nJunlei Zhang, Chang Ma, Cheng Yang, Zhihao Zhu implemented the tasks into environmental interaction format, provided labels for respective tasks, adapted the metrics, and verified the performances. Chang Ma, Junlei Zhang, Junxian He additionally verified the tasks to be unified.\\n\\nPaper writing\\nChang Ma and Junxian He finished introduction and methodology sections of the paper. Junlei Zhang and Chang Ma wrote the experiments section. Cheng Yang provided all the visualizations shown in the paper. Cheng Yang, Zhihao Zhu added results and analysis for their corresponding parts. Junxian He carefully reviewed and revised the paper and gave feedback for multiple rounds. Other authors help proofread and provide feedbacks.\\n\\nExperiments\\nChang Ma and Junlei Zhang co-lead the evaluation of the models. Zhihao Zhu conducted all the evaluations on web tasks. Cheng Yang conducted evaluation on tool tasks for several models and conducted experiments on analysis and visualization.\\n\\nData Collection and Human Annotation\\nData for task examples and progress rate annotation for each task is collected and annotated with one person in charge, and verified by at least two others: Junlei Zhang led data collection and annotation for ScienceWorld, Alfworld; Chang Ma led data collection and annotation for BabyAI, Jericho and PDDL; Zhihao Zhu led data collection and annotation for WebShop, WebArena and Sheet task in Tool-Operation; Cheng Yang led data collection and annotation for Tool-Query and Tool-Operation. Junlei Zhang led data validation for BabyAI and Tool-Query; Chang Ma led data validation for ScienceWorld and WebShop; Zhihao Zhu led data validation for Jericho, PDDL and Tool-Operation; Cheng Yang led data validation for ScienceWorld and WebArena. Junxian He is the main advisor of this project.\\n\\nB Limitations\\n\\nWhile AGENTBOARD attempts to circumvent the current pitfalls of LLM benchmarks, some limitations remain:\\n\\nHuman-dependent Annotation\\nOne limitation of AGENTBOARD is its reliance on human-annotated subgoals to measure the progress rate of LLM agents. The subjectivity of annotators calls for multiple verifications just to ensure the uniformity of the data. Also, as the complexity of tasks increases, the number of subgoals and the granularity required in annotation can grow significantly. This makes the process less scalable and more labor-intensive. One alternative is to use LLMs instead of humans to annotate these subgoals, but all LLMs currently underperform on AGENTBOARD tasks and cannot accurately generate subgoals. We look forward to better LLMs that could autonomize planning subgoal annotation.\\n\\nBenchmarking Real-World Problems\\nThe price to pay for a standardized and definitive agent benchmark is to evaluate agents in simulated environments. However, it's important to also benchmark on real-world problems for future applications. Currently, there are a few challenges with benchmarking on real-world problems that we hope to overcome in subsequent work: (1) Variable ground truth labels: most real-world environments are ever-changing, e.g., contents on a web page, and this could lead to changes in states and labels for LLM agents, creating difficulties for benchmarking. (2) Security measures: we benchmark not only on obtaining information from environments but also on operating within and altering these environments. In real-world scenarios, it could be dangerous to unleash an LLM agent, e.g., on the Internet, which could lead to harm or generate malicious information. Therefore, it is very important yet challenging to constrain the LLM agent's action space in the real world while still offering it the freedom to accomplish tasks.\"}"}
{"id": "4S8agvKjle", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C Ethics and Societal Impact\\n\\nThis paper presents work whose goal is to advance the field of Machine Learning. Potential societal implications include allowing LLM agents to access online API in tool-operation evaluation. LLM agents could access and edit online information including Google Sheets and To-do lists. However, we made sure that no personal information is leaked and no generated content is distributed online during the evaluation process.\\n\\nD Confidence Interval of LLM Agents Evaluation\\n\\nIn Table 6 and 7, we report the confidence interval of representative models. Notably, when comparing proprietary models to open-source models, the former often exhibit larger deviations in their outputs. This can be primarily attributed to the decoding process of proprietary models, which typically involves additional post-processing steps. Open-source models demonstrate smaller deviations as we have chosen to use smaller temperatures to ensure the reproducibility of the experiments.\\n\\n| Models        | ALF | SW | BA | JC | PL | WS | W | A | TQ | TO | Avg   |\\n|---------------|-----|----|----|----|----|----|---|---|---|----|------|\\n| GPT-4         | 69.1\u00b111.6 | 78.3\u00b13.0 | 65.7\u00b17.5 | 45.4\u00b112.9 | 79.7\u00b12.7 | 77.1\u00b11.4 | 42.5\u00b14.4 | 85.6\u00b10.7 | 79.1\u00b16.2 | 70.3\u00b14.7 |\\n| GPT-3.5       | 33.6\u00b14.3   | 77.8\u00b13.0   | 65.7\u00b17.5   | 45.4\u00b112.9   | 79.7\u00b12.7   | 77.1\u00b11.4   | 42.5\u00b14.4   | 85.6\u00b10.7   | 79.1\u00b16.2   | 70.3\u00b14.7   |\\n| Llama-13b     | 8.3\u00b11.6     | 1.4\u00b10.3     | 15.8\u00b14.0     | 7.0\u00b13.3     | 4.8\u00b11.0     | 60.9\u00b12.3     | 7.9\u00b10.0     | 34.5\u00b12.6     | 31.0\u00b12.9     | 19.1\u00b11.9     |\\n\\nTable 6: Confidence Interval of Progress Rate Metrics\\n\\n| Models        | ALF | SW | BA | JC | PL | WS | W | A | TQ | TO | Avg   |\\n|---------------|-----|----|----|----|----|----|---|---|---|----|------|\\n| GPT-4         | 46.3\u00b120.3 | 50.4\u00b19.6 | 52.1\u00b15.8 | 26.7\u00b110.4 | 59.5\u00b13.9 | 39.9\u00b10.8 | 20.1\u00b17.0 | 70.0\u00b11.7 | 54.2.\u00b112.3 | 47.8.\u00b15.4 |\\n| GPT-3.5       | 13.5\u00b15.2   | 3.4\u00b10.3   | 0.00\u00b10/0   | 0.00\u00b10/0   | 3.6\u00b10.1   | 0.00\u00b10/0   | 0.00\u00b10/0   | 0.00\u00b10/0   | 0.00\u00b10/0   | 0.00\u00b10/0 |\\n| Llama-13b     | 0.0\u00b10.0    | 0.0\u00b10.0   | 5.6\u00b11.0    | 0.0\u00b10.0    | 0.0\u00b10.0   | 10.0\u00b10.7   | 2.0\u00b10.0    | 0.0\u00b10.0    | 0.0\u00b10.0    | 0.6\u00b10.2 |\\n\\nTable 7: Confidence Interval of Success Rate Metrics\\n\\nE Exploration Behavior Analysis\\n\\nWe examine the exploration behavior of models in various environments, as illustrated in Table 8. The ability of agents to explore plays a significant role in their performance in partially-observable environments, as diverse exploration trajectories enable agents to acquire all the necessary information. We compare the number of locations explored by models, including rooms in BabyAI, containers in AlfWorld, and places in Jericho. This metric reflects the models' exploration capabilities. Most models are unable to explore the minimum number of locations necessary to complete the goal.\\n\\n| Tasks               | Minimum | GPT-4 | GPT-3.5-Turbo | Llama2-70b | CodeLlama-34b | Vicuna-13b-16k |\\n|---------------------|--------|-------|---------------|------------|---------------|----------------|\\n| BabyAI - UnlocktoUnlock | 3      | 1     | 1.25          | 1          | 1.25          | 1              |\\n| BabyAI - FindObjs5  | 3      | 2     | 3.5           | 2          | 2             | 1              |\\n| BabyAI - Keycorridor| 3      | 3     | 2.5           | 1.5        | 1.75          | 1.25           |\\n| AlfWorld            | 3      | 5.625 | 5.125         | 1.75       | 0.125         | 1              |\\n| Jericho - Zork1     | 5      | 5     | 5             | 1          | 5             | 1              |\\n| Jericho - Zork2     | 6      | 6     | 2             | 1          | 3             | 3              |\\n| Jericho - Zork3     | 11     | 6     | 4             | 3          | 2             | 1              |\\n\\nTable 8: Comparison of the number of locations (room in babyai, containers in alfworld, and places in Jericho) explored by models. The minimum column states the least number of locations need to explore on average in order to finish the tasks.\\n\\nF Ablation Study of Agent Framework\\n\\nIn this part we discuss alternations of framework design for testing LLM agents. Our principal goal is to test the basic agentic abilities of LLM, which calls for a simplistic framework to avoid introducing confounders. Currently there are many modular-based agent frameworks (Wang et al., 2023a; Hong et al., 2023; Wu et al., 2023b; 2023c; 2023d; 2023e; 2023f; 2023g; 2023h; 2023i; 2023j; 2023k; 2023l; 2023m; 2023n; 2023o; 2023p; 2023q; 2023r; 2023s; 2023t; 2023u; 2023v; 2023w; 2023x; 2023y; 2023z; 2023aa; 2023ab; 2023ac; 2023ad; 2023ae; 2023af; 2023ag; 2023ah; 2023ai; 2023aj; 2023ak; 2023al; 2023am; 2023an; 2023ao; 2023ap; 2023aq; 2023ar; 2023as; 2023at; 2023au; 2023av; 2023aw; 2023ax; 2023ay; 2023az; 2023ba; 2023bb; 2023bc; 2023bd; 2023be; 2023bf; 2023bg; 2023bh; 2023bi; 2023bj; 2023bk; 2023bl; 2023bm; 2023bn; 2023bo; 2023bp; 2023bq; 2023br; 2023bs; 2023bt; 2023bu; 2023bv; 2023bw; 2023bx; 2023by; 2023bz; 2023ca; 2023cb; 2023cc; 2023cd; 2023ce; 2023cf; 2023cg; 2023ch; 2023ci; 2023cj; 2023ck; 2023cl; 2023cm; 2023cn; 2023co; 2023cp; 2023cq; 2023cr; 2023cs; 2023ct; 2023cu; 2023cv; 2023cw; 2023cx; 2023cy; 2023cz; 2023da; 2023db; 2023dc; 2023dd; 2023de; 2023df; 2023dg; 2023dh; 2023di; 2023dj; 2023dk; 2023dl; 2023dm; 2023dn; 2023do; 2023dp; 2023dq; 2023dr; 2023ds; 2023dt; 2023du; 2023dv; 2023dw; 2023dx; 2023dy; 2023dz; 2023ea; 2023eb; 2023ec; 2023ed; 2023ee; 2023ef; 2023eg; 2023eh; 2023ei; 2023ej; 2023ek; 2023el; 2023em; 2023en; 2023eo; 2023ep; 2023eq; 2023er; 2023es; 2023et; 2023eu; 2023ev; 2023ew; 2023ex; 2023ey; 2023ez; 2023fa; 2023fb; 2023fc; 2023fd; 2023fe; 2023ff; 2023fg; 2023fh; 2023fi; 2023fj; 2023fk; 2023fl; 2023fm; 2023fn; 2023fo; 2023fp; 2023fq; 2023fr; 2023fs; 2023ft; 2023fu; 2023fv; 2023fw; 2023fx; 2023fy; 2023fz; 2023ga; 2023gb; 2023gc; 2023gd; 2023ge; 2023gf; 2023gg; 2023gh; 2023gi; 2023gj; 2023gk; 2023gl; 2023gm; 2023gn; 2023go; 2023gp; 2023gq; 2023gr; 2023gs; 2023gt; 2023gu; 2023gv; 2023gw; 2023gx; 2023gy; 2023gz; 2023ha; 2023hb; 2023hc; 2023hd; 2023he; 2023hf; 2023hg; 2023hh; 2023hi; 2023hj; 2023hk; 2023hl; 2023hm; 2023hn; 2023ho; 2023hp; 2023hq; 2023hr; 2023hs; 2023ht; 2023hu; 2023hv; 2023hw; 2023hx; 2023hy; 2023hz; 2023ia; 2023ib; 2023ic; 2023id; 2023ie; 2023if; 2023ig; 2023ih; 2023ij; 2023ik; 2023il; 2023im; 2023in; 2023io; 2023ip; 2023iq; 2023ir; 2023is; 2023it; 2023iu; 2023iv; 2023iw; 2023ix; 2023iy; 2023iz; 2023ja; 2023jb; 2023jc; 2023jd; 2023je; 2023jf; 2023jg; 2023jh; 2023ji; 2023jj; 2023jk; 2023jl; 2023jm; 2023jn; 2023jo; 2023jp; 2023jq; 2023jr; 2023js; 2023jt; 2023ju; 2023jv; 2023jw; 2023jx; 2023jy; 2023jz; 2023ka; 2023kb; 2023kc; 2023kd; 2023ke; 2023kf; 2023kg; 2023kh; 2023ki; 2023kj; 2023kk; 2023kl; 2023km; 2023kn; 2023ko; 2023kp; 2023kq; 2023kr; 2023ks; 2023kt; 2023ku; 2023kv; 2023kw; 2023kx; 2023ky; 2023kz; 2023la; 2023lb; 2023lc; 2023ld; 2023le; 2023lf; 2023lg; 2023lh; 2023li; 2023lj; 2023lk; 2023ll; 2023lm; 2023ln; 2023lo; 2023lp; 2023lq; 2023lr; 2023ls; 2023lt; 2023lu; 2023lv; 2023lw; 2023lx; 2023ly; 2023lz; 2023ma; 2023mb; 2023mc; 2023md; 2023me; 2023mf; 2023mg; 2023mh; 2023mi; 2023mj; 2023mk; 2023ml; 2023mm; 2023mn; 2023mo; 2"}
{"id": "4S8agvKjle", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2024) that could improve performances of agents. However, these frameworks often involve intricate design and not applicable to all LLMs. Therefore, we choose Act (Yao et al., 2023) as our framework, which requires minimal design and is applicable to most instruction-following LLMs.\\n\\nTable 9: Comparison of Act and ReAct framework using GPT-3.5-Turbo.\\n\\nOne popular alternative for our framework is to use ReAct (Yao et al., 2023) rather than Act, which uses interleaved thoughts in addition to actions to boost planning. However, our experiments on GPT-3.5-Turbo show inconsistent improvement of ReAct over performance of Act, as shown in Table 9. We hypothesized that this is due to we test long-term interactions of LLM Agents up to 30 interactions, and adding thoughts would pressure context length, thus leading to performance drop. Therefore, we use Act rather than ReAct in our benchmark.\\n\\nAnother major alternation is how to handle out of context length prompts during agent prompting. Here we provide ablation study on alternatives to our sliding window approach:\\n\\n- **Sliding Window** (Current Approach): We keep record of most recent interaction history within the prompt.\\n- **Cutoff**: This approach has been used in AgentBench (Liu et al., 2023a). It removes the sliding window and stops the interaction when the prompt (including history) overflows the maximum context length.\\n- **Summary**: This approach has been used in Xu et al. (2023a); Chase (2022). It uses the LLM to generate a summary of history to replace the interaction history when the prompt overflows maximum context length.\\n\\nWe benchmark on GPT-3.5-Turbo with only 4k context length and Mistral with 32k context length. The results are shown in Table 10. First, the memory component design barely affects models with long context length (Mistral), while greatly affects GPT-3.5-Turbo. The Summary method depends on the summarization ability of the LLM, therefore its performance varies between tasks. The cutoff method generally performs worse than sliding windows, though it is more stable than summary.\\n\\nThis shows that sliding window enables LLMs to make use of its limited context length. Its simple design also avoids introducing confounders into our benchmark, e.g. summarization abilities of LLM.\\n\\nTable 11 shows the criteria for sub-skill scoring and sub-skill scores for each task in AGENTBOARD.\\n\\nThe visualization panel supported by AGENTBOARD is shown in Figure 6. We provide a detailed explanation of panel features and usage tutorial in WandB blog.\\n\\nWe provide a case study in Figure 7 to show example usage of AGENTBOARD.\\n\\nI Details of Evaluated LLMs\\n\\n#### I.1 Evaluation Setup\\n\\nWe use greedy decoding strategy and set temperature to zero for better replicacy, and all LLMs are implemented with vLLM (Kwon et al., 2023) architecture, which has 10\u00d7 acceleration over huggingface inference. During prompting, we keep the most recent interaction histories within the maximum context length of the model. For models with different versions of checkpoints, we choose the version with best instruction following ability, with chat SFT and alignment. The following are the specific models we assess in the experiments.\"}"}
{"id": "4S8agvKjle", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: Comparison of various memory approaches to handle long-context agent prompting.\\n\\n|          | AlfWorld | ScienceWorld | BabyAI | Jericho | PDDL | WebShop | WebArena | Tool-Query | Tool-Operation |\\n|----------|----------|--------------|--------|---------|------|---------|----------|------------|----------------|\\n| **Memory** |          |              |        |         |      |         |          |            |                |\\n|          | 1. Could finish tasks within 2k tokens | 2. Could finish tasks within 4k tokens | 3. Otherwise | 1 | 2 | 1 | 3 | 2 | 3 |\\n| **Planning** |          |              |        |         |      |         |          |            |                |\\n|          | 1. $\\\\leq$ 3 subgoals on average | 2. $\\\\leq$ 5 subgoals on average | 3. Otherwise | 1 | 2 | 2 | 3 | 3 | 2 |\\n| **World Modeling** |          |              |        |         |      |         |          |            |                |\\n|          | 1. Requires no additional knowledge other than instruction | 2. Requires knowledge of the environment from exploration | 3. Requires commonsense knowledge in addition to knowledge from environment | 3 | 3 | 2 | 3 | 1 | 1 |\\n| **Self-Reflection** |          |              |        |         |      |         |          |            |                |\\n|          | 1. Detailed feedback and error message with instruction for the next step. | 2. Not very detailed feedback and error message | 3. No error message, e.g. \u201cno change in state\u201d | 3 | 2 | 2 | 1 | 3 | 2 |\\n| **Grounding** |          |              |        |         |      |         |          |            |                |\\n|          | 1. No specific action format is required, could recognize similar actions | 2. Action format is required | 3. Action format hard to follow | 2 | 3 | 2 | 1 | 3 | 3 |\\n| **Spatial Navigation** |          |              |        |         |      |         |          |            |                |\\n|          | 0. No spatial navigation | 1. 2D navigation | 1 | 1 | 0 | 0 | 1 | 0 | 0 |\\n\\nTable 11: The sub-skill scores associated with each task in AGENT BOARD.\\n\\nI.2 Details of Models\\n\\nWe list our evaluated models in Table 12.\\n\\nJ Data Quality Control\\n\\nTo ensure the quality of labeled sub-goals, we conducted three rounds of data verification for each labeled sub-goal. We developed an interactive interface through which inspectors complete tasks and observe the reward scores obtained at each step. If the inspector deems the reward score assigned during interaction with an environment to be unreasonable, additional annotators will engage in a discussion to determine if modifications to the labeled sub-goals are necessary.\"}"}
{"id": "4S8agvKjle", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Visualization Panel based on WandB, composed of a summary board with all metrics and a task board for each task.\\n\\n| Environment   | Annotation Errors |\\n|---------------|-------------------|\\n| AlfWorld      | 10.0%             |\\n| ScienceWorld  | 0%                |\\n| Babyai        | 4.2%              |\\n| Jericho       | 25%               |\\n| PDDL          | 5%                |\\n| WebShop       | 0%                |\\n| WebArena      | 0%                |\\n| Tool-Query    | 0%                |\\n| Tool-Operation| 0%                |\\n\\nTable 13: The proportion of environments with annotation errors in the second round of data checking. Environments identified with errors are subsequently analyzed to determine the underlying causes, and any environments exhibiting similar errors are amended collectively.\\n\\nThe first round of verification is a self-check. Each annotator is required to carefully review the labeled tasks in every environment they are responsible for. The second round involves a sampled inspection by two annotators for each task. They examine a sample of 5-10 items from different sub-tasks within the task and document\"}"}
{"id": "4S8agvKjle", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"GPT-4 outperforms all baselines by a large margin.\\n\\nGPT-4 demonstrates high capability score on all 6 dimensions.\\n\\nGPT-4 performance notably declines for challenging examples in Jericho.\\n\\nGPT-4 performs the worst on Jericho and WebArena.\\n\\nAlthough the performance metrics of GPT-4 are relatively low for task Jericho, it still outperforms other baselines.\\n\\nFigure 7: A case study for GPT-4 based on Panels from AGENT BOARD.\\n\\nTable 14: Statistics of 9 environments in AGENT BOARD. \\\"subgoal\\\" and \\\"match\\\" means 2 different implementations of progress rate $r_{subgoal}$ and $r_{match}$ respectively.\\n\\nNote that Sheet Environments in Tool-Operation are evaluated with $r_{match}$, while other sub-tasks are evaluated with $r_{subgoal}$.\\n\\n\u2021 For tasks without subgoal label, we state the average number of constraints to satisfy in the goal state, which is essentially the complexity of the problems.\\n\\nFor context length, we report the number of tokens generated with Llama2 tokenizer.\\n\\n\u2020 We divide problems into hard/easy based on the number of subgoals \u2013 problems with a larger number of subgoals than cutoff are viewed as hard.\\n\\nEmbodied AI\\n\\nGame\\n\\nWeb\\n\\nTool\\n\\nALF SW BA\\n\\nJC PL\\n\\nWS W A\\n\\nTQ TO\\n\\n# Environment  134  90  112  20  60  251  245  60  40\\n\\n# Turns  6  15  10  20  20  3  25  5  6\\n\\nAction Space  13  21  8  150  8  2  12  15  16\\n\\n# Avg. Subgoals  \u2021 3  5  4  6  6  4  6  5  5\\n\\nHard/Easy Cutoff  \u2020 3  3  3  4  6  1  4  4  4\\n\\nContext Length  900  2800  1800  1500  2700  1200  15000  2100  4300\\n\\nProgress Rate subgoal subgoal subgoal subgoal match match match subgoal subgoal/match\\n\\n\u2020 Success Rate (Progress Rate == 1)\"}"}
{"id": "4S8agvKjle", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Task Description: Go to the red ball\\n\\nInitial Observation: In front of you in this room, you can see several objects. The room has walls around you. You are facing a wall 1 step away. You are not carrying anything.\\n\\nFigure 8: An illustration of our sub-goal checking interface. We develop an interactive interface for annotators to checking sub-goals. Firstly, annotators play and pass the game with the interface. The reward score for each step will be given based on the labeled score. If the annotators are dissatisfied with the reward, annotators will record them and the corresponded environment will be discussed by more annotators and annotated again.\\n\\nK Details of Environments\\n\\nK.1 Details of Embodied Environments\\n\\nAlfWorld (ALF) (Shridhar et al., 2021) are Household tasks that require models to explore rooms and use commonsense reasoning to perform tasks, Within A BOARD, we evaluate a model's ability to perform tasks in physical household settings, such as \u201cput a pencil on the desk\u201d. AlfWorld is categorized into six types, comprising a total of 134 environments.\\n\\nScienceWorld (SW) (Wang et al., 2022) is a complex interactive text environment that poses a significant challenge to agents' scientific commonsense. This environment requires agents to navigate through 8 distinct functional rooms (e.g., workshop, kitchen) and utilize the tools to complete tasks such as \u201cmeasure the melting point of the orange juice\u201d. To address these issues, we re-annotate subgoals to calculate the rewards for these subgoals are uniform and distributed evenly throughout the task. To ensure that our annotated subgoals are necessary for achieving final goals, we restrict the use of tools and designated task completion rooms in the task descriptions. We show more details of we annotated subgoals in the Appendix L.3\\n\\nBabyAI (BA) (Chevalier-Boisvert et al., 2019) is an interactive environment where agents navigate and manipulate objects in a 20x20 grid space. The agent can only see objects within a limited sight and cannot perceive objects in remote rooms. The original implementation represents observations as images and only allows for tensor-based low-level actions such as \u201c0: move left\u201d, \u201c1: move right\u201d, and \u201c2: move forward\u201d. To enable text-based input and output for LLM agents, We modified it by mapping the original actions to a textual action space and providing textual descriptions of visual observations, as shown in Table 2. For each step, the environment returns a text description of the current observation, such as \u201cThere is a red ball 1 step to your right and 1 step ahead of you. There is a wall 2 steps ahead.\u201d We also introduced high-level actions, such as \u201cgo to red ball 1\u201d and \u201ctoggle and go through green locked door 1\u201d, to expand the action space and enrich the\"}"}
{"id": "4S8agvKjle", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"semantic complexity of the environment. Additionally, we implemented a new subgoal-based progress rate for the environments to increase the density of rewards compared to the original reward scores. Unlike the previous reward score in BabyAI which awards a point only after a new object is found or pickup, requiring many steps to see progress in reward score, our new approach increases density of the rewards, requiring fewer steps to achieve them. We re-annotated subgoals and calculate with the equation of \\\\( r_{\\\\text{subgoal}}_t \\\\). Subgoals are re-annotated to update the progress rate whenever the agent makes progress, such as navigating to another room, finding a red ball, and picking it up in the problem \u201cpickup a red ball\u201d.\\n\\nK.2 Details of Game Environments\\n\\nEvaluating LLM agents as strategic game playing agents demands strong planning ability of agents. We choose three tasks that are all demanding in planning and making strategies.\\n\\nJericho (JC) (Hausknecht et al., 2020) is a collection of text-based game environments that evaluate agents to perform adventures in fictional worlds. This task is unique in that it requires strong world modeling ability as agents could only gain information about the magic world through exploration and interaction. For example, for the task that requires the agent to perform actions with magic, it cannot reason with pre-trained commonsense knowledge and must perform exploration to understand the rules of the magic world. The original games are quite long (need 50-300 steps to finish), which is not suitable for LLM agents with fixed context length. To solve this issue, we rewrite the goal of each adventure to restrict the games to be finished within 15 subgoals. For example, \\\\( zork1 \\\\) game requires the player to enter a dungeon and explore the dungeon to find a bar. We rewrite the goal as \u201cYou need to find your way into a secret passage where the entrance is in the living room of the house.\u201d and the agent only needs to find the entrance to the dungeon, which can be finished in 8 steps. We use the \\\\( r_{\\\\text{subgoal}} \\\\) as progress rate metrics, and we meticulously annotate the subgoals for each problem. Each subgoal characterize that the agent has solved a small problem, e.g. \u201cfind the entrance to the house\u201d \u2192 \u201center the house\u201d \u2192 \u201cfind the living room\u201d \u2192 \u201cdiscover a trap door\u201d \u2192 \u201cfind the entrance to dungeon\u201d.\\n\\nPDDL (PL) (Vallati et al., 2015), short for Planning Domain Definition Language, is a set of strategic games defined with PDDL symbolic language. We selected 4 representative game domains, Gripper, Barman, Blocksworld, Tyreworld to benchmark LLM agents in diverse scenarios, where the agent needs to move balls across rooms, make cocktails, rearrange blocks and pump up and install new tyres to cars. This task is difficult as it requires multiple rounds of planned actions to finish a single subgoal and agents need to plan strategically to avoid repetitive steps. For example, in Barman, the player is given a menu, and is required to make a few cocktails with a few containers and ingredients. The agent could use a strategy of trying to use different containers each time to avoid repetitive cleaning and save steps. While the commonly-used environment implementation (Silver and Chitnis, 2020) requires the agent to interact with an environment with PDDL expressions, e.g. \\\\( \\\\text{clean-shaker}(\\\\text{hand1}, \\\\text{hand2}, \\\\text{shaker}) \\\\) and provides observations as set of predicates \\\\( \\\\text{ontable}(\\\\text{shaker1}) \\\\land \\\\text{empty}(\\\\text{shaker1}) \\\\). we write parser rules to offer a text-based observation to agents that allows LLMs to interact with natural language to be consistent with other tasks. e.g. \u201cShaker1 is on the table. Shaker1 is empty\u201d and enable the agents to interact with the environment with simple text commands, e.g. \u201cclean-shaker shaker1 with hand1 while hand2 is empty.\u201d We curate 10-20 problems for each of the four domains by ourselves, ensuring the problems are multi-round and diverse. We use the \\\\( r_{\\\\text{match}}_t \\\\) as progress rate metric, where the matching score compares the similarity between the properties of current state and the goal state. e.g. for the goal state \u201cBlock a is on block b. Block b is on the table\u201d, if at current state \u201cBlock a is on the table. Block b is on the table\u201d, then the matching score is 0.5. The agent will receive a 100% progress rate only if all conditions of the goal state are satisfied.\\n\\nK.3 Details of Web-based Environments\\n\\nEvaluating LLM\u2019s capability as a generalist agent in web-based scenarios has become pivotal (Shi et al., 2017a; Deng et al., 2023). Web agent is expected to navigate the network efficiently and perform diverse tasks amidst highly dynamic, intricate, and multi-turn interactions. Based on the task categorization, we\u2019ve pinpointed two tasks of high recognition and quality: the specific network task, WebShop (Yao et al., 2022), and the general network task, WebArena (Zhou et al., 2023). The latter permits unrestricted access to any supported webpage.\\n\\nWebShop (WS) (Yao et al., 2022) is a network-based simulation environment for e-commerce experiences, featuring a website with 1.18 million actual products, each with distinct labels and attributes. In this environment, the agent is allowed to interact with the system through \u2018search[QUERY]\u2019 or \u2018click[ELEMENT]\u2019 actions to purchase products matching the instructions. This process necessitates that the model possesses reasoning and grounding abilities. Based on the original implementation method (Yao et al., 2022; Shinn et al., 2023), we have improved the error feedback, including refining the observation for exceeding page limits and interacting with wrong objects. These enhancements contribute to the effective operation of the entire environment and the rationality of multi-step reasoning processes. As there are no sub-goals in the environment, to obtain a...\"}"}
{"id": "4S8agvKjle", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"continuous progress rate, we expanded the calculation rules from (Yao et al., 2022), calculating the score at different web pages (stages). To measure the distance of the current state to the final goal as the progress rate, we expanded the product scoring rules from Yao et al. (2022) to derive the score at different web pages. Please refer to Appendix L.7 for details.\\n\\nWebArena (WA) (Zhou et al., 2023) is a real web environment containing four applications: online shopping, discussion forums, collaborative development, and business content management. It supports 11 different web browsing actions, such as click (element), new tab, goto (URL), etc., and offers additional tools like maps and wikis. The observation space consists of structured web content (the accessibility tree). Completing tasks in this highly realistic environment requires the agent to possess strong memory, high-level planning, common sense, and reasoning abilities. Compared to other datasets (Deng et al., 2023; Shi et al., 2017b), WebArena offers multi-round and continuous web browsing interaction simulation. We filtered 245 instances from the original dataset for two main sub-tasks: Site Navigation and Contact & Config, each annotated with the target URLs or required content. To obtain the progress rate, we revised the existing method for calculating the final score (Zhou et al., 2023) and continuously computed the progress rate at each step, fusing the URL matching score with the content matching score, derived from the current URL and target URL, with the content matching score calculated based on the detected required content, as detailed in Appendix L.8.\\n\\nK.4 Details of Tool Environments\\n\\nIn AGENT BOARD, a tool contains a variety of functions, accessed by agents via function calling. These functions are the actions that LLM agents can take in tool environments. Drawing upon open datasets and APIs, we have developed a suite of five distinct tools, each encapsulated in its own environment. Tool Environments are categorized into two groups: Tool-Query Environments and Tool-Operation Environments, representing two general usage scenarios. Tool-Query Environments include Weather Environment, Movie Environment and Academia Environment. Tool-Operation Environments include Todo Environment and Sheet Environment.\\n\\nK.4.1 Tool-Query Environments\\n\\nWeather Environment\\nWeather Environment enables LLM agents to use the weather tool to retrieve past, present and future weather data, encompassing temperature, precipitation and air quality across various locales. We use Python codes to integrate Open-Meteo API, implement the requisite functions and subsequently develop a weather tool.\\n\\nMovie Environments\\nMovie Environment grants LLM agents to use the movie tool to access cinematic data, encompassing film details, personnel and production companies. We incorporate the API and data from The Movie Database, implement the necessary functions, and thus establish the movie tool.\\n\\nAcademia Environment\\nAcademia Environment equips LLM agents the academia tool to query information related to computer science research, including academic papers and author information. In its development, we harness data from the Citation Network Dataset, craft the relevant functions, and subsequently construct the academia tool.\\n\\nK.4.2 Tool-Operation Environments\\n\\nTodo Environment\\nTodo Environment facilitates LLM agents in querying and amending personal agenda data through the todo tool. We implement the todo tool based on the Todoist API.\\n\\nSheet Environment\\nSheet Environment allows LLM agents to use the sheet tool to access and modify spreadsheet data. We build our sheet tool upon the Google Sheets API.\"}"}
{"id": "4S8agvKjle", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Details of Progress Rate Metrics\\n\\nL.1 Explanation and Adaptations for \\\"Unique\\\" Subgoal Sequence\\n\\nWe manually edit problems for a simpler progress rate calculation setup where each final goal aligns with a unique subgoal sequence. Here we emphasize two attributes of our adaptation.\\n\\nAgentboard allows for multi-trajectory inference paths:\\n\\nNote that a single sequence of subgoals is not equivalent to a single inference path trajectory \u2013 in fact, our progress rate can be applied to tasks with multiple inference paths. We only restrict examples that have multiple different sets of subgoals to fulfill the same final goal. However, a single set of subgoals allows for very diverse inference paths. For example, in BabyAI, a problem is \\\"go to a red ball,\\\" where the agent needs to find a red ball behind it and then go to the red ball. It could either finish the task with the golden path \\\"turn right -> go to red ball 1\\\" or take detours while eventually finishing the task, such as \\\"move forward -> go to grey box 1 -> move forward -> turn right -> go to red ball 1\\\". Both trajectories fulfill subgoals \\\"find a red ball\\\" and \\\"go to the red ball\\\".\\n\\nOur adaptations for single-set subgoals only affect less than 5% of all problems:\\n\\nWe only limit situations where a model has many diverse sets of subgoals, such as in ScienceWorld, where an agent could use either a stove or a blast furnace to vaporize liquid. We simplify it by specifying a single method and clarifying the goal \\\"vaporize liquid\\\" as \\\"vaporize liquid with a stove.\\\" This kind of adaptation only applies to tasks in BabyAI and ScienceWorld, constituting less than 5% of all problems in AgentBoard. Most of the existing samples already satisfy this requirement, and our modifications do not change the task difficulty; they are solely for the purpose of annotation convenience. A full list of adaptations and their examples is detailed in Table 15.\\n\\n| Task          | Number of examples adapted for a single set of subgoals |\\n|---------------|--------------------------------------------------------|\\n| AlfWorld      | None                                                   |\\n| ScienceWorld  | 36 examples (40%)                                      |\\n| BabyAI        | 4 example (3%)                                          |\\n| Jericho       | None                                                   |\\n| PDDL          | None                                                   |\\n| WebShop       | None                                                   |\\n| WebArena      | None                                                   |\\n| Tool-Query    | None                                                   |\\n| Tool-Operation| None                                                   |\\n\\nTable 15: The proportion of environments that require adaptations for the simple setup of a single set of subgoals.\\n\\nL.2 Alfworld\\n\\nWe identify and annotate the necessary subgoals using regular expressions. For instance, for the task \\\"put a pencil on the desk\\\", we annotate one necessary observation as \\\"You pick up the pencil +. \\\". This expression would match observations like \\\"You pick up the pencil 1\\\". When the goal of an environment is achieved, the environment emits a task success flag. Specifically, for each environment, we labeled N-1 necessary subgoals as N-1 subgoals. The final success flag combined with the N-1 annotated subgoals constitutes the set of N subgoals.\\n\\nL.3 ScienceWorld\\n\\nWe compare our modified task descriptions and subgoals with the original ones in Table 16. In the original scheme, subgoals are categorized as \\\"sequential subgoals\\\" and \\\"unordered and optional subgoals\\\". For the former, achieving sequential subgoals alone is sufficient to receive full rewards (100 points). However, under the \\\"unordered and optional subgoals\\\", each completed task is only awarded low point (e.g. 1 point). These tasks are also important and necessary for accomplishing the given task. For instance, the \\\"optional subgoals\\\" outlined in Table 16, such as \\\"be in the same location as the orange juice\\\" and \\\"have the substance alone in a single container\\\" are necessary for the task and can help to evaluate a model's navigation and common sense abilities. It is inappropriate to assign such tasks a low score. Furthermore, the uneven distribution of \\\"Sequential Subgoals\\\" throughout the entire task process can lead to a disproportionately low score, which does not accurately reflect the model's progress. For example, if the model fails to complete the initial subgoals within the \\\"Sequential Subgoals\\\" category, which could be considerably distant from the start state, it can only achieve a very low score. This scoring method does not align with our motivation, which is to ensure that the progress rate adequately reflects the model's performance. Therefore, we have re-annotated the subgoals. Specifically, we label necessary observations as part of the subgoals.\"}"}
{"id": "4S8agvKjle", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Your task is to freeze orange juice. First, focus on the substance. Then, take actions that will cause it to change its state of matter.\\n\\nYour task is to freeze orange juice in the kitchen. The objects you can use are a metal pot, a freezer, a thermometer, and a fridge.\\n\\nTake actions that will cause it to change its state of matter to a solid state. Finally, examine its altered state. You should wait and monitor the temperature of the water until it changes its state.\\n\\nSubgoals Sequential Subgoals:\\n1. focus on substance\\n2. substance is in a liquid state\\n3. substance is in a solid state\\n\\nUnordered and Optional Subgoals:\\n1. be in same location as orange juice\\n2. have substance alone in a single container\\n3. have object in cooler (fridge)\\n4. have object in cooler (freezer)\\n5. cool object by at least 5\u00b0C\\n\\nNecessary Observations:\\n1. You move to the kitchen.\\n2. The freezer is now open.\\n3. The fridge is now open.\\n4. The thermometer measures a temperature of (-?0-9?-?1-9?2) degrees celsius.\\n5. Solid orange juice\\n\\nTable 16: Comparison between the original task description and subgoals of ScienceWorld and our labeled subgoals (Best viewed in color).\\n\\ndifferent progress levels across various gold paths. This disparity makes it challenging to assign a definitive progress rate to any given state. Therefore, in our task descriptions, we have restricted the locations and tools used for tasks to ensure the uniqueness of our goal paths and the necessity of observations. For the necessary observations, our initial observation is more close to the initial state and but still challenging.\\n\\nwe design an interactive UI framework (Figure 8). We ask one graduate student to interact with the environment and record the necessary observations to achieve the given goal. As a result, we revise the task descriptions to include sufficient information for achieving the subgoals and to ensure the gold path is unique.\\n\\nL.4 BabyAI\\n\\nThe original implementation of babyai provides a reward score. Different from the original reward, our progress rate is more dense and the agent does not need to accomplish many steps before getting a increase in score. Here we compare the difference between our progress rate and the original reward score, as shown in Table 17. We can see from this case that our progress rate better measures intermediate progress for agents.\\n\\nThe progress rate is labelled via an interactive UI framework (Figure 8). A graduate student interact with the environments and record the observations corresponding to subgoals needed to finish the problem.\\n\\nProblem: Unlock to Unlock Steps with Score Increase\\n\\n(Original) Steps with Score Increase (Ours)\\n\\n| Room 1 | Room 2 | Room 3 |\\n|--------|--------|--------|\\n| 1. Pickup purple ball | 1. Pickup blue key; 2. Enter room 3; 3. Pickup grey key; 4. Enter room 2; 5. Enter room 1; 6. Pickup purple ball |\\n\\nTable 17: Comparison between our progress rate for BabyAI and original reward score.\\n\\nL.5 Jericho\\n\\nThe original Jericho games are free-exploration text-based games, where the player is not given a tangent goal but allowed to explore around the environment as adventureres. For uniformity with other tasks, we first write a new goal for each problem, and we carefully select the goal so that the game could be accomplished within 15 subgoals. In contrast, the original environments requires around 50-300 interactions to get the maximum rewards.\\n\\nThe annotation of goal and subgoals are also performed by a graduate student in the interactive UI framework.\\n\\nL.6 PDDL\\n\\nIn the PDDL environment, each state is described by a conjunction of properties p_1 \u2227 p_2, . . ., \u2227 p_m, each property is a simple predicate describing the property of an object, e.g. \u201cBlock a is on the table\u201d. Given the goal state\"}"}
{"id": "4S8agvKjle", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The matching score formula is defined as:\\n\\n\\\\[\\n|G \\\\cap P| / |G| = f = \\\\text{TextMatch}(\\\\bar{y}, \\\\bar{y}^*)\\n\\\\]\\n\\nwhere \\\\(G\\\\) and \\\\(P\\\\) are sets of goal states and property states, respectively.\\n\\nThe matching score is 1 if and only if the properties of goal states are satisfied in the current state.\\n\\nIn the webshop environment, we expanded the product scoring rules from (Yao et al., 2022) to derive the score at different web pages. We can calculate the score of any product (the distance from the target product) using the original scoring formula as follows:\\n\\n\\\\[\\nf = f_{\\\\text{type}} \\\\cdot |U_{\\\\text{att}} \\\\cap Y_{\\\\text{att}}| + |U_{\\\\text{opt}} \\\\cap Y_{\\\\text{opt}}| + 1 \\\\quad \\\\text{if } y_{\\\\text{price}} \\\\leq u_{\\\\text{price}}\\n\\\\]\\n\\nEach natural language instruction, denoted as \\\\(u \\\\in U\\\\), encompasses a non-empty set of attributes, \\\\(U_{\\\\text{att}}\\\\), a set of options, \\\\(U_{\\\\text{opt}}\\\\), and a specified price, \\\\(u_{\\\\text{price}}\\\\). Meanwhile, \\\\(Y\\\\) represents the product chosen by the agent. The function \\\\(f_{\\\\text{type}} = \\\\text{TextMatch}(\\\\bar{y}, \\\\bar{y}^*)\\\\) is based on text matching heuristics to assign a low reward when \\\\(y\\\\) and \\\\(y^*\\\\) have similar attributes and options but are obviously different types of products.\\n\\nTypically, completing a web shopping task involves three continuous stages: search, product selection, and finalizing the product style before placing an order. Therefore, to measure the distance between the current state and the target state, we primarily calculate scores for three pages (states): search result page, product description page, and order confirmation page. On the search result page, we calculate the score of each product on the page and take the highest score as the score for this page. On the product description page, we compute the highest score for the product under various options as the page score. On the order confirmation page, the score of the finally selected product is considered as the score for that page. In our method, the progress rate is the average of the scores from these three pages.\\n\\nIn our method, we effectively utilize the annotation data, treating URLs as indicators of the web browsing trajectory and required contents as integral scoring points. The progress rate is formulated as follows:\\n\\n\\\\[\\nr_{\\\\text{match}} = n \\\\cdot \\\\frac{r_d(r_q + r_p)}{m} + n \\\\cdot \\\\frac{mr_c(n = 3; m = 0, 1, 2, \\\\ldots)}{m + n}\\n\\\\]\\n\\nInitially, we dissect the URL into its constituent elements: domain, query, and parameters by using \\\\(\\\\text{util.parse}\\\\). For domain verification, a binary value, \\\\(r_d\\\\), is assigned, with a score of 1 indicating a correct domain match, and 0 otherwise. Subsequently, the matching score for the query, \\\\(r_q\\\\), is determined through the application of the Longest Common Subsequence (LCS) algorithm, which assesses the similarity between the current and target queries based on their sequential nature. In contrast, the alignment between the current and target parameters is evaluated using the F1 score, denoted as \\\\(r_p\\\\), which is particularly suited for unordered sets.\\n\\nIn parallel, the content matching score, \\\\(r_c\\\\), emerges from the analysis of required content presence at each stage, calculated as the ratio of detected essential contents to the total required contents. The overall progress rate integrates these two aspects, calculated as a weighted sum of the URL matching scores (incorporating domain, query, and parameter scores) and the content matching score. Here, \\\\(n\\\\) represents the number of target URL components, and \\\\(m\\\\) denotes the count of target required contents.\\n\\nIn Tool-Query Environments, we employ \\\\(r_{\\\\text{subgoal}}\\\\) as a metric to measure progress rate. Therefore, it is necessary to annotate subgoals for these environments. In Figure 9, we present an illustration of the process of subgoal annotation for Academia Environment. Specifically, when designing actions for these environments, we ensure that each action's functionality is indecomposable (i.e., the functionality and outcome of one action cannot be achieved through other actions). This design choice results in a deterministic set of required golden actions to achieve our annotated goal. Furthermore, we ask human annotators to identify golden actions for each goal. Every output returned by executing golden actions is then processed as a subgoal.\\n\\nIn Tool-Operation Environments, we adopt \\\\(r_{\\\\text{subgoal}}\\\\) as progress rate metric. Subgoals are annotated following the same process as Tool-Query Environments. In Sheet Environments, progress rate is assessed with \\\\(r_{\\\\text{match}}\\\\).\"}"}
{"id": "4S8agvKjle", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The paper \\\"Learning the Principle of Least Action with Reinforcement Learning\\\" was published in 2021.\\n\\n**Step 2:**\\n\\nSubgoal Collection\\n\\nSubgoal 1: PaperNet is loaded.\\n\\nSubgoal 2: \\n\\n- **year**: 2021\\n- **venue**: AAAI Spring Symposium - MLPS\\n- **n citation**: 0\\n- **keywords**: []\\n- **doc type**: Conference\\n\\nSubgoal 3: 2021\\n\\n---\\n\\n**Table 18: Inference Time Estimation**\\n\\n| Model Device/API | Inference Architecture | Inference Speed | Total-time |\\n|------------------|------------------------|-----------------|------------|\\n| GPT-4 azure API  | -                      | 1.5s/round      | 5.5h       |\\n| GPT-3.5-Turbo azure API | -                  | 1s/round        | 3h         |\\n| DeepSpeed-67b    | 8*V100 vLLm            | 5s/round        | 18.5h      |\\n| Llama2-70b       | 8*V100 vLLm            | 8s/round        | 28h        |\\n| Llama2-70b       | 4*A100 vLLm            | 4s/round        | 13.5h      |\\n\\n---\\n\\n**Figure 9: An illustration of the process of subgoal annotation for Academia Environment.**\\n\\n**Model Device/API Inference Architecture Inference Speed Total-time**\\n\\n| Model Device/API | Inference Architecture | Inference Speed | Total-time |\\n|------------------|------------------------|-----------------|------------|\\n| GPT-4 azure API  | -                      | 1.5s/round      | 5.5h       |\\n| GPT-3.5-Turbo azure API | -                  | 1s/round        | 3h         |\\n| DeepSpeed-67b    | 8*V100 vLLm            | 5s/round        | 18.5h      |\\n| Llama2-70b       | 8*V100 vLLm            | 8s/round        | 28h        |\\n| Llama2-70b       | 4*A100 vLLm            | 4s/round        | 13.5h      |\\n\\n---\\n\\n**M Runtime Estimation**\\n\\nThe evaluation runtime for a language model depends on the device/API, model, and inference architecture used. In the case of open-source LLMs, the vllm inference speed is approximately 10 times faster than the huggingface pipeline. We show some time cost in Table 18.\\n\\n---\\n\\n**N Prompt Details**\\n\\nAs shown in Figure 10, we use a unified prompt template for different tasks in A GENT BOARD. Basically, a prompt consists of 5 parts. {System Prompt} represents the system prompt for the LLM, such as \\\"You are a helpful AI agent\\\". {Instruction} mainly consists of task descriptions and action definitions. {Examples} represents in-context learning examples. {Goal} is the current goal that needs to be accomplished, and {Trajectory} is the interaction history between the LLM agent and the environment.\\n\\nFor different tasks, the contents of these five parts are different. Prompt details for Embodied AI tasks are shown in Figure 11, 12 and 13. Prompt details for Game tasks are shown in Figure 14 and 15. Prompt details for Web tasks are shown in Figure 16 and 17. Prompt details for Tool tasks are shown in Figure 18 and 19, respectively.\"}"}
{"id": "4S8agvKjle", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: The unified prompt template in A\\n\\nGENT\\n\\nB\\n\\nG\\n\\nOARD\\n\\n{text} in blue font represents placeholders, which\\nvaries according to different tasks.\\n\\nPrompt Details for AlfWorld\\n\\nSystem Prompt\\n\\nYou are a helpful assistant. Generate your next step of action after Action. Action must not be empty.\\n\\ne.g. Action: put down cup.\\n\\nInstruction\\n\\nYour task is to interact with a virtual household simulator to accomplish a specific task. With each\\ninteraction, you will receive an observation. Your role is to decide on an action based on the observation.\\n\\nPlease ensure that any objects ('obj') and receptacles ('recep') you mention in your response are present\\nin the observation provided.\\n\\nHere are the available actions you can take:\\n\\n- take obj from recep\\n- put obj in/on recep\\n- open recep\\n- close recep\\n- toggle obj/recep\\n- clean obj using recep\\n- cool obj using recep\\n- heat obj using recep\\n- inventory\\n- examine recep/obj\\n- go to recep\\n\\nExamples\\n\\nYour task is to: look at statue under the desklamp.\\n\\nYou are in the middle of a room. Looking quickly around you, you see a coffeetable 1, a diningtable 1,\\na drawer 4, a drawer 3, a drawer 2, a drawer 1, a dresser 1, a garbagecan 1, a sidetable 2, a sidetable 1,\\nand a sofa 1.\\n\\nAction: go to dresser 1\\n\\nObservation: On the dresser 1, you see a cellphone 3, a newspaper 2, a statue 1, and a television 1.\\n\\nAction: take statue 1 from dresser 1\\n\\nObservation: You pick up the statue 1 from the dresser 1.\\n\\nAction: go to sidetable 1\\n\\nObservation: On the sidetable 1, you see nothing.\\n\\nAction: go to sidetable 2\\n\\nObservation: On the sidetable 2, you see a desklamp 3, a newspaper 1, and a statue 2.\\n\\nAction: use desklamp 3\\n\\nObservation: You turn on the desklamp 3.\\n\\nFigure 11: Prompt details for AlfWorld. The provided example is changed based on the type of the specific\\nenvironment instance.\\n\\nPrompt Details for ScienceWorld\\n\\nSystem Prompt\\n\\nYou are a helpful agent that interacts with the virtual science school environment to solve the given task.\\n\\nGenerate your next step of action after Action. Action must not be empty. e.g. Action: put down cup.\\n\\nInstruction\\n\\nYou are an agent in a virtual science school environment, tasked to interact with various elements. Here\\nare the commands you can use:\\n\\n- Manipulation:\\n  - open OBJ / close OBJ: Interact with a container.\\n  - pick up OBJ: Add an object to your inventory.\\n  - put down OBJ: Remove an object from your inventory.\\n\\n27\"}"}
{"id": "4S8agvKjle", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Task Description: Your task is to boil water. For compounds without a boiling point, combusting the substance is also acceptable. First, focus on the substance. Then, take actions that will cause it to change its state of matter.\\n\\nACTION: look around\\n\\nOBSERVATION: This room is called the hallway. In it, you see:\\n- a picture\\n- a substance called air\\n- the agent\\n\\nYou also see:\\n- A door to the green house (that is open)\\n- A door to the living room (that is open)\\n- A door to the art studio (that is open)\\n- A door to the kitchen (that is open)\\n- A door to the bedroom (that is open)\\n- A door to the workshop (that is open)\\n\\nACTION: open door to kitchen\\n\\nOBSERVATION: The door is already open.\\n\\nACTION: go to kitchen\\n\\nOBSERVATION: You move to the kitchen.\\n\\n...\"}"}
{"id": "4S8agvKjle", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prompt Details for BabyAI\\n\\nSystem Prompt\\n\\nYou are an exploration master that wants to finish every goal you are given. Generate your next step of action after Action. Action must not be empty. e.g. Action: put down cup.\\n\\nInstruction\\n\\nYou are placed in a room and you need to accomplish the given goal with actions. You can use the following actions:\\n\\n- turn right\\n- turn left\\n- move forward\\n- go to <obj> <id>\\n- pick up <obj> <id>\\n- go through <door> <id>:\\n  - <door> must be an open door.\\n- toggle and go through <door> <id>:\\n  - <door> can be a closed door or a locked door. If you want to open a locked door, you need to carry a key that is of the same color as the locked door.\\n- toggle: there is a closed or locked door right in front of you and you can toggle it.\\n\\nExamples\\n\\nObservation: In front of you in this room, you can see several objects: The room has walls around you. You are facing a wall 1 steps away. You are not carrying anything.\\nAction: move forward\\nObservation: There is a barrier in front of you, you can't move forward.\\nAction: turn right\\nObservation: In front of you in this room, you can see several objects: The room has walls around you. You are facing a wall 2 steps away. You are not carrying anything.\\nAction: move forward\\nObservation: In front of you in this room, you can see several objects: The room has walls around you. You are facing a wall 1 steps away. You are not carrying anything.\\nAction: turn left\\nObservation: In front of you in this room, you can see several objects: The room has walls around you. You are facing a wall 1 steps away. You are not carrying anything.\\nAction: move forward\\nObservation: There is a barrier in front of you, you can't move forward.\\nAction: turn left\\nObservation: In front of you in this room, you can see several objects: There is a grey box 1 3 steps in front of you and 1 steps to your left. There is a grey ball 1 2 steps in front of you and 2 steps to your left. There is a red ball 1 right in front of you 5 steps away. There is a grey key 1 4 steps in front of you and 1 steps to your left. The room has walls around you. You are facing a wall 6 steps away. You are not carrying anything.\\nAction: move forward\\nObservation: In front of you in this room, you can see several objects: There is a grey box 1 2 steps in front of you and 1 steps to your left. There is a grey ball 1 1 steps in front of you and 2 steps to your left. There is a red ball 1 right in front of you 4 steps away. There is a grey key 1 3 steps in front of you and 1 steps to your left. The room has walls around you. You are facing a wall 5 steps away. You are not carrying anything.\\n...\\n\\nFigure 13: Prompt details for BabyAI.\\n\\nPrompt Details for Jericho\\n\\nSystem Prompt\\n\\nYou are a game master in fictional text games. Generate your next step of action after Action. Action must not be empty. e.g. Action: put down cup.\\n\\nInstruction\\n\\nYou are in a fictional game environment and you need to accomplish goals by performing actions. Each action is a simple phrase. Here are the actions you can do:\\n\\n- Inventory: check things you are carrying\\n- Look: check your surroundings\\n\\n...\"}"}
{"id": "4S8agvKjle", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Examine place/object: check the details of something\\nTake object: pickup object\\nPut down object: leave a object at your current place.\\nDrop object\\nCheck valid actions: Check actions you can use\\nSouth: go south\\nNorth: go north\\nEast: go east\\nWest: go west\\nUp: go up\\nDown: go down\\n\\nOther available actions could be determined through check valid actions.\\n\\nExamples\\n\\nGoal: You are the warrior Link that needs to save the princess from the castle.\\nObservation: You are at the path leading to the castle. The castle is to your north. There is a barrel in\\nfront of you.\\nAction: Examine barrel\\nObservation: There is a gleaming elvish sword.\\nAction: take the sword\\nObservation: The sword is taken\\nAction: north\\nObservation: Main hall. The castle is dimly lit, with lava flowing in many places. There is a staircase\\nleading up to princess bedroom, a door to your north leading to the kitchen hall, and the door you just\\nentered on your south\\nAction: check valid actions\\nObservation: south, north, up, look, examine staircase\\nAction: up\\nObservation: Princess's bedroom. Princess is sleeping in bed.\\nAction: check valid actions\\nObservation: wake up the princess, take out sword, down\\nAction: wake up the princess\\nObservation: The princess wake up from the coma. Thank you my knight, she says. The task is finished.\\n\\nFigure 14: Prompt details for Jericho.\\n\\nPrompt Details for PDDL\\n\\nSystem Prompt\\nYou are a master in planning. Generate your next step of action after Action. Action must not be empty.\\ne.g. Action: put down cup.\\n\\nInstruction\\nThe robot has four actions: pickup, putdown, stack, and unstack. The domain assumes a world where\\nthere are a set of blocks that can be stacked on top of each other, an arm that can hold one block at a\\ntime, and a table where blocks can be placed.\\nThe actions defined in this domain include:\\n\\npickup <block>:\\nallows the arm to pick up a block from the table if it is clear and the arm is empty. After the\\npickup action, the arm will be holding the block, and the block will no longer be on the table\\nor clear.\\n\\nputdown <block>:\\nallows the arm to put down a block on the table if it is holding a block. After the\\nputdown action, the arm will be empty, and the block will be on the table and clear.\\n\\nstack <block> <block>:\\nallows the arm to stack a block on top of another block if the arm is holding\\nthe top block and the bottom block is clear. After the stack action, the arm will be empty, the top block\\nwill be on top of the bottom block, and the bottom block will no longer be clear.\\n\\nunstack <block> <block>:\\nallows the arm to unstack a block from on top of another block if the arm\\nis empty and the top block is clear. After the unstack action, the arm will be holding the top block, the\\ntop block will no longer be on top of the bottom block, and the bottom block will be clear.\"}"}
{"id": "4S8agvKjle", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the table. b2 is on the table. B3 is on the table. Robot arm is empty. The b1 is clear. The b2 is clear. The b3 is clear.\\n\\nAction: pickup b2\\nObservation: b1 is on the table. B2 is on the table. The b1 is clear. The b3 is clear. You are holding b2.\\n\\nAction: stack b2 b3\\nObservation: b1 is on the table. b1 is on b2. B3 is on the table. Robot arm is empty. The b2 is clear. The b1 is clear.\\n\\nAction: pickup b2\\nObservation: The action is not valid and therefore takes no effect. Please remember to satisfy the restriction of actions. You can also check valid actions.\\n\\nAction: check valid actions\\nObservation: valid actions are: pickup b2, unstack b1 b2.\\n\\nAction: pickup b1\\nObservation: b2 is on b3. B3 is on the table. Robot arm is empty. The b2 is clear. You are holding b1.\\n\\nAction: stack b1 b2\\nObservation: b1 is on b2. b2 is on b3. B3 is on the table. Robot arm is empty. The b1 is clear. The goal is satisfied.\\n\\nFigure 15: Prompt details for PDDL. The provided instruction/example are changed based on the type of the specific environment instance.\\n\\nPrompt Details for WebShop\\nSystem Prompt\\nYou are a helpful virtual webshop assistant that interacts with the simulated website to solve a task.\\n\\nInstruction\\nYou are now the virtual webshop assistant, navigating a website to locate and purchase items based on given commands. Our interaction will follow this structure:\\n\\nYour Actions: You will preface each of your actions with \\\"Action: \\\".\\n\\nWebsite's Response: The website will provide feedback starting with \\\"Observation: \\\".\\n\\n[click]something: Engage with specific buttons or links.\\n[search]something: Seek specific data on the website. Use this only if a [Search] button appears in the observation.\\n\\nNote: If you wish to search and there's no [Search] button, click the [Back to Search] button instead.\\n\\nObservation Format: The website will showcase its content. Elements within square brackets (like [Buy Now]) indicate clickable buttons or links.\\n\\nExamples\\nYou should perform actions to accomplish the goal: I would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars\\n\\nAction: reset[\\nObservation: WEB PAGE: {Let us begin shopping\\n[Search]}\\n\\nAction: search[3 ounce bright citrus deodorant sensitive skin]\\nObservation: WEB PAGE: {\\n[Back to Search] Page 1 (Total results: 15) [Next >]\\n[B078GWRC1J] Bright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce $10.99}\\n\\nAction: click[B078GWRC1J]\\nObservation: WEB PAGE: {31\"}"}
{"id": "4S8agvKjle", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 16: Prompt details for WebShop.\\n\\nPrompt Details for WebArena\\n\\nSystem Prompt\\n\\nYou are an autonomous intelligent agent tasked with navigating a web browser. You will be given web-based tasks. These tasks will be accomplished through the use of specific actions you can issue.\\n\\nInstruction\\n\\nHere's the information you'll have:\\n\\nThe user's objective: This is the task you're trying to complete.\\n\\nThe current web page's accessibility tree: This is a simplified representation of the windowed webpage, providing key information.\\n\\nThe current web page's URL: This is the page you're currently navigating.\\n\\nThe open tabs: These are the tabs you have open.\\n\\nThe useful websites and corresponding URL you can navigate:\\n\\n'reddit': \\\"http://reddit.com\\\"\\n\\n'online shop': \\\"http://onestopmarket.com\\\"\\n\\n'e-commerce platform': \\\"http://luma.com/admin\\\"\\n\\n'gitlab': \\\"http://gitlab.com\\\"\\n\\n'wikipedia': \\\"http://wikipedia.org\\\"\\n\\n'map': \\\"http://openstreetmap.org\\\"\\n\\nThe actions you can perform fall into several categories:\\n\\nPage Operation Actions:\\n\\n'click [id]': This action clicks on an element with a specific id on the webpage.\\n\\n'type [id] [content] [press_enter_after = 0 |1]': Use this to type the content into the field with id. By default, the \\\"Enter\\\" key is pressed after typing unless press_enter_after is set to 0.\\n\\n'hover [id]': Hover over an element with id.\\n\\n'press [key_comb]': Simulates the pressing of a key combination on the keyboard (e.g., Ctrl+v).\\n\\n'scroll [direction= down |up]': Scroll the page up or down.\\n\\nTab Management Actions:\\n\\n'new_tab': Open a new, empty browser tab.\\n\\n'tab_focus [tab_index]': Switch the browser's focus to a specific tab using its index.\\n\\n'close_tab': Close the currently active tab.\\n\\nURL Navigation Actions:\\n\\n'goto [url]': Navigate to a specific URL.\\n\\n'go_back': Navigate to the previously viewed page.\\n\\n'go_forward': Navigate to the next page (if a previous 'go_back' action was performed).\\n\\nCompletion Action:\\n\\n'stop [answer]': Apply this action when you believe the task is complete. If it is a operation-type task, use 'stop [Done]' when finished. If the objective is to give a text-based answer, provide the answer in\"}"}
{"id": "4S8agvKjle", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To be successful, it is very important to follow the following rules:\\n\\n1. You should only issue an action that is valid given the current observation.\\n2. You should only issue one action at a time.\\n3. Generate the action in the correct format and always put the action inside a pair of @. Such as, @click [1234]@.\\n4. Complete the task by interacting with the starting page, and avoid using 'goto' actions casually.\\n5. Reasonable inputs will return accurate observations, so do not repeat the same action when unnecessary.\\n\\nExamples\\n\\nYou should perform actions to accomplish the goal: Add a white desk to my wish list.\\n\\nObservation:\\n\\nWINDOWED PAGE:\\nTab 0 (current): One Stop Market\\n[1] RootWebArea 'One Stop Market' focused: True\\n[1254] link 'My Wish List'\\n[2427] StaticText 'Search'\\n[1585] combobox 'Search' autocomplete: both hasPopup: listbox required: False expanded: False\\n[2430] link 'Advanced Search'\\n[1588] button 'Search' disabled: True\\n\\nURL: http://onestopmarket.com\\n\\nAction: @type [1585] [white desk] [press_enter_after=1]@\\n\\nObservation:\\n\\nWINDOWED PAGE:\\nTab 0 (current): Search results for: 'white desk'\\n[2635] RootWebArea \\\"Search results for: 'white desk'\\\" focused: True\\n[3869] link 'My Wish List'\\n[4827] StaticText 'Search'\\n[4072] combobox 'Search' autocomplete: both hasPopup: listbox required: False expanded: False\\n[5027] StaticText 'white desk'\\n[4830] link 'Advanced Search'\\n[4075] button 'Search' disabled: True\\n[3729] main ''\\n[3842] heading \\\"Search results for: 'white desk'\\\"\\n[3907] StaticText 'Items 1-12 of 38823'\\n[4781] link 'Image'\\n[4833] img 'Image'\\n[4783] link 'Image'\\n[4849] img 'Image'\\n\\nURL: http://onestopmarket.com/catalogsearch/result/?q=white+desk\\n\\nFigure 17: Prompt details for WebArena. The provided example is changed based on the type of the specific environment instance.\\n\\nPrompt Details for Academia\\n\\nSystem Prompt\\nYou can use actions to help people solve problems.\\n\\nInstruction\\nWe detail name, description, input(parameters) and output(returns) of each action as follows:\\n\\nName: loadPaperNet()\\nDescription: Load PaperNet. In this net, nodes are papers and edges are citation relationships between papers.\"}"}
{"id": "4S8agvKjle", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Name: loadAuthorNet()\\nDescription: Load AuthorNet. In this net, nodes are authors and edges are collaboration relationships between authors.\\n\\nName: neighbourCheck(graph, node)\\nDescription: List the first-order neighbors connect to the node. In paperNet, neighbors are cited papers of the paper. In authorNet, neighbors are collaborators of the author.\\nParameters:\\n- graph (Type: string, Enum: [PaperNet, AuthorNet]): The name of the graph to check\\n- node (Type: string): The node for which neighbors will be listed\\n\\nReturns:\\n- neighbors (Type: array)\\n\\nName: paperNodeCheck(node)\\nDescription: Return detailed attribute information of a specified paper in PaperNet\\nParameters:\\n- node (Type: string): Name of the paper.\\n\\nReturns:\\n- authors : The authors of the paper\\n- year : The published year of the paper\\n- venue : The published venue of the paper\\n- n_citation : The number of citations of the paper\\n- keywords : The keywords of the paper\\n- doc_type : The document type of the paper\\n\\nName: authorNodeCheck(node)\\nDescription: Return detailed attribute information of a specified author in AuthorNet\\nParameters:\\n- node (Type: string): name of the author.\\n\\nReturns:\\n- name : The name of the author\\n- org : The organization of the author\\n\\nName: authorEdgeCheck(node1, node2)\\nDescription: Return detailed attribute information of the edge between two specified nodes in a AuthorNet.\\nParameters:\\n- node1 (Type: string): The first node of the edge\\n- node2 (Type: string): The second node of the edge\\n\\nReturns:\\n- papers : All papers that the two authors have co-authored\\n\\nName: paperEdgeCheck(node1, node2)\\nDescription: Return detailed attribute information of the edge between two specified nodes in a PaperNet.\\nParameters:\\n- node1 (Type: string): The first node of the edge\\n- node2 (Type: string): The second node of the edge\\n\\nReturns:\\nNone\\n\\nName: check_valid_actions()\\nDescription: Get supported actions for current tool.\\nReturns:\\n- actions (Type: array): Supported actions for current tool.\\n\\nName: finish(answer)\\nDescription: Return an answer and finish the task\\nParameters:\\n- answer (Type: ['string', 'number', 'array']): The answer to be returned\\n\\nIf you are finished, you will call \\\"finish\\\" action\"}"}
{"id": "4S8agvKjle", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 18: Prompt Details for Academia in Tool-Query Environments.\\n\\nPrompt Details for Todo\\n\\nSystem Prompt\\nYou can use actions to help people solve problems.\\n\\nInstruction\\nWe detail name, description, input(parameters) and output(returns) of each action as follows:\\n\\nName: get_user_current_date()\\nDescription: Get the user's current date.\\nReturns: The current date in 'YYYY-MM-DD' format.\\n\\nName: get_user_current_location()\\nDescription: Get the user's current city.\\nReturns: The user's current city.\\n\\nName: get_projects()\\nDescription: Get all projects in the Todoist account\\nReturns:\\n- Array of objects with properties:\\n  - id (Type: string)\\n  - name (Type: string)\\n  - order (Type: integer)\\n  - color (Type: string)\\n  - is_favorite (Type: boolean)\\n\\nName: update_project(project_id, is_favorite)\\nDescription: Update a project\\nParameters:\\n- project_id (Type: string)\\n- is_favorite (Type: string, Enum: [True, False])\\nReturns: Information of the updated project\\n\\nName: get_tasks(project_id)\\nDescription: Get all tasks for a given project\\nParameters:\\n- project_id (Type: string)\\nReturns:\\n- Array of objects with properties:\\n  - id (Type: string)\\n  - project_id (Type: string)\"}"}
{"id": "4S8agvKjle", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"get_task_description(task_id)\\n\\nget_task_duration(task_id)\\n\\ncomplete_task(task_id)\\n\\nupdate_task(task_id, due_date)\\n\\ndelete_task(task_id)\\n\\ncheck_valid_actions()\\n\\nfinish(answer)\"}"}
{"id": "4S8agvKjle", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Goal: Is Prepare for history quiz a task of School project? Please answer yes or no.\\n\\nAction: get_projects with Action Input: {}\\n\\nObservation: [{'id': '12345', 'order': 0, 'color': 'charcoal', 'name': 'School', 'is_favorite': false}]\\n\\nAction: get_tasks with Action Input: {\"project_id\": \"12345\"}\\n\\nObservation: [{'id': '123451', 'order': 0, 'content': 'Prepare for history quiz', 'is_completed': false, 'priority': 1, 'due_date': '2030-10-10'}, {'id': '123452', 'order': 1, 'content': 'Prepare for math quiz', 'is_completed': false, 'priority': 1, 'due_date': '2030-11-10'}]\\n\\nAction: finish with Action Input: {\"answer\": \\\"yes\\\"}\\n\\nObservation: yes\\n\\nFigure 19: Prompt Details for Todo in Tool-Operation Environments.\\n\\nChecklist\\n\\nThe checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [TODO] to [Yes], [No], or [NA]. You are strongly encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description. For example:\\n\\n\u2022 Did you include the license to the code and datasets? [Yes]\\n\\n\u2022 Did you include the license to the code and datasets? [No] The code and the data are proprietary.\\n\\n\u2022 Did you include the license to the code and datasets? [NA]\\n\\nPlease do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.\\n\\n1. For all authors...\\n\\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n\\n(b) Did you describe the limitations of your work? [Yes] see \u00a77 and Appendix B.\\n\\n(c) Did you discuss any potential negative societal impacts of your work? [Yes] see Appendix C.\\n\\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n\\n(a) Did you state the full set of assumptions of all theoretical results? [NA]\\n\\n(b) Did you include complete proofs of all theoretical results? [NA]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n\\n(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See footnote in abstract.\\n\\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Data details are Appendix J and K, prompt in N, and model details in I.\\n\\n(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] We discuss error bars in Appendix D due to lack of table space.\\n\\n(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix M.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n\\n(a) If your work uses existing assets, did you cite the creators? [Yes]\\n\\n(b) Did you mention the license of the assets? [Yes] It's in the dataset card in supplementary.\\n\\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\\n\\n(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] See Appendix J and K.\\n\\n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See Appendix C.\"}"}
{"id": "4S8agvKjle", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. If you used crowdsourcing or conducted research with human subjects...\\n\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes]\\n\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [No]\\n\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [No]\"}"}
