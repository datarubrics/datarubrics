{"id": "7NR2ZVzZxx", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LogicBench: A Benchmark for Evaluation of Logical Reasoning\\n\\nAnonymous Author(s)\\n\\nAbstract\\nRecently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really \u201cReason\u201d over the natural language? This question has been receiving significant research attention and a number of reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to \u2018logical reasoning\u2019 has remained underexplored. Existing work investigating this reasoning ability has focused only on a couple of axioms (such as modus ponens and modus tollens) of propositional and first-order logic. To study logical reasoning, we introduce LogicBench, a systematically created natural language question-answering dataset encompassing 25 reasoning patterns spanning over propositional, first-order, and non-monotonic logics. Key steps of our dataset construction consist of (1) controlled generation of sentences and their negations containing different ontologies, (2) (context, question, answer) triplets creation using heuristically designed templates, and (3) semantic variations of triplets adding more diversity. We first evaluate easily accessible and widely used LLMs such as GPT-3, ChatGPT, and FLAN-T5 and show that they do not fare well on LogicBench, achieving just above random accuracy on average (\\\\(\\\\sim 52\\\\%\\\\)). Then, we show that LLMs trained using our data exhibit a better understanding of logical reasoning leading to performance improvements on several existing logical reasoning datasets such as LogicNLI, FOLIO, LogiQA, and ReClor.\\n\\n1 Introduction\\nLarge language models such as GPT-3 [3], ChatGPT, and FLAN [18] have made remarkable progress in NLP research enabling machines to perform a variety of language tasks that were previously thought to be exclusive to humans [12, 2, 20]. However, the ability of these LLMs to reason \u201clogically\u201d over natural language text remains under-explored, even though logical reasoning is a fundamental aspect of intelligence and a crucial requirement for many practical applications, such as question-answering systems [8] and conversational agents [1]. Although several datasets have been proposed [4, 16, 7, 13] to evaluate the logical reasoning capabilities of LLMs, these datasets are limited in their scope by (1) not evaluating logical reasoning independently of other forms of reasoning such as LogiQA [11] and ReClor [19]; and (2) evaluating only a single type of logic and covering only few logical inference rules as done in FOLIO [6] and ProntoQA [14]. Thus, our aim in this work is to address the lacuna of having a more comprehensive evaluation dataset for LLMs. To this end, we propose LogicBench, a systematically created question-answering dataset for the evaluation of logical reasoning ability. As illustrated in Figure 1, LogicBench includes a total of 25\\n\\n1 Data is available at https://anonymous.4open.science/r/LogicBench-EEBB\\n\\nSubmitted to the 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks. Do not distribute.\"}"}
{"id": "7NR2ZVzZxx", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Comprehensive representation of different inference rules and reasoning patterns covered by propositional, first-order, and non-monotonic logics. To evaluate LLMs, we formulate a binary classification task in LogicBench in which the context represents logical statements and the models have to determine whether a conclusion given in the question is logically entailed by the context. For example, given the context \\\"All mammals have fur\\\" and \\\"A cat is a mammal\\\", for the question is \\\"Does a cat have fur?\\\", the correct answer is \\\"Yes\\\". (Additional examples of task instances are presented in Table 3 and Appendix B. To construct LogicBench, we use a three-stage procedure (refer to \u00a72). In the first stage, we prompt GPT-3 to generate a variety of coherent natural language sentences consisting of different 'ontologies' (i.e., a collection of concepts such as car, person, and animals) and their corresponding negations (refer to \u00a72.2.1). Then, in the second stage, we generate (context, question, answer) triplets using heuristically designed templates based on the inference rules and patterns. Finally, in the third stage, we generate semantics preserving and inverting variations of these logical rules by incorporating negations. We evaluate a range of accessible and widely used LLMs including GPT-3 [3], ChatGPT, FLAN-T5 [18], Tk-instruct [17], and UnifiedQA [9] with respect to LogicBench on the accuracy of the predicted answers (i.e., \\\"Yes\\\" or \\\"No\\\"). Experimental results reveal that these models struggle with respect to many of the inference rules and patterns (showing \u223c52% accuracy on an average), suggesting significant room for improvement in their logical reasoning abilities. We then synthetically augment LogicBench and train T5-large. Our initial experimental results show that this improves the logical reasoning ability of existing models leading to performance improvement on other logic datasets, such as LogicNLI, and FOLIO (\u223c2% on an average), and shows competitive performance on LogiQA and ReClor. In summary, our contributions are as follows:\\n\\n1. Introducing LogicBench: A systematically created dataset to assess the logical reasoning capabilities of LLMs across propositional, first-order, and non-monotonic logics. This benchmark will be publicly available for evaluation and training purposes.\\n\\n2. We propose a three-stage method to construct LogicBench consisting of GPT-3 to generate coherent natural language sentences using prompts and a template-based module to convert them into logical rules. By assessing the performance of existing LLMs, we gain insights into their logical reasoning abilities which further leads to several interesting findings.\\n\\n3. To the best of the authors' knowledge, this is the first benchmark to study non-monotonic reasoning, as well as various inference rules in propositional and first-order logics including hypothetical and disjunctive syllogism; and bidirectional, constructive, and destructive dilemmas in the NLP domain.\"}"}
{"id": "7NR2ZVzZxx", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section we discuss the logic types, inference rules, and patterns that are explored in this research. We also outline the methods for generating the data, and statistics of LogicBench.\\n\\n2.1 Logics Types\\n\\nPropositional Logic (PL)\\n\\nPropositional logic employs a collection of statements or propositions (denoted as $P = p_1, p_2, ..., p_n$, where $p_i$ represents a proposition) and builds upon them using logical connectives such as \u2018$\\\\land$\u2019, \u2018$\\\\lor$\u2019, \u2018$\\\\rightarrow$\u2019, \u2018$\\\\leftrightarrow$\u2019, and \u2018$\\\\neg$\u2019. Several inference rules for propositional logic have been defined using which given a set of premises, one can derive a sound conclusion. To illustrate this, let us consider two propositions: $p_1$, which states \\\"It is raining,\\\" and $p_2$, which states \\\"It is cloudy.\\\" From these propositions, we can construct a context (KB) consisting of two premises:\\n\\n1. $(p_1 \\\\rightarrow p_2)$\\n2. $p_1$\\n\\nBased on this KB, we can conclude $p_2$. This inference rule is written as $(((p_1 \\\\rightarrow p_2) \\\\land p_1) \\\\vdash p_2)$ and is known as 'Modus Ponens'. In our study, we explore nine distinct inference rules of propositional logic, extensions of seven of them with one-variable and a universal quantifier, and two axioms of first-order logic as shown in Table 1. These inference rules provide a systematic framework for deriving valid conclusions.\\n\\n| Names                          | Propositional Logic | Extension to a (restricted) First-order Logic |\\n|-------------------------------|---------------------|-----------------------------------------------|\\n| MP                            | $(((p_1 \\\\rightarrow p_2) \\\\land p_1) \\\\vdash p_2)$ | $(\\\\forall x (p(x) \\\\rightarrow q(x)) \\\\land p(a) \\\\vdash q(a))$ |\\n| MT                            | $(((p_1 \\\\rightarrow p_2) \\\\land \\\\neg q) \\\\vdash \\\\neg p)$ | $(\\\\forall x (p(x) \\\\rightarrow q(x)) \\\\land \\\\neg q(a) \\\\vdash \\\\neg p(a))$ |\\n| HS                            | $((p_1 \\\\rightarrow q_1) \\\\land (q_1 \\\\rightarrow q_2)) \\\\vdash (p_1 \\\\rightarrow q_2)$ | $(\\\\forall x (p(x) \\\\rightarrow q(x)) \\\\land (q(x) \\\\rightarrow r(x)) \\\\land (p(a) \\\\lor q(a)) \\\\vdash (q(a) \\\\lor r(a)))$ |\\n| DS                            | $((p_1 \\\\lor q_1) \\\\land \\\\neg p_1) \\\\vdash q_1$ | $(\\\\forall x (p(x) \\\\lor q(x)) \\\\land \\\\neg p(a) \\\\vdash q(a))$ |\\n| CD                            | $((p_1 \\\\rightarrow q_1) \\\\land (r_1 \\\\rightarrow s_1) \\\\land (p_1 \\\\lor r_1)) \\\\vdash (q_1 \\\\lor s_1)$ | $(\\\\forall x (p(x) \\\\rightarrow q(x)) \\\\land (r(x) \\\\rightarrow s(x)) \\\\land (p(a) \\\\lor r(a))) \\\\vdash (q(a) \\\\lor s(a)))$ |\\n| DD                            | $((p_1 \\\\rightarrow q_1) \\\\land (r_1 \\\\rightarrow s_1) \\\\land (\\\\neg q_1 \\\\lor \\\\neg s_1)) \\\\vdash (\\\\neg p_1 \\\\lor \\\\neg r_1)$ | $(\\\\forall x (p(x) \\\\rightarrow q(x)) \\\\land (r(x) \\\\rightarrow s(x)) \\\\land (\\\\neg q(a) \\\\lor \\\\neg s(a)) \\\\vdash (\\\\neg p(a) \\\\lor \\\\neg r(a)))$ |\\n| BD                            | $((p_1 \\\\rightarrow q_1) \\\\land (r_1 \\\\rightarrow s_1) \\\\land (p_1 \\\\lor \\\\neg s_1)) \\\\vdash (q_1 \\\\lor \\\\neg r_1)$ | $(\\\\forall x (p(x) \\\\rightarrow q(x)) \\\\land (r(x) \\\\rightarrow s(x)) \\\\land (p(a) \\\\lor \\\\neg s(a)) \\\\vdash (q(a) \\\\lor \\\\neg r(a)))$ |\\n| CT                            | $p_1 \\\\lor q_1 \\\\vdash q_1 \\\\lor p_1$ | - |\\n| MI                            | $p_1 \\\\rightarrow q_1 \\\\vdash \\\\neg p_1 \\\\lor q_1$ | - |\\n| EI                            | - $\\\\exists x P(x) \\\\Rightarrow P(a)$ | - |\\n| UI                            | - $\\\\forall x A \\\\Rightarrow A \\\\{ x \\\\rightarrow a \\\\}$ | - |\\n\\nFirst-order Logic (FOL)\\n\\nIn this work, we consider a restricted set of logical axioms for FOL that utilize quantifiers, $\\\\forall$ (universal quantifier) and $\\\\exists$ (existential quantifier). The universal quantifier ($\\\\forall$) denotes that a statement holds true for all instances within a specific category. In contrast, the existential quantifier ($\\\\exists$) indicates that a statement is true for at least one instance within its scope. For instance, a simple extension of propositional 'Modus Ponens' is an inference rule where given the premises $\\\\forall x (p(x) \\\\rightarrow q(x))$ and $p(a)$, we conclude $q(a)$ (e.g., given \\\"All kings are greedy\\\" and \\\"Sam is a king\\\", we can conclude \\\"Sam is greedy\\\"). Here, we explore various axioms and inference rules that incorporate the quantifiers shown in Table 1.\\n\\nNon-monotonic (NM) Reasoning\\n\\nIn this work, we analyze a range of logical reasoning templates in NM logics involving \\\"Default Reasoning,\\\" \\\"Reasoning about Unknown Expectations,\\\" and \\\"Reasoning about Priorities.\\\" These templates are inspired by the compilation [10] made in 1989 to evaluate the abilities of various non-monotonic logics that were being developed at that time. Below Table 2 shows examples of NM reasoning. Additional examples are given in Appendix B.3. A key aspect of NM logics is to formalize notions such as \\\"normally,\\\" \\\"typically,\\\" and \\\"usually\\\" that are not directly formalizable using classical quantifiers in the first-order setting. The general rule \\\"Heavy blocks are normally located on the table\\\" does not imply that \\\"All heavy blocks are located on the table.\\\"\"}"}
{"id": "7NR2ZVzZxx", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Basic Default Reasoning\\n\\nDefault Reasoning with Irrelevant Information\\n\\nContext: Blocks A and B are heavy.\\nHeavy blocks are typically located on the table.\\nA is not on the table.\\nConclusion: B is on the table.\\n\\nContext: Blocks A and B are heavy.\\nHeavy blocks are typically located on the table.\\nA is not on the table.\\nB is red.\\nConclusion: B is on the table.\\n\\nReasoning about Unknown Expectations\\n\\nReasoning about Priorities\\n\\nContext: Blocks A, B, and C are heavy.\\nHeavy blocks are normally located on the table.\\nAt least one of A, B is not on the table.\\nConclusion: C is on the table.\\nExactly one of A, B is not on the table.\\n\\nContext: Jack asserts that block A is on the table.\\nMary asserts that block A is not on the table.\\nWhen people assert something, they are normally right.\\nConclusion: If Mary's evidence is more reliable than Jack's.\\nthen block A is not on the table.\\n\\nTable 2: Illustrative examples of non-monotonic reasoning adapted from [10]\\n\\nalways located on the table\". Rather, this rule allows for exceptions. Our work explores various NM reasoning types, as depicted in Figure 1, to delve deeper into the nuances of this type of reasoning.\\n\\n2.2 Data Creation\\n\\nOur data creation procedure, illustrated in Figure 2, consists of three stages:\\n1. Sentence Generation: Starting with a given prompt, we generate coherent sentences and their negations that incorporate different ontologies.\\n2. NL Conversion: Using predefined templates of reasoning patterns based on their formal expressions, we convert the generated sentences into (context, question, answer) triplets.\\n3. Variation Generation: We generate semantically preserving and inverting variations of these triplets to add more diversity.\\n\\nBy following this method, we construct LogicBench, and examples of generated data corresponding to each logic type and reasoning patterns are presented in Appendix B.\\n\\n2.2.1 Sentence Generation\\n\\nHere, the first step is to generate sentences with diverse ontologies. An ontology represents a collection of concepts (e.g. car, person, animals, etc.) along with their corresponding associated\\n\\n4\"}"}
{"id": "7NR2ZVzZxx", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To generate these sentences, we prompt the GPT-3 model with instructions tailored for each inference rule. The prompt schema, as depicted in Figure 3, comprises three crucial components:\\n\\n- **Definition**: provides a detailed explanation of the task and offers a natural language representation of the reasoning pattern for which we are generating sentences.\\n\\n- **Examples**: provide sample sentences that need to be generated. We also illustrate how these sentences will be utilized in later stages, emphasizing the importance of coherence and the inclusion of relevant ontological concepts.\\n\\n- **Format**: We provide specific formatting instructions to guide the generation of sentences. An example of a prompt corresponding to the 'Modus Tollens' from PL is presented in Appendix A for better illustration. Note that our objective at this stage is not to generate logical sentences but rather to generate a diverse and coherent set of sentences that encompass various concepts. We also create a negation sentence corresponding to each generated sentence. In this work, the scope of generating negations is simple (refer to Appendix C for examples), however, negations can be more complicated in the case of logic. These generated sentences will be combined with logical connectives in a later stage to form context and questions.\\n\\n### 2.2.2 NL Conversion\\n\\nWe focus on leveraging the formal expressions of reasoning patterns to create templates that establish the desired NL formulation for each logical connective. For instance, implication: \\\"\\\\( p \\\\rightarrow q \\\\)\\\" is expressed as \\\"If \\\\( p \\\\), then \\\\( q \\\\)\\\"; conjunction: \\\"\\\\( p \\\\land q \\\\)\\\" is expressed as \\\"\\\\( p \\\\) and \\\\( q \\\\)\\\"; and disjunction: \\\"\\\\( p \\\\lor q \\\\)\\\" is expressed as \\\"At least one of the following is true: (1) \\\\( p \\\\) and (2) \\\\( q \\\\). Note that we do not know which of (1) and (2) is true. It is possible that only (1) is true, or only (2) is true, or both are true.\\\"\\n\\nWith these established formulations, we proceed to utilize the sentences generated in \u00a72.2.1 to create the context and questions corresponding to reasoning patterns. For instance, let's consider the \\\"Modus Tollens\\\" from PL ((\\\\( p \\\\rightarrow q \\\\)) \\\\( \\\\land \\\\) \\\\( \\\\neg q \\\\)) \\\\( \\\\vdash \\\\) \\\\( \\\\neg p \\\\)), and the \\\"Bidirectional Dilemma\\\" from FOL (\\\\( \\\\forall x ((p(x) \\\\rightarrow q(x)) \\\\land (r(x) \\\\rightarrow s(x))) \\\\land (p(a) \\\\lor \\\\neg s(a))) \\\\vdash (q(a) \\\\lor \\\\neg r(a)) \\\\)). Table 3 presents examples of logical context and questions for these inference rules, and Appendix C showcases further examples corresponding to each inference rule and patterns from LogicBench.\\n\\n### 2.2.3 Variation Generation\\n\\nAfter generating the context and questions in \u00a72.2.2, we generate semantically preserving and inverting variations of questions. Let's consider the example of \\\"Modus Tollens\\\" from Table 3, where the question is: \\\"If he won't order pizza for dinner, does this imply that Liam didn't finish his work early?\\\" In this question, we observe two propositions: \\\\( s_1 \\\\), representing the statement \\\"Liam didn't finish his work early,\\\" and \\\\( s_2 \\\\), representing the statement \\\"He won't order pizza for dinner.\\\" By perturbing these propositions, we can create four possible tuples: \\\\( s_1, s_2 \\\\), \\\\( \\\\neg s_1, s_2 \\\\), \\\\( s_1, \\\\neg s_2 \\\\), \\\\( \\\\neg s_1, \\\\neg s_2 \\\\). Each tuple represents a combination of true or negation values for the propositions. Although it is possible to create more combinations from \\\\( s_1, \\\\neg s_1 \\\\) and \\\\( s_2, \\\\neg s_2 \\\\), we refine and restrict the set of triplets to exclude those that undermine the validity of the inference rule. To generate question variations, we replace the propositions in the original question with the corresponding tuples from the generated variations, hence, adding more diversity to LogicBench. This process allows us to create different variations of the question, as illustrated in Figure 2 (Step 3). More examples of question variations are in Appendix B.\"}"}
{"id": "7NR2ZVzZxx", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Axiom\\n\\nGenerated sentences in stage 1\\n\\nContext and Question\\n\\nModus Tollens\\n\\n\\\\[ \\\\begin{align*}\\n    p & : \\\\text{Liam finished his work early.} \\\\\\\\\\n    \\\\neg p & : \\\\text{Liam did not finish his work early.} \\\\\\\\\\n    q & : \\\\text{He will order pizza for dinner.} \\\\\\\\\\n    \\\\neg q & : \\\\text{He will not order pizza for dinner.}\\n\\\\end{align*} \\\\]\\n\\nContext:\\nIf Liam finished his work early, then he will order pizza for dinner.\\n\\nQuestion:\\nIf he won't order pizza for dinner, does this imply that Liam didn't finish his work early?\\n\\nBidirectional Dilemma\\n\\n\\\\[ \\\\begin{align*}\\n    p(x) & : \\\\text{someone drinks lots of water} \\\\\\\\\\n    q(x) & : \\\\text{they will feel hydrated} \\\\\\\\\\n    r(x) & : \\\\text{they eat too much sugar} \\\\\\\\\\n    s(x) & : \\\\text{they will experience a sugar crash} \\\\\\\\\\n    p(a) & : \\\\text{Jane drinks lots of water} \\\\\\\\\\n    \\\\neg p(a) & : \\\\text{Jane does not drink lots of water} \\\\\\\\\\n    q(a) & : \\\\text{she will feel hydrated} \\\\\\\\\\n    \\\\neg q(a) & : \\\\text{she will not feel hydrated} \\\\\\\\\\n    r(a) & : \\\\text{she eats too much sugar} \\\\\\\\\\n    \\\\neg r(a) & : \\\\text{she does not eat too much sugar} \\\\\\\\\\n    s(a) & : \\\\text{she will experience a sugar crash} \\\\\\\\\\n    \\\\neg s(a) & : \\\\text{she will not experience a sugar crash}\\n\\\\end{align*} \\\\]\\n\\nContext:\\nIf someone drinks lots of water, then they will feel hydrated. If they eat too much sugar, then they will experience a sugar crash. We know that at least one of the following is true (1) Jane drinks lots of water and (2) she won't experience a sugar crash. Note that we do not know which ones of (1) and (2) are true. It might be the case that only (1) is true, or only (2) is true or both are true.\\n\\nQuestion:\\nIf at least one of (1) and (2) is true, can we say, at least one of the following must always be true? (a) she will feel hydrated and (b) she doesn't eat too much sugar.\\n\\nTable 3: Illustrative examples of logical context and questions created using sentences that are generated in the first stage \u00a72.2.1.\\n\\n| Dataset                      | # of Instances | per Axiom | Total # of Instances | Total # of Instances (Including Variations) |\\n|------------------------------|----------------|-----------|----------------------|---------------------------------------------|\\n| LogicBench(Eval)             | 20             | 500       | 1720                 |                                             |\\n| LogicBench(Aug)              | 150            | 3750      | 12908                |                                             |\\n\\n2.3 Statistics and Qualitative Analysis\\n\\nStatistics\\n\\nWe introduce two versions of our proposed dataset: LogicBench(Eval) and LogicBench(Aug). Statistics of both versions are presented in Table 4. Here, LogicBench(Eval) is created using the above method along with human-in-loop to ensure the quality of generated data, whereas LogicBench(Aug) is only a synthetically augmented version for training purposes.\\n\\nThese two versions aim to accommodate different evaluation and training needs to explore logical reasoning. Considering the cost and complexity associated with recent LLMs such as GPT-3, and GPT-4, we believe that LogicBench(Eval) provides a more feasible evaluation benchmark.\\n\\nQuality of Data\\n\\nThroughout the data generation phase of LogicBench(Eval), the authors conduct a review of the logical formations to ensure they adhered to the intended structure. We examine each reasoning pattern for any potential discrepancies, ensuring that they were logically sound and correctly represented the intended relationships between propositions. In addition to the logical formation, we also dedicated considerable effort to eliminating typos and validating the grammar.\\n\\n3 Results and Analysis\\n\\n3.1 Experimental Setup\\n\\nTask Formulation\\n\\nWe formulate binary classification task using LogicBench to evaluate the logical reasoning ability of LLMs. Let us consider a set of data instances \\\\( I_a,L \\\\) corresponding to axiom \\\\( a \\\\) and logic type \\\\( L \\\\). In this set, \\\\( i \\\\)th instance is represented as \\\\( I_i a,L = \\\\{ (c_i, Q_i) \\\\} \\\\) where \\\\( c_i \\\\) represents context and \\\\( Q_i = \\\\{ q_1, q_2, ..., q_n \\\\} \\\\) represents set of question and its variations corresponding to \\\\( i \\\\)th instance. As discussed in \u00a72, each context (\\\\( c \\\\)) represents logical rules (e.g., All cats have fur. Tom is a cat.) and question (\\\\( q \\\\)) represents the conclusion (e.g., Does Tom have fur?). To each context and question pair, i.e., \\\\( \\\\langle c, q \\\\rangle \\\\), we assign a label from the set \\\\( Y = \\\\{ \\\\text{Yes, No} \\\\} \\\\). We assign a label \\\\( \\\\text{Yes} \\\\) if the conclusion logically entails the context, otherwise, assign a label \\\\( \\\\text{No} \\\\). To evaluate any model on this setup, we provide \\\\( \\\\langle c, q \\\\rangle \\\\) as input to predict a label from \\\\( Y \\\\).\"}"}
{"id": "7NR2ZVzZxx", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Experiments\\n\\nWe evaluate easily available and widely used prompting models (i.e., GPT-3 (davinci-189003) and ChatGPT), and instruction-tuned models (FLAN-T5 and Tk-instruct) on LogicBench (Eval). Since logical reasoning is an important aspect of different QA tasks, we also evaluate UnifiedQA. Each model is evaluated in a zero-shot setting where the prompt is provided to the model without any in-context examples. This approach allows us to determine LLM\u2019s inherent ability to do logical reasoning (based on pre-training), as we can not expect that various logical inference rules/patterns will always be made part of prompts. However, we do evaluate these models in a few-shot setting, and present the results in Appendix F. We also present exploratory \u2013 only exploratory because of the limited availability of their inference APIs \u2013 analysis over Bard and GPT-4 in Appendix G.\\n\\nIn addition, we employed the T5-large model and trained it on the LogicBench (Aug) resulting in a model named LogicT5. LogicT5 has achieved $\\\\sim97\\\\%$ of accuracy on LogicBench (Eval) since it is evident that supervised fine-tuning improves results by a large margin. Subsequently, we performed fine-tuning on four other logical reasoning datasets: LogiQA, Reclor, LogicNLI, and FOLIO. Our experiments were carried out in two settings: single-task (fine-tuning and evaluation on one dataset) and multi-task (fine-tuning on all four datasets combined, with separate evaluations for each dataset). A detailed experimental setup is described in Appendix D.\\n\\nMetrics\\n\\nHere, we evaluate performance in terms of accuracy corresponding to each label, i.e., $A(\\\\text{Yes})$ and $A(\\\\text{No})$. We evaluate each model on three different prompts and report average results across these prompts. All prompts used for experiments are described in Appendix D.\\n\\n3.2 Benchmark Results\\n\\nTable 5 represents label-wise accuracy ($A(\\\\text{Yes})$ and $A(\\\\text{No})$) corresponding to each LLMs. Here, we focus on analyzing the $A(\\\\text{Yes})$ since the aim is to understand the model\u2019s logical reasoning capabilities in answering the question where the conclusion entails the logical context. Table 5 provides valuable insights into the performance of different models on various logic types. For PL, UnifiedQA exhibits an average performance of 15%, while FLAN-T5 and Tk-instruct achieve $\\\\sim25\\\\%$. GPT-3 demonstrates a performance of 57.6%, and ChatGPT achieves 46.8%. Moving on to FOL, these models showcase performance accuracy of 52.7%, 51.2%, 55.7%, 76.2%, and 72.6% for UnifiedQA, FLAN-T5, Tk-instruct, GPT-3, and ChatGPT, respectively. On the NM reasoning, these models show an accuracy of 63.5%, 56.2%, 56.3%, 62%, and 70.9%, respectively. Overall, these models display an average performance of $\\\\sim34\\\\%$, $\\\\sim61\\\\%$, and $\\\\sim62\\\\%$ on PL, FOL, and NL.\\n\\nFrom Table 5, we can observe that models struggle more with inference rules of PL compared to FOL and NM reasoning. Furthermore, it is noticeable that each model performs relatively better on questions with a negative response (i.e., $\\\\text{No}$) compared to questions with a positive response (i.e., $\\\\text{Yes}$). This observation suggests that the models struggle to fully comprehend the logical relationship between the context and the conclusion (i.e., lower $A(\\\\text{Yes})$). However, they demonstrate a relatively stronger understanding when the relationship is contradictory in nature (i.e., higher $A(\\\\text{No})$). However, analyzing the performance of the models on inference rules is crucial to understand their limitations. Table 5 presents the inference rule-wise performance for each model as well.\\n\\n3.3 Analysis and Discussion\\n\\nLarge models are better logical reasoners. Based on the observed performance from Table 5, it becomes evident that larger model sizes and extensive pre-training data contribute to a better understanding of logical aspects. Consequently, models with larger sizes tend to exhibit higher performance across different types of logic. Nonetheless, the average performance remains at around 52.7%, indicating room for improvement in these models\u2019 logical comprehension capabilities.\\n\\nNegations are hard to understand when embedded with logical rules. Regarding PL and FOL, it is apparent that the models struggle more with the DS, DD, and MT inference rules. A closer look at Table 1 reveals that all of these axioms include examples where the models need to draw conclusions based on negated premises. This indicates that the models encounter difficulties when negations are embedded with logical rules.\"}"}
{"id": "7NR2ZVzZxx", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5: Evaluation of LLMs in terms of label-wise accuracy on LogicBench(Eval), where **A** (Yes) and **A** (No) denote the accuracy for the Yes and No labels, respectively. DRI: Default Reasoning with Irrelevant Information, DRS: Default Reasoning with Several Defaults, DRD: Default Reasoning with a Disabled Default, DRO: Default Reasoning in an Open Domain, RE1: Reasoning about Unknown Expectations I, RE2: Reasoning about Unknown Expectations II, RE3: Reasoning about Unknown Expectations III, RAP: Reasoning about Priorities.\\n\\nEffect on other logic datasets\\n\\nTable 6 represents the accuracy comparison between LogicT5 and baseline T5-large in both single-task and multi-task settings. The results indicate that training LLMs on LogicBench(Aug) has a greater impact on logic datasets that primarily focus on logical reasoning, such as FOLIO and LogicNLI. Hence, we can observe that LogicT5 consistently outperforms the baseline for LogicT5 and FOLIO. However, LogiQA and ReClor encompass other forms of reasoning in addition to logical reasoning, hence, LogicT5 demonstrates competitive performance on them.\\n\\nHow do LLMs reason step-by-step?\\n\\nWe investigate the fraction of low-performing axioms that contain various types of logical reasoning steps to predict the answer, and whether the correctness of those steps is correlated with the performance. Here, we perform a case study on ChatGPT. We prompt ChatGPT to generate reasoning steps along with predictions. For PL, we observe that...\"}"}
{"id": "7NR2ZVzZxx", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Performance comparison between LogicT5 and baseline T5-large in terms of accuracy.\\n\\n|            | Single-Task T5-large | Multi-Task T5-large |\\n|------------|----------------------|---------------------|\\n| LogicT5    | 16.9 71.2 84.4 36.8  | 21.8 83.8 68.2 42.8 |\\n\\nwhile the model can effectively reason the initial section of the disjunctive syllogism involving two possibilities $p$ or $q$, it encounters challenges in deducing whether $q$ should follow from the $\\\\neg p$. For FOL, ChatGPT encounters challenges in comprehending longer logical contexts, resulting in a lack of confidence in establishing the relationship between given propositions. Furthermore, to derive an accurate conclusion when the rules are followed correctly, the model relies on supplementary evidence. We observe that ChatGPT encounters difficulties in comprehending the nuanced meanings of words such as \u201cusually\u201d, \u201cnormally\u201d and \u201ctypically\u201d when establishing sentence relationships within NM reasoning. Notably, when it comes to the rule of default reasoning, ChatGPT fails to grasp inherent associations between two entities that commonly share characteristics. Examples and more analysis of generated explanations for each logic type are presented in Appendix E.\\n\\n4 Related Work\\n\\nLogiQA [11] and ReClor [19] have made notable contributions by compiling multichoice questions from standardized graduate admission examinations that demand diverse forms of logical reasoning. However, in contrast to our LogicBench, these datasets involve complex mixed forms of reasoning and do not specifically focus on assessing logical reasoning in isolation. A few past attempts have been made to create datasets to evaluate only logical reasoning while excluding other forms of reasoning. For example, CLUTTER [15] covers inductive reasoning, [5] covers temporal logic, and Ruletaker [4] evaluates whether a transformer-based model emulates deductive reasoning over synthetically generated statements in a limited setting. LogicNLI [16] introduced a diagnostic benchmark for FOL reasoning, with the dataset constructed by first automatically generating logic expressions and then replacing the entity and attribute placeholders in the logic expressions with simple and random subjects and predicates. FOLIO [6] gives diverse and complex logical expressions, however, it is only limited to FOL. ProntoQA [14] provides explanation and reasoning steps but is limited to modus ponens in FOL. Additional datasets for evaluating logical reasoning also exist such as TaxiNLI [7] introduce logical taxonomy in NLI task and RuleBert [13] covers only soft logical rules. In summary, LogicBench is evaluate logical reasoning in isolation and provides more diverse inference rules and logic types compared to existing datasets. Extended related work is discussed in Appendix H.\\n\\n5 Conclusions\\n\\nTo study the logical reasoning ability of LLMs, we introduced a novel benchmark called LogicBench which consists of 25 distinct inference rules and reasoning patterns covering propositional, first-order, and non-monotonic logics. We released two versions of the dataset: LogicBench(Eval) and LogicBench(Aug). LogicBench(Eval) serves as a high-quality, cost-effective, and reliable dataset for evaluating LLMs, while LogicBench(Aug) can be utilized for training purposes. Through comprehensive experiments, we showed that models such as GPT-3 and ChatGPT do not perform well on LogicBench, even though they require the application of only a single inference rule in positive (i.e., label 'Yes') data instance. Furthermore, we demonstrated that LLMs trained using LogicBench(Aug) showcase an improved understanding of logical reasoning, resulting in a better performance on existing logic datasets. Though LogicBench facilitates the evaluation and improvement of the logical reasoning ability of LLMs, it can be further extended by incorporating other inference rules and logic types; and having data instances that require applications of multiple inference rules.\"}"}
{"id": "7NR2ZVzZxx", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Sajjad Beygi, Maryam Fazel-Zarandi, Alessandra Cervone, Prakash Krishnan, and Siddhartha Jonnalagadda. Logical reasoning for task oriented dialogue systems. In Proceedings of the Fifth Workshop on e-Commerce and NLP (ECNLP 5), pages 68\u201379, Dublin, Ireland, May 2022. Association for Computational Linguistics.\\n\\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020.\\n\\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\n[4] Peter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pages 3882\u20133890, 2021.\\n\\n[5] Christopher Hahn, Frederik Schmitt, Jens U. Kreber, Markus Norman Rabe, and Bernd Finkbeiner. Teaching temporal logics to neural networks. In International Conference on Learning Representations, 2021.\\n\\n[6] Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, et al. Folio: Natural language reasoning with first-order logic. arXiv preprint arXiv:2209.00840, 2022.\\n\\n[7] Pratik Joshi, Somak Aditya, Aalok Sathe, and Monojit Choudhury. TaxiNLI: Taking a ride up the NLU hill. In Proceedings of the 24th Conference on Computational Natural Language Learning, pages 41\u201355, Online, November 2020. Association for Computational Linguistics.\\n\\n[8] Daniel Khashabi. Reasoning-Driven Question-Answering for Natural Language Understanding. University of Pennsylvania, 2019.\\n\\n[9] Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1896\u20131907, Online, November 2020. Association for Computational Linguistics.\\n\\n[10] Vladimir Lifschitz. Benchmark problems for formal nonmonotonic reasoning: Version 2.00. In Non-Monotonic Reasoning: 2nd International Workshop Grassau, FRG, June 13\u201315, 1988 Proceedings 2, pages 202\u2013219. Springer, 1989.\\n\\n[11] Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pages 3622\u20133628, 2021.\\n\\n[12] OpenAI. GPT-4 technical report, 2023.\\n\\n[13] Mohammed Saeed, Naser Ahmadi, Preslav Nakov, and Paolo Papotti. RuleBERT: Teaching soft rules to pre-trained language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1460\u20131476, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\"}"}
{"id": "7NR2ZVzZxx", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. In The Eleventh International Conference on Learning Representations, 2023.\\n\\nKoustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4506\u20134515, Hong Kong, China, November 2019. Association for Computational Linguistics.\\n\\nJidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, and Yaohui Jin. Diagnosing the first-order logical reasoning ability through LogicNLI. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3738\u20133747, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\\n\\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\\n\\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. ICLR, 2021.\\n\\nWeihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. Reclor: A reading comprehension dataset requiring logical reasoning. In International Conference on Learning Representations.\\n\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.\\n\\n---\\n\\nPaper Checklist\\n\\nFor all authors...\\n\\n1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Yes\\n\\n2. Have you read the ethics review guidelines and ensured that your paper conforms to them? Yes\\n\\n3. Did you discuss any potential negative societal impacts of your work? No, we do not expect negative societal impacts as a direct result of the contributions in our paper\\n\\n4. Did you describe the limitations of your work? Yes, refer to Section 5.\"}"}
{"id": "7NR2ZVzZxx", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"If you are including theoretical results...\\n\\n1. Did you state the full set of assumptions of all theoretical results?\\n   - N/A\\n\\n2. Did you include complete proofs of all theoretical results?\\n   - N/A\\n\\nIf you ran experiments...\\n\\n1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?\\n   - Yes, the anonymous URL is at the end of the abstract.\\n\\n2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?\\n   - Yes, refer to Section 3.1 and Appendix D.\\n\\n3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?\\n   - Yes, we reported the average results across three prompts (refer to Section 3.1).\\n\\n4. Did you include the amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?\\n   - Yes, refer to Appendix D.\\n\\nIf you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n\\n1. If your work uses existing assets, did you cite the creators?\\n   - Yes, refer to Section 1, Section 3, and Appendix D.\\n\\n2. Did you mention the license of the assets?\\n   - Yes, refer to Appendix D.\\n\\n3. Did you include any new assets either in the supplemental material or as a URL?\\n   - Yes\\n\\n4. Did you discuss whether and how consent was obtained from people whose data you're using/curating?\\n   - N/A\\n\\n5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?\\n   - Yes, the collected data does not contain personally identifiable information or offensive content.\\n\\nIf you used crowdsourcing or conducted research with human subjects...\\n\\n1. Did you include the full text of instructions given to participants and screenshots, if applicable?\\n   - N/A\\n\\n2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?\\n   - N/A\\n\\n3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?\\n   - N/A\"}"}
