{"id": "eJhc_CPXQIT", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MOMA-LRG: Language-Refined Graphs for Multi-Object Multi-Actor Activity Parsing\\n\\nZelun Luo, Zane Durante \u2217, Linden Li \u2217, Wanze Xie, Ruochen Liu, Emily Jin, Zhuoyi Huang, Lun Yu Li, Jiajun Wu, Juan Carlos Niebles, Ehsan Adeli, Li Fei-Fei\\n\\nStanford University\\n{lalanzluo, durante, lindenli, wanzexie, ruochenl, emilyjin, zhuoyih, junally}@stanford.edu\\n{jjwu, jniebles, eadeli, feifeili}@cs.stanford.edu\\n\\nWebsite: https://moma.stanford.edu/\\nToolkit: https://github.com/StanfordVL/moma/\\nDocumentation: https://momaapi.readthedocs.io/\\n\\nAbstract\\n\\nVideo-language models (VLMs), large models pre-trained on numerous but noisy video-text pairs from the internet, have revolutionized activity recognition through their remarkable generalization and open-vocabulary capabilities. While complex human activities are often hierarchical and compositional, most existing tasks for evaluating VLMs focus only on high-level video understanding, making it difficult to accurately assess and interpret the ability of VLMs to understand complex and fine-grained human activities. Inspired by the recently proposed MOMA framework, we define activity graphs as a single universal representation of human activities that encompasses video understanding at the activity, sub-activity, and atomic action level. We redefine activity parsing as the overarching task of activity graph generation, requiring understanding human activities across all three levels. To facilitate the evaluation of models on activity parsing, we introduce MOMA-LRG (Multi-Object Multi-Actor Language-Refined Graphs), a large dataset of complex human activities with activity graph annotations that can be readily transformed into natural language sentences. Lastly, we present a model-agnostic and lightweight approach to adapting and evaluating VLMs by incorporating structured knowledge from activity graphs into VLMs, addressing the individual limitations of language and graphical models. We demonstrate strong performance on activity parsing and few-shot video classification, and our framework is intended to foster future research in the joint modeling of videos, graphs, and language.\\n\\n1 Introduction\\n\\nComputer vision is currently undergoing a paradigm shift from models trained on crowd-labeled data [1, 2] to large-scale base models [3, 4, 5, 6, 7] trained on noisy but readily accessible image-text pairs. Video understanding is no exception, with the rise of Video-Language Models (VLMs) [8, 9, 10, 11, 12, 13, 14] that have shown remarkable generalization capabilities on videos from new domains. When compared to fixed-set video classification [2, 15, 16], VLMs are able to learn and represent a wider variety of concepts and demonstrate superior low-shot abilities on many downstream tasks due to the flexibility and open-vocabulary nature of language. Besides, while annotating videos remains\"}"}
{"id": "eJhc_CPXQIT", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"one of the most laborious processes in computer vision, VLMs utilize widely and freely available video-text pairs [9, 8, 13, 17] facilitating larger and more diverse pre-training at a lower cost. Despite improved generalization and scalability, the application of VLMs to human activity recognition faces a number of challenges. The first challenge is adapting existing VLMs for fine-grained, actor-centric activity recognition. Essential computer vision applications in healthcare, surveillance, and robotics are often characterized by complex human activities involving many concurrent events between actors and objects. On the other hand, existing VLMs are typically trained on noisy, coarse-grained internet data and evaluated on downstream tasks that are focused on high-level video understanding [18, 10, 12]. It is unclear how VLMs can be effectively adapted to and accurately evaluated for recognizing fine-grained activities. The second challenge is the lack of a single overarching task for evaluating VLMs on activity recognition. Human activities are often hierarchical and compositional [19, 20, 21], requiring explicit modeling and evaluation at multiple levels of granularity. Existing downstream tasks used for VLM evaluation, such as activity classification [18], activity segmentation [10, 12], video-text retrieval [13, 12, 8] and VideoQA [10, 13], only provide an incomplete assessment of VLM performance on activity recognition. Lastly, the black-box nature of VLMs makes their predictions difficult to interpret. This hinders the application of VLMs to many risk-averse domains [22, 23, 24], where it is necessary to interpret VLM predictions in a structured and symbolic manner.\\n\\nTo address the aforementioned challenges, this paper first aims to standardize an overarching representation of human activities across varying levels of granularity and provide a unified task for VLMs by generating this representation. Inspired by the recently proposed MOMA [20] framework, we introduce activity graphs as dynamic graphs that encompass activities, sub-activities, and atomic actions in a video. Specifically, an activity graph is a representation that simultaneously (1) provides a class label on the activity level, (2) provides a dynamic sub-activity label that contains all temporal boundaries and categories of sub-activities, and (3) captures fine-grained atomic actions in multi-object multi-actor settings with spatial localization and tracking of all entities and temporal localization of all predicates. Further, we introduce activity parsing as the overarching task of predicting the activity graph from a video, thereby achieving multi-level activity recognition via activity graph generation.\\n\\nNext, we introduce the MOMA-LRG dataset, a novel activity recognition dataset that leverages both the descriptive capacity of activity graphs and the expressiveness of natural language. MOMA-LRG involves videos with Multiple Objects and Multiple Actors (MOMA) and is designed to enable models to understand a broad set of human activities. To enable few-shot activity recognition with language, MOMA-LRG provides Language-Refined Graph annotations in a format that enables easy conversion from the structured graph representation into natural language sentences.\\n\\nLastly, we introduce GraphVLM as our framework for evaluating VLMs on activity parsing, consisting of an activity parsing model and a transfer learning paradigm. We first propose an architecture for activity parsing that can be readily adapted for VLMs, featuring a video stream, a text stream, and video tokenizers shared across all three levels of activity. Although fine-tuning is a widely used transfer learning technique, it requires a fixed architecture and clip sampling approach. In GraphVLM, we propose a transfer learning approach based on knowledge distillation, which enables the adaptation of VLMs in a flexible and lightweight manner.\"}"}
{"id": "eJhc_CPXQIT", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: A comparison of MOMA-LRG\u2019s vocabulary with related video datasets. MOMA-LRG\u2019s hierarchy unifies several definitions together (src: source, trg: target, atr: actor, obj: object, c: classified, g: grounded, t: tracked).\\n\\n| Unary predicate | Binary predicate |\\n|-----------------|------------------|\\n| Dataset Name    | src_atr | src_obj | Name | src_atr | src_obj | trg_atr | trg_obj |\\n| A V A           |         |         | A V A-Kinetics [26] | Pose | g,t | Person-person/object interaction | g,t | - |\\n| Action Genome [19] | - | - | - | - | - | - | - |\\n| MEV A [30]      | Activity g,t | Activity g,t | Activity g,t | Activity g,t | Activity g,t | Activity g,t | Activity g,t |\\n| TITAN [31]      | Individual | Atomic | Action | sc,g,t | c,g,t | Communicative | c,g,t | c,g,t | Vehicle State/Action | Contextual/Transportive |\\n|                | Relationship | c,g,t | Relationship | c,g,t | c,g,t | Relationship | c,g,t | c,g,t | Relationship | c,g,t |\\n\\nVideo detection architectures [28]. MOMA-LRG encompasses and extends these existing video datasets by annotating actions, sub-activities that compose them, and rich scene graphs to describe the interactions between entities in a crowded scene.\\n\\nVideo and language models. Several works pre-train large-scale models jointly on video and language data for a variety of downstream video-language tasks, such as video captioning, VQA, and video-text retrieval [51, 18, 12, 10, 14, 13]. Pre-training these models often either relies on a combination of masked-language-modeling (MLM) and masked-frame-modeling (MFM) [18, 10, 14] or contrastive learning [51, 9, 12, 13]. These VLMs have shown promising zero-shot results for activity recognition tasks such as activity classification [18], action segmentation [12], and action step localization [12, 10]. These methods show powerful zero-shot capabilities for high-level video understanding, however, they lack explicit knowledge about fine-grained interactions between actors and objects.\\n\\nFine-tuning large pre-trained models. Finetuning pre-trained large language models for downstream tasks has recently become the most popular learning method in NLP [52, 53]. Methods for efficiently fine-tuning these large language models, such as using adapter modules [54] and prompting [55, 53], use only a small number of learnable parameters while keeping most of the pre-trained model frozen. As a result, these methods are less computationally expensive than full fine-tuning of the entire model, which is the traditional fine-tuning method used in computer vision [56, 57]. There has been recent work adopting these efficient fine-tuning techniques for vision-language models [58] and [59] both propose adapter module methods and demonstrate comparable performance to full fine-tuning.\\n\\nOn the prompt-tuning side, Zhou et al. [60] develops prompt-tuning techniques for vision-language models for zero-shot image classification, Yao et al. [61] uses prompt-tuning for grounding referring expressions in images, and Ju et al. [62] uses image-level tokens and textual prompt tuning for few-shot action recognition.\\n\\nLow-shot activity recognition. Low-shot activity recognition, which recognizes activities that were either scarce or missing from the training set, reduces the reliance on obtaining expensive labels for crowded scenes. Zero-shot action recognition tries to predict unseen classes, where approaches either project visual features into a semantic embedding space [63, 64, 65, 66, 67], an intermediate embedding space learned from textual and visual data [68, 69, 70], or a visual embedding space that is synthesized by incorporating semantic information [71, 72]. Few-shot learning tends to be based on metric learning, which learns similarities to the scarce in-domain training examples [73, 74, 75, 76, 77, 78]. Other recent methods utilized self-supervised and contrastive or meta-learning approaches [79, 80] with a high degree of success. More recently, visual-language models have shown strong results on zero-shot recognition [18, 4, 12].\\n\\nVisual grounding and scene graphs. Visual grounding merges visual and language understanding by attempting to localize an object in an image space given a text query. Some datasets associate nouns with bounding boxes in the video [81, 82], while others introduce scene graph annotations that describe the relationships between entities in the image [83]. Yu et al. [84] and Chen et al. [85] demonstrate that using hierarchical text-generated scene graphs allows for better representation of fine-grained semantics than using raw text alone. Other work has proposed the use of an action graph to generate novel videos [86, 87], defining an object-centric graph with objects as nodes and edges as...\"}"}
{"id": "eJhc_CPXQIT", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2: A comparison of MOMA-LRG with related video datasets\\n\\n| Dataset         | Atomic action | Activity | Sub-activity | Actor | Object | Unary predicate | Binary predicate | Dataset Hours | Levels | Classes | Instances | Classes | Classes | Instances | Classes | Classes | Instances | Classes | Classes | Instances | Classes | Classes | Instances |\\n|-----------------|---------------|----------|--------------|-------|--------|----------------|------------------|----------------|---------|---------|-----------|---------|---------|-----------|---------|---------|-----------|---------|---------|-----------|---------|---------|-----------|\\n| A V A           | 25            | 107.5    | -            | -     | -      | -              | -                | 1             | 424K    | -       | -         | 14      | 424K    | 66        | 651KA   |\\n| A V A-Kinetics  | 638.9         | 2        | -            | -     | -      | -              | -                | 1             | 310K    | -       | -         | 13      | 633K    | 47        | \u223c800K   |\\n| Action Genome   | 82            | 2        | 157          | 10K   | -      | -              | -                | 35            | 0.4M    | -       | -         | 25      | 1.7M    | -         | -       |\\n| FineGym         | 708           | 3        | 10           | 4.9K  | 530    | 32.7K          | -                | -             | -       | -       | -         | -       | -       | -         | -       |\\n| Home           | ...           | -        | -            | -     | -      | -              | -                | -             | -       | -       | -         | -       | -       | -         | -       |\\n| ...             | ...           | ...      | ...          | ...   | ...    | ...            | ...              | ...           | ...     | ...     | ...       | ...     | ...     | ...       | ...     |\\n| TITAN           | 3             | 1        | -            | -     | -      | -              | -                | 3             | 395K    | 2       | 249K      | 16      | 935K    | 28        | 426K    |\\n| MOMA            | 66            | 3        | 17           | 373   | 67     | 2364           | 20               | 80K           | 120     | 80K     | 52        | 12K     | 23       | 119K      |         |\\n| MOMA-LRG        | 148           | 3        | 20           | 1.4K  | 91     | 15.8K          | 26               | 740K          | 126     | 396K    | 13        | 704K    | 52       | 1.4M     |         |\"}"}
{"id": "eJhc_CPXQIT", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: An example of the results for the activity parsing task. For the activity level, the model predicts the activity class \\\\textit{haircut} for the video input. For the subactivity level, the temporally localized sub-activity predictions are displayed on the bottom, with the corresponding sub-activity classes on the legend placed on the left. For the atomic action level, the model has localized and tracked all entities (actors and objects) and predicted their interactions as displayed in the graph visualization on the right. Note: this graphic is a live animation that can be viewed in an Adobe Acrobat PDF viewer.\\n\\nMultiple sub-activity and activity instances. At this level, activity parsing evaluates the ability of the model to predict: (1) all predicates present in the global context, similar to scene graph generation and relationship retrieval, and (2) all the predicate-specified entities across time, similar to spatio-temporal atomic action detection in [25].\\n\\nAtomic actions consist of two components: entities and predicates. An entity is defined to be either a human actor or an object that is present in the scene and relevant to the action being performed. In a video frame, we annotate each entity with a bounding box, class label, and instance ID. Throughout the video instance, an entity is therefore represented as a spatio-temporal tube with a corresponding semantic label. A predicate to describe an interaction that occurs with at least one entity. There are two different types of predicates: a unary predicate defined on a specific entity is called an attribute, whereas a predicate defined on two or more entities is called a higher-order relationship. Unlike other scene graph datasets [83, 19], relationships in MOMA-LRG can involve two or more actors. To do this, we provide hyperedge annotations where higher-order interactions involving multiple entities are grouped into a single edge (e.g. multiple actors beneath an object). Note that multi-node edges can easily be converted to a set of binary edges if needed.\\n\\nIntuition and advantages. The activity graph is a single universal representation of human activities, consisting of three levels of hierarchy ranging from coarse to fine-grained: activity, sub-activity, and atomic action. This is inspired by the fact that complex human activities in real-world settings are usually hierarchical and compositional across space and time. In particular, complex human activities typically involve a number of achievable steps (activity \u2192 sub-activity). It is also essential to understand the roles of actors, the affordances of objects, and the relationships between these components in order to recognize fine-grained activities (sub-activity \u2192 atomic action). In contrast, many existing activity recognition benchmarks and tasks [2, 15, 89] only focus on a specific level of granularity.\\n\\n3.2 The MOMA-LRG Dataset\\n\\nDataset statistics. MOMA-LRG contains 148 hours of videos and provides annotations on 1,412 activity instances from 20 activity classes, ranging from 31s to 600s and with an average duration of 241s. Besides, it contains 15,842 sub-activity instances from 91 sub-activity classes, ranging from 3s to 31s and with an average duration of 9s. On the atomic action level, we provide 161,265 atomic...\"}"}
{"id": "eJhc_CPXQIT", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Language-Refined Graphs.\\n\\nOne of MOMA-LRG's distinguishing features is that it enables few-shot capabilities. To do this, we provide graphical annotations that are easily compatible with natural language through two conventions. First, predicate classes are of the form [src] [predicate] [trg], where src is the source entity and trg the target entity. This enables easy conversion to natural language given graphical annotations. For example, given an outgoing predicate edge with class [src] talking to [trg] from the entity cashier onto the entity customer, we can produce the sentence the cashier is talking to a customer. Second, all of our annotations are in the present continuous tense, e.g. the player is throwing a frisbee, which resembles a live narration in a fashion similar to existing video-language datasets (e.g. YouCook2 [90], HowTo100M [9], etc.) created from instructional YouTube videos.\\n\\nComparison with existing datasets.\\n\\nCompared to existing datasets, there are several key advantages that the MOMA-LRG dataset provides. First, MOMA-LRG grounds all associated entities. In contexts with more than one entity, it is necessary to disambiguate which entities are involved in a particular interaction. Existing ego-centric datasets [16, 27] dodge this issue since at most interaction is involved in a scene. Second, we classify each actor's role. Typical datasets [27, 28] involve one type of actor and hence do not label the person's role [25, 19, 21]. In a diverse set of scenes, the role of the actor becomes more important in understanding actions since it can provide an important signal in parsing a human activity [91]. Third, the MOMA-LRG dataset differentiates between static and dynamic predicates. For example, the dynamic predicate sitting down is a dynamic movement where an actor transitions from the standing static predicate to the sitting static predicate. We argue that observing state transitions is important for the model to learn, encouraging it to learn perceptual causality [92]. For a more detailed comparison, refer to Table 2.\\n\\nComparison with MOMA [20].\\n\\nFirst of all, MOMA-LRG introduces a new dataset and a new abstraction of human activity. MOMA-LRG contains an order of magnitude more annotations, along with longer videos from a greater variety of scenes. In addition, MOMA-LRG introduces activity graphs as the overarching graphical representation across all three levels of hierarchy, as opposed to only the atomic level. Secondly, MOMA-LRG is directly motivated by the rise and limitations of VLMs. While VLMs have demonstrated remarkable generalization on videos from new domains and improved scalability through training on free video-language pairs, there is a lack of a single overarching task for evaluating VLMs on complex activity recognition. MOMA-LRG introduces a\"}"}
{"id": "eJhc_CPXQIT", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: The GraphVLM model: We utilize two different tokenizers for entities and contexts, and a task specific head for each level of the MOMA hierarchy.\\n\\nNew annotation schema that can be easily converted from natural language to graphical annotations to enable few-shot learning, and a new framework (GraphVLM) to evaluate VLMs on activity parsing.\\n\\nEthics:\\nPrior research [93] shows that Youtube videos exhibit geographic bias. To mitigate potential ethical issues associated with the dataset, we have adopted the following protocols: (1) Taxonomy selection: We carefully selected each activity class in taxonomy to ensure they are gender-neutral, culturally inclusive, and friendly toward people from different socioeconomic backgrounds. (2) Video selection: A team of diverse researchers from different ethnicities and genders selects, examines, and discusses each video to ensure it is diverse and does not contain offensive content. We used keywords from multiple different languages (including but not limited to English, Chinese, French, and Japanese) and word choices to search for videos. Diversification of the videos not only enhances the robustness and generalization of our models, but also significantly reduces the potential bias in the dataset.\\n\\n4 Activity Parsing and the GraphVLM Model\\n\\nIn this section, we introduce a method for performing activity parsing and provide a transfer learning framework to adapt Video-Language Models (VLMs) to activity parsing.\\n\\n4.1 Activity Parsing\\nWe define activity parsing as the task of generating an activity graph as defined in Section 3.1. Specifically, given a video as input, an activity parsing model (1) returns an activity class label, (2) temporally localizes and classifies all sub-activities, as well as (3) localizes each entity in the scene. Following this, it will need to detect all predicates: i.e. all unary predicates (i.e., attributes) involving a single entity and all binary predicates (i.e. relationships) which are between pairs of distinct entities. Refer to Figure 1 for an example of the end results of activity parsing.\\n\\n4.2 GraphVLM: Video Stream\\nOur video stream module consists of two tokenizers and an encoder for each level of the activity hierarchy.\\n\\nTokenization.\\nThe first tokenizer is the context tokenizer, which consists of a clip sampler and a clip feature extractor. The clip sampler takes a video as input and samples non-overlapping short video clips. It has two parameters: the number of sampled frames \\\\( T \\\\) and temporal stride \\\\( \\\\tau_c \\\\), meaning that each sampled clip consists of \\\\( T \\\\times \\\\tau_c \\\\) total frames. For our clip feature extractor, we evaluate on a Swin-B [36], MViT-B [94], and SlowFast-R50 [32] pre-trained on Kinetics-400 [2]. The second tokenizer is the entity tokenizer, which consists of a frame sampler and an entity detector. The role of the frame sampler is to uniformly sample frames across the whole video, parameterized by \\\\( \\\\tau_e \\\\), where \\\\( \\\\tau_e \\\\) is the temporal stride (i.e. we sample one frame every \\\\( \\\\tau_e \\\\) frames). After frame sampling, we detect entities at the frame level and generate bounding boxes as well as ROI features associated with each.\"}"}
{"id": "eJhc_CPXQIT", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"entity, which we call the entity token. These two tokens are used to generate an entity-context token by applying the bounding boxes extracted from entity tokenization to the context tokens through ROIAlign.\\n\\nEntity detection.\\nTo detect entities and extract entity tokens for activity parsing, we use a Faster-RCNN [89] object detector with a ResNet-101 [95] FPN [96] backbone pre-trained on ImageNet [1]. We use maskrcnn-benchmark [2] for our implementation. For our activity parsing experiment, we treat all human role classes as a single class to facilitate downstream predicate classification.\\n\\nSeparately, we also experiment with object detection using the role classes in our dataset. We use Detectron2 [3] for actor role detection, and use pre-trained weights from COCO keypoints [97], with the same architecture described above.\\n\\nActivity encoding.\\nThe video encoder for activity videos sparsely samples N context tokens produced by the context tokenizer and performs a mean pool to get the activity feature. This encoding works both with and without a text stream: using the features, we can train an action classifier using a cross-entropy loss exclusively on the features or train jointly with the text stream utilizing a contrastive loss.\\n\\nSub-activity encoding.\\nThe video encoder for sub-activity videos densely samples N context tokens which are used to run either temporal action detection or segmentation. For temporal action detection, we input the context tokens into a G-tad [41] model and train a model to predict temporal boundaries using a cross-entropy loss. For temporal action segmentation, the encoding is flexible to work with and without the text stream: we can train a classifier with cross-entropy loss using only the features and classify each token as belonging to a sub-activity class or a background class and also train jointly with a text stream using a contrastive loss.\\n\\nAtomic action encoding.\\nAtomic action encoding consists of two parts: per-frame scene graph generation and spatio-temporal atomic action segmentation. For scene graph generation, relationships are grounded over all entities in the scene, whereas spatio-temporal atomic action segmentation is actor-centric and only considers a single actor at a time. We use entity tokens (i.e. object labels, bounding boxes, and ROI features) as input for scene graph detection. We train a RelDN [98] model for scene graph detection using Microsoft's Scene Graph Benchmark [4] for our implementation. We evaluate our model on the tasks of predicate classification, scene graph classification, and scene graph detection without graphical constraints as in Xu et al. [99], since the MOMA-LRG dataset often contains multiple relationships for a given source and target entity. For spatio-temporal atomic action segmentation, the sequence of entity-context tokens of an actor is taken as input, and the model outputs frame-level predicate labels for the actor in a multi-label classification setting. In our implementation, we train a single-layer classifier with a sigmoid activation function, though we note that our framework is compatible with using the generated natural language predicate sentences as supervision via contrastive learning.\\n\\n4.3 GraphVLM: Text Stream\\nIn order to effectively leverage the natural language capabilities of VLMs, we convert all levels of the MOMA-LRG activity graph hierarchy to natural language via our graph-to-language module.\\n\\nGraph-to-language module.\\nAt the activity level, each class name is a noun, thus it can be represented by its class name or via prompting (e.g. by prepending \\\"A video of [CLS_NAME]\\\").\\n\\nAt the sub-activity level, class names are descriptions of the sub-activities in the present continuous tense (narration-style). At the atomic action level, we tag all predicates with [src] and [trg] templates to allow for easy conversion into a full grammatically correct sentence in its present continuous form. For example, the predicate touching is represented as [src] touching [trg].\\n\\nSo, given the entities [src]=person and [trg]=table, the sentence is A person is touching the table.\\n\\nText encoding.\\nAfter converting the associated activity graph level to language, we use a pre-trained language model to encode the text. When evaluating existing VLMs (e.g. VideoCLIP [12], FiT [8])...\"}"}
{"id": "eJhc_CPXQIT", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Detection of sub-activities in activity videos with temporal action detection. AP is reported at thresholds 0.1, 0.3, and 0.5 for different backbones.\\n\\n| Temporal Detection | Backbones | Pre-train | AP@0.1 | AP@0.3 | AP@0.5 |\\n|--------------------|-----------|-----------|---------|---------|---------|\\n| MVIT-B K-400       |           | 17.906    | 9.369   | 5.107   |\\n| SlowFast-R50 K-400 |           | 21.797    | 11.782  | 4.904   |\\n| Swin-B K-400       |           | 22.102    | 10.853  | 4.860   |\\n\\nTable 4: Activity and sub-activity video classification. Results are reported for activity and sub-activity classification with different video backbones.\\n\\n| Activity Sub-activity | Model | T \u00d7 \u03c4 | Pre-train | acc@1 | acc@5 | acc@1 | acc@5 |\\n|-----------------------|-------|-------|-----------|-------|-------|-------|-------|\\n|                       | MVIT-B | 16 \u00d7 4 | Kinetics-400 | 0.7731 | 0.9468 | 0.6032 | 0.9473 |\\n|                       | MVIT-B | 16 \u00d7 4 | None      | 0.5140 | 0.8010 | 0.4375 | 0.7500 |\\n|                       | SlowFast-R50 | 8 \u00d7 8 | Kinetics-400 | 0.7569 | 0.9375 | 0.5625 | 0.9226 |\\n|                       | SlowFast-R50 | 8 \u00d7 8 | None      | 0.4375 | 0.7500 | 0.3739 | 0.7731 |\\n|                       | Swin-B   | 4 \u00d7 3  | Kinetics-400 | 0.8576 | 0.9688 | 0.6450 | 0.9781 |\\n|                       | Swin-B   | 4 \u00d7 3  | None      | 0.5282 | 0.8415 | 0.3817 | 0.7868 |\\n|                       | GCN      | 30 \u00d7 1 | None      | 0.7837 | 0.9539 | 0.3829 | 0.8276 |\\n|                       | GCN (oracle bbox) | 30 \u00d7 1 | None      | 0.9502 | 0.9964 | 0.5630 | 0.9706 |\\n\\nUsing our framework, we use their respective text encoders. For our model agnostic use-case, we use bert-base-uncased [7].\\n\\n4.4 GraphVLM: Few-shot and Transfer Learning\\nMOMA-LRG includes a few-shot split, which splits the MOMA dataset into non-overlapping activity and sub-activity classes. The few-shot training set contains 10 activity classes and 45 sub-activity classes, the validation set contains 5 activity classes and 24 sub-activity classes, and the test set contains 5 activity classes and 22 sub-activity classes. For our baseline methods, we report results using two meta-learning classifiers, OTAM [100] and CMN [101]. To evaluate the performance of video-language models in the few shot setting, we perform out-of-the-box classification for a pre-trained VideoCLIP [12] and Frozen-In-Time [8] video-language model on activity and sub-activity classification on the meta-test set. We use class names as text (either activity or sub-activity) and raw videos as input. To compute the class label for a video input, we find the text embedding that is closest in dot product similarity to the video embedding as in Xu et al. [12]. We visualize the performance of this method on the regular MOMA-LRG test set in Figure 4. For \\\\( k \\\\)-shot video classification, we sample \\\\( k \\\\) videos per class and average the representation to obtain a prototype video. We compute a weighted average between the text embedding and the video prototype and classify using the same method as in the zero-shot setting. Details for our explanation and an ablation study of our method can be found in the Appendix.\\n\\nIn addition to our framework for evaluating VLMs without training, we also propose a method for using VLMs for activity parsing in a more flexible manner than full fine-tuning. We use knowledge distillation to incorporate visual and linguistic knowledge from VLMs into the activity parsing framework. This is considerably more flexible than full fine-tuning since it is model-agnostic. In this method, we can use a different backbone network than that used by the VLMs, and can sample clips differently so long as the clips from the student and the teacher model are centered at the same frame. We report results using our framework and investigate the effect of incorporating linguistic information for spatio-temporal atomic action segmentation in Table 5. Details for our approach are explained in the Appendix.\\n\\n5 Activity Parsing Evaluation\\nIn this section, we evaluate our dataset on methods across two different tasks. First, we evaluate model performance on the activity parsing task which leverages the hierarchy of our dataset. Next, we examine our method on activity parsing in the few-shot setting.\\n\\nWe evaluate each level of activity parsing using the following metrics.\\n\\nActivity: The performance of activity recognition is measured by the top 1 accuracy (acc@1) and top 5 accuracy (acc@5) for video-level activity classification. Results are shown in Table 4 for the standard setting and Table 6 for the few-shot setting.\\n\\nSub-activity: The performance of sub-activity recognition is measured on two tasks. First, we report the sub-activity acc@1 and acc@5, where the pre-segmented sub-activity video is used as input. Results are shown in Table 4 for the standard setting and Table 6 for the few-shot setting. Second, we evaluate temporal detection using mAP at thresholds 0.1, 0.3, and 0.5 and report the average mAP following [40]. Results are shown in Table 3.\"}"}
{"id": "eJhc_CPXQIT", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Scene graph detection, entity detection, and spatio-temporal atomic action segmentation results from the methods described in Section 4.2. The entity detection results pose challenges for scene graph detection, as is evidenced by the relatively higher scores for SGCls and PredCls, where ground truth bounding boxes and class labels are known.\\n\\n### Scene Graph Detection\\n\\n| Method      | Recall@20 | Recall@50 | Recall@100 |\\n|-------------|-----------|-----------|------------|\\n| PredCls     | 58.2243   | 62.4389   | 64.0983    |\\n| SGCls       | 44.3065   | 48.3825   | 50.2992    |\\n| SGDets      | 37.6275   | 43.8594   | 47.9960    |\\n\\n### Entity Detection\\n\\n| Role          | AP         | AP50       | AP75       | APs        | APm        | AP1        |\\n|---------------|------------|------------|------------|------------|------------|------------|\\n| Actor role    | 38.3567    | 58.1256    | 41.2369    | 7.8053     | 19.4897    | 40.0392    |\\n| Entity        | 15.2896    | 29.3032    | 14.0742    | 4.3134     | 10.1288    | 17.1472    |\\n\\n### Spatial-Temporal Segmentation\\n\\n| Model         | Video only | Video + text stream |\\n|---------------|------------|---------------------|\\n| MViT-B        | 0.2130     | 0.2353              |\\n| SlowFast-R50  | 0.1975     | 0.2023              |\\n\\nFigure 4: A confusion matrix for zero-shot classification of VideoCLIP [12] on the standard MOMA-LRG sub-activity test set. The sub-activities are ordered to be adjacent to other sub-activities within the same activity. As is indicated by the green squares and the results in Table 6, there is a significant degree of within-activity confusion for zero-shot video language models.\\n\\nTable 6: Low-shot video classification. We evaluate both VideoCLIP [12] and Frozen-in-Time [8] within our few-shot framework for activity and sub-activity classification. We note that although the video-language models we tested performed well for high-level activity classification, they performed significantly worse for the more granular task of sub-activity classification.\\n\\n| Model       | 0-shot | 1-shot | 5-shot | 0-shot | 1-shot | 5-shot |\\n|-------------|--------|--------|--------|--------|--------|--------|\\n| OTAM [100]  | -      | 80.71  | 92.07  | -      | 57.14  | 72.59  |\\n| CMN [101]   | -      | 73.57  | 86.30  | -      | 52.30  | 66.60  |\\n| VideoCLIP [12] | 75.90  | 84.40  | 84.80  | 30.80  | 32.70  | 32.70  |\\n| Frozen [8]  | 90.80  | 92.30  | 92.50  | 19.10  | 26.50  | 26.30  |\\n\\nAtomic action: The performance of atomic action recognition is measured on two tasks. First, we evaluate entity detection using standard average precision (AP) metrics. To evaluate scene graph generation, we follow work in [83, 19, 99] and perform the following tasks: predicate classification (PredCls), which takes ground truth bounding boxes and object categories as input and returns predicate labels, scene graph classification (SGCls) which only takes in ground truth bounding boxes as input and predicts object categories and predicate labels, and scene graph detection (SGDet) which simply takes in an input image and predicts bounding box locations, object categories, and predicate labels. Results are shown in Table 5.\\n\\n6 Conclusion\\n\\nWe introduce the MOMA-LRG dataset, a large activity recognition dataset of complex human activities that enables the evaluation and fine-tuning of large, generalizable video-language models. We define activity parsing as the overarching task of activity graph generation, requiring video understanding at multiple levels of granularity. We demonstrate the capacity of MOMA-LRG to train video-language models by introducing a model-agnostic and lightweight approach for adaptation, and we evaluate VLMs by demonstrating strong few-shot classification performance. We hope that MOMA-LRG will enable further research into generalizable activity recognition models that are trained with multiple input modalities or generate language descriptions for videos.\\n\\nAcknowledgement\\n\\nThis work was partially supported by the Schmidt Futures, Toyota Research Institute (TRI), the Stanford Institute for Human-Centered AI (HAI), Panasonic, ONR MURI N00014-21-1-2801, ONR MURI N00014-22-1-2740, National Science Foundation grant 2026498, and Bosch, Salesforce, and Samsung. Last but not least, we would like to express our sincere gratitude to Shyamal Buch and Yuliang Zou for their discussions and constructive feedback.\"}"}
{"id": "eJhc_CPXQIT", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. IEEE, 2009.\\n\\n[2] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset, 2017.\\n\\n[3] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\\n\\n[4] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021.\\n\\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\n[6] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821\u20138831. PMLR, 2021.\\n\\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n\\n[8] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u20131738, 2021.\\n\\n[9] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In ICCV, 2019.\\n\\n[10] Hu Xu, Gargi Ghosh, Po-Yao Huang, Prahal Arora, Masoumeh Aminzadeh, Christoph Feichtenhofer, Florian Metze, and Luke Zettlemoyer. VLM: Task-agnostic video-language model pre-training for video understanding. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4227\u20134239, 2021.\\n\\n[11] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016.\\n\\n[12] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6787\u20136800, 2021.\\n\\n[13] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing Systems, 34, 2021.\\n\\n[14] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. HERO: Hierarchical encoder for video+language omni-representation pre-training. arXiv preprint arXiv:2005.00200, 2020.\"}"}
{"id": "eJhc_CPXQIT", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[11] Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 961\u2013970, 2015.\\n\\n[12] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The \u201csomething something\u201d video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 5842\u20135850, 2017.\\n\\n[13] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-to-End Learning of Visual Representations from Uncurated Instructional Videos. In CVPR, 2020.\\n\\n[14] Chen Sun, Austin Myers, Carl VonDrlick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for video and language representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.\\n\\n[15] Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan Carlos Niebles. Action genome: Actions as compositions of spatio-temporal scene graphs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10236\u201310247, 2020.\\n\\n[16] Zelun Luo, Wanze Xie, Siddharth Kapoor, Yiyun Liang, Michael Cooper, Juan Carlos Niebles, Ehsan Adeli, and Fei-Fei Li. Moma: Multi-object multi-actor activity parsing. Advances in Neural Information Processing Systems, 34, 2021.\\n\\n[17] Nishant Rai, Haofeng Chen, Jingwei Ji, Rishi Desai, Kazuki Kozuka, Shun Ishizaka, Ehsan Adeli, and Juan Carlos Niebles. Home action genome: Cooperative compositional action understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11184\u201311193, 2021.\\n\\n[18] Sungjin Park, Seongsu Bae, Jiho Kim, Tackeun Kim, and Edward Choi. Graph-text multi-modal pre-training for medical representation learning. In Conference on Health, Inference, and Learning, pages 261\u2013281. PMLR, 2022.\\n\\n[19] Hadas Kress-Gazit, Georgios E Fainekos, and George J Pappas. Translating structured english to robot controllers. Advanced Robotics, 22(12):1343\u20131359, 2008.\\n\\n[20] Calin Belta, Antonio Bicchi, Magnus Egerstedt, Emilio Frazzoli, Eric Klavins, and George J Pappas. Symbolic planning and control of robot motion [grand challenges of robotics]. IEEE Robotics & Automation Magazine, 14(1):61\u201370, 2007.\\n\\n[21] Chunhui Gu, Chen Sun, David A Ross, Carl VonDrlick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, et al. Ava: A video dataset of spatio-temporally localized atomic visual actions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6047\u20136056, 2018.\\n\\n[22] Ang Li, Meghana Thotakuri, David A. Ross, Jo\u00e3o Carreira, Alexander Vostrikov, and Andrew Zisserman. The ava-kinetics localized human actions video dataset. CoRR, abs/2005.00214, 2020.\\n\\n[23] Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: A hierarchical video dataset for fine-grained action understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2616\u20132625, 2020.\\n\\n[24] Yixuan Li, Lei Chen, Runyu He, Zhenzhi Wang, Gangshan Wu, and Limin Wang. Multisports: A multi-person video dataset of spatio-temporally localized sports actions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13536\u201313545, 2021.\\n\\n[25] Philippe Weinzaepfel, Xavier Martin, and Cordelia Schmid. Human action localization with sparse spatial supervision. arXiv preprint arXiv:1605.05197, 2016.\"}"}
{"id": "eJhc_CPXQIT", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kellie Corona, Katie Osterdahl, Roderic Collins, and Anthony Hoogs. Meva: A large-scale multiview, multimodal video dataset for activity detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 1060\u20131068, January 2021.\\n\\nSrikanth Malla, Behzad Dariush, and Chiho Choi. TITAN: future forecast using action priors. CoRR, abs/2003.13886, 2020.\\n\\nChristoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6202\u20136211, 2019.\\n\\nJoao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299\u20136308, 2017.\\n\\nChristoph Feichtenhofer. X3d: Expanding architectures for efficient video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 203\u2013213, 2020.\\n\\nYanyi Zhang, Xinyu Li, Chunhui Liu, Bing Shuai, Yi Zhu, Biagio Brattoli, Hao Chen, Ivan Marsic, and Joseph Tighe. Vidtr: Video transformer without convolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13577\u201313587, 2021.\\n\\nZe Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. arXiv preprint arXiv:2106.13230, 2021.\\n\\nAnurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu \u02c7ci\u00b4c, and Cordelia Schmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6836\u20136846, 2021.\\n\\nGedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding. arXiv preprint arXiv:2102.05095, 2(3):4, 2021.\\n\\nRoei Herzig, Elad Ben-Avraham, Karttikeya Mangalam, Amir Bar, Gal Chechik, Anna Rohrbach, Trevor Darrell, and Amir Globerson. Object-region video transformers. ArXiv, abs/2110.06915, 2021.\\n\\nHaroon Idrees, Amir R Zamir, Yu-Gang Jiang, Alex Gorban, Ivan Laptev, Rahul Sukthankar, and Mubarak Shah. The thumos challenge on action recognition for videos \u201cin the wild\u201d. Computer Vision and Image Understanding, 155:1\u201323, 2017.\\n\\nMengmeng Xu, Chen Zhao, David S Rojas, Ali Thabet, and Bernard Ghanem. G-tad: Subgraph localization for temporal action detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10156\u201310165, 2020.\\n\\nTianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen. Bmn: Boundary-matching network for temporal action proposal generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3889\u20133898, 2019.\\n\\nZheng Shou, Dongang Wang, and Shih-Fu Chang. Temporal action localization in untrimmed videos via multi-stage cnns. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1049\u20131058, 2016.\\n\\nYuanjun Xiong, Yue Zhao, Limin Wang, Dahua Lin, and Xiaoou Tang. A pursuit of temporal accuracy in general activity detection. arXiv preprint arXiv:1703.02716, 2017.\\n\\nJiyang Gao, Zhenheng Yang, Kan Chen, Chen Sun, and Ram Nevatia. Turn tap: Temporal unit regression network for temporal action proposals. In Proceedings of the IEEE international conference on computer vision, pages 3628\u20133636, 2017.\\n\\nHumam Alwassel, Fabian Caba Heilbron, and Bernard Ghanem. Action search: Spotting actions in videos and its application to temporal action localization. In Proceedings of the European Conference on Computer Vision (ECCV), pages 251\u2013266, 2018.\"}"}
{"id": "eJhc_CPXQIT", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shyamal Buch, Victor Escorcia, Bernard Ghanem, Li Fei-Fei, and Juan Carlos Niebles. End-to-end, single-stream temporal action detection in untrimmed videos. In Proceedings of the British Machine Vision Conference 2017. British Machine Vision Association, 2019.\\n\\nChao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krahenbuhl, and Ross Girshick. Long-term feature banks for detailed video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\\n\\nYutong Feng, Jianwen Jiang, Ziyuan Huang, Zhiwu Qing, Xiang Wang, Shiwei Zhang, Mingqian Tang, and Yue Gao. Relation modeling in spatio-temporal action localization. arXiv preprint arXiv:2106.08061, 2021.\\n\\nJunting Pan, Siyu Chen, Zheng Shou, Jing Shao, and Hongsheng Li. Actor-context-actor relation network for spatio-temporal action localization. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 464\u2013474, 2021.\\n\\nHassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. In Advances in Neural Information Processing Systems, 2021.\\n\\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021.\\n\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586, 2021.\\n\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In ICML, 2019.\\n\\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582\u20134597, 2021.\\n\\nZhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935\u20132947, 2017.\\n\\nWei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at few-shot classification. In International Conference on Learning Representations, 2018.\\n\\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks. arXiv preprint arXiv:2112.06825, 2021.\\n\\nPeng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. arXiv preprint arXiv:2110.04544, 2021.\\n\\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. arXiv preprint arXiv:2109.01134, 2021.\\n\\nYuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Cpt: Colorful prompt tuning for pre-trained vision-language models. arXiv preprint arXiv:2109.11797, 2021.\\n\\nChen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi Xie. Prompting visual-language models for efficient video understanding. arXiv preprint arXiv:2112.04478, 2021.\\n\\nJingen Liu, Benjamin Kuipers, and Silvio Savarese. Recognizing human actions by attributes. In CVPR 2011, pages 3337\u20133344. IEEE, 2011.\"}"}
{"id": "eJhc_CPXQIT", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yanwei Fu, Timothy M Hospedales, Tao Xiang, and Shaogang Gong. Attribute learning for understanding unstructured social activity. In European Conference on Computer Vision, pages 530\u2013543. Springer, 2012.\\n\\nBiagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka. Re-thinking zero-shot video classification: End-to-end training for realistic applications. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4613\u20134623, 2020.\\n\\nMina Bishay, Georgios Zoumpourlis, and Ioannis Patras. Tarn: Temporal attentive relation network for few-shot and zero-shot action recognition. arXiv preprint arXiv:1907.09021, 2019.\\n\\nIoannis Alexiou, Tao Xiang, and Shaogang Gong. Exploring synonyms as context in zero-shot action recognition. In 2016 IEEE International Conference on Image Processing (ICIP), pages 4190\u20134194. IEEE, 2016.\\n\\nAJ Piergiovanni and Michael S Ryoo. Learning shared multimodal embeddings with unpaired data. CoRR, 2018.\\n\\nBowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and hierarchical modeling of video and text. In Proceedings of the European Conference on Computer Vision (ECCV), pages 374\u2013390, 2018.\\n\\nPallabi Ghosh, Nirat Saini, Larry S Davis, and Abhinav Shrivastava. All about knowledge graphs for actions. arXiv preprint arXiv:2008.12432, 2020.\\n\\nDevraj Mandal, Sanath Narayan, Sai Kumar Dwivedi, Vikram Gupta, Shuaib Ahmed, Fahad Shahbaz Khan, and Ling Shao. Out-of-distribution detection for generalized zero-shot action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9985\u20139993, 2019.\\n\\nValter Estevam, Helio Pedrini, and David Menotti. Zero-shot action recognition in videos: A survey. Neurocomputing, 439:159\u2013175, 2021.\\n\\nHan-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Few-shot learning via embedding adaptation with set-to-set functions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8808\u20138817, 2020.\\n\\nChi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Few-shot image classification with differentiable earth mover's distance and structured classifiers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12203\u201312213, 2020.\\n\\nKaidi Cao, Jingwei Ji, Zhangjie Cao, Chien-Yi Chang, and Juan Carlos Niebles. Few-shot video classification via temporal alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10618\u201310627, 2020.\\n\\nLinchoo Zhu and Yi Yang. Label independent memory for semi-supervised few-shot video classification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(1):273\u2013285, 2022.\\n\\nChris Careaga, Brian Hutchinson, Nathan Hodas, and Lawrence Phillips. Metric-based few-shot learning for video action recognition. arXiv preprint arXiv:1909.09602, 2019.\\n\\nZhenxi Zhu, Limin Wang, Sheng Guo, and Gangshan Wu. A closer look at few-shot video classification: A new baseline and benchmark. arXiv preprint arXiv:2110.12358, 2021.\\n\\nXiatian Zhu, Antoine Toisoul, Juan-Manuel Perez-Rua, Li Zhang, Brais Martinez, and Tao Xiang. Few-shot action recognition with prototype-centered attentive learning. arXiv preprint arXiv:2101.08085, 2021.\\n\\nHongguang Zhang, Li Zhang, Xiaojuan Qi, Hongdong Li, Philip HS Torr, and Piotr Koniusz. Few-shot action recognition with permutation-invariant attention. In European Conference on Computer Vision, pages 525\u2013542. Springer, 2020.\"}"}
{"id": "eJhc_CPXQIT", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706\u2013715, 2017.\\n\\nLuowei Zhou, Yannis Kalantidis, Xinlei Chen, Jason J Corso, and Marcus Rohrbach. Grounded video description. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6578\u20136587, 2019.\\n\\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):32\u201373, 2017.\\n\\nFei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-vil: Knowledge enhanced vision-language representations through scene graphs. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 3208\u20133216, 2021.\\n\\nShizhe Chen, Yida Zhao, Qin Jin, and Qi Wu. Fine-grained video-text retrieval with hierarchical graph reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10638\u201310647, 2020.\\n\\nAmir Bar, Roei Herzig, Xiaolong Wang, Anna Rohrbach, Gal Chechik, Trevor Darrell, and Amir Globerson. Compositional video synthesis with action graphs. arXiv preprint arXiv:2006.15327, 2020.\\n\\nRoei Herzig, Elad Levi, Huijuan Xu, Hang Gao, Eli Brosh, Xiaolong Wang, Amir Globerson, and Trevor Darrell. Spatio-temporal action graph networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pages 0\u20130, 2019.\\n\\nXiaolong Wang and Abhinav Gupta. Videos as space-time region graphs. In Proceedings of the European conference on computer vision (ECCV), pages 399\u2013417, 2018.\\n\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.\\n\\nLuowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from web instructional videos. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.\\n\\nVignesh Ramanathan, Bangpeng Yao, and Li Fei-Fei. Social role discovery in human events. 2013 IEEE Conference on Computer Vision and Pattern Recognition, pages 2475\u20132482, 2013.\\n\\nAmy Fire and Song-Chun Zhu. Learning perceptual causality from video. ACM Transactions on Intelligent Systems and Technology (TIST), 7(2):1\u201322, 2015.\\n\\nAnders Brodersen, Salvatore Scellato, and Mirjam Wattenhofer. Youtube around the world: Geographic popularity of videos. In Proceedings of the 21st international conference on World Wide Web, pages 241\u2013250, 2012.\\n\\nHaoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6824\u20136835, 2021.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\\n\\nTsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2117\u20132125, 2017.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.\\n\\n16\"}"}
{"id": "eJhc_CPXQIT", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ji Zhang, Kevin J Shih, Ahmed Elgammal, Andrew Tao, and Bryan Catanzaro. Graphical contrastive losses for scene graph parsing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11535\u201311543, 2019.\\n\\nDanfei Xu, Yuke Zhu, Christopher Bongsoo Choy, and Li Fei-Fei. Scene graph generation by iterative message passing. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3097\u20133106, 2017.\\n\\nKaidi Cao, Jingwei Ji, Zhangjie Cao, Chien-Yi Chang, and Juan Carlos Niebles. Few-shot video classification via temporal alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10618\u201310627, 2020.\\n\\nLinchao Zhu and Yi Yang. Compound memory networks for few-shot video classification. In Proceedings of the European Conference on Computer Vision (ECCV), pages 751\u2013766, 2018.\"}"}
