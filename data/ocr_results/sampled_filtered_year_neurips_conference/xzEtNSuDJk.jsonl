{"id": "xzEtNSuDJk", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning\\n\\n\u2020 Bo Liu \u21e4, \u2020 Yifeng Zhu \u21e4, \u2021 Chongkai Gao \u21e4, \u2020 Yihao Feng \u2020 Qiang Liu, \u2020 Yuke Zhu, \u2020\u00a7 Peter Stone\\n\\nThe University of Texas at Austin, \u00a7 Sony AI, \u2021 Tsinghua University\\n\\n{bliu,yifengz,lqiang,yukez,pstone}@cs.utexas.edu\\nyihao.ac@gmail.com, gck20@mails.tsinghua.edu.cn\\n\\nAbstract\\nLifelong learning offers a promising paradigm of building a generalist agent that learns and adapts over its lifespan. Unlike traditional lifelong learning problems in image and text domains, which primarily involve the transfer of declarative knowledge of entities and concepts, lifelong learning in decision-making (LLDM) also necessitates the transfer of procedural knowledge, such as actions and behaviors. To advance research in LLDM, we introduce LIBERO, a novel benchmark of lifelong learning for robot manipulation. Specifically, LIBERO highlights five key research topics in LLDM:\\n\\n1) how to efficiently transfer declarative knowledge, procedural knowledge, or the mixture of both;\\n2) how to design effective policy architectures and effective algorithms for LLDM;\\n3) the robustness of a lifelong learner with respect to task ordering;\\n4) the effect of model pretraining for LLDM.\\n\\nWe develop an extendible procedural generation pipeline that can in principle generate infinitely many tasks. For benchmarking purpose, we create four task suites (130 tasks in total) that we use to investigate the above-mentioned research topics. To support sample-efficient learning, we provide high-quality human-teleoperated demonstration data for all tasks. Our extensive experiments present several insightful or even unexpected discoveries: sequential finetuning outperforms existing lifelong learning methods in forward transfer, no single visual encoder architecture excels at all types of knowledge transfer, and naive supervised pretraining can hinder agents' performance in the subsequent LLDM.\\n\\nIntroduction\\nA longstanding goal in machine learning is to develop a generalist agent that can perform a wide range of tasks. While multitask learning [10] is one approach, it is computationally demanding and not adaptable to ongoing changes. Lifelong learning [65], however, offers a practical solution by amortizing the learning process over the agent's lifespan. Its goal is to leverage prior knowledge to facilitate learning new tasks (forward transfer) and use the newly acquired knowledge to enhance performance on prior tasks (backward transfer).\\n\\nThe main body of the lifelong learning literature has focused on how agents transfer declarative knowledge in visual or language tasks, which pertains to declarative knowledge about entities and concepts [7, 40]. Yet it is understudied how agents transfer knowledge in decision-making tasks, which involves a mixture of both declarative and procedural knowledge (knowledge about how to do something). Consider a scenario where a robot, initially trained to retrieve juice from a fridge, fails...\"}"}
{"id": "xzEtNSuDJk", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Procedural Generation Involve declarative knowledge Involve procedural knowledge\\n\\nLIBERO-Object\\n\\nLIBERO-Spatial\\n\\nLIBERO-Goal\\n\\nLIBERO-100\\n\\nDifferent layouts, same objects\\n\\nDifferent objects, same layout\\n\\nDifferent goals, same objects & layout\\n\\nDiverse objects, layouts, backgrounds\\n\\nPretraining Effects Task Orderings Neural Architectures Algorithmic Designs Distribution Shifts\\n\\nFigure 1: Top: LIBERO has four procedurally-generated task suites: LIBERO-Spatial, LIBERO-Object, and LIBERO-Goal have 10 tasks each and require transferring knowledge about spatial relationships, objects, and task goals; LIBERO-100 has 100 tasks and requires the transfer of entangled knowledge. Bottom: we investigate five key research topics in LLDM on LIBERO.\\n\\nafter learning new tasks. This could be due to forgetting the juice or fridge's location (declarative knowledge) or how to open the fridge or grasp the juice (procedural knowledge). So far, we lack methods to systematically and quantitatively analyze this complex knowledge transfer.\\n\\nTo bridge this research gap, this paper introduces a new simulation benchmark, LIfelong learning BEchmark on RObot manipulation tasks, LIBERO, to facilitate the systematic study of lifelong learning in decision making (LLDM). An ideal LLDM testbed should enable continuous learning across an expanding set of diverse tasks that share concepts and actions. LIBERO supports this through a procedural generation pipeline for endless task creation, based on robot manipulation tasks with shared visual concepts (declarative knowledge) and interactions (procedural knowledge).\\n\\nFor benchmarking purpose, LIBERO generates 130 language-conditioned robot manipulation tasks inspired by human activities [22] and, grouped into four suites. The four task suites are designed to examine distribution shifts in the object types, the spatial arrangement of objects, the task goals, or the mixture of the previous three (top row of Figure 1).\\n\\nLIBERO is scalable, extendable, and designed explicitly for studying lifelong learning in robot manipulation. To support efficient learning, we provide high-quality, human-teleoperated demonstration data for all 130 tasks.\\n\\nWe present an initial study using LIBERO to investigate five major research topics in LLDM (Figure 1):\\n\\n1) knowledge transfer with different types of distribution shift;\\n2) neural architecture design;\\n3) lifelong learning algorithm design;\\n4) robustness of the learner to task ordering; and\\n5) how to leverage pre-trained models in LLDM (bottom row of Figure 1).\\n\\nWe perform extensive experiments across different policy architectures and different lifelong learning algorithms. Based on our experiments, we make several insightful or even unexpected observations:\\n\\n1. Policy architecture design is as crucial as lifelong learning algorithms. The transformer architecture is better at abstracting temporal information than a recurrent neural network. Vision transformers work well on tasks with rich visual information (e.g., a variety of objects). Convolution networks work well when tasks primarily need procedural knowledge.\\n2. While the lifelong learning algorithms we evaluated are effective at preventing forgetting, they generally perform worse than sequential finetuning in terms of forward transfer.\\n3. Our experiment shows that using pretrained language embeddings of semantically-rich task descriptions yields performance no better than using those of the task IDs.\"}"}
{"id": "xzEtNSuDJk", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. Basic supervised pretraining on a large-scale offline dataset can have a negative impact on the learner's downstream performance in LLDM.\\n\\n2 Background\\n\\nThis section introduces the problem formulation and defines key terms used throughout the paper.\\n\\n2.1 Markov Decision Process for Robot Learning\\n\\nA robot learning problem can be formulated as a finite-horizon Markov Decision Process: \\\\( M = (S, A, T, H, \\\\mu_0, R) \\\\). Here, \\\\( S \\\\) and \\\\( A \\\\) are the state and action spaces of the robot. \\\\( \\\\mu_0 \\\\) is the initial state distribution, \\\\( R: S \\\\times A \\\\to R \\\\) is the reward function, and \\\\( T: S \\\\times A \\\\to S \\\\) is the transition function. In this work, we assume a sparse-reward setting and replace \\\\( R \\\\) with a goal predicate \\\\( g: S \\\\to \\\\{0, 1\\\\} \\\\). The robot's objective is to learn a policy \\\\( \\\\pi \\\\) that maximizes the expected return: \\\\( \\\\max \\\\pi \\\\mathcal{J}(\\\\pi) = \\\\mathbb{E}_{s_t, a_t \\\\sim \\\\pi, \\\\mu_0} \\\\left[ \\\\prod_{t=1}^{H} g(s_t) \\\\right] \\\\).\\n\\n2.2 Lifelong Robot Learning Problem\\n\\nIn a lifelong robot learning problem, a robot sequentially learns over \\\\( K \\\\) tasks \\\\( \\\\{T_1, \\\\ldots, T_K\\\\} \\\\) with a single policy \\\\( \\\\pi \\\\). We assume \\\\( \\\\pi \\\\) is conditioned on the task, i.e., \\\\( \\\\pi(s|\\\\cdot; T) \\\\). For each task, \\\\( T_k = \\\\|= (\\\\mu_0, g_k) \\\\) is defined by the initial state distribution \\\\( \\\\mu_0 \\\\) and the goal predicate \\\\( g_k \\\\).\\n\\n3 Research Topics in LLDM\\n\\nWe outline five major research topics in LLDM that motivate the design of LIBERO and our study.\\n\\n(T1) Transfer of Different Types of Knowledge\\n\\nIn order to accomplish a task such as \\\"put the ketchup next to the plate in the basket\\\", a robot must understand the concept of ketchup, the location of the plate/basket, and how to put the ketchup in the basket. Indeed, robot manipulation tasks in general necessitate different types of knowledge, making it hard to determine the cause of failure. We present four task suites in Section 4.2: three task suites for studying the transfer of knowledge about spatial relationships, object concepts, and task goals in a disentangled manner, and one suite for studying the transfer of mixed types of knowledge.\\n\\n3 Throughout the paper, a superscript/subscript is used to index the task/time step.\"}"}
{"id": "xzEtNSuDJk", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: LIBERO\u2019s procedural generation pipeline: Extracting behavioral templates from a large-scale human activity dataset (1), Ego4D, for generating task instructions (2); Based on the task description, selecting the scene and generating the PDDL description file (3) that specifies the objects (A), the initial object configurations (B), and the task goal (C).\\n\\nNeural Architecture Design\\nAn important research question in LLDM is how to design effective neural architectures to abstract the multi-modal observations (images, language descriptions, and robot states) and transfer only relevant knowledge when learning new tasks.\\n\\nLifelong Learning Algorithm Design\\nGiven a policy architecture, it is crucial to determine what learning algorithms to apply for LLDM. Specifically, the sequential nature of LLDM suggests that even minor forgetting over successive steps can potentially lead to a total failure in execution. As such, we consider the design of lifelong learning algorithms to be an open area of research in LLDM.\\n\\nRobustness to Task Ordering\\nIt is well-known that task curriculum influences policy learning [6, 48]. A robot in the real world, however, often cannot choose which task to encounter first. Therefore, a good lifelong learning algorithm should be robust to different task orderings.\\n\\nUsage of Pretrained Models\\nIn practice, robots will be most likely pretrained on large datasets in factories before deployment [28]. However, it is not well-understood whether or how pretraining could benefit subsequent LLDM.\\n\\nLIBERO\\nThis section introduces the components in LIBERO: the procedural generation pipeline that allows the never-ending creation of tasks (Section 4.1), the four task suites we generate for benchmarking (Section 4.2), five algorithms (Section 4.3), and three neural architectures (Section 4.4).\\n\\n4.1 Procedural Generation of Tasks\\nResearch in LLDM requires a systematic way to create new tasks while maintaining task diversity and relevance to existing tasks. LIBERO procedurally generates new tasks in three steps: 1) extract behavioral templates from language annotations of human activities and generate sampled tasks described in natural language based on such templates; 2) specify an initial object distribution given a task description; and 3) specify task goals using a propositional formula that aligns with the language instructions. Our generation pipeline is built on top of Robosuite [76], a modular manipulation simulator that offers seamless integration. Figure 2 illustrates an example of task creation using this pipeline, and each component is expanded upon below.\\n\\nBehavioral Templates and Instruction Generation\\nHuman activities serve as a fertile source of tasks that can inspire and generate a vast number of manipulation tasks. We choose a large-scale activity dataset, Ego4D [22], which includes a large variety of everyday activities with language annotations. We pre-process the dataset by extracting the language descriptions and then summarize them into a large set of commonly used language templates. After this pre-processing step, we use the templates and select objects available in the simulator to generate a set of task descriptions in the...\"}"}
{"id": "xzEtNSuDJk", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"form of language instructions. For example, we can generate an instruction \\\"Open the drawer of the cabinet\\\" from the template \\\"Open ...\\n\\nInitial State Distribution ($\\\\mu_0$)\\n\\nTo specify $\\\\mu_0$, we first sample a scene layout that matches the objects/behaviors in a provided instruction. For instance, a kitchen scene is selected for an instruction \\\"Open the top drawer of the cabinet and put the bowl in it\\\". Then, the details about $\\\\mu_0$ are generated in the PDDL language [43, 63]. Concretely, $\\\\mu_0$ contains information about object categories and their placement (Figure 2-(A)), and their initial status (Figure 2-(B)).\\n\\nGoal Specifications ($g$)\\n\\nBased on $\\\\mu_0$ and the language instruction, we specify the task goal using a conjunction of predicates. Predicates include unary predicates that describe the properties of an object, such as $\\\\text{Open}(X)$ or $\\\\text{TurnOff}(X)$, and binary predicates that describe spatial relations between objects, such as $\\\\text{On}(A, B)$ or $\\\\text{In}(A, B)$. An example of the goal specification using PDDL language can be found in Figure 2-(C). The simulation terminates when all predicates are verified true.\\n\\n4.2 Task Suites\\n\\nWhile the pipeline in Section 4.1 supports the generation of an unlimited number of tasks, we offer fixed sets of tasks for benchmarking purposes. LIBERO has four task suites: LIBERO-S, LIBERO-O, LIBERO-G, and LIBERO-100. The first three task suites are curated to disentangle the transfer of declarative and procedural knowledge (as mentioned in (T1)), while LIBERO-100 is a suite of 100 tasks with entangled knowledge transfer.\\n\\nLIBERO-S, LIBERO-O, and LIBERO-G all have 10 tasks and are designed to investigate the controlled transfer of knowledge about spatial information (declarative), objects (declarative), and task goals (procedural). Specifically, all tasks in LIBERO-S request the robot to place a bowl, among the same set of objects, on a plate. But there are two identical bowls that differ only in their location or spatial relationship to other objects. Hence, to successfully complete LIBERO-S, the robot needs to continually learn and memorize new spatial relationships. All tasks in LIBERO-O request the robot to pick-place a unique object. Hence, to accomplish LIBERO-O, the robot needs to continually learn and memorize new object types. All tasks in LIBERO-G share the same objects with fixed spatial relationships but differ only in the task goal. Hence, to accomplish LIBERO-G, the robot needs to continually learn new knowledge about motions and behaviors. More details are in Appendix C.\\n\\nLIBERO-100 contains 100 tasks that entail diverse object interactions and versatile motor skills. In this paper, we split LIBERO-100 into 90 short-horizon tasks (LIBERO-90) and 10 long-horizon tasks (LIBERO-L). LIBERO-90 serves as the data source for pretraining and LIBERO-L for downstream evaluation of lifelong learning algorithms.\\n\\n4.3 Lifelong Learning Algorithms\\n\\nWe implement three representative lifelong learning algorithms to facilitate research in algorithmic design for LLDM. Specifically, we implement Experience Replay (ER) [13], Elastic Weight Consolidation (EWC) [33], and PACE [41]. We pick ER, EWC, and PACE because they correspond to the memory-based, regularization-based, and dynamic-architecture-based methods for lifelong learning. In addition, prior research [69] has discovered that they are state-of-the-art methods.\\n\\nBesides these three methods, we also implement sequential finetuning (SEQ) and multitask learning (MTL), which serve as a lower bound and upper bound for lifelong learning algorithms, respectively. More details about the algorithms are in Appendix B.1.\\n\\n4.4 Neural Network Architectures\\n\\nWe implement three vision-language policy networks, RESNET-RNN, RESNET-T, and VIT-T, that integrate visual, temporal, and linguistic information for LLDM. Language instructions of tasks are encoded using pretrained BERT embeddings [19]. The RESNET-RNN [42] uses a ResNet as the visual backbone that encodes per-step visual observations and an LSTM as the temporal backbone to process a sequence of encoded visual information. The language instruction is incorporated into the ResNet features using the FiLM method [50] and added to the LSTM inputs, respectively.\"}"}
{"id": "xzEtNSuDJk", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"architecture uses a similar ResNet-based visual backbone, but a transformer decoder as\\nthe temporal backbone to process outputs from ResNet, which are a temporal sequence of visual\\ntokens. The language embedding is treated as a separate token in inputs to the transformer alongside\\nthe visual tokens. The \\\\( \\\\text{ViT-T} \\\\) architecture, which is widely used in visual-language tasks, uses a\\nVision Transformer (ViT) as the visual backbone and a transformer decoder as the temporal backbone.\\nThe language embedding is treated as a separate token in inputs of both ViT and the transformer\\ndecoder. All the temporal backbones output a latent vector for every decision-making step. We\\ncompute the multi-modal distribution over manipulation actions using a Gaussian-Mixture-Model\\n(GMM) based output head. In the end, a robot executes a policy by sampling a continuous\\nvalue for end-effector action from the output distribution. Figure 6 visualizes the three architectures.\\n\\nFor all the lifelong learning algorithms and neural architectures, we use behavioral cloning (BC) to train policies for individual tasks (See (2)). BC allows for efficient policy learning such that we\\ncan study lifelong learning algorithms with limited computational resources. To train BC, we provide\\n50 trajectories of high-quality demonstrations for every single task in the generated task suites. The\\ndemonstrations are collected by human experts through teleoperation with 3Dconnexion Spacemouse.\\n\\nExperiments\\nExperiments are conducted as an initial study for the five research topics mentioned in Section 3. We\\nfirst introduce the evaluation metric used in experiments, and present analysis of empirical results in\\nLIBERO. The detailed experimental setup is in Appendix D. Our experiments focus on addressing\\nthe following research questions:\\n\\nQ1: How do different architectures/LL algorithms perform under specific distribution shifts?\\n\\nQ2: To what extent does neural architecture impact knowledge transfer in LLDM, and are there any\\ndiscernible patterns in the specialized capabilities of each architecture?\\n\\nQ3: How do existing algorithms from lifelong supervised learning perform on LLDM tasks?\\n\\nQ4: To what extent does language embedding affect knowledge transfer in LLDM?\\n\\nQ5: How robust are different LL algorithms to task ordering in LLDM?\\n\\nQ6: Can supervised pretraining improve downstream lifelong learning performance in LLDM?\\n\\n5.1 Evaluation Metrics\\nWe report three metrics: FWT (forward transfer), NBT (negative backward transfer), and\\nAUC (area under the success rate curve). All metrics are computed in terms of success rate, as\\nprevious literature has shown that the success rate is a more reliable metric than training loss for\\nmanipulation policies (Detailed explanation in Appendix E.2). Lower NBT means a policy\\nhas better performance in the previously seen tasks, higher FWT means a policy learns faster on a\\nnew task, and higher AUC means an overall better performance considering both NBT and FWT.\\n\\nSpecifically, denote \\\\( c_{i,j,e} \\\\) as the agent's success rate on task \\\\( j \\\\) when it learned over \\\\( i \\\\) previous tasks\\nand has just learned \\\\( e \\\\) epochs (\\\\( e \\\\in \\\\{0, 5, \\\\ldots, 50\\\\} \\\\)). Let \\\\( c_{i,i} \\\\) be the best success rate over all\\nevaluated epochs \\\\( e \\\\) for the current task \\\\( i \\\\) (i.e., \\\\( c_{i,i} = \\\\max_e c_{i,i,e} \\\\)). Then, we find the earliest epoch\\n\\\\( e^\\\\ast_i \\\\) in which the agent achieves the best performance on task \\\\( i \\\\) (i.e., \\\\( e^\\\\ast_i = \\\\arg\\\\min_e c_{i,i,e} \\\\)), and\\nassume for all \\\\( e \\\\leq e^\\\\ast_i \\\\), \\\\( c_{i,i,e} = c_{i,i} \\\\).\\n\\nGiven a different task \\\\( j \\\\neq i \\\\), we define \\\\( c_{i,j} = c_{i,j,e^\\\\ast_i} \\\\). Then the\\nthree metrics are defined:\\n\\n- \\\\( \\\\text{FWT} = \\\\sum_{k=2}^{K} \\\\max_{e} c_{k,k,e} \\\\)\\n- \\\\( \\\\text{NBT} = \\\\sum_{k=2}^{K} \\\\frac{1}{K} \\\\sum_{\\\\tau=k+1}^{K} c_{\\\\tau,k} - c_{\\\\tau,k} \\\\)\\n- \\\\( \\\\text{AUC} = \\\\sum_{k=2}^{K} \\\\frac{1}{K} \\\\sum_{\\\\tau=k+1}^{K} \\\\text{FWT} + \\\\sum_{\\\\tau=k+1}^{K} c_{\\\\tau,k} \\\\)\\n\\nIn practice, it's possible that the agent's performance on task \\\\( i \\\\) is not monotonically increasing due to the\\nvariance of learning. But we keep the best checkpoint among those saved at epochs \\\\( \\\\{e\\\\} \\\\) as if the agent stops\\nlearning after \\\\( e^\\\\ast_i \\\\).\"}"}
{"id": "xzEtNSuDJk", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"5.2 Experimental Results\\n\\nWe present empirical results to address the research questions. Please refer to Appendix E.1 for the full results across all algorithms, policy architectures, and task suites.\\n\\nStudy on the Policy's Neural Architectures (Q1, Q2)\\n\\nTable 1 reports the agent's lifelong learning performance using the three different neural architectures on the four task suites. Results are reported when ER and P\\n\\n| Policy Arch. | ER  | P  | NET  | FWT(\u201c) | NBT(#) | AUC(\u201c) | FWT(\u201c) | NBT(#) | AUC(\u201c) |\\n|--------------|-----|----|------|---------|--------|--------|---------|--------|--------|\\n| LIBERO-LONG  | 0.16\u00b1 0.02 | 0.16\u00b1 0.02 | 0.08\u00b1 0.01 | 0.13\u00b1 0.00 | 0.21\u00b1 0.01 | 0.03\u00b1 0.00 |\\n| RES-Net-RNN  | 0.48\u00b1 0.02 | 0.32\u00b1 0.04 | 0.32\u00b1 0.01 | 0.22\u00b1 0.01 | 0.08\u00b1 0.01 | 0.25\u00b1 0.00 |\\n| VIT-T  | 0.38\u00b1 0.05 | 0.29\u00b1 0.06 | 0.25\u00b1 0.02 | 0.36\u00b1 0.01 | 0.14\u00b1 0.01 | 0.34\u00b1 0.01 |\\n| LIBERO-Spatial  | 0.40\u00b1 0.02 | 0.29\u00b1 0.02 | 0.29\u00b1 0.01 | 0.27\u00b1 0.03 | 0.38\u00b1 0.03 | 0.06\u00b1 0.01 |\\n| RES-Net-T  | 0.65\u00b1 0.03 | 0.27\u00b1 0.03 | 0.56\u00b1 0.01 | 0.55\u00b1 0.01 | 0.07\u00b1 0.02 | 0.63\u00b1 0.00 |\\n| VIT-T  | 0.63\u00b1 0.01 | 0.29\u00b1 0.02 | 0.50\u00b1 0.02 | 0.57\u00b1 0.04 | 0.15\u00b1 0.00 | 0.59\u00b1 0.03 |\\n| LIBERO-Object  | 0.30\u00b1 0.01 | 0.27\u00b1 0.05 | 0.17\u00b1 0.05 | 0.29\u00b1 0.02 | 0.35\u00b1 0.02 | 0.13\u00b1 0.01 |\\n| RES-Net-T  | 0.67\u00b1 0.07 | 0.43\u00b1 0.04 | 0.44\u00b1 0.06 | 0.60\u00b1 0.07 | 0.17\u00b1 0.05 | 0.60\u00b1 0.05 |\\n| VIT-T  | 0.70\u00b1 0.02 | 0.28\u00b1 0.01 | 0.57\u00b1 0.01 | 0.58\u00b1 0.03 | 0.18\u00b1 0.02 | 0.56\u00b1 0.04 |\\n\\nTable 1: Performance of the three neural architectures using ER and P\\n\\nFindings: First, we observe that RES-Net-T and VIT-T work much better than RES-Net-RNN on average, indicating that using a transformer on the \u201ctemporal\u201d level could be a better option than using an RNN model. Second, the performance difference among different architectures depends on the underlying lifelong learning algorithm. If P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P\\n\\nIf P"}
{"id": "xzEtNSuDJk", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"used, we observe no significant performance difference between R\\\\textit{ES}N\\\\textit{ET}-T and V\\\\textit{I}T-T except on the LIBERO-L\\\\textit{ONG} task suite where V\\\\textit{I}T-T performs much better than R\\\\textit{ES}N\\\\textit{ET}-T. In contrast, if ER is used, we observe that R\\\\textit{ES}N\\\\textit{ET}-T performs better than V\\\\textit{I}T-T on all task suites except LIBERO-O\\\\textit{BJECT}. This potentially indicates that the ViT architecture is better at processing visual information with more object varieties than the ResNet architecture when the network capacity is sufficiently large (See the MTL results in Table 8 on LIBERO-O\\\\textit{BJECT} as the supporting evidence).\\n\\nThe above findings shed light on how one can improve architecture design for better processing of spatial and temporal information in LLDM.\\n\\n**Study on Lifelong Learning Algorithms (Q1, Q3)**\\n\\nTable 2 reports the lifelong learning performance of the three lifelong learning algorithms, together with the S\\\\textit{EQ}L and MTL baselines. All experiments use the same R\\\\textit{ES}N\\\\textit{ET}-T architecture as it performs the best across all policy architectures.\\n\\n| Lifelong Algo. | FWT | NBT | AUC |\\n|----------------|-----|-----|-----|\\n| S\\\\textit{EQ}L | 0.54 \u00b1 0.01 | 0.63 \u00b1 0.01 | 0.15 \u00b1 0.00 |\\n| ER             | 0.48 \u00b1 0.02 | 0.32 \u00b1 0.04 | 0.32 \u00b1 0.01 |\\n| EWC            | 0.13 \u00b1 0.02 | 0.22 \u00b1 0.03 | 0.02 \u00b1 0.00 |\\n| P\\\\textit{ACK}N\\\\textit{ET} | 0.22 \u00b1 0.01 | 0.08 \u00b1 0.01 | 0.25 \u00b1 0.00 |\\n\\n| Lifelong Algo. | FWT | NBT | AUC |\\n|----------------|-----|-----|-----|\\n| S\\\\textit{EQ}L | 0.78 \u00b1 0.04 | 0.76 \u00b1 0.04 | 0.26 \u00b1 0.02 |\\n| ER             | 0.67 \u00b1 0.07 | 0.43 \u00b1 0.04 | 0.44 \u00b1 0.06 |\\n| EWC            | 0.56 \u00b1 0.03 | 0.69 \u00b1 0.02 | 0.16 \u00b1 0.02 |\\n| P\\\\textit{ACK}N\\\\textit{ET} | 0.60 \u00b1 0.07 | 0.17 \u00b1 0.05 | 0.60 \u00b1 0.05 |\\n\\nFindings: We observed a series of interesting findings that could potentially benefit future research on algorithm design for LLDM:\\n\\n1) S\\\\textit{EQ}L shows the best FWT over all task suites. This is surprising since it indicates all lifelong learning algorithms we consider actually hurt forward transfer;\\n2) P\\\\textit{ACK}N\\\\textit{ET} outperforms other lifelong learning algorithms on LIBERO-X but is outperformed by ER significantly on LIBERO-L\\\\textit{ONG}, mainly because of low forward transfer. This confirms that the dynamic architecture approach is good at preventing forgetting. But since P\\\\textit{ACK}N\\\\textit{ET} splits the network into different sub-networks, the essential capacity of the network for learning any individual task is smaller. Therefore, we conjecture that P\\\\textit{ACK}N\\\\textit{ET} is not rich enough to learn on LIBERO-L\\\\textit{ONG};\\n3) EWC works worse than S\\\\textit{EQ}L, showing that the regularization on the loss term can actually impede the agent's performance on LLDM problems (See Appendix E.2); and\\n4) ER, the rehearsal method, is robust across all task suites.\\n\\n**Study on Language Embeddings as the Task Identifier (Q4)**\\n\\nTo investigate to what extent language embedding play a role in LLDM, we compare the performance of the same lifelong learner using four different pretrained language embeddings. Namely, we choose BERT [19], CLIP [52], GPT-2 [53] and the Task-ID embedding. Task-ID embeddings are produced by feeding a string such as \\\"Task 5\\\" into a pretrained BERT model.\\n\\nFindings: From Table 3, we observe no statistically significant difference among various language embeddings, including the Task-ID embedding. This, we believe, is due to sentence embeddings functioning as bag-of-words that differentiates different tasks. This insight calls for better language encoding to harness the semantic information in task descriptions. Despite the similar performance, we opt for BERT embeddings as our default task embedding.\"}"}
{"id": "xzEtNSuDJk", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Performance of a lifelong learner using four different language embeddings on LIBERO-LONG, where we fix the policy architecture to RESNET-T and the lifelong learning algorithm to ER.\\n\\nThe Task-ID embeddings are retrieved by feeding \u201cTask + ID\u201d into a pretrained BERT model. Results are averaged over three seeds and we report the mean and standard error. The best performance is bolded. No statistically significant difference is observed among the different language embeddings.\\n\\nStudy on task ordering (Q5)\\n\\nFigure 4 shows the result of the study on Q4. For all experiments in this study, we used RESNET-T as the neural architecture and evaluated both ER and PACTNET. As the figure illustrates, the performance of both algorithms varies across different task orderings. This finding highlights an important direction for future research: developing algorithms or architectures that are robust to varying task orderings.\\n\\nFigure 4: Performance of ER and PACTNET using RESNET-T on five different task orderings. An error bar shows the performance standard deviation for a fixed ordering.\\n\\nFindings: From Figure 4, we observe that indeed different task ordering could result in very different performances for the same algorithm. Specifically, such difference is statistically significant for PACTNET.\\n\\nStudy on How Pretraining Affects Downstream LLDM (Q6)\\n\\nFig 5 reports the results on LIBERO-LONG of five combinations of algorithms and policy architectures, when the underlying model is pretrained on the 90 short-horizon tasks in LIBERO-100 or learned from scratch. For pretraining, we apply behavioral cloning on the 90 tasks using the three policy architectures for 50 epochs. We save a checkpoint every 5 epochs of training and then pick the checkpoint for each architecture that has the best performance as the pretrained model for downstream LLDM.\\n\\nFigure 5: Performance of different combinations of algorithms and architectures without pretraining or with pretraining. The multi-task learning performance is also included for reference.\\n\\nFindings: We observe that the basic supervised pretraining can hurt the model's downstream lifelong learning performance. This, together with the results seen in Table 2 (e.g., naive sequential fine-tuning...\"}"}
{"id": "xzEtNSuDJk", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"has better forward transfer than when lifelong learning algorithms are applied), indicates that better pretraining techniques are needed.\\n\\nAttention Visualization:\\nTo better understand what type of knowledge the agent forgets during the lifelong learning process, we visualize the agent's attention map on each observed image input. The visualized saliency maps and the discussion can be found in Appendix E.4.\\n\\n6 Related Work\\nThis section provides an overview of existing benchmarks for lifelong learning and robot learning. We refer the reader to Appendix B.1 for a detailed review of lifelong learning algorithms.\\n\\nLifelong Learning Benchmarks\\nPioneering work has adapted standard vision or language datasets for studying LL. This line of work includes image classification datasets like MNIST, CIFAR, and ImageNet; segmentation datasets like Core50; and natural language understanding datasets like GLUE and SuperGLUE. Besides supervised learning datasets, video game benchmarks (e.g., Atari, XLand, and VisDoom) in reinforcement learning (RL) have also been used for studying LL. However, LL in standard supervised learning does not involve procedural knowledge transfer, while RL problems in games do not represent human activities. ContinualWorld modifies the 50 manipulation tasks in MetaWorld for LL. CORA builds four lifelong RL benchmarks based on Atari, Procgen, MiniHack, and ALFRED. F-SIOI-310 and OpenLORIS are challenging real-world lifelong object learning datasets that are captured from robotic vision systems. Prior works have also analyzed different components in a LL agent, but they do not focus on robot manipulation problems.\\n\\nRobot Learning Benchmarks\\nA variety of robot learning benchmarks have been proposed to address challenges in meta learning (MetaWorld), causality learning (CausalWorld), multi-task learning, policy generalization to unseen objects, and compositional learning. Compared to existing benchmarks in lifelong learning and robot learning, the task suites in LIBERO are curated to address the research topics of LLDM. The benchmark includes a large number of tasks based on everyday human activities that feature rich interactive behaviors with a diverse range of objects. Additionally, the tasks in LIBERO are procedurally generated, making the benchmark scalable and adaptable. Moreover, the provided high-quality human demonstration dataset in LIBERO supports and encourages learning efficiency.\\n\\n7 Conclusion and Limitations\\nThis paper introduces LIBERO, a new benchmark in the robot manipulation domain for supporting research in LLDM. LIBERO includes a procedural generation pipeline that can create an infinite number of manipulation tasks in the simulator. We use this pipeline to create 130 standardized tasks and conduct a comprehensive set of experiments on policy and algorithm designs. The empirical results suggest several future research directions: 1) how to design a better neural architecture to better process spatial information or temporal information; 2) how to design a better algorithm to improve forward transfer ability; and 3) how to use pretraining to help improve lifelong learning performance.\\n\\nIn the short term, we do not envision any negative societal impacts triggered by LIBERO. But as the lifelong learner mainly learns from humans, studying how to preserve user privacy within LLDM is crucial in the long run.\\n\\nReferences\\n[1] Ossama Ahmed, Frederik Tr\u00e4uble, Anirudh Goyal, Alexander Neitz, Yoshua Bengio, Bernhard Sch\u00f6lkopf, Manuel W\u00fcthrich, and Stefan Bauer. Causalworld: A robotic manipulation benchmark for causal structure and transfer learning. arXiv preprint arXiv:2010.04296, 2020.\\n[2] Ali Ayub and Carter Fendley. Few-shot continual active learning by a robot. arXiv preprint arXiv:2210.04137, 2022.\\n[3] Ali Ayub and Alan R Wagner. F-sioi-310: A robotic dataset and benchmark for few-shot incremental object learning. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 13496\u201313502. IEEE, 2021.\"}"}
{"id": "xzEtNSuDJk", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "xzEtNSuDJk", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18995\u201319012, 2022.\\n\\nSam Greydanus, Anurag Koul, Jonathan Dodge, and Alan Fern. Visualizing and understanding Atari agents. ArXiv, abs/1711.00138, 2017.\\n\\nJiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, et al. Maniskill2: A unified benchmark for generalizable manipulation skills. arXiv preprint arXiv:2302.04659, 2023.\\n\\nMatthew Hausknecht and Peter Stone. Deep recurrent Q-learning for partially observable MDPs. In 2015 AAAI Fall Symposium Series, 2015.\\n\\nChing-Yi Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming Chan, and Chu-Song Chen. Compacting, picking and growing for unforgetting continual learning. Advances in Neural Information Processing Systems, 32, 2019.\\n\\nStephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. RLbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):3019\u20133026, 2020.\\n\\nLeslie Pack Kaelbling. The foundation of efficient robot learning. Science, 369(6506):915\u2013916, 2020.\\n\\nMinsoo Kang, Jaeyoo Park, and Bohyung Han. Class-incremental learning by knowledge distillation with adaptive feature consolidation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16071\u201316080, 2022.\\n\\nMicha\u0142 Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja\u015bkowski. Vizdoom: A Doom-based AI research platform for visual reinforcement learning. In 2016 IEEE Conference on Computational Intelligence and Games (CIG), pages 1\u20138. IEEE, 2016.\\n\\nWonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583\u20135594. PMLR, 2021.\\n\\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521\u20133526, 2017.\\n\\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\\n\\nChengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Mart\u00edn-Mart\u00edn, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: A benchmark for embodied AI with 1,000 everyday activities and realistic simulation. In Conference on Robot Learning, pages 80\u201393. PMLR, 2023.\\n\\nB. Liu, Qian Liu, and Peter Stone. Continual learning and private unlearning. In CoLLAs, 2022.\\n\\nHao Liu and Huaping Liu. Continual learning with recursive gradient optimization. arXiv preprint arXiv:2201.12522, 2022.\\n\\nVincenzo Lomonaco and Davide Maltoni. Core50: A new dataset and benchmark for continuous object recognition. In Conference on Robot Learning, pages 17\u201326. PMLR, 2017.\\n\\nDavid Lopez-Paz and Marc'Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in Neural Information Processing Systems, 30, 2017.\"}"}
{"id": "xzEtNSuDJk", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner. Online continual learning in image classification: An empirical survey. *Neurocomputing*, 469:28\u201351, 2022.\\n\\nArun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In *Proceedings of the IEEE conference on Computer Vision and Pattern Recognition*, pages 7765\u20137773, 2018.\\n\\nAjay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto Mart\u00edn-Mart\u00edn. What matters in learning from offline human demonstrations for robot manipulation. *arXiv preprint arXiv:2108.03298*, 2021.\\n\\nDrew McDermott, Malik Ghallab, Adele Howe, Craig Knoblock, Ashwin Ram, Manuela Veloso, Daniel Weld, and David Wilkins. Pddl\u2014the planning domain definition language. 1998.\\n\\nJorge A Mendez, Marcel Hussing, Meghna Gummadi, and Eric Eaton. Composuite: A compositional reinforcement learning benchmark. *arXiv preprint arXiv:2207.04136*, 2022.\\n\\nSeyed Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Timothy Nguyen, Razvan Pascanu, Dilan Gorur, and Mehrdad Farajtabar. Architecture matters in continual learning. *arXiv preprint arXiv:2202.00275*, 2022.\\n\\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. *arXiv preprint arXiv:1312.5602*, 2013.\\n\\nTongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Yang, Xuanlin Li, Stone Tao, Zhiao Huang, Zhiwei Jia, and Hao Su. Maniskill: Generalizable manipulation skill benchmark with large-scale demonstrations. *arXiv preprint arXiv:2107.14483*, 2021.\\n\\nSanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, and Peter Stone. Curriculum learning for reinforcement learning domains: A framework and survey. *arXiv preprint arXiv:2003.04960*, 2020.\\n\\nGerman I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. *Neural Networks*, 113:54\u201371, 2019.\\n\\nEthan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 32, 2018.\\n\\nSam Powers, Eliot Xing, Eric Kolve, Roozbeh Mottaghi, and Abhinav Gupta. Cora: Benchmarks, baselines, and metrics as a platform for continual reinforcement learning agents. *arXiv preprint arXiv:2110.10067*, 2021.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In *International conference on machine learning*, pages 8748\u20138763. PMLR, 2021.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. *OpenAI blog*, 1(8):9, 2019.\\n\\nAmanda Rios and Laurent Itti. Lifelong learning without a task oracle. In *2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)*, pages 255\u2013263. IEEE, 2020.\\n\\nSt\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In *Proceedings of the fourteenth international conference on artificial intelligence and statistics*, pages 627\u2013635. JMLR Workshop and Conference Proceedings, 2011.\"}"}
{"id": "xzEtNSuDJk", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.\\n\\nGobinda Saha, Isha Garg, Aayush Ankit, and Kaushik Roy. Space: Structured compression and sharing of representational space for continual learning. IEEE Access, 9:150480\u2013150494, 2021.\\n\\nMikayel Samvelyan, Robert Kirk, Vitaly Kurin, Jack Parker-Holder, Minqi Jiang, Eric Hambro, Fabio Petroni, Heinrich K\u00fcttler, Edward Grefenstette, and Tim Rockt\u00e4schel. Minihack the planet: A sandbox for open-ended reinforcement learning research. arXiv preprint arXiv:2109.13202, 2021.\\n\\nPaul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4938\u20134947, 2020.\\n\\nJonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for continual learning. In International Conference on Machine Learning, pages 4528\u20134537. PMLR, 2018.\\n\\nQi She, Fan Feng, Xinyue Hao, Qihan Yang, Chuanlin Lan, Vincenzo Lomonaco, Xuesong Shi, Zhengwei Wang, Yao Guo, Yimin Zhang, et al. Openloris-object: A robotic vision dataset and benchmark for lifelong deep learning. In 2020 IEEE international conference on robotics and automation (ICRA), pages 4767\u20134773. IEEE, 2020.\\n\\nMohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Motlaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10740\u201310749, 2020.\\n\\nSanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart\u00edn-Mart\u00edn, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. In Conference on Robot Learning, pages 477\u2013490. PMLR, 2022.\\n\\nOpen Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, et al. Open-ended learning leads to generally capable agents. arXiv preprint arXiv:2107.12808, 2021.\\n\\nSebastian Thrun and Tom M. Mitchell. Lifelong robot learning. Robotics and autonomous systems, 15(1-2):25\u201346, 1995.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\\n\\nChen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang, Li Fei-Fei, Danfei Xu, Yuke Zhu, and Anima Anandkumar. Mimicplay: Long-horizon imitation learning by watching human play. arXiv preprint arXiv:2302.12422, 2023.\\n\\nMaciej Wo\u0142czyk, Michal Zajkac, Razvan Pascanu, Lukasz Kuci\u0144ski, and Piotr Mi\u0142o\u015b. Continual world: A robotic benchmark for continual reinforcement learning. In Neural Information Processing Systems, 2021.\\n\\nMaciej Wo\u0142czyk, Michal Zajkac, Razvan Pascanu, Lukasz Kuci\u0144ski, and Piotr Mi\u0142o\u015b. Disentangling transfer in continual reinforcement learning. ArXiv, abs/2209.13900, 2022.\"}"}
{"id": "xzEtNSuDJk", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lemeng Wu, Bo Liu, Peter Stone, and Qiang Liu. Firefly neural architecture descent: a general approach for growing neural networks. Advances in Neural Information Processing Systems, 33:22373\u201322383, 2020.\\n\\nJaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically expandable networks. arXiv preprint arXiv:1708.01547, 2017.\\n\\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 1094\u20131100. PMLR, 2020.\\n\\nDa-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, Liang Ma, Shiliang Pu, and De-Chuan Zhan. Forward compatible few-shot class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9046\u20139056, 2022.\\n\\nYifeng Zhu, Abhishek Joshi, Peter Stone, and Yuke Zhu. Viola: Imitation learning for vision-based manipulation with object proposal priors. arXiv preprint arXiv:2210.11339, 2022.\\n\\nYuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Mart\u00edn-Mart\u00edn. robosuite: A modular simulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293, 2020.\"}"}
{"id": "xzEtNSuDJk", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n   (b) Did you describe the limitations of your work? [Yes]\\n   (c) Did you discuss any potential negative societal impacts of your work? [Yes]\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n   (b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes]\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes]\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes]\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes]\\n   (b) Did you mention the license of the assets? [N/A]\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\\n   (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\"}"}
