{"id": "jbdp9m7nr0R", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Would The Viewer Feel?\\n\\nEstimating Wellbeing From Video Scenarios\\n\\nMantas Mazeika\\n\\n\u2217UIUC\\n\\nEric Tang\\n\\n\u2217UC Berkeley\\n\\nAndy Zou\\n\\nUC Berkeley\\n\\nSteven Basart\\n\\nUChicago\\n\\nJun Shern Chan\\n\\nUC Berkeley\\n\\nDawn Song\\n\\nUC Berkeley\\n\\nDavid Forsyth\\n\\nUIUC\\n\\nJacob Steinhardt\\n\\nUC Berkeley\\n\\nDan Hendrycks\\n\\nUC Berkeley\\n\\nAbstract\\n\\nIn recent years, deep neural networks have demonstrated increasingly strong abilities to recognize objects and activities in videos. However, as video understanding becomes widely used in real-world applications, a key consideration is developing human-centric systems that understand not only the content of the video but also how it would affect the wellbeing and emotional state of viewers. To facilitate research in this setting, we introduce two large-scale datasets with over 60,000 videos manually annotated for emotional response and subjective wellbeing. The Video Cognitive Empathy (VCE) dataset contains annotations for distributions of fine-grained emotional responses, allowing models to gain a detailed understanding of affective states. The Video to Valence (V2V) dataset contains annotations of relative pleasantness between videos, which enables predicting a continuous spectrum of wellbeing. In experiments, we show how video models that are primarily trained to recognize actions and find contours of objects can be repurposed to understand human preferences and the emotional content of videos. Although there is room for improvement, predicting wellbeing and emotional response is on the horizon for state-of-the-art models. We hope our datasets can help foster further advances at the intersection of commonsense video understanding and human preference learning.\\n\\n1 Introduction\\n\\nVideos are a rich source of data that depict vast quantities of information about humans and the world. As deep learning has progressed, models have begun to reliably exhibit various aspects of video understanding, including action recognition (Kay et al., 2017a), object tracking (Zhao et al., 2021), segmentation (Huang et al., 2019; He et al., 2020), and more. However, vision models do not exist in a vacuum and will eventually require social perception abilities, so models need to begin understanding how humans interpret and respond to visual inputs. As video models become more widely used in real-world applications, they should be able to reliably predict not only \u201cwhat is where\u201d in a visual input but also predict how it would make a human feel.\\n\\nThe subjective experience of human viewers on video data is broadly valuable to characterize and predict. When humans pursue goals in the world, their actions are often driven by intuitive processes (Kahneman, 2011), a significant part of which is the experience of emotions or affective states (Oatley et al., 2006). Emotions can be thought of as evaluations of events in relation to goals (Scherer et al., 2001; Frijda, 1988), and hence are important to study in relation to behavior in diverse settings. However, they are also important to understand in their own right, as they are strong indicators of what people value (Hume, 1739). For example, if a situation makes one feel happy, then that is often preferred to a situation that induces feelings of fear. Additionally, emodiversity\u2014the variety of emotions\u2014can be crucial in understanding human preferences and wellbeing.\\n\\n\u2217Equal Contribution.\"}"}
{"id": "jbdp9m7nr0R", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: We introduce two large-scale datasets for predicting subjective responses to videos, including relative pleasantness between videos and distributions of fine-grained emotional responses. This enables training state-of-the-art vision models to predict continuous, consistent scores for the pleasantness of videos and a rich distribution of likely emotional responses.\\n\\nEmotions are an important indicator of the overall health of the human emotional ecosystem, and emodiversity over positive emotions can improve mental wellbeing (Quoidbach et al., 2014). Thus, understanding the emotional responses and preferences of humans on video data could be a useful avenue toward modeling basic human desires, values, and overall wellbeing.\\n\\nVideo recommender systems already attempt to capture human preferences over videos but for practical reasons often base their recommendations on imperfect proxy metrics (Ridgway, 1956). It is hard to directly measure the values of users and how video content affects their wellbeing. Thus, recommender systems often rely on metrics that are easier to obtain, such as engagement and watch time. This simplifies the problem but can result in unintended consequences and safety concerns (Hendrycks et al., 2021b). Simplifying metrics loses sight of the experiencer (Scott, 1999) and can result in situations where engagement is maximized but users are unhappy (Russell, 2019; Kross et al., 2013; Facebook; Stray, 2020; Stray et al., 2021). For instance, content that evokes feelings of envy or anger can be highly engaging but is nonetheless unhealthy to be constantly exposed to. Thus, systems that recommend videos could substantially improve user experience through content-based inferences about how it would affect the emotional state and wellbeing of viewers.\\n\\nTo facilitate research on understanding how viewers feel while watching videos, we introduce two large-scale datasets for predicting emotional state and wellbeing of viewers directly from videos. First, we introduce the Video Cognitive Empathy (VCE) dataset for predicting fine-grained emotional responses to videos. The VCE dataset contains approximately 60,000 videos with human annotations for 27 emotion categories, ranging from the six basics (joy, sadness, fear, disgust, anger, surprise) (Ekman, 1992) to more nuanced emotions such as admiration and awkwardness, altogether covering the spectrum of affective states (Cowen and Keltner, 2017). As emotional responses can be considered evaluations of events in relation to a person's unique goals, they can vary significantly across human viewers. To capture the diversity of human responses, we collect a distribution\u2014not just a single label\u2014of emotional responses for each video. This enables evaluating models on their ability to inclusively predict the likely range of responses to a video across our large pool of annotators.\\n\\nTo estimate how videos affect the wellbeing of human viewers, we introduce a second dataset, Video to Valence (V2V). The V2V dataset contains approximately 25,000 videos with human-annotated rankings of pleasantness between videos. Pleasantness captures the overall positive or negative affect that viewers feel when watching a video and serves as a measure of wellbeing (Sidgwick, 1907; de Lazari-Radek and Singer, 2017). Since our annotations are for pairwise or listwise comparisons across videos, we can train utility-style models to predict continuous wellbeing scores (Hendrycks et al., 2021a), capturing gradations of wellbeing rather than a binary indicator. For instance, two scary videos may both be unpleasant, but our dataset enables predicting which video is more unpleasant, enabling a deeper understanding of human preferences.\\n\\nOur datasets come with strong baselines. We train state-of-the-art video Transformers (Vaswani et al., 2017) on our tasks and find that these models, which are primarily used for understanding the literal content of videos, can predict the subjective state of viewers with surprising reliability. Although there is room for improvement, models that understand how viewers feel when watching...\"}"}
{"id": "jbdp9m7nr0R", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Dataset                          | Annotation Type                        | Number of Videos |\\n|---------------------------------|----------------------------------------|------------------|\\n| COGNIMUSE (Zlatintsi et al., 2017) | affective labels                       | 7                |\\n| HUMAINE (Douglas-Cowie et al., 2007) | affective labels                       | 50               |\\n| FilmStim (Schaefer et al., 2010)   | affective labels                       | 70               |\\n| DEAP (Koelstra et al., 2012)       | affective labels, face video           | 120              |\\n| VideoEmotion (Jiang et al., 2014) | discrete emotions                      | 1,101            |\\n| LIRIS-ACCEDE (Baveye et al., 2015) | valence, arousal                        | 160              |\\n| EEV (Sun et al., 2020)            | performative expressions               | 5,153            |\\n| Video Cognitive Empathy (Ours)     | fine-grained emotions                  | 61,046           |\\n| Video to Valence (Ours)           | relative pleasantness                  | 26,670           |\\n\\nTable 1: Comparisons between datasets for predicting the subjective states that human viewers would feel while watching videos. We introduce two new datasets with substantially more scenarios than prior work. Our datasets are annotated with subjective self-reports, enabling high-quality evaluations.\\n\\nVideos are on the horizon and may thus prove useful in numerous applications. Our datasets and experiment code can be found at github.com/hendrycks/emodiversity. We hope our datasets can help foster further research into the important problem of understanding human emotions and wellbeing.\\n\\nRelated Work\\n\\nVideo Understanding With DNNs.\\n\\nMuch work in video understanding has focused on identifying various aspects of the scenarios depicted in videos. These include recognizing human motion and actions (Schuldt et al., 2004; Kuehne et al., 2011; Soomro et al., 2012; Wang et al., 2014; Karpathy et al., 2014; Caba Heilbron et al., 2015; Abu-El-Haija et al., 2016; Kay et al., 2017b; Goyal et al., 2017; Zhang et al., 2019), arbitrary event recognition (Monfort et al., 2019), spatial localization and tracking (Yilmaz et al., 2006; Milan et al., 2016; Kang and Wildes, 2016; V ondrick et al., 2018), and video segmentation (Pont-Tuset et al., 2017; Xu et al., 2018; Garcia-Garcia et al., 2018). Some work focuses on recognizing emotions and goals expressed by humans in videos, including facial emotion recognition (Lyons et al., 1998; Lucey et al., 2010; Bargal et al., 2016; Li and Deng, 2020) and recognizing unintended actions (Epstein et al., 2020). Numerous video models have been proposed and benchmarked on tasks for understanding \u201cwhat is where\u201d in videos (Gorelick et al., 2007; Tran et al., 2015, 2018; Feichtenhofer et al., 2019; Sharir et al., 2021). However, relatively little work has investigated the context in which videos are often consumed\u2014namely, that humans watch videos and have subjective experiences deriving from said videos. Our work focuses on this important, less explored area of study.\\n\\nPredicting Subjective Responses.\\n\\nPredicting the subjective responses of humans to various stimuli is an important topic of study spanning numerous fields. The International Affective Picture System (IAPS) (Lang and Bradley, 2007) and Open Affective Standardized Image Set (OASIS) (Kurdi et al., 2017) both contain approximately 1,000 images selected to evoke a range of emotional responses. Achlioptas et al. (2021) explore affective explanations of paintings as a source of training for deep learning. Eliciting emotions in text is harder, although many works have investigated predicting emotions expressed by writing (Strapparava and Mihalcea, 2007; Oberl\u00e4nder and Klinger, 2018; Demszky et al., 2020). Unlike still images and text, video is better suited to studying subjective responses, as video stimuli can be far more evocative. Numerous datasets have been proposed to study emotional responses to video (Zlatintsi et al., 2017; Douglas-Cowie et al., 2007; Schaefer et al., 2010; Koelstra et al., 2012; Jiang et al., 2014; Baveye et al., 2015; Sun et al., 2020). Notably, Cowen and Keltner (2017) collect self-reported emotional states on a bank of 2,185 online videos and find that reported emotional states factor into 27 distinct emotions, which we use as a framework for building our VCE dataset, which is 30 \u00d7 larger. Comparisons of our datasets to existing work are given in Table 1. Our datasets have a much greater scale and diversity of videos than prior work, enabling research on predicting subjective responses with state-of-the-art deep learning models.\\n\\nValue Learning.\\n\\nBuilding machine learning systems that interact with humans and pursue human values may require understanding aspects of human subjective experience. Many argue that values are derived from subjective experience (Hume, 1739; Sidgwick, 1907; de Lazari-Radek and Singer, 2017) and that some of the main components of subjective experience are emotions and valence. Learning...\"}"}
{"id": "jbdp9m7nr0R", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Examples from the Video Cognitive Empathy (VCE) dataset. Each video is annotated with a distribution of emotional responses from forced choice decisions across multiple annotators. We ask whether models can predict emotional responses solely from the semantic content of videos.\\n\\nRepresentations of values is necessary for creating safe machine learning systems (Hendrycks et al., 2021b) that operate in an open world. In natural language processing, models are trained to assign wellbeing or pleasantness scores to arbitrary text scenarios (Hendrycks et al., 2021a). Recent work in machine ethics (Anderson and Anderson, 2011) has translated this knowledge into action by using wellbeing scores to steer agents in diverse environments (Hendrycks et al., 2021c). However, this recent line of work so far exclusively considers text inputs rather than raw visual inputs.\\n\\nEmodiversity. A large body of work in psychology seeks to understand and quantify the richness and complexity of human emotional life (Barrett, 2009; Lindquist and Barrett, 2008; Carstensen et al., 2000). An important concept in this area is emodiversity, the variety and relative abundance of emotions experienced by an individual, which has been linked with reduced levels of anxiety and depression (Quoidbach et al., 2014). Although prior work studies emodiversity in self-reports of emotion without stimuli, we hypothesize that the emodiversity of visual stimuli may be an important concept to quantify and understand. Thus, we investigate how our new datasets enable measuring the emodiversity of in-the-wild videos on a large scale.\\n\\n3 Video Cognitive Empathy (VCE) Dataset\\n\\nWhen watching videos, humans feel a wide range of emotions based on the semantic content depicted in the video. These emotional responses may depend on the video in complex ways, requiring reasoning about the implications of depicted events as well as a robust understanding of human values. We are interested in whether deep models can exhibit cognitive empathy, the ability to understand how someone else is feeling or would feel in a certain situation. To test whether state-of-the-art video models can predict emotional responses, we introduce the Video Cognitive Empathy (VCE) dataset.\\n\\nDataset Description. The VCE dataset contains 61,046 videos with annotations for the emotional response of human viewers. The data are split into a training and test set of 50,000 and 11,046 videos, respectively. Each video lasts an average of 14.1 seconds for a total of 239 hours of manually annotated videos.\"}"}
{"id": "jbdp9m7nr0R", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: t-SNE plot of all 27-dimensional annotation vectors in the Video Cognitive Empathy dataset. Points are colored according to the most prevalent evoked emotion. Groups of emotions cluster together in natural ways, allowing for intuitively reasonable traversals through the space of emotions.\\n\\nannotated data. While movies often evoke emotions with soundtracks and appropriate choices of colors and lighting, we are interested in how emotions depend on the semantic content of videos and less so on how engineered cues can evoke desired emotions. Thus, we remove audio cues that could serve as confounding variables. We also filter out inappropriate videos using an automated nudity detector followed by manual filtering for each video based on video thumbnails. VCE is the first dataset of its size with manual annotations that is suitable for evaluating modern deep video models. The annotations in VCE are modeled after the analysis performed by Cowen and Keltner (2017). By collecting reported emotional experiences from humans on a set of 27,000 videos, they find that emotional responses exhibit 27 dimensions associated with reliably distinct situations. These correspond to 27 descriptive emotional states, such as \u201cadmiration\u201d, \u201canger\u201d, and \u201camusement\u201d. We adopt this fine-grained categorization of emotions and ask annotators to indicate which emotions they felt the most while watching a video. In Figure 2 of the Supplementary Material, we show the number of annotations per emotion.\\n\\nAs emotional responses can vary across annotators, we capture the distribution of responses by gathering a large number of annotations per video. For each video in VCE, we gather an average of 13 annotations (minimum of 12, maximum of 15). Rather than only keeping examples with high inter-annotator agreement, which would result in a small dataset, we consider the distribution of responses to be the target for learning. This is justified because while individual emotional responses are variable, the distribution of emotional responses tends to change with the stimuli. For example, scary movies might not scare everyone, but the dominant response is fear. However, responses to certain content such as political videos can vary considerably across populations. Hence, our annotations should not be taken to be representative of all emotional responses and are primarily intended for studying whether deep networks can acquire cognitive empathy.\\n\\nDataset Construction. Annotations for VCE were collected using Amazon Mechanical Turk (MTurk) with IRB approval. For each video, workers were asked to view the video without audio and select from the set of 27 emotions the emotions that the video most strongly evoked. For each selected emotion, workers were asked to rank the intensity of that emotion from 1 to 10. To ensure that labels are high quality, we required that MTurkers pass a qualification test, and provided them with detailed\"}"}
{"id": "jbdp9m7nr0R", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"definitions of each of the 27 emotions. We also ensured that workers viewed the entire video, only worked on one task at a time, and asked workers to mark videos that would rely too heavily on audio in order to rate.\\n\\nNote that the MTurk annotators and individuals depicted in the videos may not form a representative sample of diverse cultural backgrounds. Hence, our annotations should not be taken to represent accurate emotional responses across a broad range of cultures or on an individual level, and we discourage their use in deployment contexts. The VCE and V2V datasets are designed to give a high-level understanding of how well current video models can predict subjective responses to videos. We support work on large-scale data collection that considers differences in emotional responses across cultures and individuals, and we think this is an interesting direction for future research.\\n\\n3.1 Metrics\\n\\nWe evaluate models on VCE using a top-$k$ accuracy metric. Let $(x, y) \\\\in D$ be a sample video and annotation. The annotation $y$ is a $27$-dimensional vector with non-negative entries that indicates the intensity of responses for each of the $27$ emotion categories. Let $f(x)$ be the predicted output distribution of a model $f$ on video $x$. The top-$k$ accuracy is computed as\\n\\n$$\\\\frac{1}{|D|} \\\\sum_{(x,y) \\\\in D} \\\\mathbb{1}_{\\\\text{arg max } f(x) \\\\in \\\\text{arg sort } y} - k,$$\\n\\nwhere $\\\\text{argsort}$ is in ascending order and the colon notation indicates the last $k$ indices of the resulting array. This measures the fraction of test examples where the maximum predicted emotion is in the top $k$ emotions of the ground-truth distribution. We set $k = 3$ for our evaluations.\\n\\n3.2 Analysis\\n\\nEmotion Clusters. Cowen and Keltner (2017) find that emotions vary continuously and cluster in reasonable ways. For example, one can smoothly traverse their $27$-dimensional space of emotions by going from calmness to aesthetic appreciation to awe. To investigate whether our responses exhibit this behavior, we perform dimensionality reduction on the $27$-dimensional VCE response distribution using t-SNE. We visualize results in Figure 3. Points are colored according to the maximum emotion in the response distribution. We find that emotions cluster together and that clusters group in natural ways. The groupings exhibit smooth transitions similar to Cowen and Keltner (2017). For example, one can smoothly transition through calmness $\\\\rightarrow$ aesthetic appreciation $\\\\rightarrow$ awe, and adoration $\\\\rightarrow$ amusement $\\\\rightarrow$ surprise. This demonstrates that the distributions of emotional responses contain significant hidden information beyond the top emotion for a given video.\\n\\nEmodiversity. The emotion distribution labels in VCE enable measuring per-video emodiversity. Emodiversity is an indicator of the overall health of the human emotional ecosystem and is positively correlated with mental wellbeing (Quoidbach et al., 2014). Prior work measures emodiversity using the Shannon entropy (Quoidbach et al., 2014), but this metric can be hard to interpret (Magurran, 2003). Thus, we quantify emodiversity using perplexity of the normalized emotion distribution, computed as\\n\\n$$\\\\exp \\\\left( -\\\\frac{1}{N} \\\\sum_{i} y_i \\\\log y_i \\\\right)$$\\n\\nwhere $y_i$ is the normalized probability assigned to the $i$th emotion in video label $y$. One may also exclude negative emotions like disgust when computing emodiversity, since emodiversity over positive emotions is more relevant to improving wellbeing. This metric has a minimum of $1$ and a maximum of the number of emotion clusters. It can be interpreted as the number of emotions that a video evokes, assuming uniform responses for all evoked emotions. In Figure 4, we show the distribution of emodiversity across VCE. This shows that most videos evoke a diverse range of emotions across the population of viewers.\"}"}
{"id": "jbdp9m7nr0R", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Example predictions on the V2V dataset. We train video models to predict continuous pleasantness scores by enforcing consistency with pleasantness rankings in the V2V training set. This results in intuitively reasonable outputs that capture preferences over the content depicted in videos.\\n\\n4 Video to Valence (V2V) Dataset\\n\\nFigure 6: An example video pair in the Video to Valence (V2V) dataset. The annotators have high agreement that the video on the left is less pleasant than the video on the right.\\n\\nA defining attribute of many emotional states is valence, which indicates how positive or negative an emotion is. For instance, feelings of joy typically have high valence, and feelings of fear typically have low valence. In addition to cognitive empathy via fine-grained prediction of which emotions are likely to be felt on a video, we also want video models to have a robust understanding of how a video would affect the valence of viewers' emotional state and by extension their overall wellbeing.\\n\\nAn important and underexplored characteristic of valence is that it varies continuously. Even within emotions such as fear, some experiences can be more pleasant or preferable than others. Thus, simply binning videos as \u201cpositive\u201d or \u201cnegative\u201d is a vast oversimplification that misses substantial portions of human experience. To enable developing robust models of gradations of wellbeing experienced while watching videos, we introduce the Video to Valence (V2V) dataset.\\n\\nDataset Description. The V2V dataset contains $26,670$ videos with annotations for rankings of pleasantness across videos. The data are split into a training and test set of $16,125$ and $10,545$ videos, respectively. The training set contains $11,038$ pairwise annotations, and the test set contains $4,947$ pairwise and listwise annotations. Each video lasts an average of $14.3$ seconds for a total of $106$ hours of manually annotated data. As in VCE, we are interested in how subjective state depends on the semantic content of videos rather than on audio or lighting cues. Additionally, the videos in V2V are a subset of VCE, enabling a richer analysis of the interplay between fine-grained emotional states and rankings of pleasantness.\"}"}
{"id": "jbdp9m7nr0R", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: The coefficients from a linear model that predicts video valence (V2V) from emotions data (VCE). The emotions that contribute most strongly to pleasantness have higher positive coefficients and vice versa. This provides evidence that predicting emotional responses and estimating wellbeing are complimentary tasks that can benefit from being studied together.\\n\\nThe annotations in V2V are for relative pleasantness between videos. Compared to binary pleasantness, relative pleasantness enables building models of gradations of wellbeing that capture much more detail about what people value. Additionally, rankings on pairs of videos are more repeatable and consistent across annotators than alternatives such as Likert scales. Accordingly, we find that annotators have much higher agreement rates for ranking the pleasantness of videos than for reporting fine-grained emotional responses. Consequently, all the annotations in V2V are for clear-cut comparisons with a high agreement rate across 9 independent annotations.\\n\\nWhen annotating relative pleasantness between pairs of videos, an important consideration is ensuring that comparisons are informative and interesting. For example, comparing videos that primarily evoke joy and videos that primarily evoke fear introduces very little novel information, as joy is preferable to fear for most people. In natural language datasets, one can simply construct counterfactual scenarios where slight differences have large effects on valence (Hendrycks et al., 2021a). However, this strategy is not currently viable for videos. Thus, we choose a balanced sampling strategy that selects pairs of videos based on multiple criteria, including similarity between emotional responses. Consequently, the construction of V2V depends on the VCE annotations. Additional details are in the Supplementary Materials.\\n\\nDataset Construction.\\n\\nAnnotations for V2V were collected using MTurk with IRB approval. We required workers to pass a qualification test and monitored agreement rate among workers over time, dropping workers who appeared to be selecting more randomly. We collected 9 pairwise annotations for each video pair, keeping annotations that 8 or 9 distinct workers agreed on. We first collected 6 pairwise annotations for each pair, then paused labeling for pairs that already had high disagreement. For the remaining high agreement pairs, 3 more labels were collected, after which the pair was either added to the dataset or discarded.\\n\\n4.1 Metrics\\n\\nWe evaluate models on V2V using the accuracy of predicted pairwise comparisons. Let (i, j) \u2208 I be a set of indices in our dataset with a pairwise comparison, where video i is less pleasant than video j by convention. Let x_i, x_j \u2208 X be corresponding videos, and let y_{ij} \u2208 Y be the pairwise label, where y_{ij} = 0 if video i is more pleasant than video j and y_{ij} = 1 if video j is more pleasant than video i.\\n\\nLet f(x_i, x_j) be the prediction of model f for the pairwise label. Pairwise accuracy is computed as \\\\[\\\\frac{1}{|I|} \\\\sum_{(i,j) \\\\in I} \\\\mathbb{1}[f(x_i, x_j) = y_{ij}]\\\\]. As V2V has a substantial number of pairwise comparisons, it is possible to consider the pairwise comparisons between one video and multiple other videos. Thus, we also evaluate models on 8\"}"}
{"id": "jbdp9m7nr0R", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Emotion prediction results on VCE. All models outperform random chance ($11.1\\\\%$), and Video Transformers have the highest accuracy.\\n\\n4.2 Analysis\\nSince V2V videos are a subset of VCE videos, we can analyze how the two tasks are related. A particularly interesting question is whether binary pleasantness is sufficient to predict ranking annotations in V2V. We do not directly collect binary pleasantness annotations, so we operationalize positive valence as the value of the \u201cjoy\u201d emotion in VCE annotations. We train a logistic regression model using this unidimensional feature and find that performance on the V2V test set is near chance, at $51\\\\%$ pairwise accuracy. This indicates that the mere presence of positive emotions is insufficient for predicting gradations of valence.\\n\\nTo analyze the importance of the full distribution of emotional responses, we repeat the above experiment with all $27$ emotions as features. In this case, pairwise accuracy increases to $89.6\\\\%$, indicating that the information encoded by multiple emotions can be combined to predict pleasantness rankings with high accuracy. To analyze the behavior of this model, we plot the logistic regression weights for each emotion in Figure 7. The learned weights make intuitive sense; high-valence emotions have large weights, and low-valence emotions have low weights. This suggests that distributions of emotional responses can serve as strong features for predicting continuous measures of wellbeing.\"}"}
{"id": "jbdp9m7nr0R", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Wellbeing results on V2V. Pretraining greatly improves performance, although there is still much room for improvement. Random chance for pairwise and listwise accuracy is 50% and 17%.\\n\\nFigure 8: Accuracy on VCE increases logarithmically with the number of training examples. Our large dataset size helps drive high performance.\\n\\nWellbeing Prediction. On the V2V dataset, we train models to output continuous scores with ranking supervision. This is achieved by letting models output a single, continuous value \\\\( f(x) \\\\) on input \\\\( x \\\\) and enforcing consistency with all rankings in the training set. For a given ranking \\\\((x_i, x_j, y_{ij})\\\\) in the training set, the training loss is BCE \\\\((\\\\sigma(f(x_j) - f(x_i)), y_{ij})\\\\), where BCE is the binary cross-entropy. Previous work has used this loss to train utility functions on general scenarios in text (Hendrycks et al., 2020). We focus on STAM models due to their efficiency, evaluating performance on V2V with and without Kinetics pretraining and with different temporal context lengths. The STAM-8 model takes 8 frames as input, and STAM-16 takes 16 frames. We train with batch size of 8 comparisons (16 videos) and learning rate 0.005 for 10 epochs for all models with a single sampling of frames from each video for both training and testing, as described in Sharir et al. (2021). We show quantitative results on V2V in Tab. 3 and qualitative results in Figure 5. Pairwise accuracy is substantially above random chance, and pretraining on Kinetics results in large improvements, showing that representations for recognizing actions transfer to predicting subjective judgments of relative pleasantness. We experiment with augmenting the training loss with the \\\\( \\\\ell_1 \\\\) VCE loss scaled by 0.5, but this does not improve performance in all cases. Listwise accuracy is far below pairwise accuracy, and performance on both metrics is far from the ceiling, showing that while models are beginning to gain cognitive empathy and the ability to predict judgments of relative pleasantness, there is still room for improvement.\\n\\nConclusion\\nWe introduced the Video Cognitive Empathy (VCE) and Video to Valence (V2V) datasets for predicting subjective responses to videos. We collected over 60,000 videos and hundreds of thousands of annotations for fine-grained evoked emotions and relative pleasantness. In analyses of our data, we showed that the full distribution of emotional responses on a video is a strong feature for predicting relative pleasantness, suggesting that studying emotions may be important for understanding general preferences over videos. In experiments with state-of-the-art video models, we found that models perform substantially better than chance, although there is still room to improve. As models become better predictors of experienced emotions and factors such as emodiversity, they will become increasingly relevant for monitoring wellbeing.\"}"}
{"id": "jbdp9m7nr0R", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan. Youtube-8m: A large-scale video classification benchmark. arXiv preprint arXiv:1609.08675, 2016.\\n\\nPanos Achlioptas, Maks Ovsjanikov, Kilichbek Haydarov, Mohamed Elhoseiny, and Leonidas J Guibas. Artemis: Affective language for visual art. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11569\u201311579, 2021.\\n\\nMichael Anderson and Susan Leigh Anderson. Machine ethics. 2011.\\n\\nSarah Adel Bargal, Emad Barsoum, Cristian Canton Ferrer, and Cha Zhang. Emotion recognition in the wild from videos using images. In Proceedings of the 18th ACM International Conference on Multimodal Interaction, pages 433\u2013436, 2016.\\n\\nLisa Feldman Barrett. Variety is the spice of life: A psychological construction approach to understanding variability in emotion. Cognition and Emotion, 23(7):1284\u20131306, 2009.\\n\\nYoann Baveye, Emmanuel Dellandrea, Christel Chamaret, and Liming Chen. Liris-accede: A video database for affective content analysis. IEEE Transactions on Affective Computing, 6(1):43\u201355, 2015.\\n\\nFabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision and pattern recognition, pages 961\u2013970, 2015.\\n\\nLaura L Carstensen, Monisha Pasupathi, Ulrich Mayr, and John R Nesselroade. Emotional experience in everyday life across the adult life span. Journal of personality and social psychology, 79(4):644, 2000.\\n\\nAlan S. Cowen and Dacher Keltner. Self-report captures 27 distinct categories of emotion bridged by continuous gradients. Proceedings of the National Academy of Sciences, 2017.\\n\\nKatarzyna de Lazari-Radek and Peter Singer. Utilitarianism: A very short introduction. 2017.\\n\\nDorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav Nemade, and Sujith Ravi. Goemotions: A dataset of fine-grained emotions. arXiv preprint arXiv:2005.00547, 2020.\\n\\nEllen Douglas-Cowie, Roddy Cowie, Ian Sneddon, Cate Cox, Orla Lowry, Margaret Mcrorie, Jean-Claude Martin, Laurence Devillers, Sarkis Abrilian, Anton Batliner, et al. The humaine database: Addressing the collection and annotation of naturalistic and induced emotional data. In International conference on affective computing and intelligent interaction, pages 488\u2013500. Springer, 2007.\\n\\nPaul Ekman. An argument for basic emotions. Cognition & Emotion, 6:169\u2013200, 1992.\\n\\nDave Epstein, Boyuan Chen, and Carl Vondrick. Oops! predicting unintentional action in video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 919\u2013929, 2020.\\n\\nFacebook. Bringing people closer together. URL https://about.fb.com/news/2018/01/news-feed-fyi-bringing-people-closer-together/.\\n\\nChristoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6202\u20136211, 2019.\\n\\nNico H. Frijda. The laws of emotion. The American psychologist, 1988.\\n\\nAlberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu Oprea, Victor Villena-Martinez, Pablo Martinez-Gonzalez, and Jose Garcia-Rodriguez. A survey on deep learning techniques for image and video semantic segmentation. Applied Soft Computing, 70:41\u201365, 2018.\\n\\n11\"}"}
{"id": "jbdp9m7nr0R", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deepti Ghadiyaram, Du Tran, and Dhruv Mahajan. Large-scale weakly-supervised pre-training for video action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12046\u201312055, 2019.\\n\\nLena Gorelick, Moshe Blank, Eli Shechtman, Michal Irani, and Ronen Basri. Actions as space-time shapes. IEEE transactions on pattern analysis and machine intelligence, 29(12):2247\u20132253, 2007.\\n\\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The \\\"something something\\\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 5842\u20135850, 2017.\\n\\nKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross B. Girshick. Mask r-cnn. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:386\u2013397, 2020.\\n\\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000\u201316009, 2022.\\n\\nDan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. Aligning AI with shared human values. 2020.\\n\\nDan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. Aligning AI with shared human values. ICLR, 2021a.\\n\\nDan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in ml safety. arXiv preprint arXiv:2109.13916, 2021b.\\n\\nDan Hendrycks, Mantas Mazeika, Andy Zou, Sahil Patel, Christine Zhu, Jesus Navarro, Dawn Song, Bo Li, and Jacob Steinhardt. What would jiminy cricket do? towards agents that behave morally. NeurIPS, 2021c.\\n\\nZilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, Humphrey Shi, and Wenyu Liu. Ccnet: Criss-cross attention for semantic segmentation. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 603\u2013612, 2019.\\n\\nDavid Hume. A Treatise of Human Nature. 1739.\\n\\nYu-Gang Jiang, Baohan Xu, and Xiangyang Xue. Predicting emotions in user-generated videos. In Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014.\\n\\nDaniel Kahneman. Thinking, fast and slow. 2011.\\n\\nSoo Min Kang and Richard P Wildes. Review of action recognition and detection methods. arXiv preprint arXiv:1610.06906, 2016.\\n\\nAndrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 1725\u20131732, 2014.\\n\\nWill Kay, Jo\u00e3o Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Apostol Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset. ArXiv, 2017a.\\n\\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017b.\\n\\nSander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan Yazdani, Touradj Ebrahimi, Thierry Pun, Anton Nijholt, and Ioannis Patras. Deap: A database for emotion analysis; using physiological signals. IEEE transactions on affective computing, 3(1):18\u201331, 2012.\\n\\nEthan Kross, Philippe Verduyn, Emre Demiralp, Jiyoung Park, David Seungjae Lee, Natalie Lin, Holly Shablack, John Jonides, and Oscar Ybarra. Facebook use predicts declines in subjective well-being in young adults. PloS one, 8(8):e69841, 2013.\"}"}
{"id": "jbdp9m7nr0R", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hildegard Kuehne, Hueihan Jhuang, Est\u00edbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a large video database for human motion recognition. In 2011 International conference on computer vision, pages 2556\u20132563. IEEE, 2011.\\n\\nBenedek Kurdi, Shayn Lozano, and Mahzarin R Banaji. Introducing the open affective standardized image set (oasis). Behavior research methods, 49(2):457\u2013470, 2017.\\n\\nPeter Lang and Margaret M Bradley. The international affective picture system (iaps) in the study of emotion and attention. Handbook of emotion elicitation and assessment, 29:70\u201373, 2007.\\n\\nShan Li and Weihong Deng. Deep facial expression recognition: A survey. IEEE transactions on affective computing, 2020.\\n\\nKristen A Lindquist and Lisa Feldman Barrett. Emotional complexity. 2008.\\n\\nPatrick Lucey, Jeffrey F Cohn, Takeo Kanade, Jason Saragih, Zara Ambadar, and Iain Matthews. The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression. In 2010 ieee computer society conference on computer vision and pattern recognition-workshops, pages 94\u2013101. IEEE, 2010.\\n\\nMichael Lyons, Shigeru Akamatsu, Miyuki Kamachi, and Jiro Gyoba. Coding facial expressions with gabor wavelets. In Proceedings Third IEEE international conference on automatic face and gesture recognition, pages 200\u2013205. IEEE, 1998.\\n\\nAnne E Magurran. Measuring biological diversity. John Wiley & Sons, 2003.\\n\\nAnton Milan, Laura Leal-Taix\u00e9, Ian Reid, Stefan Roth, and Konrad Schindler. Mot16: A benchmark for multi-object tracking. arXiv preprint arXiv:1603.00831, 2016.\\n\\nMathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfreund, Carl VonDrinck, et al. Moments in time dataset: one million videos for event understanding. IEEE transactions on pattern analysis and machine intelligence, 42(2):502\u2013508, 2019.\\n\\nKeith Oatley, Dacher Keltner, and Jennifer M. Jenkins. Understanding emotions, 2nd ed. 2006.\\n\\nLaura Ana Maria Oberl\u00e4nder and Roman Klinger. An analysis of annotated corpora for emotion classification in text. In Proceedings of the 27th International Conference on Computational Linguistics, pages 2104\u20132119, 2018.\\n\\nJordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel\u00e1ez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017.\\n\\nJordi Quoidbach, June Gruber, Mo\u00efra Mikolajczak, Alexsandr Kogan, Ilios Kotsou, and Michael I Norton. Emodiversity and the emotional ecosystem. Journal of experimental psychology: General, 143(6):2057, 2014.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.\\n\\nV. Ridgway. Dysfunctional consequences of performance measurements. Administrative Science Quarterly, 1956.\\n\\nStuart Russell. Human compatible: Artificial intelligence and the problem of control. 2019.\\n\\nAlexandre Schaefer, Fr\u00e9d\u00e9ric Nils, Xavier Sanchez, and Pierre Philippot. Assessing the effectiveness of a large database of emotion-eliciting films: A new tool for emotion researchers. Cognition and Emotion, 24(7):1153\u20131172, 2010.\\n\\nKlaus R. Scherer, Angela Schorr, and Tom Johnstone. Appraisal processes in emotion: Theory, methods, research. 2001.\"}"}
{"id": "jbdp9m7nr0R", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Christian Schuldt, Ivan Laptev, and Barbara Caputo. Recognizing human actions: a local SVM approach. In Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004., volume 3, pages 32\u201336. IEEE, 2004.\\n\\nJames C. Scott. Seeing like a state: How certain schemes to improve the human condition have failed. 1999.\\n\\nGilad Sharir, Asaf Noy, and Lihi Zelnik-Manor. An image is worth 16x16 words, what is a video worth? arXiv preprint arXiv:2103.13915, 2021.\\n\\nHenry Sidgwick. The Methods of Ethics. 1907.\\n\\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.\\n\\nCarlo Strapparava and Rada Mihalcea. Semeval-2007 task 14: Affective text. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 70\u201374, 2007.\\n\\nJonathan Stray. Aligning AI optimization to community well-being. International Journal of Community Well-Being, 2020.\\n\\nJonathan Stray, Ivan Vendrov, Jeremy Nixon, Steven Adler, and Dylan Hadfield-Menell. What are you optimizing for? aligning recommender systems with human values. ArXiv, abs/2107.10939, 2021.\\n\\nJennifer J. Sun, Ting Liu, Alan S. Cowen, Florian Schroff, Hartwig Adam, and Gautam Prasad. EEV dataset: Predicting expressions evoked by diverse videos. ArXiv, abs/2001.05488, 2020.\\n\\nZhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602, 2022.\\n\\nDu Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3D convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 4489\u20134497, 2015.\\n\\nDu Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 6450\u20136459, 2018.\\n\\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. ArXiv, abs/1706.03762, 2017.\\n\\nCarl von Ondr\u00ecck, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama, and Kevin Murphy. Tracking emerges by colorizing videos. In Proceedings of the European conference on computer vision (ECCV), pages 391\u2013408, 2018.\\n\\nLimin Wang, Yu Qiao, and Xiaoou Tang. Action recognition and detection by combining motion and appearance features. THUMOS14 Action Recognition Challenge, 1(2):2, 2014.\\n\\nNing Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas Huang. Youtube-vos: A large-scale video object segmentation benchmark. arXiv preprint arXiv:1809.03327, 2018.\\n\\nAlper Y\u0131lmaz, Omar Javed, and Mubarak Shah. Object tracking: A survey. Acm computing surveys (CSUR), 38(4):13\u2013es, 2006.\\n\\nHong-Bo Zhang, Yi-Xiang Zhang, Bineng Zhong, Qing Lei, Lijie Yang, Ji-Xiang Du, and Duan-Sheng Chen. A comprehensive survey of vision-based human action recognition methods. Sensors, 19(5):1005, 2019.\\n\\nBin Zhao, Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu Timofte. Generating masks from boxes by mining spatio-temporal consistencies in videos. ArXiv, 2021.\\n\\nAthanasia Zlatintsi, Petros Koutras, Georgios Evangelopoulos, Nikolaos Malandrakis, Niki Efthymiou, Katerina Pastra, Alexandros Potamianos, and Petros Maragos. Cognimuse: A multimodal video database annotated with saliency, events, semantics and emotion with application to summarization. EURASIP Journal on Image and Video Processing, 2017(1):54, 2017.\"}"}
{"id": "jbdp9m7nr0R", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Checklist\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n   (b) Did you describe the limitations of your work? [Yes]\\n   (c) Did you discuss any potential negative societal impacts of your work? [Yes]\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n   (b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes]\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes]\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] In our baseline experiments, we observe clear trends with substantial effect sizes that would be unlikely to occur by random chance, so we did not find this to be necessary.\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] We use internal resources (NVIDIA GPUs) to train baseline models. The compute cost was moderate (one to three days of training on a single machine for each baseline model).\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [N/A]\\n   (b) Did you mention the license of the assets? [Yes]\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\\n   (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes]\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes]\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [Yes]\\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes]\"}"}
