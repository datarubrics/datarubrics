{"id": "q1NaqDadKM", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models\\n\\nAbstract\\n\\nLarge Vision-Language Models (LVLM) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building an LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates categories of multimodal capabilities of LVLMs such as visual question answering and embodied artificial intelligence on standard text-related visual benchmarks, while the latter provides the user-level evaluation of LVLMs in an open-world question-answering scenario. The study reveals several innovative findings. First, Instruction-tuned LVLM with massive in-domain data such as InstructBLIP may overfit many existing tasks, generalizing poorly in the open-world scenario. Second, Instruction-tuned LVLM with moderate instruction-following data may result in object hallucination issues (i.e., generate objects that are inconsistent with target images in the descriptions). It either makes the current evaluation metric such as CIDER for image captioning ineffective or generates wrong answers. Third, employing a multi-turn reasoning evaluation framework could mitigate the issue of object hallucination, shedding light on developing an effective metric for LVLM evaluation. The findings provide a foundational framework for the conception and assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques. The evaluation pipeline will be available at vlarena page.\\n\\nIntroduction\\n\\nLarge Language Models (LLMs), such as LLaMA [1], GPT-3 [2], and Vicuna [3], have demonstrated remarkable progress in Natural Language Processing (NLP). These models leverage large-scale pre-training data and huge networks to achieve impressive results in NLP benchmarks. Recently, GPT-4 [4] further expanded the impact to the multimodal community, stimulating the rapid development of large vision-language models (LVLMs) and revolutionizing the landscape of artificial intelligence. Large Vision-Language Models (LVLM) have achieved remarkable progress in multimodal vision-language learning for various multimodal tasks such as visual question answering and multimodal conversation. Specifically, LVLMs capitalize on the knowledge from LLMs and effectively align visual features with the textual space. Flamingo [5], a pioneering LVLM, integrates visual features into LLMs through cross-attention layers. Later studies proposed more efficient vision-text interactions [6], more efficient training methods [7, 8], and employing instruction tuning [9, 7, 9, 10, 11, 12, 13, 8]. However, despite the great success, few efforts have been made to provide systematic evaluations of LVLMs. But evaluation plays a critical role in understanding the strengths and weaknesses of LVLMs.\"}"}
{"id": "q1NaqDadKM", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Comparative analysis of LVLMs within the LVLM eHub. (a) illustrates the variances in quantitative capability performance across six distinct aspects among LVLMs. (b) presents the Elo rating ranking of LVLMs within the LVLM Arena.\\n\\nVisual Reasoning, Visual Commonsense, Visual Knowledge Acquisition, Visual Perception, Object Hallucination, Embodied Intelligence.\\n\\n(a) Quantitative Capability Evaluation\\n\\n(b) LVLMs Arena Ranking\\n\\nthereby guiding their future development. Recent work presents a systematic investigation of object hallucination of LVLMs by proposing a polling-based object probing evaluation method. Moreover, ImageNetVC studies how well LVLMs can master visual commonsense knowledge. Liu et al. comprehensively evaluate the performance of LVLMs in visual recognition with text recognition, such as optical character recognition. GVT evaluates LVLM's visual semantic understanding and fine-grained perception capabilities. Nevertheless, these studies only evaluate a portion of LVLMs on specific tasks, lacking an overall understanding of LVLM's capabilities.\\n\\nIn pursuit of a comprehensive evaluation of LVLMs, we build an LVLM Evaluation hub (LVLM-eHub) consolidating representative LVLMs such as InstructBLIP and MiniGPT-4. The detailed information about model configuration and training data is listed in Table 1. Our LVLM-eHub consists of a quantitative capability evaluation and an online arena platform, providing a thorough investigation of the selected LVLMs. Specifically, the quantitative capability evaluation extensively evaluates 6 categories of multimodal capabilities of LVLMs including visual perception, visual knowledge acquisition, visual reasoning, visual commonsense, object hallucination, and embodied intelligence (see Fig. 1 (a)), by collecting standard text-related visual benchmarks. On the other hand, the online arena platform features anonymous randomized pairwise battles in a crowd-sourced manner, providing a user-level model ranking in the open-world question-answering scenario (see Fig. 1 (b)).\\n\\nOur LVLM-eHub comprehensively evaluates LVLMs, revealing several innovative findings. (1) Instruction-tuned LVLM with massive in-domain data suffers from overfitting and generalizes poorly in open-world scenarios, such as InstructBLIP (see Fig. 1 (a)). (2) With moderate instruction-following data, Instruction-tuned LVLM may cause object hallucination issues, generating objects that are inconsistent with target images in the descriptions. This leads to incorrect answers or renders current evaluation metrics, such as CIDER for image captioning, ineffective. (3) We find that a multi-turn reasoning evaluation pipeline can mitigate the issue of object hallucination, indicating that developing an effective metric for LVLM evaluation is urgent.\\n\\nThe contributions of our work are summarized follows. (1) We propose LVLM-eHub which is the first comprehensive evaluation benchmark for large vision-language models, to our best knowledge. (2) LVLM-eHub provides extensive evaluation on 6 categories of multimodal capabilities of LVLMs in more than 40 text-based visual tasks. (3) LVLM-eHub builds an online arena platform for LVLMs, which features anonymous randomized pairwise user-level comparison in a open-world scenario. (4) Our evaluation results reveal several innovative findings, providing a foundational framework for the assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques.\\n\\n2 LVLM Evaluation Hub\\n\\nIn this section, we introduce representative LVLMs, multimodal capabilities of interest, and evaluation methods. The whole LVLM Evaluation Hub is illustrated in Fig. 2. Our LVLM evaluation hub...\"}"}
{"id": "q1NaqDadKM", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model | Image-Text Data | Visual Instruction Data |\\n|-------|-----------------|-------------------------|\\n| BILP2 | \u2020               |                        |\\n| ViT-g/14 | \u2020             | FlanT5-XL       |\\n| Q-Former 4B 107M 32 |            |                        |\\n| -VG-SBU-L400 129M |            |                        |\\n| LLaV A |                  |                        |\\n| ViT-L/14 | \u2020               | Vicuna FC layer 7B 7B 256 |\\n| CC3M 595K |              |                        |\\n| LLaV A-I 158K |            |                        |\\n| LA-V2 |                  |                        |\\n| ViT-L/14 | \u2020               | LLaMA          |\\n| B-Tuning 7B 63.1M 10 |      |                        |\\n| L400 200M |              |                        |\\n| LLaV A-I+G4L 210K |            |                        |\\n| MiniGPT-4 |              |                        |\\n| BLIP2-VE | \u2020               | Vicuna         |\\n| \u2020       |                 |                        |\\n| Vicuna |                  |                        |\\n| FC layer 7B 3.1M 32 |          |                        |\\n| CC-SBU-L400 5M |            |                        |\\n| CC+ChatGPT 3.5K |          |                        |\\n| mPLUG-Owl |              |                        |\\n| ViT-L/14 | \u2020               | LLaMA          |\\n| LoRA 7B 1.1B 65 |      |                        |\\n| L400 204M |              |                        |\\n| LLaV A-I 158K |            |                        |\\n| Otter |                  |                        |\\n| ViT-L/14 | \u2020               | LLaMA         |\\n| Resampler 9B 1.3B 64 |      |                        |\\n| L400 200M |              |                        |\\n| LLaV A-I 158K |            |                        |\\n| InstructBLIP |            |                        |\\n| ViT-g/14 | \u2020               | Vicuna         |\\n| Q-Former 7B 107M 32 |          |                        |\\n| CC-SBU-L400 13.8M |          |                        |\\n| CC+ChatGPT 3.5K |          |                        |\\n| VPGTrans |              |                        |\\n| ViT-g/14 | \u2020               | Vicuna         |\\n| Q-Former 7B 107M 32 |          |                        |\\n| COCO-VG-SBU 13.8M |          |                        |\\n| CC+ChatGPT 3.5K |          |                        |\\n\\nTable 1: Comparison of Different LVLMs.\\n\\n'VE', 'Adapt', 'ToP', 'TuP', and '# Token' represent the visual encoder, adaption module, number of total parameters, tuning parameters, and visual tokens fed into the text encoder, respectively.\\n\\n\u2020 indicates that the model is frozen. CC consists of COCO, CC3M, and CC12M. CC, VG, SBU CY, and L400 indicate Conceptual Caption, Visual Genome, COYO-700M, and LAION 400M, respectively. LLaV A-I and G4L represent 158K multimodal instruction-following data in LLaV A and data generated by GPT-4 for building instruction-following LLMs.\\n\\nQA denotes 13 question-answering datasets in InstructBLIP. We count all the data and tuning parameters needed to convert the pretrained vision model and LLM into a visual instruction model. The average score is obtained by normalizing over each row and taking the average of each column.\"}"}
{"id": "q1NaqDadKM", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Is this a rabbit?\\nNo, this is not a rabbit. It is a dog dressed in a bunny costume.\\n\\nNo, it is a dog wearing a bunny costume.\\n\\nWins\\nTie\\nBoth are bad\\n\\nVisual Perception\\nImage Classification\\nObject Counting\\n\\nVisual Knowledge Acquisition\\nMulti-Class Identification\\n\\nOptical Characters Recognition\\n\\n4 benchmarks COCO, VCR\\nCOCO, VCR\\nNoCaps, Flickr-30k\\nSROIE, FUNSD\\n\\n12 benchmarks\\n\\nVisual Reasoning\\nVisual Question Answering\\nVisual Entailment\\n\\n9 benchmarks\\n\\nSNLI-VE\\n\\nKnowledge-grounded Image Description\\nScienceQA, VizWiz\\n\\nVisual Commonsense\\nVisual Commonsense QA\\n\\n9 benchmarks\\n\\nSNLI-VE\\n\\nFigure 2: Our evaluation encompasses quantitative evaluation and online LVLM Arena. Plentiful benchmarks are employed to comprehensively evaluate the six critical capabilities of the models in the quantitative evaluation. In the LVLM Arena, an online platform, users can participate in an online evaluation by chatting with two anonymous models and choosing their preferred model.\\n\\nthe model's understanding of commonly shared human knowledge about generic visual concepts using ImageNetVC [15] and visual commonsense reasoning (VCR) [48]. Specifically, ImageNetVC is utilized for zero-shot visual commonsense evaluation, such as color and shape, while VCR covers various scenes, such as spatial, casual, and mental commonsense.\\n\\nEmbodied Intelligence.\\nEmbodied intelligence aims to create agents, such as robots, which learn to solve challenging tasks requiring environmental interaction. Recently, LLM and LVLM exhibited exceptional effectiveness in guiding the agent to complete a series of tasks. In this evaluation, we utilize high-level tasks as in EmbodiedGPT [49] and employ Minecraft [50], VirtualHome [51], Meta-World [52], and Franks Kitchen [52] as benchmarks.\\n\\nObject Hallucination.\\nIt is known that LVLM suffers from the object hallucination problem, i.e., the generated results are inconsistent with the target images in the descriptions [14]. Evaluating the degree of object hallucination for different LVLMs help understand their respective weaknesses. To this end, we evaluate the object hallucination problem of LVLMs on the MSCOCO dataset [18].\\n\\n2.2 Online Evaluation with LVLM Arena\\nDesigning quantitative evaluations for LVLM to satisfy all capabilities is challenging, as evaluating LVLM responses constitutes an open-ended problem. Inspired by FastChat [53], we introduce the LVLM Arena, an online evaluation framework for LVLMs' pairwise battle with human judgment.\\n\\nFigure 2 illustrates the LVLM Arena, comprising three primary components: matchmaking, chat, and voting. Initially, two models are sampled from the model zoo. Users then converse side-by-side with the models, who remain anonymous. Subsequently, users vote for the superior model.\"}"}
{"id": "q1NaqDadKM", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Illustration of our adopted evaluation methods. To evaluate the zero-shot performance of LVLMs on diverse downstream tasks, we employ four methods including question answering, prefix-based score, multi-turn reasoning, and user study.\\n\\nMatchmaking. The matchmaking module samples two models in a tournament style based on their Elo rating. However, due to the currently limited size of the model hub, we employ random sampling.\\n\\nChat. Users chat side-by-side with two sampled models (which remain anonymous) using images or text inputs. Different from quantitative evaluation, users can chat about anything. Our existing online platform supports only single-round chats due to multi-round chats' high computational and memory demands. Future updates will address this constraint.\\n\\nVoting. After the chat session, users vote for their preferred model. Four options are available: Model A, Model B, Tie, and Both are bad. The Elo rating is subsequently updated using voting results.\\n\\nIn contrast to limited quantitative evaluations, the LVLM Arena provides an open-world evaluation framework that enables users to chat with models about anything, emulating real-world conditions. Besides, users serve as the judge for the battle, which brings more convincing evaluation results than traditional evaluation metrics.\\n\\n2.3 Zero-shot Evaluation\\n\\nLVLMs are capable of capturing a wide range of multimodal patterns and relationships. We evaluate the above 6 categories of capabilities of LVLMs by investigating their zero-shot performance on various tasks. Zero-shot evaluation allows us to evaluate the LVLMs' ability to generalize to new tasks without training the model, which is competent for large-scale evaluation. To be specific, we treat the zero-shot evaluation as various forms of prompt engineering for different tasks (see Fig. 3) as presented in the following.\\n\\n\u2022 Question Answering. Prompting with visual question answering can be used to solve many downstream tasks, which assess how well an LVLM understands the underlying language and visual features. We design proper prompts to ensure that the LLM can produce meaningful results. For example, text prompts of OCR can be \\\"what is written in the image?\\\". Then, we evaluate the answers generated by the LLM using the corresponding metric such as accuracy.\\n\\n\u2022 Prefix-based Score. For multi-choice QA tasks, we can utilize a visual encoder to obtain visual prompts for a given image. Then, the visual prompts are prefixed into the text embeddings, which are fed into the LLM. The likelihood of image-text pair can be generated, which is referred to as a prefix-based score. We can obtain a prefix-based score for each text prompt of the candidate's answer. The answer with the largest prefix-based score is selected as the final answer. We provide the formulation in Sec. A.3 of Appendix.\\n\\n\u2022 Multi-turn Reasoning. Following IdealGPT [16], we use a multi-turn reasoning framework to evaluate complex visual reasoning tasks. Specifically, we utilize an LLM such as ChatGPT to generate sub-questions for a given question, an LVLM to provide corresponding sub-answers, and...\"}"}
{"id": "q1NaqDadKM", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Datasets\\n\\n| Dataset     | ImageNet1K [54] | CIFAR10 [26] | Pets37 [27] | Flowers102 [28] | COCO          | VCR          |\\n|-------------|-----------------|--------------|-------------|-----------------|----------------|--------------|\\n|             | 23.71 24.51 25.89 | 58.20 67.24 64.86 | 34.83 39.17 24.56 | 30.90 32.79 32.05 | 48.90 46.65 38.50 | 25.05 29.29 26.51  |\\n|             | 23.50 21.58     | 61.17 53.09 65.42 | 19.81 33.66 5.91 | 11.99 29.74 20.15 | 20.56 20.86 27.51 | 20.60  |\\n|             |                 | 91.10        | 96.70       |                  | 51.03          |              |\\n| Avg.        | 0.879           | 0.946        | 0.820       | 0.617           | 0.731          | 0.669        |\\n\\nTable 2: Evaluation results of visual perception capability of LVLMs on tasks of Image Classification (Imgcls), Object Counting (OC), and Multi-class Identification (MCI). The best result is bold while the second is underlined. S-SOTA indicates the supervised state-of-the-art results.\\n\\nUser Study.\\n\\nEvaluating the quality of the text generated by an LVLM requires a thorough understanding of the underlying language and context. In embedded artificial intelligence tasks, the LVLM generates a plan for the given instruction, which should be evaluated through various aspects such as recognition accuracy and conciseness in answers. It is hard to implement such an evaluation using an existing metric. Thus, user studies are conducted to assess the quality, relevance, and usefulness of the text generated by the LVLM in a specific context. To maintain evaluation fairness, we randomly shuffle the model\u2019s output order and anonymize outputs during evaluation. Note that our user study does not involve direct interactions with human participants and does not involve potential risks to participants, such as the collection of personal information, or any other aspects that could impact the participants\u2019 rights or well-being. Currently, we do not include an IRB Approval. We are dedicated to addressing the ethical and moral considerations regarding the user evaluation method with thoroughness and commitment, while also providing effective solutions.\\n\\n3 Experiment and Analysis\\n\\nIn this section, we perform a zero-shot evaluation to assess the kinds of capabilities of LVLMs. Specifically, visual perception ability, visual knowledge acquisition, visual reasoning, visual commonsense understanding, visual object hallucination, and embodied intelligence are assessed in Sec. 3.1 \u223c Sec.3.6, respectively. The LVLM arena evaluation result is presented in Sec.3.7. More quantitative results can be found in Appendix C.\\n\\n3.1 Results on Visual Perception\\n\\nVisual perception is an important ability of LVLMs. As presented in Sec. 2.1, we evaluate through image classification (ImgCls), multi-class identification (MCI), and object counting (OC). The evaluation details of tasks are demonstrated in Appendix.B.1. The evaluation results are reported in Table 2. We have three observations. (1) mPLUG-Owl and LLaV A perform best on coarse-grained classification tasks (i.e., ImageNet1K and CIFAR10). The commonality is that they update LLM with 158K instruction-following data. (2) InstructBLIP presents good perception ability in fine-grained ImgCls, OC, and MCI tasks. The main reason is that InstructBLIP may be fine-tuned on various existing VQA datasets, which may make it overfit on these tasks. (3) The performances of LVLMs on ImgCls are significantly inferior to supervised SOTA, indicating plenty of room for LVLM\u2019s perception ability.\\n\\n3.2 Results on Visual Knowledge Acquisition\\n\\nVisual knowledge acquisition involves going beyond image perception to acquire deeper understanding and knowledge. In our study, we evaluate the acquisition of visual knowledge through various tasks, namely Optical Character Recognition (OCR), Key Information Extraction (KIE), and Image Captioning, all performed in a Visual Question Answering (VQA) fashion. The evaluation details of tasks are demonstrated in Appendix.B.2. Table 3 shows the zero-shot performance in visual knowledge acquisition, and we have the following observations. First, BLIP2, InstructBLIP, and other models perform well in OCR and KIE tasks. Second, the performance of Image Captioning is significantly inferior to supervised SOTA, indicating a significant gap in visual knowledge acquisition ability.\"}"}
{"id": "q1NaqDadKM", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Datasets\\n\\n| Dataset       | BLIP2 | InstructBLIP | LLaMA-Adapter-v2 | LLaV | A MiniGPT-4 | mPLUG-Owl | Otter | VPGTrans | S-SOTA | IIIT5K | 80.17 | 83.90 | 36.30 | 31.57 | 25.13 | 26.50 | 17.57 | 51.50 | 99.20 |\\n|--------------|-------|--------------|------------------|------|-------------|-----------|-------|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\\n| IC13         |       |              |                  |      |             |           |       |          |        |        | 81.13  | 82.08  | 20.87 | 16.39 | 16.75 | 14.86 | 09.67 | 61.67 | 98.41 |\\n| IC15         |       |              |                  |      |             |           |       |          |        |        | 66.68  | 73.57  | 29.40 | 26.58 | 21.43 | 21.14 | 18.49 | 42.00 | 91.42 |\\n| Total-Text   |       |              |                  |      |             |           |       |          |        |        | 68.31  | 71.51  | 30.93 | 24.51 | 18.65 | 21.08 | 14.81 | 43.60 | 90.51 |\\n| CUTE80       |       |              |                  |      |             |           |       |          |        |        | 85.07  | 86.11  | 35.76 | 36.46 | 33.33 | 34.03 | 18.75 | 62.85 | 99.31 |\\n| SVT          |       |              |                  |      |             |           |       |          |        |        | 85.78  | 86.86  | 20.40 | 18.55 | 17.47 | 13.45 | 10.51 | 51.16 | 98.31 |\\n| SVTP         |       |              |                  |      |             |           |       |          |        |        | 77.34  | 80.93  | 31.01 | 27.44 | 19.69 | 20.78 | 19.22 | 47.13 | 97.21 |\\n| COCO-Text    |       |              |                  |      |             |           |       |          |        |        | 53.62  | 58.25  | 20.94 | 18.05 | 12.05 | 13.50 | 11.30 | 27.00 | 81.13 |\\n| WordArt      |       |              |                  |      |             |           |       |          |        |        | 73.66  | 75.12  | 38.98 | 35.87 | 31.57 | 32.36 | 21.05 | 53.30 | 72.51 |\\n| CTW          |       |              |                  |      |             |           |       |          |        |        | 67.43  | 68.58  | 18.13 | 16.73 | 15.14 | 12.91 | 10.05 | 40.80 | 88.31 |\\n| HOST         |       |              |                  |      |             |           |       |          |        |        | 57.28  | 61.22  | 16.60 | 15.94 | 14.57 | 11.92 | 10.14 | 32.20 | 77.51 |\\n| WOST         |       |              |                  |      |             |           |       |          |        |        | 68.83  | 73.26  | 21.73 | 20.49 | 17.47 | 14.45 | 12.29 | 37.91 | 87.51 |\\n| KIE          |       |              |                  |      |             |           |       |          |        |        | 0.08   | 0.09   | 0.02  | 0.01  | 0.01  | 0.01  | 0.01  | 0.06  | 97.81 |\\n| SROIE        |       |              |                  |      |             |           |       |          |        |        | 1.02   | 1.03   | 2.16  | 1.93  | 1.20  | 0.41  | 1.91  | 1.27  | 89.45 |\\n| FUNSD        |       |              |                  |      |             |           |       |          |        |        | 1.02   | 1.03   | 2.16  | 1.93  | 1.20  | 0.41  | 1.91  | 1.27  | 89.45 |\\n| Image Captioning |   |              |                  |      |             |           |       |          |        |        | 48.60  | 46.61  | 33.69 | 1.56  | 5.84  | 0.26  | 11.56 | 36.20 | 124.77 |\\n| NoCaps       |       |              |                  |      |             |           |       |          |        |        | 46.65  | 50.69  | 23.85 | 2.23  | 2.66  | 0.02  | 7.12  | -     | -     |\\n\\nTable 3: Comparison of Zero-shot Performance for Large-scale Vision and Language Models (LVLMs) on OCR, KIE, and Image Captioning Tasks. Evaluation metrics include word accuracy for OCR datasets, entity-level F1 score for KIE datasets, and CIDEr score for image captioning datasets.\\n\\n| Dataset       | BLIP2 | InstructBLIP | DocVQA | 4.75 | 5.89 | 8.13 | 6.26 | 3.57 | 2.24 | 3.44 | 2.64 | 54.48 |\\n|--------------|-------|--------------|--------|------|------|------|------|------|------|------|------|--------|\\n| TextVQA      |       |              |        | 31.98 | 39.60 | 43.76 | 38.92 | 21.78 | 38.76 | 21.52 | 17.52 | 73.10 |\\n| STVQA        |       |              |        | 20.98 | 28.30 | 32.33 | 28.40 | 12.20 | 8.30  | 15.23 | 12.88 | -      |\\n| OCR-VQA      |       |              |        | 38.85 | 60.20 | 38.12 | 23.40 | 16.15 | 3.40  | 19.50 | 16.97 | -      |\\n| OKVQA        |       |              |        | 44.93 | 60.52 | 55.93 | 54.36 | 30.06 | 22.89 | 49.01 | 45.31 | -      |\\n| GQA          |       |              |        | 45.53 | 49.96 | 43.93 | 41.30 | 27.03 | 12.60 | 38.12 | 38.54 | 72.10 |\\n| Visdial      |       |              |        | 10.73 | 45.20 | 12.92 | 14.66 | 7.97  | 13.34 | 11.67 | 12.10 | 68.92 |\\n| IconQA       |       |              |        | 62.82 | 56.25 | 41.83 | 42.95 | 28.20 | 09.12 | 26.77 | 25.73 | 83.62 |\\n| VSR          |       |              |        | 63.63 | 41.28 | 50.63 | 51.24 | 41.04 | 10.11 | 06.40 | 37.00 | 70.10 |\\n| KGID         |       |              |        | 60.73 | 46.26 | 54.19 | 49.33 | 20.18 | 2.80  | 27.22 | 20.43 | 92.53 |\\n| ScienceQA    |       |              |        | 65.44 | 65.31 | 62.07 | 62.42 | 40.76 | 11.14 | 50.04 | 11.99 | 73.30 |\\n| IMG          |       |              |        | 34.00 | 56.20 | 56.80 | 57.00 | 52.60 | 55.00 | 56.60 | 47.60 | -      |\\n| VizWiz       |       |              |        | 65.44 | 65.31 | 62.07 | 62.42 | 40.76 | 11.14 | 50.04 | 11.99 | 73.30 |\\n| VE           |       |              |        | 34.00 | 56.20 | 56.80 | 57.00 | 52.60 | 55.00 | 56.60 | 47.60 | -      |\\n| SNLI-VE      |       |              |        | 34.00 | 56.20 | 56.80 | 57.00 | 52.60 | 55.00 | 56.60 | 47.60 | -      |\\n| Average Score|       |              |        | 0.758 | 0.900 | 0.835 | 0.769 | 0.481 | 0.324 | 0.523 | -     | -      |\\n\\nTable 4: Comparison of Zero-shot Performance for LVLM Models on VQA, KGID, and VE Tasks. For VQA and KGID tasks, Mean Reciprocal Rank (MRR) is used for the Visdial, while top-1 accuracy is employed for the remaining tasks.\\n\\nVPGTrans achieve dominant performance in all tasks. This may be because these models use a large visual encoder (i.e., ViT-g/14) and Q-Former updated with massive image-text pairs. A stronger visual encoder and adaption module can extract better tokens entailed with the global and local context, leading to remarkable improvement in visual knowledge acquisition. Second, InstructBLIP presents consistently the best results on all tasks. The main reason is that InstructBLIP overfits these tasks by fine-tuning massive VQA data.\\n\\n3.3 Results on Visual Reasoning\\n\\nVisual reasoning encompasses the ability to comprehensively understand images and perform cognitive tasks. In this section, we evaluate the visual reasoning ability of LVLMs on various tasks, including Visual Question Answering (VQA), Knowledge-Grounded Image Description (KGID), and Visual Entailment (VE) tasks. The evaluation details of tasks are demonstrated in Appendix.B.3.\\n\\nTable 4 shows the zero-shot performance in visual reasoning, and we have the following observations.\\n\\nFirst, compared with BLIP2, InstructBLIP again presents better results overall because it overfits many tasks by fine-tuning massive VQA data. Second, compared with BLIP2, instruction-tuned LVLMs, except for InstructBLIP, generally perform worse than BLIP2. The common words in the instruction data often influence the generated content, which can not be evaluated by the current metrics (see Appendix C). Third, instruction-tuned LVLMs consistently surpass BLIP2 on SNLI-VE where the final answer is obtained by multi-turn reasoning. It shows that instruction-following fine-tuning can produce promising content once a good evaluation scheme is employed.\\n\\n3.4 Results on Visual Commonsense\\n\\nThe visual commonsense evaluation aims to evaluate the model's comprehension of commonly shared human knowledge about generic visual concepts. We use two challenging visual commonsense tasks: SCIENCEQA and IMG.\\n\\nThe visual commonsense evaluation aims to evaluate the model's comprehension of commonly shared human knowledge about generic visual concepts. We use two challenging visual commonsense tasks: SCIENCEQA and IMG.\"}"}
{"id": "q1NaqDadKM", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Datasets\\n\\n|                | BLIP2 | InstructBLIP | LA-V2 | LLaMA | A MiniGPT-4 | mPLUG-Owl | Otter | VPGTrans | S-SOTA |\\n|----------------|-------|--------------|-------|-------|-------------|-----------|-------|----------|--------|\\n| **Color**      | 44.60 | 67.79        | 23.16 | 41.92 | 26.57       | 25.56     | 26.21 | 24.72    | 44.70  |\\n| **Shape**      | 40.14 | 59.06        | 28.16 | 38.74 | 22.88       | 30.72     | 34.19 | 24.69    | 40.50  |\\n| **Mater.**     | 61.49 | 63.58        | 32.51 | 64.91 | 29.50       | 34.24     | 35.81 | 27.21    | 61.90  |\\n| **Compo.**     | 53.86 | 83.25        | 50.38 | 58.53 | 59.96       | 49.47     | 50.72 | 57.21    | 54.00  |\\n| **Others**     | 51.50 | 68.37        | 32.64 | 59.06 | 38.86       | 35.11     | 34.39 | 36.39    | 51.70  |\\n| **Avg**        | 50.30 | 68.41        | 33.37 | 52.63 | 35.55       | 35.02     | 36.26 | 34.04    | 50.50  |\\n\\nVCR\\n\\n|                | VCR   |\\n|----------------|-------|\\n| **36.80**      | 45.60 |\\n| **46.20**      | 46.20 |\\n| **44.40**      | 39.40 |\\n| **39.60**      | 39.60 |\\n| **Average Score** | 0.747 |\\n\\nTable 5: Comparisons of Zero-shot visual commonsense Performance for LVLM Models on VCR and ImageNetVC datasets. Top-1 accuracy is employed for the two datasets.\\n\\nTable 6: Evaluation results of POPE [14] performance of LVLMs on MSCOCO. The accuracy is used to assess the performance.\\n\\nbenchmarks in a zero-shot setting, including ImageNetVC and Visual Commonsense Reasoning (VCR). The evaluation details of tasks are demonstrated in Appendix.B.4. As shown in Table 5, we can find that all those LVLMs represent their abilities to solve visual commonsense problems. First, InstructBLIP performs best (68.41%) among those LVLMs on the ImageNetVC dataset. The main reason is that it is fine-tuned on 1.6M fine-grained VQA data, making it adapt to answer visual common questions. Second, LLaMA-Adapter V2 (46.20%) and LLaV A (46.20%) show the same best performance among those LVLMs on the VCR dataset. The main reason is that instruction-flowing data is used to update the LLM. Note that the final answer of VCR is obtained by multi-turn reasoning. It also shows the significant role of a good evaluation scheme in producing promising content for instruction-tuned models.\\n\\n3.5 Results on Object Hallucination\\n\\nAlthough LVLMs have made significant progress, they still struggle with the issue of hallucination, which refers to their tendency to produce objects that do not align with the descriptions provided in the target images. In this section, we focus on evaluating such object hallucination problems on MSCOCO captioning dataset. Following POPE [14] evaluation pipeline which is a multi-step QA procedure, we prompt LVLMs with multiple Yes-or-No questions. For example, \u2018Is there a person in the image?\u2019 We use accuracy as the evaluation metric. From Table 6, we could come to the following conclusions. InstructBlip performs best in the hallucination problem, followed by BLIP2, whose average accuracy both reached more than 80%. We find that instruction-tuned models, except for InstructBLIP, perform worse than BLIP2 because they tend to answer \u2018Yes\u2019 to the question, which shows that LVLMs are prone to generate objects frequently occurring in the instruction data. Such object hallucination problem can be alleviated by a multi-turn reasoning pipeline shown in the experiments on SNLI-VE and VCR.\\n\\n3.6 Results on Embodied Intelligence\\n\\nIn this section, we present the evaluation results focusing on embodied intelligence. To appraise the effectiveness of planning outputs using the given image, we conducted a user study involving 15 participants. The study comprised 6 household scenarios carefully selected from VirtualHome [51]. Specifically, the participants rated the generated plans from different LVLM models using a scoring system similar to [49]. The evaluation comprised five dimensions with scores ranging from 1 to 5. These dimensions included object recognition accuracy, spatial relationship understanding, level of conciseness in the response, reasonability of the planning, and executability of the planning. The resulting average scores for the different models among the participants are presented in Table 7 below. Furthermore, in the Appendix C, we present quantitative evaluation results for Franka Kitchen [52], Minecraft [50], and Meta-World [72]. Based on the evaluation results, we observe that visual...\"}"}
{"id": "q1NaqDadKM", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Generated planning quality evaluation on embodied tasks. Five dimensions including object recognition, spatial relationship, conciseness, reasonability, and executability are used to assess the performance.\\n\\n3.7 Results on Online Arena Evaluation\\n\\nThe arena features anonymous and randomized pairwise battles in a crowd-sourced manner. We have collected 634 pieces of evaluation data since we launch the LVLM arena. The collected data shows almost the same number of battle outcomes for 'Model A wins' and 'Model B wins.' Moreover, 8% battle outcomes are voted as 'both are bad,' implying that the current LVLMs still struggle to generate good answers for open-world visual questions. Furthermore, we rank the selected LVLMs with Elo rating [73] using the collected data by following Fastchat [53] and [74]. As shown in Fig. 1 (b), mPLUG-Owl, MiniGPT-4, and Otter, which are fine-tuned with amounts of instruction-following data with updating many parameters, are the top-3 best models in the open-world VQA scenario, indicating the significance of instruction-following tuning and effective parameter update. Moreover, InstructBLIP perform best on in-domain capability evaluation, while being much worse than many instruction-tuned models, implying severe overfitting issue, as shown in Fig. 1.\\n\\n3.8 Takeaway Analysis\\n\\nWe can conclude some actionable insights from our evaluation results. First, the quality of visual instruction data matters more than quantity in the open-world VQA. We observe that MiniGPT-4, which is tuned by only 3.5K high-quality visual instruction data performs much better than InstructBLIP tuned on visual instruction data adapted from various existing VQA datasets in our Multi-Modality Arena. Second, a strong visual encoder can help extract detailed information from the image, leading to good performance in OCR tasks. For instance, we see that BLIP2, InstructBLIP, and VPGTrans achieve better performance than the remaining 5 LVLMs. This may be because the visual encoder ViT-g/14 used in BLIP2, InstructBLIP, and VPGTrans is more powerful than ViT-L/14 employed in the remaining LVLMs. Third, multi-turn reasoning helps alleviate the hallucination issue, indicating that the evaluation method with critical thinking can induce the correct prediction from the model. We find that LVLM with multi-turn reasoning can determine whether an object exists in the image more accurately than single-turn reasoning. Hence, multi-turn reasoning is appropriate to assess the full potential of the model. Fourth, LVLMs tuned with high-quality instruction-following data present more promising planning ability than models without being tuned with instruction data as demonstrated in Table 7.\\n\\n4 Conclusion\\n\\nThis paper proposes a comprehensive evaluation benchmark for large vision-language models called LVLM-eHub that incorporates both quantitative performance evaluation and human feedback evaluation. For the quantitative evaluation, we employ 16 tasks spanning over 40+ text-related visual datasets to assess the six essential capabilities of LVLM models. Additionally, we have established an online LVLM Arena to gather human feedback on LVLM models continually. This arena serves as an invaluable resource, providing an Elo rating rank that offers LVLMs ranking in the open-world scenario. Our evaluation results reveal several important findings, stimulating the future development of LVLMs. We will make ongoing efforts to build a platform for LVLM evaluation as discussed in Sec. A.4.\"}"}
{"id": "q1NaqDadKM", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\\n\\n[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\\n\\n[3] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality, March 2023.\\n\\n[4] OpenAI. Gpt-4 technical report, 2023.\\n\\n[5] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems 35:23716\u201323736, 2022.\\n\\n[6] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\\n\\n[7] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023.\\n\\n[8] Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu, and Tat-Seng Chua. Transfer visual prompt generator across llms. arXiv preprint arXiv:2305.01278, 2023.\\n\\n[9] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.\\n\\n[10] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\\n\\n[11] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.\\n\\n[12] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023.\\n\\n[13] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023.\\n\\n[14] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023.\\n\\n[15] Heming Xia, Qingxiu Dong, Lei Li, Jingjing Xu, Ziwei Qin, and Zhifang Sui. Imagenetvc: Zero-shot visual commonsense evaluation on 1000 imagenet categories. arXiv preprint arXiv:2305.15028, 2023.\\n\\n[16] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023.\"}"}
{"id": "q1NaqDadKM", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Guangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan Kankanhalli, and Ying Shan. What makes for good visual tokenizers for large language models? arXiv preprint arXiv:2305.12223, 2023.\\n\\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.\\n\\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, 2018.\\n\\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021.\\n\\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32\u201373, 2017.\\n\\nMinwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Sae-hoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset, 2022.\\n\\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\\n\\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.\\n\\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211\u2013252, 2015.\\n\\nAlex Krizhevsky. Learning multiple layers of features from tiny images. 2009.\\n\\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE Conference on Computer Vision and Pattern Recognition, 2012.\\n\\nMaria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing, Dec 2008.\\n\\nGuangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan Kankanhalli, and Ying Shan. What makes for good visual tokenizers for large language models? arXiv preprint arXiv:2305.12223, 2023.\\n\\nAnand Mishra, Karteek Alahari, and C. V. Jawahar. Top-down and bottom-up cues for scene text recognition. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 2687\u20132694, 2012.\\n\\nDimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles Mestre, Joan Mas, David Fernandez Mota, Jon Almaz\u00e0n Almaz\u00e0n, and Llu\u00eds Pere de las Heras. Icdar 2013 robust reading competition. In 2013 12th International Conference on Document Analysis and Recognition, pages 1484\u20131493, 2013.\\n\\nDimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian Lu, Faisal Shafait, Seiichi Uchida, and Ernest Valveny. Icdar 2015 competition on robust reading. In 2015 13th International Conference on Document Analysis and Recognition (ICDAR), pages 1156\u20131160, 2015.\"}"}
{"id": "q1NaqDadKM", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[34] Anhar Risnumawan, Palaiahankote Shivakumara, Chee Seng Chan, and Chew Lim Tan. A robust arbitrary text detection system for natural scene images. *Expert Systems with Applications*, 41(18):8027\u20138048, 2014.\\n\\n[35] Cunzhao Shi, Chunheng Wang, Baihua Xiao, Song Gao, and Jinlong Hu. End-to-end scene text recognition using tree-structured models. *Pattern Recognition*, 47(9):2853\u20132866, 2014.\\n\\n[36] Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan Tian, and Chew Lim Tan. Recognizing text with perspective distortion in natural scenes. In *2013 IEEE International Conference on Computer Vision*, pages 569\u2013576, 2013.\\n\\n[37] Andreas Veit, Tomas Matera, Luk\u00e1s Neumann, Jiri Matas, and Serge J. Belongie. Coco-text: Dataset and benchmark for text detection and recognition in natural images. *ArXiv*, abs/1601.07140, 2016.\\n\\n[38] Xudong Xie, Ling Fu, Zhifei Zhang, Zhaowen Wang, and Xiang Bai. Toward understanding wordart: Corner-guided transformer for scene text recognition. 2022.\\n\\n[39] Yuliang Liu, Lianwen Jin, Shuaitao Zhang, Canjie Luo, and Sheng Zhang. Curved scene text detection via transverse and longitudinal sequence connection. *Pattern Recogn.*, 90(C):337\u2013345, Jun 2019.\\n\\n[40] Yuxin Wang, Hongtao Xie, Shancheng Fang, Jing Wang, Shenggao Zhu, and Yongdong Zhang. From two to one: A new scene text recognizer with visual language modeling network. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 14194\u201314203, 2021.\\n\\n[41] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. In *2019 International Conference on Document Analysis and Recognition (ICDAR)*, pages 1516\u20131520. IEEE, 2019.\\n\\n[42] Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. Funsd: A dataset for form understanding in noisy scanned documents. In *2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)*, volume 2, pages 1\u20136. IEEE, 2019.\\n\\n[43] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In *2019 IEEE/CVF International Conference on Computer Vision (ICCV)*, pages 8947\u20138956, 2019.\\n\\n[44] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. *Transactions of the Association for Computational Linguistics*, 2:67\u201378, 02 2014.\\n\\n[45] Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual entailment: A novel task for fine-grained image understanding. *arXiv preprint arXiv:1901.06706*, 2019.\\n\\n[46] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. *Advances in Neural Information Processing Systems*, 35:2507\u20132521, 2022.\\n\\n[47] Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. Vizwiz: nearly real-time answers to visual questions. In *Proceedings of the 23nd annual ACM symposium on User interface software and technology*, pages 333\u2013342, 2010.\"}"}
{"id": "q1NaqDadKM", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6720\u20136731, 2019.\\n\\nYao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. arXiv preprint arXiv:2305.15021, 2023.\\n\\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.\\n\\nXavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8494\u20138502, 2018.\\n\\nAbhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint arXiv:1910.11956, 2019.\\n\\nWei-Lin Chiang Hao Zhang Joseph E. Gonzalez Lianmin Zheng, Ying Sheng and Ion Stoica. Fastchat. https://github.com/lm-sys/FastChat, 2023.\\n\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255, 2009.\\n\\nXiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V. Le. Symbolic discovery of optimization algorithms, 2023.\\n\\nH M Dipu Kabir. Reduction of class activation uncertainty with background information, 2023.\\n\\nMaxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv\u00e9 Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023.\\n\\nQin Xu, Jiahui Wang, Bo Jiang, and Bin Luo. Fine-grained visual classification via internal ensemble learning transformer. IEEE Transactions on Multimedia, pages 1\u201314, 2023.\\n\\nShuai Zhao, Xiaohan Wang, Linchao Zhu, and Yi Yang. Clip4str: A simple baseline for scene text recognition with pre-trained vision-language model, 2023.\\n\\nDarwin Bautista and Rowel Atienza. Scene text recognition with permuted autoregressive sequence models. In European Conference on Computer Vision, pages 178\u2013196, Cham, 10 2022. Springer Nature Switzerland.\\n\\nTao Sheng, Jie Chen, and Zhouhui Lian. Centripetaltext: An efficient text instance representation for scene text detection. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.\\n\\nYang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou. Layoutlmv2: Multi-modal pre-training for visually-rich document understanding. In ACL-IJCNLP 2021, January 2021.\\n\\nChuwei Luo, Changxu Cheng, Qi Zheng, and Cong Yao. Geolayoutlm: Geometric pre-training for visual information extraction. CoRR, abs/2304.10759, 2023.\"}"}
{"id": "q1NaqDadKM", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. GIT: A generative image-to-text transformer for vision and language. Transactions on Machine Learning Research, 2022.\\n\\nMinesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 2200\u20132209, 2021.\\n\\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Alexander Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali: A jointly-scaled multilingual language-image model. 2023.\\n\\nBinh X Nguyen, Tuong Do, Huy Tran, Erman Tjiputra, Quang D Tran, and Anh Nguyen. Coarse-to-fine reasoning for visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4558\u20134566, 2022.\\n\\nIdan Schwartz, Seunghak Yu, Tamir Hazan, and Alexander G Schwing. Factor graph attention. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2039\u20132048, 2019.\\n\\nPan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. In The 35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, 2021.\\n\\nFangyu Liu, Guy Edward Toh Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 2023.\\n\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.\\n\\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 1094\u20131100. PMLR, 2020.\\n\\nArpad E Elo. The proposed uscf rating system. Its development, theory, and applications. Chess Life, 22(8):242\u2013247, 1967.\\n\\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.\\n\\nNandan Thakur, Nils Reimers, Johannes Daxenberger, and Iryna Gurevych. Augmented SBERT: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 296\u2013310, Online, June 2021. Association for Computational Linguistics.\\n\\nYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19358\u201319369, 2023.\\n\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\"}"}
{"id": "q1NaqDadKM", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045, 2023.\\n\\nAmanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8309\u20138318, 2019.\\n\\nAli Furkan Biten, Rub\u00e8n Tito, Andr\u00e9s Mafla, Lluis Gomez, Mar\u00e7al Rusi\u00f1ol, Minesh Mathew, C.V. Jawahar, Ernest Valveny, and Dimosthenis Karatzas. Icdar 2019 competition on scene text visual question answering. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1563\u20131570, 2019.\\n\\nAnand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 947\u2013952, 2019.\\n\\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3190\u20133199, 2019.\\n\\nDrew A. Hudson and Christopher D. Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6693\u20136702, 2019.\\n\\nMert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? In The Eleventh International Conference on Learning Representations, 2022.\\n\\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888\u201312900. PMLR, 2022.\\n\\nAbhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 M.F. Moura, Devi Parikh, and Dhruv Batra. Visual Dialog. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\\n\\nPeter Young, Alice Lai, Micah Hodosh, and J. Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014.\"}"}
