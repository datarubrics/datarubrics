{"id": "ddPXQt-gM--", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Benchmarking Heterogeneous Treatment Effect Models through the Lens of Interpretability\\n\\nJonathan Crabb\u00e9\\nUniversity of Cambridge\\njc2133@cam.ac.uk\\n\\nAlicia Curth\\nUniversity of Cambridge\\namc253@cam.ac.uk\\n\\nIoana Bica\\nUniversity of Oxford\\nThe Alan Turing Institute\\nioana.bica@eng.ox.ac.uk\\n\\nMihaela van der Schaar\\nUniversity of Cambridge\\nThe Alan Turing Institute\\nUCLA\\nmv472@cam.ac.uk\\n\\nAbstract\\nEstimating personalized effects of treatments is a complex, yet pervasive problem. To tackle it, recent developments in the machine learning (ML) literature on heterogeneous treatment effect estimation gave rise to many sophisticated, but opaque, tools: due to their flexibility, modularity and ability to learn constrained representations, neural networks in particular have become central to this literature. Unfortunately, the assets of such black boxes come at a cost: models typically involve countless nontrivial operations, making it difficult to understand what they have learned. Yet, understanding these models can be crucial \u2013 in a medical context, for example, discovered knowledge on treatment effect heterogeneity could inform treatment prescription in clinical practice. In this work, we therefore use post-hoc feature importance methods to identify features that influence the model's predictions. This allows us to evaluate treatment effect estimators along a new and important dimension that has been overlooked in previous work: We construct a benchmarking environment to empirically investigate the ability of personalized treatment effect models to identify predictive covariates \u2013 covariates that determine differential responses to treatment. Our benchmarking environment then enables us to provide new insight into the strengths and weaknesses of different types of treatment effects models as we modulate different challenges specific to treatment effect estimation \u2013 e.g. the ratio of prognostic to predictive information, the possible nonlinearity of potential outcomes and the presence and type of confounding.\\n\\n1 Introduction\\nThe need to estimate the effects of actions \u2013 such as treatments, policies and other interventions \u2013 is ubiquitous in many domains, ranging from economics to medicine. Many applications where treatment effects are of interest additionally operate under high stakes, for example treatment decisions in a hospital setting \u2013 making it particularly important that estimates leading to individual decisions are reliable. As interest in designing personalized treatments is growing across fields, a substantial literature on learning treatment effect heterogeneity has emerged in machine learning (ML) in recent years. In this context, a plethora of sophisticated methods for estimating conditional average treatment...\"}"}
{"id": "ddPXQt-gM--", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Illustration of the ITErpretability benchmarking environment. (1) Covariates are extracted from any dataset. (2) These covariates are labeled with a transparent data generating process for the assignments and the outcomes. This results in a semi-synthetic dataset $D$. (3) Since the data generating process is transparent, we know the indices $I_{\\\\text{pred}}$ of predictive covariates. (4) We fit a black-box CATE estimator $\\\\hat{\\\\tau}$ on $D$. (5) We then use a feature importance method to assign feature importance scores $a_i(\\\\hat{\\\\tau}, x)$ to each covariate $x_i$. (6) We evaluate each CATE estimator based on a new metric measuring the accordance between the most important features and the predictive covariates.\\n\\nEffects (CATE) and/or an individual's potential outcomes (POs) under different treatments have been proposed in the recent ML literature (see e.g. [1]), all aiming to improve the precision in estimating effects. In particular, recent work has produced both model-agnostic estimation strategies, which can incorporate any ML method into the estimation of effects [2\u20134], and model-specific strategies, which adapt specific ML methods to the treatment effect estimation problem \u2013 for example random forests [5, 6], gaussian processes [7, 8] and, most predominantly, neural networks [4, 9\u201313].\\n\\nDespite the growing interest in such methods, [14] noted that their evaluation has been quite one-dimensional: Most, if not all, of this work has focussed on assessing performance of proposed algorithms in terms of the Precision of Estimating Heterogeneous Effects (PEHE) criterion of [15], which measures the root mean squared error (RMSE) of the estimated CATE function on a test-set. However, in many applications, black-box predictions of expected treatment effects do not suffice. In the context of drug development, for example, it is at least equally important to assess whether an algorithm discovers the correct drivers of the underlying effect heterogeneity or leads to the right interpretation thereof [16]: discovering effect-modifiers can be important both in exploratory clinical trial phases, e.g. to identify potential biomarkers which may help to explain an enhanced treatment effect for future drug development, and in confirmatory trial phases, e.g. to rule out that a drug is ineffective for some biomarker signatures or to otherwise adapt prescription criteria [16, 17]. Such interpretation could also be important in clinical practice e.g. when explaining treatment recommendations derived from an estimated CATE function. In this paper, we therefore leverage recent advances in explainable artificial intelligence [18\u201320] to interpret the discoveries of ML-based CATE estimators, and propose a benchmarking environment which we use to provide insight into the performance of different learning strategies in discovering drivers of heterogeneity.\\n\\nRelated work. How to interpret CATE estimators has received little attention in the ML literature thus far. Some work implicitly enables interpretation by relying on methods that are inherently more interpretable, e.g. linear regressions [21, 22] or tree-based models [23, 24]. Another recent stream of work explicitly focuses on interpretability, similarly proposing the use of methods that are inherently interpretable: [25] rely on decision lists, [26] construct interpretable hyper-boxes for matching, [27] use causal rule ensembles, [28] use mixture models with sparse components, [29] use the fused lasso for estimation of subgroup piece-wise constant treatment effects and [30] rely on an additive neural network architecture and [31] propose using transformer backbones that can be interpreted using their attention weights. With a goal similar to our work, [16] investigate how well different forest-based CATE estimation strategies discover effect modifiers, but they rely on variable importance scores inherent to random forests to do so. To the best of our knowledge, the only work that considers post-hoc interpretability of already fitted, arbitrary black-box CATE models uses model distillation approaches to do so: [32] use the fitted black-box CATE estimator as a \u201cteacher\u201d and a multi-task decision tree as an interpretable \u201cstudent\u201d and similarly, [33] propose to create interpretable CATE estimators by fitting an arbitrary interpretable model on top of the potential outcome predictions of a NN-based first stage estimator. Such approaches do not provide an interpretation of the black-box output directly and therefore need to rely on the interpretable student model being good enough to cover the complexities of the original estimator. In this paper, we take a different approach and consider the use of post-hoc feature importance methods to interpret black-box CATE estimators directly.\"}"}
{"id": "ddPXQt-gM--", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Contributions & Outlook.\\n\\nOur contributions are threefold: (i) we study how to interpret black-box CATE estimators and use this to evaluate them along a new and important dimension that has been overlooked in previous work \u2013 namely, their ability to correctly discover drivers of effect heterogeneity \u2013, (ii) we propose a benchmark environment to do so, and (iii) we provide new insights into the performance of existing methods on this new task. We proceed as follows: We begin by recalling fundamentals of the CATE estimation setting and discuss its unique characteristics in Section 2. In Section 3, we review feature importance methods and discuss how they can be applied to interpret what CATE estimators have learned about treatment effect heterogeneity, with the ultimate goal to compare different CATE estimators on their ability to identify predictive covariates/features (i.e. covariates that determine differential responses to treatment and, hence, are the ones that truly matter when estimating treatment effects). In Section 4, we then introduce a new benchmark environment for evaluating black-box CATE estimators, supplemented with feature importance methods to interpret their output, on precisely this task. This ITErpretability benchmark, as illustrated in Figure 1, can be used with any covariates dataset and is semi-synthetic: it relies on real covariates with synthetic treatment assignment and outcomes, supplying us with otherwise unavailable ground-truth access to predictive covariates. Finally, in Section 5, we use this benchmark to investigate the performance of existing, neural network-based, CATE estimation strategies on this new task, and provide interesting insights into the impact of typical difficulties in CATE problems: the strength of predictive relative to prognostic information, the possible nonlinearity of potential outcomes and the presence of confounding.\\n\\n2 Setting: The CATE Estimation Problem\\n\\nWe consider a standard treatment effect estimation setting as formalized within the Neyman-Rubin potential outcomes framework \\\\[34\\\\]. We assume access to an observational i.i.d. dataset \\\\[D = \\\\{(Y_n, X_n, W_n)\\\\}_{n=1}^N\\\\]. Here, \\\\(Y_n \\\\in \\\\mathbb{R}\\\\) is a binary or continuous outcome of interest, \\\\(X_n \\\\in \\\\mathbb{R}^d\\\\) is a vector of pre-treatment covariates, and \\\\(W_n \\\\in \\\\{0, 1\\\\}\\\\) is a binary treatment assigned according to a (usually unknown) propensity score \\\\(\\\\pi(x) = P(W = 1 | X = x)\\\\). Each individual has multiple potential outcomes (POs) \\\\(Y_n(w)\\\\) associated with different treatment values, yet only the outcome associated with the received treatment is observed, i.e. \\\\(Y_n = Y_n(W_n)\\\\). To investigate treatment effect heterogeneity, we focus on the conditional average treatment effect (CATE), computed as the expected difference between the two POs for an individual with covariates \\\\(X = x\\\\):\\n\\n\\\\[\\n\\\\tau(x) = E_{P}[Y(1) - Y(0) | X = x] = \\\\mu_1(x) - \\\\mu_0(x)\\n\\\\]\\n\\nwith \\\\(\\\\mu_w(x) = E_{P}[Y(w) | X = x]\\\\) the expected potential outcome.\\n\\nWhat Makes CATE Estimation Special Relative to Standard Supervised Learning?\\n\\nAs we discuss in more detail in Appendix A, three characteristics are generally considered central to the CATE estimation problem (see e.g. \\\\[13, 14\\\\]):\\n\\n1. The need to rely on strong untestable assumptions to ensure identifiability of treatment effects from observational data (here: ignorability assumptions \\\\[35\\\\], as discussed further in Appendix A).\\n2. The presence of covariate shift due to confounding (correlation between covariates and treatment assignment).\\n3. The absence of the target label of interest \\\\((Y(1), Y(0))\\\\) as only one of the POs can be observed in practice, or conversely, that CATE can arise either as a single regression or as a difference between two functions\\n\\nIn our context, 1 & 2 are mainly important for the construction of benchmarks: as discussed in Section 4, characteristic 1 (and to some extent 3) leads to the necessity to rely on (semi-)synthetic data as ground truth knowledge of the data-generating process is generally unavailable from real data (see also the discussion in \\\\[14\\\\]). Characteristic 2 leads to a natural experimental knob to include in a benchmark to be modulated when evaluating models. As we discuss in detail in Section 3, it is 3 that leads to most interesting considerations when studying how to interpret CATE estimators. It also lead to the emergence of different CATE estimation strategies: generally, as discussed in \\\\[4\\\\], \\\\(\\\\tau(x)\\\\) can either be estimated indirectly using the difference between the PO regression estimates (i.e. as \\\\(\\\\hat{\\\\mu}_1 - \\\\hat{\\\\mu}_0\\\\)) as in most methods proposed in the recent ML literature \\\\[7, 8, 10, 12, 13\\\\] or directly by fitting a regression model using pseudo-outcome \\\\(\\\\hat{\\\\tau}(x)\\\\) as a surrogate for the unobserved PO difference (relying on initial estimates of some of the nuisance functions \\\\(\\\\pi = (\\\\mu_0, \\\\mu_1, \\\\pi)\\\\)) as in e.g. \\\\[2\u20134\\\\]. We discuss different instantiations of these strategies considered in our experiments in Section 5.\"}"}
{"id": "ddPXQt-gM--", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Feature Importance in the Context of Treatment Effect Estimation\\n\\nIn this section, we discuss how to use feature importance to investigate what a CATE estimator (or model) has learned about treatment effect heterogeneity. We start by describing a general feature importance formalism: We assume that we have access to a black-box model \\\\( \\\\hat{\\\\mu} \\\\) : \\\\( \\\\mathbb{R}^d \\\\to \\\\mathbb{R} \\\\) to estimate the CATE \\\\( \\\\hat{\\\\mu} \\\\). Feature importance methods permit to understand the prediction of this model by highlighting features (covariates) the model is sensitive to. Concretely, this is done by assigning an importance score \\\\( a_i(\\\\hat{\\\\mu}, x) \\\\in \\\\mathbb{R} \\\\) to each feature \\\\( x_i \\\\) contained in the vector \\\\( x \\\\) with \\\\( i \\\\in [d] \\\\). This score reflects the importance of \\\\( x_i \\\\) to predict the CATE \\\\( \\\\hat{\\\\mu}(x) \\\\). The score is such that the importance of a feature \\\\( x_i \\\\) increases with \\\\( |a_i(\\\\hat{\\\\mu}, x)| \\\\). When it comes to the sign, features with \\\\( a_i(\\\\hat{\\\\mu}, x) > 0 \\\\) tend to increase the CATE \\\\( \\\\hat{\\\\mu}(x) \\\\) and features with \\\\( a_i(\\\\hat{\\\\mu}, x) < 0 \\\\) tend to decrease it.\\n\\nThere exist many methods to assign importance scores \\\\( a_i(\\\\hat{\\\\mu}, x) \\\\) to the features. Different feature importance methods tend to attribute different relative importance between features \\\\( [36, 37] \\\\). This is because they measure different characteristics of the black-box model: gradient-based methods compute scores based on the model's gradient with respect to the features \\\\( [38\u201341] \\\\); perturbation-based methods compute scores based on the model's sensitivity to features perturbations \\\\( [42\u201344] \\\\); and some other methods rely on the neuron's activation to compute the importance scores \\\\( [45, 46] \\\\).\\n\\nIn Section 5, we use Integrated Gradients \\\\( [39] \\\\) as our main feature importance method. This is because it offers the best performances empirically and is typically more computationally efficient than the previous methods. A comparison between different feature importance methods is provided in Appendix B.\\n\\nWe will now discuss the specificity of CATE models in an interpretability perspective.\\n\\nWhat Makes CATE Interpretability Special?\\n\\nAbove, we fixed the black-box to interpret to be the CATE estimate \\\\( \\\\hat{\\\\mu} \\\\). However, CATE can of course also be written (and, as in most methods proposed in the recent ML literature, be estimated) in terms of the expected POs, i.e. \\\\( \\\\mu = \\\\mu_1 - \\\\mu_0 \\\\), which have their own feature importance \\\\( a_i(\\\\mu_0, x) \\\\) and \\\\( a_i(\\\\mu_1, x) \\\\). In general, feature importance for CATE will differ from those of the POs, leading to different insights. This multiplicity of interpretation certainly distinguishes CATE models from interpreting standard supervised learners and deserves a discussion.\\n\\nIn order to attach meaning to those possible interpretations, it is useful to consider an important distinction between so-called predictive and prognostic covariates made in the medical literature \\\\( [47, 48] \\\\). Prognostic covariates determine outcome regardless of treatment assignment \u2013 common risk factors such as age or gender may fall in this category. Such variables are taken into account by the two potential outcomes \\\\( \\\\mu_w \\\\) in a similar way. In this way, prognostic covariates correspond to features \\\\( x_i \\\\) that are important for both POs \\\\( a_i(\\\\mu_0, x) \\\\neq 0 \\\\) and \\\\( a_i(\\\\mu_1, x) \\\\neq 0 \\\\). Predictive covariates, on the other end, are predictive of effect heterogeneity, i.e they determine differential responses to treatment \u2013 hormone receptor status in cancer patients is one such example \\\\( [49] \\\\). In this way, predictive covariates correspond to features \\\\( x_i \\\\) that are important for the CATE \\\\( a_i(\\\\mu, x) \\\\neq 0 \\\\). If we assume that the potential outcomes \\\\( \\\\mu_0, \\\\mu_1 \\\\) are differentiable with respect to the covariates, one can extend the previous definitions globally. In this case, a feature \\\\( x_i \\\\) is prognostic if both potential outcomes depend on that feature:\\n\\n\\\\[\\n\\\\frac{\\\\partial \\\\mu_0}{\\\\partial x_i} \\\\neq 0 \\\\quad \\\\text{and} \\\\quad \\\\frac{\\\\partial \\\\mu_1}{\\\\partial x_i} \\\\neq 0.\\n\\\\]\\n\\nSimilarly, a feature \\\\( x_i \\\\) is predictive if the CATE depends on that feature:\\n\\n\\\\[\\n\\\\frac{\\\\partial \\\\mu}{\\\\partial x_i} \\\\neq 0.\\n\\\\]\\n\\nIn the medical context, any measured patient covariate could be prognostic, predictive or both (or, of course, irrelevant).\\n\\nWhereas \\\\( [33] \\\\) used a model-distillation approach to interpret only the PO models, we argue that it is most interesting in the CATE estimation context to interpret the learned CATE models directly by focusing on the discovery of predictive covariates. Identifying such predictive covariates can, for example, provide precious information to support exploratory analyses in clinical trials, which in turn allow pharmaceutical companies to refine the target population for a treatment, improving the likelihood of a successful later stage trial \\\\( [16] \\\\). From that perspective, CATE models \\\\( \\\\hat{\\\\mu} \\\\) that are better at identifying predictive covariates are clearly preferable. Due to the absence of treatment effect labels (Section 2), it is far from obvious whether all estimation strategies result in successful identification of predictive covariates. Furthermore, since indirect learners target the CATE indirectly through the POs, there is no guarantee that the resulting models can distinguish between prognostic and predictive co-variates. With these simple observations, it is obvious that CATE interpretability comes with a unique set of challenges. We will now introduce a benchmark to study CATE models through this angle.\"}"}
{"id": "ddPXQt-gM--", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The ITErpretability Benchmark\\n\\nNext, we describe our proposed ITErpretability benchmark that uses ideas from interpretability to measure the ability of treatment effect estimators to identify predictive covariates. We propose a framework that relies on a semi-synthetic data generating process (DGP), which is standard in the CATE estimation literature: because identifiability assumptions are generally untestable, simulating outcomes and treatment assignments ensures that they hold; additionally it ensures that the underlying CATE function is known. In our context, we cannot rely on existing and established semi-synthetic benchmarks such as IHDP or ACIC2016, most importantly because they did not record which covariates are predictive or prognostic. Further, the experimental knobs considered therein are not of primary interest in our setting; instead we thus design our own DGPs that allow to us to obtain interesting new insights in our experiments. Below, we discuss our DGP and proposed metrics.\\n\\nDGP.\\n\\nWe would like to rely on a DGP that covers a range of realistic scenarios and for which we can clearly identify prognostic and predictive covariates. Since this last information is generally not available in real observational data, we use a semi-synthetic approach in which we reuse covariates \\\\( X_n \\\\) from a real dataset and synthetically generate the treatment assignments \\\\( W_n \\\\) and outcomes \\\\( Y_n \\\\) for all \\\\( n \\\\in [N] \\\\). In this way, the resulting semi-synthetic dataset has realistic covariates and we have a full knowledge on how outcomes are generated. In particular, we can restrict to DGPs for which prognostic and predictive covariates are clearly distinct and identifiable. We implement this by selecting non-overlapping subsets \\\\( I_{\\\\text{prog}} \\\\subseteq [d] \\\\) of prognostic and two subsets \\\\( I_0, I_1 \\\\subseteq [d] \\\\) of predictive covariates.\\n\\nThe prognostic covariates similarly contribute to both POs \\\\( Y(0), Y(1) \\\\) through a function \\\\( x \\\\mapsto \\\\mu_{\\\\text{prog}}(x_{I_{\\\\text{prog}}}) \\\\), where we let \\\\( x_{I_{\\\\text{prog}}} \\\\) denote the vector \\\\( (x_i)_{i \\\\in I_{\\\\text{prog}}} \\\\). The predictive covariates, on the other hand, contribute to either only \\\\( Y(0) \\\\) through a function \\\\( x \\\\mapsto \\\\mu_{\\\\text{pred}0}(x_{I_0}) \\\\) or to \\\\( Y(1) \\\\) through a function \\\\( x \\\\mapsto \\\\mu_{\\\\text{pred}1}(x_{I_1}) \\\\).\\n\\nWe include a predictive scale \\\\( \\\\sigma_{\\\\text{pred}} \\\\geq 0 \\\\) that permits to tune the relative strength between the prognostic and predictive contributions to the POs. This full process is detailed in Algorithm 1. All the experiments from Section 5 are produced by varying the inputs of this algorithm; in particular, we consider different types of outcome functions and propensity scores.\\n\\nAlgorithm 1: Semi-Synthetic Data Generating Process\\n\\nInput: Covariates dataset \\\\( \\\\{X_n\\\\}_{n=1}^N \\\\), Prognostic function \\\\( \\\\mu_{\\\\text{prog}}: \\\\mathbb{R} \\\\to \\\\mathbb{R} \\\\), Predictive functions \\\\( \\\\mu_{\\\\text{pred}0}, \\\\mu_{\\\\text{pred}1}: \\\\mathbb{R} \\\\to \\\\mathbb{R} \\\\), Propensity score \\\\( \\\\tau: X \\\\to \\\\mathbb{R} \\\\), Feature sets size \\\\( n_{I_{\\\\text{prog}}} \\\\in [N] \\\\), Predictive scale \\\\( \\\\sigma_{\\\\text{pred}} \\\\geq 0 \\\\), Noise level \\\\( \\\\epsilon \\\\geq 0 \\\\).\\n\\nOutput: Semi-synthetic observational dataset \\\\( D = \\\\{(Y_n, X_n, W_n)\\\\}_{n=1}^N \\\\), Prognostic features \\\\( I_{\\\\text{prog}} \\\\subseteq [d] \\\\), Predictive features \\\\( I_{\\\\text{pred}} = I_0 \\\\cup I_1 \\\\).\\n\\nEnsure: \\\\( d > 3 \\\\cdot n_{I_{\\\\text{prog}}} \\\\) /* Avoid overlap between \\\\( I_{\\\\text{prog}} \\\\), \\\\( I_0 \\\\) and \\\\( I_1 \\\\) */\\n\\nSample \\\\( 3 \\\\cdot n_{I_{\\\\text{prog}}} \\\\) elements from \\\\( [d] \\\\) without replacement /* Get relevant features */ \\\\( I_{\\\\text{prog}}, I_0, I_1 \\\\)\\n\\nSplit \\\\( I \\\\) into 3 sets of size \\\\( n_{I_{\\\\text{prog}}} \\\\) /* Get prog. and pred. features */ \\\\( D; /* Initialize dataset */\\n\\nfor \\\\( n \\\\in [N] \\\\) do\\n\\n\\\\( Y(0) \\\\) \\\\( \\\\mu_{\\\\text{prog}}(X_n_{I_{\\\\text{prog}}}) + \\\\sigma_{\\\\text{pred}} \\\\cdot \\\\mu_{\\\\text{pred}0}(X_n_{I_0}) \\\\) /* Get untreated outcome */\\n\\n\\\\( Y(1) \\\\) \\\\( \\\\mu_{\\\\text{prog}}(X_n_{I_{\\\\text{prog}}}) + \\\\sigma_{\\\\text{pred}} \\\\cdot \\\\mu_{\\\\text{pred}1}(X_n_{I_1}) \\\\) /* Get treated outcome */\\n\\n\\\\( W_n \\\\) \\\\( \\\\sim \\\\text{Bernoulli}(\\\\tau(X_n)) \\\\) /* Sample treatment assignment */\\n\\n\\\\( \\\\epsilon \\\\) \\\\( \\\\sim \\\\mathcal{N}(0, \\\\sigma_{\\\\text{pred}}^2) \\\\) /* Sample noise */\\n\\n\\\\( Y_n = W_n \\\\cdot Y(1) + (1 - W_n) \\\\cdot Y(0) + \\\\epsilon \\\\) /* Get observed outcome */\\n\\n\\\\( D \\\\) \\\\( \\\\{ (Y_n, X_n, W_n) \\\\}_{n=1}^N \\\\) /* Append dataset */\\n\\nend\\n\\nreturn \\\\( D, I_{\\\\text{prog}}, I_{\\\\text{pred}} = I_0 \\\\cup I_1 \\\\).\\n\\nMetrics.\\n\\nAfter using Algorithm 1, we split the generated dataset into a training set \\\\( D_{\\\\text{train}} \\\\) and a testing set \\\\( D_{\\\\text{test}} \\\\) (80% 20% split). We fit a model \\\\( \\\\hat{\\\\tau} \\\\) to estimate the CATE on the training set \\\\( D_{\\\\text{train}} \\\\) and evaluate the model on the held-out test set \\\\( D_{\\\\text{test}} \\\\). As aforementioned, our purpose is to assess if this...\"}"}
{"id": "ddPXQt-gM--", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The model has correctly identified the predictive covariates. With our choice of DGP, we know that those predictive covariates correspond to the indices in $I_{\\\\text{pred}} = I_0 + I_1$. By recalling that the importance attributed to covariate $x_i$ for a prediction $\\\\hat{\\\\tau}(x)$ increases with the absolute value $|a_i(\\\\hat{\\\\tau}, x)|$, we can compute the average proportion of the attribution correctly allocated to the predictive covariates:\\n\\n$$\\\\text{Attr}_{\\\\text{pred}} = \\\\frac{1}{|D_{\\\\text{test}}|} \\\\sum_{x \\\\in X_{\\\\text{test}}} \\\\sum_{d_i = 1} P_i |a_i(\\\\hat{\\\\tau}, X)| \\\\text{ if } d_i = 1.$$ \\n\\nNote that $\\\\text{Attr}_{\\\\text{pred}} \\\\in [0, 1]$, where $\\\\text{Attr}_{\\\\text{pred}} = 0$ corresponds to a model that does not identify any predictive covariate and $\\\\text{Attr}_{\\\\text{pred}} = 1$ corresponds to a model that only identifies predictive covariates.\\n\\nIdeally, predictive covariates should be the most important for a model that estimates the CATE; thus we expect good models to score high with respect to this metric. A similar metric $\\\\text{Attr}_{\\\\text{prog}}$ can be defined analogously to measure the fraction of the feature attribution incorrectly allocated to the prognostic covariates by replacing $I_{\\\\text{pred}}$ in (2) with $I_{\\\\text{prog}}$.\\n\\nFinally, we will sometimes also report the standard PEHE metric, i.e. the RMSE of estimating CATE:\\n\\n$$\\\\text{PEHE} = \\\\sqrt{\\\\frac{1}{n_{\\\\text{test}}} \\\\sum_{x \\\\in X_{\\\\text{test}}} \\\\sum_{d_i = 1} (\\\\hat{\\\\tau}(X) - \\\\tau(X))^2}.$$ \\n\\n### 5 Experiments\\n\\nIn this section, we benchmark different types of CATE estimators on their ability to identify predictive covariates through feature importance scores. We study 3 different characteristics of the data generating process which we expect to impact this ability: (i) the relative strength between the prognostic contribution $\\\\mu_{\\\\text{prog}}$ and the predictive contributions $\\\\mu_{\\\\text{pred0}}, \\\\mu_{\\\\text{pred1}}$ to the POs $Y(0), Y(1)$ (Sec. 5.1); (ii) The presence of nonlinearities in the prognostic and predictive functions $\\\\mu_{\\\\text{prog}}, \\\\mu_{\\\\text{pred0}}, \\\\mu_{\\\\text{pred1}}$ (Sec. 5.2), and (iii) the fact that the treatment assignment might be biased according to a nontrivial propensity score $\\\\tau$ (Sec. 5.3). All the experiments are done by varying the inputs of Algorithm 1.\\n\\nThe code to reproduce the experiments is available at https://github.com/JonathanCrabbe/ITErpretability and https://github.com/vanderschaarlab/ITErpretability.\\n\\n#### Datasets.\\n\\nWe extract covariates to use in our benchmarking environment from the following four datasets: TCGA [51], Twins [52], News [53] and ACIC2016 [50], which were selected due to their diverse characteristics in terms of the number of features, mixture of categorical/continuous features and population size. The number of covariates in these datasets range from $d = 39$ to $d = 100$ and we set $n_{\\\\text{I}} = b_0 \\\\cdot d_c^2$. Refer to Appendix C for details of the datasets.\\n\\n#### Learners.\\n\\nWe consider a number of CATE estimators based on neural networks throughout our experiments. As direct estimators, we use [3]'s DR-learner, which relies on a doubly robust pseudo-outcome, and [2]'s X-learner, which uses a weighted average of two direct (singly-robust) treatment effect estimates from both treatment groups. As indirect estimators, we use [2]'s T-learner, which fits two regression models $\\\\hat{\\\\mu}_w(x)$ (one for each treatment group) and sets $\\\\hat{\\\\tau}(x) = \\\\hat{\\\\mu}_1(x) - \\\\hat{\\\\mu}_0(x)$, and S-learner, which includes the treatment assignment variable $W$ as a standard feature in a single regression $\\\\hat{\\\\mu}(x, w)$ and sets $\\\\hat{\\\\tau}(x) = \\\\hat{\\\\mu}(x, 1) - \\\\hat{\\\\mu}(x, 0)$. Finally, we consider [10]'s TARNet which can be seen as a hybrid between S- and T-learner [4]: it learns a representation $(x)$ shared between treatment groups, which is used by treatment-specific outcome heads $h_w(x)$ so that $\\\\hat{\\\\tau}(x) = h_1(x) - h_0(x)$. In the experiments with confounding, we also use [10]'s CFRNet, which differs from TARNet only in a regularization term that encourages the representation to be balanced (follow a similar distribution) across treatment groups. We discuss all models in more detail in Appendix C, where we also detail how we fix hyperparameters across all models to ensure similar capacity.\\n\\n### 5.1 Experiment 1: Altering the Strength of Predictive Effects\\n\\n#### Setup.\\n\\nWe begin by investigating how the strength of predictive effects relative to prognostic effects influences the ability of different learners to discover predictive covariates. This is an interesting question, as in practice predictive signals are often assumed to be much weaker than prognostic ones [2, 13, 16]. Thus, ideally estimators should be able to correctly identify predictive covariates even when effects are weak; yet some of [16]'s empirical results comparing different random forest-based learners across DGPs with different predictive effects, show that this is not always the case. For our experiments, we thus continuously vary predictive effect size in a DGP with a linear parametrization for the prognostic and predictive functions:\\n\\n$$\\\\mu_{\\\\text{prog}}(x) = \\\\beta_0 + \\\\beta_1 x_1 + \\\\beta_2 x_2 + \\\\cdots + \\\\beta_d x_d,$$\\n\\n$$\\\\mu_{\\\\text{pred0}}(x) = \\\\gamma_0 + \\\\gamma_1 x_1 + \\\\gamma_2 x_2 + \\\\cdots + \\\\gamma_d x_d,$$\\n\\n$$\\\\mu_{\\\\text{pred1}}(x) = \\\\delta_0 + \\\\delta_1 x_1 + \\\\delta_2 x_2 + \\\\cdots + \\\\delta_d x_d.$$\"}"}
{"id": "ddPXQt-gM--", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Performance comparison in terms of Attr\\\\textsubscript{pred} (top, higher is better), Attr\\\\textsubscript{prog} (middle, lower is better) and PEHE (bottom, lower is better) when varying the predictive scale, using four feature datasets (TGCA, Twins, News, ACIC). Averaged across multiple runs, shaded areas indicate one standard error.\\n\\n\\\\[\\n\\\\mu_{\\\\text{prog}}(x) = \\\\mathbb{E}_{I_{\\\\text{prog}}} x I_{\\\\text{prog}}, \\\\mu_{\\\\text{pred}}^{0}(x) = \\\\mathbb{E}_{I_{0}} x I_{0} \\\\text{ and } \\\\mu_{\\\\text{pred}}^{1}(x) = \\\\mathbb{E}_{I_{1}} x I_{1}\\n\\\\]\\n\\nwith weights sampled randomly \\\\(\\\\mathbb{U}([1/n, 1])\\\\), where \\\\(\\\\mathbb{U}\\\\) denotes the uniform distribution. To vary the relative strength between the prognostic and the predictive contributions to the POs, we change the predictive scale \\n\\\\(\\\\lambda_{\\\\text{pred}} \\\\in \\\\{10^3, 10^2, 10^1, 0.5, 1\\\\}\\\\). Here, treatments are assigned completely at random, i.e. \\\\(\\\\mathbb{P}_{x} = 0.5\\\\).\\n\\nResults. In Fig. 2 we present results on correct attributions Attr\\\\textsubscript{pred} and misattributions Attr\\\\textsubscript{prog}, as well as the standard PEHE metric for comparison, for all datasets. We make a number of interesting observations:\\n\\n1. Attribution trends indeed vary with \\\\(\\\\lambda_{\\\\text{pred}}\\\\). Correct predictive attributions Attr\\\\textsubscript{pred} substantially increase as \\\\(\\\\lambda_{\\\\text{pred}}\\\\) increases for all learners and across all datasets considered \u2013 this confirms the intuition that the stronger the predictive effects are, the easier it is to identify their origin. Conversely, prognostic misattributions Attr\\\\textsubscript{prog} decrease as \\\\(\\\\lambda_{\\\\text{pred}}\\\\) increases, indicating that one reason for the low Attr\\\\textsubscript{prog} at low \\\\(\\\\lambda_{\\\\text{pred}}\\\\) is that learners confuse prognostic effects for predictive ones.\\n\\n2. Comparing learners, S-Learners appear to struggle most to make correct attributions. The most salient observation across all datasets is that the S-Learner does does substantially worse at Attr\\\\textsubscript{pred}. With the exception of the Twins dataset, this usually also translates into higher Attr\\\\textsubscript{prog} than all other learners. We believe that this is because the S-Learner uniquely neither has a treatment-group specific component (like T-learner and TARNet do) nor models CATE directly (like DR- and X-learner do); learned treatment effect heterogeneity thus has to arise through learned interactions with the treatment indicator \u2013 which appears to lead to less reliable predictive covariate discoveries. All other methods perform very similar to each other.\\n\\n3. Using attribution metrics indeed leads to interesting new insights relative to considering only PEHE. We observe that the S-learner does best in terms of PEHE when predictive strength is low, while the T-learner does worst (as discussed below, this is in line with expectations and empirical observations previously made in e.g. [2, 13]) \u2013 yet, as we saw above, this does not translate into better discoveries. Similarly, the better performance in terms of PEHE of some other strategies relative to T-learner when \\\\(\\\\lambda_{\\\\text{pred}}\\\\) is small also does not lead to better discoveries. We attribute this to the fact that when \\\\(\\\\lambda_{\\\\text{pred}}\\\\) is very small, PEHE will favour any method that outputs near-zero treatment effects; indeed all considered methods except the T-learner incorporate an implicit inductive bias that shrinks effects [13] \u2013 which appears to help only in terms of PEHE. Note also that PEHE is not directly comparable across different values of \\\\(\\\\lambda_{\\\\text{pred}}\\\\) as it naturally increases as the scale of CATE changes. Finally, we investigate performance on this task using a dataset with higher dimensional input in Appendix C.4.\\n\\n5.2 Experiment 2: Incorporating Nonlinearities\\n\\nDescription. In practice, there is no particular reason to expect that POs are linear functions of the covariates. Next we therefore investigate how nonlinearities in the POs influence the ability of CATE...\"}"}
{"id": "ddPXQt-gM--", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Performance comparison in terms of $\\\\text{Attr}_{\\\\text{pred}}$ (top, higher is better) and $\\\\text{Attr}_{\\\\text{prog}}$ (bottom, lower is better) when varying the nonlinearity scale, using four feature datasets (TGCA, Twins, News, ACIC). Averaged across multiple runs, shaded areas indicates one standard error.\\n\\nEstimators to identify predictive covariates. To do so, we use a parametrization for the prognostic and predictive functions that allows us to control the strength of the nonlinearities through parameter $nl$:\\n\\n$$\\n\\\\mu_{\\\\text{prog}}(x) = (1-nl) \\\\cdot |\\\\text{prog}| x + nl \\\\cdot (|\\\\text{prog}| x)\\n$$\\n\\n$$\\n\\\\mu_{\\\\text{pred0}}(x) = (1-nl) \\\\cdot |\\\\text{pred0}| x + nl \\\\cdot (|\\\\text{pred0}| x)\\n$$\\n\\nand\\n\\n$$\\n\\\\mu_{\\\\text{pred1}}(x) = (1-nl) \\\\cdot |\\\\text{pred1}| x + nl \\\\cdot (|\\\\text{pred1}| x)\\n$$\\n\\nwith weights and nonlinearity sampled randomly $|\\\\text{prog}|, |\\\\text{pred0}|, |\\\\text{pred1}| \\\\sim U([1,1])$, $|\\\\text{pred1}| \\\\sim U(F)$, where $F$ is a set of 10 nonlinear functions $\\\\mathbb{R} \\\\rightarrow \\\\mathbb{R}$ (specified in Appendix C.5).\\n\\nTo vary the strength of the nonlinearities, we let the nonlinearity scale vary $nl \\\\in [0,1]$. We note that $nl=0$ corresponds to linear function as in the previous experiment. On the other hand, $nl=1$ corresponds to purely nonlinear functions. Here, we set $\\\\text{pred}=1$ \u2013 a setting for which all learners performed well in the previous experiment and $\\\\gamma(x) = 0$.\\n\\n5. Results.\\n\\nWe present attribution score results across all datasets in Fig. 3. We find that attribution trends vary with $nl$.\\n\\nAs $nl$ increases and the underlying DGP is dominated by the nonlinearity, hence becoming more difficult to learn, we observe that correct attribution ($\\\\text{Attr}_{\\\\text{pred}}$) decreases for all methods. Also here, we observe that this is mirrored by an increase in confusion of prognostic effects for predictive ones (as seen in the increasing $\\\\text{Attr}_{\\\\text{prog}}$). Further, we note that relative ordering of methods does not change. The S-Learner continues to underperform compared to all other learners and the performance gap does not substantially change across values of $nl$.5.3 Experiment 3: The Effect of Confounding\\n\\nDescription.\\n\\nFinally, we turn to examine the effect of confounding, i.e. covariate shift between treatment groups resulting from treatment assignment being based on observables \u2013 a problem that much of the ML literature proposing CATE estimators has focussed on (e.g. [9, 10, 12]). A popular solution to deal with said covariate shift has been to rely on balancing regularization that penalizes distributional distance (here: MMD$^2$) between treatment groups in representation space; here we therefore also consider [10]\u2019s CFRNet which is identical to TARNet but includes a discrepancy-based regularization term controlled by hyperparameter $\\\\gamma$. We note that if the covariates that determine treatment assignment are either predictive or prognostic \u2013 i.e. they are true confounders \u2013 it is generally not possible to remove all covariate shift without removing predictive/prognostic information. We therefore consider a final experiment where we structurally vary not only the degree of assignment bias (through propensity scale $\\\\gamma$) but also what type of information assignment is based on. We achieve this by modifying the propensity score:\\n\\n$$\\n\\\\gamma(x) = \\\\text{Sigmoid}(\\\\gamma \\\\cdot \\\\text{Z score}(x)),\\n$$\\n\\nwhere $\\\\text{Z score}(\\\\cdot)$ indicates normalization across the generated training dataset (this ensures well-behaved propensity scores centered at 0.5) and $X \\\\in \\\\mathbb{R}$ controls the type of confounding.\\n\\nWe consider 3 types of confounding, each corresponding to a different choice for $X$:\\n\\nPredictive confounding corresponds to setting $X=\\\\mu_{\\\\text{pred1}}-\\\\mu_{\\\\text{pred0}}$. It mimics a scenario where treatment assignment is biased towards those with characteristics making them most likely to respond well to it, e.g. a doctor assigning treatment with knowledge of CATE.\\n\\nPrognostic confounding corresponds to setting $X=\\\\mu_{\\\\text{prog}}$. It mimics the most classical confounding setting where treatment assignment is biased towards those with characteristics making them more likely to have a good outcome regardless of treatment, e.g. self-selection into a treatment program. Finally, we consider a non-confounded setting.\"}"}
{"id": "ddPXQt-gM--", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Performance comparison when increasing the propensity scale. Averaged across multiple runs, shaded areas indicates one standard error.\\n\\npropensity: \\\\( (x) = x_i \\\\) for some irrelevant covariate \\\\( i \\\\). In this case, the treatment selection is not based on a covariate that affects outcome; note that the distribution of covariates in \\\\( I_{\\\\text{pred}} I_{\\\\text{prog}} \\\\) might still differ across treatment groups if they are correlated with the chosen 'irrelevant'. Note that for \\\\( \\\\! \\\\! \\\\pi = 0 \\\\), all settings are identical and reduce to the previous \\\\( \\\\! \\\\! \\\\pi(x) = 0 \\\\).\\n\\nFor the potential outcomes, we consider the previous setting with \\\\( \\\\! \\\\! \\\\pi_{\\\\text{pred}} = 1 \\\\) and \\\\( \\\\! \\\\! \\\\pi_{\\\\text{nl}} = 0 \\\\). We believe that such an explicit distinction between different confounding types has not yet been investigated in related work.\\n\\nResults. We present results comparing \\\\( \\\\text{Attr}_{\\\\text{pred}} \\\\) and PEHE across the three settings for each dataset in Fig. 4. We make numerous interesting observations:\\n\\n1. Attribution trends indeed vary with \\\\( \\\\! \\\\! \\\\pi \\\\). Also in this experiment attribution scores vary systematically as we change \\\\( \\\\! \\\\! \\\\pi \\\\); across the different settings the quality of attributions worsens the more biased treatment assignment is, albeit only marginally for some settings.\\n\\n2. There are systematic differences across the three settings. We observe that performance deteriorates the most in the setting with predictive confounding. We believe that this is a result of less observed variation across predictive covariates within a group due to the assignment bias, making it harder for any model to learn that treatment effect varies systematically across these covariates. Overall, we observe the least deterioration in attribution performance in the prognostic confounding setting. That is because, while the prognostic confounding and non-confounded setting are almost identical in terms of \\\\( \\\\text{Attr}_{\\\\text{pred}} \\\\) for TCGA, Twins and ACIC, we observe an interesting difference in News: Perhaps surprisingly, we find that performance as measured by \\\\( \\\\text{Attr}_{\\\\text{pred}} \\\\) in News deteriorates less in the prognostic confounding setting than in the non-confounded setting. We attribute this to two competing forces being at play here: First, having any sort of covariate imbalance, as in the non-confounded setting, appears to have the potential to lead to a decrease in \\\\( \\\\text{Attr}_{\\\\text{pred}} \\\\). Second, in the prognostic confounding setting, there is a similar effect as in the predictive setting that can offset some of the initial performance decrease: as prognostic covariates now have less variation within a group, models are less likely to misattribute a predictive effect to them.\\n\\n3. CFRNet's balancing regularization has different effects across settings. We find that...\"}"}
{"id": "ddPXQt-gM--", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the addition of the balancing regularization term can lead to a very large drop in $\\\\text{Attr}_{\\\\text{pred}}$ in the predictive confounding setting \u2013 this is expected as overly aligning distributions should lead to a loss in explanatory power, which is also reflected in the PEHE. In the other two settings, no such effect is visible when considering $\\\\text{Attr}_{\\\\text{pred}}$; yet, we observe that PEHE does worsen considerably in the prognostic confounding setting. We believe that this is an effect of the prognostic component of the POs being estimated less accurately due to the balancing regularization, making the CATE estimate, the difference between the two estimated POs, less accurate overall.\\n\\n6 Discussion\\n\\nIn this paper, we have introduced the ITErpretability benchmark, a new environment to benchmark CATE models with the help of feature importance methods. We empirically demonstrated on various datasets that this benchmark provides insights that are not accessible with the metrics and benchmarks considered standard in the CATE literature. We believe that this work opens up many interesting avenues for future research: First, our environment could be used to extend insights to many more of the CATE estimators proposed in recent literature. One could, for example, replicate our propensity experiments with learners tackling confounding with methods other than balancing, e.g. importance weighting as in [12], or compare performance across different classes of underlying ML methods \u2013 i.e. compare how discoveries differ across implementations relying on neural networks with e.g. random forests as in [16] or the gaussian processes of [7, 8]. Second, it may be worthwhile to further study possible failure modes of the considered learning strategies within and beyond our benchmarking environment. This could be achieved both by considering an even wider range of DGPs to investigate how our results generalize empirically, and by complementing our empirical findings with theoretical ones studying under which conditions what types of learners are guaranteed to discover the correct predictive covariates. Third, we believe that it would be interesting to consider how to improve existing or develop new CATE estimation strategies with the help of interpretability techniques or insights derived from experiments such as the ones from Sec. 5. Finally, note that we have exclusively focused on feature importance methods here. Another possible extension of this work would then be to perform a similar study with other type of explanation methods, such as example-based explanation methods like Influence Functions [55] and hybrid methods like SimplEx [56].\\n\\nAcknowledgments and Disclosure of Funding\\n\\nThe authors are grateful to Javier Abad Martinez and Bogdan Cebere for implementing and running preliminary, yet different, experiments in the early and explorative stages of the project. Jonathan Crabb\u00e9 is funded by Aviva, Alicia Curth is funded by AstraZeneca, Ioana Bica is funded by the Alan Turing Institute (under the EPSRC grant EP/N510129/1) and Mihaela van der Schaar is funded by the Office of Naval Research (ONR under the grant NSF 1722516). The authors have no competing interests to disclose.\"}"}
{"id": "ddPXQt-gM--", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Ioana Bica, Ahmed M Alaa, Craig Lambert, and Mihaela van der Schaar. From real-world patient data to individualized treatment effects using machine learning: current and future methods to address underlying challenges. *Clinical Pharmacology & Therapeutics*, 109(1):87\u2013100, 2021.\\n\\n[2] S\u00f6ren R K\u00fcnzel, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. Metalearners for estimating heterogeneous treatment effects using machine learning. *Proceedings of the National Academy of Sciences*, 116(10):4156\u20134165, 2019.\\n\\n[3] Edward H. Kennedy. Optimal doubly robust estimation of heterogeneous causal effects. *arXiv: Statistics Theory*, 2020.\\n\\n[4] Alicia Curth and Mihaela van der Schaar. Nonparametric estimation of heterogeneous treatment effects: From theory to learning algorithms. In *International Conference on Artificial Intelligence and Statistics*, pages 1810\u20131818. PMLR, 2021.\\n\\n[5] Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using random forests. *Journal of the American Statistical Association*, 113(523):1228\u20131242, 2018.\\n\\n[6] Susan Athey, Julie Tibshirani, Stefan Wager, et al. Generalized random forests. *The Annals of Statistics*, 47(2):1148\u20131178, 2019.\\n\\n[7] Ahmed M Alaa and Mihaela van der Schaar. Bayesian inference of individualized treatment effects using multi-task Gaussian processes. *Advances in Neural Information Processing Systems*, 30:3424\u20133432, 2017.\\n\\n[8] Ahmed Alaa and Mihaela van der Schaar. Limits of estimating heterogeneous treatment effects: Guidelines for practical algorithm design. In *International Conference on Machine Learning*, pages 129\u2013138, 2018.\\n\\n[9] Fredrik Johansson, Uri Shalit, and David Sontag. Learning representations for counterfactual inference. In *International Conference on Machine Learning*, pages 3020\u20133029. PMLR, 2016.\\n\\n[10] Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect: Generalization bounds and algorithms. In *International Conference on Machine Learning*, pages 3076\u20133085. PMLR, 2017.\\n\\n[11] Negar Hassanpour and Russell Greiner. Learning disentangled representations for counterfactual regression. In *International Conference on Learning Representations*, 2019.\\n\\n[12] Serge Assaad, Shuxi Zeng, Chenyang Tao, Shounak Datta, Nikhil Mehta, Ricardo Henao, Fan Li, and Lawrence Carin Duke. Counterfactual representation learning with balancing weights. In *International Conference on Artificial Intelligence and Statistics*, pages 1972\u20131980. PMLR, 2021.\\n\\n[13] Alicia Curth and Mihaela van der Schaar. On inductive biases for heterogeneous treatment effect estimation. *Advances in Neural Information Processing Systems*, 34, 2021.\\n\\n[14] Alicia Curth, David Svensson, Jim Weatherall, and Mihaela van der Schaar. Really doing great at estimating CATE? A critical look at ML benchmarking practices in treatment effect estimation. In *Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)*, 2021.\\n\\n[15] Jennifer L Hill. Bayesian nonparametric modeling for causal inference. *Journal of Computational and Graphical Statistics*, 20(1):217\u2013240, 2011.\\n\\n[16] Erik Hermansson and David Svensson. On discovering treatment-effect modifiers using virtual twins and causal forest ML in the presence of prognostic biomarkers. In *International Conference on Computational Science and Its Applications*, pages 624\u2013640. Springer, 2021.\"}"}
{"id": "ddPXQt-gM--", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alex Dmitrienko, Ilya Lipkovich, Aaron Dane, and Christoph Muysers. Data-driven and confirmatory subgroup analysis in clinical trials. In Design and Analysis of Subgroups with Biopharmaceutical Applications, pages 33\u201391. Springer, 2020.\\n\\nAlejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58(December 2019):82\u2013115, jun 2020.\\n\\nErico Tjoa and Cuntai Guan. A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI. IEEE Transactions on Neural Networks and Learning Systems, pages 1\u201321, 2020.\\n\\nArun Das and Paul Rad. Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey. arXiv, jun 2020.\\n\\nKosuke Imai, Marc Ratkovic, et al. Estimating treatment effect heterogeneity in randomized program evaluation. The Annals of Applied Statistics, 7(1):443\u2013470, 2013.\\n\\nP Richard Hahn, Carlos M Carvalho, David Puelz, Jingyu He, et al. Regularization and confounding in linear regression for treatment effect estimation. Bayesian Analysis, 13(1):163\u2013182, 2018.\\n\\nJ. Foster, J. Taylor, and S. Ruberg. Subgroup identification from randomized clinical trial data. Statistics in medicine, 30 24:2867\u201380, 2011.\\n\\nSusan Athey and Guido Imbens. Recursive partitioning for heterogeneous causal effects. Proceedings of the National Academy of Sciences, 113(27):7353\u20137360, 2016.\\n\\nHimabindu Lakkaraju and Cynthia Rudin. Learning cost-effective and interpretable treatment regimes. In Artificial intelligence and statistics, pages 166\u2013175. PMLR, 2017.\\n\\nMarco Morucci, Vittorio Orlandi, Sudeepa Roy, Cynthia Rudin, and Alexander Volfovsky. Adaptive hyper-box matching for interpretable individualized treatment effect estimation. In Conference on Uncertainty in Artificial Intelligence, pages 1089\u20131098. PMLR, 2020.\\n\\nKwonsang Lee, Falco J Bargagli-Stoffi, and Francesca Dominici. Causal rule ensemble: Interpretable inference of heterogeneous treatment effects. arXiv preprint arXiv:2009.09036, 2020.\\n\\nChirag Nagpal, Dennis Wei, Bhanukiran Vinzamuri, Monica Shekhar, Sara E Berger, Subhro Das, and Kush R Varshney. Interpretable subgroup discovery in treatment effect estimation with application to opioid prescribing guidelines. In Proceedings of the ACM Conference on Health, Inference, and Learning, pages 19\u201329, 2020.\\n\\nOscar Hernan Madrid Padilla, Peng Ding, Yanzhen Chen, and Gabriel Ruiz. A causal fused lasso for interpretable heterogeneous treatment effects estimation. arXiv preprint arXiv:2110.00901, 2021.\\n\\nKan Chen, Qishuo Yin, and Qi Long. Covariate-balancing-aware interpretable deep learning models for treatment effect estimation. arXiv preprint arXiv:2203.03185, 2022.\\n\\nYi-Fan Zhang, Hanlin Zhang, Zachary C. Lipton, Li Erran Li, and Eric P. Xing. Exploring transformer backbones for heterogeneous treatment effect estimation, 2022.\\n\\nHan Wu, Sarah Tan, Weiwei Li, Mia Garrard, Adam Obeng, Drew Dimmery, Shaun Singh, Hanson Wang, Daniel Jiang, and Eytan Bakshy. Distilling heterogeneity: From explanations of heterogeneous treatment effect models to interpretable policies. arXiv preprint arXiv:2111.03267, 2021.\\n\\nCarolyn Kim and Osbert Bastani. Learning interpretable models with causal guarantees. arXiv preprint arXiv:1901.08576, 2019.\\n\\nDonald B Rubin. Causal inference using potential outcomes: Design, modeling, decisions. Journal of the American Statistical Association, 100(469):322\u2013331, 2005.\"}"}
{"id": "ddPXQt-gM--", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational studies for causal effects. *Biometrika*, 70(1):41\u201355, 1983.\\n\\nMengjiao Yang and Been Kim. Benchmarking Attribution Methods with Relative Feature Importance. *arXiv*, 2019.\\n\\nMirka Saarela and Susanne Jauhiainen. Comparison of feature importance measures as explanations for classification models. *SN Applied Sciences*, 3(2):1\u201312, 2021.\\n\\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. In *2nd International Conference on Learning Representations, ICLR 2014 - Workshop Track Proceedings*, 2013.\\n\\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic Attribution for Deep Networks. In *Proceedings of the 34th International Conference on Machine Learning*, volume 70, pages 3319\u20133328. International Machine Learning Society (IMLS), 2017.\\n\\nScott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In *Neural Information Processing Systems*, 2017.\\n\\nGabriel Erion, Joseph D. Janizek, Pascal Sturmfels, Scott M. Lundberg, and Su-In Lee. Improving performance of deep learning models with axiomatic attribution priors and expected gradients. *Nature Machine Intelligence 2021 3:7*, 3(7):620\u2013631, may 2021.\\n\\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \u201cWhy should i trust you?\u201d Explaining the predictions of any classifier. In *Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*, pages 1135\u20131144. Association for Computing Machinery, 2016.\\n\\nRuth C Fong and Andrea Vedaldi. Interpretable Explanations of Black Boxes by Meaningful Perturbation. *Proceedings of the IEEE International Conference on Computer Vision*, pages 3449\u20133457, 2017.\\n\\nJonathan Crabb\u00e9 and Mihaela van der Schaar. Explaining Time Series Predictions with Dynamic Masks. *International Conference on Machine Learning*, 38, 2021.\\n\\nAlexander Binder, Gr\u00e9goire Montavon, Sebastian Lapuschkin, Klaus Robert M\u00fcller, and Wojciech Samek. Layer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers. *Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)*, 9887 LNCS:63\u201371, 2016.\\n\\nAvanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning Important Features Through Propagating Activation Differences. *34th International Conference on Machine Learning, ICML 2017*, 7:4844\u20134866, apr 2017.\\n\\nKarla V Ballman. Biomarker: predictive or prognostic? *Journal of clinical oncology: official journal of the American Society of Clinical Oncology*, 33(33):3968\u20133971, 2015.\\n\\nKonstantinos Sechidis, Konstantinos Papangelou, Paul D Metcalfe, David Svensson, James Weatherall, and Gavin Brown. Distinguishing prognostic and predictive biomarkers: an information theoretic approach. *Bioinformatics*, 34(19):3365\u20133376, 2018.\\n\\nFrancesca Rastelli and Sergio Crispino. Factors predictive of response to hormone therapy in breast cancer. *Tumori Journal*, 94(3):370\u2013383, 2008.\\n\\nVincent Dorie, Jennifer Hill, Uri Shalit, Marc Scott, and Dan Cervone. Automated versus do-it-yourself methods for causal inference: Lessons learned from a data analysis competition. *Statistical Science*, 34(1):43\u201368, 2019.\\n\\nJohn N Weinstein, Eric A Collisson, Gordon B Mills, Kenna R Shaw, Brad A Ozenberger, Kyle Ellrott, Ilya Shmulevich, Chris Sander, and Joshua M Stuart. The cancer genome atlas pan-cancer analysis project. *Nature genetics*, 45(10):1113\u20131120, 2013.\\n\\nDouglas Almond, Kenneth Y Chay, and David S Lee. The costs of low birth weight. *The Quarterly Journal of Economics*, 120(3):1031\u20131083, 2005.\"}"}
{"id": "ddPXQt-gM--", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"David Newman. Bag of words data set. UCI Machine Learning Repository, 2008.\\n\\nEleonora Giunchiglia, Anton Nemchenko, and Mihaela van der Schaar. Rnn-surv: A deep recurrent model for survival analysis. In International conference on artificial neural networks, pages 23\u201332. Springer, 2018.\\n\\nPang Wei Koh and Percy Liang. Understanding Black-box Predictions via Influence Functions. 34th International Conference on Machine Learning, ICML 2017, 4:2976\u20132987, 2017.\\n\\nJonathan Crabb\u00e9, Zhaozhi Qian, Fergus Imrie, and Mihaela van der Schaar. Explaining Latent Representations with a Corpus of Examples. In Advances in Neural Information Processing Systems, 2021.\\n\\nJavier Castro, Daniel G\u00f3mez, and Juan Tejada. Polynomial calculation of the Shapley value based on sampling. Computers and Operations Research, 36(5):1726\u20131730, 2009.\\n\\nJames M Robins and Andrea Rotnitzky. Semiparametric efficiency in multivariate regression models with missing data. Journal of the American Statistical Association, 90(429):122\u2013129, 1995.\\n\\nXinkun Nie and Stefan Wager. Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2):299\u2013319, 2021.\\n\\nJean Kaddour, Yuchen Zhu, Qi Liu, Matt J Kusner, and Ricardo Silva. Causal effect inference for structured treatments. Advances in Neural Information Processing Systems, 34:24841\u201324854, 2021.\\n\\nPeter M Robinson. Root-n-consistent semiparametric regression. Econometrica: Journal of the Econometric Society, pages 931\u2013954, 1988.\\n\\nPatrick Schwab, Lorenz Linhardt, Stefan Bauer, Joachim M Buhmann, and Walter Karlen. Learning counterfactual representations for estimating individual dose-response curves. arXiv preprint arXiv:1902.00981, 2019.\\n\\nPascal Sturmfels, Scott Lundberg, and Su-In Lee. Visualizing the impact of feature attribution baselines. Distill, 5(1):e22, 2020.\"}"}
{"id": "ddPXQt-gM--", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Checklist\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\\n      [Yes] All the claims are empirically supported in Section 5.\\n   (b) Did you describe the limitations of your work?\\n      [Yes] Some limitations and possible extensions of our work are discussed in Section 6.\\n   (c) Did you discuss any potential negative societal impacts of your work?\\n      [No] Our benchmark helps to investigate the quality of CATE models, hence mitigating their potential negative impact.\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them?\\n      [Yes] We have reviewed the guidelines and we ensured that our paper is conform.\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results?\\n      [N/A] Our paper is purely empirical.\\n   (b) Did you include complete proofs of all theoretical results?\\n      [N/A] Our paper is purely empirical.\\n\\n3. If you ran experiments (e.g., for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?\\n      [Yes] The code to reproduce the experiments is available at https://github.com/JonathanCrabbe/ITErpretability and https://github.com/vanderschaarlab/ITErpretability.\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?\\n      [Yes] All the training details are provided in Appendix C and in the enclosed code.\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?\\n      [Yes] All our Figure contain standard errors resulting from running the experiments with different seeds.\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?\\n      [Yes] Our computing resources are described in Appendix C.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators?\\n      [Yes] All the relevant references are provided in Section 5.\\n   (b) Did you mention the license of the assets?\\n      [Yes] The relevant licenses are given in Appendix C.\\n   (c) Did you include any new assets either in the supplemental material or as a URL?\\n      [Yes] We provide the implementation of our benchmarking environment in the supplementary materials.\\n   (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating?\\n      [Yes] The relevant discussions are in Appendix C.\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?\\n      [N/A] All the datasets we use have been de-identified.\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable?\\n      [N/A] No research with human subjects.\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?\\n      [N/A] No research with human subjects.\\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?\\n      [N/A] No research with human subjects.\"}"}
