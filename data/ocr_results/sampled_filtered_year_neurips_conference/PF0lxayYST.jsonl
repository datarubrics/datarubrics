{"id": "PF0lxayYST", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6. Average target domain accuracy of all algorithms on the relatively high sample sizes in testing. The standard deviation here reflects the stability of performances across all 50 target states. And in US Accident performances across different target states.\\n\\n| Algorithm          | Threshold | Exp | \u00b1 2 | EP | \u00b1 2 | DWR | \u00b1 2 | CVaR-DORO | \u00b1 2 | SUBG | \u00b1 2 | CVaR-DRO | \u00b1 2 | RWY | \u00b1 2 | CVaR | \u00b1 2 | LightGBM | \u00b1 2 | SVM | \u00b1 2 | LR | \u00b1 2 |\\n|--------------------|-----------|-----|-----|----|-----|-----|-----|----------|-----|------|-----|----------|-----|-----|-----|------|-----|---------|-----|-----|-----|-----|-----|\\n| Threshold          | 80.5      | 74.5|     |    |     |     |     | 78.7     | 72.7|     |     | 78.5    |     | 81.9 |     | 81.4 |     | 80.6     | 74.6|     |     | 81.1 |     |\\n| Exp                | 80.5      | 74.8|     |    |     |     |     | 80.3     | 74.7|     |     | 80.2    |     | 78.2 |     | 81.4 |     | 80.6     | 74.6|     |     | 79.4 |     |\\n| \u00b1 2                | 80.3      | 74.7|     |    |     |     |     | 80.2     | 74.3|     |     | 80.2    |     | 78.2 |     | 81.4 |     | 80.6     | 74.6|     |     | 79.4 |     |\\n| EP                 | 80.5      | 74.8|     |    |     |     |     | 80.3     | 74.7|     |     | 80.2    |     | 78.2 |     | 81.4 |     | 80.6     | 74.6|     |     | 79.4 |     |\\n| \u00b1 2                | 80.3      | 74.7|     |    |     |     |     | 80.2     | 74.3|     |     | 80.2    |     | 78.2 |     | 81.4 |     | 80.6     | 74.6|     |     | 79.4 |     |\\n| DWR                | 81.9      | 77.5|     |    |     |     |     | 81.4     | 77.7|     |     | 81.5    |     | 81.5 |     | 82.1 |     | 81.8     | 77.0|     |     | 82.1 |     |\\n| \u00b1 2                | 81.9      | 77.5|     |    |     |     |     | 81.4     | 77.7|     |     | 81.5    |     | 81.5 |     | 82.1 |     | 81.8     | 77.0|     |     | 82.1 |     |\\n| CVaR-DORO          | 80.9      | 76.7|     |    |     |     |     | 78.7     | 72.7|     |     | 78.5    |     | 81.9 |     | 81.2 |     | 81.4     | 77.0|     |     | 81.5 |     |\\n| \u00b1 2                | 80.9      | 76.7|     |    |     |     |     | 78.7     | 72.7|     |     | 78.5    |     | 81.9 |     | 81.2 |     | 81.4     | 77.0|     |     | 81.5 |     |\\n| SUBG               | 78.2      | 71.9|     |    |     |     |     | 78.3     | 73.9|     |     | 78.3    |     | 81.5 |     | 80.7 |     | 81.2     | 76.9|     |     | 81.2 |     |\\n| \u00b1 2                | 78.2      | 71.9|     |    |     |     |     | 78.3     | 73.9|     |     | 78.3    |     | 81.5 |     | 80.7 |     | 81.2     | 76.9|     |     | 81.2 |     |\\n| CVaR-DRO           | 81.5      | 77.4|     |    |     |     |     | 77.2     | 72.8|     |     | 77.5    |     | 81.7 |     | 81.5 |     | 81.8     | 77.0|     |     | 82.1 |     |\\n| \u00b1 2                | 81.5      | 77.4|     |    |     |     |     | 77.2     | 72.8|     |     | 77.5    |     | 81.7 |     | 81.5 |     | 81.8     | 77.0|     |     | 82.1 |     |\\n| RWY                | 81.5      | 77.4|     |    |     |     |     | 80.5     | 72.9|     |     | 80.5    |     | 82.1 |     | 81.2 |     | 81.5     | 77.0|     |     | 82.1 |     |\\n| \u00b1 2                | 81.5      | 77.4|     |    |     |     |     | 80.5     | 72.9|     |     | 80.5    |     | 82.1 |     | 81.2 |     | 81.5     | 77.0|     |     | 82.1 |     |\\n| LightGBM           | 81.8      | 76.9|     |    |     |     |     | 81.7     | 77.9|     |     | 81.8    |     | 82.1 |     | 81.5 |     | 81.8     | 77.0|     |     | 82.1 |     |\\n| \u00b1 2                | 81.8      | 76.9|     |    |     |     |     | 81.7     | 77.9|     |     | 81.8    |     | 82.1 |     | 81.5 |     | 81.8     | 77.0|     |     | 82.1 |     |\\n| SVM                | 79.4      | 74.8|     |    |     |     |     | 78.6     | 74.3|     |     | 78.6    |     | 82.1 |     | 80.7 |     | 81.8     | 77.0|     |     | 82.1 |     |\\n| \u00b1 2                | 79.4      | 74.8|     |    |     |     |     | 78.6     | 74.3|     |     | 78.6    |     | 82.1 |     | 80.7 |     | 81.8     | 77.0|     |     | 82.1 |     |\\n| LR                 | 81.1      | 77.3|     |    |     |     |     | 80.7     | 75.2|     |     | 80.7    |     | 81.5 |     | 81.2 |     | 81.5     | 77.0|     |     | 82.1 |     |\\n| \u00b1 2                | 81.1      | 77.3|     |    |     |     |     | 80.7     | 75.2|     |     | 80.7    |     | 81.5 |     | 81.2 |     | 81.5     | 77.0|     |     | 82.1 |     |\\n| SVM                | 80.7      | 75.2|     |    |     |     |     | 78.8     | 74.2|     |     | 78.8    |     | 82.1 |     | 80.7 |     | 81.5     | 77.0|     |     | 82.1 |     |\\n| \u00b1 2                | 80.7      | 75.2|     |    |     |     |     | 78.8     | 74.2|     |     | 78.8    |     | 82.1 |     | 80.7 |     | 81.5     | 77.0|     |     | 82.1 |     |\\n| LR                 | 81.5      | 77.4|     |    |     |     |     | 80.9     | 76.8|     |     | 80.9    |     | 82.1 |     | 81.2 |     | 81.5     | 77.0|     |     | 82.1 |     |\\n| \u00b1 2                | 81.5      | 77.4|     |    |     |     |     | 80.9     | 76.8|     |     | 80.9    |     | 82.1 |     | 81.2 |     | 81.5     | 77.0|     |     | 82.1 |     |\\n| SVM                | 80.7      | 75.2|     |    |     |     |     | 79.0     | 74.2|     |     | 79.0    |     | 82.1 |     | 80.7 |     | 81.5     | 77.0|     |     | 82.1 |     |\\n| \u00b1 2                | 80.7      | 75.2|     |    |     |     |     | 79.0     | 74.2|     |     | 79.0    |     | 82.1 |     | 80.7 |     | 81.5     | 77.0|     |     | 82.1 |     |\\n| LR                 | 81.5      | 77.4|     |    |     |     |     | 80.9     | 76.8|     |     | 80.9    |     | 82.1 |     | 81.2 |     | 81.5     | 77.0|     |     | 82.1 |     |\\n| \u00b1 2                | 81.5      | 77.4|     |    |     |     |     | 80.9     | 76.8|     |     | 80.9    |     | 82.1 |     | 81.2 |     | 81.5     | 77.0|     |     | 82.1 |     |\\n| SVM                | 80.7      | 75.2|     |    |     |     |     | 79.0     | 74.2|     |     | 79.0    |     | 82.1 |     | 80.7 |     | 81.5     | 77.0|     |     | 82.1 |     |\\n| \u00b1 2                | 80.7      | 75.2|     |    |     |     |     | 79.0     | 74.2|     |     | 79.0    |     | 82.1 |     | 80.7 |     | 81.5     | 77.0|     |     | 82.1 |     |\\n| LR                 | 81.5      | 77.4|     |    |     |     |     | 80.9     | 76.8|     |     | 80.9    |     | 82.1 |     | 81.2 |     | 81.5     | 77.0|     |     | 82.1 |     |\\n| \u00b1 2                | 81.5      | 77.4|     |    |     |     |     | 80.9     | 76.8|     |     | 80.9    |     | 82.1 |     | 81.2 |     | 81.5     | 77.0|     |     | 82.1 |     |\"}"}
{"id": "PF0lxayYST", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 8: Average target domain accuracy of all algorithms on the datasets.\\n\\nIn each setting, we test all algorithms on the 50 American states and report the mean and standard deviation across all 3 target domains. The ACS Income (Synthetic) and ACS Pub.Cov datasets. In (Temporal), we simulate strong covariate shifts according to the \\\"Age\\\" feature, where 10% and 20% means the minor group ratio in training, respectively.\\n\\n| Algorithm      | Threshold | Exp | EP | EO | DWR | JTT | RWG | SUBG | RWY | SUBY | Group DRO | CVaR-DORO | CVaR-DRO | XGBoost | LightGBM | Random Forest | MLP | SVM | LR |\\n|----------------|-----------|-----|----|----|-----|-----|-----|------|-----|-----|-----------|-----------|-----------|----------|---------|-----------|-------------|-----|-----|----|\\n|                | 77.9      | 71.8| 77.0| 69.8| 77.5| 70.9| 78.0| 62.8 | 84.3| 57.3| 86.2      | 52.4      | 85.8      | 52.9     | 80.0     | 59.9       |     |     |    |\\n|                | 81.4      | 70.6| 81.0| 69.4| 81.8| 70.4| 79.1| 68.8 | 84.5| 55.4| 86.8      | 53.6      | 86.4      | 53.6     | 80.9     | 61.5       |     |     |    |\\n|                | 81.8      | 70.7| 81.9| 70.5| 82.1| 70.8| 80.0| 70.6 | 84.4| 55.5| 86.8      | 54.4      | 86.4      | 53.6     | 80.8     | 61.6       |     |     |    |\\n|                | 81.3      | 70.3| 81.0| 68.8| 81.8| 70.7| 79.3| 68.4 | 84.3| 57.4| 86.6      | 53.7      | 86.4      | 52.9     | 80.6     | 57.8       |     |     |    |\\n|                | 81.9      | 71.2| 81.6| 71.0| 82.1| 69.0| 80.1| 72.0 | 84.3| 58.1| 86.8      | 54.2      | 85.9      | 52.8     | 80.6     | 59.4       |     |     |    |\\n|                | 78.3      | 68.4| 78.1| 71.2| 79.1| 71.6| 77.9| 68.6 | 80.6| 59.4| 83.1      | 56.2      | 84.7      | 55.4     | 77.6     | 56.7       |     |     |    |\\n|                | 82.0      | 71.3| 81.9| 70.6| 82.2| 70.9| 80.2| 70.6 | 84.5| 56.8| 86.8      | 54.8      | 86.7      | 53.7     | 81.0     | 57.2       |     |     |    |\\n|                | 81.8      | 72.6| 81.3| 71.8| 81.7| 69.2| 79.8| 0.68 | 83.3| 53.2| 85.4      | 53.2      | 83.5      | 51.3     | 72.2     | 43.6       |     |     |    |\\n|                | 80.9      | 68.7| 80.5| 64.9| 81.1| 64.3| 79.6| 69.1 | 80.7| 49.5| 84.3      | 52.4      | 81.9      | 52.3     | 76.4     | 56.2       |     |     |    |\\n|                | 81.4      | 72.1| 81.2| 70.7| 82.0| 70.0| 79.7| 68.4 | 82.6| 52.2| 85.2      | 51.7      | 83.3      | 52.3     | 77.8     | 53.9       |     |     |    |\\n|                | 82.0      | 72.2| 81.9| 70.1| 82.2| 70.9| 80.5| 70.5 | 84.6| 54.8| 86.7      | 53.7      | 87.3      | 55.4     | 81.2     | 59.6       |     |     |    |\\n|                | 81.8      | 71.1| 82.1| 69.5| 82.3| 70.1| 80.2| 69.6 | 84.6| 56.9| 86.9      | 53.9      | 86.9      | 52.3     | 81.2     | 60.4       |     |     |    |\\n|                | 81.7      | 72.6| 81.2| 71.6| 81.5| 71.2| 80.2| 68.3 | 84.6| 54.2| 86.8      | 52.8      | 86.7      | 52.9     | 81.2     | 59.0       |     |     |    |\\n|                | 81.8      | 68.3| 81.4| 70.8| 81.7| 71.3| 80.5| 70.5 | 83.3| 54.3| 85.4      | 53.9      | 84.5      | 52.9     | 78.8     | 56.7       |     |     |    |\\n|                | 79.4      | 67.1| 78.6| 61.6| 80.1| 62.2| 79.3| 64.6 | 80.5| 53.0| 84.2      | 50.3      | 84.8      | 50.4     | 77.1     | 57.4       |     |     |    |\\n|                | 81.1      | 72.3| 80.7| 70.5| 80.9| 71.1| 80.1| 69.4 | 81.3| 49.6| 84.1      | 49.0      | 84.3      | 0.52     | 78.6     | 54.7       |     |     |    |\\n| **Mean**       | **81.9**  | **71.4**| **81.6**| **70.1**| **82.0**| **68.4**| **80.2**| **70.3** | **84.3**| **56.1**| **86.7**  | **53.0**  | **86.3**  | **53.6** | **80.8** | **60.2**   |   |   |   |\\n| **Standard Deviation** | **81.9** | **71.4**| **81.6**| **70.1**| **82.0**| **68.4**| **80.2**| **70.3** | **84.3**| **56.1**| **86.7**  | **53.0**  | **86.3**  | **53.6** | **80.8** | **60.2**   |   |   |   |\\n\\n### Table 9: Worst target domain accuracy of all algorithms on the datasets.\\n\\nIn each setting, we test all algorithms on the 50 American states and report the worst accuracy among all 50 target states. The ACS Income (Synthetic), we simulate strong covariate shifts according to the \\\"Age\\\" feature, where 10% and 20% means the minor group ratio in training, respectively.\\n\\n| Algorithm      | Threshold | Exp | EP | EO | DWR | JTT | RWG | SUBG | RWY | SUBY | Group DRO | CVaR-DORO | CVaR-DRO | XGBoost | LightGBM | Random Forest | MLP | SVM | LR |\\n|----------------|-----------|-----|----|----|-----|-----|-----|------|-----|-----|-----------|-----------|-----------|----------|---------|-------------|-----|-----|----|\\n|                | 83.4      | 70.4| 83.4| 70.1| 83.4| 71.1| 82.0| 70.5 | 83.3| 54.3| 85.4      | 53.9      | 84.5      | 52.9     | 78.8     | 56.7       |     |     |    |\\n|                | 83.7      | 71.1| 83.5| 71.0| 83.1| 71.3| 83.8| 71.2 | 83.8| 53.4| 85.7      | 70.9      | 87.1      | 71.8     | 85.6     | 71.3       |     |     |    |\\n|                | 82.2      | 70.6| 82.1| 71.6| 82.2| 71.2| 82.8| 71.3 | 83.7| 52.4| 86.3      | 72.4      | 86.3      | 71.4     | 85.9     | 72.4       |     |     |    |\\n|                | 81.7      | 72.6| 81.2| 71.6| 81.5| 71.2| 80.2| 68.3 | 84.6| 54.2| 86.8      | 52.9      | 81.2      | 59.0     | 81.2     | 59.0       |     |     |    |\\n|                | 81.8      | 68.3| 81.4| 70.8| 81.7| 71.3| 80.5| 70.5 | 83.3| 54.3| 85.4      | 53.9      | 84.5      | 52.9     | 78.8     | 56.7       |     |     |    |\\n|                | 79.4      | 67.1| 78.6| 61.6| 80.1| 62.2| 79.3| 64.6 | 80.5| 53.0| 84.2      | 50.3      | 84.8      | 50.4     | 77.1     | 57.4       |     |     |    |\\n|                | 81.1      | 72.3| 80.7| 70.5| 80.9| 71.1| 80.1| 69.4 | 81.3| 49.6| 84.1      | 49.0      | 84.3      | 0.52     | 78.6     | 54.7       |     |     |    |\\n| **Mean**       | **81.9**  | **71.4**| **81.6**| **70.1**| **82.0**| **68.4**| **80.2**| **70.3** | **84.3**| **56.1**| **86.7**  | **53.0**  | **86.3**  | **53.6** | **80.8** | **60.2**   |   |   |   |\\n| **Standard Deviation** | **81.9** | **71.4**| **81.6**| **70.1**| **82.0**| **68.4**| **80.2**| **70.3** | **84.3**| **56.1**| **86.7**  | **53.0**  | **86.3**  | **53.6** | **80.8** | **60.2**   |   |   |   |\"}"}
{"id": "PF0lxayYST", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"In each setting, we test all algorithms on the 50 American states and report the mean\\n\\n| Dataset          | Source State | ACS Income | ACS Pub.Cov |\\n|------------------|--------------|------------|-------------|\\n|                  | Source State | ACS Mobility | US Accident |\\n\\nTable 10. Worst target domain accuracy of all algorithms on the datasets. In\\n\\n| Threshold | 80.5 | 69.0 | 78.6 | 69.2 | 78.2 | 66.7 | 78.5 | 68.7 | 84.3 | 64.5 | 91.2 | 55.7 | 82.0 | 64.0 |\\n|-----------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|\\n| Exp       | 80.5 | 69.0 | 78.9 | 68.6 | 81.0 | 68.8 | 78.4 | 69.0 | 85.9 | 65.4 | 90.1 | 58.7 | 93.1 | 56.5 |\\n| EO        | 80.3 | 69.3 | 78.9 | 68.9 | 79.8 | 0.68 | 78.5 | 68.5 | 86.6 | 65.6 | 93.2 | 58.8 | 95.2 | 58.5 |\\n| DWR       | 79.7 | 67.4 | 78.3 | 68.5 | 79.1 | 67.4 | 77.5 | 69.0 | 87.2 | 65.8 | 93.4 | 58.1 | 95.3 | 58.5 |\\n| JTT       | 78.3 | 64.7 | 76.5 | 64.7 | 77.6 | 66.6 | 75.1 | 65.6 | 86.5 | 65.9 | 92.7 | 58.6 | 94.9 | 58.6 |\\n| SUBG      | 80.2 | 68.8 | 79.2 | 68.9 | 79.5 | 67.0 | 78.6 | 69.2 | 86.3 | 66.4 | 93.0 | 58.1 | 95.1 | 58.4 |\\n| RWY       | 78.5 | 64.7 | 76.7 | 65.0 | 76.5 | 65.6 | 74.5 | 64.6 | 86.9 | 64.8 | 93.4 | 58.3 | 95.3 | 58.5 |\\n| SUBY      | 78.2 | 65.0 | 76.3 | 64.6 | 76.6 | 64.7 | 74.7 | 65.0 | 86.5 | 64.2 | 93.7 | 58.8 | 95.2 | 58.5 |\\n| Group DRO | 78.6 | 65.5 | 77.1 | 65.1 | 76.0 | 64.5 | 76.0 | 66.8 | N/A   | N/A   | N/A   | N/A   | N/A   | N/A |\\n| CVaR-DORO | 78.7 | 64.7 | 78.0 | 67.4 | 77.4 | 64.7 | 75.3 | 64.9 | 84.9 | 57.8 | 92.8 | 58.1 | 94.9 | 57.5 |\\n| CVaR-DRO  | 78.6 | 65.0 | 77.2 | 65.6 | 77.7 | 64.4 | 75.7 | 67.6 | 84.7 | 57.2 | 92.7 | 57.5 | 95.0 | 58.4 |\\n| \u03c7\u00b2XGBoost | 80.4 | 68.6 | 79.0 | 69.0 | 80.9 | 68.3 | 79.0 | 69.4 | 87.1 | 65.6 | 94.6 | 59.0 | 95.3 | 58.8 |\\n| LightGBM  | 80.6 | 68.7 | 78.7 | 69.0 | 80.0 | 67.9 | 78.6 | 69.8 | 87.1 | 65.3 | 93.3 | 59.0 | 95.4 | 58.6 |\\n| Random Forest | 80.4 | 68.0 | 79.0 | 68.2 | 80.6 | 67.5 | 78.6 | 68.7 | 86.6 | 66.6 | 93.5 | 58.2 | 95.2 | 58.5 |\\n\\nTable 11. Average target domain Macro-F1 score of all algorithms on the datasets. In\"}"}
{"id": "PF0lxayYST", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"In ACS Income, we test all algorithms on the 50 American states and report the mean and standard deviation across all 3 target states. The standard deviation here reflects the stability of performances across different target states with relatively high sample sizes in testing. We will discuss this in a later section.\\n\\nACS Mobility US Accident Dataset\\n\\n| Algorithm  | Threshold | 0.629 | 0.587 | 0.622 | 0.581 | 0.657 | 0.627 | 0.621 | 0.612 |\\n|------------|-----------|-------|-------|-------|-------|-------|-------|-------|-------|\\n| DWR        | 0.509     | 0.501 | 0.566 | 0.528 | 0.625 | 0.578 | 0.566 | 0.528 | 0.625 |\\n| RWY        | 0.649     | 0.603 | 0.676 | 0.587 | 0.674 | 0.587 | 0.663 | 0.571 | 0.663 |\\n| DP         | 0.842     | 0.696 | 0.816 | 0.732 | 0.817 | 0.796 | 0.817 | 0.796 | 0.817 |\\n| LR         | 0.760     | 0.762 | 0.754 | 0.695 | 0.754 | 0.695 | 0.754 | 0.695 | 0.754 |\\n| Random Forest | 0.760    | 0.762 | 0.754 | 0.695 | 0.754 | 0.695 | 0.754 | 0.695 | 0.754 |\\n| XGBoost    | 0.637     | 0.591 | 0.649 | 0.603 | 0.669 | 0.605 | 0.649 | 0.603 | 0.669 |\\n| LightGBM   | 0.637     | 0.589 | 0.669 | 0.595 | 0.674 | 0.587 | 0.649 | 0.591 | 0.669 |\\n| MLP        | 0.600     | 0.583 | 0.566 | 0.528 | 0.600 | 0.583 | 0.566 | 0.528 | 0.600 |\\n| 2-DORO     | 0.509     | 0.501 | 0.566 | 0.528 | 0.566 | 0.528 | 0.566 | 0.528 | 0.566 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.616 |\\n| CVaR-DRO   | 0.621     | 0.612 | 0.620 | 0.605 | 0.616 | 0.574 | 0.621 | 0.605 | 0.6"}
{"id": "PF0lxayYST", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 14. Worst target domain Macro-F1 Score of all algorithms on the ACS Income and ACS Pub.Cov datasets. In each setting, we test all algorithms on the 50 American states and report the worst Macro-F1 Score among all 50 target states.\\n\\n| Dataset                  | Source State | ACS INCOME | ACS PUB.COV |\\n|--------------------------|--------------|------------|-------------|\\n|                          | Source State |            |             |\\n|                          | CA           | 0.804      | 0.610       |\\n| BaseMethods              | CA           | 0.807      | 0.597       |\\n|                          | MA           | 0.809      | 0.599       |\\n|                          | SD           | 0.714      | 0.644       |\\n|                          | FL           | 0.652      | 0.478       |\\n|                          | TX           | 0.644      | 0.644       |\\n|                          | NE           | 0.432      | 0.468       |\\n|                          | IN           | 0.698      | 0.514       |\\n| SVM                      | CA           | 0.539      | 0.696       |\\n|                          | MA           | 0.790      | 0.559       |\\n|                          | SD           | 0.795      | 0.457       |\\n|                          | FL           | 0.696      | 0.474       |\\n|                          | TX           | 0.622      | 0.750       |\\n|                          | NE           | 0.603      | 0.562       |\\n|                          | IN           | 0.624      | 0.523       |\\n| MLP                      | CA           | 0.574      | 0.696       |\\n|                          | MA           | 0.591      | 0.479       |\\n|                          | SD           | 0.814      | 0.518       |\\n|                          | FL           | 0.713      | 0.518       |\\n|                          | TX           | 0.669      | 0.578       |\\n|                          | NE           | 0.734      | 0.496       |\\n|                          | IN           | 0.664      | 0.578       |\\n| TreeEnsembleMethods      | CA           | 0.608      | 0.600       |\\n|                          | MA           | 0.812      | 0.599       |\\n|                          | SD           | 0.814      | 0.595       |\\n|                          | FL           | 0.687      | 0.687       |\\n|                          | TX           | 0.600      | 0.623       |\\n|                          | NE           | 0.715      | 0.590       |\\n|                          | IN           | 0.503      | 0.727       |\\n|                          |                 | 0.484      | 0.484       |\\n|                          |                 | 0.492      | 0.492       |\\n|                          |                 | 0.746      | 0.573       |\\n| DROMethods               | CA           | 0.583      | 0.584       |\\n|                          | MA           | 0.812      | 0.584       |\\n|                          | SD           | 0.816      | 0.570       |\\n|                          | FL           | 0.732      | 0.669       |\\n|                          | TX           | 0.669      | 0.546       |\\n|                          | NE           | 0.669      | 0.546       |\\n|                          | IN           | 0.652      | 0.685       |\\n|                          |                 | 0.536      | 0.514       |\\n|                          |                 | 0.517      | 0.509       |\\n|                          |                 | 0.649      | 0.603       |\\n|                          |                 | 0.562      | 0.594       |\\n| ImbalancedLearningMethods| CA           | 0.562      | 0.574       |\\n|                          | MA           | 0.807      | 0.574       |\\n|                          | SD           | 0.820      | 0.559       |\\n|                          | FL           | 0.719      | 0.669       |\\n|                          | TX           | 0.606      | 0.549       |\\n|                          | NE           | 0.710      | 0.526       |\\n|                          | IN           | 0.642      | 0.476       |\\n|                          |                 | 0.609      | 0.518       |\\n|                          |                 | 0.525      | 0.518       |\\n|                          |                 | 0.718      | 0.548       |\\n|                          |                 | 0.548      | 0.548       |\\n|                          |                 | 0.748      | 0.660       |\\n|                          |                 | 0.660      | 0.660       |\\n|                          |                 | 0.587      | 0.587       |\\n|                          |                 | 0.528      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.528      | 0.528       |\\n|                          |                 | 0.716      | 0.528       |\\n|                          |                 | 0.755      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | 0.573       |\\n|                          |                 | 0.855      | 0.528       |\\n|                          |                 | 0.573      | "}
{"id": "PF0lxayYST", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D.10 Agreement and Maintenance Plan\\n\\nHosting Platform.\\nWe will use Github as the hosting platform of our code. We provide detailed preprocessing Python scripts to guide users to replicate and process data from scratch. We also illustrate methodologies to run full experiment results in the code. We also list the license for each dataset and user guidance in the Readme file in that Github.\\n\\nDependencies.\\nThe benchmark is built upon Python 3.8+ and depends on PyTorch, aif360, fairlearn, xgboost, lightgbm. Besides, it uses numpy, scipy, and pandas for basic data manipulation.\\n\\nMaintenance Plan.\\nThe datasets provided here will be maintained by the authors of the paper, which can be contacted by raising an issue on GitHub or by contacting the first authors directly. Our benchmark and a simple open-sourced Python package based on that may be updated at the discretion of authors in the future, which includes more refined algorithm implementations, datasets, and framework with improved efficiency.\\n\\nAuthor Statements.\\nTo the best of our knowledge, the proposed benchmark is based on existing datasets and does not violate any existing licenses non contain personally identifiable or privacy-related information. And we claim all the responsibility in case of a violation of rights if such a violation were to exist.\"}"}
{"id": "PF0lxayYST", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets\\n\\nJiashuo Liu,\u2217 Tianyu Wang,\u2217 Peng Cui, Hongseok Namkoong\\n\\n1Department of Computer Science and Technology, Tsinghua University\\n2Department of Industrial Engineering and Operations Research, Columbia University\\n3Decision, Risk, and Operations Division, Columbia Business School\\nliujiashuo77@gmail.com, tw2837@columbia.edu\\n\\ncuip@tsinghua.edu.cn, namkoong@gsb.columbia.edu\\n\\nAbstract\\nDifferent distribution shifts require different algorithmic and operational interventions. Methodological research must be grounded by the specific shifts they address. Although nascent benchmarks provide a promising empirical foundation, they implicitly focus on covariate shifts, and the validity of empirical findings depends on the type of shift, e.g., previous observations on algorithmic performance can fail to be valid when the $Y|X$ distribution changes. We conduct a thorough investigation of natural shifts in 5 tabular datasets over 86,000 model configurations, and find that $Y|X$-shifts are most prevalent. To encourage researchers to develop a refined language for distribution shifts, we build $WHYSHIFT$, an empirical testbed of curated real-world shifts where we characterize the type of shift we benchmark performance over. Since $Y|X$-shifts are prevalent in tabular settings, we identify covariate regions that suffer the biggest $Y|X$-shifts and discuss implications for algorithmic and data-based interventions. Our testbed highlights the importance of future research that builds an understanding of why distributions differ.\\n\\n1 Introduction\\nThe performance of predictive models has been observed to degrade under distribution shifts in a wide range of applications, such as healthcare [9, 95, 76, 93], economics [36, 25], education [6], vision [74, 63, 86, 98], and language [62, 7]. Distribution shifts vary in type, typically defined as either a change in the marginal distribution of the covariates ($X$-shifts) or the conditional relationship between the outcome and covariate ($Y|X$-shifts). Real-world scenarios comprise of both types of shifts. In computer vision [62, 47, 81, 38, 101], $Y|X$-shifts are less likely to occur as $Y$ is constructed from human knowledge given an input $X$, unless the labeling noise is severe. For tabular datasets, $Y|X$-shifts can be more common because of missing variables and hidden confounders. For example, the prevalence of a disease may be affected by unrecorded covariates whose distribution changes across domains, such as lifestyle factors and socioeconomic status [39, 103, 93].\\n\\nDifferent types of distribution shifts require different solutions. When facing $X$-shifts, the implicit goal of many researchers is to develop a single robust model that can be generalized effectively across multiple domains. Various algorithms have been developed to align the marginal distributions ($P_X$), including domain adaptation and importance sampling methods. However, under $Y|X$-shifts, there may be a fundamental trade-off between learning algorithms: to perform well on a target distribution, a model may have to necessarily perform worse on others. Algorithmically, typical methods for\\n\\n\u2217Equal contribution\\n\\nMore information on the data, codes and python packages about $WHYSHIFT$ are available at https://github.com/namkoong-lab/whyshift.\\n\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\"}"}
{"id": "PF0lxayYST", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"addressing $Y|X$-shifts include distributionally robust optimization (DRO) \\\\[13, 85, 28, 80, 27\\\\] and causal learning methods \\\\[72, 8, 83, 46\\\\]. Operationally, the modeler can identify and collect an unobserved confounder $C$ such that $Y|X,C$ remains invariant across domains, or resort to overhauling the entire model development pipeline to collect more samples from the target. However, existing distribution shift benchmarks only focus on $X$-shifts \\\\[74, 47, 107, 81, 101\\\\]. To illustrate this concretely, consider popular tabular datasets used to benchmark model performance over demographic subgroups: Adult, BRFSS, COMPAS, ACS Public Coverage, and ACS Income \\\\[3, 102, 31\\\\]. We take the largest demographic group as training $P$ and the smallest as target $Q$ to simulate subgroup shifts, e.g., in Adult, $P =$ white men and $Q =$ non-white women. We measure the optimality gap of the model $f_P$ trained on $P$ as measured on the target $Q$ using the relative regret $E_Q[\\\\ell(Y, f_P(X))]$ \\\\[1.1\\\\], where $f_P \\\\in \\\\text{argmin}_{f \\\\in F} E_P[\\\\ell(Y, f(X))]$ and $\\\\ell(\\\\cdot, \\\\cdot)$ is the 0-1 loss. For these widely-used benchmarks, the relative regret is small (left 5 bars in Figure 1), suggesting the $Y|X$ distribution is largely transferable across those demographic groups.\\n\\nTo study diverse distribution shift patterns, we consider 5 real-world tabular datasets constructed from the US Census (as proposed by Ding et al. \\\\[25\\\\]) and traffic measurements \\\\[64, 65, 2, 1\\\\]. We focus on spatiotemporal shifts to model most common natural shifts. Our full benchmark covers 22 settings (see Table 3 in Appendix D), where each setting includes one source (e.g., California) and a number of possible targets (e.g., other states). For illustration purposes, we focus on 7 settings covering 169 possible source-target pairs (see Table 2) and carefully select one target per setting to represent a wide range of $Y|X$-shifts (right bars in Figure 1). We find $Y|X$-shifts constitute a substantial proportion of real-world distribution shifts, yet previous (unqualified) empirical findings in the literature only hold over mild $X$-shifts and fail to hold over $Y|X$-shifts (Section 2). Out of 169 source-target pairs with significant performance degradation (> 8 percentage points of accuracy drop), 80% of them are primarily attributed to $Y|X$-shifts. $Y|X$-shifts introduce considerable performance variations on the target distribution, leading to different relationships between in- and out-of-distribution performances across settings and datasets. This is in stark contrast to the recently observed accuracy-on-the-line phenomenon \\\\[63\\\\], where the in- and out-of-distribution performance have been posited to exhibit a strong linear relationship. In Figure 2, we showcase how the accuracy-on-the-line trend fails to hold when $Y|X$-shifts are strong. Our results imply that the standard practice of blindly evaluating performance over various shifts is only justified over $X$-shifts, where we expect there to be a single model that is robust across domains. For severe $Y|X$-shifts, the training data may not even be informative for modeling the $Y|X$ relationship in the target. To inform algorithmic and data-based interventions, we must understand why the distribution changed. In Section 3, we illustrate the need for more methodological research that builds a deep understanding of distributional differences. As a concrete example, we show that a simple approach for identifying covariate regions with strong $Y|X$-shifts can suggest data-based interventions. Our case study shows it can be useful to collect target data over a particular covariate region, or features $C$ such that the $Y|X,C$ distribution is more stable across source and target.\"}"}
{"id": "PF0lxayYST", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2. Target vs. source accuracies for 22 algorithms and datasets in our benchmark. A linear fit (green line) and its corresponding $R^2$ value is reported on the top left of each figure. Each blue point represents one hyperparameter configuration. (a)-(f): six examples of ACS Income, ACS Mobility, Taxi, ACS Pub.Cov, US Accident datasets. (g): simulated covariate shifts on on sub-sampled ACS Income dataset.\\n\\n- DRO methods are sensitive to configurations and exhibit significant performance variations.\\n- Imbalance and fairness methods show similar performance with the base learner (XGBoost).\\n- A small validation data from the target distribution goes a long way, and more generally, non-algorithmic interventions warrant greater consideration.\\n\\n2 Distribution Shifts in Tabular Settings\\n\\nTo illustrate how complex distribution shift patterns arise in tabular data, we compare 22 algorithms including tree ensemble methods, robust learning, imbalance, and fairness methods. On 5 real-world tabular datasets (ACS Income, ACS Public Coverage (ACS Pub.Cov), ACS Mobility, US Accident and Taxi), we consider the natural spatial shifts between states/cities, e.g., California to Puerto Rico. For the ACS Pub.Cov dataset, we also consider temporal shifts, e.g., from 2010 to 2017. Since all natural distribution shifts we consider are largely induced by $Y|X$-shifts, we construct a synthetic subgroup shift from younger people to older people in order to simulate $X$-shifts.\\n\\nDeferring a detailed summary to Section 4.1, we focus on introducing representative phenomena in this section.\\n\\nIn Figure 2, we present the source (in-distribution) and target (out-of-distribution) performances of 22 algorithms, each with 200 hyperparameter configurations. To understand shift patterns, we utilize the recently proposed DIstribution Shift DEcomposition (DISDE) framework which decomposes the performance degradation into components attributed to $Y|X$- and $X$-shifts. Using the best XGBoost configuration as the baseline model for each source-target pair, we present the total performance degradation and the proportion attributed to $Y|X$-shifts.\\n\\nDistribution shifts are predominantly $Y|X$-shifts in our empirical study. We find performance degradation under natural shifts is overwhelmingly attributed to $Y|X$-shifts, as illustrated in the curated list in Figure 2. More generally, out of the 169 source-target pairs whose performance degradation is larger than 8 percentage points, 87.2% of them have over 50% of the performance degradation attributed to $Y|X$-shifts (70.2% of them have over 60% of the gap attributed to $Y|X$-shifts). We conjecture that $Y|X$-shifts are prevalent in tabular data due to missing features. For example, in the context of income prediction, individual outcomes may change due to unobserved economic and political factors whose distribution changes over geographical locations. In contrast, in vision and language tasks, the input (e.g., pixels and words) often encapsulates most of the necessary information for predicting the outcome, making strong $Y|X$-shifts less likely unless the labeling noise is severe. Consequently, compared to vision and language data in domain generalization...\"}"}
{"id": "PF0lxayYST", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our findings highlight the importance of understanding the cause of the distribution shift. Accuracy-on-the-line fails to hold over $Y \\\\mid X$-shifts. We find significant variation in the relationship between source and target performance throughout all natural distribution shifts presented in Figure 2 (a)-(g). The correlation between source and target performance is relatively weak, and we tend to see poor linear fits (low $R^2$) when the bulk of the performance degradation is attributed to $Y \\\\mid X$-shifts. That is, we see in Figures 2 (a)-(f) that the relationship between the two performances exhibits significant fluctuations across different source-target pairs. In Figure 3, we observe that performance rankings of algorithms substantially vary across different $Y \\\\mid X$-shifts. Our finding highlights the inherent complexity associated with real distribution shifts in tabular datasets, which stands in sharp contrast to the \\\"accuracy-on-the-line\\\" phenomena [63].\\n\\nSource and target performances are correlated when $X$-shifts dominate. Across all natural shifts we study, we find $X$-shifts are only prominent in temporal shifts (ACS Time dataset; Figure 2f). To better investigate the role of $X$-shifts, we subsample the data to artificially induce strong covariate shifts over an individual's age. Specifically, we focus on individuals from California and form two groups according to whether their age is $\\\\geq 25$. The source data oversamples low age groups where 80% is drawn from the age $\\\\leq 25$ group; proportions are reversed in the target data. On this synthetic shift we construct, the DISDE [16] method attributes the bulk of the performance degradation to $X$-shifts in Figure 2g. Our finding confirms the intuition that unobserved economic factors remain relatively consistent for individuals from the same state (CA). In this synthetic example with $X$-shifts, we observe a relatively strong correlation between source and target performance. Moreover, the large performance degradation on these datasets suggests that existing robust learning methods are still severely affected by covariate shifts, indicating the need for future research that addresses covariate shifts in tabular data.\\n\\n3 Case Study: Understanding Distribution Shifts Facilitates Interventions\\n\\nTypical algorithmic approaches to handling practical distribution shifts aim to optimize performance over a postulated set of distribution shifts. Causal learning assumes the underlying causal structure can be learned to withstand distribution shifts [72, 83, 82, 77], while DRO methods explicitly optimize worst-case performance over a set of distributions [13, 53, 28, 27]. Despite progress in algorithm design, there are few efforts that examine the patterns of real-world distribution shifts. It remains unclear whether the data assumptions made by algorithms hold in practice, and this mismatch often leads to poor empirical performance [40, 95, 76, 19, 46].\\n\\nComplementing the active literature on algorithmic development, we present an empirical study that underscores the practical significance of tools that provide a qualitative understanding of the shift at hand. In light of the prevalence of $Y \\\\mid X$-shifts in tabular data, we introduce a simple yet effective approach for identifying covariate regions that suffer strong $Y \\\\mid X$-shifts. We demonstrate our approach on the income prediction task (ACS Income), and show that it can guide operational interventions for addressing distribution shifts. Our case study is not meant to be a rigorous scientific analysis, but rather a (heuristic) vignette illustrating the need for future research on methodologies that can generate qualitative insights on distributional differences.\"}"}
{"id": "PF0lxayYST", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"With the likelihood ratios, we estimate the best prediction model under $P$. We provide more discussion on the choices of Appendix C.4. Appendix C.5 and Appendix C.6. We show that the node splitting criterion in a standard decision trees training procedure is equivalent with our goal of finding regions with the largest discrepancy in operational and modeling interventions. Letting $\\\\alpha$ be random variable supported on the space $X\\\\times Y$, consider a model $f\\\\in F$ and region $R\\\\subseteq X$. The pseudo-code is summarized in the Algorithm 1; To allow simple interpretation, we plug these empirical estimands in to obtain the estimated likelihood ratios $w$. Then, for any threshold $\\\\lambda$, we can express the likelihood ratios as:\\n\\n$$w(x) = \\\\frac{P(x|q)\\\\pi(x)}{P(x|s)\\\\pi(x)} = \\\\frac{P(x|q)}{P(x|s)}\\\\frac{\\\\pi(x)}{\\\\pi(x)} = \\\\lambda,$$\\n\\nwhere $q(X) = \\\\arg\\\\min f(x)$ and $s(X) = \\\\arg\\\\min f(x)$.\\n\\nEmpirically, given samples $\\\\{X_i, y_i\\\\}_{i=1}^n$ from distribution $P$, we can express the likelihood ratios as:\\n\\n$$w(x) = \\\\frac{P(x|q)\\\\pi(x)}{P(x|s)\\\\pi(x)} = \\\\frac{P(x|q)}{P(x|s)}\\\\frac{\\\\pi(x)}{\\\\pi(x)} = \\\\lambda.$$\\n\\nHere we propose a simple yet effective method for identifying covariate regions with strong distribution shifts. Despite its simplicity, we demonstrate in the following subsections that our method can inspire more advanced methods. We focus on the design of interventions. We focus on the design of interventions.\\n\\n### 3.2 Data-based Interventions\\n\\nUsing Algorithm 1, we now demonstrate how a better understanding of distribution shifts can facilitate the design of interventions. We choose a specific density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high, and low density whenever either is small. Following Cai et al. [16] introduced a shared distribution approach. The shared distribution has high density when both $p$ and $q$ are high,"}
{"id": "PF0lxayYST", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1: Identify Regions with Strong $|Y|_X$-Shifts.\\n\\n**Input:**\\n- Source samples $\\\\{(x^P_i, y^P_i)\\\\}_{i \\\\in [n^P]}$ i.i.d. $\\\\sim P$ and target samples $\\\\{(x^Q_j, y^Q_j)\\\\}_{j \\\\in [n^Q]}$ i.i.d. $\\\\sim Q$.\\n\\n1. Estimate $\\\\hat{\\\\pi}(x) \\\\approx P(\\\\tilde{X} \\\\sim Q | \\\\tilde{X} = x)$ by training a classifier on the source and target samples.\\n2. Calculate density ratios $w_{\\\\mu}(\\\\hat{\\\\pi}(x), \\\\hat{\\\\alpha})$ according to Equation (3.2) and (3.3) for $\\\\mu = P, Q$.\\n3. Fit prediction models $f_{\\\\mu}$ according to Equation (3.4) replacing $w_{\\\\mu}(\\\\pi^\\\\ast(x), \\\\alpha^\\\\ast)$ there with $w_{\\\\mu}(\\\\hat{\\\\pi}(x), \\\\hat{\\\\alpha})$ for $\\\\mu = P, Q$.\\n4. Fit a model $h(x)$ to predict $|f^P(x) - f^Q(x)|$ using samples $\\\\{(x^P_i, y^P_i)\\\\}_{i \\\\in [n^P]}$ and $\\\\{(x^Q_j, y^Q_j)\\\\}_{j \\\\in [n^Q]}$, each with the weight $\\\\lambda^P_i$ (or $\\\\lambda^Q_j$ respectively), according to Equation (3.5).\\n\\n**Output:** Region $R = \\\\{x \\\\in X: h(x) \\\\geq b\\\\}$.\\n\\nGiven the considerable disparities in the economy, job markets, and cost of living between CA and PR/SD, we observe substantial performance degradation due to distribution shifts. In Figure 4a, we first decompose the performance degradation from CA to PR to understand the shift and find $|Y|_X$-shifts are the predominant factor. The calculation of $X$-shifts and $|Y|_X$-shifts is deferred to Appendix C.1. We dive deeper into the significant $|Y|_X$-shifts and identify from CA to PR for the XGBoost and MLP classifier. From the region shown in Figure 4c and Figure 4d, we find college-educated individuals in business and educational roles (such as management, business, and educational work) exhibit large $|Y|_X$ differences.\\n\\nTo illustrate how our analysis can inspire subsequent operational interventions to enhance performance on the target distribution, we study two operational interventions.\\n\\n**Collect specific data from the target**\\n\\nTo improve target performance, the most natural operational intervention is to collect additional data from the target distribution. While a rich body of work on domain adaptation [69, 23, 30, 90, 89] study how to effectively utilize data from the target distribution to improve performance, there is little work that discusses how to efficiently collect supervised data from the target distribution to maximize out-of-distribution generalization. To highlight the need for future research in this space, we use the interpretable region identified by Algorithm 1 as shown in Figure 4c to simulate a concerted data collection effort.\\n\\nSince indiscriminately collecting data from the target distribution can be resource-intensive, we concentrate sampling efforts on the subpopulation that may suffer from $|Y|_X$-shifts and selectively gather data on them. For five base methods (logistic regression, MLP, random forest, lightGBM, and XGBoost), we randomly sample 250 points from the whole target distribution and the identified region suffering prominent $|Y|_X$-shifts, respectively. We report the test accuracies in Figure 4e, and observe that incorporating data from this region is more effective in enhancing OOD generalization.\\n\\nWhile preliminary, our results demonstrate the potential robustness benefits of efficiently allocating resources toward concerted data collection. Future methodological research in this direction may be fruitful; potential connections may exist with active learning algorithms [84, 96, 59].\\n\\n**Add more relevant features**\\n\\nWe now illustrate the potential benefits of generating qualitative insights on the distribution shift at hand. Our analysis in Figure 4c suggests educated individuals in financial, educational, and legal professions tend to experience large $|Y|_X$-shifts from CA to PR. These roles typically need communication skills, and language barriers could potentially affect their incomes. In California (CA), English is the primary language, while in Puerto Rico (PR), despite both English and Spanish being recognized as official languages, Spanish is predominantly spoken. Consequently, for a model trained on CA data and tested on PR data, incorporating a new feature that denotes English language proficiency (hereafter denoted \u201cENG\u201d) might prove beneficial in improving generalization performances. However, this feature is not included in the ACS Income dataset.\\n\\nTo address this, we went back to the Census Bureau\u2019s American Community Survey database to include the ENG feature in the set of covariates. In Figure 4b, we observe that the inclusion of this feature substantially reduces the degradation due to $|Y|_X$-shifts, verifying that the originally missing feature is relevant in improving generalization performances.\"}"}
{"id": "PF0lxayYST", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Case study illustrations. (a)-(b) Decomposition of performance degradation for the XGBoost classifier from CA to PR. Figure (a) is for the original setting and (b) corresponds to the results post-integration of the \\\"ENG\\\" feature. (c)-(d) Demonstration of Algorithm 1: an interpretable version of the region with strong $Y \\\\mid X$-shifts for the XGBoost and MLP models, respectively. (e) Test accuracies of five typical base methods trained on the source, post addition of 250 randomly selected target observations, and 250 observations from the identified risk region. (f)-(g) Performances of all algorithms prior to and following the addition of the \\\"ENG\\\" feature. Figure (f) corresponds to the CA to PR, and Figure (g) is CA to SD.\\n\\nTable 1: Overview of datasets and 7 selected settings.\\n\\n| #ID | Dataset Type | #Samples | #Features | Outcome | #Domains | Selected Settings | Shift Patterns |\\n|-----|--------------|----------|-----------|---------|----------|------------------|---------------|\\n| 1   | ACS Income  | Natural  | 1,599,229 | 9        | Income $\\\\geq 50k$ | 51 California $\\\\rightarrow$ Puerto Rico | $Y \\\\mid X \\\\gg X$ |\\n| 2   | ACS Mobility| Natural  | 620,937   | 21       | Residential Address | 51 Mississippi $\\\\rightarrow$ Hawaii | $Y \\\\mid X \\\\gg X$ |\\n| 3   | Taxi        | Natural  | 1,506,769 | 7        | Duration time $\\\\geq 30$ min | 4 New York City $\\\\rightarrow$ Botog\u00e1 | $Y \\\\mid X \\\\gg X$ |\\n| 4   | ACS Pub.Cov | Natural  | 1,127,446 | 18       | Public Ins. Coverage | 51 Nebraska $\\\\rightarrow$ Louisiana | $Y \\\\mid X > X$ |\\n| 5   | US Accident | Natural  | 297,132   | 47       | Severity of Accident | 14 California $\\\\rightarrow$ Oregon | $Y \\\\mid X > X$ |\\n| 6   | ACS Pub.Cov | Natural  | 859,632   | 18       | Public Ins. Coverage | 4 2010 (NY) $\\\\rightarrow$ 2017 (NY) | $Y \\\\mid X < X$ |\\n| 7   | ACS Income  | Synthetic | 195,665   | 9        | Income $\\\\geq 50k$ | 2 Younger $\\\\rightarrow$ Older | $Y \\\\mid X \\\\ll X$ |\\n\\nENG feature may be one cause of $Y \\\\mid X$-shifts. Figure 4f contrasts the performances of 22 algorithms (each with 200 hyperparameter configurations) with original features with those that additionally use the ENG feature. The new feature significantly improves target performances across all algorithms; roughly speaking, we posit that we have identified a variable $C$ such that $Y \\\\mid X,C$ remains similar across CA and PR. However, when we extend this comparison to the source-target pair (CA $\\\\rightarrow$ SD), we observe no significant improvement (Figure 4g). This highlights that the selection of new features should be undertaken judiciously depending on the target distributions of interest. A feature that proves effective in one target distribution might not yield similar results in another.\"}"}
{"id": "PF0lxayYST", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we detail our benchmark and summarize the main observations. Our finding highlights\\nthe importance of future research that builds an understanding of\\nwhy\\nthe distribution has shifted.\\n\\n4.1 Setup\\nDatasets\\nWe explore distribution shifts on 5 real-world tabular datasets from the economic and\\ntraffic sectors with\\nnatural\\nspatiotemporal distribution shifts. For economic data, we use\\nACS\\nIncome\\n, ACS Mobility\\n, and ACS Public Coverage\\ndatasets from the US-wide ACS PUMS data\\n[25]\\n, where the outcome is whether an individual's income exceeds 50k, whether an individual\\nchanged the residential address one year ago, and whether an individual is covered by public health\\ninsurance, respectively. We primarily focus on spatial shifts across different states in the US. To\\ncomplement spatial shifts, we derive an\\nACS Time\\n task based on the\\nACS Public Coverage\\ndataset,\\nwhere there are temporal shifts between different years (2010 to 2021). For traffic data, we use\\nUS Accident\\n[64, 65]\\nand\\nTaxi\\n[2, 1]\\n, where the outcome is whether an accident is severe and\\nwhether the total ride duration time exceeds 30 minutes, respectively. We focus on spatial shifts\\nbetween different states/cities. We summarize the datasets in Table 1 and defer a full description to\\nthe Appendix D.1.\\n\\nAlgorithms\\nWe evaluate\\n22\\nalgorithms that span a wide range of learning strategies on tabular\\ndata, and compare their performances under different patterns of distribution shifts we construct.\\nConcretely, these algorithms include: (1) base learners: Logistic Regression, SVM, fully-connected\\nneural networks (MLP) with standard ERM optimization; (2) tree ensemble models: Random Forest,\\nXGBoost, LightGBM; (3) robust learning: CVaR-DRO and \\\\( \\\\chi^2 \\\\)-DRO with fast implementation\\n[53]\\n, CVaR-DRO and \\\\( \\\\chi^2 \\\\)-DRO of outlier-robust enhancement\\n[105]\\n, Group DRO\\n[79]\\n; (4) imbalanced\\nlearning: JTT\\n[57]\\n, SUBY , RWY , SUBG, RWG\\n[42]\\n, DWR\\n[51]\\nand (5) fairness-enhancing methods:\\ninprocessing method\\n[4] with demographic parity, equal opportunity, error parity as constraints,\\npostprocessing method\\n[37] with exponential and threshold controls. For DRO methods (i.e. (3)),\\nwe use MLP as the backbone model. For other algorithms compatible with tree ensemble models\\n(i.e. (4-5)), we use the XGBoost model due to its superior performance on tabular data\\n[34]\\n. For\\nalgorithms requiring group labels, we use 'hour' for\\nUS Accident\\nand\\nTaxi\\n, and 'sex' for the others.\\nDetailed descriptions for each algorithm can be found in Appendix D.5.\\n\\nBenchmarks\\nWe conduct experiments with more than 86,000 model configurations on various\\nsource-target distribution shift pairs, and carefully select\\n7 selected pairs with different distribution\\nshift patterns\\n. In Table 1, we characterize the shift patterns of these 7 source-target pairs, which\\ncontain different proportions of\\n\\\\( Y \\\\mid X \\\\)-shifts and\\n\\\\( X \\\\)-shifts corresponding with plots in Figure 2. The\\nfirst six settings are natural shifts. In the last setting, we sub-sample the dataset according to age to\\nintroduce covariate shift, where we focus on individuals from CA and form two groups according to\\nwhether their age is\\n\\\\( \\\\geq 25 \\\\). The source data over-samples the low age group where 80% is drawn from\\nthe group where the individual's age\\n\\\\( \\\\geq 25 \\\\), and the proportions are reversed in the target data.\\n\\nIn Figure 5 and Figure 6, we plot the performance of algorithms using their best hyperparameter\\nconfiguration on the validation dataset (i.i.d. with the source distribution). Additional results with\\nvarious source distributions are in the Appendix. Our benchmark is designed to support empirical\\nresearch, including new learning algorithms and diagnostics that provide qualitative insights on\\ndistribution shifts.\"}"}
{"id": "PF0lxayYST", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Performance drop between source and target data of all algorithms in our selected 7 settings.\\n\\nFigure 7. (a) Sensitivity of DRO methods and Imbalance Methods w.r.t. configurations. (b) Performances under different validations on ACS Income dataset.\\n\\nHyper-parameter Tuning\\nFor each model, we conduct a grid search over a large set of hyper-parameters. See Appendix D.3 for the complete search space for each method. When one method includes another as a \\\"base\\\" learner (e.g., DRO with MLP, RWY with XGBoost), we explore the full tuning space for the base model (e.g., the cross-product of all MLP hyper-parameters with all DRO hyper-parameters). To control for computational effort, each method is run with 200 configurations for each source-target pair and we select the best configuration according to the i.i.d. validation performance. In Figure 7b, we further compare different choices of validation protocols.\\n\\nEvaluation Metrics\\nIn our benchmark, we include different metrics for a thorough evaluation. Specifically, we use Average Accuracy (micro-average), Worst-group Accuracy, and Macro-F1 score in our main results where we only have one target distribution. For the results with multiple target distributions (i.e. 3 in Taxi, 13 in US Accident and 50 in the others), we present all target accuracies and Macro-F1 scores, as well as the worst-distribution accuracy and Macro-F1 score among all target distributions in Appendix D.6, D.7, D.8, D.9.\\n\\n4.2 Analysis\\nDifferent algorithms do not exhibit consistent rankings over different shift patterns. In Figure 5, we observe the rankings across different shifts are quite different, especially for ACS Income (CA \u2192 PR) and ACS Mobility (MS \u2192 HI) where Y \u2223 X-shifts dominate. This observation reaffirms the phenomena in Figure 2 that as Y \u2223 X-shifts become stronger, the relationship between source vs target performances becomes less consistent. In Appendix D.3, we also show that even for a fixed source distribution in one fixed prediction task, algorithmic rankings of performances on different target distributions vary a lot.\\n\\nTree ensemble methods show competitive performance, but do not significantly improve the generalization drop between source and target data. From Figure 5, tree-based ensembles (yellow bars) show robust and competitive performance on the target distribution in 6 out of 7 settings. However, in Figure 6 which plots the performance degradation between source and target, tree ensembles do not show improved robustness. This suggests that they do not actually achieve better robustness against real-world distribution shifts, and their better performances on target data may simply be due to better fitting the source distribution.\\n\\nDRO methods are sensitive to configurations, with rankings varying significantly across 7 different settings. From Figure 5, DRO methods exhibit competitive performances on ACS Mobility (MS \u2192 HI), Taxi (NYC \u2192 BOG), and ACS Income (Young \u2192 Old), yet underperform in others. This sensitivity to configurations, as shown in Figure 7a (red points), could be attributed to...\"}"}
{"id": "PF0lxayYST", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the worst-case optimization that perturbs the training distribution within a pre-defined uncertainty set, without any information regarding the target distribution. However, when target information is incorporated for hyper-parameter tuning, as shown in Figure 7b, there is a notable improvement in the performance of DRO methods. Our observations suggest potential avenues for building more refined uncertainty sets in DRO methods.\\n\\nImbalance methods and fairness methods show similar performance with the base learner (XGBoost).\\n\\nIn our experiments, we choose the XGBoost model as the base learner for imbalance and fairness methods due to its superior performance on tabular data [34]. However, from Figure 5 and Figure 6, imbalance methods and fairness methods do not show a clear improvement upon their base learner (XGBoost, last yellow bar). Further, as shown in Figure 7a, imbalance methods (green) are also quite sensitive to configurations, and their performances do not improve much when their hyperparameters are tuned over the target data (Oracle).\\n\\nTarget information matters in validation.\\n\\nBased on the ACS Income (CA\u2192PR) dataset, we compare different validation protocols, including the best average accuracy, minimum subgroup discrepancy, and best worst-subgroup accuracy on validation data generated from the source distribution. We also use the Oracle validation that chooses the configuration with the best average accuracy on validation data generated from the target distribution. In Figure 7b, we find the first three protocols do not show a significant difference. However, oracle validation with target information substantially improves the effectiveness of both DRO and tree ensemble methods. We conclude using target information for model selection can provide robustness gains even with a small target dataset.\\n\\nNon-algorithmic interventions warrant greater consideration.\\n\\nReflecting on Section 3, it is clear that operational interventions yield significant enhancements for various methods, as demonstrated in Figure 4e and Figure 4f. In comparison to algorithmic interventions, such as designing different algorithms (e.g., DRO, Imbalance methods), a data-centric approach can be more effective in addressing distribution shifts. For instance, research on feature collection and feature engineering methods may prove impactful. Another avenue for future work is developing methods that can optimally incorporate expensive samples from the target distribution.\\n\\n5 Discussion\\n\\nWe explore the complexity of distribution shifts in real-world tabular datasets in depth. Using natural shifts from 5 real-world tabular datasets across different domains, we specify each shift pattern and evaluate 22 methods via experiments with over 86k trained models. Our benchmark WSHIFT encompasses various distribution shift patterns to evaluate the robustness of the methods.\\n\\nWe propose a simple but effective algorithm to identify regions with large Y|X-shifts, and through a comprehensive case study, we demonstrate how a better understanding of distribution shifts facilitates algorithmic and data-based interventions. Our findings highlight the importance of future research to understand how and why distributions differ in real-world applications.\\n\\nOur study leaves many open directions for improvements in future work. Our benchmark only includes tabular datasets from the economic and transportation domain. Considering datasets from other domains such as the medicine or those involving feature embeddings may highlight different types of distribution shift. On the algorithmic side, our region-identification algorithm requires some target data to identify risky regions and cannot be used in cases where the target distribution is completely unknown. Furthermore, targeted data collection on regions of Y|X-shifts may be pose ethical and privacy concerns for marginalized groups. We provide more discussion in Appendix B.\"}"}
{"id": "PF0lxayYST", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgement\\n\\nWe thank Tiffany Cai for her help with implementing the DISDE method on our benchmarks. Peng Cui was supported in part by National Key R&D Program of China (No. 2018AAA0102004), National Natural Science Foundation of China (No. U1936219, 62141607). Hongseok Namkoong was partially supported by the Amazon Research Award.\\n\\nReferences\\n\\n[1] Other taxi dataset. https://www.kaggle.com/datasets/mnavas/taxi-routes-for-mexico-city-and-quito.\\n\\n[2] Us taxi dataset. https://www.kaggle.com/competitions/nyc-taxi-trip-duration/data.\\n\\n[3] Robert Adragna, Elliot Creager, David Madras, and Richard S. Zemel. Fairness and robustness in invariant learning: A case study in toxicity classification. CoRR, abs/2011.06485, 2020.\\n\\n[4] Alekh Agarwal, Alina Beygelzimer, Miroslav Dud\u00edk, John Langford, and Hanna Wallach. A reductions approach to fair classification. In International Conference on Machine Learning, pages 60\u201369. PMLR, 2018.\\n\\n[5] Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant risk minimization games. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 145\u2013155. PMLR, 13\u201318 Jul 2020.\\n\\n[6] Evelin Amorim, Marcia Can\u00e7ado, and Adriano Veloso. Automated essay scoring in the presence of biased ratings. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 229\u2013237, 2018.\\n\\n[7] Shushan Arakelyan, Rocktim Jyoti Das, Yi Mao, and Xiang Ren. Exploring distributional shifts in large language models for code analysis. CoRR, abs/2303.09128, 2023.\\n\\n[8] Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.\\n\\n[9] Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. IEEE transactions on medical imaging, 38(2):550\u2013560, 2018.\\n\\n[10] Sara Beery, Arushi Agarwal, Elijah Cole, and Vighnesh Birodkar. The iwildcam 2021 competition dataset. arXiv preprint arXiv:2105.03494, 2021.\\n\\n[11] Rachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, Seema Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan Saha, Prasanna Sattigeri, Moninder Singh, Kush R. Varshney, and Yunfeng Zhang. AI fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. IBM J. Res. Dev., 63(4/5):4:1\u20134:15, 2019.\\n\\n[12] Sarah Bird, Miro Dud\u00edk, Richard Edgar, Brandon Horn, Roman Lutz, Vanessa Milan, Mehrnoosh Sameki, Hanna Wallach, and Kathleen Walker. Fairlearn: A toolkit for assessing and improving fairness in ai. Microsoft, Tech. Rep. MSR-TR-2020-32, 2020.\\n\\n[13] Jose Blanchet, Yang Kang, and Karthyek Murthy. Robust wasserstein profile inference and applications to machine learning. Journal of Applied Probability, 56(3):830\u2013857, 2019.\"}"}
{"id": "PF0lxayYST", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics for measuring unintended bias with real data for text classification. In WWW, pages 491\u2013500, 2019.\\n\\nKailash Budhathoki, Dominik Janzing, Patrick Bloebaum, and Hoiyi Ng. Why did the distribution change? In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, pages 1666\u20131674. PMLR, 2021.\\n\\nTiffany Cai, Hongseok Namkoong, Steve Yadlowsky, et al. Diagnosing model performance under distribution shift. arXiv preprint arXiv:2303.02011, 2023.\\n\\nShiyu Chang, Yang Zhang, Mo Yu, and Tommi Jaakkola. Invariant rationalization. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 1448\u20131458. PMLR, 13\u201318 Jul 2020.\\n\\nRita Chattopadhyay, Wei Fan, Ian Davidson, Sethuraman Panchanathan, and Jieping Ye. Joint transfer and batch-mode active learning. In International conference on machine learning, pages 253\u2013261. PMLR, 2013.\\n\\nLu Cheng, Ruocheng Guo, Raha Moraffah, Paras Sheth, K Sel\u00e7uk Candan, and Huan Liu. Evaluation methods and measures for causal learning algorithms. IEEE Transactions on Artificial Intelligence, 3(6):924\u2013943, 2022.\\n\\nDavid A Cohn, Zoubin Ghahramani, and Michael I Jordan. Active learning with statistical models. Journal of artificial intelligence research, 4:129\u2013145, 1996.\\n\\nIan C Covert, Scott Lundberg, and Su-In Lee. Explaining by removing: A unified framework for model explanation. The Journal of Machine Learning Research, 22(1):9477\u20139566, 2021.\\n\\nElliot Creager, Joern-Henrik Jacobsen, and Richard Zemel. Environment Inference for Invariant Learning. In Proceedings of the 38th International Conference on Machine Learning, pages 2189\u20132200. PMLR, 2021.\\n\\nGabriela Csurka. A comprehensive survey on domain adaptation for visual applications. In Gabriela Csurka, editor, Domain Adaptation in Computer Vision Applications, Advances in Computer Vision and Pattern Recognition, pages 1\u201335. Springer, 2017.\\n\\nPeng Cui and Susan Athey. Stable learning establishes some common ground between causal inference and machine learning. Nat. Mach. Intell., 4(2):110\u2013115, 2022.\\n\\nFrances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine learning. Advances in neural information processing systems, 34:6478\u20136490, 2021.\\n\\nDheeru Dua and Casey Graff. UCI machine learning repository, 2017.\\n\\nJohn Duchi, Tatsunori Hashimoto, and Hongseok Namkoong. Distributionally robust losses for latent covariate mixtures. Operations Research, 71(2):649\u2013664, 2023.\\n\\nJohn C Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally robust optimization. The Annals of Statistics, 49(3):1378\u20131406, 2021.\\n\\nChen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In ICCV, pages 1657\u20131664, 2013.\\n\\nAbolfazl Farahani, Sahar V oghoei, Khaled Rasheed, and Hamid R. Arabnia. A brief review of domain adaptation. CoRR, abs/2010.03978, 2020.\\n\\nJosh Gardner, Zoran Popovi\u00b4c, and Ludwig Schmidt. Subgroup Robustness Grows On Trees: An Empirical Baseline Investigation, 2022.\\n\\nAmirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine learning. In International Conference on Machine Learning, pages 2242\u20132251. PMLR, 2019.\"}"}
{"id": "PF0lxayYST", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[33] Amirata Ghorbani, James Zou, and Andre Esteva. Data shapley valuation for efficient batch active learning. In 2022 56th Asilomar Conference on Signals, Systems, and Computers, pages 1456\u20131462. IEEE, 2022.\\n\\n[34] L\u00e9o Grinsztajn, Edouard Oyallon, and Ga\u00ebl Varoquaux. Why do tree-based models still outperform deep learning on tabular data?, 2022.\\n\\n[35] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\\n\\n[36] David J Hand. Classifier technology and the illusion of progress. Statistical Science, 2006.\\n\\n[37] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances in neural information processing systems, 29, 2016.\\n\\n[38] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization, 2021.\\n\\n[39] Miguel A Hern\u00e1n, Sonia Hern\u00e1ndez-D\u00edaz, and James M Robins. A structural approach to selection bias. Epidemiology, pages 615\u2013625, 2004.\\n\\n[40] Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does Distributionally Robust Supervised Learning Give Robust Classifiers? In Proceedings of the 35th International Conference on Machine Learning, pages 2029\u20132037. PMLR, 2018.\\n\\n[41] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation learning: A survey of learning methods. ACM Computing Surveys (CSUR), 50(2):1\u201335, 2017.\\n\\n[42] Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data balancing achieves competitive worst-group-accuracy. In Proceedings of the First Conference on Causal Learning and Reasoning, pages 336\u2013351. PMLR, 2022.\\n\\n[43] Bertrand Iooss and Paul Lema\u00eetre. A review on global sensitivity analysis methods. Uncertainty management in simulation-optimization of complex systems: algorithms and applications, pages 101\u2013122, 2015.\\n\\n[44] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nick Hynes, Nezihe Merve G\u00fcrel, Bo Li, Ce Zhang, Dawn Song, and Costas J Spanos. Towards efficient data valuation based on the shapley value. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1167\u20131176. PMLR, 2019.\\n\\n[45] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. Scientific data, 3(1):1\u20139, 2016.\\n\\n[46] Jean Kaddour, Aengus Lynch, Qi Liu, Matt J Kusner, and Ricardo Silva. Causal machine learning: A survey and open problems. arXiv preprint arXiv:2206.15475, 2022.\\n\\n[47] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M. Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A Benchmark of in-the-Wild Distribution Shifts. In Proceedings of the 38th International Conference on Machine Learning, pages 5637\u20135664. PMLR, 2021.\\n\\n[48] Ksenia Konyushkova, Raphael Sznitman, and Pascal Fua. Learning active learning from data. Advances in neural information processing systems, 30, 2017.\\n\\n[49] Masanori Koyama and Shoichiro Yamaguchi. When is invariance useful in an out-of-distribution generalization problem ?, 2021.\"}"}
{"id": "PF0lxayYST", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 5815\u20135826. PMLR, 18\u201324 Jul 2021.\\n\\nKun Kuang, Ruoxuan Xiong, Peng Cui, Susan Athey, and Bo Li. Stable prediction with model misspecification and agnostic distribution shift. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 4485\u20134492, 2020.\\n\\nSean Kulinski and David I. Inouye. Towards explaining distribution shifts. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 17931\u201317952. PMLR, 23\u201329 Jul 2023.\\n\\nDaniel Levy, Yair Carmon, John C Duchi, and Aaron Sidford. Large-scale methods for distributionally robust optimization. Advances in Neural Information Processing Systems, 33:8847\u20138860, 2020.\\n\\nDa Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In ICCV, pages 5542\u20135550, 2017.\\n\\nRichard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E Gonzalez, and Ion Stoica. Tune: A research platform for distributed model selection and training. arXiv preprint arXiv:1807.05118, 2018.\\n\\nJustin Lim, Christina X. Ji, Michael Oberst, Saul Blecker, Leora Horwitz, and David Sontag. Finding Regions of Heterogeneity in Decision-Making via Expected Conditional Covariance. https://arxiv.org/abs/2110.14508v1, 2021.\\n\\nEvan Z. Liu, Behzad Haghgoo, Annie S. Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just Train Twice: Improving Group Robustness without Training Group Information. In Proceedings of the 38th International Conference on Machine Learning, pages 6781\u20136792. PMLR, 2021.\\n\\nJiashuo Liu, Zheyuan Hu, Peng Cui, Bo Li, and Zheyan Shen. Heterogeneous risk minimization. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 6804\u20136814. PMLR, 2021.\\n\\nZhuoming Liu, Hao Ding, Huaping Zhong, Weijia Li, Jifeng Dai, and Conghui He. Influence selection for active learning. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 9254\u20139263. IEEE, 2021.\\n\\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in neural information processing systems, 30, 2017.\\n\\nRafid Mahmood, Sanja Fidler, and Marc T. Law. Low-budget active learning via wasserstein distance: An integer programming approach. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.\\n\\nJohn Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt. The effect of natural distribution shift on question answering models. In International Conference on Machine Learning, pages 6905\u20136916. PMLR, 2020.\\n\\nJohn P. Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization. In Proceedings of the 38th International Conference on Machine Learning, pages 7721\u20137735. PMLR, 2021.\\n\\nSobhan Moosavi, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath. A countrywide traffic accident dataset. CoRR, abs/1906.05409, 2019.\"}"}
{"id": "PF0lxayYST", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "PF0lxayYST", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization. arXiv:1911.08731, 2020.\\n\\nRoshni Sahoo, Lihua Lei, and Stefan Wager. Learning from a biased sample. arXiv preprint arXiv:2209.01754, 2022.\\n\\nShibani Santurkar, Dimitris Tsipras, and Aleksander Madry. Breeds: Benchmarks for subpopulation shift. In International Conference on Learning Representations, 2021.\\n\\nBernhard Sch\u00f6lkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris M. Mooij. On causal and anticausal learning. In Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress, 2012.\\n\\nBernhard Sch\u00f6lkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proc. IEEE, 109(5):612\u2013634, 2021.\\n\\nBurr Settles. Active learning literature survey. 2009.\\n\\nSoroosh Shafieezadeh-Abadeh, Daniel Kuhn, and Peyman Mohajerin Esfahani. Regularization via mass transportation. Journal of Machine Learning Research, 20(103):1\u201368, 2019.\\n\\nVaishaal Shankar, Achal Dave, Rebecca Roelofs, Deva Ramanan, Benjamin Recht, and Ludwig Schmidt. Do image classifiers generalize across time? In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9661\u20139669, 2021.\\n\\nJie-Jing Shao, Yunlu Xu, Zhanzhan Cheng, and Yu-Feng Li. Active model adaptation under unknown shift. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1558\u20131566, 2022.\\n\\nJong-Chyi Su, Yi-Hsuan Tsai, Kihyuk Sohn, Buyu Liu, Subhransu Maji, and Manmohan Chandraker. Active adversarial domain adaptation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 739\u2013748, 2020.\\n\\nBaochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.\\n\\nEric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7167\u20137176, 2017.\\n\\nHemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In CVPR, pages 5018\u20135027, 2017.\\n\\nQing Wang, Jiechao Xiong, Lei Han, Han Liu, Tong Zhang, et al. Exponentially weighted imitation learning for batched historical data. Advances in Neural Information Processing Systems, 31, 2018.\\n\\nAndrew Wong, Erkin Otles, John P Donnelly, Andrew Krumm, Jeffrey McCullough, Olivia DeTroyer-Cooley, Justin Pestrue, Marie Phillips, Judy Konye, Carleen Penoza, et al. External validation of a widely implemented proprietary sepsis prediction model in hospitalized patients. JAMA Internal Medicine, 181(8):1065\u20131070, 2021.\\n\\nYueh-Hua Wu, Nontawat Charoenphakdee, Han Bao, Voot Tangkaratt, and Masashi Sugiyama. Imitation learning from imperfect demonstration. In International Conference on Machine Learning, pages 6818\u20136827. PMLR, 2019.\\n\\nLaure Wynants, Ben Van Calster, Gary S Collins, Richard D Riley, Georg Heinze, Ewoud Schuit, Elena Albu, Banafsheh Arshi, Vanesa Bellou, Marc MJ Bonten, et al. Prediction models for diagnosis and prognosis of covid-19: systematic review and critical appraisal. bmj, 369, 2020.\"}"}
{"id": "PF0lxayYST", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Binhui Xie, Longhui Yuan, Shuang Li, Chi Harold Liu, and Xinjing Cheng. Towards fewer annotations: Active learning via region impurity and prediction uncertainty for domain adaptive semantic segmentation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 8058\u20138068. IEEE, 2022.\\n\\nLinyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang. Glue-x: Evaluating natural language understanding models from an out-of-distribution generalization perspective, 2023.\\n\\nYuzhe Yang, Haoran Zhang, Dina Katabi, and Marzyeh Ghassemi. Change is hard: A closer look at subpopulation shift. CoRR, abs/2302.12254, 2023.\\n\\nYuzhe Yang, Haoran Zhang, Dina Katabi, and Marzyeh Ghassemi. Change is Hard: A Closer Look at Subpopulation Shift, 2023.\\n\\nHuaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang Wei Koh, and Chelsea Finn. Wild-time: A benchmark of in-the-wild distribution shift over time. In NeurIPS, 2022.\\n\\nHuaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang Wei W Koh, and Chelsea Finn. Wild-time: A benchmark of in-the-wild distribution shift over time. Advances in Neural Information Processing Systems, 35:10309\u201310324, 2022.\\n\\nHan Yu, Peng Cui, Yue He, Zheyan Shen, Yong Lin, Renzhe Xu, and Xingxuan Zhang. Stable learning via sparse variable independence. AAAI, abs/2212.00992, 2022.\\n\\nJohn R Zech, Marcus A Badgeley, Manway Liu, Anthony B Costa, Joseph J Titano, and Eric Karl Oermann. Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study. PLoS medicine, 15(11):e1002683, 2018.\\n\\nRich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In International conference on machine learning, pages 325\u2013333. PMLR, 2013.\\n\\nRuntian Zhai, Chen Dan, Zico Kolter, and Pradeep Ravikumar. DORO: Distributional and Outlier Robust Optimization. In Proceedings of the 38th International Conference on Machine Learning, pages 12345\u201312355. PMLR, 2021.\\n\\nHaoran Zhang, Harvineet Singh, Marzyeh Ghassemi, and Shalmali Joshi. \u201cWhy did the Model Fail?\u201d: Attributing Model Performance Changes to Distribution Shifts, 2023.\\n\\nXingxuan Zhang, Yue He, Tan Wang, Jiaxin Qi, Han Yu, Zimu Wang, Jie Peng, Renzhe Xu, Zheyan Shen, Yulei Niu, et al. Nico challenge: Out-of-distribution generalization for image recognition challenges. In Computer Vision\u2013ECCV 2022 Workshops: Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part VI, pages 433\u2013450. Springer, 2023.\\n\\nSicheng Zhao, Bo Li, Xiangyu Yue, Yang Gu, Pengfei Xu, Runbo Hu, Hua Chai, and Kurt Keutzer. Multi-source domain adaptation for semantic segmentation. arXiv preprint arXiv:1910.12181, 2019.\"}"}
{"id": "PF0lxayYST", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendices\\n\\nA Relevant Work\\n\\nA.1 On Distribution Shifts\\n\\nDistribution Shift Benchmarks. Existing distribution shift benchmarks primarily concentrate on image and language datasets [74, 47, 107, 81, 101] to assess the robustness and efficiency of algorithms in real applications. We briefly review benchmarks that address distribution shifts across various data types, including image and language data. For image data, several datasets capture natural distribution shifts, such as spatial and temporal variations. PACS [54] and Office-Home [91] categorize environments to be image styles. VLCS [29] and iWildCam [10] set their primary environments as data sources. DomainNet [108] is built on PACS and provides a more extensive selection with additional domains and categories. In recent developments, Koh et al. [47] collect several datasets to establish WILDS, setting a new structure for OOD generalization. Similarly, Yao et al. [100] introduced Wild-Time, highlighting temporal distribution shifts across diverse real-world scenarios. For language data, CivilComments [14] and Amazon [66] consist of individual comments collected from different users and distinctive groups (e.g., male and female). GLUE-X [97] provides a unified benchmark for evaluating OOD robustness in NLP models. For other types of data, OGB-MolPCBA [75] collects...\"}"}
{"id": "PF0lxayYST", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"molecular graphs in over 100,000 scaffolds and formulates a molecular property prediction task across different scaffolds. Towards auto-engineering, Py150 [73] contains codes from 8,421 git repositories for code completion generalization. However, these datasets/benchmarks do not specify or investigate in-depth the distribution shift patterns, and there is a noticeable lack of benchmark papers that specifically address real-world tabular datasets. These tabular datasets often present distinct patterns compared to image/language datasets, including prevalent methods [34] and shift patterns. Therefore, it is important to understand detailed distribution patterns among these datasets to develop corresponding methodologies to address specific shifts, which is also observed in a recent benchmark [101]. Yang et al. [99] recently introduce one benchmark characterizing different patterns of subpopulation shift. This benchmark focuses on the relationship between a specific attribute and covariates, such as spurious correlation, which can be challenging to identify in tabular datasets. Besides, they focus on changes in the subpopulation, which is a subset of distribution shift patterns. For example, individuals in different states may suffer from little subpopulation shifts but still incur a large distribution shift. Individuals' demographic features are similar but the income level differs greatly between CA and PR in ACS Income, as demonstrated in Table 2. Besides, Kulinski and Inouye [52] use optimal transport to explain the shift between two distributions recently. But their method could not explicitly decompose the performance degradation as in our work. Motivated by the challenges above, our work hopes to fill in the gap by demonstrating one benchmark on tabular datasets with detailed analysis.\\n\\n### Table 2: Descriptive Characteristics under different states in ACS income dataset\\n\\n| State CA | MA | NY | NE | MT | SD | PR |\\n|----------|----|----|----|----|----|----|\\n| Total Size | 195665 | 40114 | 103021 | 10785 | 5463 | 4899 | 9071 |\\n| White Man | 0.3311 | 0.4161 | 0.3687 | 0.4822 | 0.4959 | 0.4691 | 0.3526 |\\n| Nonwhite Man | 0.1969 | 0.0878 | 0.1379 | 0.0454 | 0.0428 | 0.0492 | 0.1818 |\\n| White Woman | 0.2873 | 0.4066 | 0.3464 | 0.4318 | 0.4221 | 0.4297 | 0.3124 |\\n| NonWhite Woman | 0.1847 | 0.0895 | 0.1471 | 0.0404 | 0.0392 | 0.0521 | 0.1532 |\\n| P(\\\\(Y=0\\\\)) | 0.589 | 0.532 | 0.585 | 0.688 | 0.707 | 0.730 | 0.894 |\\n\\n### Choice of Domains in Distribution Shifts\\n\\nAlthough it is popular in the fairness literature to set each demographic subgroup in the adult and ACS dataset as one domain [104, 22, 25], the relative regret of models trained on these domains is quite small shown in Figure 1 empirically. This implicitly shows that \\\\(Y|X\\\\)-shifts are not strong across different demographic subgroups. In contrast, the model usually experiences relatively large \\\\(Y|X\\\\)-shifts under different spatial domains, which corresponds to the right-hand side of Figure 1. Besides, from the practical perspective, machine learning models often need to be deployed in spatial-temporal domains (by city, state, and country-wise; by year) only with few observed data while the model is trained with abundant data from the source domain. This is why we mainly choose spatiotemporal domains to benchmark distribution shifts in our case.\\n\\n### A.2 Connection between Non-algorithmic Interventions and Algorithmic Interventions\\n\\nHere we discuss the connections between our non-algorithmic interventions and several branches of methods addressing the distribution shifts, including active learning, imitation learning, causal learning, feature sensitivity analysis, and feature importance analysis.\\n\\n### Active Learning\\n\\nActive learning aims to improve model performance by acquiring a limited number of labels from the target distribution. See the survey [84] for a detailed reference. The challenge here is to quantify the value of unlabeled data so that we can select samples better. The selection criterion of existing approaches includes estimated variance [20], influence on the model performance [59], and Shapley value [32, 33, 44], or source-target distance metrics through the importance weighting [18, 61]. Some work also proposed running a regression problem for the query procedure based on learning strategies from the existing dataset [48]. Specifically, the target distribution where we query data and aim to evaluate the model subsequently differs from the source...\"}"}
{"id": "PF0lxayYST", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Some work also develops active learning methods on domain adaptation to select additional samples from the target distribution [88, 87]. However, these works usually assume some restricted distribution structures between the source and target distribution such as only $X$-shifts occur, which may not hold in the tabular data. In contrast, we assume we have a few labeled target data but impose no restrictions on the two distributions. And we sample data in specific regions after identifying regions where we experience the largest $Y|X$-shifts.\\n\\n**Imitation Learning**\\n\\nImitation learning aims to mimic the behavior of the expert with some off-policy data. This usually occurs in the reinforcement learning setup where a learner aims to learn the best action given each state [41]. Here, distribution shifts occur due to the mismatch of the state coverage between the observed data (source domain) and the environment to deploy (target domain). This raises the need for the importance sampling method to match the two distributions in imitation learning [92, 94]. In fact, the underlying conditional distribution does not change only with covariate (state) shifts in the imitation learning setup. Therefore, this does not fit into our case where the conditional distribution between the source and target domains differ.\\n\\n**Causal Learning**\\n\\nCausal learning methods receive much attention in the field of machine learning. The core idea of causal learning [72, 8] is to learn causally invariant relationships across multiple pre-defined training environments. Arjovsky et al. [8] propose Invariant Risk Minimization (IRM) to learn invariant representations across environments, and follow-up works [17, 49, 5, 50] propose variations with similar invariant regularizations. However, the effectiveness of these methods are challenged both theoretically and empirically. Theoretically, Rosenfeld et al. [78] illustrate that IRM could fail in a nonlinear context, and Liu et al. [58] demonstrate that the learned invariance property largely depends on the quality of pre-defined environments. Empirically, Gulrajani and Lopez-Paz [35] show that when carefully implemented and tuned, ERM still outperforms the state-of-the-art methods in terms of average performance.\\n\\nCompared with the simple non-algorithmic interventions in this work, causal learning methods rely heavily on the invariance assumptions and have strict requirements on the quality of multi-environment data. This restricts their applicability in practice, since modern datasets are often collected without explicit environment labels, and in many scenarios, it is quite hard to pre-define meaningful environment labels. Our proposed non-algorithmic interventions (collecting features and data) do not rely on the invariance assumption, and it could serve as a \\\"solution\\\" when observing performance degradation, which helps to analyze the model failure and to direct further improvements. And we hope that these simple data-centric interventions could inspire future research in this direction to mitigate the effects of distribution shifts.\\n\\nSince we are also considering other missing features to improve the performance under distribution shifts, there are some works providing insights on the feature / region importance to the final output.\\n\\n**Feature Analysis: Sensitivity and Importance.**\\n\\nThere are two streams of literature measuring the relationship between input features and the response variable. These are feature sensitivity analysis and feature importance. Feature sensitivity analysis aims to quantify the sensitivity of the performance metrics to each input. This helps understand the impact of variations in input features on the output of a simulation system. Classical metrics include ANOVA, Sobol indices [68] and Morris methods [43]. Meanwhile, feature importance aims to understand the performance decomposition of different algorithms. Shapley value-based approaches gain the most popularity in understanding the attribution of predictions to each input feature [60, 21]. Some work leverages these ideas to understand the difference in distribution shifts to each existing input feature in the data [15, 106]. They decompose the shift on the joint distribution to particular $X$-shifts or the condition $X_j|X_i$-shifts ($X_i$ and $X_j$ denote different features) under a known causal graph. In contrast, we investigate the additional missing feature beyond existing datasets and aim to reduce the performance drop under distribution shifts in our paper. Specifically, we focus on the local regions where the distribution incurs the largest shifts and add the feature ENG in our main body due to our prior knowledge that \\\"ENG\\\" feature would yield the largest difference in that subpopulation between two distributions. Our region-based\"}"}
{"id": "PF0lxayYST", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"approach, in fact, can be further rigorously extended to investigate the marginal distribution of what features would yield the largest difference in that region between two distributions. We hope this can inspire researchers to apply refined tools in this line such as Shapley value to help understand and mitigate distribution shifts.\\n\\nRegion Analysis: Attribution of Distribution Shifts.\\n\\nSimilar to our algorithmic goal in Section 3.1 of identifying regions where model learners are different, Oberst et al. [67] and Lim et al. [56] developed specific methods to identify specific covariate regions where model learners are different. The difference between their setup and ours lies in the sampling and assumption of observed data. In their approach, the observed data is sampled i.i.d. across various prediction models, without any distribution shift. Besides, they can observe only one selected prediction result per sample from all the model learners. However, in our case study, the models are built on datasets from two domains that experience distribution shifts. As a result, we can further isolate the model difference based on the shared input space $X$ and differentiate it from the total difference, specifically focusing on the $Y|X$-shifts.\\n\\nB Discussion on Limitations\\n\\nWe discuss some limitations of our work.\\n\\nFor Benchmark\\n\\nFirst, we only consider the source-target transfer pairs from datasets including the economic and transportation domain, and we leave the detailed pattern evaluation of these datasets in other domains such as the medical area (e.g. MIMIC-III [45]) as an interesting direction of the future work. Besides, we only consider the source and target from one fixed domain (i.e. one state or city). In practice, it is reasonable to extend this benchmark to consider the source and target distribution with multiple domains of varying proportions. Our results of characterizing the distribution shift patterns highlight the importance of utilizing other refined tools. These tools can help us understand the difference between real-world distribution shifts and enable further investigation and analysis.\\n\\nFor Algorithm 1\\n\\nOur Algorithm 1 is proposed to identify risky regions with large $Y|X$-shifts when observing severe model degradation between the source target distributions. Therefore, one limitation is that it requires target data to identify the risky regions with large $Y|X$-shifts. This algorithm cannot evaluate the generalization performance when the target distribution is completely unknown. Furthermore, when conducting non-algorithmic interventions based on the risky regions learned by Algorithm 1, researchers should be careful and incorporate more background knowledge to find the proper way. For example, in Section 3.2, we analyze the risky regions and find that the \u201cENG\u201d feature may be important to mitigate the $Y|X$-shifts. This is from our prior knowledge that the official language between the two states is different. Also, when Algorithm 1 is misused (e.g., the learned risky regions are used without destination or not being checked carefully), it might harm some vulnerable groups.\\n\\nFor Data-Collection\\n\\nIn Section 3.2, we propose two simple non-algorithmic interventions to mitigate the $Y|X$-shifts, one of which is to efficiently collect target data from the risky region. It achieves much better results combined with several typical methods on our benchmark. However, in practice, we acknowledge that this technique could only be used when we can obtain data from the target distribution and the data collection procedure does not raise any privacy concerns and predatory inclusion.\\n\\nApplicability Across Different Data Modalities\\n\\nIn this work, we focus on real-world distribution shifts in tabular data settings. We propose Algorithm 1 to find the risk regions and come up with some simple data-based interventions. Therefore, we do not investigate in-depth the applicability of the proposed methods across other data types. However, we argue that our proposed Algorithm 1 could generalize to complicated data types (e.g., image data) with corresponding deep models.\"}"}
{"id": "PF0lxayYST", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Specifically, both the domain classifier (to estimate $P(X \\\\mid Q)$) and region learner $h(X)$ should be replaced by deep neural networks. And our data-based interventions also have the potential to incorporate with more complicated data types. We leave this investigation to future work.\\n\\nC Case Study Details\\n\\nIn this section, we provide more details about our case study in the main body.\\n\\nC.1 DISDE to $X$-shifts and $Y \\\\mid X$-shifts\\n\\nWhen facing performance degradation under distribution shifts, one direct idea is to figure out the reasons why the performance drop. To this end, Cai et al. [16] propose DIstribution Shift DEcomposition (DISDE) to attribute the total performance degradation to $Y \\\\mid X$-shifts and $X$-shifts. Specifically, given samples $(X,Y)$ from distributions $P$ and $Q$, to quantify the discrepancy between $P_{Y \\\\mid X}$ and $Q_{Y \\\\mid X}$, they first control the marginal distribution on $X$ by introducing the shared distribution $S_X$.\\n\\nFrom that, we can estimate the performance degradation caused by $Y \\\\mid X$-shifts and that caused by $X$-shifts could also be estimated by comparing $S_X$ with $P_X$ and $Q_X$, respectively. Note that DISDE could be used in image datasets (see Section 4.2 in [16]). The official code for DISDE could be found at https://github.com/namkoong-lab/disde. We specify the formula of DISDE as follows:\\n\\n$$E_Q[\\\\ell(f_P(X), Y)] - E_P[\\\\ell(f_P(X), Y)] = E_{S_X}[R_Q(X)] - E_P[R_P(X)] \\\\quad (I)$$\\n$$+ E_{S_X}[R_Q(X) - R_P(X)] \\\\quad (II)$$\\n$$+ E_Q[R_Q(X) - E_{S_X}[R_Q(X)]] \\\\quad (III)$$\\n\\nwhere $R_\\\\mu(X) := E_\\\\mu[\\\\ell(f_P(X), Y) \\\\mid X = x]$ for $\\\\mu = P, Q$ is denoted as the conditional risks on $P$ and $Q$.\\n\\n$S_X$ is the share distribution with support contained in both $P_X$ and $Q_X$. Then we see the sum of the two terms (I) and (III) as the performance drop attributed to $X$-shifts and the term (II) as the performance drop attributed to $Y \\\\mid X$-shifts.\\n\\nBesides, we also implement DISDE in our released package named WHYSHIFT, which could be found at https://github.com/namkoong-lab/whyshift.\\n\\nC.2 Prevalence of $Y \\\\mid X$-Shifts\\n\\nFor each of the 7 settings in Table 1, we only select one target distribution in the main body, while our benchmark supports multiple target distributions. Specifically, for ACS Income, ACS Mobility, ACS Pub.Cov, we have 50 target distributions (the other 50 American states) in total for each setting; for Taxi, we have 3 target distributions (other cities Mexico City, Bogot\u00e1, Quitio); for the temporal shift version of ACS Pub.Cov (i.e. setting 6), we have 3 target distributions (i.e. the year 2014, 2017, and 2021); and for US Accident, we have 13 target distributions (13 American states). Therefore, we have 169 source-target transfer pairs in total for these 7 settings.\\n\\nWe use XGBoost classifier and calculate the decomposition of performance degradation via DISDE [16]. We calculate the $Y \\\\mid X$-shift ratio from the source distribution $P$ to the target distribution $Q$:\\n\\n$$Y \\\\mid X \\\\text{-shift ratio} = \\\\frac{E_{S_X}[R_Q(X) - R_P(X)]}{E_Q[\\\\ell(f_P(X), Y)] - E_P[\\\\ell(f_P(X), Y)]}$$\\n\\nwhere $R_\\\\mu(X)$ is defined in Appendix C.1. We first focus on pairs with relatively strong distribution shifts (i.e. performance degradation larger than 8 percentage points). Across these pairs, we find that 70.2% pairs have over 60% $Y \\\\mid X$-shifts, and 87.2% pairs have over 50% $Y \\\\mid X$-shifts, indicating the prevalence of $Y \\\\mid X$ shifts. To visualize the result better, in all 22 settings in our benchmark (as shown in Table 3), we focus on transfer pairs with performance degradation larger than 5 percentage points and plot the histogram of the ratio of $Y \\\\mid X$-shifts in Figure 8. From Figure 8, we could see that $Y \\\\mid X$-shifts are prevalent in real-world distribution shifts.\\n\\nC.3 Details of Algorithm 1\\n\\nIn this section, we provide a more detailed introduction of our proposed Algorithm 1. We propose a simple yet efficient method to identify data regions with strong $Y \\\\mid X$ shifts, and it could inspire operational and modeling interventions as shown in Section 3.2.\"}"}
{"id": "PF0lxayYST", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Consider a model $f: X \\\\rightarrow Y$ that predicts outcome $Y$ from covariates $X$. Let $\\\\ell(f(x), y)$ be a loss function denoting a notion of predictive error (e.g., cross-entropy loss, mean-square error). Our goal is to identify a region $S \\\\subseteq X$ where $P(Y|X)$ differs a lot from $Q(Y|X)$. In order to directly compare $P(Y|X)$ and $Q(Y|X)$, we must do an apples-to-apples comparison: we cannot know $Q(Y|X)$ for $X$'s that are primarily observed in $P(X)$ (and vice versa).\\n\\nTo address this, we first construct a shared distribution $S_X$ over $X$ whose support is contained in that of both $P_X$ and $Q_X$. We choose a specific shared distribution $S_X$ over $X$ whose support is contained in that of $P_X$ and $Q_X$ (following [16]). Ideally, the chosen shared distribution would exhibit a higher density when both $P_X$ and $Q_X$ densities are high, and a lower density when either of the two possesses a low density. This strategy effectively allows regions of shared density to be more pronounced. Recall that $p_X, q_X, s_X$ are the densities of $X$ under $P, Q$ and $S$, we formulate $s_X$ as $s_X(x) \\\\propto p_X(x)\\\\frac{q_X(x)}{p_X(x) + q_X(x)}$. (C.1)\\n\\n**Choices of $S_X$**\\n\\nHere we would like to discuss different choices of $S_X$ to provide more intuitions. As demonstrated in [16], we can use many forms of the shared distribution. For example, we could choose the following form: $s_X(x) \\\\propto \\\\min\\\\{p_X(x), q_X(x)\\\\}$, (C.2) which guarantees that the support of $S_X$ is contained in that of both $P_X$ and $Q_X$. Another choice is: $s_X(x) \\\\propto \\\\begin{cases} p_X(x) + q_X(x) & \\\\text{if } \\\\min\\\\{p_X(x), q_X(x)\\\\} \\\\geq \\\\epsilon, \\\\\\\\ 0 & \\\\text{otherwise,} \\\\end{cases}$ (C.3) for some $\\\\epsilon \\\\geq 0$. This form of $S_X$ defines shared samples as those with high likelihood ratios. Notably, for all the three forms of $S_X$, if $P_X = Q_X$, then $S_X = P_X = Q_X$. And when $p_X(x) \\\\gg q_X(x)$ or $p_X(x) \\\\ll q_X(x)$, Equation (C.1) and Equation (C.2) become similar. In our Algorithm 1, we use the form of Equation (C.1), and Cai et al. [16] observe that in practice the qualitative conclusions are not very sensitive to the specific choice of shared distribution.\\n\\n**Intuitions behind Algorithm 1**\\n\\nSince we do not have access to samples from the shared distribution $S_X$, we reweight samples from $P_X$ and $Q_X$ using the likelihood ratios: $s_X p_X(x) \\\\propto q_X(x)p_X(x) + q_X(x)$ and $s_X q_X(x) \\\\propto p_X(x)p_X(x) + q_X(x)$. (C.4) Then we define $\\\\hat{\\\\alpha}$ as the proportion of the pooled data that comes from distribution $Q$: $\\\\hat{\\\\alpha} = \\\\frac{n_Q}{n_P + n_Q}$ and $\\\\hat{\\\\pi}(x) = P(\\\\tilde{X} \\\\text{ from } Q_X | \\\\tilde{X} = x)$, (C.5)\"}"}
{"id": "PF0lxayYST", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where \\\\( \\\\hat{\\\\pi}(x) \\\\) denotes the probability of a sample to come from \\\\( Q \\\\). Using Bayes' rule, we have:\\n\\n\\\\[\\n\\\\hat{\\\\pi}(x) = \\\\frac{P(\\\\hat{X} = x | \\\\hat{X} \\\\text{ from } Q)}{P(\\\\hat{X} \\\\text{ from } Q)} \\\\frac{P(\\\\hat{X} \\\\text{ from } Q)}{P(x)} = \\\\frac{\\\\hat{\\\\alpha} q(x)}{\\\\hat{\\\\alpha} q(x) + (1 - \\\\hat{\\\\alpha}) p(x)},\\n\\\\]\\n\\n\\\\( (C.6) \\\\)\\n\\nNoting that the ratio \\\\( \\\\hat{\\\\pi}(x) \\\\) can be modeled as the probability that an input \\\\( x \\\\) came from \\\\( P \\\\) vs \\\\( Q \\\\), we train a binary \\\"domain\\\" classifier to estimate the ratios. (The \\\"domain\\\" classifier can be any black-box method, and we use XGBoost throughout.)\\n\\nThen the likelihood ratios that we care about could be reformulated as:\\n\\n\\\\[\\ns_X p_X(x) \\\\propto \\\\frac{1}{p_X(x) q_X(x)} + 1\\\\]\\n\\n\\\\( s_X q_X(x) \\\\propto p_X(x) q_X(x) p_X(x) q_X(x) + 1\\\\)\\n\\n\\\\( (C.7) \\\\)\\n\\nwhich gives that:\\n\\n\\\\[\\ns_X p_X(x) \\\\propto \\\\hat{\\\\pi}(x) (1 - \\\\hat{\\\\alpha}) \\\\hat{\\\\pi}(x) + \\\\hat{\\\\alpha} (1 - \\\\hat{\\\\pi}(x))\\\\]\\n\\n\\\\( s_X q_X(x) \\\\propto (1 - \\\\hat{\\\\pi}(x)) \\\\hat{\\\\pi}(x) + \\\\hat{\\\\alpha} (1 - \\\\hat{\\\\pi}(x))\\\\)\\n\\n\\\\( (C.8) \\\\)\\n\\nAfter obtaining the likelihood ratios \\\\( s_X p_X(x) \\\\) and \\\\( s_X q_X(x) \\\\), we could do an apples-to-apples comparison:\\n\\nwe estimate \\\\( P(Y|X) \\\\text{ and } Q(Y|X) \\\\) over the shared distribution \\\\( S \\\\) (using XGBoost)\\n\\n\\\\[\\nf_P = \\\\arg\\\\min_{f \\\\in F} \\\\{ E_S \\\\left[ E_P [\\\\ell(f(X),Y) | X] \\\\right] \\\\} = E_P [\\\\ell(f(X),Y) dS \\\\text{ } dP (X)]\\\\]\\n\\n\\\\( (C.9) \\\\)\\n\\n\\\\[\\nf_Q = \\\\arg\\\\min_{f \\\\in F} \\\\{ E_S \\\\left[ E_Q [\\\\ell(f(X),Y) | X] \\\\right] \\\\} = E_Q [\\\\ell(f(X),Y) dS \\\\text{ } dQ (X)]\\\\]\\n\\n\\\\( (C.10) \\\\)\\n\\nThen, for any threshold \\\\( b \\\\in [0,1] \\\\), \\\\( \\\\{x \\\\in X: |f_P(x) - f_Q(x)| \\\\geq b\\\\} \\\\) suggests a region that may suffer model performance degradation due to \\\\( Y|X \\\\)-shifts.\\n\\nC.4 Analysis of Decision Tree\\n\\nIn Algorithm 1, we use a shallow decision tree \\\\( h(x) \\\\) to approximate \\\\( y = |f_P(x) - f_Q(x)| \\\\) on the shared distribution \\\\( S \\\\) to find the covariate region with highest discrepancy. In our decision tree, we use the squared error as the splitting criterion. And below we demonstrate that this criterion is equivalent to maximizing the discrepancy between two children nodes.\\n\\nSuppose there are \\\\( N \\\\) samples with outcomes \\\\( \\\\{y_i\\\\}_{i \\\\in [N]} \\\\) belonging to tree node \\\\( fa \\\\), and these samples are split into two children nodes \\\\( s_1, s_2 \\\\), where the node \\\\( s_1, s_2 \\\\) denote the set of sample indices in the two children nodes respectively. The squared error criterion to split \\\\( fa \\\\) into \\\\( s_1 \\\\) and \\\\( s_2 \\\\) is:\\n\\n\\\\[\\n\\\\min_{s_1, s_2} \\\\{ L(s_1, s_2) := \\\\frac{1}{N} \\\\sum_{i \\\\in s_1} (y_i - \\\\mu_{Y,1})^2 + \\\\sum_{i \\\\in s_2} (y_i - \\\\mu_{Y,2})^2 \\\\}\\\\]\\n\\n\\\\( (C.11) \\\\)\\n\\nwhere \\\\( \\\\mu_{Y,1} := \\\\sum_{i = 1}^{N} y_i 1\\\\{i \\\\in s_1\\\\} / \\\\sum_{i = 1}^{N} 1\\\\{i \\\\in s_1\\\\} \\\\), \\\\( \\\\mu_{Y,2} := \\\\sum_{i = 1}^{N} y_i 1\\\\{i \\\\in s_2\\\\} / \\\\sum_{i = 1}^{N} 1\\\\{i \\\\in s_2\\\\} \\\\)\\n\\n\\\\( (C.12) \\\\)\\n\\ndenote the mean values of the outcome \\\\( Y \\\\) with samples in children nodes \\\\( s_1, s_2 \\\\). Denote the distribution of the outcome \\\\( Y \\\\) follows the empirical distribution over the \\\\( N \\\\) samples \\\\( \\\\{y_i\\\\}_{i \\\\in [N]} \\\\).\\n\\nSimplifying \\\\( (C.11) \\\\), we have:\\n\\n\\\\[\\nL(s_1, s_2) = P(Y \\\\in s_1) \\\\text{Var}_s_1(Y) + P(Y \\\\in s_2) \\\\text{Var}_s_2(Y) = E_S [\\\\text{Var}(Y|S)]\\\\]\\n\\n\\\\( (C.13) \\\\)\\n\\nwhere \\\\( \\\\text{Var}_s(Y) \\\\) denotes the variance of the outcome variable \\\\( Y \\\\) in node \\\\( s \\\\), \\\\( S = \\\\{s_1, s_2\\\\} \\\\) is the variable representing the children nodes. Therefore, given that \\\\( \\\\text{Var}_{fa}(Y) := E_S [\\\\text{Var}(Y|S)] + E_S [\\\\text{Var}(Y|S)] \\\\) is constant, the minimal \\\\( E_S [\\\\text{Var}(Y|S)] \\\\) corresponds with the largest \\\\( \\\\text{Var}_s(E_Y|S) \\\\), which maximizes the discrepancy of the outcome between two children nodes.\"}"}
{"id": "PF0lxayYST", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.5 Details of Non-algorithmic Interventions\\n\\nIn Section 3.2, we propose two potential non-algorithmic interventions to mitigate the performance degradation. In this section, we introduce in detail the intervention of collecting specific data from the target.\\n\\nExperiment Setup\\n\\nWe focus on the income prediction task using the ACS Income dataset. Consider a practical scenario where the training set consists of 20,000 samples from California (CA) and the trained model was deployed in Puerto Rico (PR) in trial. After the trial deployment in PR, we got a small amount of samples from PR with labels and observed performance degradation. Under this setting, we investigate the effect of non-algorithmic interventions.\\n\\nCollect specific data from the target\\n\\nWe first identify the regions with high discrepancy between source and target. Note that the sample size of the target state is small compared to the training samples. Then for typical algorithms like logistic regression (LR), MLP, random forest (RF), LightGBM and XGBoost, we compare the performances of:\\n\\n- original setting (only 20,000 samples from the source);\\n- original setting with \\\\( N \\\\) additional random samples drawn from the whole target state;\\n- original setting with \\\\( N \\\\) additional random samples drawn from the risk region of the target state.\\n\\nIn this experiment, we first select the best configuration of each method according to the i.i.d validation set in the original setting (only samples from CA), and fix it for the other two interventions. We vary \\\\( N \\\\) as 100, 200, 300 and the results are shown in Figure 9. From Figure 9, incorporating data from the risk region leads to a stable improvement on typical algorithms even for small target sample sizes. However, we observe that LightGBM and XGBoost would easily overfit on the target data, and we use random forest under this setup as an alternative. It is worth investigating approaches to find risk regions effectively under small/imbalanced sample sizes in the future. The approach mentioned here is a simple way of non-algorithmic and explainable interventions and we hope it could inspire further research in this direction.\\n\\nC.6 Potential Alternative Approach of Identifying Risk Region\\n\\nIn Algorithm 1, we propose a simple way to identify the risk region to explain the cause of \\\\( Y \\\\mid X \\\\)-shifts. And the identified region could be used to guide the collecting process of target data, which could help to further reduce the effects of performance degradation. However, in practice, when the amount of target samples is quite small, it may be hard to train the \\\\( f_Q \\\\) only with target samples accurately.\"}"}
{"id": "PF0lxayYST", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Therefore, it may be better to propose a method that does not need $f_Q$ to fit $Q(Y|X)$ on the target distribution. Following this idea, we propose an alternative way of Algorithm 1. Since the training data is enough, it is feasible to fit $f_P$ on the shared distribution $S_X$ as:\\n\\n$$f_P := \\\\arg \\\\min_{f \\\\in F} E_P \\\\left[ d_{S_X} d_{P_X} \\\\ell(f(X), Y) \\\\right].$$  \\\\hfill (C.14)\\n\\nThen for $n_Q$ samples from target distribution $Q$, we could use a prediction model $h(x)$ to approximate $|Y - f_P(X)|$ on the shared distribution (by reweighting density ratio $d_{S_X}/d_{P_X}$). Note that this method does not need to train $f_Q$ on target samples, but the quality of the density ratio $d_{S_X}/d_{P_X}$ and the prediction model $h(x)$ still depend on the target samples. We hope the estimation of the ratio and the residuals $|Y - f_P(X)|$ can be less affected by the low sample size $n_Q$. This non-algorithmic intervention is not the main focus of this work, but we hope this idea could help to promote this line of research.\\n\\n### D Benchmark Details\\n\\nIn this section, we provide the details of our benchmark. In our benchmark, we explore distribution shifts on 5 real-world tabular datasets from the economic and traffic sectors with natural spatiotemporal distribution shifts. We carefully design 22 settings utilizing these 5 datasets, and the overview of 22 settings is shown in Table 3. Note that in our main body, due to space limitations, we only pick 7 typical settings and select only one representative target domain for each setting, as shown in Table 1. Detailed introductions of all datasets, algorithms, and experiment settings will be given in the following sections. A Python package for our benchmark can be found at https://github.com/namkoong-lab/whyshift.\\n\\n| #ID | Dataset Type | #Features | Outcome | Source | #Train Samples | #Test Domains | Dom. Ratio |\\n|-----|--------------|-----------|---------|--------|---------------|--------------|------------|\\n| 1   | ACS Income  | Spatial   | Income \u2265 50k | California | 195,665        | 50           | $Y|X$: 13/14 |\\n| 2   | ACS Income  | Spatial   | Income \u2265 50k | Connecticut | 19,785        | 50           | $Y|X$: 24/24 |\\n| 3   | ACS Income  | Spatial   | Income \u2265 50k | Massachusetts | 40,114       | 50           | $Y|X$: 21/22 |\\n| 4   | ACS Income  | Spatial   | Income \u2265 50k | South Dakota | 4,899         | 50           | $Y|X$: 9/9    |\\n| 5   | ACS Mobility | Spatial   | Residential Address | Mississippi | 5,318         | 50           | $Y|X$: 28/34 |\\n| 6   | ACS Mobility | Spatial   | Residential Address | New York | 40,463        | 50           | $Y|X$: 30/31 |\\n| 7   | ACS Mobility | Spatial   | Residential Address | California | 80,329        | 50           | $Y|X$: 9/17  |\\n| 8   | ACS Mobility | Spatial   | Residential Address | Pennsylvania | 23,918       | 50           | $Y|X$: 17/17 |\\n| 9   | Taxi        | Spatial   | Duration time \u2265 30 min | Bogot\u00e1 | 3,063         | 3            | $Y|X$: 1/21  |\\n| 10  | Taxi        | Spatial   | Duration time \u2265 30 min | New York City | 1,458,646   | 3            | $Y|X$: 3/31  |\\n| 11  | ACS Pub.Cov | Spatial   | Public Ins. Coverage | Nebraska | 6,332         | 50           | $Y|X$: 32/39 |\\n| 12  | ACS Pub.Cov | Spatial   | Public Ins. Coverage | Florida | 71,297        | 50           | $Y|X$: 28/29 |\\n| 13  | ACS Pub.Cov | Spatial   | Public Ins. Coverage | Texas | 98,928        | 50           | $Y|X$: 33/34 |\\n| 14  | ACS Pub.Cov | Spatial   | Public Ins. Coverage | Indiana | 24,330        | 50           | $Y|X$: 11/13 |\\n| 15  | ACS Pub.Cov | Temporal  | Public Ins. Coverage | Year 2010 (NY) | 73,208       | 3            | $X$: 2/2   |\\n| 16  | ACS Pub.Cov | Temporal  | Public Ins. Coverage | Year 2010 (CA) | 149,441      | 3            | $X$: 2/2   |\\n| 17  | ACS Income  | Synthetic | Income \u2265 50k | Younger People (80%) | 20,000       | 1            | $X$: 1/1   |\\n| 18  | ACS Income  | Synthetic | Income \u2265 50k | Younger People (90%) | 20,000       | 1            | $X$: 1/1   |\\n\\n| #ID | Dataset Type | #Features | Outcome | Source | #Train Samples | #Test Domains | Dom. Ratio |\\n|-----|--------------|-----------|---------|--------|---------------|--------------|------------|\\n| 19  | US Accident | Spatial   | Severity of Accident | Texas | 26,664        | 13           | $Y|X$: 7/7   |\\n| 20  | US Accident | Spatial   | Severity of Accident | California | 64,909       | 13           | $X$: 22/31 |\\n| 21  | US Accident | Spatial   | Severity of Accident | Florida | 32,278       | 13           | $X$: 5/7   |\\n| 22  | US Accident | Spatial   | Severity of Accident | Minnesota | 8,927        | 13           | $X$: 8/11  |\\n| 23  | ACS Pub.Cov | Temporal  | Public Ins. Coverage | Year 2010 (NY) | 73,208       | 3            | $X$: 2/2   |\\n| 24  | ACS Pub.Cov | Temporal  | Public Ins. Coverage | Year 2010 (CA) | 149,441      | 3            | $X$: 2/2   |\\n\\n### D.1 Datasets & Settings\\n\\nHere we introduce 5 real-world tabular datasets in detail. ACS Income, ACS Mobility, ACS Public Coverage are based on the American Community Survey (ACS) Public Use Microdata Sample [25]. And here we use the same data filtering as [25].\"}"}
{"id": "PF0lxayYST", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The task is to predict whether an individual's income is above $50,000. We filter the dataset to only include individuals above the age of 16, usual working hours of at least 1 hour per week in the past year, and an income of at least $100.\\n\\nIn Setting 1, we use data from 2018 and the source/target domain refers to American states, which contain natural spatial distribution shifts. We train methods on data from California, Connecticut, Massachusetts, and South Dakota respectively, and test the generalization performance on the other 50 American states.\\n\\nIn Setting 2, we sub-sample the dataset according to age to introduce covariate shift, where we focus on individuals from California and form two groups according to whether their age is \\\\( \\\\geq 25 \\\\). In Setting 21, the source data over-samples the low age group where 80% is drawn from the age \\\\( \\\\geq 25 \\\\) group, and the proportions are reversed in the target data (20% from age \\\\( \\\\geq 25 \\\\) group). In Setting 22, the source data over-samples the low age group where 90% is drawn from the age \\\\( \\\\geq 25 \\\\) group, and the proportions are reversed in the target data (10% from age \\\\( \\\\geq 25 \\\\) group).\\n\\nThe task is to predict whether an individual had the same residential address one year ago. We filter the dataset to only include individuals between the ages of 18 and 35, which increases the difficulty of the prediction task.\\n\\nIn Setting 5, we use data from 2018 and the source/target domain refers to American states, which contain natural spatial distribution shifts. We train methods on data from Mississippi, New York, California, and Pennsylvania respectively, and test the generalization performance on the other 50 American states.\\n\\nThe task is to predict whether an individual has public health insurance. We focus on low-income individuals who are not eligible for Medicare by filtering the dataset to only include individuals under the age of 65 and with an income of less than $30,000.\\n\\nIn Setting 11, we use data from 2018 and the source/target domain refers to American states, which contain natural spatial distribution shifts. We train methods on data from Nebraska, Florida, Texas, and Indiana respectively, and test the generalization performance on the other 50 American states.\\n\\nIn Setting 19, we consider the temporal shifts. We use data from 2010 in training and data from 2014, 2017, and 2021 in testing (3 test domains). In Setting 19, the training data come from New York, and in Setting 20, the training data come from California.\\n\\nThe task is to predict whether an accident is severe (long delay) or not (short delay) based on weather features and Road conditions features.\\n\\nIn Setting 15, the source/target domain refers to American states, which contain natural spatial distribution shifts. We train methods on data from California, Florida, Texas, and Minnesota respectively, and test the generalization performance on other 13 American states. Here we only involve 13 test domains because the sample sizes in the other states are quite small.\\n\\nThe task is to predict whether the total ride duration time exceeds 30 minutes, based on location and temporal features. We filter the data in 2017 and remove some extremely large or small features (e.g. samples with too long distances which can be easily classified).\\n\\nIn Setting 9, we use data from 2016 and the source/target domain refers to different cities. We train methods on data from Bogota and New York City and test the generalization performance in the other cities.\\n\\nIn our benchmark, we evaluate 22 algorithms that span a wide range of learning strategies on tabular data and compare their performances under different patterns of distribution shifts we construct. Concretely, these algorithms include:\\n\\n27\"}"}
{"id": "PF0lxayYST", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Base Methods\\n\\nWe include some typical supervised learning methods for tabular data: Logistic Regression (LR), SVM, and fully-connected neural networks (MLP) with standard ERM optimization. For LR and SVM, we use the standard implementation in scikit-learn[71] and train them on CPUs. For MLP, we implement it via PyTorch[70] and train it on GPUs.\\n\\nTree Ensemble Models\\n\\nAs shown by Gardner et al.[31], several tree-based methods achieve good performances on tabular datasets. And gradient-boosted trees (e.g., XGBoost, LightGBM) are widely considered as the state-of-the-art methods on tabular data. Therefore, we evaluate XGBoost and LightGBM in our benchmark. Also, we evaluate Random Forest (RF) to incorporate the performance of tree bagging methods.\\n\\nDRO Methods\\n\\nDistributionally Robust Optimization (DRO) methods are proposed to address the distribution shifts, which is the form of:\\n\\n$$\\\\min_{\\\\theta \\\\in \\\\Theta} \\\\sup_{Q \\\\in \\\\mathcal{P}} \\\\mathbb{E}_Q[\\\\ell(f_{\\\\theta}(X), Y)],$$\\n\\n(D.1)\\n\\nwhere \\\\(P(P_{tr})\\\\) denotes the uncertainty set around the training distribution \\\\(P_{tr}\\\\). Following Gardner et al.[31], we implement two typical variants of DRO, namely CVaR-DRO and \\\\(\\\\chi^2\\\\)-DRO. CVaR-DRO is equivalent to Conditional Value at Risk (CVaR), and \\\\(\\\\chi^2\\\\)-DRO uses \\\\(\\\\chi^2\\\\)-divergence to regulate the uncertainty set. We use the fast implementation[53] in PyTorch[70]. We also consider the DORO [105], which discards a proportion \\\\(\\\\epsilon\\\\) of the largest error points in each iteration to mitigate the outliers in DRO with two variants, CVaR-DORO and \\\\(\\\\chi^2\\\\)-DORO. We also evaluate the Group DRO [79], which is proposed to minimize the worst-group loss and shows good generalization performances on many vision tasks. This method needs the group label and we define groups according to the \\\"SEX\\\" feature on ACS datasets. For US Accident and Taxi, we do not run Group DRO, since the current Group DRO model in the codebase only accepts the input with few groups while it is hard to define such group here. For all DRO methods, we use the MLP as the backbone model. We do not choose tree ensemble methods since tree ensemble methods are difficult to adapt to the distributionally robust case. Therefore, we leave their method developments and implementations as our future work.\\n\\nImbalanced Learning Methods\\n\\nRecently, some simple data balancing methods[42] have shown good worst-group performances under distribution shifts. In our benchmark, we implement 4 typical balancing methods, namely Sub-Sampling Y (SUBY), Reweighting Y (RWY), Sub-Sampling Group (SUBG), and Reweighting Group (RWG). Besides, \\\"Just Train Twice\\\" (JTT[57]) exhibits good performances in many vision tasks, and therefore we also evaluate it in our tabular settings. Furthermore, stable learning methods[51, 24] propose to de-correlate sample covariates for an accurate estimation of causal relationships via global balancing, which could mitigate distribution shifts. In our benchmark, we implement one typical method named DWR[51]. For these imbalanced learning methods, we use XGBoost as the backbone model due to its superiority on tabular data and adjust sample weight or training procedure accordingly for each of the methods.\\n\\nFairness-enhancing Methods\\n\\nFollowing Ding et al.[25] and Gardner et al.[31], fairness-enhancing methods have the potential to mitigate the performance degradation under distribution shifts. In our benchmark, we evaluate the in-processing and post-processing intervention methods. The in-processing method[4] minimizes the prediction error subject to some fairness constraints, and in our benchmark, we choose three typical fairness constraints, including demographic parity (DP), equal opportunity (EO), error parity (EP). And the post-processing method[37] randomizes the predictions of a fixed classifier to satisfy equalized odds criterion, and we use exponential and threshold controls in our benchmark. We use the implementations of aif360[11] and fairlearn[12].\\n\\nD.3 Parameter Search Space\\n\\nWe provide the hyperparameter grids in Table 4. We mainly use the hyperparameter grids proposed in [31], and we restrict the grid size of each method in each setting to 200 in consideration of computational costs. For each setting, we randomly pick 200 configurations for each algorithm for a fair comparison. For methods incorporating backbone models (e.g., MLP/XGBoost), we choose the top 10 best configurations for that backbone model to reduce the search space, making the searched best configuration represent its best performance more accurately. Moreover, to accelerate the grid search process, we utilize Ray[55] to run experiments in parallel.\"}"}
{"id": "PF0lxayYST", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model               | Total Grid Size | Hyperparameter Value Range                                                                 |\\n|---------------------|-----------------|--------------------------------------------------------------------------------------------|\\n| Base Methods        |                 |                                                                                           |\\n| MLP                 | 270             | - Learning Rate: \\\\{0.001, 0.003, 0.0001, 0.005, 0.01\\\\}                              |\\n|                     |                 | - Batch Size: \\\\{64, 128, 256\\\\}                                                       |\\n|                     |                 | - Hidden Units: \\\\{16, 32, 64\\\\}                                                     |\\n|                     |                 | - Dropout Ratio: \\\\{0, 0.1\\\\}                                                        |\\n|                     |                 | - Train Epoch: \\\\{50, 100, 200\\\\}                                                   |\\n| SVM                 | 96              | - C: \\\\{0.01, 0.1, 1, 10, 100, 1000\\\\}                                              |\\n|                     |                 | - Kernel: \\\\{linear, RBF\\\\}                                                          |\\n|                     |                 | - Loss: \\\\{Squared Hinge\\\\}                                                          |\\n|                     |                 | - \u03b3: \\\\{0.1, 0.3, 0.5, 1.0, 1.5, 2.0, scale, auto\\\\}                                |\\n| Logistic Regression| 23              | - L2 penalty: \\\\{0.001, 0.03, 0.005, 0.007, 0.01, 0.03, 0.05, 0.1, 0.3, 1.0, 10.0, 50.0, 1e2, 1e3, 5e3, 1e4\\\\} |\\n| Tree Ensemble Methods|               |                                                                                           |\\n| Random Forest       | 640             | - Num. Estimators: \\\\{32, 64, 128, 256, 512\\\\}                                        |\\n|                     |                 | - Max Features: \\\\{sqrt, log2\\\\}                                                     |\\n|                     |                 | - Min. Samples Split: \\\\{2, 4, 8, 16\\\\}                                               |\\n|                     |                 | - Min. Samples Leaf: \\\\{1, 2, 4, 8\\\\}                                                 |\\n|                     |                 | - Cost-Complexity \u03b1: \\\\{0, 0.001, 0.01, 0.1\\\\}                                       |\\n| XGBoost             | 1944            | - Learning Rate: \\\\{0.1, 0.3, 1.0, 2.0\\\\}                                           |\\n|                     |                 | - Min. Split Loss: \\\\{0, 0.01, 0.5\\\\}                                                |\\n|                     |                 | - Max. Depth: \\\\{4, 6, 8\\\\}                                                          |\\n|                     |                 | - Column Subsample Ratio (tree): \\\\{0.7, 0.9, 1\\\\}                                   |\\n|                     |                 | - Column Subsample Ratio (level): \\\\{0.7, 0.9, 1\\\\}                                  |\\n|                     |                 | - Max. Bins: \\\\{128, 256, 512\\\\}                                                    |\\n|                     |                 | - Growth Policy: \\\\{Depthwise, Loss Guide\\\\}                                          |\\n| LightGBM            | 1680            | - Learning Rate: \\\\{0.01, 0.1, 0.5, 1.0\\\\}                                           |\\n|                     |                 | - Num. Estimators: \\\\{64, 128, 256, 512\\\\}                                           |\\n|                     |                 | - L2 penalty: \\\\{0, 0.001, 0.01, 0.1\\\\}                                              |\\n|                     |                 | - Min. Child Samples: \\\\{1, 2, 4, 8, 16, 32, 64\\\\}                                 |\\n|                     |                 | - Column Subsample Ratio (tree): \\\\{0.5, 0.8, 1\\\\}                                   |\\n|                     |                 | - Column Subsample Ratio (level): \\\\{0.5, 0.8, 1\\\\}                                  |\\n| DRO                 |                 |                                                                                           |\\n| \u03c7\u00b2                  | 1890            | - Uncertainty set size: \\\\{0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6\\\\}                   |\\n| DRO CVaR            | 1890            | - Uncertainty set size: \\\\{0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6\\\\}              |\\n| DRO                  |                 |                                                                                           |\\n| \u03c7\u00b2                  | 8100            | - Uncertainty set size: \\\\{0.1, 0.2, 0.3, 0.4, 0.5, 0.6\\\\}                         |\\n| DRO CVaR            | 8100            | - Uncertainty set size: \\\\{0.1, 0.2, 0.3, 0.4, 0.5, 0.6\\\\}                         |\\n| Group DRO           |                 |                                                                                           |\\n| \u03c7\u00b2                  | 1080            | - Group weights step size: \\\\{0.001, 0.01, 0.1, 0.2\\\\}                              |\\n| DRO CVaR            | 1080            | - Group weights step size: \\\\{0.001, 0.01, 0.1, 0.2\\\\}                              |\\n| Fairness Methods    |                 |                                                                                           |\\n| In-processing       |                 | - Constraint Type: \\\\{DP, EO, Error Parity\\\\}                                        |\\n| Post-processing     |                 | - Constraint Type: \\\\{Exp, Threshold\\\\}                                               |\"}"}
{"id": "PF0lxayYST", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We select the top-10 configurations according to the validation set.\\n\\nIn the main body, due to space limitations, we only visualize the target performances of different DROMethods (base: MLP).\\n\\nFor settings with US Accident, we only randomly sample 8,000 samples for the source/target datasets, we only randomly sample 20,000 samples from the source domain for training, 20,000 samples with training data and report the target performances of different algorithms.\\n\\nTable 5.\\n\\n| Dataset     | Target Pair | DOR  | 81.2 | SUBG 81.2 | RWG 81.4 | RWY 80.9 | SUBY 80.5 | EXP 81.1 | Random Forest 81.1 | Fair 78.8 |\\n|-------------|-------------|------|------|----------|----------|----------|-----------|----------|---------------------|-----------|\\n| ACS Income  | ACS Public Coverage | \u00b1    | 81.3 | 81.2     | \u00b1        | \u00b1        | \u00b1         | 81.2     | 81.2                | \u00b1         |\\n| HI NYC      | \u00b1           | \u00b1    | 86.3 | 85.8     | \u00b1        | \u00b1        | \u00b1         | 86.3     | 86.3                | \u00b1         |\\n| OR 2010     | \u00b1           | \u00b1    | 85.6 | 85.8     | \u00b1        | \u00b1        | \u00b1         | 85.6     | 85.6                | \u00b1         |\\n| 2017 Young  | \u00b1           | \u00b1    | 86.4 | 86.7     | \u00b1        | \u00b1        | \u00b1         | 86.4     | 86.4                | \u00b1         |\\n\\nFor settings with ACS Public Coverage, we found that the bi-search in ACS Public Coverage, dominates Imb > Others > DOR > Tree. Base dominates OR 2010 > 2017 Young. For settings with HI NYC, we found that DOR > Tree. Base dominates Imb > Others > DOR > Tree. Base dominates OR 2010 > 2017 Young.\"}"}
{"id": "PF0lxayYST", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 11. Target vs. source accuracies for 22 algorithms and datasets in our benchmark. Each point represents one hyperparameter configuration.\\n\\n(a)-(b): two examples of ACS Income dataset with California (CA) as the source state, and Puerto Rico (PR) and South Dakota (SD) as targets.\\n\\n(c)-(g): five examples of ACS Mobility, Taxi, ACS Pub.Cov, US Accident datasets.\\n\\n(h): simulated covariate shifts on on sub-sampled ACS Income dataset.\\n\\nAnalysis\\n\\nFrom the numbers of numerical results in that Table, we give a more detailed analysis corresponding with our main body.\\n\\n\u2022 Different algorithms do not exhibit consistent rankings over different distribution shift patterns. In Table 5, we boldface the best target performance within each class of methods. And we show the top-3 classes at the bottom. The results show that the algorithmic rankings across different settings and tasks are quite different. And even the rankings of algorithms within the same class vary a lot. This further demonstrates the complexity of $Y|X$ shifts.\\n\\n\u2022 Tree ensemble methods show competitive performances but do not significantly eliminate the generalization error between source and target data, characterized by the difference.\"}"}
{"id": "PF0lxayYST", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"between o.o.d. and i.i.d. model performance. From the bottom, we could see that tree ensemble methods achieve top-3 performances in most of the selected settings (6 out of 7), exhibiting their superiority on tabular data. However, the performance degradation between source and target is still large.\\n\\n\u2022 Imbalance methods and fairness methods show similar performance with the base learner (XGBoost). We find these methods do not have a significant improvement over the base learner (XGBoost). This may be due to that they are not designed to incorporate tree ensemble methods.\\n\\nD.6 Full Results of 22 Settings\\n\\nIn this section, we provide the full results of our 22 settings in Table 3.\\n\\nD.6.1 Average Accuracy\\n\\nIn Table 6, Table 7 and Table 8, we report the source accuracy and the target accuracy of all algorithms. Since in these settings, we have multiple target domains, as for the target accuracy of each algorithm, we report the average accuracy as well as the standard deviation calculated on all target domains. Here the standard deviation reflects the stability of out-of-distribution generalization performances across different target domains different instead of random seeds.\\n\\nFrom the average result over, Table 6, Table 7 and Table 8, we can still obtain similar findings compared to that from the 7 selected settings in the main body. Especially, while tree ensemble methods serve as a strong in-sample benchmark illustrated by their competitive i.i.d. performance, these methods do not yield better performance than other methods in the target distribution uniformly. And when we average across different target domains, DRO methods do not perform better than their empirical counterpart, indicating that the worst-case optimization from the uncertainty sets constructed by existing DRO methods can rarely occur in practical setups and does not help much compared with standard ERM cases. This tricky part of worst-case intervention also holds when we compare imbalanced learning and fairness methods with the XGB base learner. These methods can lead to better o.o.d. results than the base learner sometimes indeed, while this may not hold true when we average over different target domains. In general, existing methods cannot generalize well averaging over different target domains, and it would be interesting to develop methods to close that gap.\\n\\nD.7 Worst-Domain Accuracy\\n\\nAnother widely-used metric in evaluating generalization performance is worst-case accuracy. In settings built on ACS Income, ACS Pub.Cov, ACS Mobility, and US Accident, for the target accuracy of each algorithm, we report the worst target domain accuracy in Table 9 and Table 10. For other settings, we do not report the worst-case accuracy because there are not many target domains, and the average & standard deviation results could reflect the population generalization performance well. In terms of the worst target accuracy for each setting, we usually observe a large performance degradation (usually over a 10 percent performance drop) in all of the existing methods. And algorithms do not show a consistent and stable ranking performance in the o.o.d. performance. DRO / Imbalanced learning methods can perform quite well in one o.o.d. setup but are dominated by other methods in some other cases. This demonstrates a need to carefully understand the difference between different domains first and develop corresponding methods then.\\n\\nD.8 Average Macro-F1 Score\\n\\nIn consideration of the label imbalance in real-world tabular datasets, we also calculate the Macro-F1 Score for all algorithms under our settings. In this section, we report the average results in Table 11, Table 12, and Table 13. Here the standard deviation reflects the stability of out-of-distribution generalization performances (not the randomness).\\n\\nD.9 Worst-Domain Macro-F1 Score\\n\\nIn settings built on ACS Income, ACS Pub.Cov, ACS Mobility, and US Accident, for the target accuracy of each algorithm, we report the worst target domain Macro-F1 score in Table 14 and Table 15.\"}"}
