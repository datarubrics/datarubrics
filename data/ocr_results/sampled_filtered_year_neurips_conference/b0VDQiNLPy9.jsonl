{"id": "b0VDQiNLPy9", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ETAB: A Benchmark Suite for Visual Representation Learning in Echocardiography\\n\\nAhmed M. Alaa  \\nUC Berkeley and UCSF  \\nBerkeley, CA  \\namalaa@berkeley.edu\\n\\nAnthony Philippakis  \\nBroad Institute of MIT and Harvard  \\nCambridge, MA\\n\\nDavid Sontag  \\nMIT CSAIL and IMES  \\nCambridge, MA  \\ndsontag@csail.mit.edu\\n\\nAbstract\\n\\nEchocardiography is one of the most commonly used diagnostic imaging modalities in cardiology. Application of deep learning models to echocardiograms can enable automated identification of cardiac structures, estimation of cardiac function, and prediction of clinical outcomes. However, a major hindrance to realizing the full potential of deep learning is the lack of large-scale, fully curated and annotated data sets required for supervised training. High-quality pre-trained representations that can transfer useful visual features of echocardiograms to downstream tasks can help adapt deep learning models to new setups using fewer examples. In this paper, we design a suite of benchmarks that can be used to pre-train and evaluate echocardiographic representations with respect to various clinically-relevant tasks using publicly accessible data sets. In addition, we develop a unified evaluation protocol\u2014which we call the echocardiographic task adaptation benchmark (ETAB)\u2014that measures how well a visual representation of echocardiograms generalizes to common downstream tasks of interest. We use our benchmarking framework to evaluate state-of-the-art vision modeling pipelines. We envision that our standardized, publicly accessible benchmarks would encourage future research and expedite progress in applying deep learning to high-impact problems in cardiovascular medicine.\\n\\nDataset\\n\\n| View annotations | Dataset | View | Task | Source task | Target task |\\n|------------------|---------|------|------|-------------|-------------|\\n| PLAX/AP4CH views | EchoNet | AP4CH | LV segmentation | PLAX/AP4CH views | LV measurements |\\n| CAMUS | AP2CH | Myocardial segmentation | CAMUS | Myocardial segmentation |\\n| Unity | PLAX/PSAX views | LV segmentation | Unity | LV segmentation |\\n\\nFigure 1: Visual task adaptation for echocardiography. Using multiple open- and public-access echocardiogram data sets, we develop a suite of benchmarks for representation and transfer learning in echocardiography.\"}"}
{"id": "b0VDQiNLPy9", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1 Introduction and Motivation\\n\\nEchocardiography is one of the most commonly used non-invasive imaging techniques for assessing cardiac function, examining heart anatomy and diagnosing cardiovascular diseases. An \\\"echo\\\" study is an ultrasound of the heart acquired by a cardiac sonographer through a transducer device\u2014different acquisition angles by which the device is placed relative to the patient's heart provide different \\\"views\\\" of the heart anatomy. Echocardiograms inform cardiologists, surgeons, oncologists and emergency physicians on clinical decisions pertaining to treatment management and surgical planning [1\u20136].\\n\\nBecause of the central role it plays in cardiovascular medicine, there has been a significant interest in applying deep learning-based computer vision models to echocardiograms, with the ultimate goal of automating cardiac evaluation, reducing variance and improving reproducibility in interpreting echocardiograms, and predicting patient-specific clinical outcomes [7\u201314].\\n\\nA key hindrance to wider engagement in research focusing on applying deep learning to echocardiography is the lack of large, standardized, publicly-accessible data sets with the annotations required for all downstream tasks of interest. This is because, in addition to the regulatory hurdles associated with publicly sharing clinical data, building such data sets is already a difficult task in itself\u2014i.e., collecting all labels of interest would require examination of clinical reports, pre-selection of relevant echo views, and retrospective manual annotation by experts. Consequently, existing public data sets for echocardiography (e.g., [15\u201317]) tend to contain a modest number of samples, with only subsets of the annotations and views required for all possible downstream analyses.\\n\\nBecause many downstream tasks would share the same relevant echocardiographic features, access to high-quality representations of echocardiograms\u2014i.e., representations that retain these features\u2014could enable reusing the same pre-trained representations across many tasks rather than training a new model from scratch for each task [18\u201320], hence enabling adaptation of pre-trained models in setups where fewer annotated data might be available. We shall call such a procedure visual task adaptation\u2014see Figure 1 for a pictorial illustration. Motivated by such thinking, this paper develops a standardized set of benchmark tasks for pre-training and/or evaluating visual models of echocardiography. Our goal is to increase engagement of researchers in a high-impact application domain by providing a systematic domain-specific benchmark through which researchers can focus their effort on tackling challenging problems peculiar to echocardiography, and translate methodological advances in representation learning to practical problems in cardiovascular medicine.\\n\\nThe contribution of this paper is two-fold.\\n\\nFirst, we develop a comprehensive suite of benchmark tasks tailored to echocardiography using a meta-dataset of public-access sources of echocardiographic data. These benchmark tasks can be used to test vision modeling pipelines, pre-train visual representations of echoes, and evaluate the transferability of representations across pairs of source-target tasks which might differ in the data source, echo views and annotations.\\n\\nSecond, using our suite of benchmark tasks we specify a unified evaluation protocol for readily available pre-trained representations\u2014the echocardiographic task adaptation benchmark (ETAB)\u2014which is meant to evaluate the extent by which a given representation generalizes to different tasks, views and patient cohorts. The ETAB benchmark enables researchers to share a unified and publicly-accessible evaluation protocol, even when the representations themselves are pre-trained on private hospital data. ETAB is implemented as a full-fledged software package with a user-friendly API for researchers to design and evaluate visual representations for echocardiograms (Code available at: https://github.com/ahmedmalaa/ETAB).\\n\\n2 Echocardiogram Datasets\\n\\nThe ETAB benchmark suite is based on 5 publicly accessible echocardiogram data sets that span different cohorts and involve different echocardiographic views and annotations (Table 1). In what follows, we provide a brief overview of all data sets currently supported in ETAB.\\n\\nEchoNet. This data set has two variants: EchoNet-Dynamic and EchoNet-LVH. In the EchoNet-Dynamic data set, introduced in [15], one apical-4 chamber (AP4CH) 2D gray-scale video is extracted from each echo study. A total of 10,036 videos are collected from 10,036 distinct individuals who underwent echocardiography between 2006 and 2018 as part of routine care at a University Hospital. Individuals in the data set were selected at random from hospital records. Along with each video, cardiac function assessments and calculations obtained by a registered sonographer and verified by a level-3 echocardiographer are provided. The second data set, EchoNet-LVH, was collected for a...\"}"}
{"id": "b0VDQiNLPy9", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Data set | Sample size | Annotations\\n--- | --- | ---\\nEchoNet-Dynamic | 10,036 | AP4CH LV traces and LV ejection fraction (EF)\\nEchoNet-LVH | 12,000 | PLAX LV internal dimension, LV posterior wall, Intraventricular septum\\nUnity | 1,224 | AP4CH, PLAX LV measurements and longitudinal strain\\nCAMUS | 500 | AP4CH, AP2CH LV EF, ES/ED frames, LV epicardium, endocardium and left atrium segments\\nTMED | 2,341 | PLAX, PSAX, Other Aortic Stenosis diagnoses, view annotations\\n\\nTable 1: Publicly accessible data sets involved in our ETAB framework.\\n\\nSimilar cohort and comprises detailed measurements of the LV dimensions based on parasternal long axis (PLAX) views. Both data sets were released by Stanford AIMI Center and are accessible via: https://echonet.github.io/echoNet/ and https://echonet.github.io/lvh/.\\n\\nUnity Imaging Collaborative. These echocardiograms were obtained from the British echocardiology laboratories and were retrospectively annotated by echocardiography-certified cardiologists through a UK-wide initiative that involved 17 hospitals [21]. The data set comprises a mixture of apical and parasternal views; the former is accompanied with measures of longitudinal strain, and the latter involves measures of LV dimensions. The data is accessible via: https://data.unityimaging.net/.\\n\\nCAMUS. The Cardiac Acquisitions for Multi-structure Ultrasound Segmentation (CAMUS) is an open-access data set from 500 patients, acquired at the University Hospital of St. Etienne (France) [16]. Compared to EchoNet, this data set has a smaller sample size but is more elaborately annotated, hence it serves as an appropriate target data set for a variety of downstream tasks. The data set contains apical two- and four-chamber acquisitions (AP2CH and AP4CH). Half of the population has a left ventricle (LV) ejection fraction lower than 45%, thus being considered at pathological risk. The data set contains full annotation of the left atrium, the endocardium and epicardium borders of the LV, performed by a cardiologist. To identify the beginning and end of each cardiac cycle, ED and ES frames within each echo clip are labeled. Data is accessible via: https://www.creatis.insa-lyon.fr/Challenge/camus/.\\n\\nTMED. The Tufts Medical Echocardiogram Dataset (TMED) contains still echocardiogram imagery acquired in the course of routine care from 2015 to 2020 [17]. This data set contains both labeled and unlabeled images\u2014labels include a categorization of views into parasternal long and short axis views (PLAX and PSAX), and diagnostic severity ratings for aortic stenosis (AS). Unlike EchoNet, Unity and CAMUS, this data set contains only still images rather than sequences of frames. The data set is publicly available and can be accessed through: https://tmed.cs.tufts.edu/.\\n\\n3 A Benchmark Suite for Echocardiographic Representation Learning\\n\\nOur benchmark suite encapsulates a set of standardized tasks that cover a wide variety of echocardiographic modeling problems of interest. Each task comprises a selection of a data set, a view and an annotation from the options listed in Table 1. These standardized tasks can be used to benchmark the performance of vision models, pre-train visual representations, or evaluate the quality of externally pre-trained representations of echocardiograms. In Section 3.1, we present the ETAB benchmark tasks, and in Section 3.2 we describe our proposed ETAB evaluation protocol.\\n\\n3.1 The ETAB benchmark tasks\\n\\nBased on the data sets listed in Section 2, we design 31 benchmark tasks that cover various echocardiographic tasks of interest. Each benchmark task comprises a specification of the data set, echo view and annotations. The benchmarks are divided into four categories: (\u2022) cardiac structure identification (e.g., detecting the borders of the LV), (\u2022) cardiac function estimation (e.g., estimating the LV ejection fraction), (\u2022) view classification, and (\u2022) clinical prediction. Benchmarks within the same category share a color code that will be later used in visualizations of experimental results. These benchmark tasks constitute the core of the ETAB evaluation protocol presented later in Section 3.2.\"}"}
{"id": "b0VDQiNLPy9", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: List of all ETAB benchmark tasks.\\nThe tasks cover four categories: (\u2022) cardiac structure identification, (\u2022) cardiac function estimation, (\u2022) view classification, and (\u2022) clinical prediction. Each benchmark task is assigned a code that encodes a specification of the source data set, echocardiographic view and annotations.\\n\\nEach benchmark is encoded through a 5-character code of the form XX-XX-X. The encoding scheme describes each benchmark in terms of the data source, echo view and modeling task follows:\\n\\n- Characters 0-1: Task identifier\\n- Characters 2-3: View identifier\\n- Characters 4-5: Dataset identifier\\n\\nThe data sets are encoded as follows: E: EchoNet, C: CAMUS, U: Unity, and T: TMED. The views are encoded as follows: A2: Apical 2-chamber, A4: Apical 4-chamber, PL: parasternal long axis, and PS: parasternal short axis. The 4 task categories are encoded as a, b, c and d, and the detailed list of codes for all tasks is provided in the Appendix. The task encoding provides a systematic way for running benchmark experiments and reporting results through the ETAB software library.\\n\\nTransfer learning benchmarks. In addition to the core benchmark tasks listed in Table 2, benchmark setups for transfer learning can be constructed by picking pairs of source and target tasks from the ETAB task suite. The pairs of benchmark tasks would be typically selected so that at least one of the three elements of a task specification (i.e., data set, view or annotation) differs between the source and the target. These \\\"adaptation\\\" setups can be used to test the ability of a transfer learning algorithm.\"}"}
{"id": "b0VDQiNLPy9", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to repurpose pre-trained representations for new downstream tasks, and provide insights into which tasks and model architectures lead to the most generalizable representations of echocardiograms. A transfer learning benchmark is encoded as: Source-task-code/Target-task-code. For example, benchmark a0-A4-E/b3-PL-U involves pre-training a representation to segment the left ventricle using apical 4-chamber studies from EchoNet-Dynamic, and then using the resulting representation to estimate the LV internal dimension using PLAX studies from the unity data set.\\n\\nVision modeling pipelines. Each of the core benchmark tasks in Table 2 can be conducted using a vision modeling pipeline \\\\( M \\\\) that comprises (1) a choice of a backbone architecture for the visual representation and a (2) task-specific model head as depicted in Figure 2. The backbone representation can either be pretrained using external data sources or trained from scratch on the benchmark task. When applied to a transfer learning setup (i.e., a pair of source and target benchmark tasks from Table 2), the modeling pipeline retains the backbone representation trained on the source task and finetunes a task-specific head using data from the target task. This could help us answer questions such as: is LV segmentation a good pretext task for learning useful representations in patients with aortic stenosis? which architectures result in representations that generalize well across patient cohorts? How many echo studies do we need to annotate in order to finetune a pretrained model for a given downstream task? Answers to these questions can inform practical modeling choices and data collection requirements.\\n\\nFigure 2: Depiction of a vision modeling pipeline \\\\( M \\\\).\\n\\n3.2 The ETAB evaluation protocol\\n\\nIn many practical scenarios, we might be interested in evaluating how well a visual representation pre-trained on an external data set captures echocardiographic features, e.g., private echocardiogram data, data for other cardiac imaging modalities, or even non-medical data sets (such as ImageNet). The echocardiographic task adaptation benchmark (ETAB) is a unified evaluation protocol that uses the core benchmark tasks in Table 2 to evaluate the usefulness of a given (pre-trained) visual representation for a wide variety of common downstream tasks in echocardiography.\\n\\nLet \\\\( K = \\\\{1, \\\\ldots, K\\\\} \\\\) be the benchmark categories, and let \\\\( T_k = \\\\{t_{1,k}, \\\\ldots, t_{T_k,k}\\\\} \\\\) be the tasks within category \\\\( k \\\\in K \\\\). (In Table 2, we have \\\\( K = 4 \\\\) task categories.) Let \\\\( D_{t,k} \\\\) be the data set associated with the \\\\( t \\\\)-th task within the \\\\( k \\\\)-th category. Let \\\\( R_\\\\theta \\\\) be a pre-trained representation with an architectural specification \\\\( R \\\\) and parameters \\\\( \\\\theta \\\\). For each task \\\\( t_k \\\\), we construct a model \\\\( M_{\\\\theta,\\\\phi}(t_k) = R_\\\\theta \\\\circ h_\\\\phi(t_k) \\\\), where \\\\( h_\\\\phi(t_k) \\\\) is a model head (e.g., a linear layer) that is specific to task \\\\( t_k \\\\) with parameters \\\\( \\\\phi(t_k) \\\\). For each task \\\\( t_k \\\\), the corresponding data set \\\\( D_{n,t,k} = \\\\{(X_{i,t,k}, Y_{i,t,k})\\\\}_{i=1}^n \\\\) is used to optimize the parameters of the task-specific head \\\\( \\\\phi^* \\\\), with the representation parameters \\\\( \\\\theta \\\\) fixed. Let \\\\( E_{t,k} \\\\) be the evaluation metric used to assess performance on task \\\\( t_k \\\\), which we assume takes on values in \\\\([0, 1]\\\\). The ETAB score of a pre-trained representation \\\\( R_\\\\theta \\\\) is thus defined as:\\n\\n\\\\[\\nETAB_n(R_\\\\theta) \\\\equiv E_{t \\\\sim P_{T_k}} E_{t,k} M_{\\\\theta,\\\\phi^*}(t_k)(D_{n,t,k}),\\n\\\\]\\n\\n\\\\[\\nETAB(R_\\\\theta) \\\\equiv E_{k \\\\sim P_K} ETAB_n(R_\\\\theta).\\n\\\\]\\n\\nThe ETAB score can be evaluated for different values of \\\\( n \\\\) to assess the sample efficiency, i.e., transfer-ability, of a pre-trained representation. We implement the metric in (1) by assigning equal weights to all benchmark tasks across all task categories, i.e., \\\\( P_{T_k} \\\\) and \\\\( P_K \\\\) are uniform distributions. A weighted ETAB score (with non-uniform choice of \\\\( P_{T_k} \\\\) and \\\\( P_K \\\\)) can be defined to reflect the relative importance or frequency of the different echocardiographic tasks in Table 2 in real-world settings.\\n\\nThe ETAB protocol is model-agnostic, hence it can serve as a benchmark against which all existing and future architectural choices \\\\( R \\\\) and pre-training algorithms can be evaluated. This includes representations pre-trained using supervised, semi-supervised, self-supervised, and generative approaches. Unlike the individual benchmark tasks in Section 3.1\u2014which assess specific downstream or transfer learning scenarios\u2014the ETAB protocol assesses the general usefulness of a visual representation to...\"}"}
{"id": "b0VDQiNLPy9", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"echocardiographic tasks through a single procedure and metric. Currently, the total number of benchmark tasks involved in ETAB is 31. We envision that the number of ETAB tasks will grow as more data sets and annotations for echocardiograms are released for public access over time.\\n\\nTo the best of our knowledge, ETAB is the first systematic benchmarking setup dedicated to evaluating echocardiographic representations. Our benchmarking setup can be considered as the domain-specific analogue of the general benchmark for vision tasks developed in [19, 20]. (Further discussion on related work is provided in the supplement.)\\n\\nModels that perform well on general benchmarks may not necessarily be as competitive on downstream tasks specific to echo data, which creates the need for benchmarks that are tailored to the tasks most relevant to cardiovascular problems. A schematic depiction of the protocol is provided in Figure 3.\\n\\n![Figure 3: Schematic depiction of the ETAB protocol.](image)\\n\\n### 4 The ETAB Software Library\\n\\nThe ETAB library is a PyTorch-based implementation of our benchmarking framework that provides a unified and easy-to-use API for running benchmark experiments out-of-the-box, developing new vision modeling pipelines for echocardiography and evaluating pre-trained representations using the ETAB score. The ETAB library offers two key features:\\n\\n- A unified API for loading and processing publicly-accessible echocardiography datasets.\\n- An easy-to-use API for creating modeling pipelines that supports a wide variety of backbone architectures and covers the benchmark tasks in Table 2.\\n\\nThe ETAB package can be installed via https://github.com/ahmedmalaa/ETAB. In this Section, we describe the key features of the package along with representative results for benchmark experiments.\\n\\n#### 4.1 Key features and examples of usage\\n\\n**ETAB data sets.** The ETAB library supports a unified API for all public-access data sets listed in Table 1. The `ETAB_dataset` class is a container for echocardiography data with a multitude of options that specify the data source, echo view, annotations and parameters of the echo study clip (e.g., resolution and frame size, clip length, frame sampling rates). An example for instantiating a data set of apical 4-chamber echo studies with LV EF measurements is provided below:\\n\\n```python\\nimport etab\\n\\nechonet = ETAB_dataset(name=\\\"echonet\\\", target=\\\"EF\\\", view=\\\"A4CH\\\", video=False, normalize=True, frame_l=224, frame_w=224, clip_l=16, fps=50, padding=None)\\n```\\n\\nThe `ETAB_dataset` comes with built-in functionalities for loading and processing the data. A detailed description of these functionalities can be found in the ETAB package documentation.\\n\\n**ETAB model zoo.** The ETAB package supports a variety of built-in vision modeling pipelines. Currently, the package supports 12 backbone representations that belong to the convolutional neural network (CNN) and vision transformer (ViT) families. Note that some benchmark tasks (e.g., estimation of LV ejection fraction) are defined with respect to video clips rather than still images, whereas other tasks and datasets are limited to 2D images. In the current release of ETAB, we restrict the backbone representations to frame embeddings and use these representations repeatedly over sequences of images. By restricting the backbone representations to frame embeddings, we ensure that the backbone architecture can be used systematically across all benchmark tasks. In Section 4.3,\"}"}
{"id": "b0VDQiNLPy9", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we describe the task-specific heads for all benchmark categories in Table 2. The ETAB model zoo (all of the supported backbone and model heads) is available in the Supplementary material.\\n\\nETAB score computation.\\n\\nTo compute the ETAB score for a given pre-trained representation, the user can load the pre-trained weights for a PyTorch implementation of any of the supported backbones, and then invoke the `ETABscore` function to evaluate the quality of the representation as follows:\\n\\n```python\\nfrom etab.scores import ETABscore\\nfrom torchvision.models import resnet50\\n\\nbackbone = resnet50(weights=\\\"IMAGENET1K_V1\\\")\\netab_score = ETABscore(backbone_architecture=\\\"ResNet-50\\\",\\nbackbone_model=backbone)\\n```\\n\\nIn the example above, we load a ResNet-50 model pre-trained on the ImageNet-1K data set and then evaluate its ETAB score, i.e., measure how well a ResNet-50 trained on non-medical imaging data captures echocardiographic features. By default, the ETAB score is evaluated assuming a uniform weight for all benchmark tasks and with the pre-trained weights being frozen across all tasks. The `ETABscore` function also supports full finetuning and weighted sampling of benchmark tasks.\\n\\nEvaluation metrics.\\n\\nWe use the following evaluation metrics within the ETAB evaluation protocol in Equation (1). For all segmentation tasks, the metric $E_{t,k}$ corresponds to the DICE score. For classification tasks, $E_{t,k}$ is defined as the area under the ROC curve (AUC-ROC). For regression tasks (i.e., predicting LV EF), we use $R^2$ as the metric $E_{t,k}$. All metrics are computed on held-out samples. Note that, while we use different metrics for the different tasks, all metrics are defined in the normalized range $[0, 1]$ (higher is better). To compute the ETAB score for a given baseline, we run all benchmark tasks in Table 2 and average the resulting evaluation metrics.\\n\\n4.2 ETAB modeling pipelines\\n\\nAs discussed in the previous Section, ETAB supports backbone representations $\\\\mathcal{R}_\\\\theta$ that belong to the CNN and ViT architectural families. The CNN backbones include: ResNet [22], ResNeXt [23], DenseNet [24], Inception [25], MobileNet [26] and ConvNext [27]. The ViT backbones include: Mix Transformer encoders (MiT) [28], Pyramid Vision Transformer (PVT) [29], Multi-scale vision Transformer (ResT) [30], PoolFormer [31], and UniFormer [32]. In what follows, we describe the task-specific heads $h_\\\\phi$ for the four benchmark categories in Table 2.\\n\\n\u2022 Task-specific heads for cardiac structure identification benchmarks.\\n\\nThe \u201ccardiac structure identification\u201d tasks involve semantic segmentation of ventricular and other anatomic structures. We consider the following segmentation heads: U-Net [33], U-Net++ [34], MAnet [35], Linknet [36], PSPNet [37], DeepLabV3 [38], TopFormer and SegFormer [28]. Depending on the architecture of the selected backbone representation, the default segmentation head used in the `ETABscore` function is either a U-Net or a SegFormer.\\n\\n\u2022 Task-specific heads for cardiac function estimation benchmarks.\\n\\nThis category of tasks involve estimating a patient\u2019s cardiac output from an echocardiography video. For all tasks in this category, we apply the backbone representations to obtain frame-level embeddings for the sequence of frames in each echo study. The task-specific heads are variants of recurrent neural networks (RNN) applied to these frame embeddings to obtain a representation for the sequence.\\n\\n\u2022 Task-specific heads for view classification and clinical predictions.\\n\\nFor all benchmark tasks in this category, we considered baseline models that train attach a simple linear layer on top of the backbone representation to evaluate the output prediction. The `ETABmodel` class in ETAB provides a unified sklearn-like API for all vision modeling pipelines. Examples of usage of the ETAB modeling pipelines are provided in the demo notebooks within the ETAB package documentation. Evaluations of the ETAB scores for state-of-the-art backbone models are maintained and regularly updated on the online ETAB leaderboard. In the rest of this Section, we provide sample results for applying the vision modeling pipelines to the benchmark tasks in Table 2.\"}"}
{"id": "b0VDQiNLPy9", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Results for the cardiac structure identification benchmarks (\u2022). (a) Performance of the SegFormer model head with a ResNet-50 backbone with and without pre-training on the source data in each transfer learning benchmark. (b) Ground-truth traces (top) and samples of the recovered segmentation by the model (bottom).\\n\\n4.3 Transfer learning benchmarks\\n\\nIn this Section, we consider the cardiac structure identification benchmarks (\u2022) in Table 2. Here, the goal is to segment various anatomic structures of relevance to cardiac diagnostics. In particular, the tasks cover segmentation of the left ventricle (LV), the left atrium (LA), and myocardial wall (MY).\\n\\nLV segmentation is used to compute cardiac contractile function parameters (e.g., systolic/diastolic volumes, ejection fraction, and myocardium mass), which are key indicators of cardiac health [39]. Similarly, segmental evaluation of the MY and LA confer diagnostic value since MY wall thickness and LA enlargement are associated with pathological conditions such as hypertrophy [40].\\n\\nBased on these benchmarks, we consider 3 experimental setups designed to test the transferability of features across data sets, echo views and tasks. Setup a0-A4-C/a0-A2-C can be used to assess the transferability of ML models across 4- and 2-chamber views, setups a0-A4-C/a1-A4-C and a0-A4-C/a2-A4-C assess transferability across targets, whereas a0-A4-E/a0-A4-C tests transferability across data sets. Setups a0-A4-E/a0-A2-C and a0-A4-E/a2-A2-C are more challenging as they involve changing data sets, views and labels between the source and target tasks.\\n\\nIn Figure 4 (a), we show the performance of the SegFormer head with a ResNet-50 backbone for setups a0-A4-E/a0-A4-C, a0-A4-C/a0-A2-C and a0-A4-C/a2-A4-C. We evaluate the sample efficiency of the baseline model by fixing the number of training examples in the source task, and varying the number of training examples within the target task. This setup emulates the real-world scenario where we would have few echocardiographic studies extracted from hospital records for a cohort of interest that are manually annotated by a cardiologist, and we would like to adapt a model pre-trained on data from a different hospital to the cohort under study. Stronger baselines would enable us to collect and manually annotate less data points.\\n\\nThe results in Figure 4 (a) demonstrate the relative difficulty of the different transfer learning tasks in terms of the number of (target) examples required for competitive performance. For each of the benchmark tasks, we compare an adaptation pipeline that pre-trains the model on the source task with training from scratch on the target task. For the a0-A4-E/a0-A4-C and a0-A4-C/a0-A2-C setups, we observe that pre-training on source task significantly outperforms supervised learning from scratch when the number of target samples is as few as 16 target examples. From a practical perspective, this means that we can use these benchmarks to select adaptation pipelines for small-scale studies involving echocardiography data (e.g., clinical trial data), and expect the selected model to per-\"}"}
{"id": "b0VDQiNLPy9", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Performance of different segmentation heads on cardiac structure identification benchmarks.\\n\\nIn all experiments, we use 32 training examples in the target task. All baselines use ResNet-50 backbone as the encoder architecture. Reported numbers are DICE scores on a test set; bold values correspond to best performing baseline.\\n\\nFigure 4 (a) shows that, while transferring learned features across data sets and across echo views is feasible, transferring representations across different labels (i.e., LV and MY) is a difficult task. We observe that for setup a0-A4-C/a2-A4-C, pre-training on the source task was no better than training from scratch on target data. The difficulty of this benchmark task inspires future research that focuses on developing adaptation pipelines that use other approaches, such as self-supervised or multi-task learning, to improve the transferability of features across the LV and MY labels.\\n\\nTable 3 demonstrates the performance of all segmentation heads with the ResNet-50 backbone on 6 transfer learning benchmarks. We observe that no modeling pipeline uniformly outperforms others on all benchmarks, which suggests that our benchmark suite can be used for model selection in new downstream tasks. Overall, we found that transfer learning benchmarks involving label shift (e.g., a0-A4-C/a1-A4-C and a0-A4-C/a2-A4-C) are more difficult than benchmarks involving shift in data sets or views. Benchmarks that involved label shifts also exhibited larger variance in the observed performance of the different modeling baselines.\\n\\nTable 4: ETAB scores for various backbone representations.\\n\\n4.4 Evaluating pre-trained representations using the ETAB protocol\\n\\nFinally, we compare various pre-trained backbone representations with respect to their ETAB scores and report the results in Table 4. Here, we evaluate the ETAB scores by tuning the different backbone models to tasks a0-A4-E, a0-A4-C, a0-A2-C, a1-A4-C, and a1-A2-C, with equal weights for all tasks. We explore two variants of each backbone model: freezing the pre-trained ImageNet-1K weights and tuning the head, and fully finetuning the entire model for each benchmark task. The backbone models included CNN architectures (ResNet, MobileNet, ConvNext), as well ViT models (MiT and PoolFormer). Table 4 lists the ETAB scores for all backbone models. A visualization for the break down of the task-specific ETAB scores is available in the online leaderboard.\\n\\nOverall, we found that the CNN-variants outperformed their ViT counterparts. Moreover, full fine-tuning on the ETAB data sets did not significantly improve the ETAB scores for most backbone models. An interesting use case for our the ETAB framework is to explore whether pre-training backbone representations on large-scale medical imaging data sets, such as RadImageNet [41], could outperform the ImageNet-pretrained backbone with respect to the ETAB score.\"}"}
{"id": "b0VDQiNLPy9", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Conclusion\\n\\nDeep learning can help automate the echocardiography workflow and refine our understanding of heart disease subtypes using cardiac ultrasound data. The scarcity and sporadicity of publicly accessible echocardiogram data sets create a need for standardized benchmarks in order to encourage engagement in research applying state-of-the-art representation learning methods to echocardiographic data. This paper takes a first step towards building a standardized framework for benchmarking the quality of representations for echocardiograms. We hope that the benchmark tasks and evaluation methods presented in this paper will lower the barrier to translating advances in visual representation learning into impactful applications in the medical domain.\\n\\nAcknowledgments and Disclosure of Funding\\n\\nFunding to support this research was provided for by the Eric and Wendy Schmidt Center at the Broad Institute of MIT and Harvard (https://www.broadinstitute.org/ewsc.).\\n\\nReferences\\n\\n[1] Robert R Moss, Emma Ivens, Sanjeevan Pasupati, Karin Humphries, Christopher R Thompson, Brad Munt, Ajay Sinhal, and John G Webb. Role of echocardiography in percutaneous aortic valve implantation. JACC: Cardiovascular Imaging, 1(1):15\u201324, 2008.\\n\\n[2] G Habib and A Torbicki. The role of echocardiography in the diagnosis and management of patients with pulmonary hypertension. European Respiratory Review, 19(118):288\u2013299, 2010.\\n\\n[3] Jennifer Liu, Jose Banchs, Negareh Mousavi, Juan Carlos Plana, Marielle Scherrer-Crosbie, Paaladinesh Thavendiranathan, and Ana Barac. Contemporary role of echocardiography for clinical decision making in patients during and after cancer therapy. JACC: Cardiovascular Imaging, 11(8):1122\u20131131, 2018.\\n\\n[4] Guy R Randolph, Donald J Hagler, Heidi M Connolly, Joseph A Dearani, Francisco J Puga, Gordon K Danielson, Martin D Abel, V Shane Pankratz, and Patrick W O\u2019Leary. Intraoperative transesophageal echocardiography during surgery for congenital heart defects. The Journal of Thoracic and Cardiovascular Surgery, 124(6):1176\u20131182, 2002.\\n\\n[5] Meredith K Ford, W Scott Beattie, and Duminda N Wijeysundera. Systematic review: prediction of perioperative cardiac complications and mortality by the revised cardiac risk index. Annals of internal medicine, 152(1):26\u201335, 2010.\\n\\n[6] Raphael Rosenhek, Ursula Klaar, Michael Schemper, Christine Scholten, Maria Heger, Harald Gabriel, Thomas Binder, Gerald Maurer, and Helmut Baumgartner. Mild and moderate aortic stenosis: natural history and risk stratification by echocardiography. European Heart Journal, 25(3):199\u2013205, 2004.\\n\\n[7] David Ouyang, Bryan He, Amirata Ghorbani, Neal Yuan, Joseph Ebinger, Curtis P Langlotz, Paul A Heidenreich, Robert A Harrington, David H Liang, Euan A Ashley, et al. Video-based ai for beat-to-beat assessment of cardiac function. Nature, 580(7802):252\u2013256, 2020.\\n\\n[8] Grant Duffy, Paul P Cheng, Neal Yuan, Bryan He, Alan C Kwan, Matthew J Shun-Shin, Kevin M Alexander, Joseph Ebinger, Matthew P Lungren, Florian Rader, et al. High-throughput precision phenotyping of left ventricular hypertrophy with cardiovascular deep learning. JAMA cardiology, 7(4):386\u2013395, 2022.\\n\\n[9] KY Esther Leung and Johan G Bosch. Automated border detection in three-dimensional echocardiography: principles and promises. European journal of echocardiography, 11(2):97\u2013108, 2010.\\n\\n[10] Sumeet Gandhi, Wassim Mosleh, Joshua Shen, and Chi-Ming Chow. Automation, machine learning, and artificial intelligence in echocardiography: a brave new world. Echocardiography, 35(9):1402\u20131418, 2018.\\n\\n[11] Sukrit Narula, Khader Shameer, Alaa Mabrouk Salem Omar, Joel T Dudley, and Partho P Sengupta. Machine-learning algorithms to automate morphological and functional assessments in 2d echocardiography. Journal of the American College of Cardiology, 68(21):2287\u20132295, 2016.\"}"}
{"id": "b0VDQiNLPy9", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Manar D Samad, Alvaro Ulloa, Gregory J Wehner, Linyuan Jing, Dustin Hartzel, Christopher W Good, Brent A Williams, Christopher M Haggerty, and Brandon K Fornwalt. Predicting survival from large echocardiography and electronic health record datasets: optimization with machine learning. JACC: Cardiovascular Imaging, 12(4):681\u2013689, 2019.\\n\\nKenya Kusunose, Akihiro Haga, Takashi Abe, and Masataka Sata. Utilization of artificial intelligence in echocardiography. Circulation Journal, pages CJ\u201319, 2019.\\n\\nFederico M Asch, Theodore Abraham, Madeline Jankowski, Jayne Cleve, Mike Adams, Nathanael Romano, Nicolas Polivert, Ha Hong, and Roberto Lang. Accuracy and reproducibility of a novel artificial intelligence deep learning-based algorithm for automated calculation of ejection fraction in echocardiography. Journal of the American College of Cardiology, 73(9S1):1447\u20131447, 2019.\\n\\nDavid Ouyang, Bryan He, Amirata Ghorbani, Matt P Lungren, Euan A Ashley, David H Liang, and James Y Zou. Echonet-dynamic: a large new cardiac motion video data resource for medical machine learning. In NeurIPS ML4H Workshop: Vancouver, BC, Canada, 2019.\\n\\nSarah Leclerc, Erik Smistad, Joao Pedrosa, Andreas \u00d8stvik, Frederic Cervenansky, Florian Espinosa, Torvald Espeland, Erik Andreas Rye Berg, Pierre-Marc Jodoin, Thomas Grenier, et al. Deep learning for segmentation using an open large-scale dataset in 2D echocardiography. IEEE transactions on medical imaging, 38(9):2198\u20132210, 2019.\\n\\nZhe Huang, Gary Long, Benjamin Wessler, and Michael C Hughes. A new semi-supervised learning benchmark for classifying view and diagnosing aortic stenosis from echocardiograms. In Machine Learning for Healthcare Conference, pages 614\u2013647. PMLR, 2021.\\n\\nFuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43\u201376, 2020.\\n\\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. The visual task adaptation benchmark. 2019.\\n\\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019.\\n\\nJames P Howard, Catherine C Stowell, Graham D Cole, Kajaluxy Ananthan, Camelia D Demetrescu, Keith Pearce, Ronak Rajani, Jobanpreet Sehmi, Kavitha Vimalvaran, G Sunthar Kanaganayagam, et al. Automated left ventricular dimension assessment using artificial intelligence developed and validated by a UK-wide collaborative. Circulation: Cardiovascular Imaging, 14(5):e011951, 2021.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\\n\\nSaining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492\u20131500, 2017.\\n\\nForrest Iandola, Matt Moskewicz, Sergey Karayev, Ross Girshick, Trevor Darrell, and Kurt Keutzer. Densenet: Implementing efficient convnet descriptor pyramids. arXiv preprint arXiv:1404.1869, 2014.\\n\\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818\u20132826, 2016.\\n\\nAndrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1314\u20131324, 2019.\\n\\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. arXiv preprint arXiv:2201.03545, 2022.\"}"}
{"id": "b0VDQiNLPy9", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media, 8(3):415\u2013424, 2022.\\n\\nQinglong Zhang and Yu-Bin Yang. Rest: An efficient transformer for visual recognition. Advances in Neural Information Processing Systems, 34:15475\u201315485, 2021.\\n\\nWeihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10819\u201310829, 2022.\\n\\nKunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unifying convolution and self-attention for visual recognition. arXiv preprint arXiv:2201.09450, 2022.\\n\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234\u2013241. Springer, 2015.\\n\\nZongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang. Unet++: A nested u-net architecture for medical image segmentation. In Deep learning in medical image analysis and multimodal learning for clinical decision support, pages 3\u201311. Springer, 2018.\\n\\nTongle Fan, Guanglei Wang, Yan Li, and Hongrui Wang. Ma-net: A multi-scale attention network for liver and tumor segmentation. IEEE Access, 8:179656\u2013179665, 2020.\\n\\nAbhishek Chaurasia and Eugenio Culurciello. Linknet: Exploiting encoder representations for efficient semantic segmentation. In 2017 IEEE Visual Communications and Image Processing (VCIP), pages 1\u20134. IEEE, 2017.\\n\\nHengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2881\u20132890, 2017.\\n\\nLiang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.\\n\\nShusil Dangi, Ziv Yaniv, and Cristian A Linte. Left ventricle segmentation and quantification from cardiac cine mr images via multi-task learning. In International Workshop on Statistical Atlases and Computational Models of the Heart, pages 21\u201331. Springer, 2018.\\n\\nPhong T Lee, Marc R Dweck, Sparsh Prasher, Anoop Shah, Steve E Humphries, Dudley J Pennell, Hugh E Montgomery, and John R Payne. Left ventricular wall thickness and the presence of asymmetric hypertrophy in healthy young army recruits: data from the large heart study. Circulation: Cardiovascular Imaging, 6(2):262\u2013267, 2013.\\n\\nXueyan Mei, Zelong Liu, Philip M Robson, Brett Marinelli, Mingqian Huang, Amish Doshi, Adam Jacobi, Chendi Cao, Katherine E Link, Thomas Yang, et al. Radimagenet: An open radiologic deep learning research dataset for effective transfer learning. Radiology: Artificial Intelligence, 4(5):e210315, 2022.\"}"}
