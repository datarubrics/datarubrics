{"id": "ttxAvIQA4i_", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "ttxAvIQA4i_", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Checklist\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] We include a clear list of our contributions at the end of Sec. 1.\\n   (b) Did you describe the limitations of your work? [Yes] We include the limitations of our work in Sec. 5.\\n   (c) Did you discuss any potential negative societal impacts of your work? [Yes] We discuss the societal impacts of our work in supplementary.\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n   (b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] We provide the url to both data and code in Sec. 1.\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] We provide the implementation and training details in supplementary.\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A] We follow the common experimental setting for video question-answering without multiple trials.\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] We provide the resources used for experiments in supplementary.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes]\\n   (b) Did you mention the license of the assets? [Yes] The authors listed no license for the existing dataset.\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We include new assets curated as a URL.\\n   (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] We provide the details of data collection and question-answering in supplementary.\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes] We provide the details for waging participants in supplementary.\"}"}
{"id": "ttxAvIQA4i_", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"EgoTaskQA: Understanding Human Tasks in Egocentric Videos\\n\\nBaoxiong Jia\\\\(^1\\\\), Ting Lei\\\\(^2\\\\), Song-Chun Zhu\\\\(^2\\\\), Siyuan Huang\\\\(^2\\\\)\\n\\n\\\\(^1\\\\)UCLA Center for Vision, Cognition, Learning, and Autonomy (VCLA)\\n\\n\\\\(^2\\\\)Beijing Institute for General Artificial Intelligence (BIGAI)\\n\\n\\\\(^3\\\\)Institute for Artificial Intelligence, Peking University\\n\\n\\\\(^4\\\\)Department of Automation, Tsinghua University\\n\\nbaoxiongjia@ucla.edu, ting_lei@pku.edu.cn, sczhu@bigai.ai, syhuang@bigai.ai\\n\\nAbstract\\n\\nUnderstanding human tasks through video observations is an essential capability of intelligent agents. The challenges of such capability lie in the difficulty of generating a detailed understanding of situated actions, their effects on object states (i.e., state changes), and their causal dependencies. These challenges are further aggravated by the natural parallelism from multi-tasking and partial observations in multi-agent collaboration. Most prior works leverage action localization or future prediction as an indirect metric for evaluating such task understanding from videos. To make a direct evaluation, we introduce the EgoTaskQA benchmark that provides a single home for the crucial dimensions of task understanding through question-answering on real-world egocentric videos. We meticulously design questions that target the understanding of (1) action dependencies and effects, (2) intents and goals, and (3) agents' beliefs about others. These questions are divided into four types, including descriptive (what status?), predictive (what will?), explanatory (what caused?), and counterfactual (what if?) to provide diagnostic analyses on spatial, temporal, and causal understandings of goal-oriented tasks.\\n\\nWe evaluate state-of-the-art video reasoning models on our benchmark and show their significant gaps between humans in understanding complex goal-oriented egocentric videos. We hope this effort will drive the vision community to move onward with goal-oriented video understanding and reasoning.\\n\\n1 Introduction\\n\\nThe study of human motion perception has suggested that humans perceive motion as goal-directed behaviors rather than plain pattern movements [1\u20133]. Developmental psychologists [4] categorized such an ability into two distinct mechanisms: (1) action-effect associations that the desired effects activate the corresponding action; and (2) simulative procedures, which argues that goal attribution comes from planning under the rational action principle in others' shoes. Both mechanisms require detailed knowledge of action dependencies and effects, agent's intents and goals, and beliefs about other agents. With such knowledge playing crucial roles in human cognitive development, learning them from visual observation is pivotal for building more intelligent agents.\\n\\nWork done during internship at BIGAI.\"}"}
{"id": "ttxAvIQA4i_", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As shown in Tab. 1, EgoTaskQA complements existing video reasoning benchmarks on various dimensions. With models exhibiting large performance gaps compared with humans, we devise diagnostic experiments to reveal both the easy and challenging spots in our benchmark. We hope such designs and analyses will foster new insights into goal-oriented activity understanding.\\n\\nContributions\\n\\nOur contributions include:\\n\\n1. Designing a new egocentric video question-answering benchmark based on the LEMMA dataset [11]. The LEMMA dataset collects egocentric videos in multiple domains of goal-oriented task understanding. To make the benchmark more general, we extend it with annotations consisting of object status, human-object and multi-agent relationships, and causal dependency structures between actions.\\n\\n2. By extending the LEMMA dataset with annotations, we present EgoTaskQA, a challenging egocentric, goal-oriented video question-answering benchmark. This benchmark considers how humans learn from visual observations to obtain knowledge for more in-depth evaluation metric for task understanding.\\n\\n3. We introduce a new question type called indirect reference understanding; see Fig. 1 for an example and more details in Sec. 3.\\n\\n4. EgoTaskQA includes four types of questions: (1) actions with world state transitions and their dependencies, (2) agents' goals, (3) agents' capabilities over spatial intervals with different question types, targeting semantics and question scopes. Note that we use both direct and indirect references on actions and objects, where the same color indicates the same referred actions (best viewed in color).\\n\\n5. We take a closer look at how humans learn from interacting with the world, we locate objects, change their positions and manipulate them in various ways, all presumably under visual control. Considering how humans learn from visual observations to obtain knowledge for more in-depth evaluation metric for task understanding.\\n\\n6. Others. With their essential roles in human cognitive development, we urge the need for a benchmark that addresses these missing dimensions in egocentric activity understanding.\\n\\n7. The past few years have witnessed significant progress in egocentric video understanding, especially predictive, explanatory, and counterfactual, to systematically test models' abilities in predicting next actions, explaining actions, and understanding the causal dependency structures between actions.\"}"}
{"id": "ttxAvIQA4i_", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: A comparison between EgoTaskQA and existing video question-answering benchmarks. We use \u201cworld\u201d for world model-related information, including action preconditions, post-effects, and dependencies. We use FPV as short for egocentric and TPV for third-person-view videos. We use MC as short for multiple-choice question-answering, and OP for open-answer question-answering.\\n\\n| Dataset   | Video Type | Question Scope | Question type | AnswerType | # questions |\\n|-----------|------------|----------------|---------------|------------|-------------|\\n|           |            | Dataset Video  | Question      |            |             |\\n|           |            | Question Scope | Scope         | Question   | AnswerType  | # questions |\\n| MarioQA   | TPV        | World          | Intents & Goals | Multi-agent | Descriptive | 188K         |\\n| OP        |            |                |               |            | Predictive  |             |\\n| OP        |            |                |               |            | Explanatory |             |\\n| OP        |            |                |               |            | Counterfactual |             |\\n| CLEVRER   | TPV        | World          | Intents & Goals | Multi-agent | Descriptive | 9K          |\\n| OP+MC     | FPV        | World          | Intents & Goals | Multi-agent | Descriptive | 282K         |\\n| MovieQA   | TPV        | World          | Intents & Goals | Multi-agent | Descriptive | 14K          |\\n| Social-IQ | TPV        | World          | Intents & Goals | Multi-agent | Descriptive | 7.5K         |\\n| TVQA      | TPV        | World          | Intents & Goals | Multi-agent | Descriptive | 152.5K       |\\n| TVQA+     | TPV        | World          | Intents & Goals | Multi-agent | Descriptive | 29.4K        |\\n| MSVD-QA   | TPV        | World          | Intents & Goals | Multi-agent | Descriptive | 50.5K        |\\n| MSRVTT-QA | TPV        | World          | Intents & Goals | Multi-agent | Descriptive | 243K         |\\n| Video-QA  | TPV        | World          | Intents & Goals | Multi-agent | Descriptive | 175K         |\\n| ActivityNet-QA | TPV | World | Intents & Goals | Multi-agent | Descriptive | 58K          |\\n| TGI-FQA   | TPV        | World          | Intents & Goals | Multi-agent | Descriptive | 165.2K       |\\n| How2QA    | TPV        | World          | Intents & Goals | Multi-agent | Descriptive | 44K          |\\n| HowToVQA69M | TPV | World | Intents & Goals | Multi-agent | Descriptive | 69M          |\\n| AGQA      | TPV        | World          | Intents & Goals | Multi-agent | Descriptive | 3.6M         |\\n| MNExT-QA  | TPV        | World          | Intents & Goals | Multi-agent | Descriptive | 52K          |\\n| STAR      | TPV        | World          | Intents & Goals | Multi-agent | Descriptive | 60K          |\\n| EgoVQA    | FPV        | World          | Intents & Goals | Multi-agent | Descriptive | 520          |\\n| EgoTaskQA (Ours) | FPV | World | Intents & Goals | Multi-agent | Descriptive | 40K          |\\n\\n\u2022 We construct a balanced video question-answering benchmark, EgoTaskQA, to measure models\u2019 capability in understanding action dependencies and effects, intents and goals, as well as beliefs in multi-agent scenarios. We procedurally generate four challenging types of questions (descriptive, predictive, explanatory, and counterfactual) with both direct and indirect references for our benchmark and potential research on video-grounded compositional reasoning.\\n\\n\u2022 We devise challenging benchmarking splits over EgoTaskQA to provide a systematic evaluation of goal-oriented reasoning and indirect reference understanding. We experiment with various state-of-the-art video reasoning models, show their performance gap compared with humans, and Acknowledgement analyze their strengths and weaknesses to promote future research on goal-oriented task understanding.\\n\\n2 Related Work\\n\\nAction as Inverse Planning\\n\\nAction understanding has been seen as an inverse planning problem on agents' mental states [14, 15]. Early studies formulate it as reasoning on the first-order logic formulae that describes actions' preconditions and post-effects [16, 17]. This symbolic formalism is later paired with domain-specific language and algorithms to become mainstays in robotics planning [18, 19]. In computer vision, similar attempts have been made to link visual observations with world states and actions [20\u201322]. Various methods treated actions as transformations on images to solve action-state recognition [23\u201327] and video prediction [28\u201330]. With the emerging interest in language-grounded understanding, Zellers et al. [31] proposed PIGLeT to study the binding between images, world states, and action descriptions. Padmakumar et al. [32] further studies the problem of language understanding and task execution by designing an intelligent embodied agent that can chat during task execution. However, these works are mostly limited to atomic actions, missing the important action dependency in task execution. To tackle this problem, instructional videos [33\u201336] are studied with its goal-oriented multi-step activities. In these videos, external knowledge [37, 38] can be used as guidance for advanced tasks like temporal dynamics learning [39] and visually grounded planning [40, 41]. Unfortunately, these videos highlight the instructions and include no task-level noise, which is much simpler than the partially observable, highly paralleled, multi-agent environment that humans learn from and as presented in our benchmark. These complexities make the goal-oriented action understanding a challenging task remaining to be solved.\\n\\nEgocentric Vision\\n\\nEgocentric vision offers a unique perspective for actively engaging with the world. Aside from traditional video understanding tasks like video summarization [60, 61], activity recognition [62\u201364] and future anticipation [65\u201369], egocentric videos provide fine-grained information for tasks like human-object interaction understanding [70\u201376] and gaze/attention prediction [77, 10]. With its natural reflectance of partial observability, egocentric videos are also used for social understanding tasks such as joint attention modeling [78, 79], perspective taking [80, 81] and communicative modeling [82, 7]. However, with various egocentric datasets curated over the\"}"}
{"id": "ttxAvIQA4i_", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"last decade [8, 60, 9], data and detailed annotations for human tasks are still largely missing. Large-scale daily lifelog datasets like EPIC-KITCHENS [12] and Ego4D [13] cover certain aspects of action-dependencies, effects, and social scenarios in their recordings, but are unsuitable for detailed annotation due to their size. The other stream of datasets collects activities by providing coarse task instructions to both single actor [83] and multiple agent collaborations [11]. They annotate tasks and compositional actions to reveal agents' execution and collaboration process for multi-step goal-directed tasks. Despite all the preferred characteristics of these goal-oriented activity videos, none of them successfully addressed action-dependencies and effects, nor multi-agent belief modeling.\\n\\nVideo Question-Answering Benchmarks\\n\\nVisual question-answering can be designed to evaluate a wide spectrum of model capabilities, spanning from visual concept recognition and spatial relationship reasoning [84\u201387], abstract reasoning [88\u201393], to common sense reasoning [94, 95]. In the temporal domain, synthetic environments are used for questions that involve simple action-effect reasoning [42, 43]. Crowdsourced videos [53, 52, 48, 55] are used for collecting questions on basic spatial-temporal reasoning like event counting [53], grounding [49], and episodic memory [13]. Recent advances in video question-answering aim for more profound reasoning capabilities. Gao et al. [45] leverages an indoor synthetic environment to generate questions on spatial relationships and simple action-effect reasoning from an egocentric perspective. Xiao et al. [57] designs NExT-QA containing questions about knowledge of the past, present, and future on both temporal and causal domains. Grunde-McLaughlin et al. [56] programmatically generates questions for compositional spatial-temporal reasoning and generalization. Wu et al. [58] focus on short atomic action clips for situated reasoning. Yi et al. [44] generates synthetic videos for studying counterfactual predictions on collisions. Zadeh et al. [47] collects questions for social intelligence evaluation. Nevertheless, none of these benchmarks addressed the aforementioned critical dimensions of goal-oriented activity understanding from a real-world egocentric perspective.\\n\\n3 The EgoTaskQA Benchmark\\n\\nThe EgoTaskQA benchmark contains 40K balanced question-answer pairs selected from 368K programmatically generated questions generated over 2K egocentric videos. We target the crucial dimensions for understanding goal-oriented human tasks, including action effects and dependencies, intent and goals, and multi-agent belief modeling. We further evaluate models' capabilities to describe, explain, anticipate, and make counterfactual predictions about goal-oriented events. A detailed comparison between EgoTaskQA and existing benchmarks is shown in Tab. 1.\\n\\n3.1 Data Collection\\n\\nWe select egocentric videos from the LEMMA dataset [11] as base video sources. Compared to similar egocentric datasets, human activities in LEMMA are highly goal-oriented and multi-tasked. These activities contain rich human-object interactions and action dependencies in both single-agent and two-agent collaboration scenarios. We take advantage of these desired characteristics and augment LEMMA with ground truths of object states, relationships, and agents' beliefs about others. More specifically, we augment LEMMA on the following aspects:\\n\\nWorld States\\n\\nWe focus on world states consisting of object states, object-object relationships, and human-object relationships. First, we build the vocabulary of relationships and state attributes from activity knowledge defined in previous works [37, 96]. We manually filter irrelevant relationships and attributes by removing dataset-specific (e.g., under the car) and detailed numerical (e.g., cut in three) relationships. Next, we gather similar relationships to obtain 48 relationships and 14 object attributes. This vocabulary covers spatial relationships (e.g., on top of), object affordances (e.g., openable), and time-varying attributes (e.g., shape). We build on top of action annotations from LEMMA and use Amazon Mechanical Turk (AMT) to annotate this information before and after the changing action for all time-varying objects. With these annotations, we reconstruct the transition chain for each interacted object and obtain their temporal status. We provide the complete list of relationships and object attributes in the supplementary.\"}"}
{"id": "ttxAvIQA4i_", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use machine-generated questions to evaluate models' task understanding capabilities. We focus on understanding visual hints for future actions (e.g., cutting watermelon into dice instead of pieces for making juice) and observed environmental constraints (e.g., the cup is already washed when the person pours juice), and their dependencies, (2) agents' intents and goals, and (3) agents' beliefs about others.\\n\\nWe design questions that pinpoint scopes, including (1) action preconditions, post-effects, and their dependencies, (2) agents' intents and goals, and (3) agents' beliefs about others.\\n\\n3.2 Question-Answer Generation\\n\\nWe annotate states and relationships for e.g., the actor is aware of the helper's action if the helper is not in sight. As this annotation is usually subjective, we take the majority vote of three workers as ground truth.\\n\\nWe instruct AMT workers to first go through the egocentric view video of both agents to get familiar with actions performed by the actor and the helper. Next, we ask AMT workers to replay the video of the 1's video in Fig. 1 as an example to visualize annotations in EgoTaskQA. We annotate states and relationships for\\n\\nWe design questions that pinpoint scopes, including (1) action preconditions, post-effects, and their dependencies, (2) agents' intents and goals, and (3) agents' beliefs about others.\"}"}
{"id": "ttxAvIQA4i_", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"If the person did not wash something, will he/she be able to change the shape of apple?\\n\\nWhat did the other person change on kettle while the person cut something?\\n\\nFigure 3: Generation and statistics of the question-answer pairs. (a) Question distribution according to question scope (top), question type (middle), and targeting semantics (bottom). (b) Examples of natural language questions and their corresponding executable programs. Operators and parameters of the program are represented by black and blue contour rectangles, respectively. (c) Answer distribution for the top 20 reasoning types and top 15 answers for each type before (left) and after (right) balancing, the reasoning types are abbreviated with the concatenation of their initial letters (e.g., DW AQ for descriptive, world, action, and query).\\n\\n- **Descriptive** questions evaluate the understanding of detailed spatial-temporal information. We provide spatial-temporal references in the questions to identify a unique interval for answering queries on objects and actions. These properties include object states and changes, relationships, human actions, and multi-agent-related information. We generate this type of question by randomly sampling an interval in the video clip and then gathering all related annotations for question generation. Answers in this category are generated based on the interval annotation and contain both open-ended queries and statement verifications.\\n\\n- **Predictive** questions aim at understanding intents and task planning. Given a video clip, we ask about possible future object states and actions for both the actor and the helper. These predictions include both direct predictions on actions and objects, as well as more challenging task-dependent predictions such as the executability of actions and the desired states of objects. Questions and answers for predictive questions are generated by gathering the future action/object annotations in a fixed window size after the truncated interval (i.e., unseen future video) in the long original video. Answers in this category are open-ended action, object, and state queries.\\n\\n- **Counterfactual** questions aim at understanding action preconditions and post-effects. Based on the causal trace of actions, we generate counterfactual questions with hypothetical conditions that certain actions in the sequence were not executed. Under this condition, we query both the affected and unaffected actions about their executability and whether the corresponding changes of object states associated with these actions will occur. We generate counterfactual questions by adding or removing actions in the causal trace and adjusting the depending actions' executability recursively. Answers in this category contain action executability verifications and object state queries.\\n\\n- **Explanatory** questions evaluate the understanding of task-related object changes as well as action preconditions and post-effects. Given the object state annotations and the causal trace, we query the cause of state changes, the leading factor that satisfies the preconditions of specific actions, as well as why would the post-effect of certain actions affects other actions in the video clip. We generate explanatory questions by querying both the annotations as well as the causal trace. Answers for explanatory questions contain both open-ended and verification queries.\\n\\n**Answer Generation**\\n\\nIn EgoTaskQA, we consider both open-answer queries and binary statement verifications. To generate question-answer pairs for these questions, we design both text templates and the corresponding functional program templates as shown in Fig. 3 (b). Each program consists of a sequence of modules for querying the answer from the annotations and the causal trace. We exhaustively execute all possible program instantiations on videos to obtain answers by substituting arguments with instances in the available sample space. As all questions take action grounding as a\"}"}
{"id": "ttxAvIQA4i_", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"prerequisite, we add indirect references (e.g., the first..., the action before...) to actions and objects when making substitutions to reflect this challenge; see details in supplementary. After this initial process, we obtain 368K question-answer pairs over 2K videos as the full question set.\\n\\nAnswer Distribution Balancing\\nWe balance our answer distribution to avoid shortcuts from exploiting imbalances. Following the scheme introduced in [56], we tag each question template with its scope, type, and the targeting semantic category (e.g., actions, objects, states) and use the composition of all tags as the unique reasoning type for each question. We balance binary verification questions to have an equal proportion of each answer within each reasoning type. For open-answer questions, we use rejection sampling to ensure that the top 20% frequent answers for each reasoning type do not appear as answers for more than 33.3% of questions in the same type. After balancing by reasoning types, we proportionally sample questions to obtain a 40K diverse and balanced question set with a 1:2 ratio of binary and open-answer questions. We visualize the statistics of questions and answers and the effect of answer balancing in Fig. 3. More details are provided in supplementary.\\n\\nBenchmark Splits\\nWe provide two benchmarking splits normal and indirect for video question-answering on EgoTaskQA. For the normal split, we randomly sample questions according to their answer distribution and reasoning types to have a 3:1:1 split over training, validation, and test sets. The indirect split is motivated by the fact that during task execution, actions, objects, and their changes are often strongly correlated. It leaves the chance for the model to perform well by simply over-fitting these strong correlations without thorough task understanding; see Sec. 4.2 for a more in-depth discussion. We leverage the indirect references in our question to inspect the models' capability to use the learned knowledge for multi-step reasoning and generalize them to indirect references without over-fitting. More specifically, we filter questions without indirect references and simple indirect reference questions without multiple reasoning steps (e.g., what is the first action this person did? what did the person do before action \\\"putting something\\\"?) from all question-answer pairs to form the training set, and split all indirect reference questions with multiple reasoning steps as validation and test sets. Under this setting, the indirect split has a portion of 2:1:1 for training, validation, and test sets, respectively. We leave the remaining discussion of the indirect split to Sec. 4.3.\\n\\n4 Experiments\\nIn this section, we evaluate and analyze the performance of video question-answering models on EgoTaskQA. We report how well models perform on different question scopes, types as well as targeting semantics on both normal and indirect splits. We also provide diagnostic experiments on the language modality to show the necessity of the indirect split.\\n\\nBaselines\\nIn our experiments, we evaluate six state-of-the-art video question-answering models: Vi-\\\\text{sualBERT} [97], PSAC [98], HME [99], HGA [100], HCRN [101], and ClipBERT [102]. VisualBERT is a VL-BERT model designed for vision-language tasks. PSAC uses positional self-attention and co-attention network blocks to fuse visual and language features. HME uses external memory blocks for both visual inputs and questions on top of an LSTM-based encoder-decoder structure. HGA formulates video question-answering by constructing graphs for both videos and questions and aligning them. HCRN adopts a hierarchical framework by stacking relational modules over motion, question, and visual features. ClipBERT leverages sparsely sampled video clips and grid features [103] in a transformer architecture and achieves state-of-the-art results on video question-answering. We formulate question-answering in EgoTaskQA as a classification problem over all answer vocabulary and use models' accuracy as the evaluation metric under different settings. We provide details on model implementation, hyperparameter selection, and the training process in supplementary.\\n\\n4.1 Comparative Analysis\\nWe provide experimental results of baseline models on the EgoTaskQA normal split in Tab. 2. Model performances are evaluated on question scopes, types, targeting semantics, and overall answer categories. To quantify the naturalness and correctness of questions and answers in the EgoTaskQA benchmark, we provide human evaluation following the consistency check introduced in [87, 56]. More specifically, we randomly sample 50 questions for each category and instruct AMT workers to evaluate the quality of the generated answer. Additionally, we compare all baseline models with a simple frequency-based baseline, namely \\\"Most Likely\\\" in Tab. 2, where we select the most likely answer for each category to answer all questions in that category.\"}"}
{"id": "ttxAvIQA4i_", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Model performance on the EgoTaskQA normal split.\\n\\n| Category          | Most Likely | VisualBERT [97] | PSAC [98] | HME [99] | HGA [100] | HCRN [101] | ClipBERT [102] | Human |\\n|-------------------|-------------|-----------------|-----------|----------|-----------|------------|----------------|-------|\\n| Scope             |             |                 |           |          |           |            |                 |       |\\n| world             | 18.62       | 39.73           | 40.76     | 41.91    | 38.82     | 44.27      | 42.15          | 74.15 |\\n| intent            | 2.54        | 44.51           | 46.19     | 48.92    | 42.12     | 49.77      | 40.94          | 82.60 |\\n| multi-agent       | 10.92       | 26.29           | 30.59     | 27.98    | 23.43     | 31.36      | 27.63          | 76.04 |\\n| Type              |             |                 |           |          |           |            |                 |       |\\n| descriptive       | 18.64       | 41.99           | 40.63     | 41.45    | 38.04     | 43.48      | 38.45          | 88.45 |\\n| predictive        | 1.57        | 30.37           | 31.98     | 35.88    | 25.57     | 36.56      | 31.50          | 88.45 |\\n| counterfactual    | 23.62       | 41.99           | 41.89     | 44.13    | 41.94     | 48.00      | 46.75          | 80.40 |\\n| explanatory       | 7.97        | 37.42           | 37.99     | 38.85    | 35.97     | 40.60      | 42.39          | 74.04 |\\n| Semantic          |             |                 |           |          |           |            |                 |       |\\n| action            | 10.05       | 15.02           | 14.75     | 14.99    | 15.08     | 14.92      | 22.91          | 70.05 |\\n| object            | 2.07        | 23.26           | 36.53     | 36.05    | 19.09     | 45.31      | 21.80          | 82.00 |\\n| state             | 6.05        | 59.20           | 61.89     | 63.44    | 55.65     | 68.28      | 54.36          | 80.00 |\\n| change            | 41.97       | 68.27           | 65.05     | 68.87    | 68.38     | 67.38      | 66.58          | 82.00 |\\n| Overall           |             |                 |           |          |           |            |                 |       |\\n| open              | 0.70        | 24.62           | 26.97     | 27.66    | 22.75     | 30.23      | 27.70          | 82.00 |\\n| binary            | 50.46       | 68.08           | 65.95     | 68.60    | 68.53     | 69.42      | 67.52          | 76.04 |\\n| all               | 15.40       | 37.93           | 38.90     | 40.16    | 36.77     | 42.20      | 39.87          | 80.00 |\\n\\nAs shown in Tab. 2, the low performance of the most likely answer proves that our answer distribution is correctly balanced. For certain categories (e.g., change), the most likely answer has relatively high accuracy (41.97%) as it covers both open-answer and binary questions. Next, we observe relative low human performance in certain categories (e.g., action and explanatory). This indicates that identifying causal dependency between actions and conducting multi-step reasoning is not a trivial task for humans as also discovered in [56]. However, we still observe a large gap between state-of-the-art models and human performance. Among all models, we find HME, HCRN, and ClipBERT to perform the best. This result is reasonable since they leverage different ways to provide better visual representations and interactions between video and language. Among all question scopes, we recognize a relatively low accuracy on multi-agent-related questions among all question scopes. It implies that understanding other agents' actions during task execution is still difficult without explicit modeling. It is significant in egocentric vision as a person's view changes dramatically, and only glances can be taken to acquire others' information. Meanwhile, we notice that these models perform relatively well for questions on states and changing attributes. We conjecture that this is attributed to the task knowledge embedded in textual descriptions of questions since actions, objects, and state changes are strongly correlated, as mentioned in Sec. 3.2.\\n\\n4.2 The Effectiveness of Language\\n\\nObject information\\n\\nWe found the object information in the texts to be highly beneficial for question-answering on task-related knowledge during initial experiments. Compared to the original LEMMA action annotation (e.g., drinking [cereal] with [cup]), we use verbs to refer to actions in EgoTaskQA and obfuscate object information at different levels (e.g., drink something with cup, drink something with something) as similarly done in [56, 58]. While both types of action references localize to the same action interval, it contains different levels of knowledge in the language modality. Intuitively, the combination of action verbs (e.g., cut) and targeting objects (e.g., watermelon) provide object state information (e.g., diced) under certain scenarios. Therefore, we compare models' performance at different levels of object information obfuscation. As shown in Fig. 4, we recognize a significant performance gain for all models by gradually removing object information obfuscation in text, i.e., substituting \u201csomething\u201d with the original object. This result supports the hypothesis that with fine-grained action annotations, we can learn task-related knowledge reasonably well by simply exploiting texts. It shares the same conclusion with recent works on leveraging text-based knowledge for helping instructional video understanding [104]. To further investigate the effectiveness of the language modality, we conduct ablative experiments on the EgoTaskQA normal split.\\n\\nLanguage-Only\\n\\nLanguage has been shown to provide knowledge that helps visual question-answering [105]. To study the role of language in EgoTaskQA, we design a text-only setting for VisualBERT and HCRN, testing BERT [106] and HCRN without vision against their vision-language counterparts. As shown in Tab. 3, the performance for most question categories dropped significantly. For the task of video question-answering, we should expect that dropping the vision branch will significantly affect the models' performance. As shown in Tab. 3, we observe the general performance for the two models decreased as we expected. Among all categories, the models' performance for the objects decreased the most, which is consistent with the fact that the object queries highly depend on the situation provided in the videos (e.g., which object changed its status in the video?). However, we\"}"}
{"id": "ttxAvIQA4i_", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"observe a slight performance gain on object state change. This further suggests that the knowledge of world state change, i.e., which object attribute could change under actions, is embedded within question texts. Models could exploit question texts to learn simple associations between attribute types and action verbs (e.g., cleanliness and wash, emptiness and pour, shape and cut, etc.).\\n\\n### 4.3 Generalizing to indirect references\\n\\nOn the EgoTaskQA indirect split, we evaluate models\u2019 capability to leverage learned task knowledge for solving more complicated indirect reference tasks. With the normal split allowing for shortcuts on action-state associations, the indirect split forbids such exploitation by differentiating references during training and testing. As shown in Tab. 4, we observe more significant performance drops in language-only models compared to their vision-language counterparts. More specifically, the performance of BERT and language-only HCRN dropped 20.8% and 26.3% on the \u201cchange\u201d category, where we observed potential exploitation on question texts in Sec. 4.2. This serves as a shred of evidence that the indirect split helps reduce the possibility of exploiting simple associations in texts.\\n\\nAs for baseline models, we recognize a common performance decrease shared by most models on the indirect split. Among them, we notice a significant performance drop for ClipBERT, which conflicts with the dominating role of large-scale pretrained vision-language models on various reasoning tasks. We suspect that this degeneration might originate from two lines of problems: (1) the model design on sampling fewer videos and aligning visual/text graphs directly, which conflicts with the intuition that detailed spatial-temporal information and reasoning is indispensable for grounding indirect references; and (2) adopting large-scale pre-trained models directly to a specific domain is non-trivial, especially with challenges in grounding knowledge to visual signals. Overall, our experiments on the\"}"}
{"id": "ttxAvIQA4i_", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Conclusions\\n\\nWe introduce the EgoTaskQA benchmark to systematically evaluate models\u2019 understanding of goal-oriented activities from an egocentric perspective. We annotate object states, relationships, and agents\u2019 beliefs on the LEMMA dataset. We generate diverse questions covering different reasoning capabilities and target the crucial dimensions of task understanding: action dependencies and effects, agents\u2019 intents and goals, and belief modeling. We evaluate state-of-the-art video question-answering models and show their gaps compared with the human on two challenging splits, normal and indirect, to promote future study on indirect reference understanding and goal-oriented reasoning.\\n\\nEthics\\n\\nThe EgoTaskQA benchmark is built upon LEMMA and contains different subjects. As noted in LEMMA, the authors obtain consent from subjects by signing an agreement form with potential impacts adequately informed before recording. We mainly annotated objective world status and multi-agent information using multiple-choice selection and asked annotators to annotate the awareness of other subjects\u2019 actions with binary options for subjective annotations. For the annotation process, the workers\u2019 agreements are obtained by the publicly available annotation service platform AMT we adopted. In summary, we forbid subjective comments with no personally identifiable information revealed, and all participants\u2019 agreements are well addressed.\\n\\nLimitations\\n\\nOur work is primarily limited to two aspects. (i) Constrained by the data and annotation complexity, the scope of our activities is limited to indoor goal-oriented tasks. We believe that adding more diverse activities to EgoTaskQA will further increase its value as a general benchmark. (ii) Although EgoTaskQA supports many more additional supervisions, we currently limit our discussions to the six state-of-the-art models to have a fair comparison. With the current result showing insights into problems and challenges in EgoTaskQA, we leave the exploration of model design to future work and briefly discuss the potential solutions as follows. Meanwhile, we will continue this data curation for a broader range of human activities.\\n\\nFuture work\\n\\nWe plan to investigate the following two branches in the future, (i) explicit spatial-temporal grounding for modularized video QA models and (ii) prompting large-scale pre-trained models (both visual and language) for the domain-specific video QA challenges. Firstly, egocentric data can provide finer information and ease the challenge of grounding in modularized neuro-symbolic models. This could complement existing video reasoning methods and test the potential of neuro-symbolic models on complex reasoning tasks from a real-world, multi-agent, and causal perspective. Next, with increasing efforts in adapting large-scale pre-trained models for reasoning, our experiments suggest that adopting such models directly to a specific domain is non-trivial. Compared to their capabilities in commonsense reasoning, how to enable pre-trained models with the ability to fastly adapt to complex reasoning tasks still remains an interesting problem to be solved.\\n\\nBroader impact\\n\\nWith most existing intelligent robots depending on the understanding of world states to act and plan, we hope that the augmented LEMMA dataset can bridge the study of world-model learning in simulated environments and real-world complex event understanding. Additionally, we believe the EgoTaskQA benchmark proposes challenges on goal-oriented reasoning and hope such efforts can foster research in broader video understanding directions, including video-language understanding, spatial-temporal grounding, task learning, future anticipation. We also see its potential in imitation learning and knowledge acquisition, which will further drive the study of intelligent assistive robots that can perform tasks coordinately with humans.\\n\\nPublic access\\n\\nWe host our videos, annotations, metadata, and question-answering pairs on our website. We provide videos in .mp4 format, metadata, annotations in JSON and pandas DataFrames, and question-answer pairs with their corresponding metadata in JSON format. We make our data publicly available under the CC BY-NC-SA license, which allows reusers to distribute, remix, adapt and build upon the material for noncommercial purposes only and only so long as attribution is given to the creator. We bear all responsibility in case of violation of rights.\\n\\nAcknowledgement\\n\\nWe thank all colleagues from VCLA and BIGAI for fruitful discussions. We would also like to thank the anonymous reviewers for their constructive feedback. This work reported herein was supported by National Key R&D Program of China (2021ZD0150200).\"}"}
{"id": "ttxAvIQA4i_", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Dare A Baldwin, Jodie A Baird, Megan M Saylor, and M Angela Clark. Infants parse dynamic action. *Child development*, 72(3):708\u2013717, 2001.\\n\\n[2] Gy\u00f6rgy Gergely, Harold Bekkering, and Ildik\u00f3 Kir\u00e1ly. Rational imitation in preverbal infants. *Nature*, 415(6873):755\u2013755, 2002.\\n\\n[3] Amanda L Woodward. Infants selectively encode the goal object of an actor's reach. *Cognition*, 69(1):1\u201334, 1998.\\n\\n[4] Gergely Csibra and Gy\u00f6rgy Gergely. \u2018obsessed with goals\u2019: Functions and mechanisms of teleological interpretation of actions in humans. *Acta psychologica*, 2007.\\n\\n[5] Michael Land, Neil Mennie, and Jennifer Rusted. The roles of vision and eye movements in the control of activities of daily living. *Perception*, 28(11):1311\u20131328, 1999.\\n\\n[6] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3M: A universal visual representation for robot manipulation. *arXiv preprint arXiv:2203.12601*, 2022.\\n\\n[7] Lifeng Fan, Shuwen Qiu, Zilong Zheng, Tao Gao, Song-Chun Zhu, and Yixin Zhu. Learning triadic belief dynamics in nonverbal communication from videos. In *Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)*, 2021.\\n\\n[8] Hamed Pirsiavash and Deva Ramanan. Detecting activities of daily living in first-person camera views. In *Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)*, 2012.\\n\\n[9] Gunnar A Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, and Karteek Alahari. Charades-ego: A large-scale dataset of paired third and first person videos. *arXiv preprint arXiv:1804.09626*, 2018.\\n\\n[10] Yin Li, Miao Liu, and James M Rehg. In the eye of beholder: Joint learning of gaze and actions in first person video. In *Proceedings of European Conference on Computer Vision (ECCV)*, 2018.\\n\\n[11] Baoxiong Jia, Yixin Chen, Siyuan Huang, Yixin Zhu, and Song-chun Zhu. Lemma: A multi-view dataset for learning multi-agent multi-task activities. In *Proceedings of European Conference on Computer Vision (ECCV)*, 2020.\\n\\n[12] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kaza-Kos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. *International Journal of Computer Vision (IJCV)*, 130(1):33\u201355, 2022.\\n\\n[13] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. *Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)*, 2022.\\n\\n[14] Chris L Baker, Rebecca Saxe, and Joshua B Tenenbaum. Action understanding as inverse planning. *Cognition*, 113(3):329\u2013349, 2009.\\n\\n[15] Michael Shum, Max Kleiman-Weiner, Michael L Littman, and Joshua B Tenenbaum. Theory of minds: Understanding behavior in groups through inverse planning. In *Proceedings of AAAI Conference on Artificial Intelligence (AAAI)*, 2019.\\n\\n[16] John McCarthy. Situations, actions, and causal laws. Technical report, STANFORD UNIV CA DEPT OF COMPUTER SCIENCE, 1963.\\n\\n[17] Raymond Reiter. The frame problem in the situation calculus: A simple solution (sometimes) and a completeness result for goal regression. In *Artificial and Mathematical Theory of Computation*, 1991.\"}"}
{"id": "ttxAvIQA4i_", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Richard E. Fikes and Nils J. Nilsson. Strips: A new approach to the application of theorem proving to problem solving. *Artificial Intelligence*, 2(3-4):189\u2013208, 1971.\\n\\nDrew McDermott, Malik Ghallab, Adele Howe, Craig Knoblock, Ashwin Ram, Manuela Veloso, Daniel Weld, and David Wilkins. PDDL\u2014the planning domain definition language. 1998.\\n\\nKun Duan, Devi Parikh, David Crandall, and Kristen Grauman. Discovering localized attributes for fine-grained recognition. In *Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)*, 2012.\\n\\nPhillip Isola, Joseph J. Lim, and Edward H. Adelson. Discovering states and transformations in image collections. In *Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)*, 2015.\\n\\nTushar Nagarajan and Kristen Grauman. Attributes as operators: factorizing unseen attribute-object compositions. In *Proceedings of European Conference on Computer Vision (ECCV)*, 2018.\\n\\nAlireza Fathi and James M. Rehg. Modeling actions through state changes. In *Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)*, 2013.\\n\\nAmy Fire and Song-Chun Zhu. Learning perceptual causality from video. *ACM Transactions on Intelligent Systems and Technology (TIST)*, 7(2):1\u201322, 2015.\\n\\nXiaolong Wang, Ali Farhadi, and Abhinav Gupta. Actions transformations. In *Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)*, 2016.\\n\\nJean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, and Simon Lacoste-Julien. Joint discovery of object states and manipulation actions. In *Proceedings of International Conference on Computer Vision (ICCV)*, 2017.\\n\\nYang Liu, Ping Wei, and Song-Chun Zhu. Jointly recognizing object fluents and tasks in egocentric videos. In *Proceedings of International Conference on Computer Vision (ICCV)*, pages 2924\u20132932, 2017.\\n\\nJunhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L. Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in Atari games. *Proceedings of Advances in Neural Information Processing Systems (NeurIPS)*, 2015.\\n\\nCarl von Ondruck, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. *Advances in neural information processing systems*, 29, 2016.\\n\\nDavid Ha and J\u00fcrgen Schmidhuber. Recurrent world models facilitate policy evolution. *Proceedings of Advances in Neural Information Processing Systems (NeurIPS)*, 2018.\\n\\nRowan Zellers, Ari Holtzman, Matthew Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali Farhadi, and Yejin Choi. Piglet: Language grounding through neuro-symbolic interaction in a 3d world. *Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)*, 2021.\\n\\nAishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, and Dilek Hakkani-Tur. Teach: Task-driven embodied agents that chat. In *Proceedings of AAAI Conference on Artificial Intelligence (AAAI)*, 2022.\\n\\nHilde Kuehne, Ali Arslan, and Thomas Serre. The language of actions: Recovering the syntax and semantics of goal-directed human activities. In *Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)*, 2014.\\n\\nJean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Josef Sivic, Ivan Laptev, and Simon Lacoste-Julien. Unsupervised learning from narrated instruction videos. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 4575\u20134583, 2016.\"}"}
{"id": "ttxAvIQA4i_", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: A large-scale dataset for comprehensive instructional video analysis. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\nAntoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In Proceedings of International Conference on Computer Vision (ICCV), 2019.\\n\\nDavid Paulius, Yongqiang Huang, Roger Milton, William D Buchanan, Jeanine Sam, and Yu Sun. Functional object-oriented network for manipulation learning. In Proceedings of International Conference on Intelligent Robots and Systems (IROS), 2016.\\n\\nMahnaz Koupaee and William Yang Wang. Wikihow: A large scale text summarization dataset. arXiv preprint arXiv:1810.09305, 2018.\\n\\nDave Epstein, Jiajun Wu, Cordelia Schmid, and Chen Sun. Learning temporal dynamics from cycles in narrated video. In Proceedings of International Conference on Computer Vision (ICCV), 2021.\\n\\nChien-Yi Chang, De-An Huang, Danfei Xu, Ehsan Adeli, Li Fei-Fei, and Juan Carlos Niebles. Procedure planning in instructional videos. In Proceedings of European Conference on Computer Vision (ECCV), 2020.\\n\\nJiankai Sun, De-An Huang, Bo Lu, Yun-Hui Liu, Bolei Zhou, and Animesh Garg. Plate: Visually-grounded planning with transformers in procedural tasks. IEEE Robotics and Automation Letters (RAL), 7(2):4924\u20134930, 2022.\\n\\nJonghwan Mun, Paul Hongsuck Seo, Ilchae Jung, and Bohyung Han. Marioqa: Answering questions by watching gameplay videos. In Proceedings of International Conference on Computer Vision (ICCV), 2017.\\n\\nKyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and Byoung-Tak Zhang. Deepstory: Video story QA by deep embedded memory networks. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI), 2017.\\n\\nKexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B. Tenenbaum. CLEVRER: Collision events for video representation and reasoning. In International Conference on Learning Representations (ICLR), 2020.\\n\\nDifei Gao, Ruiping Wang, Ziyi Bai, and Xilin Chen. Env-qa: A video question answering benchmark for comprehensive understanding of dynamic environments. In Proceedings of International Conference on Computer Vision (ICCV), 2021.\\n\\nMakarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Movieqa: Understanding stories in movies through question-answering. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\\n\\nAmir Zadeh, Michael Chan, Paul Pu Liang, Edmund Tong, and Louis-Philippe Morency. Social-iq: A question answering benchmark for artificial social intelligence. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\nJie Lei, Licheng Yu, Mohit Bansal, and Tamara L. Berg. TVQA: Localized, compositional video question answering. In Proceedings of the conference on Empirical Methods in Natural Language Processing (EMNLP), 2018.\\n\\nJie Lei, Licheng Yu, Tamara L. Berg, and Mohit Bansal. TVQA+: Spatio-temporal grounding for video question answering. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2020.\\n\\nDejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of ACM International Conference on Multimedia (MM), 2017.\"}"}
{"id": "ttxAvIQA4i_", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kuo-Hao Zeng, Tseng-Hung Chen, Ching-Yao Chuang, Yuan-Hong Liao, Juan Carlos Niebles, and Min Sun. Leveraging video descriptions to learn video question answering. In Proceedings of AAAI Conference on Artificial Intelligence (AAAI), 2017.\\n\\nZhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: A dataset for understanding complex web videos via question answering. In Proceedings of AAAI Conference on Artificial Intelligence (AAAI), 2019.\\n\\nYunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. TGIF-QA: toward spatio-temporal reasoning in visual question answering. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\\n\\nLinjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. HERO: hierarchical encoder for video+language omni-representation pre-training. In Proceedings of the conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.\\n\\nAntoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer questions from millions of narrated videos. In Proceedings of International Conference on Computer Vision (ICCV), 2021.\\n\\nMadeleine Grunde-McLaughlin, Ranjay Krishna, and Maneesh Agrawala. AGQA: A benchmark for compositional spatio-temporal reasoning. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nJunbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nBo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum, and Chuang Gan. Star: A benchmark for situated reasoning in real-world videos. In Proceedings of Advances in Neural Information Processing Systems Datasets and Benchmarks (NeurIPS Datasets and Benchmarks Track), 2021.\\n\\nChenyou Fan. EgoVQA: An egocentric video question answering benchmark dataset. In Proceedings of International Conference on Computer Vision Workshops (ICCVW), 2019.\\n\\nYong Jae Lee, Joydeep Ghosh, and Kristen Grauman. Discovering important people and objects for egocentric video summarization. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\\n\\nZheng Lu and Kristen Grauman. Story-driven summarization for egocentric video. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2013.\\n\\nChristoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of International Conference on Computer Vision (ICCV), pages 6202\u20136211, 2019.\\n\\nChao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition. Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nRohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, and Ishan Misra. Omnivore: A Single Model for Many Visual Modalities. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nTushar Nagarajan, Yanghao Li, Christoph Feichtenhofer, and Kristen Grauman. Ego-topo: Environment affordances from egocentric video. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\nAntonino Furnari and Giovanni Maria Farinella. Rolling-unrolling lstms for action anticipation from first-person video. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 43(11):4021\u20134036, 2020.\"}"}
{"id": "ttxAvIQA4i_", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Siyuan Qi, Baoxiong Jia, and Song-Chun Zhu. Generalized earley parser: Bridging symbolic grammars and sequence data for future prediction. In Proceedings of International Conference on Machine Learning (ICML), 2018.\\n\\nSiyuan Qi, Baoxiong Jia, Siyuan Huang, Ping Wei, and Song-Chun Zhu. A generalized earley parser for human activity parsing and prediction. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(8):2538\u20132554, 2020.\\n\\nRohit Girdhar and Kristen Grauman. Anticipative video transformer. In Proceedings of International Conference on Computer Vision (ICCV), 2021.\\n\\nTushar Nagarajan, Christoph Feichtenhofer, and Kristen Grauman. Grounded human-object interaction hotspots from video. In Proceedings of International Conference on Computer Vision (ICCV), 2019.\\n\\nDima Damen, Teesid Leelasawassuk, and Walterio Mayol-Cuevas. You-do, i-learn: Egocentric unsupervised discovery of objects and their modes of interaction towards video-based guidance. Computer Vision and Image Understanding (CVIU), 149:98\u2013112, 2016.\\n\\nMinjie Cai, Kris M Kitani, and Yoichi Sato. Understanding hand-object manipulation with grasp types and object attributes. In Proceedings of Robotics: Science and Systems (RSS), 2016.\\n\\nSven Bambach, Stefan Lee, David J Crandall, and Chen Yu. Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions. In Proceedings of International Conference on Computer Vision (ICCV), 2015.\\n\\nSiyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen, and Song-Chun Zhu. Learning human-object interactions by graph parsing neural networks. In Proceedings of European Conference on Computer Vision (ECCV), 2018.\\n\\nGuillermo Garcia-Hernando, Shanxin Yuan, Seungryul Baek, and Tae-Kyun Kim. First-person hand action benchmark with rgb-d videos and 3d hand pose annotations. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\nMinghuang Ma, Haoqi Fan, and Kris M Kitani. Going deeper into first-person activity recognition. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\\n\\nPing Wei, Yang Liu, Tianmin Shu, Nanning Zheng, and Song-Chun Zhu. Where and why are they looking? jointly inferring human attention and intentions in complex tasks. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\nAlircza Fathi, Jessica K Hodgins, and James M Rehg. Social interactions: A first-person perspective. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\\n\\nHyun Soo Park and Jianbo Shi. Social saliency prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4777\u20134785, 2015.\\n\\nTakuma Yagi, Karttikeya Mangalam, Ryo Yonetani, and Yoichi Sato. Future person localization in first-person videos. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\nEvonne Ng, Donglai Xiang, Hanbyul Joo, and Kristen Grauman. You2me: Inferring body pose in egocentric video via first and second person interactions. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\nCurtis Northcutt, Shengxin Zha, Steven Lovegrove, and Richard Newcombe. Egocom: A multi-person multi-modal egocentric communications dataset. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020.\"}"}
{"id": "ttxAvIQA4i_", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Nishant Rai, Haofeng Chen, Jingwei Ji, Rishi Desai, Kazuki Kozuka, Shun Ishizaka, Ehsan Adeli, and Juan Carlos Niebles. Home action genome: Cooperative compositional action understanding. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nKewei Tu, Meng Meng, Mun Wai Lee, Tae Eun Choe, and Song-Chun Zhu. Joint video and text parsing for understanding events and answering queries. IEEE MultiMedia, 21(2):42\u201370, 2014.\\n\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. VQA: Visual question answering. In Proceedings of International Conference on Computer Vision (ICCV), 2015.\\n\\nJustin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\\n\\nDrew A Hudson and Christopher D Manning. GQA: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\nDavid Barrett, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap. Measuring abstract reasoning in neural networks. In Proceedings of International Conference on Machine Learning (ICML). PMLR, 2018.\\n\\nChi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: A dataset for relational and analogical visual reasoning. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\nWeili Nie, Zhiding Yu, Lei Mao, Ankit B Patel, Yuke Zhu, and Anima Anandkumar. Bongard-logo: A new benchmark for human-level concept learning and reasoning. Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\nChi Zhang, Baoxiong Jia, Song-Chun Zhu, and Yixin Zhu. Abstract spatial-temporal reasoning via probabilistic abduction and execution. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nChi Zhang, Baoxiong Jia, Mark Edmonds, Song-Chun Zhu, and Yixin Zhu. ACRE: Abstract causal reasoning beyond covariation. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nChi Zhang, Sirui Xie, Baoxiong Jia, Ying Nian Wu, Song-Chun Zhu, and Yixin Zhu. Learning algebraic representation for systematic generalization in abstract reasoning. In Proceedings of European Conference on Computer Vision (ECCV), 2022.\\n\\nJae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, and Yejin Choi. Visualcomet: Reasoning about the dynamic context of a still image. In European Conference on Computer Vision, pages 508\u2013524. Springer, 2020.\\n\\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\nJingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan Carlos Niebles. Action genome: Actions as compositions of spatio-temporal scene graphs. In Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.\"}"}
