{"id": "pYNl76onJL", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models\\n\\nWenhao Wang\\nUniversity of Technology Sydney\\nwangwenhao0716@gmail.com\\n\\nYi Yang\\n\u2217Zhejiang University\\nyangyics@zju.edu.cn\\n\\nFigure 1: VidProM is the first dataset featuring 1.67 million unique text-to-video prompts and 6.69 million videos generated from 4 different state-of-the-art diffusion models. It inspires many exciting new research areas, such as Text-to-Video Prompt Engineering, Efficient Video Generation, Fake Video Detection, and Video Copy Detection for Diffusion Models.\\n\\nAbstract\\n\\nThe arrival of Sora marks a new era for text-to-video diffusion models, bringing significant advancements in video generation and potential applications. However, Sora, along with other text-to-video diffusion models, is highly reliant on prompts, and there is no publicly available dataset that features a study of text-to-video prompts. In this paper, we introduce VidProM, the first large-scale dataset comprising 1.67 Million unique text-to-video prompts from real users. Additionally, this dataset includes 6.69 million videos generated by four state-of-the-art diffusion models, alongside some related data. We initially discuss the curation of this large-scale dataset, a process that is both time-consuming and costly. Subsequently, we underscore the need for a new prompt dataset specifically designed for text-to-video generation by illustrating how VidProM differs from DiffusionDB, a large-scale prompt-gallery dataset for image generation. Our extensive and diverse dataset also opens up many exciting new research areas. For instance, we suggest\\n\\n\u2217Corresponding Author.\\n\\n38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.\"}"}
{"id": "pYNl76onJL", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"exploring text-to-video prompt engineering, efficient video generation, and video copy detection for diffusion models to develop better, more efficient, and safer models. The project (including the collected dataset VidProM and related code) is publicly available at https://vidprom.github.io under the CC-BY-NC 4.0 License.\\n\\n1 Introduction\\nThe Sora [1] initializes a new era for text-to-video diffusion models, revolutionizing video generation with significant advancements. This breakthrough provides new possibilities for storytelling, immersive experiences, and content creation, as Sora [1] transforms textual descriptions into high-quality videos with ease. However, Sora [1] and other text-to-video diffusion models [2, 3, 4, 5] heavily relies on the prompts used. Despite their importance, there is no publicly available dataset focusing on text-to-video prompts, which may hinder the development of these models and related researches.\\n\\nIn this paper, we present the first systematic research on the text-to-video prompts. Specifically, our efforts primarily focus on building the first text-to-video prompt-gallery dataset VidProM, analyzing the necessity of collecting a new prompt dataset specialized for text-to-video diffusion models, and introducing new research directions based on our VidProM. The demonstration of VidProM is shown in Fig. 1.\\n\\n\u2022 The first text-to-video prompt-gallery dataset. Our large-scale VidProM includes 1.67 million unique text-to-video prompts from real users and 6.69 million generated videos by 4 state-of-the-art diffusion models. The prompts are from official Pika Discord channels, and the videos are generated by Pika [2], Text2Video-Zero [3], VideoCraft2 [4], and ModelScope [5]. We distribute the generation process across 10 servers, each equipped with 8 Nvidia V100 GPUs. Each prompt is embedded using the powerful text-embedding-3-large model from OpenAI and assigned six not-safe-for-work (NSFW) probabilities, consisting of toxicity, obscenity, identity attack, insult, threat, and sexual explicitness. We also add a Universally Unique Identifier (UUID) and a time stamp to each data point in our VidProM. In addition to the main dataset, we introduce a subset named VidProS, which consists of semantically unique prompts. That means, in this subset, the cosine similarity between any two prompts is less than 0.8, ensuring a high level of semantic diversity.\\n\\n\u2022 The necessity of collecting a new prompt dataset specialized for text-to-video diffusion models. We notice that there exists a text-to-image prompt-gallery dataset, DiffusionDB [6]. By analyzing the basic information and the prompts, we conclude that the differences between our VidProM and DiffusionDB [6] mainly lies in: (1) Semantics: The semantics of our prompts are significantly different from those in DiffusionDB, with our text-to-video prompts generally being more dynamic, more complex, and longer. (2) Modality: DiffusionDB [6] focuses on images, while our VidProM is specialized in videos. (3) Techniques: We utilize some recent advanced techniques, such as latest text embedding model (OpenAI-text-embedding-3-large), to build our VidProM.\\n\\n\u2022 Inspiring new research directions. The introduction of our new text-to-video prompt-gallery dataset, VidProM, opens up many exciting research directions. With the help of our VidProM, researchers can develop better, more efficient, and safer text-to-video diffusion models: (1) For better models, researchers can utilize our VidProM as a comprehensive set of prompts to evaluate their trained models, distill new models using our prompt-(generated)-video pairs, and engage in prompt engineering. (2) For more efficient models, researchers can search for related prompts in our VidProM and reconstruct new videos from similar existing videos, thereby avoiding the need to generate videos from scratch. (3) For safer models, researchers can develop specialized models to distinguish generated videos from real videos to combat misinformation, and train video copy detection models to identify potential copyright issues.\\n\\nTo sum up, this paper makes the following contributions: (1) We contribute the first text-to-video prompt-gallery dataset, VidProM, which includes 1.67 million unique prompts from real users and 6.69 million generated videos by 4 state-of-the-art diffusion models. (2) We highlight the necessity of collecting a new prompt dataset specialized for text-to-video diffusion models by comparing our VidProM with DiffusionDB. (3) We reveal several exciting research directions inspired by VidProM and position it as a rich database for future studies.\"}"}
{"id": "pYNl76onJL", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Text-Video Datasets.\\n\\nText-to-video diffusion models \\\\([3, 7, 4, 8, 5, 1, 9, 10, 2, 11, 12]\\\\) have become a powerful tool for producing high-resolution videos. In contrast, our VidProM contains prompts authored by real users to generate complete videos: Pika, VideoCraft2, Text2Video-Zero, and ModelScope. To better understand our video-projection model, we first outline the related works and then introduce the proposed VidProM.\\n\\n2 Related Works\\n\\nWhile several published text-video datasets exist \\\\([13, 14, 15, 16, 17, 18, 19, 20]\\\\), they primarily consist of caption-(real)-video pairs rather than prompt-(generated)-video pairs. For instance, HDVILA-100M \\\\([17]\\\\) is a comprehensive video-language dataset for multimodal representation learning, offering high-resolution and diverse content. HDVILA-100M evolves from a text-to-image model by adding spatio-temporal blocks for consistent frame generation and smooth movement transitions. This paper uses these four publicly accessible sources (access to certain datasets requires NUS credentials) for constructing our VidProM. We hope that the collection of 3072-dim Prompt Embedding Pika VideoCraft2 Text2Video-Zero ModelScope\\n\\nFigure 2: A data point in the proposed VidProM. Please click the corresponding links to view the data.\\n\\n### 2.2 Existing Datasets\\n\\n- **VideoCrafter2**\\n  - Producing high-quality video content from textual prompts.\\n- **Pika**\\n  - Enabling exciting space adventures and exploring the vastness of the universe with realistic spacecraft visuals.\\n- **ModelScope**\\n  - Offering a diverse range of video content, from serene landscapes to dynamic space missions.\\n- **Text2Video-Zero**\\n  - Enhancing creativity with a wide array of video possibilities.\\n- **HDVILA-100M**\\n  - A rich dataset for developing video understanding models.\\n\\nThese datasets provide a foundation for our VidProM, which aims to bridge the gap between text and video content creation.\"}"}
{"id": "pYNl76onJL", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: The number of prompts with an NSFW probability greater than 0.2 constitutes only a very small fraction of our total of 1,672,243 unique prompts.\\n\\nPrompt Datasets. Existing datasets underscore the significance of compiling a set of prompts. In the text-to-text domain, studies [21] demonstrate that gathering and analyzing prompts can aid in developing language models that respond more effectively to prompts. PromptSource [22] recognizes the growing popularity of using prompts to train and query language models, and thus create a system for generating, sharing, and utilizing natural language prompts. In the text-to-image domain, DiffusionDB [6], which is nominated for the best paper of ACL 2023, collects a large-scale prompt-image dataset, revealing its potential to open up new avenues for research. Given the importance of prompt datasets and the new era of text-to-video generation brought by Sora [1], this paper presents the first prompt dataset specifically collected for text-to-video generation.\\n\\n3 Curating VidProM\\nIn Fig. 2, we illustrate a single data point in the proposed VidProM. This data point includes a prompt, a UUID, a timestamp, six NSFW probabilities, a 3072-dimensional prompt embedding, and four generated videos. We show the steps of curating our VidProM in this section.\\n\\nCollecting Source HTML Files. We gather chat messages from the official Pika Discord channels between July 2023 and February 2024 using DiscordChatExporter [23] and store them as HTML files. Our focus is on 10 channels where users input prompts and request a bot to execute the Pika text-to-video diffusion model for video generation. The user inputs and outputs are made available by Pika Lab under the Creative Commons Noncommercial 4.0 Attribution International License (CC BY-NC 4.0), as detailed in Section 4.5.a of their official terms of service. Consequently, the text-to-video prompts and Pika videos in our dataset are open-sourced under the same license.\\n\\nExtracting and Embedding Prompts. The HTML files are then processed using regular expressions to extract prompts and timestamps. We subsequently filter out prompts used for image-to-video generation (because the images are not publicly available) and prompts without associated videos (these prompts may have been banned by Pika or hidden by the users). Finally, we remove duplicate prompts and assign a UUID to each prompt, resulting in a total of 1,672,243 unique prompts. Because the text-to-video prompts are significantly complex and long, we use OpenAI\u2019s text-embedding-3-largest API, which supports up to 8192 tokens, to embed all of our prompts. We retain the original 3072-dimensional output, allowing any customized dimensionality reduction.\\n\\nAssigning NSFW Probabilities. We select the public Discord channels of Pika Labs, which prohibit NSFW content, as the source for our text-to-video prompts. Consequently, if a user submits a harmful prompt, the channel will automatically reject it. However, we find VidProM still includes NSFW prompts that were not filtered by Pika. We employ a state-of-the-art NSFW model, Detoxify [24], to assign probabilities in six aspects of NSFW content, including toxicity, obscenity, identity attack, insult, threat, and sexual explicitness, to each prompt. In Fig. 3, we visualize the number of prompts with a NSFW probability greater than 0.2. We conclude that only a very small fraction (less than 4% in 2023) of prompts are NSFW.\"}"}
{"id": "pYNl76onJL", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: The comparison of basic information of VidProM and DiffusionDB [6]. To ensure a fair comparison of semantically unique prompts, we use the text-embedding-3-large API to re-embed prompts in DiffusionDB [6].\\n\\n| Aspects                          | Details                  |\\n|---------------------------------|--------------------------|\\n| No. of unique prompts           | DiffusionDB [6]: 1,819,808, VidProM: 1,672,243 |\\n| No. of semantically unique prompts | DiffusionDB [6]: 739,010, VidProM: 1,038,805 |\\n| Embedding of prompts            | OpenAI-CLIP, OpenAI-text-embedding-3-large |\\n| Maximum length of prompts       | DiffusionDB [6]: 77 tokens, VidProM: 8192 tokens |\\n| Time span                       | Aug 2022, Jul 2023, \u223cFeb 2024 |\\n| Images                          | No. of images/videos: DiffusionDB [6]: \u223c14 million images, VidProM: \u223c6.69 million videos |\\n| No. of sources                  | 1, 4 |\\n| Average repetition rate per source | DiffusionDB [6]: \u223c8.2, VidProM: 1 |\\n| Collection method               | Web scraping, Web scraping + Local generation |\\n| GPU consumption                 | V100 GPU hours: \u223c50,631, Total seconds: \u223c14,381,289.8 |\\n\\nWe provide six separate NSFW probabilities, enabling researchers to set a suitable threshold for filtering out potentially unsafe data for their tasks.\\n\\nScraping and Generating Videos. We enhance our VidProM diversity by not only scraping Pika videos from extracted links but also utilizing three state-of-the-art open-source text-to-video diffusion models for video generation. This process demands significant computational resources: we distribute the text-to-video generation across 10 servers, each equipped with 8 Nvidia V100 GPUs. It costs us approximately 50,631 GPU hours and results in 6.69 million videos (4 \u00d7 1,672,243), totaling 14,381,289.8 seconds in duration. The breakdown of video lengths is as follows: 3.0 seconds for Pika, 1.6 seconds for VideoCraft2, 2.0 seconds for Text2Video-Zero, and 2.0 seconds for ModelScope.\\n\\nSelecting Semantically Unique Prompts. Beyond general uniqueness, we introduce a new concept: semantically unique prompts. We define a dataset as containing only semantically unique prompts if, for any two arbitrary prompts, their cosine similarity calculated using text-embedding-3-large embeddings is less than 0.8. After semantic de-duplication (see a detailed description of this process in the Appendix (Section A)), our VidProM still contains 1,038,805 semantically unique prompts, and we denote it as VidProS. More semantically unique prompts imply covering a broader range of topics, increasing the diversity and richness of the content.\\n\\n4 The Necessity of Introducing VidProM\\n\\nWe notice that there exists a text-to-image prompt-gallery dataset, DiffusionDB [6]. This section highlights how our VidProM is different from this dataset from two aspects, i.e. basic information and semantics of prompts.\\n\\n4.1 Basic Information\\n\\nIn Table 1, we provide a comparison of the basic information between our VidProM and DiffusionDB [6]. We have the following observations:\\n\\n(1) Although the total number of unique prompts in our VidProM and DiffusionDB are similar, VidProM contains significantly more (+40.6%) semantically unique prompts. This shows VidProM is a more diverse and representative dataset.\\n\\n(2) Unlike DiffusionDB, which uses the OpenAI-CLIP embedding method, our approach leverages the latest OpenAI text-embedding model, namely text-embedding-3-large. One advantage of this approach is its ability to accept much longer prompts compared to CLIP, supporting up to 8192 tokens versus CLIP's 77 tokens. As illustrated by the comparison of the number of words per prompt between VidProM and DiffusionDB in Fig. 4, the prompts used for generating videos are much more longer. Therefore, the capability of text-embedding-3-large is particularly suitable for them. Another advantage is text-embedding-3-large has stronger performance than CLIP on several standard benchmarks, potentially benefiting users of our VidProM.\\n\\n(3) The time span for collecting prompts in VidProM is much longer than in DiffusionDB. We collect prompts written by real users over a period of 8 months, while DiffusionDB's collection spans only 1 month. A longer collection period implies a broader range of topics and themes covered, as demonstrated by the comparison of the number of semantically unique prompts.\"}"}
{"id": "pYNl76onJL", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"It had been flying for a long time and was feeling extremely parched. Finally, after a while, it spotted a small village with a few houses and trees.\\n\\nThis is a 1-minute realistic short video depicting a young person developing good habits by using a height-adjustable automated lift desk for work in a bright and minimalistic bedroom. The true-to-life footage vividly shows him working on the computer, writing at the desk, as well as clips of him standing up for exercise. The concise narration explains the practical benefits of cultivating healthy work and lifestyle habits with this type of smart furniture.\\n\\nTakeaway: Our VidProM dataset contains a larger number of semantically unique prompts, which are embedded by a more advanced model and collected over a longer period. Images or videos. DiffusionDB focuses on images, while our VidProM is specialized in videos. Therefore, given that generating videos is much more expensive than images, it is reasonable that the number of videos in VidProM is smaller than the number of images in DiffusionDB. We also make several efforts to mitigate this disadvantage: (1) The number of source diffusion models for our VidProM is much larger than those of DiffusionDB. Our videos are generated by 4 state-of-the-art text-to-video diffusion models, while DiffusionDB contains only images generated by Stable Diffusion. As a result, the average repetition rate per source is only 1 for our VidProM compared to about 8.2 for DiffusionDB. (2) We devote significantly more resources to VidProM. Unlike DiffusionDB, which only collects images through web scraping, we also deploy three open-source text-to-video models on our local servers, dedicating over 50,000 V100 GPU hours to video generation.\"}"}
{"id": "pYNl76onJL", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Takeaway: Our VidProM dataset contains a considerable amount of videos generated by various state-of-the-art text-to-video diffusion models, utilizing a substantial amount of resources.\\n\\n4.2 Semantics of prompts\\nIn this section, we analyze how the semantics of prompts in our VidProM dataset are different from DiffusionDB [6].\\n\\nFirstly, as shown in Fig. 4 (a), the semantics of the prompts differ in three aspects: (1) Time dimension: text-to-video prompts usually need to include a description of the time dimension, such as 'changes in actions' and 'transitions in scenes'; while text-to-image prompts typically describe a scene or object. (2) Dynamic description: text-to-video prompts often need to describe the dynamic behavior of objects, such as 'flying', 'working', and 'writing'; while text-to-image prompts focus more on describing the static appearance of objects. (3) Duration: text-to-video prompts may need to specify the duration of the video or an action, such as 'a long time' and '1-minute', while text-to-image prompts do not need to consider the time factor.\\n\\nSecondly, as shown in Fig. 4 (a) and (b), text-to-video prompts are generally more complex and longer than text-to-image prompts, due to the need to describe additional dimensions and dynamic changes. This phenomenon is also observed in the prompts used by Sora. For instance, the prompt for the 'Tokyo Girl' video contains 64 words, while the longest prompt on the OpenAI official website comprises 95 words. Our VidProM dataset prominently features this characteristic: (1) the number of prompts with more than 70 words is nearly 60,000 for our VidProM, compared to only about 15,000 for DiffusionDB; and (2) our VidProM still has over 25,000 prompts with more than 100 words, whereas this number is close to 0 for DiffusionDB.\"}"}
{"id": "pYNl76onJL", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Finally, as shown in Fig. 4 (c) and Fig. 5, the prompts from our VidProM dataset and DiffusionDB exhibit different distributions. We use the text-embedding-3-large model to re-extract the features of the prompts in DiffusionDB. For visualizing with t-SNE [25], we randomly select 10,000 prompt features from both our VidProM dataset and DiffusionDB, respectively. We find that these prompts have significantly different distributions and are nearly linearly separable. Beyond the traditional t-SNE [25], we use a recent method, WizMap [26], to analyze the topics preferred by people using all prompts from our VidProM and DiffusionDB. WizMap [26] is a visualization tool to navigate and interpret large-scale embedding spaces with ease. From the visualization results shown in Fig. 5, we conclude that, despite some overlaps, there are distinct differences in their interests. For example, both groups are interested in topics like cars, food, and cute animals. However, text-to-image users tend to focus on generating art paintings, whereas text-to-video users show little interest in this area. Instead, text-to-video users are more inclined to create general human activities, such as walking, which are rarely produced by text-to-image users.\\n\\nTakeaway: The significant difference in semantics between text-to-image prompts and our text-to-video prompts indicates the need to collect a new dataset of prompts specifically for video generation. By analyzing prompt usage distribution, future researchers can design generative models to better cater to popular topics in prompts.\\n\\n5 Inspiring New Research\\nThe proposed million-scale VidProM dataset inspires new directions for researchers to develop better, more efficient, and safer text-to-video diffusion models. Video Generative Model Evaluation aims to assess the performance and quality of text-to-video generative models. Current evaluation efforts, such as [27, 28, 29, 30], are conducted using carefully designed and small-scale prompts. Our VidProM dataset brings imagination to this field: (1) Instead of using carefully designed prompts, researchers could consider whether their models can generalize to prompts from real users, which would make their evaluation more practical. (2) Performing evaluations on large-scale datasets will make their arguments more convincing.\\n\\nText-to-Video Diffusion Model Development aims to create diffusion models capable of converting textual descriptions into dynamic and realistic videos. The current methods [3, 7, 4, 8, 5, 1, 9, 10, 2, 11, 12] are trained on caption-(real)-video pairs. Two natural questions arise: (1) Will the domain gap between captions and prompts from real users (see the evidence of existing domain gap in the Appendix (Section B)) hinder these models' performance? For instance, 60% of the training data for Open-Sora-Plan v1.0.0 [31] consist of landscape videos, leading to the trained model's suboptimal performance in generating scenes involving humans and cute animals. (2) Can researchers train or distill new text-to-video diffusion models on prompt-(generated)-video pairs? Although some studies indicate that this approach may lead to irreversible defects [32], as we will likely run out of data on websites and some methods are proposed to prevent such collapse [33], exploring synthetic data remains a valuable endeavor. The studies of training text-to-video diffusion models with our VidProM may provide answers to these two questions.\\n\\nText-to-Video Prompt Engineering is to optimize the interaction between humans and text-to-video models, ensuring that the models understand the task at hand and generate relevant, accurate, and coherent videos. The prompt engineering field has gained attention in large language models [34, 35], text-to-image diffusion models [36, 37], and visual in-context learning [38, 39]. However, as far as we know, there is no related research in the text-to-video community. Our VidProM provides an abundant resource for text-to-video prompt engineering. In the Section 6, we train a large language model on our VidProM for automatic text-to-video prompt completion for instance.\\n\\nEfficient Video Generation. The current text-to-video diffusion models are very time-consuming. For example, on a single V100 GPU, ModelScope [5] requires 43 seconds, while VideoCrafter2 [4] needs 51 seconds to generate a video, respectively. Our large-scale VidProM provides a unique opportunity for efficient video generation. Given an input prompt, a straightforward approach is to search for the most closely related prompts in our VidProM and reconstruct a video from the corresponding existing videos, instead of generating a new video from noise.\"}"}
{"id": "pYNl76onJL", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fake Video Detection aims to distinguish between real videos and those generated by diffusion models. While there are some works focusing on fake image detection, fake video detection presents unique challenges: (1) The generalization problem: Existing fake image detectors may not generalize well to video frames (see experiments in the Appendix). For instance, a model trained on images generated by Stable Diffusion may fail to identify frames from videos generated by Pika or Sora. (2) The efficiency problem: Currently, there is no detector that can take an entire video as input. As a result, to achieve higher accuracy, we may use fake image detectors to examine all or representative frames, which can be time-consuming.\\n\\nWith our VidProM, researchers can (1) train specialized Fake Video Detection models on millions of generated videos, and (2) use millions of prompts to generate more videos from more diffusion models to further improve the detection performance.\\n\\nVideo Copy Detection for Diffusion Models aims to answer whether videos generated by diffusion models replicate the contents of existing ones (see a detailed discussion in the Appendix). Videos generated with replicated content may infringe on the copyrights of the original videos. However, current video copy detection researches focus on detecting hand-crafted copies generated by transformations like horizontal flips and random crops, and overlook the challenge from diffusion models. Researchers can utilize our dataset VidProM as input queries and employ existing video copy detection models to pre-filter potential replications. Subsequently, these filtered videos can be manually annotated to determine whether they are indeed replications and regarded as training sources.\\n\\nAutomatic Text-to-video Prompt Completion\\n\\nThis section describes the training and output of our text-to-video prompt completion model.\\n\\nTraining details. Similar to text2image-prompt-generator, we treat the automatic text-to-video prompt completion as a general text generation task. The training code is available at: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py. We fine-tune the recent state-of-the-art large language model, Mistral-7B-v0.1, on our dataset VidProM. The fine-tuning process is completed using DeepSpeed with 8 A100 GPUs in 2 hours.\\n\\nDemonstration. We release our trained model on: https://huggingface.co/WenhaoWang/AutoT2VPrompt. It can be freely downloaded and tested under CC BY-NC 4.0 License, and we also demonstrate some of its generation across various topics below.\\n\\nInput: A cat sitting\\n\\nGenerated prompts:\\n\\n1. A cat sitting on a desk, typing on its laptop. The room has soft, natural light from the window, casting a warm glow on the cat.\\n2. A cat sitting in a tree, blinking its eyes. The cat is talking to the camera. A woman is eating a hotdog in the street, sunset.\\n3. A cat sitting at an office desk, facing the camera. The cat has a human body shape. The cat is black in color, and the desk has some office supplies. The scene is shot at an angle.\\n\\nInput: An underwater world\\n\\nGenerated prompts:\\n\\n1. An underwater world. The ocean is filled with bioluminescence as the water reflects a soft glow from a bioluminescent phosphorescent light source. The camera slowly moves away and zooms in.\\n2. An underwater world, the ocean is full of discarded items, the water flows, and the light penetrating through the water.\\n3. An underwater world, 25 year old boy, with aqua-green eyes, dark sandy blond hair, from the back, and on his back a fish, 23 year old wearing glasses, cartoon character.\"}"}
{"id": "pYNl76onJL", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. A landscape, the camera zooms in on an open field where a lone tree stands. A single tree, weathered by time, stands tall and proud, its branches reaching towards the heavens.\\n\\n2. A landscape of the Middle East, a vast and desolate desert, with dunes of golden sand stretching to the horizon.\\n\\n3. A landscape filled with vibrant colors and a sense of peace and harmony.\\n\\n1. a woman in the style of a classic Disney animation, dark white and dark beige and yellow, lo-fi, lush scenery, iconic album covers, 8k resolution, detailed character design.\\n\\n2. a woman in a white dress, in the background a large window, from outside the window you can see the heavy rain and thunder, night, cinematic, foggy, mysterious, 8k, HD, Fujifilm.\\n\\n3. a woman 25 years old, wearing black t-shirt and blue jeans with black sneakers walking in a train station.\\n\\n1. Spiderman, 4k, cinematic light, high production value, intricate details, high resolution.\\n\\n2. Spiderman and his incredible strength and speed are on full display. The cityscape around him is a blur of colors, from bright blue to purple.\\n\\n3. Spiderman and his face is a canvas of shifting geometric patterns, his eyes sparkling in the reflection of a nearby window.\\n\\n1. A Chinese dragon flies high in the sky, with the word \u201cdragon\u201d printed on its body.\\n\\n2. A Chinese dragon flies in a dark and mysterious universe, with a Chinese aesthetic. A girl walking on the street. A 1970s Chinese film about a woman standing on a rock.\\n\\n3. A Chinese dragon is flying in the air, and the camera lens is rotating around the dragon.\\n\\nConclusion\\n\\nThis paper provides the first systematic research on text-to-video prompts. Specifically, we introduce VidProM, the first dataset comprising 1.67 million unique text-to-video prompts, 6.69 million videos generated by four state-of-the-art diffusion models, along with NSFW probabilities, 3072-dimensional prompt embeddings, and additional related metadata. To highlight the necessity of VidProM, we compare the basic information and the semantics of prompts of our VidProM to DiffusionDB, a text-to-image prompt dataset. Finally, we outline the potential research directions inspired by our VidProM, such as text-to-video prompt engineering, fake video detection, and video copy detection for diffusion models. We hope the curated large and diverse prompt-video dataset will advance research in the text-video domain.\\n\\nLimitation\\n\\nWe recognize that the videos currently generated are short and not of the highest quality. In the future, we intend to enhance our dataset by incorporating high-quality videos produced by more advanced models, like Sora, using our long and detailed prompts. At the time of camera-ready version, we utilize three new powerful text-to-video models to generate 10,000 videos with each example: 10,000 videos of 8 seconds at 720p quality for StreamingT2V [58], 10,000 videos of 8 seconds at 720p quality for Open-Sora 1.2 [59], and 10,000 videos of 6 seconds at 720p quality for CogVideoX-2B [60]. They are publicly available at https://huggingface.co/datasets/WenhaoWang/VidProM/tree/main/example.\"}"}
{"id": "pYNl76onJL", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] OpenAI. Video generation models as world simulators. https://openai.com/research/video-generation-models-as-world-simulators. Accessed: 2024-03-06.\\n\\n[2] Pika art. https://pika.art/. Accessed: 2024-03-06.\\n\\n[3] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023.\\n\\n[4] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024.\\n\\n[5] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023.\\n\\n[6] Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. DiffusionDB: A large-scale prompt gallery dataset for text-to-image generative models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 893\u2013911. Association for Computational Linguistics, July 2023.\\n\\n[7] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation, 2023.\\n\\n[8] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023.\\n\\n[9] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.\\n\\n[10] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jos\u00e9 Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: A large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023.\\n\\n[11] Morph studio. https://app.morphstudio.com. Accessed: 2024-03-06.\\n\\n[12] Genie 2024. https://sites.google.com/view/genie-2024/home. Accessed: 2024-03-06.\\n\\n[13] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation. arXiv preprint arXiv:2305.10874, 2023.\\n\\n[14] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5036\u20135045, 2022.\\n\\n[15] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016.\\n\\n[16] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision, 2021.\\n\\n[17] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\n[18] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. 2024.\"}"}
{"id": "pYNl76onJL", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[10] Jianhui Yu, Hao Zhu, Liming Jiang, Chen Change Loy, Weidong Cai, and Wayne Wu. Celebtext: A large-scale facial text-video dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14805\u201314814, 2023.\\n\\n[12] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: A large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023.\\n\\n[14] Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. arXiv preprint arXiv:2104.04670, 2021.\\n\\n[16] Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike Tian-jian Jiang, and Alexander Rush. PromptSource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 93\u2013104. Association for Computational Linguistics, 2022.\\n\\n[18] Tyrrrz. Discordchatexporter. https://github.com/Tyrrrz/DiscordChatExporter. Accessed: 2024-03-06.\\n\\n[20] Laura Hanu and Unitary team. Detoxify. Github. https://github.com/unitaryai/detoxify, 2020.\\n\\n[22] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(86):2579\u20132605, 2008.\\n\\n[24] Zijie J. Wang, Fred Hohman, and Duen Horng Chau. WizMap: Scalable Interactive Visualization for Exploring Large Machine Learning Embeddings. arXiv 2306.09328, 2023.\\n\\n[26] Jay Zhangjie Wu, Guian Fang, Haoning Wu, Xintao Wang, Yixiao Ge, Xiaodong Cun, David Junhao Zhang, Jia-Wei Liu, Yuchao Gu, Rui Zhao, Weisi Lin, Wynne Hsu, Ying Shan, and Mike Zheng Shou. Towards a better metric for text-to-video generation. arXiv preprint arXiv:2401.07781, 2024.\\n\\n[28] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.\\n\\n[30] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou. Fetv: A benchmark for fine-grained evaluation of open-domain text-to-video generation. Advances in Neural Information Processing Systems, 36, 2023.\"}"}
{"id": "pYNl76onJL", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[36] Sam Witteveen and Martin Andrews. Investigating prompt engineering in diffusion models. arXiv preprint arXiv:2211.15462, 2022.\\n\\n[37] Chang Yu, Junran Peng, Xiangyu Zhu, Zhaoxiang Zhang, Qi Tian, and Zhen Lei. Seek for incantations: Towards accurate text-to-image diffusion synthesis through prompt engineering. arXiv preprint arXiv:2401.06345, 2024.\\n\\n[38] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. What makes good examples for visual in-context learning? Advances in Neural Information Processing Systems, 36, 2024.\\n\\n[39] Yanpeng Sun, Qiang Chen, Jian Wang, Jingdong Wang, and Zechao Li. Exploring effective factors for improving visual in-context learning. arXiv preprint arXiv:2304.04748, 2023.\\n\\n[40] You-Ming Chang, Chen Yeh, Wei-Chen Chiu, and Ning Yu. Antifakeprompt: Prompt-tuned vision-language models are fake image detectors. arXiv preprint arXiv:2310.17419, 2023.\\n\\n[41] Chandler Timm Doloriel and Ngai-Man Cheung. Frequency masking for universal deepfake detection. arXiv preprint arXiv:2401.06506, 2024.\\n\\n[42] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. CNN-generated images are surprisingly easy to spot...for now. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.\\n\\n[43] Yan Ju, Shan Jia, Lipeng Ke, Hongfei Xue, Koki Nagano, and Siwei Lyu. Fusing global and local features for generalized AI-synthesized image detection. In 2022 IEEE International Conference on Image Processing (ICIP), pages 3465\u20133469. IEEE, 2022.\\n\\n[44] Joel Frank, Thorsten Eisenhofer, Lea Sch\u00f6nherr, Asja Fischer, Dorothea Kolossa, and Thorsten Holz. Leveraging frequency analysis for deep fake image recognition. In International conference on machine learning, pages 3247\u20133258. PMLR, 2020.\\n\\n[45] Bo Liu, Fan Yang, Xiuli Bi, Bin Xiao, Weisheng Li, and Xinbo Gao. Detecting generated images by real images. In European Conference on Computer Vision, pages 95\u2013110. Springer, 2022.\\n\\n[46] Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, and Yunchao Wei. Learning on gradients: Generalized artifacts representation for GAN-generated images detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12105\u201312114, 2023.\\n\\n[47] Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, and Houqiang Li. Dire for diffusion-generated image detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 22445\u201322455, October 2023.\\n\\n[48] Rethinking the up-sampling operations in CNN-based generative network for generalizable deepfake detection. 2024.\\n\\n[49] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards universal fake image detectors that generalize across generative models. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2023.\\n\\n[50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models, 2021.\\n\\n[51] Zhenhua Liu, Feipeng Ma, Tianyi Wang, and Fengyun Rao. A similarity alignment model for video copy segment matching. arXiv preprint arXiv:2305.15679, 2023.\\n\\n[52] Tianyi Wang, Feipeng Ma, Zhenhua Liu, and Fengyun Rao. A dual-level detection method for video copy detection. arXiv preprint arXiv:2305.12361, 2023.\\n\\n[53] Wenhao Wang, Yifan Sun, and Yi Yang. Feature-compatible progressive learning for video copy detection. arXiv preprint arXiv:2304.10305, 2023.\\n\\n[54] Shuhei Yokoo, Peifei Zhu, Junki Ishikawa, and Rintaro Hasegawa. 3rd place solution to meta AI video similarity challenge. arXiv preprint arXiv:2304.11964, 2023.\\n\\n[55] Succinctly. Text2image prompt generator. https://huggingface.co/succinctly/text2image-prompt-generator, 2023. Hugging Face Model Hub.\"}"}
{"id": "pYNl76onJL", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505\u20133506, 2020.\\n\\nRoberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024.\\n\\nZangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, March 2024.\\n\\nZhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024.\\n\\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535\u2013547, 2019.\\n\\nAbhinav Ramesh Kashyap, Devamanyu Hazarika, Min-Yen Kan, Roger Zimmermann, and Soujanya Poria. So different yet so alike! constrained unsupervised text style transfer. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 416\u2013431. Association for Computational Linguistics, May 2022.\\n\\nL\u00e9o Laugier, John Pavlopoulos, Jeffrey Sorensen, and Lucas Dixon. Civil rephrases of toxic texts with self-supervised transformers. In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty, editors, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1442\u20131461. Association for Computational Linguistics.\\n\\nZhengzhe Liu, Xiaojuan Qi, and Philip H.S. Torr. Global texture enhancement for fake face detection in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\\n\\nEd Pizzi, Giorgos Kordopatis-Zilos, Hiral Patel, Gheorghe Postelnicu, Sugosh Nagavara Ravindr, Akshay Gupta, Symeon Papadopoulos, Giorgos Tolias, and Matthijs Douze. The 2023 video similarity dataset and challenge. Computer Vision and Image Understanding, page 103997, 2024.\\n\\nWenhao Wang, Weipu Zhang, Yifan Sun, and Yi Yang. Bag of tricks and a strong baseline for image copy detection. arXiv preprint arXiv:2111.08004, 2021.\\n\\nZo\u00eb Papakipos, Giorgos Tolias, Tomas Jenicek, Ed Pizzi, Shuhei Yokoo, Wenhao Wang, Yifan Sun, Weipu Zhang, Yi Yang, Sanjay Addicam, et al. Results and findings of the 2021 image similarity challenge. In NeurIPS 2021 Competitions and Demonstrations Track, pages 1\u201312. PMLR, 2022.\"}"}
{"id": "pYNl76onJL", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. For all authors...\\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] We follow this common practice for writing a paper.\\n(b) Did you describe the limitations of your work? [Yes] See the Limitation at the end of our main paper.\\n(c) Did you discuss any potential negative societal impacts of your work? [Yes] Please see the Data Sheet.\\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] We follow the NeurIPS Code of Ethics.\\n\\n2. If you are including theoretical results...\\n(a) Did you state the full set of assumptions of all theoretical results? [N/A] This is not a theory paper.\\n(b) Did you include complete proofs of all theoretical results? [N/A] This is not a theory paper.\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] The project (including the collected dataset VidProM and related code) is publicly available at https://vidprom.github.io under the CC-BY-NC 4.0 License.\\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] The experimental settings and details are provided in the main paper as well as the Appendix.\\n(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] All the experimental results are significant and stable, and error bars are not reported because it would be too computationally expensive.\\n(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] We include the experiments compute resources in each related section.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n(a) If your work uses existing assets, did you cite the creators? [Yes] We cite all the assets used in our paper.\\n(b) Did you mention the license of the assets? [Yes] All the corresponding licenses are mentioned and respected.\\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes] The project (including the collected dataset VidProM and related code) is publicly available at https://vidprom.github.io under the CC-BY-NC 4.0 License.\\n(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] Please see the curating process of our VidProM.\\n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] Please see the curating process of our VidProM.\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] The paper does not involve that.\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] The paper does not involve that.\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] The paper does not involve that.\"}"}
{"id": "pYNl76onJL", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Semantic De-duplication Algorithm\\n\\nThis section shows how we select semantically unique prompts, i.e., for any two arbitrary prompts, their cosine similarity is less than $\\\\theta = 0.8$. We first show our algorithm:\\n\\nAlgorithm 1\\n\\nInput:\\nunique prompts $P_{unique} = \\\\{p_1, p_2, ..., p_N\\\\}$, prompt embeddings $E \\\\in \\\\mathbb{R}^{N \\\\times d}$, filtering threshold $\\\\theta$\\n\\n1: Compute similarity matrix $S = EE^T$ and initialize $P_{similar} = \\\\{}$\\n2: for $i = 1$ to $N$ do\\n3: for $j = i + 1$ to $N$ do\\n4: if $S_{i,j} > \\\\theta$ then\\n5: Add $\\\\{p_i, p_j\\\\}$ to $P_{similar}$\\n6: end if\\n7: end for\\n8: end for\\n9: Initialize $P_{all} = \\\\{}$\\n10: for each pair $\\\\{p_i, p_j\\\\}$ in $P_{similar}$ do\\n11: if $p_i$ not in $P_{all}$ then\\n12: Add $p_i$ to $P_{all}$\\n13: end if\\n14: if $p_j$ not in $P_{all}$ then\\n15: Add $p_j$ to $P_{all}$\\n16: end if\\n17: end for\\n18: Initialize $P_{del} = \\\\{}$\\n19: for each pair $\\\\{p_i, p_j\\\\}$ in $P_{similar}$ do\\n20: if $p_i$ and $p_j$ are both in $P_{all}$ then\\n21: Add $p_i$ to $P_{del}$\\n22: end if\\n23: end for\\n24: Initialize $P_{unique}_{-sem} = \\\\{}$\\n25: for each $p_i$ in $P_{unique}$ do\\n26: if $p_i$ is not in $P_{del}$ then\\n27: Add $p_i$ to $P_{unique}_{-sem}$\\n28: end if\\n29: end for\\nOutput: $P_{unique}_{-sem}$\\n\\nThen we analyze the efficiency of the algorithm. The most time-consuming part of our algorithm lies in building $P_{similar}$, which includes calculating the similarity matrix and selecting pairs with a similarity greater than $\\\\theta$. To complete this process with $N = 1,672,243$ and $d = 3072$, we distribute the workload across 8 A100 GPUs and 128 CPU cores using Faiss [61]. It only takes approximately 0.604 hours.\\n\\nB Domain Gap between Video Captions and Text-to-Video Prompts\\n\\nThis section highlights the domain gap between training video captions and real user prompts. The domain gap may hinder the model's ability to generate satisfactory videos due to a lack of exposure to topics of interest to users and the style difference between captions and prompts. In Fig. 6, we randomly select 1.6 million captions from Panda-70M [18] and visualize them with our VidProM. We find that their distributions are significantly different.\\n\\nOn one hand, the covered topics are different. For instance, users may want to generate videos of city nights or superheroes like Spider-Man. However, in the training data, there are few or no related videos on these topics. One possible solution is to filter out training videos that cover topics of human interest before training text-to-video models. On the other hand, there may be a difference in styles of describing videos. For instance, users may start a prompt with \\\"Generate a video of\\\" and end it with \\\"4K\\\". However,\"}"}
{"id": "pYNl76onJL", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C Benchmarking Existing Fake Image Detection on VidProM\\n\\nThis section shows the current fake image detection methods fail to generalize to fake video detection. Implementation details. To benchmark existing fake image detection algorithms on our dataset, we test eight open-source, state-of-the-art models available at https://github.com/Ekko-zn/AIGCDetectBenchmark. We randomly select 10,000 videos generated by Pika, VideoCraft2, Text2Video-Zero, and ModelScope, respectively, to serve as fake samples. Similarly, we choose 10,000 real videos randomly from DVSC2023. Since none of the state-of-the-art models can process entire videos directly, we extracted the middle frame from each video to use as the input image for each model. We evaluate the models using two metrics: Accuracy and Mean Average Precision (mAP).\\n\\nExperimental results. The performance of the models is detailed in Tables 2 and 3. Our observations reveal that all existing models struggle to differentiate between fake and real videos: (1) Methods initially designed for detecting GAN-generated images, such as [42] and [46], are ineffective with videos produced by diffusion models, likely because they are trained on images from significantly different generative methods. (2) Techniques specifically developed for the diffusion process [47] or for generalizing across various generative models [49] also fail in detecting fake videos, indicating that the key features identified for spotting diffusion-generated images may not be as widely applicable as previously assumed. (3) Interestingly, methods that rely on traditional cues, such as global texture statistics [64] and frequency analysis [44], are the most effective. This underscores the continuing importance of traditional image processing knowledge.\"}"}
{"id": "pYNl76onJL", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: The accuracy of fake image detection methods on fake video detection task.\\n\\n|                  | Pika   | VideoCraft2 | Text2Video-Zero | ModelScope | Average |\\n|------------------|--------|-------------|-----------------|------------|---------|\\n| CNNSpot [42]     | 51.17  | 50.18       | 49.97           | 50.31      | 50.41   |\\n| FreDect [44]     | 50.07  | 54.03       | 69.88           | 69.94      | 60.98   |\\n| Fusing [43]      | 50.60  | 50.07       | 49.81           | 51.28      | 50.44   |\\n| Gram-Net [64]    | 84.19  | 67.42       | 52.48           | 50.46      | 63.64   |\\n| LGrad [46]       | 53.73  | 51.75       | 41.05           | 60.22      | 51.69   |\\n| LNP [45]         | 43.48  | 45.10       | 47.50           | 45.21      | 45.32   |\\n| DIRE [47]        | 50.53  | 49.95       | 48.96           | 48.32      | 49.44   |\\n| UnivFD [49]      | 49.41  | 48.65       | 49.58           | 57.43      | 51.27   |\\n\\nTable 3: The mAP of fake image detection methods on fake video detection task.\\n\\n|                  | Pika   | VideoCraft2 | Text2Video-Zero | ModelScope | Average |\\n|------------------|--------|-------------|-----------------|------------|---------|\\n| CNNSpot [42]     | 54.63  | 41.12       | 44.56           | 46.95      | 46.82   |\\n| FreDect [44]     | 47.82  | 56.67       | 75.31           | 64.15      | 60.99   |\\n| Fusing [43]      | 57.64  | 41.64       | 40.51           | 56.09      | 48.97   |\\n| Gram-Net [64]    | 94.32  | 80.72       | 57.73           | 43.54      | 69.08   |\\n| LGrad [46]       | 54.49  | 53.21       | 36.69           | 66.53      | 52.73   |\\n| LNP [45]         | 44.28  | 44.08       | 46.81           | 39.62      | 43.70   |\\n| DIRE [47]        | 49.21  | 50.44       | 44.52           | 48.64      | 48.20   |\\n| UnivFD [49]      | 48.63  | 42.36       | 48.46           | 70.75      | 52.55   |\\n\\nD Video Copy Detection for Diffusion Models\\n\\nThis sections shows whether the videos generated by text-to-video diffusion models replicate content of their training or existing videos.\\n\\nD.1 Experimental Setup\\n\\nThe original videos for matching. We randomly select 1 million videos from the training source of VideoCraft2 [4] and ModelScope [5] to analyze the extent to which generated videos replicate their training data. We acknowledge that using this source may result in an underestimation of the replication ratio, but it is sufficient for analyzing the phenomenon of replication.\\n\\nThe model for video copy detection. We select our previous winning solution, FCPL [53], as the model for video copy detection. It achieves state-of-the-art on DVSC2023 [65] while maintaining high efficiency.\\n\\nD.2 Observations\\n\\nQualitative observations. In Fig. 7, we demonstrate that text-to-video diffusion models can replicate the content from their training data or existing videos. This replication is acceptable for fair use purposes such as education, news reporting, and parody. However, misusing videos that contain replicated content with copyright could constitute an infringement. For example, in Fig. 7, the video generated by the commercial model Pika closely replicate the content of the famous painting, \u201cThe Persistence of Memory\u201d, which is copyrighted by the Gala-Salvador Dal\u00ed Foundation. If someone uses this generated video for profit without permission, it could potentially constitute copyright infringement.\\n\\nQuantitative observations. We calculate the maximum normalized score for each generated video by comparing it to all reference videos. As shown in Fig. 8, we count the number of generated videos with a maximum normalized score exceeding 0. Generally, a normalized score greater than 0 suggests a high likelihood of replicated content [66, 67, 65]. Our observations include: (1) Although replication occurs, it represents only a very small proportion of the generated videos. For instance, only about 2% of the videos generated by Text2Video-Zero have a maximum normalized score greater than 0. (2) The commercial model Pika has the lowest replication rate, while the open-source model Text2Video-Zero has the highest.\"}"}
{"id": "pYNl76onJL", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Some generated videos (top) by text-to-video diffusion models may replicate content of their training or existing videos (bottom). We use the matched frame in one video to represent the whole video.\\n\\nFigure 8: The counts of generated videos with different maximum normalized scores. This difference may be partly because commercial models have implemented measures to prevent replication and mitigate related legal risks.\\n\\nD.3 Limitations and Future Directions\\n\\nIn Fig. 9, we visualize some failure cases gained by the current video copy detection models. We observe that, although the generated videos replicate content from existing ones, the video copy detection model fails to produce a normalized score higher than 0. The underlying issue is that these models are trained only on manually-generated transformations (as shown in Fig. 10), and therefore, they do not generalize well to replication patterns produced by diffusion models.\\n\\nTo establish video copy detection for diffusion models, a feasible approach is to utilize existing video copy detection systems as filters to identify potential pairs containing replicated content. Following this, human labelers can be employed to verify these pairs. The validated pairs can then be used to train a new detection model.\"}"}
{"id": "pYNl76onJL", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Using existing video copy detection models fails to regard these cases as replicated content.\\n\\nFigure 10: The transformations used to train current video copy detection models [51, 52, 53, 54].\\n\\nFor what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.\\n\\nThe arrival of Sora marks a new era for text-to-video diffusion models, bringing significant advancements in video generation and potential applications. However, Sora, along with other text-to-video diffusion models, is highly reliant on prompts, and there is no publicly available dataset that features a study of text-to-video prompts. We underscore the need for a new prompt dataset specifically designed for text-to-video generation by illustrating how VidProM differs from DiffusionDB, a large-scale prompt-gallery dataset for image generation.\\n\\nWho created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\\n\\nThe dataset was created by Wenhao Wang (University of Technology Sydney) and Yi Yang (Zhejiang University).\\n\\nWho funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.\\n\\nFunded in part by Faculty of Engineering and Information Technology Scholarship, University of Technology Sydney.\\n\\nAny other comments?\\n\\nNone.\"}"}
{"id": "pYNl76onJL", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Composition\\n\\nWhat do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?\\n\\nAre there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.\\n\\nEach instance consists of a text-to-video prompt, a UUID, a timestamp, six NSFW probabilities, a 3072-dimensional prompt embedding, and four generated videos from Pika, VideoCraft2, Text2Video-Zero, and ModelScope.\\n\\nHow many instances are there in total (of each type, if appropriate)?\\n\\nThere are 1,672,243 instances in total.\\n\\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?\\n\\nIf the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).\\n\\nThe dataset contains all possible instances up to February 2024.\\n\\nWhat data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images) or features? In either case, please provide a description.\\n\\nEach instance consists of a text-to-video prompt, a UUID, a timestamp, six NSFW probabilities, a 3072-dimensional prompt embedding, and four generated videos from Pika, VideoCraft2, Text2Video-Zero, and ModelScope.\\n\\nIs there a label or target associated with each instance?\\n\\nIf so, please provide a description.\\n\\nThe labels associated with each video are the prompts and other related data.\\n\\nIs any information missing from individual instances?\\n\\nIf so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.\\n\\nEverything is included. No data is missing.\\n\\nAre relationships between individual instances made explicit (e.g., users' movie ratings, social network links)?\\n\\nIf so, please describe how these relationships are made explicit.\\n\\nNot applicable.\\n\\nAre there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.\\n\\nNo. This dataset is not for ML model benchmarking. Researchers can use any subsets of it.\\n\\nAre there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.\\n\\nNo. All videos and prompts are extracted as is from the Discord chat log.\\n\\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?\\n\\nThe dataset is entirely self-contained.\\n\\nDoes the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor\u2013patient confidentiality, data that includes the content of individuals' nonpublic communications)? If so, please provide a description.\\n\\nUnknown to the authors of the datasheet.\\n\\nIt is possible that some prompts contain sensitive information. However, it would be rare, as the Pika Discord has rules against writing personal information in the prompts, and there are moderators removing messages that violate the Discord rules.\"}"}
{"id": "pYNl76onJL", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why.\\n\\nWe collect videos and their prompts from the Pika discord server. Even though the discord server has rules against users sharing any NSFW (not suitable for work, such as sexual and violent content) and illegal images, VidProM still contains some NSFW videos and prompts that were not removed by the server moderators. To mitigate this, we provide six separate NSFW probabilities, allowing researchers to determine a suitable threshold for filtering out potentially unsafe data specific to their tasks.\\n\\nDoes the dataset identify any subpopulations (e.g., by age, gender)?\\n\\nNo.\\n\\nIs it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? If so, please describe how.\\n\\nNo.\\n\\nAny other comments?\\n\\nNone.\\n\\nCollection\\n\\nHow was the data associated with each instance acquired?\\n\\nWas the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If the data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.\\n\\nThe data was directly observed from the Pika Discord Channel. It was gathered from channels where users can generate videos by interacting with a bot, which consisted of messages of user generated videos and the prompts used to generate those images.\\n\\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)? How were these mechanisms or procedures validated?\\n\\nThe data was gathered using a DiscordChatExporter [23], which collected videos and chat messages from each channel specified. We then extracted and linked prompts to videos. Random videos and prompts were selected and manually verified to validate the prompt-video mapping.\\n\\nIf the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?\\n\\nVidProM does not sample from a larger set.\\n\\nWho was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?\\n\\nNo crowdworkers are needed in the data collection process.\\n\\nOver what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.\\n\\nAll messages were generated between July 2023 and February 2024. VidProM includes the generation timestamps of all videos.\\n\\nWere any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.\\n\\nThere were no ethical review processes conducted.\"}"}
{"id": "pYNl76onJL", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?\\n\\nThe data was directly obtained from individual messages in the Discord server.\\n\\nWere the individuals in question notified about the data collection?\\n\\nIf so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.\\n\\nUsers of the channel were not notified about this specific gathering of data but agree to open-source their input and output under the Creative Commons Noncommercial 4.0 Attribution International License (CC BY-NC 4.0). The exact language is as follows:\\n\\nYou hereby grant Mellis and other users a license to any of your Inputs and Outputs that you make available to other users on the Service under the Creative Commons Noncommercial 4.0 Attribution International License (as accessible here: https://creativecommons.org/licenses/by-nc/4.0/legalcode).\\n\\nDid the individuals in question consent to the collection and use of their data?\\n\\nIf so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.\\n\\nBy using the server and tools, users consented to the regulations posed by Pika Lab, the company that both made Pika text-to-video model and runs the Discord server. This implies consent by using the tool. The exact wording is as follows:\\n\\nYou hereby grant Mellis and other users a license to any of your Inputs and Outputs that you make available to other users on the Service under the Creative Commons Noncommercial 4.0 Attribution International License (as accessible here: https://creativecommons.org/licenses/by-nc/4.0/legalcode).\\n\\nIf consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses?\\n\\nIf so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).\\n\\nUsers will have the option to report harmful content or withdraw videos they created by emailing Wenhao Wang (wangwenhao0716@gmail.com).\\n\\nHas an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted?\\n\\nIf so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.\\n\\nNo analysis has been conducted.\\n\\nAny other comments?\\n\\nNone.\\n\\n**Preprocessing**\\n\\nWas any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?\\n\\nIf so, please provide a description. If not, you may skip the remaining questions in this section.\\n\\nNo, we provide the original prompts and generated videos.\\n\\nWas the \\\"raw\\\" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?\\n\\nIf so, please provide a link or other access point to the \\\"raw\\\" data.\\n\\nYes, raw data is saved.\\n\\nIs the software that was used to preprocess/clean/label the data available?\\n\\nIf so, please provide a link or other access point.\"}"}
{"id": "pYNl76onJL", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Not applicable.\\n\\nAny other comments?\\nNone.\\n\\nHas the dataset been used for any tasks already?\\nNo.\\n\\nIs there a repository that links to any or all papers or systems that use the dataset?\\nNo.\\n\\nWhat (other) tasks could the dataset be used for?\\nThis dataset can be used for video generative model evaluation, text-to-video diffusion model development, text-to-video prompt engineering, efficient video generation, fake video detection, video copy detection for diffusion models, and multimodal learning from synthetic videos.\\n\\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?\\nFor example, is there anything that a dataset consumer might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other risks or harms (e.g., legal risks, financial harms)? If so, please provide a description. Is there anything a dataset consumer could do to mitigate these risks or harms?\\nThere is minimal risk for harm: the data were already public. Personally identifiable data (e.g., discord usernames) were not collected.\\n\\nAre there tasks for which the dataset should not be used?\\nIf so, please provide a description.\\nAll tasks that utilize this dataset should follow the licensing policies posed by Pika Lab.\\n\\nAny other comments?\\nNone.\\n\\nDistribution\\nWill the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?\\nYes, the dataset is publicly available on the internet.\\n\\nHow will the dataset will be distributed (e.g., tarball on website, API, GitHub)?\\n\\nDoes the dataset have a digital object identifier (DOI)?\\nThe dataset is distributed on the project website: https://vidprom.github.io/. The dataset shares the same DOI as this paper.\\n\\nWhen will the dataset be distributed?\\nThe dataset is released on March 12th, 2024.\\n\\nWill the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?\\nIf so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.\\nThe prompts and videos generated by Pika in our VidProM are licensed under the CC BY-NC 4.0 license from the Pika Discord, allowing for non-commercial use with attribution. Additionally, similar to their original repositories, the videos from VideoCraft2, Text2Video-Zero, and ModelScope...\"}"}
{"id": "pYNl76onJL", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"are released under the Apache license, the CreativeML Open RAIL-M license, and the CC BY-NC 4.0 license, respectively.\\n\\nHave any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.\\n\\nNo.\\n\\nDo any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.\\n\\nNo.\\n\\nAny other comments?\\n\\nNone.\\n\\nWho will be supporting/hosting/maintaining the dataset?\\nThe authors of this paper will be supporting and maintaining the dataset.\\n\\nHow can the owner/curator/manager of the dataset be contacted (e.g., email address)?\\nThe contact information of the curators of the dataset is listed on the project website: https://vidprom.github.io/.\\n\\nIs there an erratum? If so, please provide a link or other access point.\\n\\nThere is no erratum for our initial release. Errata will be documented in future releases on the dataset website.\\n\\nWill the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to dataset consumers (e.g., mailing list, GitHub).\\n\\nYes, we will monitor cases when users can report harmful videos and creators can remove their videos. We will update the dataset bimonthly. Updates will be posted on the project website https://vidprom.github.io/. In the future, we intend to enhance our dataset by incorporating high-quality videos produced by more advanced models, like Sora, using our detailed prompts.\\n\\nIf the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.\\n\\nPeople can write an email to remove specific instances from VidProM.\\n\\nWill older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers.\\n\\nWe will continue to support older versions of the dataset.\\n\\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to dataset consumers? If so, please provide a description.\\n\\nAnyone can extend/augment/build on/contribute to VidProM. Potential collaborators can contact the dataset authors.\\n\\nAny other comments?\\n\\nNone.\"}"}
