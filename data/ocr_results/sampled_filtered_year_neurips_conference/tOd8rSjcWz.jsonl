{"id": "tOd8rSjcWz", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nIn-context vision and language models like Flamingo \\\\cite{2} support arbitrarily interleaved sequences of images and text as input. This format not only enables few-shot learning via interleaving independent supervised (image, text) examples, but also, more complex prompts involving interaction between images, e.g., \u201cWhat do image A and image B have in common?\u201d To support this interface, pretraining occurs over web corpora that similarly contain interleaved images+text. To date, however, large-scale data of this form have not been publicly available.\\n\\nWe release Multimodal C4 (mmc4), an augmentation of the popular text-only c4 corpus\\\\textsuperscript{2} with images interleaved. We use a linear assignment algorithm to place images into longer bodies of text using CLIP features \\\\cite{24}, a process that we show outperforms alternatives. mmc4 spans everyday topics like cooking, travel, technology, etc. A manual inspection of a random sample of documents shows that a vast majority (88\\\\%) of images are topically relevant, and that linear assignment frequently selects individual sentences specifically well-aligned with each image (80\\\\%). After filtering NSFW images, ads, etc., the resulting mmc4 corpus consists of 101.2M documents with 571M images interleaved in 43B English tokens.\\n\\n1 Introduction\\n\\nIn-context learning \\\\cite{7} enables sequence models to adapt to new tasks without any parameter updates. By interleaving a few supervised examples in a prompt, few-shot learning can be formatted as a next-token prediction task, i.e., \\\\(x_1, y_1, x_2, y_2, \\\\ldots, x_n\\\\) is input to predict \\\\(\\\\hat{y}_n\\\\). Some image+text models also support in-context learning via interleaving of images/text jointly. Prior experiments \\\\cite{2} suggest that performant multimodal in-context learning is dependent upon pretraining on similarly interleaved sequences of images and text (rather than single image/caption pairs). However, such a large-scale corpus has not been made publicly available.\\n\\nTo address this, we introduce Multimodal C4 (mmc4), a public, billion-scale image-text dataset consisting of interleaved image/text sequences. mmc4 is constructed from public webpages contained in the cleaned English c4 corpus. In addition to standard preprocessing steps like deduplication,\"}"}
{"id": "tOd8rSjcWz", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison of mmc4 with other interleaved image/text pretraining corpora. In addition to the full version of the dataset, we also release: 1) fewer-faces subsets, which aim to remove all depicted human faces; and 2) \u201ccore\u201d subsets, result from more stringent filtering.\\n\\n| Dataset                        | # images  | # docs    | # tokens  | Public? |\\n|-------------------------------|-----------|-----------|-----------|---------|\\n| M3W (Flamingo) [2]            | 185M      | 43M       | -         |         |\\n| Interleaved training data for CM3 [1] | 25M       | 61M       | 223B      |         |\\n| Interleaved training data for KOSMOS-1 [17] | 355M      | 71M       | -         |         |\\n| Multimodal C4 (mmc4)          | 571M      | 101.2M    | 43B       | \u2713       |\\n| Multimodal C4 fewer-faces (mmc4-ff) | 375M      | 77.7M     | 33B       | \u2713       |\\n| mmc4 core (mmc4-core)         | 29.9M     | 7.3M      | 2.4B      | \u2713       |\\n| mmc4 core fewer-faces (mmc4-core-ff) | 22.4M     | 5.5M      | 1.8B      | \u2713       |\\n\\nNSFW removal, etc., we place images into sequences of sentences by treating each document as an instance of a bipartite linear assignment problem, with images being assigned to sentences (under the constraint that each sentence is assigned at most one image). We show that applying CLIP ViT-L/14 [24] to estimate bipartite weights in a zero-shot fashion results in state-of-the-art performance on intra-document alignment benchmarks, and then apply this process to 100M+ documents to construct mmc4. Apart from the full corpus, we have created two additional subsets: mmc4-ff, which removes images with detected faces, and mmc4-core, a more strictly filtered and downsized version of the corpus, serving as an initial corpus for developers. We explore mmc4, showing that: 1) the text and images in the corpus span expected everyday topics like cooking and travel; 2) filters like NSFW/ad removal work with high accuracy; and 3) the resulting images are relevant to the associated documents, and often, appropriately aligned to the most-relevant individual sentence. We conclude by discussing initial use-cases of mmc4, including OpenFlamingo [3], an open source version of Flamingo [2]. Initial ablations show that training on the sequences of mmc4 enables few-shot, in-context adaptation to image captioning datasets.\\n\\n2 Related Dataset Work\\n\\nMost million/billion-scale, public multimodal pretraining datasets consist of images paired with their literal descriptions, e.g., LAION-2B [26], CC-12M [8], YFCC100M [32]. However, literal description is only one of many ways images can relate to text on the web [21]. mmc4 aims to capture a broader range of these relationship types. Some web datasets collect multiple images for one text snippet (e.g., the Google Local Restaurant Reviews Dataset [36] with 4.4M images), or situate images in longer bodies of text (e.g., the Wikipedia-based Image Text Dataset [30] with 11.5M images), but do not directly cover multi-image/multi-sentence interleaving. Table 1 provides summary statistics of other large-scale interleaved pretraining datasets. mmc4 contains more images than prior non-public datasets. [5] highlight risks associated with web-scale multimodal data.\\n\\nIn addition to the detailed curation steps described in \u00a7 3 and the considerations for data release outlined in \u00a7 3.1, we are hopeful that the availability of mmc4 can facilitate a more transparent and critical examination of interleaved corpora compared to previous privately held training sets. Models trained on mmc4 inherit its risks; we selected the widely-adopted c4 corpus as a starting point in part because there are existing auditing efforts on the text-only corpus, see \u00a7 3 and [23] for more discussion of transparency.\\n\\n3 Data Curation Process\\n\\nInitial data collection. Multimodal C4 is an expansion of the text-only c4 dataset [25], which was created by taking the April 2019 snapshot from Common Crawl [5] and applying several filters with the intention of retaining high-quality, natural English text. Each document in c4 consists of the text.\"}"}
{"id": "tOd8rSjcWz", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: A T-SNE projection of LDA topic clusters from a random sample of 22K documents from mmc4; mmc4 spans a variety of everyday topics, e.g., cooking, technology travel, etc. For 6 selected topics, we also show a sample of most-central images to the topic according to CLIP ViT-L/14.\\n\\nThe full c4 dataset has 365M documents and 156B tokens, covering many domains; it was first used to train T5. We built the mmc4 dataset on top of c4 because: 1) c4 is a web-scale dataset widely adopted as a pre-training corpus; 2) c4 is constructed from web pages, which frequently contain multimedia content like images, which makes it a suitable basis for extending to a multimodal sequence version; and 3) the specific underlying subset from which we construct mmc4 has already been processed with several data-cleaning steps (including English-language identification by langdetect with at least 0.99 confidence; text deduplication removing duplicate three-sentence spans + placeholder text like \\\"lorem ipsum\\\"); and removal of any document containing any word on the \\\"List of Dirty, Naughty, Obscene or Otherwise Bad Words\\\").\\n\\nSee [25] for more information about the text-only c4. Importantly, by building on the popular text-only c4, prior text-only documentation efforts can provide insight about potential biases and risks that could arise when training on our multimodal extension. We use the NLTK sentence tokenizer to chunk each c4 document into a list of sentences.\\n\\nGathering images. We first retrieve the original webpages for each document in the c4-en dataset from the Common Crawl version 2019-18, which is the default version for c4. Next, we extract the URLs for downloadable images from the raw WAT files. We restrict the image extension to either png/jpeg/jpg, and exclude image URLs that contain the following tokens: logo, button, icon, plugin, widget. We attempt to download from these URLs, and resize images to a maximum dimension of 800px. We eliminate any c4 documents that do not contain valid, downloadable images at the time of collection (mid-to-late 2022). The starting point after this step is 115M documents and 1.37B images.\\n\\nDe-duplication+small resolution. We next run duplicate image detection using opennota's findimagedupes which uses phash to identify visually similar images. We keep only one copy of an image if multiple versions are detected within the same document. We also remove images with more than 10 duplicates in a sample of 60K images. We discard images with a width or height smaller than 150px; this accounts for many small icons, e.g., navigation buttons. We discard images with an aspect ratio of greater than 2 or less than 0.5; this accounts for many banner-like ads. In 6, see [25] for more information about the text-only c4. Importantly, by building on the popular text-only c4, prior text-only documentation efforts can provide insight about potential biases and risks that could arise when training on our multimodal extension. We use the NLTK sentence tokenizer to chunk each c4 document into a list of sentences.\\n\\nGathering images. We first retrieve the original webpages for each document in the c4-en dataset from the Common Crawl version 2019-18, which is the default version for c4. Next, we extract the URLs for downloadable images from the raw WAT files. We restrict the image extension to either png/jpeg/jpg, and exclude image URLs that contain the following tokens: logo, button, icon, plugin, widget. We attempt to download from these URLs, and resize images to a maximum dimension of 800px. We eliminate any c4 documents that do not contain valid, downloadable images at the time of collection (mid-to-late 2022). The starting point after this step is 115M documents and 1.37B images.\\n\\nDe-duplication+small resolution. We next run duplicate image detection using opennota's findimagedupes which uses phash to identify visually similar images. We keep only one copy of an image if multiple versions are detected within the same document. We also remove images with more than 10 duplicates in a sample of 60K images. We discard images with a width or height smaller than 150px; this accounts for many small icons, e.g., navigation buttons. We discard images with an aspect ratio of greater than 2 or less than 0.5; this accounts for many banner-like ads. In 6, see [25] for more information about the text-only c4. Importantly, by building on the popular text-only c4, prior text-only documentation efforts can provide insight about potential biases and risks that could arise when training on our multimodal extension. We use the NLTK sentence tokenizer to chunk each c4 document into a list of sentences.\\n\\nGathering images. We first retrieve the original webpages for each document in the c4-en dataset from the Common Crawl version 2019-18, which is the default version for c4. Next, we extract the URLs for downloadable images from the raw WAT files. We restrict the image extension to either png/jpeg/jpg, and exclude image URLs that contain the following tokens: logo, button, icon, plugin, widget. We attempt to download from these URLs, and resize images to a maximum dimension of 800px. We eliminate any c4 documents that do not contain valid, downloadable images at the time of collection (mid-to-late 2022). The starting point after this step is 115M documents and 1.37B images.\\n\\nDe-duplication+small resolution. We next run duplicate image detection using opennota's findimagedupes which uses phash to identify visually similar images. We keep only one copy of an image if multiple versions are detected within the same document. We also remove images with more than 10 duplicates in a sample of 60K images. We discard images with a width or height smaller than 150px; this accounts for many small icons, e.g., navigation buttons. We discard images with an aspect ratio of greater than 2 or less than 0.5; this accounts for many banner-like ads. We discard images with a width or height smaller than 150px; this accounts for many small icons, e.g., navigation buttons. We discard images with an aspect ratio of greater than 2 or less than 0.5; this accounts for many banner-like ads. We discard images with a width or height smaller than 150px; this accounts for many small icons, e.g., navigation buttons. We discard images with an aspect ratio of greater than 2 or less than 0.5; this accounts for many banner-like ads. We discard images with a width or height smaller than 150px; this accounts for many small icons, e.g., navigation buttons. We discard images with an aspect ratio of greater than 2 or less than 0.5; this accounts for many banner-like ads. We discard images with a width or height smaller than 150px; this accounts for many small icons, e.g., navigation buttons. We discard images with an aspect ratio of greater than 2 or less than 0.5; this accounts for many banner-like ads.\\n\\nWhile some duplicates survive this process, we qualitatively found a threshold of 5 to be an appropriate balance of false positives/negatives. While some duplicates survive this process, we qualitatively found a threshold of 5 to be an appropriate balance of false positives/negatives. While some duplicates survive this process, we qualitatively found a threshold of 5 to be an appropriate balance of false positives/negatives.\"}"}
{"id": "tOd8rSjcWz", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Performance on single document image-text benchmarks from [16] (higher=better in all cases). Applying CLIP ViT-L/14 in a zero-shot fashion [24] produces better within-document alignments compared to prior methods which rely on fine-tuning.\\n\\n| Method                        | AUC  | p@1  | AUC  | p@1  | AUC  | p@1  | AUC  | p@1  |\\n|-------------------------------|------|------|------|------|------|------|------|------|\\n| Random                        | 49.7 | 5.0  | 49.4 | 19.5 | 50.0 | 19.4 | 50.0 | 2.0  |\\n| Hessel et al. (2019) [16]     | 98.7 | 91.0 | 82.6 | 70.5 | 68.5 | 50.5 | 95.3 | 65.5 |\\n| Li et al. (2021) [20]         | 99.3 |     | 85.5 | 77.2 | 70.2 | 53.1 | \u2013    | \u2013    |\\n| CLIP ViT-L/14 (Zero Shot)     | 99.4 |     | 92.8 | 93.9 | 79.1 | 73.3 | 98.7 | 93.0 |\\n\\nA manual sample of 3.7K images that survive this (and the NSFW) filter, 91 images (2.5%) were identified as ads potentially unrelated to document contents.\\n\\nDiscarding NSFW images.\\n\\nWe employ strict NSFW image filtering, using DataComp's [14] dataset2metadata NSFW binary image classifier. The model is a 4-layer MLP, trained on the NSFW dataset introduced in LAION-2B [26]. This MLP takes as input image features extracted from OpenAI's CLIP ViT-L/14 [24] and achieves 97.4% accuracy on the NSFW test set. We run this classifier on each image and discard cases with a model-predicted NSFW probability over 0.1, which removes approximately 10% of remaining images. Because the data distribution of the classifier and mmc4 may be slightly different, we also conduct a spot check on images that are marked safe for work. In a manual sample of 3.7K images, we discovered zero NSFW images.\\n\\nAligning images and sentences.\\n\\nAfter collecting a set of images for each document, we now describe our intra-document alignment process to interleave the collected images with the sentences. Given that the scope of the images and sentences may be different \u2013 the image set is collected from the whole webpage, while the sentence list is subject to preprocessing within the c4 dataset and thus may not represent the complete content of the webpage \u2013 we did not rely on Document Object Model placements in the raw HTML to establish the alignment between images and sentences in each document. Instead, to associate each image with a sentence, we consider each document as an instance of a bipartite assignment problem [19, 16], and use CLIP ViT-L/14 compute pairwise similarities between all sentences/images on a single page. Then, we discard images without at least a 0.1 CLIP cosine similarity to at least one sentence in the document. Finally, we use [18] to compute a bipartite assignment of images to sentences, under the constraint that each sentence can only be assigned a single image.\\n\\nTable 2 shows that this zero-shot application of CLIP ViT-L/14 for within-document matching surpasses prior competitive, fine-tuned methods on image-text alignment benchmarks from [16] (we also distribute the raw intra-document similarity matrices with mmc4 so alternate assignment methods can be explored). Figure 2 illustrates two example documents with the images interleaved before or after the assigned sentences.\\n\\n3.1 Considerations for data release\\n\\nmmc4 contains all images that survive the previously described filters. In addition to the full version of the corpus, we construct two additional types of subsets.\\n\\n3.1.1 Fewer Faces (mmc4-ff)\\n\\nLike the text-only version of c4, mmc4 may contain webpages with personal information that individuals had not explicitly intended to make available for model training. For an initial public release, we make a version of mmc4 available, mmc4-ff (ff stands for \u201cfewer faces\u201d); similar to some prior image dataset curation efforts [13, 11], mmc4-ff aims to remove images containing detected faces.\\n\\n12 The delineation between an \u201cirrelevant advertisement\u201d and a \u201crelevant image\u201d is inexact: for example, we discovered images advertising specific, small events, e.g., ones hosted by a fishing club within a city (this type of image was not included in this count). We later assess advertisement-ess in the context of the text of documents, rather than assessing based on the image alone.\"}"}
{"id": "tOd8rSjcWz", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This Walnut and Blue Cheese Stuffed Mushrooms recipe is sponsored by Fisher Nuts.\\n\\nStuffed mushrooms are an appetizer that always grabs my attention at a party. If you are a mushroom lover, like me, you probably feel the same. The ideas for stuffing mushrooms are endless, so many combinations to play with, a couple of my personal favorites are these Mediterranean Stuffed Mushrooms and these Spinach and Toasted Pine Nut Stuffed Mushrooms. Well, you can officially add these Walnut and Blue Cheese Stuffed Mushrooms to my favorites list.\\n\\nThe ingredients for the stuffing are simple, which is always best. ... Check out Shane Driscoll's take on sustainable communities and how his photograph fits this year's Green Cities theme.\\n\\nMan-made platforms like the one pictured here allow these fish-eating birds of prey to thrive in developed coastal areas. A city surrounded by mountains.\\n\\nI took this photo in October on a hike in New Hampshire. It is looking at Mt. Chicora from the middle sister mountain. Getting people out into beautiful places like this is becoming more and more popular, and each time we bring a little piece of nature back with us that inspires us to make our cities better.\\n\\nRemoving images with detected faces. To detect faces at billion-scale with the intent of removing them from the dataset, we first run RetinaFace over a sample of 60K images with the default settings. This detector runs at a high resolution and would be computationally prohibitive to run in full precision for the whole corpus; it produces detailed localization information about the coordinates of each face in each image (which we discard). Using an 80/20 train/test split, we train a cross-validated logistic regression over CLIP ViT-L/14 features to predict whether or not RetinaFace detects a face: this classifier is several orders of magnitude faster compared to RetinaFace. This approximation performs well: we choose a confidence cutoff that achieves 95% recall for the label \\\"RetinaFace detected any face\\\" over the test set while preserving 65% of the original images.\\n\\nManual sample-based face image risk assessment. We performed a manual verification of face removal. In a random sample of 912 images that pass all filters including the \\\"no faces\\\" filter, 23 (2.5%) images arguably contain a mostly-un-obscured human face. In most cases (12/23), faces are very low resolution, e.g., a 150x150px image of a crowd of people from a distance, where each face accounts for 3x4 pixels, or are motion shots where the face is blurred. In one case, the face is Marilyn Monroe's as depicted in art on a wall. In 6 cases, there is a plausibly identifiable face depicted: in 2 cases, these are models posing in ads; in 1 case, there is a low resolution image of politicians giving a speech; in 2 cases, the faces are obscured; in 1 case, a passerby was caught in the background of a city photograph and could feasibly be individually identified. Overall: the rate of unobscured, high-resolution, identifiable faces in mmc4-ff is low.\\n\\n3.1.2 Core (mmc4-core) Early conversations with some model developers revealed a desire to work with a smaller subset of the corpus as an initial step. We thus additionally release core versions of mmc4 (and mmc4-ff), which apply even more stringent filtration criteria. The aim of core is to identify a \\\"higher-precision\\\" subset of documents that: 1) have a minimum/maximum number of sentences/images per document; 2) pass an even stricter deduplication step; and 3) have a higher image-text similarity. Hyperparameters are selected heuristically and are balanced to downsize the original corpus by an order of magnitude. \\n\\nAs implemented by [27, 28] available from https://github.com/serengil/retinaface. RetinaFace is not perfectly accurate, so selecting a more aggressive threshold (e.g., 99.99%) would not necessarily result in significantly fewer face-containing images removed. \\n\\nMin/max number of sentences: 4/40; min/max number of images 2/15; findimagedupes applied with a threshold of 10; documents are required to have at least 75% of image assignments have CLIP ViT-L/14 similarity of greater than 25.\"}"}
{"id": "tOd8rSjcWz", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Using linear assignment results in comparable image-text similarities to max assignment, but the former spreads images much more evenly, e.g., the per-document mean percent of sentences with an associated image increases from 22% to 34%.\\n\\nFigure 5: Distribution of images and sentences per document; the median document has 2 images/13 sentences. Documents with more sentences tend to have more images, but the correlation is weak (Spearman $\\\\rho = 10.3$).\\n\\nTable 1 gives basic summary statistics of mmc4 (and fewer-faces/core subsets) compared to some other interleaved image/text corpora. Overall, the full version of mmc4 is larger than prior non-public datasets across axes like number of images/number of documents. In addition, the various subsets of the corpus offer trade-offs between privacy, image/text similarity thresholds, etc. Figure 5 gives details about the mean/median number of images/sentences in each document (mean/median # sent.=2.0/5.7; # im = 13.0/24.3) based on a random sample of 22K documents.\\n\\nSources of documents & images. We trace back the top-level domains of documents (webpages) and images to better understand the origins of contents in mmc4. Figure 6 presents the top-20 top-level domains that host the highest number of documents and images in mmc4. The distribution of document sources in mmc4 reveals a relatively uniform pattern, with 101.2M documents distributed across 6.0M unique domains. On average, each domain contains approximately 16.9 documents, with a median value of 2.0. The top 10% most frequently appeared domains account for 77% of all documents in mmc4. The documents are most commonly hosted on news media outlets (e.g., BBC, NY Times, Daily Express, Daily Mail), academic publication sites (e.g., Springer), online encyclopedias (e.g., Wikipedia), and e-commerce sites (e.g., iTunes, Etsy). Conversely, the sources of images in mmc4 exhibit a higher level of clustering. The 571.4M images are hosted on 4.9M domains, with each domain having an average of 116.0 images and a median value of 7.0 images. The top 10% most frequent domains are responsible for hosting 89% of all images. Images are most commonly hosted on blogs (e.g., Blogspot, WordPress), shopping sites (e.g., Amazon), cloud storage sites (e.g., AWS S3, Google storage), or general image hosting sites (e.g., Flickr, Imgur). More detailed lists of top document/image domains in mmc4 and mmc4-core can be found in Appendix C.\\n\\nImage-text similarity. Figure 4 provides detail about the linear assignment process compared to a \u201cmax\u201d assignment alternative, where each image is simply assigned to its maximally CLIP-similar sentence. The linear assignment process slightly decreases the average CLIP similarity between images/sentences (from 24.5 $\\\\to$ 24.0), but significantly more evenly \u201cspreads\u201d images throughout the documents: per-document, the mean percentage of sentences with an associated image rises from 22% $\\\\to$ 34%.\\n\\nTopic-based assessment. We ran LDA [6] as implemented by Mallet [22] on a random sample of 22K documents from mmc4 with $k = 30$ topics. The resulting clusters span a broad set of topics like cooking, communities, travel, music, art, etc. Figure 1 shows some example LDA topic clusters.\"}"}
{"id": "tOd8rSjcWz", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"addition, we explore a sample of the images most associated with the corresponding topic, finding that, in general, image topic clusters align with qualitative expectations.\\n\\nManual verification of image relevance+properties.\\nWe randomly sample 200 documents from mmc4 with the goal of assessing how relevant the images contained in the document are to the assigned sentences and to the document as a whole. Table 3 shows the results on the 836 images contained in the 200 documents. 87.7% of all examined images are topically related to the corresponding document, and 80.4% images are well-aligned to the assigned sentences within each document.\\n\\nWe also assessed several other factors, finding that: 1) 28.3% contain recognizable human faces; 2) 1.6% contain recognizable watermarks; 3) 3.9% are related to logos; 4) 3.2% are related to advertisements; and 5) 0.7% are duplicated with other images in the same document. Appendix D.1 shows more discussion of images with watermarks, ads/logos, etc.\\n\\n5 OpenFlamingo: An Early Application of mmc4\\nThe first publicly available model to be trained on mmc4 is OpenFlamingo [3]. We run ablations on a small version of OpenFlamingo (3B: backbone = OPT-1.3B [37] language model and CLIP ViT-L/14 [24] vision model) to compare direct training on image captions (LAION-2B [26]) to the interleaved sequences of mmc4-core.\\n\\nTo flatten mmc4 documents to training sequences, we: 1) sample a 256 token sub-sequence from each training document; 2) discard images with CLIP image-text similarity less than 20; 3) discard sequences that contain no images after filtering; 4) discard images if there are more than 5 in the resulting sequence. As in [17] we randomly drop sequences with a single image to increase multi-image sequences in the sample.\\n\\nWe compute the mean CLIP ViT-L/14 image vector for each topic by associating each image in a document the document's most common topic; then, we compute the mean image vector per topic. Finally, cosine similarity to this mean vector is used to identify the \\\"most topically central\\\" images per-topic.\\n\\nThe alignment between an image and its assigned sentence is a qualitative criterion. We consider an image-sentence pair to be \\\"well-aligned\\\" when the visual elements of the image have a direct and relevant relationship with the text. This can include instances where the image depicts the context or content of the sentence, or where there is a plausible literal overlap between the text and the image, etc.\\n\\nThe logos can be website logos, commercial logos used by businesses or companies to represent their brand or product, or logos for organizations or events. In all cases, the label is assigned if the logo is the primary focus of the image.\\n\\nThese experiments were conducted using a preliminary v1 of the mmc4-core corpus, see this pull request for discussion of small bugfixes in the current v1.1 version.\\n\\nFuture work would be well-suited to investigate the impact of various flattening schemes on downstream performance; the method described here is just one possible method.\\n\\nSimilar to [2], we find that training on a maximum of five image sequences can be sufficient for Open-Flamingo models to generalize to 32 shots during inference.\"}"}
{"id": "tOd8rSjcWz", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Few shot, in-context MSCOCO captioning performance of OpenFlamingo-3B when training on just captions from LAION-2B vs. mixing in mmc4-core sequences. The model trained on mmc4 sequences is able to generalize to MSCOCO-style captions more effectively vs. the model trained just on LAION-2B image/caption pairs.\\n\\nTable 3: Results of manual verification of 200 randomly sampled documents containing 836 images. A majority of images are topically relevant and well sentence-aligned. The rate of watermarks, ads, duplicates, etc. is low.\\n\\n| Percentage | Description          |\\n|------------|---------------------|\\n| 87.7%      | Topically-related    |\\n| 80.4%      | Sentence-aligned    |\\n| 28.3%      | Has face?           |\\n| 1.6%       | Has watermark?      |\\n| 3.9%       | Logo-related        |\\n| 3.2%       | Ads-related         |\\n| 0.7%       | Duplicated          |\\n\\nValidation CIDEr [35] results for COCO image captioning are in Figure 7. For 4/8-shot in-context learning settings, the model trained on mmc4-core shows 20-30 CIDEr point improvements. The performance of OpenFlamingo-3B trained on just 5M captions/2.5M mmc4 sequences also exceeds a zero-shot application of OpenFlamingo-3B trained on much more data (15M LAION-2B captions); this provides additional evidence that the interleaving in-context setup enables adaptation to MSCOCO-style captions. The performance of the captions-only OpenFlamingo-3B model degrades from 4-shot to 8-shot learning presumably because these longer sequences are significantly different from the single image/captions it's seen at training time.\\n\\n6 Conclusion\\n\\nWe introduce mmc4, a corpus of 100M+ documents with 571M images interleaved in 43B English tokens from the popular c4 dataset. Initial experimental results show that models trained on image/text sequences from mmc4 can more effectively perform multimodal in-context learning compared to models trained on single image/captions. We expect interleaving will be important not only for few-shot learning, but also for more diverse multimodal language technologies wherein users may seek to converse with agents with and about visual content in new ways. Future work includes:\\n\\n1. More precise empirical evaluation of in-context abilities: can models really reason across images/texts in a prompt in flexible ways, or are they limited to interleaved and independent supervised examples?\\n\\n2. Data scaling: is the performance of in-context vision+language learning bottlenecked by the availability of large-scale interleaved corpora? Or is improved single-modal pretraining sufficient to un-bottleneck multimodal models?\\n\\n3. Instruction tuning: while interleaving of independent supervised image+text examples enables in-context learning, training an instruction-following multimodal model directly for this case is a promising complementary direction.\\n\\nAcknowledgements\\n\\nWe thank the OpenFlamingo team, Sangho Lee, and Jiasen Lu for the helpful discussions, and for being early adopters of mmc4. In addition, we thank Jingkang Yang for helpful discussions inspiring mmc4-core. We thank Stability AI for the compute for the OpenFlamingo experiments. This work was supported in part by DARPA MCS program through NIWC Pacific (N66001-19-2-4031), the...\"}"}
{"id": "tOd8rSjcWz", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. Cm3: A causal masked multimodal model of the internet. ArXiv, abs/2201.07520, 2022.\\n\\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.\\n\\n[3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023.\\n\\n[4] Steven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python: analyzing text with the natural language toolkit. \\\"O'Reilly Media, Inc.\\\", 2009.\\n\\n[5] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny, pornography, and malignant stereotypes. arXiv preprint arXiv:2110.01963, 2021.\\n\\n[6] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):993\u20131022, 2003.\\n\\n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\n[8] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021.\\n\\n[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. ArXiv, abs/2204.02311, 2022.\\n\\n[10] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. Retinaface: Single-shot multi-level face localisation in the wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020.\\n\\n[11] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. RedCaps: Web-curated image-text data created by the people, for the people. In NeurIPS Datasets and Benchmarks, 2021.\\n\\n[12] Jesse Dodge, Maarten Sap, Ana Marasovi\u0107, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1286\u20131305, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\"}"}
{"id": "tOd8rSjcWz", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[10] Andrea Frome, German Cheung, Ahmad Abdulkader, Marco Zennaro, Bo Wu, Alessandro Bissacco, Hartwig Adam, Hartmut Neven, and Luc Vincent. Large-scale privacy protection in google street view. In 2009 IEEE 12th international conference on computer vision, pages 2373\u20132380. IEEE, 2009.\\n\\n[11] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander J. Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alexandros G. Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023.\\n\\n[12] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64(12):86\u201392, 2021.\\n\\n[13] Jack Hessel, Lillian Lee, and David Mimno. Unsupervised discovery of multimodal links in multi-image, multi-sentence documents. In EMNLP, 2019.\\n\\n[14] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning perception with language models. ArXiv, abs/2302.14045, 2023.\\n\\n[15] Roy Jonker and Ton Volgenant. A shortest augmenting path algorithm for dense and sparse linear assignment problems. In DGOR/NSOR: Papers of the 16th Annual Meeting of DGOR in Cooperation with NSOR/Vortr\u00e4ge der 16. Jahrestagung der DGOR zusammen mit der NSOR, pages 622\u2013622. Springer, 1988.\\n\\n[16] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83\u201397, 1955.\\n\\n[17] Zejun Li, Zhongyu Wei, Zhihao Fan, Haijun Shan, and Xuanjing Huang. An unsupervised sampling approach for image-sentence matching using document-level structural information. In AAAI, 2021.\\n\\n[18] Emily E Marsh and Marilyn Domas White. A taxonomy of relationships between images and text. Journal of documentation, 59(6):647\u2013672, 2003.\\n\\n[19] Andrew Kachites McCallum. Mallet: A machine learning for language toolkit, 2002.\\n\\n[20] Alex Mei, Michael Saxon, Shiyu Chang, Zachary C Lipton, and William Yang Wang. Users are the north star for ai transparency. arXiv preprint arXiv:2303.05500, 2023.\\n\\n[21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\\n\\n[22] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2020.\\n\\n[23] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\\n\\n[24] Sefik Ilkin Serengil and Alper Ozpinar. Lightface: A hybrid deep face recognition framework. In 2020 Innovations in Intelligent Systems and Applications Conference (ASYU), pages 23\u201327. IEEE, 2020.\"}"}
{"id": "tOd8rSjcWz", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[32] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. YFCC100M: the new data in multimedia research. Communications of the ACM, 59(2):64\u201373, 2016.\\n\\n[33] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam M. Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, Yaguang Li, Hongrae Lee, Huaixiu Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, I. A. Krivokon, Willard James Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Hartz S\u00f8raker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark D\u00edaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravindran Rajakumar, Alena Butryna, Matthew Lamm, V. O. Kuzmina, Joseph Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Huai hsin Chi, and Quoc Le. Lambda: Language models for dialog applications. ArXiv, abs/2201.08239, 2022.\\n\\n[34] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.\\n\\n[35] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. CIDEr: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566\u20134575, 2015.\\n\\n[36] An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, and Julian McAuley. Personalized showcases: Generating multi-modal explanations for recommendations. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2251\u20132255, 2023.\\n\\n[37] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models. ArXiv, abs/2205.01068, 2022.\"}"}
{"id": "tOd8rSjcWz", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4: LDA[6] topic modeling outputs (k=30 topics) when trained on a random sample of documents from mmc4. Topic frequencies are determined by taking the mean distribution over documents in the corpus. Topic names are generated by GPT-4 conditioned on the top 20 words for each topic, prompted by a request for a short 1-2 word summary.\\n\\n| Topic Name      | Rate | Top Words                                      |\\n|-----------------|------|------------------------------------------------|\\n| E-commerce      | 4.61%| products, quality, price, product, online, offer, buy, customers, services, order       |\\n| Healthcare      | 2.55%| health, care, body, patients, treatment, medical, pain, cancer, blood, mental           |\\n| Travel          | 3.98%| city, hotel, park, visit, travel, trip, tour, enjoy, beach, town                        |\\n| Celebrations    | 3.94%| fun, wedding, beautiful, christmas, happy, card, birthday, gift, blog, perfect           |\\n| Music           | 2.50%| music, band, album, song, sound, dance, show, live, musical                            |\\n| Religion        | 2.05%| god, church, jesus, lord, faith, man, father, heart, christ, gods                        |\\n| Fashion         | 4.86%| black, white, size, color, design, wear, style, fabric, cut, fit                        |\\n| Nature          | 3.05%| water, dog, river, fish, dogs, species, animals, fishing, sea, weather                   |\\n| Geography       | 3.56%| city, county, state, york, san, north, west, st, john, south                           |\\n| Business        | 4.15%| management, company, marketing, technology, data, services, team, industry, project, clients |\\n| Technology      | 4.89%| page, app, site, download, website, data, click, google, web, email                     |\\n| Education       | 2.39%| students, school, learning, skills, children, education, learn, student, training, class |\\n| Research        | 1.43%| data, download, research, analysis, study, al, cells, memory, studies, results          |\\n| Food            | 3.31%| food, add, recipe, minutes, chocolate, cream, delicious, chicken, sugar, cheese         |\\n| Law             | 2.14%| law, insurance, court, legal, case, state, letter, act, cover, policy                    |\\n| Wellness        | 1.92%| skin, hair, oil, natural, organic, wine, plant, products, plants, water                |\\n| Self-improvement| 5.27%| change, youre, mind, point, means, fact, thing, ways, question, process                  |\\n| Politics        | 2.73%| government, president, police, political, war, trump, military, state, party, security   |\\n| Engineering     | 2.81%| water, energy, system, power, air, temperature, heat, systems, gas, solar               |\\n| Sports          | 3.01%| game, games, team, play, season, players, win, league, player, football                  |\\n| Economy         | 2.29%| percent, market, million, \u2014, trade, billion, growth, price, company, report             |\\n| Architecture    | 3.08%| room, space, house, kitchen, floor, living, pool, building, large, bedroom              |\\n| Automotive      | 3.20%| car, vehicle, camera, engine, power, system, model, control, speed, phone               |\\n| Community       | 3.91%| community, university, program, research, members, support, development, public, national, group |\\n| Finance         | 1.72%| money, credit, card, real, property, estate, loan, pay, financial, tax                   |\\n| International   | 2.31%| international, india, countries, china, south, history, united, country, europe, indian |\\n| Events          | 3.93%| 2018, event, pm, 2019, 2017, april, 2016, posted, friday, june                           |\\n| Literature      | 3.73%| book, story, books, film, series, movie, read, characters, stories, reading              |\\n| Personal        | 7.96%| ive, didnt, thing, bit, thought, week, wanted, started, pretty, id                       |\\n| Art             | 2.70%| art, design, de, images, ikea, image, painting, collection, piano, photo                 |\\n\\n12\"}"}
{"id": "tOd8rSjcWz", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5 and Table 6 list the top-50 most frequent top-level domains for documents and images as discussed in \u00a7 4. We show domain statistics in both mmc4 and mmc4-core.\\n\\nFigure 8 shows the top-50 top-level domains of documents in c4-en for reference purposes. The domains are sorted by the frequency of occurrence, as the same with results presented in Figure 6.\\n\\nPrevious work also discussed the most represented websites in c4-en ranked by the total number of tokens [12].\\n\\nwww.wikipedia.com\\nwww.nytimes.com\\nwww.bbc.com\\nwww.springer.com\\nwww.latimes.com\\ndo5.b00kmedia.ru\\nwww.theguardian.com\\nwww.huffpost.com\\nwww.forbes.com\\npatents.google.com\\nwww.eventbrite.com\\nwww.businessinsider.com\\nwww.dailymail.co.uk\\nwww.chicagotribune.com\\nwww.foxnews.com\\nwww.telegraph.co.uk\\nwww.cnn.com\\nwww.aljazeera.com\\nwww.ncbi.nlm.nih.gov\\nwww.cnet.com\\nwww.express.co.uk\\nwww.oreilly.com\\nwww.foxbusiness.com\\nwww.theatlantic.com\\nwww.zdnet.com\\nwww.ibtimes.co.uk\\nwww.washingtonpost.com\\nmashable.com\\nwww.deviantart.com\\nwww.rt.com\\nitunes.apple.com\\nforums.macrumors.com\\nwww.booking.com\\nwww.reuters.com\\nwww.fool.com\\nwww.si.com\\nwww.prweb.com\\nwww.instructables.com\\nwww.etsy.com\\nwww.agreatertown.com\\ngithub.com\\nanswers.sap.com\\nwww.youtube.com\\nwww.usatoday.com\\nwww.reference.com\\nwww.hindustantimes.com\\nwww.npr.org\\nnypost.com\\nwww.tripadvisor.com\\n\\nFigure 8: The top-50 most frequent top-level domains for documents in c4-en.\\n\\nD Demonstrative Examples\\n\\nD.1 Images w/ Watermarks/Ads/Logos\\n\\nFigure 9a depicts a few sample images containing watermarks in various forms, Figure 9b shows images that are associated with logos, and Figure 9c lists a few sample images related to advertisements. Notice that the dissimilarity between images associated with logos and those pertaining to advertisements is relatively modest. Although images connected to advertisements may occasionally encompass promotional language or persuasive expressions, they may also solely feature logos. Notably, the principal criterion for determining whether an image is ad-related is contingent upon assessing its relevance to the document. If the image is less related to the document, it is more aptly categorized as ad-related. For instance, the interleaved document presented in Table 7 contains two images associated with logos that are intricately linked to the commercial brand being presented within the document. Consequently, these two images are not classified as advertisements.\\n\\nD.2 Interleaved Document\\n\\nTable 7 and Table 8 show two interleaved docs from mmc4, displaying the list of sentences and the corresponding assigned images, alongside the CLIP ViT/L-14 image-text similarity score.\"}"}
{"id": "tOd8rSjcWz", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Domain Name       | Percentage |\\n|------------------|------------|\\n| www.bbc.com      | 0.0994%    |\\n| www.dailymail.co.uk | 0.2352%   |\\n| www.springer.com | 0.0993%    |\\n| www.alibaba.com  | 0.1601%    |\\n| www.wikipedia.com | 0.0750%    |\\n| www.indiamart.com | 0.1287%    |\\n| www.nytimes.com  | 0.0690%    |\\n| www.teacherspayteachers.com | 0.1116% |\\n| www.express.co.uk | 0.0573%    |\\n| www.rt.com       | 0.0858%    |\\n| www.dailymail.co.uk | 0.0530%   |\\n| www.bbc.com      | 0.0730%    |\\n| www.rt.com       | 0.0519%    |\\n| www.digit-life.com | 0.0728%    |\\n| itunes.apple.com | 0.0508%    |\\n| www.cbc.ca       | 0.0673%    |\\n| www.etsy.com     | 0.0475%    |\\n| www.stitcher.com | 0.0665%    |\\n| www.agreatertown.com | 0.0468%  |\\n| local.firestonecompleteautocare.com | 0.0636% |\\n| app-wiringdiagram.herokuapp.com | 0.0429% |\\n| www.monfrague.online | 0.0629%  |\\n| fineartamerica.com | 0.0425%   |\\n| www.firstpost.com | 0.0555%    |\\n| www.cnn.com      | 0.0407%    |\\n| www.express.co.uk | 0.0552%    |\\n| www.booking.com  | 0.0406%    |\\n| www.androidpolice.com | 0.0535% |\\n| www.tripadvisor.com | 0.0393%   |\\n| www.usatoday.com | 0.0528%    |\\n| www.firstpost.com | 0.0377%    |\\n| www.audible.com  | 0.0481%    |\\n| www.npr.org      | 0.0368%    |\\n| itunes.apple.com | 0.0479%    |\\n| www.wired.com    | 0.0367%    |\\n| inhabitat.com    | 0.0455%    |\\n| www.breitbart.com | 0.0367%    |\\n| www.cnn.com      | 0.0435%    |\\n| www.indiamart.com | 0.0364%    |\\n| www.giftacrossindia.com | 0.0433% |\\n| www.audible.com  | 0.0346%    |\\n| www.houzz.com    | 0.0428%    |\\n| medium.com       | 0.0342%    |\\n| appadvice.com    | 0.0421%    |\\n| www.dailystar.co.uk | 0.0338%   |\\n| www.prweb.com    | 0.0419%    |\\n| www.weddingwire.com | 0.0336%  |\\n| www.timeout.com  | 0.0414%    |\\n| appadvice.com    | 0.0333%    |\\n| wccftech.com     | 0.0412%    |\\n| www.businessinsider.com | 0.0310% |\\n| www.ifompt.com   | 0.0403%    |\\n| hubpages.com     | 0.0303%    |\\n| phys.org         | 0.0383%    |\\n| www.shutterstock.com | 0.0285% |\\n| www.abc.net.au   | 0.0381%    |\\n| www.alibaba.com  | 0.0282%    |\\n| www.acahome.org  | 0.0371%    |\\n| www.techradar.com | 0.0276%   |\\n| www.npr.org      | 0.0368%    |\\n| www.timeout.com  | 0.0265%    |\\n| www.redmondpie.com | 0.0368%  |\\n| economistimes.indiatimes.com | 0.0259% |\\n| babyology.com.au | 0.0367%    |\\n| www.prweb.com    | 0.0256%    |\\n| www.etsy.com     | 0.0367%    |\\n| www.cbc.ca       | 0.0246%    |\\n| fgontheweb.com   | 0.0365%    |\\n| www.houzz.com    | 0.0244%    |\\n| www.pcworld.com  | 0.0359%    |\\n| www.ndtv.com     | 0.0243%    |\\n| www.dailystar.co.uk | 0.0350%   |\\n| www.gsmarena.com | 0.0243%    |\\n| www.realtor.com  | 0.0348%    |\\n| gizmodo.com      | 0.0243%    |\\n| www.wikipedia.com | 0.0342%   |\\n| www.wn.com       | 0.0242%    |\\n| www.advanceduninstaller.com | 0.0342% |\\n| www.thestar.com  | 0.0240%    |\\n| shopwizion.com   | 0.0337%    |\\n| www.deviantart.com | 0.0240%   |\\n| www.drivermax.com | 0.0337%    |\\n| www.indiebound.org | 0.0238%  |\\n| www.template.net | 0.0334%    |\\n| www.telegraph.co.uk | 0.0238%  |\\n| clemsontigers.com | 0.0330%    |\\n| www.teacherspayteachers.com | 0.0236% |\\n| www.imdb.com     | 0.0234%    |\\n| maybeloan.com    | 0.0320%    |\\n| sg.carousell.com | 0.0233%    |\\n| medium.com       | 0.0320%    |\\n| pixels.com       | 0.0228%    |\\n| shoplionly.com   | 0.0320%    |\\n| timesofindia.indiatimes.com | 0.0227% |\\n| www.replacement-laptop-battery.com | 0.0314% |\\n| www.blogtalkradio.com | 0.0227%  |\\n| www.businessinsider.com.au | 0.0312% |\\n| www.glamour.com  | 0.0223%    |\\n| www.dummies.com  | 0.0312%    |\\n| www.comparometer.in | 0.0329%  |\"}"}
{"id": "tOd8rSjcWz", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: Top-50 top-level domains for images in mmc4 and mmc4-core. The symbol \\\"*\\\" is employed to denote specific patterns, such as digits or location acronyms, commonly utilized to differentiate sub-sites within the same domain.\\n\\n| Domain Name         | Percentage |\\n|---------------------|------------|\\n| *.bp.blogspot.com   | 8.7454%    |\\n| s3.amazonaws.com    | 2.1629%    |\\n| i*.wp.com           | 1.3176%    |\\n| images-*.ssl-images-amazon.com | 1.7976% |\\n| *.staticflickr.com  | 1.2530%    |\\n| static*.squarespace.com | 0.9530% |\\n| cdn.atwilltech.com  | 0.9009%    |\\n| i.pinimg.com        | 0.6992%    |\\n| i.ytimg.com         | 0.6644%    |\\n| i*.photobucket.com | 0.5075%    |\\n| res.cloudinary.com  | 0.3683%    |\\n| bt-photos.global.ssl.fastly.net | 0.3827% |\\n| storage.googleapis.com | 0.3466% |\\n| i.imgur.com        | 0.2858%    |\\n| i.etsystatic.com    | 0.3494%    |\\n| lh*.googleusercontent.com | 0.2762% |\\n| *.bstatic.com       | 0.2436%    |\\n| i.dailymail.co.uk   | 0.2896%    |\\n| s-media-cache-ak*.pinimg.com | 0.2270% |\\n| img.youtube.com     | 0.1954%    |\\n| photos.smugmug.com  | 0.1934%    |\\n| cdn.photos.sparkplatform.com | 0.1915% |\\n| img.youtube.com     | 0.1954%    |\\n| i.imgur.com        | 0.2638%    |\\n| photos.smugmug.com  | 0.1934%    |\\n| cdn.photos.sparkplatform.com | 0.1915% |\\n| img.youtube.com     | 0.1954%    |\\n| i.imgur.com        | 0.2638%    |\"}"}
{"id": "tOd8rSjcWz", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) Images with watermarks.\\n(b) Images related to logos.\\n(c) Images related to ads.\\n\\nFigure 9: Manually labeled images with watermarks and images related to logos or ads.\\n\\nTable 7: An example document from mmc4 with interleaved sentences and images, together with the CLIP ViT/-14 image-text similarities. This document contains two logo-related images (the 2nd & 3rd images with \u201cNELO\u201d) that are relevant to the content of this document, and are therefore excluded from the category of advertisement.\\n\\n| Sentence                                                                 | Image | CLIP Similarity |\\n|-------------------------------------------------------------------------|-------|-----------------|\\n| Our new service for teams to manage their fleets for racing.             |       | 23.51           |\\n| Getting boats has never been this easy.                                  |       | 22.40           |\\n| Get a step ahead with the planning for your team and get all the boats you need for next season races. |       |                 |\\n| As easy as adding boats to a list, this service aims to be the simplest way to rent boats, no extra knowledge needed and with full support from our staff. |       |                 |\\n| Get all the features of a Nelo boat, from having great equipment to our service team for a fraction of the price of a new boat. |       | 28.76           |\\n| All our rental boats for racing are carefully maintained and revised between each race so each boat is as good as new. |       |                 |\"}"}
{"id": "tOd8rSjcWz", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 8: A document instance retrieved from the mmc4 dataset is presented, consisting of interleaved textual sentences and accompanying images, along with the CLIP ViT/-14 image-text similarity scores.\\n\\n| Sentence                                                                 | Image Score | CLIP Similarity |\\n|--------------------------------------------------------------------------|-------------|-----------------|\\n| Are you thinking about running a retreat for your own group of people?   |             | 25.93           |\\n| We are happy to help you hosting and organizing your own retreat.        |             | 19.71           |\\n| We work with your interest in mind in designing your retreat, and we facilitate the logistics, supporting you all the way for a great experience. |             | 21.29           |\\n| Nestled within powerful and deeply inspiring nature, in the heart of Tuscany, Italy, Podere Di Maggio is a place born of dreams. |             | 22.35           |\\n| The dream to be close to and learn from nature.                          |             | 19.37           |\\n| The dream to create and share beauty.                                    |             | 19.16           |\\n| The dream to discover and develop the poetry of being and doing.         |             | 18.21           |\\n| We offer an invitation to explore a wide range of life arts: poetry, dance, music, yoga, meditation, ritual, ceramics, painting, singing, photography, seeing, hearing, touching, feeling, cooking, communicating and collaborating; sharing and daring to discover and unfold yourself. |             | 22.69           |\"}"}
