{"id": "HfKOIPCvsv", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendices\\n\\nA Baseline Configurations\\n\\nWe provide the configurations for our real-time baselines (\u00a72.4). We use the open-source, Transformers library and ensure easy replication of our results. Table 4 lists the configurations for dense passage retrieval (Karpukhin et al., 2020) and retrieval-augmented generation (Lewis et al., 2020b). We generally follow the default settings from the Transformers library. Seen in Table 5 is the configuration for the closed-book T5 baseline. We again generally follow the default setting.\\n\\nTable 4: Configurations for dense passage retrieval (Karpukhin et al., 2020) and retrieval-augmented generation (Lewis et al., 2020b) from the Transformers library (Wolf et al., 2020).\\n\\n| Option          | Value              |\\n|-----------------|--------------------|\\n| n_docs          | 5                  |\\n| max_combined_length | 300               |\\n| retrieval_vector_size | 768               |\\n| retrieval_batch_size | 8                |\\n| is_encoder_decoder | True              |\\n| prefix          | None               |\\n| bos_token_id    | None               |\\n| pad_token_id    | None               |\\n| eos_token_id    | None               |\\n| decoder_start_token_id | None           |\\n| title_sep       | '/'                |\\n| doc_sep         | '//'               |\\n| dataset         | wiki_dpr           |\\n| dataset_split   | train              |\\n| index_name      | compressed         |\\n| index_path      | None               |\\n| passages_path   | None               |\\n| use_dummy_dataset | False             |\\n| reduce_loss     | False              |\\n| label_smoothing | 0.0                |\\n| do_deduplication| True               |\\n| exclude_bos_score| False             |\\n| do_marginalize  | False              |\\n| output_retrieved| False              |\\n| use_cache       | True               |\\n| forced_eos_token_id | None              |\\n\\nB RETIME QA Interface\\n\\nFig. 7 shows our RETIME QA interface. It gets updated every week, and all six baselines are evaluated as soon as the questions are available. Submissions will be shown on the same page, together with their submission time.\\n\\nC RETIME QA Statistics\\n\\nTable 6 provide more detailed statistics of RETIME QA from the first four weeks. We analyze the questions from the first four weeks along genres and answer types. We also found that \u223c10% of the questions were not strictly time-sensitive. These questions include, for example, \u201cTemperatures in Britain are set to soar this weekend, but what is the hottest UK temperature on record?\u201d from June 17, 2022. We do not filter out these cases to simulate information-seeking, naturally-occurring scenarios.\"}"}
{"id": "HfKOIPCvsv", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5: Configuration for the closed-book T5 baseline (Raffel et al., 2020) from the Transformer library.\\n\\n| Option            | Value                          |\\n|-------------------|--------------------------------|\\n| _name_or_path     | /home/patrick/t5/t5-11b-ssm-nq |\\n| architectures     | [\\\"T5ForConditionalGeneration\\\"]|\\n| d_f               | 65536                          |\\n| d_kv              | 128                            |\\n| d_model           | 1024                           |\\n| decoder_start_token_id | 0                     |\\n| dropout_rate      | 0.1                            |\\n| eos_token_id      | 1                              |\\n| feed_forward_proj | \\\"relu\\\"                        |\\n| initializer_factor| 1.0                            |\\n| is_encoder_decoder| True                           |\\n| layer_norm_epsilon| 1e-06                         |\\n| model_type        | t5                             |\\n| num_decoder_layers| 24                             |\\n| num_heads         | 128                            |\\n| num_layers        | 24                             |\\n| output_past       | True                           |\\n| pad_token_id      | 0                              |\\n| relative_attention_num_buckets | 32                      |\\n| tokenizer_class   | T5Tokenizer                    |\\n| vocab_size        | 32128                          |\\n\\nFigure 7: REALTIME QA interface. It is updated every week, and all six baselines are evaluated as soon as the questions are available. Submissions will be shown on the same page, together with their submission time.\"}"}
{"id": "HfKOIPCvsv", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: Detailed statistics (\\\\%) of RETIME QA. We analyze the questions from the first four weeks along genres and answer types. We also found that \\\\( \\\\sim 12 \\\\% \\\\) of the questions were not strictly time-sensitive. These questions include, for example, \u201cTemperatures in Britain are set to soar this weekend, but what is the hottest UK temperature on record?\u201d from June 17, 2022. We do not filter out these cases to simulate information-seeking, naturally-occurring scenarios.\\n\\n| Genre        | Politics | Business | Entertain | Science | Technology | Health | Disaster |\\n|--------------|----------|----------|-----------|---------|------------|--------|----------|\\n|              | 36.9%    | 17.5%    | 17.5%     | 7.0%    | 7.0%       | 8.8%   | 5.2%     |\\n\\n| Answer Type   | Person | Location | Time | Number | Organization | Event | Miscellaneous |\\n|---------------|--------|----------|------|--------|--------------|-------|---------------|\\n|               | 12.3%  | 19.2%    | 5.3% | 22.8%  | 3.5%         | 8.8%  | 28.1%         |\"}"}
{"id": "HfKOIPCvsv", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nWe introduce REALTIME QA, a dynamic question answering (QA) platform that announces questions and evaluates systems on a regular basis (weekly in this version). REALTIME QA inquires about the current world, and QA systems need to answer questions about novel events or information. It therefore challenges static, conventional assumptions in open-domain QA datasets and pursues instantaneous applications. We build strong baseline models upon large pretrained language models, including GPT-3 and T5. Our benchmark is an ongoing effort, and this paper presents real-time evaluation results over the past year. Our experimental results show that GPT-3 can often properly update its generation results, based on newly-retrieved documents, highlighting the importance of up-to-date information retrieval. Nonetheless, we find that GPT-3 tends to return outdated answers when retrieved documents do not provide sufficient information to find an answer. This suggests an important avenue for future research: can an open-domain QA system identify such unanswerable cases and communicate with the user or even the retrieval module to modify the retrieval results? We hope that REALTIME QA will spur progress in instantaneous applications of question answering and beyond.\\n\\nIntroduction\\n\\nQ: How many home runs has Shohei Ohtani hit?\\nA: 24\\nQ: How many home runs has Shohei Ohtani hit?\\nA: 25\\n\\nFigure 1: REALTIME QA establishes a framework to benchmark question answering at the present time: answers (e.g., the number of Shohei Ohtani's home runs) change in real time.\\n\\nHow many home runs has Shohei Ohtani hit so far this season?\\nA user of a question answering (QA) system might ask such time-sensitive questions and seek out answers in real time. Widely-used evaluation benchmarks of QA systems, however, implicitly assume that answers are static regardless of the time of inquiry. Several recent works (Jia et al., 2018; Chen et al., 2021; Zhang and Choi, 2021; Li\u0161ka et al., 2022) challenged this assumption and proposed QA datasets that specify the temporal context (e.g., who was the President of the U.S. in 1940?). We extend these recent efforts on time-sensitive QA to fulfill real-time, more instantaneous information needs from users: we establish \u2217Work was done during JK's internship at AI2.\\n\\nhttps://realtimeqa.github.io/.\\n\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\"}"}
{"id": "HfKOIPCvsv", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We develop an annotation framework (\u00a72) and a benchmarking timeline for real-time QA system submissions. Every week, REALTIME QA retrieves news articles and human-written, multiple-choice questions from news websites (CNN, THE WEEK, and USA Today), covering a wide range of topics, including politics, business, sports, and entertainment. We upload these data, as well as our baseline results, to our website, and any model submission can be evaluated until the next set of questions is posted. This dynamic scheme contrasts with the well-established QA annotations (Chen et al., 2017; Chen and Yih, 2020) that are performed only once with information available at the time. Such annotations are effective for factoid (Berant et al., 2013; Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) or commonsense questions (Zellers et al., 2018, 2019; Talmor et al., 2019; Sakaguchi et al., 2020), but not the real-time information needs that are our target.\\n\\nWe present two classes of real-time baseline systems that are built on strong, recent models (GPT-3: Brown et al., 2020; T5: Raffel et al., 2020; BART: Lewis et al., 2020a): open-book and closed-book QA models. We present a prompting method to use GPT-3 for open-domain QA. The former class uses an external knowledge source, such as Wikipedia (Min et al., 2019; Guu et al., 2020; Lewis et al., 2020b; Izacard and Grave, 2021) or news articles. The latter class of closed-book models directly outputs an answer to each question. By design, these closed-book baselines have no access to information more recent than the time of pretraining or finetuning, thereby helping us understand the degree to which real-time information is truly necessary. Notably, a small number of questions in REALTIME QA (\u223c12%) do not strictly require recent information; for example, Shohei Ohtani hits a home run today, leading one to ask where he was born. This is consistent with information-seeking, naturally-occurring scenarios that we target in this work, as seen in Clark et al. (2020). Most users of a QA system do not exclusively ask time-sensitive questions, even though these questions may be stimulated by current events; QA systems should aim to address these questions as well.\\n\\nWe evaluate six baselines both in multiple-choice and generation settings in real time and report the results over the period of June 17 through June 2, 2023. These evaluation data resulted in a total of 1,470 QA pairs (Fig. 2). Further, we provide 2,886 QA pairs that are collected in the same way but preceded our real-time evaluations. These can be used in later work for model development (e.g., finetuning). Our results show that an open-book GPT-3 model augmented with up-to-date text retrieval substantially outperforms closed-book baselines, as well as open-book models with retrieval from a past Wikipedia dump (Lewis et al., 2020b). This result illustrates that large language models...\"}"}
{"id": "HfKOIPCvsv", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Examples of weekly quizzes from CNN and THE WEEK that are extracted during annotations of REAL TIME QA. They span diverse genres, including politics, business, and entertainment.\\n\\ncan adjust their knowledge, based on the retrieved passages (\u00a73). Nonetheless, we find that they still struggle, especially when the multiple choices include uncertainty (e.g., \u201cnone of the above\u201d). Most of the errors originate from retrieval, rather than reading comprehension. The REAL TIME QA benchmark, therefore, highlights the importance of fast, up-to-date text retrieval (Seo et al., 2019) to better serve instantaneous information needs. We share all data and code to reproduce our baselines so that follow-up work can build upon our first attempts to tackle this task.\\n\\nREAL TIME QA can also serve as an important step toward much-needed, broader, real-time applications of NLP. For example, a QA system with timely updates can improve emergency management of natural disasters (Imran et al., 2013, 2015, 2016; Nguyen et al., 2016) or pandemics (e.g., COVID-19; Wang et al., 2020; Lee et al., 2020; M\u00f6ller et al., 2020; Alzubi et al., 2021). With the advent of online news, prior work developed automated systems that regularly retrieve and summarize news articles from the Internet (Allan et al., 2001; Radev et al., 2001; McKeown et al., 2002, 2003; Evans et al., 2004). Models developed for the REAL TIME QA task can be further enhanced with such retrieval/summarization systems. We hope that our REAL TIME QA interface and baseline models will serve as a useful platform for research and real-world applications.\\n\\n2 REAL TIME QA Framework\\n\\nOur current version announces questions every week, based on news articles published within the past week. Here we establish the workflow (\u00a72.1) and the framework for annotations (\u00a72.2) and evaluations (\u00a72.3). We then discuss our built-in baselines (\u00a72.4) that are continually evaluated every week. Our user interface and more detailed statistics (e.g., genres and answer types) are available in Appendices B and C.\\n\\n2.1 Workflow\\n\\nFig. 3 depicts the REAL TIME QA workflow for each week. We announce \u223c30 multiple-choice questions at 3 am GMT every Saturday. We internally run API search (Google custom search, GCS) for these questions and share a set of documents (mostly news articles) with the URLs that are available at that time. Participants run their model on these questions, optionally using the documents from our API search as a knowledge source (indicated as dashed lines in Fig. 3). While we provide our document set to lower barriers to submission, participants are also allowed to create and use knowledge sources by themselves (e.g., custom retrieval models or other external APIs such as Twitter API). System submissions are shared on our website with their performance and submission time. The submission window closes when the new set of questions is announced the next week. Note that fair, retroactive comparisons of systems are also possible, as long as they use data available when the submission window was still open. For instance, participants might be interested in evaluating their model against a past submission on the Week N questions. In this case, they can do so by ensuring that their system only relies on data up to Week N and simulating how their system would have performed at that time. Our platform still focuses on real-time evaluations and encourages every participant to submit real-time results to better reflect real-world applications.\"}"}
{"id": "HfKOIPCvsv", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.2 Annotation\\n\\nQuestion Extraction\\n\\nThe authors of this paper perform weekly annotations in a human-in-the-loop way. We first find web pages for \\\"weekly quizzes\\\" from three news websites: CNN (US-based), USA Today, and The WEEK (UK-based).\\n\\nShown in Fig. 4 are examples that span politics and business genres. We then execute our extraction script to collect multiple-choice questions. Each of these three websites posts $\\\\sim 10$ questions per week, resulting in $\\\\sim 120$ questions in total every month. Weekly quizzes are also available from the New York Times and ABC Australia, but they are not included in the current version, due to issues with automatic extraction or a paid subscription system.\\n\\nAPI Search\\n\\nUsing each of these questions as a retrieval query, we run Google custom search to collect the top-10 documents from the web. The retrieval target is all articles from CNN, USA Today, and THE WEEK. We then parse every document using the newspaper3k package and store the text as well as metadata, such as the publication date and author name. In some rare cases, articles from the search get taken down, in which case we disregard them. This indeed illustrates a unique challenge of real-time applications with constantly-changing, dynamic information.\\n\\n2.3 Evaluation\\n\\nMultiple Choice\\n\\nSince REALTIME QA is a multiple-choice question dataset, we can simply measure performance by accuracy. We also explored a NOTA (none of the above) setting: one of the original choices is randomly replaced with \\\"none of the above,\\\" thereby helping prevent models from exploiting heuristics (Rajpurkar et al., 2018). As expected, the NOTA setting resulted in performance degradation across the board ($\\\\S3$). NOTA choices can be found in other multiple-choice QA or reading comprehension datasets (Richardson et al., 2013; Lai et al., 2017).\\n\\nGeneration\\n\\nWe also experiment with a generation setting where no choices are given, to better reflect real-world applications. Under this setting, we evaluate performance with exact matching and token-based F1 scores, following the standard practice in question answering (Rajpurkar et al., 2016).\\n\\nHuman Performance\\n\\nWe randomly sampled 10 weeks from June 17, 2022 through January 13, 2023 (300 questions in total), and the authors of this paper answered multiple-choice questions using Google search. This resulted in the accuracy of 96.7%. Most questions in REALTIME QA are straightforward (e.g., single-hop questions) and a human with Internet access can easily answer them.\\n\\nFor the sustainability of the dynamic benchmark, we do not provide an estimate of human performance on a regular basis.\\n\\n2.4 Real-time Baselines\\n\\nREALTIME QA executes six baselines in real time that are based on strong pretrained models: four open-book and two closed-book models. These six models are evaluated and made publicly available when weekly questions are announced. Any submission to REALTIME QA is compared against them.\\n\\nParticipants can also build their model upon our baselines. See Appendix $\\\\S A$ for more detail.\\n\\n2.4.1 Open-book QA Models\\n\\nOpen-book QA models follow a two-step pipeline: document retrieval that finds evidence documents from an external knowledge source (e.g., Wikipedia) and answer prediction (or reading comprehension) that outputs an answer conditioned on the question and evidence documents. For either step, we experiment with two variants, resulting in a total of four configurations. Open-book systems have the advantage of being capable of updating the external knowledge source at test time (Lewis et al., 2020b). This property is particularly crucial for questions in REALTIME QA that inquire about information at the present time.\\n\\n3 Fair use is allowed under Section 107 of the Copyright Act in the U.S.: https://www.copyright.gov/title17/92chap1.html#107.\\n\\n4 https://programmablesearchengine.google.com/.\\n\\n5 https://github.com/codelucas/newspaper.\\n\\n6 In fact, USA Today has a record of human top scorers every week, and they all get perfect scores. E.g., https://www.usatoday.com/storytelling/quiz/news-quiz/2022-07-01.\"}"}
{"id": "HfKOIPCvsv", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For the retrieval step, we experiment with two configurations: top-5 Wikipedia documents from dense passage retrieval (DPR; Karpukhin et al., 2020) and top-5 news articles from GCS (\u00a72.2). In DPR, English Wikipedia articles from the December 2018 dump are segmented into 100-word documents (Wang et al., 2019). DPR encodes the question and every document into 768-dimensional vectors; it then computes the inner product to obtain a matching score and selects documents with top-5 matching scores. We use the BERT-based model (Devlin et al., 2019), finetuned on the Natural Questions dataset (Kwiatkowski et al., 2019) from the Hugging Face Transformers library (Wolf et al., 2020). GCS uses an external API, and we found that it sometimes returned fewer than five documents (\u223c10% of the time); in this case, we add top documents from DPR to create a top-5 document set.\\n\\nWe explore two methods for answer prediction, conditioned on the question and the corresponding retrieved text: retrieval-augmented generation (RAG; Lewis et al., 2020b) and a prompting method with GPT-3 (text-davinci-002; Brown et al., 2020). In the multiple-choice setting, we compute the log probability of every choice and normalize it by the generation sequence length. We then select the choice with the best score. For the generation setting, we simply perform text decoding.\\n\\nFor the RAG baseline, we use the BART-based (Lewis et al., 2020a) RAG-sequence model, again finetuned on Natural Questions from the Transformers library. This model predicts the answer sequence $y$ autoregressively from left to right while marginalizing over the set of top-5 retrieved documents ($Z$):\\n\\n$$P(y) = \\\\sum_{z \\\\in Z} P(z) \\\\prod_{t=1}^{\\\\left|y\\\\right|} P(y_t | z, y_{\\\\leq t})$$\\n\\nHere $P(z)$ is given by the matching score from the retrieval step. In the equation, the conditioned-upon question is suppressed for brevity.\\n\\nWe propose a straightforward GPT-3 prompting method with temporal contexts (Fig. 5). We prepend to every question the title and the first two paragraphs of the top-5 articles from the document retrieval step. The publication date is inserted, using the metadata of each retrieved article (e.g., \u201cArticle on November 2, 2021\u201d in Fig. 5). For Wikipedia passages retrieved by DPR, we prepend \u201cDecember 31, 2018,\u201d based on the Wikipedia dump date (Karpukhin et al., 2020). Our ablation studies on date insertion will show that the open-book GPT-3 system benefits from specifying the dates of the question and the retrieved articles to some extent (\u00a73.2).\\n\\n2.4.2 Closed-book QA Models\\n\\nClosed-book QA models directly answer questions without access to external knowledge. They have proven competitive with open-book models on some QA datasets (Roberts et al., 2020; Guu et al., 2020). Since these models are trained/finetuned on the data available at that time, they cannot address questions about new events or updated information. Nonetheless, some of the real-time information needs do not necessarily require up-to-date information. Indeed, REALTIME QA contains a small portion of such questions (\u223c10%). For instance, Microsoft retired its Internet Explorer browser this week. What year did it debut? Such questions are triggered by a new event but inquire about facts in the past that have not changed recently. Most users of a QA system do not exclusively raise time-sensitive questions, and QA systems should aim to address these questions as well. Closed-book baselines thus quantify the degree to which up-to-date information is necessary to answer questions in REALTIME QA. We use the following two strong methods for closed-book QA.\\n\\nUnlike DPR, GCS does not provide matching scores. We treat top-5 documents with equal probabilities. See Lazaridou et al. (2022) for other prompt templates. This substantially reduces the inference cost. They contain most of the key information in each article.\"}"}
{"id": "HfKOIPCvsv", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Finetuning Method\\n\\nWe use the T5 model (T5-11B; Raffel et al., 2020) finetuned on the Natural Questions data, again from the Transformers library. Following the open-book baseline, we select the choice with the maximum average log score in the multiple-choice setting.\\n\\nPrompting Method\\n\\nSimilar to the open-book baselines (\u00a72.4.1), we apply a prompting method to GPT-3 (text-davinci-002). We use the same prompt as Fig. 5, except that no articles are inserted before the question. Again following the open-book baselines, we select the choice with the maximum average log score in the multiple-choice setting.\\n\\n3 Experiments and Analysis\\n\\nWe started our real-time experiments on June 17 2022, spanning a year as of June 2 2023 (1470 questions in total). We will continue our weekly annotations, but here we report our experimental and analysis results so far and give guidance to future participants.\\n\\nTable 1: Results from the past year (from June 17, 2022 through June 2, 2023). GCS: Google custom search; DPR: dense passage retrieval (Karpukhin et al., 2020); RAG: retrieval-augmented generation (Lewis et al., 2020b).\\n\\n|                     | Multi-choice Generation | Retrieve | Predict | Orig. | NOTA | EM | F1 |\\n|---------------------|------------------------|----------|---------|-------|------|----|----|\\n| Open                |                        |          |         |       |      |    |    |\\n| DPR                 | 27.4                   | 24.8     | 2.4     | 4.1   |      |    |    |\\n| DPR GPT-3           | 43.9                   | 35.8     | 13.3    | 19.7  |      |    |    |\\n| GCS                 | 46.9                   | 37.9     | 17.5    | 22.1  |      |    |    |\\n| GCS GPT-3           | 66.5                   | 58.4     | 34.6    | 45.3  |      |    |    |\\n| Closed              |                        |          |         |       |      |    |    |\\n| \u2014 T5                | 39.1                   | 35.3     | 9.7     | 14.7  |      |    |    |\\n| \u2014 GPT-3             | 44.9                   | 34.1     | 15.3    | 22.3  |      |    |    |\\n\\nTable 2: Ablation studies on date insertion in the prompt for the open-book (Google custom search; GCS) and close-book GPT-3 baselines. All results are averaged over the first six weeks: June 17 through July 22, 2022.\\n\\n| Date Insert | Multi-choice Generation | Articles | Qs | Orig. | NOTA | EM | F1 |\\n|------------|-------------------------|----------|----|-------|------|----|----|\\n| Open       |                         | \u2713        | \u2713  | 69.3  | 59.8 | 28.7 | 39.4 |\\n|            |                         | \u2713        | \u2717  | 66.5  | 62.6 | 24.7 | 36.3 |\\n|            |                         | \u2717        | \u2713  | 67.0  | 57.5 | 28.1 | 38.2 |\\n|            |                         | \u2717        | \u2717  | 65.9  | 61.5 | 28.7 | 38.3 |\\n| Closed     | \u2713                       | 39.7     | 31.3 | 7.3  | 15.2 |    |    |\\n|            | \u2717                       | 45.8     | 38.5 | 9.0  | 15.9 |    |    |\\n\\n3.1 Results\\n\\nSeen in Table 1 are the results from the past year. In all three settings (original/NOTA multiple choice and generation), GPT-3 with Google custom search (GCS) retrieval achieves the best performance. In particular, GPT-3 with GCS substantially outperforms both closed-book GPT-3 and GPT-3 with DPR (from a December 2018 Wikipedia dump): e.g., 34.6 vs. 15.3/13.3 in generation exact matching. This suggests that GPT-3 is able to answer questions based on the given prompt, rather than relying on past information from pretraining. Nevertheless, we still see a large performance drop of all baselines from the original multiple-choice setting to NOTA (\u201cnone of the above\u201d): e.g., 58.4 vs. 66.5 for GPT-3 with GCS retrieval. Future work can further improve GPT-3\u2019s ability of reading comprehension, especially regarding answer uncertainty.\\n\\n3.2 Analysis and Ablations\\n\\nDate Insertion for Prompting\\n\\nOur prompt for the GPT-3 baselines prepends date information both to the articles and question (Fig. 5). Table 2 shows results from ablation studies on date insertion for the open-book (GPT-3 with Google custom search) and closed-book GPT-3 models. Temporal specification almost always helps the open-book GPT-3 model. Interestingly, it hurts the performance of the closed-book model, perhaps because the specified date is generally unseen during pretraining and the prompt becomes \u201cout-of-domain.\u201d\\n\\nError Breakdown\\n\\nWe conducted a manual error analysis of the results so far. In particular, we categorized answers from the best generation model (open-book GPT-3 with GCS) into three categories: correct, retrieval error, and reading comprehension error. For the questions from the first six weeks, the breakdown was the following: correct (52%), retrieval error (34%), and reading comprehension error (13%). This suggests that the key to instantaneous applications of question answering is accurate, up-to-date information retrieval.\"}"}
{"id": "HfKOIPCvsv", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Performance vs. submission time (hours after the announcement of questions, 3 am GMT on Saturday) over the three evaluation settings (A: original multiple choice; B: none of the above; C: generation). All results are from open-book GPT-3 with Google custom search (GCS) and averaged over the questions from June 17, 2022 through July 22, 2022.\\n\\nExamples\\nTable 3 shows some examples that compare the closed-book and open-book GPT-3 models. The first three examples illustrate that GPT-3 can correctly update its answer based on the retrieved documents across diverse genres: natural disasters, the COVID-19 pandemic, and entertainment. The last three cases, on the other hand, demonstrate a critical limitation of current large language models in temporal understanding: the retrieved documents do not suffice to answer the questions due to a temporal gap, and GPT-3 still generates an outdated answer. Ideally, GPT-3 should inform the user or even the retrieval module that it does not have enough evidence to answer the question. This way, the retrieval module can expand its search, or the user can consult other resources.\\n\\nNote that it is possible to limit the retrieval target to recent articles, but there are potential failure modes. Firstly, some questions in REAL TIME QA inquire about the past, and models can benefit from older articles when answering such questions. Further, the appropriate date range for retrieval varies from question to question in real-world applications; some questions inquire about this year, while others about this week. We thus do not implement such filtering for the current real-time baselines.\\n\\n4 Related Work\\nREAL TIME QA has time sensitivity, which several prior works addressed on various NLP tasks. Here we discuss its relation to long-standing summarization and text retrieval tasks, as well as recent work on temporal misalignment between training and evaluation. We then discuss its connections to dynamic evaluations and open-domain QA.\\n\\nSummarization/Retrieval over Time\\nTemporal (or timeline) summarization is a task that retrieves documents from the web and provides their summary over time (Catizone et al., 2006; Aslam et al., 2013, 2014, 2015; Martschat and Markert, 2017, 2018). Update summarization (Witte et al., 2007; Dang and Owczarzak, 2008) and new event detection/track (Allan et al., 1998; Li et al., 2005) are tasks that monitor and track newly-added information. Prior work created datasets and systems for these tasks (Tran et al., 2013, 2015; Wang et al., 2015; Chen et al., 2019; Gholipour Ghalandari and Ifrim, 2020). Their evaluations are usually executed statically, with information available at the time of data collection.\\n\\nIn contrast, the TREC real-time summarization track evaluates systems in real time during a 1\u20132 week evaluation period (Lin et al., 2016, 2017; Sequiera et al., 2018). Several other works and initiatives focused particularly on financial news summarization (Filippova et al., 2009; Passali et al., 2021) or emergency management technology (Temnikova et al., 2014; Ghosh et al., 2017; McCreadie et al., 2019), including the COVID-19 pandemic (Buntain et al., 2020; Pasquali et al., 2021). This work regularly evaluates question answering systems over diverse topics, but we share the goal of dealing with novel and evolving information over time; retrieval or summarization methods from these tasks (e.g., Yan et al., 2011a,b, 2012; Shou et al., 2013) can be combined with models in REAL TIME QA to serve various time-sensitive information needs from users. REAL TIME QA can also be used to evaluate time-sensitive retrieval systems by the downstream QA performance.\\n\\nIndeed, GCS has a paid version with a date range feature that filters retrieval results by date.\"}"}
{"id": "HfKOIPCvsv", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Examples that compare closed-book and open-book GPT-3 answers with top-5 articles from Google custom search (GCS) retrieval. As in the first three examples, GPT-3 can adjust its answer based on newly-retrieved documents. When the retrieved documents are outdated or unrelated, however, GPT-3 ignores the temporal gap and yields an outdated answer.\\n\\n| Question                                                                 | Retrieved Documents (Top-5)                                                                 | Answer                                                                 |\\n|--------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|----------------------------------------------------------------------|\\n| Historic rainfall led to flooding, mudslides and visitor evacuations at which national park? | Date: June 17, 2022                                                                      | Yellowstone National Park                                           |\\n|                                                                          | Answer: Yellowstone National Park                                                          | Closed GPT-3: Yosemite National Park                                  |\\n|                                                                          | Open GPT-3: Yellowstone National Park                                                      |                                                                     |\\n|                                                                          | June 14, 2022                                                                            |                                                                      |\\n|                                                                          | Yellowstone National Park flooding 'still raging'...                                      |                                                                      |\\n|                                                                          | June 13, 2022                                                                            |                                                                      |\\n|                                                                          | Yellowstone National Park closes entrances, evacuates visitors amid 'unprecedented' rainfall... |                                                                      |\\n|                                                                          | June 15, 2022                                                                            |                                                                      |\\n|                                                                          | Dozens evacuated as unprecedented flooding forces Yellowstone National Park to close...    |                                                                      |\\n|                                                                          | June 15, 2022                                                                            |                                                                      |\\n|                                                                          | Yellowstone still closed as flooding recedes and thousands evacuate...                    |                                                                      |\\n|                                                                          | June 14, 2022                                                                            |                                                                      |\\n|                                                                          | Home swept away as Yellowstone National Park is hit by major floods and mudslides...       |                                                                      |\\n| Covid-19 vaccinations in the US began for which age group this week? | Date: June 24, 2022                                                                      | Children under 5                                                   |\\n|                                                                          | Answer: Children under 5                                                                  | Closed GPT-3: 18 and up                                             |\\n|                                                                          | Open GPT-3: Children under 5                                                              |                                                                     |\\n|                                                                          | November 2, 2021                                                                          | CDC recommends Pfizer COVID-19 vaccine for kids 5-11, shots expected to roll out this week... |\\n|                                                                          | June 23, 2022                                                                            |                                                                      |\\n|                                                                          | Covid-19 vaccinations begin for US children under 5...                                    |                                                                      |\\n|                                                                          | July 22, 2021                                                                            | Biden says kids under 12 could be eligible for COVID vaccines in weeks... |\\n|                                                                          | November 10, 2021                                                                         | COVID-19 cases on the rise again in Iowa...                         |\\n| Which wildly popular show was recently green lit for a new season? | Date: June 17, 2022                                                                      | Squid Game                                                        |\\n|                                                                          | Answer: Squid Game                                                                       | Closed GPT-3: The show \\\"Game of Thrones\\\" was recently green lit for a new season... |\\n|                                                                          | Open GPT-3: Squid Game                                                                   |                                                                     |\\n|                                                                          | June 12, 2022                                                                            | Netflix green lights 'Squid Game' season 2...                       |\\n|                                                                          | June 17, 2022                                                                            | 5 things to know for June 17...                                    |\\n|                                                                          | June 4, 2019                                                                             | 'Looking for Alaska' details revealed for Hulu limited series...    |\\n|                                                                          | February 4, 2022                                                                         | The Busch Light Clash goes green this weekend...                   |\\n|                                                                          | September 26, 2018                                                                       | Dip into 4 new mysteries for fall, including Kate Atkinson's spy novel 'Transcription'... |\\n| The IRS announced it will do what this week? | Date: June 24, 2022                                                                      | Finish processing the backlog of 2021 tax returns                  |\\n|                                                                          | Answer: Finish processing the backlog of 2021 tax returns                                 | Closed GPT-3: The IRS announced it will begin processing tax returns this week. |\\n|                                                                          | Open GPT-3: The IRS announced it will begin processing 2021 tax returns as soon as Jan. 24... |                                                                     |\\n|                                                                          | January 10, 2022                                                                         | IRS 2022 tax season set to begin 2 weeks early on Jan. 24...       |\\n|                                                                          | March 12, 2021                                                                           | When will I get my third stimulus check?...                        |\\n| Which country is now \\\"bankrupt,\\\" according to a statement this week from its administration? | Date: July 8, 2022                                                                      | Sri Lanka                                                        |\\n|                                                                          | Answer: Sri Lanka                                                                       | Closed GPT-3: Greece                                               |\\n|                                                                          | Open GPT-3: Venezuela                                                                    |                                                                     |\\n|                                                                          | March 2, 2022                                                                            | Gun manufacturers are not entirely exempt from being sued... the now-bankrupt gun manufacturer... |\\n|                                                                          | March 12, 2021                                                                           | Mitch McConnell seeks to end Democrat's 'crazy policy' of beefed-up unemployment benefits... let states go bankrupt... |\\n| Which head of state announced his resignation this week? | Date: July 8, 2022                                                                      | UK Prime Minister Boris Johnson                                     |\\n|                                                                          | Answer: UK Prime Minister Boris Johnson                                                   | Closed GPT-3: Japanese Prime Minister Shinzo Abe announced his resignation this week. |\\n|                                                                          | Open GPT-3: Andrew Cuomo                                                                  |                                                                     |\\n|                                                                          | August 11, 2021                                                                          | NY Gov. Andrew Cuomo will resign in two weeks...                   |\\n|                                                                          | September 21, 2021                                                                       | Maricopa County Supervisor Steve Chucri to resign...               |\\n|                                                                          | January 25, 2016                                                                         | Ball State president Ferguson resigns...                           |\\n|                                                                          | March 23, 2021                                                                          | Oregon State University President F. King Alexander resigns...    |\\n|                                                                          | August 10, 2021                                                                         | NY Gov. Andrew Cuomo to resign amid scandal...                     |\\n|                                                                          |\"}"}
{"id": "HfKOIPCvsv", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Temporal Misalignment and Degradation\\n\\nWhile not particularly motivated by instantaneous information needs like REALTIME QA, prior work also explored temporal aspects of a variety of NLP tasks. A flurry of recent work analyzed performance degradation from temporal misalignment between (pre)training and evaluation/deployment on many NLP tasks (Lazaridou et al., 2021; R\u00f6ttger and Pierrehumbert, 2021; Luu et al., 2022; Onoe et al., 2022) and proposed mitigation methods (Huang and Paul, 2018, 2019; Dhingra et al., 2022; Jang et al., 2022a,b; Lee et al., 2022). An open-book QA model conditions answer generation upon newly-retrieved documents (Lewis et al., 2020b), but the extent to which answer generation can be updated based on the retrieved documents is limited (Longpre et al., 2021b). Temporal degradation is, therefore, one of the challenges that models in REALTIME QA need to address.\\n\\nDynamic Benchmarks\\n\\nUnlike the majority of datasets in natural language processing, REALTIME QA evaluates systems dynamically and its evaluations change over time. Several other prior works update challenge test sets (Kiela et al., 2021; Potts et al., 2021; Ma et al., 2021), evaluation tasks (Thrush et al., 2022), or metrics (Gehrmann et al., 2021, 2022; Mishra and Arunkumar, 2021; Kasai et al., 2022). REALTIME QA hosts a similar online platform and adopts a dynamic scheme specifically to pursue instantaneous applications.\\n\\nOpen-Domain QA\\n\\nMuch prior work proposed datasets for open-domain QA for English and beyond (Clark et al., 2020; Asai et al., 2021, 2022; Longpre et al., 2021a; Zhang et al., 2021). Several recent works challenged the conventional problem setups (Chen and Yih, 2020) where correct answers can be found from a fixed, external knowledge source, such as Wikipedia. Similar to REALTIME QA, Zhang and Choi (2021); Li\u0161ka et al. (2022) focused on temporal or geographical contexts that can change the answer to the same question. Consistent with these prior efforts, REALTIME QA aims toward broader applications of question answering beyond the conventional framework.\\n\\n5 Conclusion and Future Work\\n\\nWe introduce REALTIME QA, a dynamic, open-domain QA benchmark that asks questions at the present time. Our platform announces questions every week and continually evaluates six real-time baselines. Our experiments from the first year suggest that accurate, up-to-date information retrieval is particularly important to serve speedy information needs. We hope that REALTIME QA encourages research efforts toward fast, accurate applications of natural language processing.\\n\\nLimitations\\n\\nThis work aims to develop a QA benchmark for addressing instantaneous information needs, including emergency management. The current version of REALTIME QA has two major limitations due to our annotation framework (\u00a72.2): 1) question/answer pairs are all written in English, and the covered topics tend to be English-centric (US and UK); 2) questions are announced on a weekly basis, rather than a truly instantaneous basis. Nevertheless, our benchmark departs from many static datasets from prior work and provides an important step towards the research goal. We hope to develop future versions of REALTIME QA that mitigate these limitations.\\n\\nAcknowledgements\\n\\nWe thank Noriyuki Kojima, Alisa Liu, Ofir Press, Koji Shiono, Wenya Wang, the ARK group at the UW, and the Mosaic team at the Allen Institute for AI for their helpful feedback on this work.\\n\\nReferences\\n\\nJames Allan, Rahul Gupta, and Vikas Khandelwal. 2001. Temporal summaries of new topics. In Proc. of SIGIR.\\n\\nJames Allan, Ron Papka, and Victor Lavrenko. 1998. On-line new event detection and tracking. In Proc. of SIGIR.\"}"}
{"id": "HfKOIPCvsv", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "HfKOIPCvsv", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "HfKOIPCvsv", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, and Minjoon Seo. 2022a. TemporalWiki: A lifelong benchmark for training and evaluating ever-evolving language models.\\n\\nJoel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, and Minjoon Seo. 2022b. Towards continual knowledge learning of language models. In Proc. of ICLR.\\n\\nZhen Jia, Abdalghani Abujabal, Rishiraj Saha Roy, Jannik Str\u00f6tgen, and Gerhard Weikum. 2018. TempQuestions: A benchmark for temporal question answering. In Companion of the WWW.\\n\\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proc. of ACL.\\n\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proc. of EMNLP.\\n\\nJungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Lavinia Dunagan, Jacob Morrison, Alexander R. Fabbri, Yejin Choi, and Noah A. Smith. 2022. Bidimensional leaderboards: Generate and evaluate language hand in hand. In Proc. of NAACL.\\n\\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. 2021. Dynabench: Rethinking benchmarking in NLP. In Proc. of NAACL.\\n\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. TACL.\\n\\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale ReAding comprehension dataset from examinations. In Proc. of EMNLP.\\n\\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022. Internet-augmented language models through few-shot prompting for open-domain question answering. Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d'Autume, Tom\u00e1s Kocisk\u00fd, Sebastian Ruder, Dani Yogatama, Kris Cao, Susannah Young, and Phil Blunsom. 2021. Mind the gap: Assessing temporal generalization in neural language models. In Proc. of NeurIPS.\\n\\nJinhyuk Lee, Sean S. Yi, Minbyul Jeong, Mujeen Sung, WonJin Yoon, Yonghwa Choi, Miyoung Ko, and Jaewoo Kang. 2020. Answering questions on COVID-19 in real-time. In Proc. of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020.\\n\\nKyungjae Lee, Wookje Han, Seung-won Hwang, Hwaran Lee, Joonsuk Park, and Sang-Woo Lee. 2022. Plug-and-play adaptation for continuously-updated QA. In Findings of the ACL: ACL 2022.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proc. of ACL.\\n\\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020b. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Proc. of NeurIPS.\\n\\nZhiwei Li, Bin Wang, Mingjing Li, and Wei-Ying Ma. 2005. A probabilistic model for retrospective news event detection. In Proc. of SIGIR.\\n\\nJimmy Lin, Salman Mohammed, Royal Sequiera, Luchen Tan, Nimesh Ghelani, Mustafa Abualsaud, Richard McCreadie, Dmitrijs Milajevs, and Ellen M. Voorhees. 2017. Overview of the TREC 2017 real-time summarization track. In Proc. of TREC.\"}"}
{"id": "HfKOIPCvsv", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jimmy Lin, Adam Roegiest, Luchen Tan, Richard McCreadie, Ellen M. Voorhees, and Fernando Diaz. 2016. Overview of the TREC 2016 real-time summarization track. In Proc. of TREC.\\n\\nAdam Li\u0161ka, Tom\u00e1\u0161 Ko\u010disk\u00fd, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal, Cyprien de Masson d'Autume, Tim Scholtes, Manzil Zaheer, Susannah Young, Ellen Gilsenan-McMahon Sophia Austin, Phil Blunsom, and Angeliki Lazaridou. 2022. StreamingQA: A benchmark for adaptation to new knowledge over time in question answering models.\\n\\nShayne Longpre, Yi Lu, and Joachim Daiber. 2021a. MKQA: A linguistically diverse benchmark for multilingual open domain question answering. TACL.\\n\\nShayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021b. Entity-based knowledge conflicts in question answering. In Proc. of EMNLP.\\n\\nKelvin Luu, Daniel Khashabi, Suchin Gururangan, Karishma Mandyam, and Noah A. Smith. 2022. Time waits for no one! analysis and challenges of temporal misalignment. In Proc. of NAACL.\\n\\nZhiyi Ma, Kawin Ethayarajh, Tristan Thrush, Somya Jain, Ledell Wu, Robin Jia, Christopher Potts, Adina Williams, and Douwe Kiela. 2021. Dynaboard: An evaluation-as-a-service platform for holistic next-generation benchmarking. In Proc. of NeurIPS.\\n\\nSebastian Martschat and Katja Markert. 2017. Improving ROUGE for timeline summarization. In Proc. of EACL.\\n\\nSebastian Martschat and Katja Markert. 2018. A temporally sensitive submodularity framework for timeline summarization. In Proc. of CoNLL.\\n\\nRichard McCreadie, Cody Buntain, and Ian Soboroff. 2019. TREC incident streams: Finding actionable information on social media. In Proc. of ISCRAM.\\n\\nKathleen McKeown, Regina Barzilay, John Chen, David Elson, David Evans, Judith Klavans, Ani Nenkova, Barry Schiffman, and Sergey Sigelman. 2003. Columbia's newsblaster: New features and future directions. In Proc. of NAACL: Demonstrations.\\n\\nKathleen R. McKeown, Regina Barzilay, David Evans, Vasileios Hatzivassiloglou, Judith L. Klavans, Ani Nenkova, Carl Sable, Barry Schiffman, and Sergey Sigelman. 2002. Tracking and summarizing news on a daily basis with Columbia's Newsblaster. In Proc. of HLT.\\n\\nSewon Min, Danqi Chen, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019. Knowledge guided text retrieval and reading for open domain question answering.\\n\\nSwaroop Mishra and Anjana Arunkumar. 2021. How robust are model rankings: A leaderboard customization approach for equitable evaluation. In Proc. of AAAI.\\n\\nTimo M\u00f6ller, Anthony Reina, Raghavan Jayakumar, and Malte Pietsch. 2020. COVID-QA: A question answering dataset for COVID-19. In Proc. of the 1st Workshop on NLP for COVID-19 at ACL 2020.\\n\\nDat Tien Nguyen, Kamla Al-Mannai, Shafiq R. Joty, Hassan Sajjad, Muhammad Imran, and Prasenjit Mitra. 2016. Rapid classification of crisis-related data on social networks using convolutional neural networks. In Proc. of ICWSM.\\n\\nYasumasa Onoe, Michael J. Q. Zhang, Eunsol Choi, and Greg Durrett. 2022. Entity cloze by date: What LMs know about unseen entities. In Findings of the ACL: NAACL 2022.\\n\\nArian Pasquali, Ricardo Campos, Alexandre Ribeiro, Brenda Santana, Al\u00edpio Jorge, and Adam Jatowt. 2021. TLS-Covid19: A new annotated corpus for timeline summarization. In Advances in Information Retrieval.\\n\\nTatiana Passali, Alexios Gidiotis, Efstathios Chatzikyriakidis, and Grigorios Tsoumakas. 2021. Towards human-centered summarization: A case study on financial news. In Proc. of the First Workshop on Bridging Human\u2013Computer Interaction and Natural Language Processing.\\n\\nChristopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. 2021. DynaSent: A dynamic benchmark for sentiment analysis. In Proc. of ACL.\"}"}
{"id": "HfKOIPCvsv", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "HfKOIPCvsv", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "HfKOIPCvsv", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) If your work uses existing assets, did you cite the creators? [Yes]\\n\\n(b) Did you mention the license of the assets? [Yes]\\n\\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\\n\\n(d) Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? [N/A]\\n\\n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\\n\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\"}"}
