{"id": "aTXhTD44nF", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"USDC: A Dataset of User Stance and Dogmatism in Long Conversations\\n\\nAnonymous Author(s)\\n\\nSubmission Title: Capitalism vs. Socialism\\nSubmission Body: What are your thoughts on capitalism vs. socialism? I feel socialism is somewhat better. Which system do you think is better and why?\\n\\n\u2282\u22821: I believe capitalism drives innovation and economic growth. It rewards hard work and creativity.\\n\u2282\u22822: But capitalism also leads to income inequality and can neglect social welfare.\\n\u2282\u22823: True, but socialism can stifle innovation & lead to inefficiencies. There's a balance to be found.\\n\u2282\u22825: A mixed economy that combines elements of both systems might be the best approach. What do you all think?\\n\u2282\u22824: Agreed. Capitalism inherently nurtures innovation and efficiency. Socialism often suffers from bureaucratic red tape and lack of competition.\\n\u2282\u22826: I appreciate your perspective, but now I feel that capitalism, when allowed to operate with minimal interference, has consistently proven to drive innovation, efficiency, and economic growth.\\n\\nMis\\\\text{tral Large Strongly Against Somewhat In Favor Somewhat Against Strongly Against Stance Not Inferrable Strongly In Favor}\\n\\nGPT4 Firm But Open Open To Dialogue Flexible\\n\\nStance Dogmatism\\n\\nFigure 1: Sample Reddit conversation on \u201cCapitalism vs. Socialism\u201d with Stance (for every comment \\\\{c_i\\\\}_{i=1}^6) and Dogmatism (for every author \\\\{a_j\\\\}_{j=1}^3) labels from Mistral Large and GPT-4. The submission content favors to socialism and examines how the authors position their opinions regarding socialism vs. capitalism.\\n\\nAbstract\\n\\nIdentifying user's opinions and stances in long conversation threads on various topics can be extremely critical for enhanced personalization, market research, political campaigns, customer service, conflict resolution, targeted advertising and content moderation. Hence, training language models to automate this task is critical. However, to train such models, gathering manual annotations has multiple challenges: 1) It is time-consuming and costly; 2) Conversation threads could be very long, increasing chances of noisy annotations; and 3) Interpreting instances where a user changes their opinion within a conversation is difficult because often such transitions are subtle and not expressed explicitly. Inspired by the recent success of large language models (LLMs) for complex natural language processing (NLP) tasks, we leverage Mistral Large and GPT-4 to automate the human annotation process on the following two tasks while also providing reasoning: i) User Stance classification, which involves labeling a user's Stance of a post in a conversation on a five-point scale; ii) User Dogmatism classification, which deals with labeling a user's overall opinion in the conversation on a four-point scale. The majority voting on zero-shot, one-shot, and few-shot annotations from these two LLMs on 764 multi-user Reddit conversations helps us curate the USDC dataset. USDC is then used to finetune and instruction-tune multiple deployable small models.\\n\\nSubmitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.\"}"}
{"id": "aTXhTD44nF", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We make the code and dataset publicly available.\\n\\nIntroduction\\n\\nUnderstanding the user's (or author's) opinion in a conversation is a fundamental aspect of successful interpersonal interactions, and it is essential for developing better interpersonal communication skills, empathy development, and informed decision-making. This user understanding is particularly relevant in the context of dogmatism, a phenomenon observed in various areas such as politics, religion, culture, intellect, and science, where rigid adherence to beliefs often hinders open-mindedness and empathy (Rokeach, 1954). Advertisers can target their campaigns more effectively by aligning with the opinions and stances of potential customers. Companies can use this information for market research to tailor products and services to meet consumer needs and preferences. Political groups can gauge public reaction to policies and campaigns and adjust their strategies accordingly. Identifying differing opinions can help conflict resolution by understanding the perspectives of all parties' perspectives. Society can promote tolerance and maintain social harmony by recognizing and respecting diverse opinions.\\n\\nFig. 1 shows a sample Reddit conversation on the topic of Capitalism vs. Socialism. We refer to an author's initial post (containing title and body) as a submission. Multiple authors can then share their opinions as comments on the submission. Specifically this example contains 6 comments \\\\(c_i\\\\) from 3 authors \\\\(a_j\\\\). We also show stance and dogmatism predictions from two large language models (LLMs): Mistral Large and GPT-4. Some authors like \\\\(a_1\\\\) change their views during the discussion based on the beliefs or opinions of others. At the beginning of the dialogue, we note that author \\\\(a_1\\\\) is somewhat favoring socialism (in submission and \\\\(c_2\\\\)). But the author shifts their stance to somewhat favors capitalism (in \\\\(c_4\\\\)) after considering the viewpoints of author \\\\(a_2\\\\) in comments \\\\(c_1\\\\) and \\\\(c_3\\\\), illustrating author \\\\(a_1\\\\)'s firm yet open-minded approach. On the other hand, author \\\\(a_3\\\\) seems very flexible based on their comment \\\\(c_5\\\\). Understanding conversations requires understanding the fine-grained topics being discussed and the dynamic viewpoints of the individual users. Given the importance of understanding these user dynamics in conversations, training language models to perform this task automatically at scale is critical. While numerous datasets are available for analyzing individual user posts (Fast & Horvitz, 2016; Sakketou et al., 2022), typically through random subsampling of posts or selecting posts with a limited number of tokens, the exploration of a specific user's opinion across each post within an entire conversational thread remains under-explored. Crowdsourcing is one possible approach to address the need for a suitable dataset. However, a significant limitation in manually annotating datasets for user opinions is the time-consuming nature of the process, as annotators must read entire conversations to label each user's post, making data acquisition costly. Additionally, manual annotation often faces challenges related to quality, as accurately labeling opinions requires understanding demographic details and domain-specific knowledge. Given these limitations, achieving a comprehensive and accurate set of user opinions corresponding to posts about a topic often requires multiple annotators or iterative rounds of annotation. Since users could change their opinion (often times with subtle transitions and not with explicit statements) within a conversation, tracking such changes across multiple users manually becomes very cumbersome.\\n\\nRecently, large language models (LLMs), especially those built on Transformer architectures (Vaswani et al., 2017) and pretrained on large datasets, have resulted in state-of-the-art accuracies on several complex natural language processing (NLP) tasks (Brown et al., 2020; Chung et al., 2024). LLMs are also being frequently used for dialog response generation (Zhang et al., 2020; Bao et al., 2019; Roller et al., 2021; Adiwardana et al., 2020). Given the complex and cumbersome nature of conversation understanding, we hypothesize that LLMs can be effective in capturing nuances involved in understanding user opinions and their shifts in multi-user conversational contexts. Also, since these models possess long-range memory capabilities, we believe that they can reason over extended conversational threads involving numerous participants, as good as human annotators, if not better.\\n\\nIn this work, we leverage LLMs like Mistral Large and GPT-4 to perform two tasks: i) User Stance classification, which involves labeling a user's stance of a post in a conversation on a five-point...\"}"}
{"id": "aTXhTD44nF", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Generating annotations using LLMs: We pass the entire conversation for each Reddit thread in JSON format. The JSON highlights the top two authors who posted the most comments, alongside annotation guidelines for stance and dogmatism labels in the system prompt.\\n\\nWe experiment with these two models as human-like annotators to generate user opinions in full-length, multi-user Reddit conversations in a zero-shot, one-shot as well as few-shot setup. Thus, overall for every sample, we obtain six annotations ($\\\\{\\\\text{Mistral Large, GPT-4}\\\\} \\\\times \\\\{\\\\text{zero-shot, one-shot, few-shot}\\\\}$). Fig. 2 presents our LLM-based annotation pipeline for user-level Stance and Dogmatism tasks. We consider majority voting over these six as our final annotations. Overall, this helps us curate our USDC (a dataset of user stance and dogmatism in conversations) dataset, which consists of 764 multi-user conversations from 22 subreddits, including 1,528 user-level dogmatism samples and 9,618 stance samples across all posts from selected users.\\n\\nOverall, the annotations in the dataset highlight specific user opinions in each post related to stance, track opinion fluctuations leading to a dogmatic nature, and provide reasoning about why users hold specific opinions.\\n\\nUSDC addresses several weaknesses of existing post-level stance and dogmatism datasets. First, the full-length multi-user conversation aspect of USDC enables it to capture contextual and opinion shifts of multiple users. This feature allows it to serve as both an instruction-tuning user opinion dataset and an evaluation benchmark. We believe that the ability to perform instruction tuning for user opinions at a large scale can bridge the gap between open-source and commercial user trait understanding models. Additionally, the in-context learning annotations using state-of-the-art LLMs in USDC make it a more comprehensive measure of how current LLMs understand complex tasks like capturing opinions. This aspect makes it a valuable resource, especially for social media agents seeking deeper insights into user behavior.\\n\\nIn this work, we utilize our USDC dataset to finetune as well as instruction-tune open-source LLMs for generating stance and dogmatism labels for users. We experiment with three pretrained small language models (SLMs) like LLaMA-2-7B, LLaMA-3-8B, and Falcon-7B. We also experiment with four instruction-tuned SLMs like LLaMA-2-chat-7B, LLaMA-3-8B-instruct, Vicuna-7B-v.1.5, and Falcon-7B-instruct. We report weighted F1 scores obtained using these models for both the tasks.\\n\\nWe make the following contributions: 1) We contribute USDC (a dataset of user stance and dogmatism in conversations) dataset consisting of 764 multi-user conversations labeled with 1,528 user-level dogmatism samples and 9,618 stance samples. 2) We report initial results for the stance and dogmatism detection tasks using seven small language models for the USDC dataset. We find that stance detection performance improves with instruction-tuning (F1-score of 56.2) compared to fine-tuning (F1-score of 54.9). However, dogmatism detection performs worse with instruction-tuning (F1-score of 49.2) compared to fine-tuning (F1-score of 51.4), highlighting the complexity of this task. 3) We make the code and dataset publicly available. Also, the finetuned and instruction-tuned models are made available as well.\"}"}
{"id": "aTXhTD44nF", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Related Work\\n\\nOpinion fluctuations in user conversations. Our work is closely related to previous studies (Fast & Horvitz, 2016; Sakketou et al., 2022), which explore Stance and Dogmatism at the post level, where posts are randomly sampled from conversation threads. Fast & Horvitz (2016) predicted user dogmatism on randomly sampled Reddit posts from conversations, with each post limited to 200-300 characters. One major limitation of this work is the unavailability of a public dataset and missing annotator demographic details. Sakketou et al. (2022) created the post-level Stance dataset, SPINOS, where each post is considered independent, and submission posts are missing while annotators label the data. Additionally, the quality of the dataset is not validated due to missing demographic details of these annotators. Our work overcomes the limitations of previous studies and presents Stance detection for posts and Dogmatism labels of users in conversations, considering the entire context, while preserving submission IDs. Hence, our dataset provides clear user-level posts and dogmatism data, which are useful for modeling dynamic user representations.\\n\\nGenerating annotations for NLP tasks using Large Language Models\\n\\nOur work also relates to a growing body of literature suggesting that large language models can perform similarly to human annotators in labeling complex NLP tasks (Zhou et al., 2022; Zhang et al., 2023; Bansal & Sharma, 2023; Lowmanstone et al., 2023; Wadhwa et al., 2023; Honovich et al., 2023; Zheng et al., 2024; Ye et al., 2022a; Meng et al., 2022). Several studies have explored LLM-based annotation generation in zero-shot or few-shot task settings (Ye et al., 2022a; Meng et al., 2022; Ye et al., 2022b), while others have compared pairs of language models to assess the quality of annotations generated by these LLMs (Zheng et al., 2024). However, these studies focused on generating annotations for NLP tasks such as sentiment analysis, natural language inference (Gilardi et al., 2023; Alizadeh et al., 2023), or creating synthetic dialogues, but only for dyadic conversations (Lee et al., 2023). Our approach complements these previous studies by focusing on generating annotations of user opinions in complex multi-user conversations.\\n\\n3 USDC Dataset Curation\\n\\nIn this section, we will discuss three main things: 1) Collection of Reddit conversations, 2) Obtaining LLM annotations, and 3) Inter-annotator agreement with LLMs as annotators.\\n\\n3.1 Collection of Reddit Conversation Threads\\n\\nInitial crawl. We crawl an year (2022) worth of multi-user conversation data from 22 subreddits of Reddit using praw API. This dataset includes submissions and all associated user comments. Each submission, which serves as the initial message of the conversation, contains a title and content body. This is followed by comments and replies to the submission or other comments. Overall, we crawled 3,619 Reddit conversations across the 22 subreddits. A sample Reddit conversation is displayed in Fig. 1.\\n\\nQuality filtering of conversations. Since submission content on Reddit can sometimes include videos, we perform the following filtering steps. 1) We only consider submissions where the content is text. 2) We remove conversations with [deleted] tags and empty content. 3) We exclude conversations where the posts were either discarded by users or removed by moderators. Reddit user conversations can be very long and we observed up to 591 comments in a single crawled conversation data. Considering the maximum sequence length allowed by various language models, we retained only those conversations that contain at least 20 and at most 70 comments. Considering conversations with fewer than 20 comments results in too few comments to accurately gauge user opinions based on small samples. Further, we ensure that at least two users covering \u223c50% of the comments in the conversations. We did not remove any comments or reduce the post length in the selected conversations. Out of the initial 3,619 conversations, these filtering steps result into 764 conversations getting selected. Table. 4 in the Appendix shows detailed subreddit level statistics.\"}"}
{"id": "aTXhTD44nF", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 Obtaining LLM Annotations\\n\\nRepresenting Reddit conversations in JSON format. To create the prompt, we follow the nested hierarchical structure of Reddit conversations to maintain the context. Specifically, we maintain a JSON structure for each conversation, where each author has their post IDs, and comments or replies are available in the body section. An example of a Reddit conversation in JSON format is provided in Appendix D. Note that the JSON explicitly includes the top-2 authors who posted the most comments in the conversation, as well as their respective post IDs. Our emphasis on these top-2 users (covering 47% posts of total posts on average) aimed at accurately assigning Stance and Dogmatism labels, acknowledging the challenge of modeling a user\u2019s opinion belief based on a very number of posts within a conversation.\\n\\nUsing LLMs as human-like annotators. To annotate the position (or Stance) of a user towards a subreddit topic at each post and the overall opinion (or Dogmatism level) of a user in a conversation, we employ two well-known commercialized API-based LLMs: GPT-4 (OpenAI, 2023) and Mistral Large (Jiang et al., 2024). OpenAI GPT-4 is a decoder-based language model which features a context window of 32k to 128k tokens. Mistral Large features a context window of 32k tokens. Additionally, we also examined other versions of these models, such as GPT-3.5 and Mistral-small and medium, but found that these models failed to produce annotations in the desired format. We briefly discuss these limitations in Section 6.\\n\\nFor both GPT-4 and Mistral Large, we supplied a system prompt that contains the definition of Stance and Dogmatism, guidelines for annotating each user conversation, and the necessary labels for Stance and Dogmatism, as shown in Fig 2. The system prompt is detailed in the Appendix B. Along with the system prompt, we provided a user prompt comprising the entire user conversation in a structured JSON format, as discussed above. Additionally, we prompted the model to generate reasoning for each label, explaining why the LLMs assigned a particular label to a specific user post. We used zero-shot, one-shot, and few-shot settings to get the LLM-based annotations. For the few-shot setting, we added two examples in the prompt. Samples of generated outputs using GPT-4 in zero-shot, one-shot, and few-shot settings are shown in Appendix E.1, E.2, E.3 respectively. Similarly, samples of generated outputs using Mistral Large in zero-shot, one-shot, and few-shot settings are shown in Appendix E.4, E.5, E.6 respectively.\\n\\nAnnotation tasks. We prompt the LLMs to perform two annotation tasks: 1) Stance detection, which determines if a user comment or post is Strongly In Favor, Strongly Against, Stance Not Inferrable, Somewhat In Favor, or Somewhat Against towards specific subreddit submission content; 2) Dogmatism identification, which evaluates the user\u2019s overall opinion in conversation and categorizes them into one of four categories: Firm but Open, Open to Dialogue, Flexible or Deeply Rooted. This assessment reveals whether a user is open to changing their beliefs or remains steadfast in their opinions based on interactions with other users.\\n\\nAddressing LLM response and JSON parsing failures. Sometimes the LLMs got confused with the author IDs and missed Stance labels for some author IDs (Fig. 3 (left)). Sometimes, there were minor errors in key naming (\u2018label\u2019 vs \u2018body\u2019 in Fig. 3 (right)). For each LLM setting, we observed\"}"}
{"id": "aTXhTD44nF", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"such errors in around 15 cases on average. We manually fixed such JSON parse errors and missing Stance labels for some author IDs.\\n\\nMajority voting. After obtaining six annotations (\\\\{Mistral Large, GPT-4\\\\} \u00d7 \\\\{zero-shot, one-shot, few-shot\\\\}) for each sample, we aggregate using majority voting to determine the final gold annotations for the Stance and Dogmatism tasks. Fig. 4 presents the class distributions for both the annotation tasks. Additionally, we present the class distributions obtained from each model with the three settings (zero-shot, one-shot and few-shot) for two tasks in Appendix Figs. 5 and 6 respectively.\\n\\n3.3 Inter-annotator Agreement with LLMs as Annotators\\n\\nAs the quality of labeling on subjective tasks is challenging, we validated the inter-annotator agreement (IAA) between the six LLMs (GPT-4 Zero-shot, GPT-4 One-shot, GPT-4 Few-shot, Mistral Large Zero-shot, Mistral Large One-shot, and Mistral Large Few-shot) for the Stance as well as Dogmatism tasks. We perform IAA using two approaches: i) Cohen's kappa score (Cohen, 1960) and ii) Fleiss' kappa score (Fleiss, 1971). Cohen's kappa measures the agreement between two raters, while Fleiss' kappa extends this to multiple raters. Hence, we employed Cohen's kappa for pairwise comparisons and Fleiss' kappa for overall agreement across all models.\\n\\nFig. 7 in Appendix shows the pairwise Cohen's kappa values for both Stance and Dogmatism tasks. We observe that Cohen's kappa values range from 0.36 to 0.72 for Stance and 0.31 to 0.61 for dogmatism, indicating moderate agreement between the models. Broadly kappa values are higher for model pairs within a family (GPT-4 or Mistral large). Thus, the large variance in the kappa scores is not due to the various in-context learning settings (ZS, OS, FS) but rather due to architectural differences.\\n\\nThe overall Fleiss' kappa value was calculated as 0.485 for Stance and 0.435 for Dogmatism, suggesting moderate agreement among all six models. Comparing LLM IAA with previous studies, we observe that for dogmatism, the LLM IAA of 0.435 matches with 0.44 as mentioned in Fast & Horvitz (2016). Similarly, for Stance, the LLM IAA of 0.485 is much higher than 0.34 as reported in Sakketou et al. (2022). It is important to note that previous studies on Stance and Dogmatism datasets were created on post-level data with limited token lengths, whereas our work focuses on entire user conversations. This suggests that LLMs can be considered as competent annotators for complex subjective tasks. However, the moderate agreement levels indicate potential areas for improvement and align with the observed performance variations among the models.\\n\\n4 Training Small Language Models\\n\\nIn this section, we briefly discuss the small language models that we experiment with. We also discuss their finetuning and instruction tuning details.\"}"}
{"id": "aTXhTD44nF", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.1 Small Language Models\\n\\nWe train three pretrained small language models (LLaMA-2-7B, LLaMA-3-8B, Falcon-7B) and four instruction-tuned small language models (LLaMA-2-chat-7B, LLaMA-3-8B-instruct, Vicuna-7B-v.1.5, and Falcon-7B-instruct). We finetune as well as instruction tune these models using the proposed USDC dataset. We use pretrained models checkpoints from Hugging Face. All of these LLMs have context length of 4096 tokens.\\n\\n**LLaMA** models (Touvron et al., 2023a) are decoder-only LLMs trained on 1.6 trillion tokens from a mixture of corpora including C4, English CommonCrawl, Wikipedia, Github, and more. We use two versions of models in our study: LLaMa-2-7B (Touvron et al., 2023b) and LLaMa-3-8B and their instruction tuned variants.\\n\\n**Falcon** models (Almazrouei et al., 2023) are decoder-only LLMs trained on \u22651 trillion tokens of text, with a particular emphasis on the RefinedWeb corpus. For Falcon, we use both the pretrained and instruction tuned 7B parameter variants in our study.\\n\\n**Vicuna** model (Chiang et al., 2023) is finetuned from the LLaMA 7B model on approximately 70K user-shared conversations gathered from ShareGPT.com and we used the 7B parameter variants.\\n\\n### 4.2 Experimental Setup\\n\\n**Train-test setup.** We conducted both finetuning and instruction-tuning of small language models. For this purpose, we divided the dataset of 764 conversations into train (\u223c75%) and test splits (\u223c25%).\\n\\nThe training dataset comprised 564 conversations, including 1128 samples of Dogmatism labels and 7520 samples of Stance labels. Conversely, the testing dataset consisted of 200 conversations, with 400 samples of Dogmatism labels and 1831 samples of Stance labels across two authors posts.\\n\\n**Implementation details for reproducibility.** All experiments were conducted on a machine equipped with an NVIDIA A100 GPU with 80 GB of GPU RAM, partitioned into two devices of 40 GB each. We employed 4-bit quantization with normalized floating precision (nf4) from the bitsandbytes library. Additionally, we utilized LoRA (Hu et al., 2021) with a rank of 64 and an alpha value of 16 during task-based instruction tuning. Finally, we use PEFT (Parameter Efficient Finetuning) library to train large language models with SFTT (Supervised Finetuning Trainer) setting. To further enhance performance, we divided the training dataset into a validation set, comprising a randomly chosen 10% subset from the training set, used exclusively for hyperparameter tuning. More details about bitsandbytes, PEFT and SFTT parameters are reported in Appendix.\\n\\n### 4.3 Finetuning and Instruction Tuning of Small Language Models (SLMs)\\n\\n**Finetuning of SLMs.** For Stance classification, we treat each user post as an independent sample. In contrast, for Dogmatism classification, we consider the entire user conversation as a single sample by concatenating all the threads from a user in that conversation. To load the pretrained SLMs, we perform 4-bit quantization, apply the LoRA technique (Hu et al., 2021), and fine-tune the models with SFTT before saving the fine-tuned model. For finetuning, we used prompt for Stance classification as shown in Fig. 8 (see Appendix). Similarly, Fig. 9 (see Appendix) displays prompt for Dogmatism identification.\\n\\n**Instruction tuning of SLMs.** We instruction tune the SLMs on user conversations along with their gold labels from the training part of the USDC dataset. For instruction tuning, we use the same prompt as used for LLMs to generate the USDC dataset (also shown in Appendix B). Similar to finetuning, we use same train-test splits for instruction tuning.\\n\\n### 5 Results\\n\\nDo SLMs finetuned with task-specific LLM annotations accurately perform Stance and Dogmatism tasks on user opinions?\\n\\nWe show the weighted F1 of various SLMs finetuned with task-specific LLM annotations on the stance and dogmatism detection tasks on the USDC test set in Table 1. We\\n\\n4 https://pypi.org/project/bitsandbytes/\\n\\n5 https://github.com/huggingface/peft\"}"}
{"id": "aTXhTD44nF", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Finetuning: weighted F1 score for Stance classification using SLMs on USDC test set. ZS: Zero-shot, OS: One-shot, FS: Few-shot.\\n\\n| Model                  | ZS   | OS   | FS   | ZS   | OS   | FS   | ZS   | OS   | FS   | ZS   | OS   | FS   | ZS   | OS   | FS   |\\n|------------------------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|\\n| LLaMA-2-7B             | 51.8 | 52.9 | 52.7 | 35.1 | 49.2 | 46.0 | 54.0 | 42.1 | 44.2 | 45.2 | 39.3 | 47.6 | 43.7 | 43.4 |\\n| LLaMA-2-chat-7B        | 52.8 | 51.4 | 51.8 | 34.7 | 47.5 | 46.5 | 51.3 | 42.1 | 42.5 | 48.8 | 41.1 | 49.7 | 45.5 | 48.3 |\\n| LLaMA-3-8B             | 51.3 | 52.2 | 52.9 | 34.9 | 48.5 | 47.0 | 54.9 | 42.0 | 47.8 | 45.3 | 39.9 | 47.4 | 36.3 | 51.4 |\\n| LLaMA-3-8B-instruct    | 51.2 | 52.6 | 52.7 | 33.9 | 49.5 | 45.6 | 54.5 | 44.8 | 46.2 | 49.7 | 46.1 | 45.8 | 46.1 | 50.8 |\\n| Falcon-7B              | 50.7 | 51.1 | 51.6 | 34.9 | 47.2 | 43.9 | 53.2 | 41.5 | 42.1 | 43.3 | 36.5 | 38.4 | 37.5 | 40.1 |\\n| Falcon-7B-instruct     | 51.2 | 51.5 | 51.6 | 35.1 | 47.7 | 44.2 | 51.0 | 41.7 | 42.1 | 42.9 | 36.8 | 38.5 | 36.9 | 39.7 |\\n| Vicuna-7B-v.1.5        | 51.0 | 53.0 | 53.2 | 35.1 | 48.5 | 45.8 | 54.7 | 42.6 | 48.3 | 40.8 | 45.9 | 42.6 | 46.2 | 42.3 |\\n\\nTable 2: Instruction-tuning: weighted F1 score for Stance classification using SLMs on USDC test set. ZS: Zero-shot, OS: One-shot, FS: Few-shot.\\n\\n| Model                  | ZS   | OS   | FS   | ZS   | OS   | FS   | ZS   | OS   | FS   | ZS   | OS   | FS   | ZS   | OS   | FS   |\\n|------------------------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|\\n| LLaMA-2-7B             | 53.2 | 54.0 | 54.5 | 36.8 | 50.3 | 47.2 | 55.5 | 43.0 | 45.0 | 46.3 | 40.6 | 48.2 | 45.0 | 44.0 |\\n| LLaMA-2-chat-7B        | 54.0 | 54.5 | 55.0 | 36.5 | 50.7 | 47.6 | 54.0 | 43.2 | 45.5 | 47.0 | 40.8 | 48.5 | 45.5 | 43.8 |\\n| LLaMA-3-8B             | 53.5 | 54.8 | 55.5 | 37.0 | 50.5 | 48.0 | 56.2 | 43.5 | 46.0 | 47.5 | 41.0 | 48.8 | 45.8 | 45.1 |\\n| LLaMA-3-8B-instruct    | 53.0 | 54.2 | 55.0 | 36.0 | 50.0 | 47.0 | 55.5 | 43.8 | 46.5 | 47.8 | 41.5 | 49.2 | 46.0 | 44.8 |\\n| Falcon-7B              | 52.8 | 53.4 | 54.0 | 36.5 | 49.5 | 46.5 | 54.8 | 42.5 | 44.6 | 45.8 | 39.8 | 47.0 | 44.0 | 43.8 |\\n| Falcon-7B-instruct     | 53.0 | 53.8 | 54.2 | 36.8 | 49.8 | 46.8 | 54.5 | 42.8 | 44.8 | 46.0 | 40.0 | 47.2 | 44.2 | 43.0 |\\n| Vicuna-7B-v.1.5        | 53.3 | 54.5 | 55.2 | 37.0 | 50.2 | 47.8 | 55.2 | 43.7 | 46.8 | 47.2 | 41.2 | 48.2 | 46.5 | 44.8 |\\n\\nWe make the following observations from these results:\\n1) For both tasks, the majority voting labels as ground truth, has a relatively high performance, scoring above 50% weighted F1-score across several models.\\n2) LLaMa-3 models (LLaMA-3-8B and LLaMA-3-8B-instruct) perform better across both the tasks.\\n3) For GPT-4 annotations, in most cases, SLMs finetuned with few-shot annotations outperform those trained with zero and one-shot annotations. For Mistral Large annotations, typically SLMs finetuned with one-shot annotations performs the best.\\n4) Specifically, for Stance detection task, Vicuna-7B-v.1.5 finetuned using few-shot annotations is the best model trained with GPT-4 annotations. Similarly, LLaMA-3-8B-instruct finetuned with one-shot annotations is the best model trained with Mistral Large annotations.\\n5) For the Dogmatism detection task, LLaMA-3-8B-instruct finetuned using few-shot annotations is the best model trained with GPT-4 annotations. Similarly, LLaMA-2-chat-7B finetuned with one-shot annotations is the best model trained with Mistral Large annotations.\\n6) Overall, we observe that instruction tuned SLMs perform better than the pretrained SLMs.\\n\\nDo SLMs instruction-tuned with task-specific LLM annotations perform better than SLMs finetuned with task-specific LLM annotations for the Stance and Dogmatism tasks? We show the weighted F1 of various SLMs instruction-tuned with task-specific LLM annotations, on the stance and dogmatism detection tasks on the USDC test set in Table 2. We report AUC scores and other qualitative analysis in Appendix F (see Fig. 13). We make the following observations from these results:\\n1) SLMs with instruction-tuning result in higher weighted F1-scores than SLMs with finetuning for stance detection, while SLMs with finetuning outperform SLMs with instruction-tuning in dogmatism detection.\\n2) Contrary to results in Table 1, Table 2 demonstrates that using majority voting labels as ground truth, SLM instruction-tuning yields relatively high performance only for the stance detection task, but not for the dogmatism detection.\\n3) Similar to results in Table 1, LLaMA-3 models (LLaMA-3-8B and LLaMA-3-8B-instruct) perform better across both tasks. Additionally, GPT-4 annotations yield the best results in the few-shot setting, while Mistral Large annotations perform best in the one-shot setting.\\n\\nOverall, we draw the following conclusions when comparing SLM finetuning and instruction-tuning:\\n1) Since dogmatism detection is inherently a more complex and varied task than stance detection, the model might struggle to generalize from the instructional data.\\n2) The system prompt used in finetuning is much simpler than the original system prompt for instruction-tuning, making it challenging to handle the context length for longer conversations.\\n\\nWe perform an error analysis to further analyze the results in the next subsection.\\n\\nError Analysis\\nTable 3 illustrates the confusion matrix for stance detection for LLaMa-3-8B finetuning and instruction-tuning. We make the following observations this table:\\n1) For both finetuning and instruction-tuning, there is a significant misclassification between \\\"Somewhat Against\\\" and \\\"Somewhat In Favor,\\\" as well as between \\\"Somewhat In Favor\\\" and \\\"Stance Not Inferrable.\\\" These overlaps suggest challenges in distinguishing moderate stances, indicating a need for enhanced...\"}"}
{"id": "aTXhTD44nF", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Confusion matrix for LLaMa-3-8B Stance detection models on USDC test set: finetuning (left) and instruction-tuning (right). SOA: Somewhat Against, SOIF: Somewhat In Favor, SNI: Stance Not Inferrable, SGA: Strongly Against, SIF: Strongly In Favor.\\n\\nVerification using Human Interaction.\\n\\nDue to the time-consuming nature of the manual annotation process, we perform human annotations on the set of 200 test conversations. In the forms for human annotations, we displayed the top 2 author's Reddit posts from the conversation, along with the submission title and content. We also provided a link to the original Reddit URL for annotators to look at the full conversation. We provided detailed annotation guidelines (similar to the ones mentioned in the prompt in Appendix B) to instruct human annotators in carrying out these tasks. Here is a sample Google form.\\n\\nWith three human annotators on a sample of 10 conversations, the agreement of majority labels (i.e., USDC test set labels) with human labels is 0.56 for the stance detection task and 0.45 for the dogmatism task. The annotators included two males and one female, affiliated with both academia and industry, aged between 20 and 40, and were very familiar with Reddit topics.\"}"}
{"id": "aTXhTD44nF", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\nDaniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977, 2020.\\n\\nMeysam Alizadeh, Ma\u00ebl Kubli, Zeynab Samei, Shirin Dehghani, Juan Diego Bermeo, Maria Kobroberynikova, and Fabrizio Gilardi. Open-source large language models outperform crowd workers and approach chatgpt in text-annotation tasks. arXiv preprint arXiv:2307.02179, 2023.\\n\\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocarucu, M\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The falcon series of open language models. arXiv preprint arXiv:2311.16867, 2023.\\n\\nParikshit Bansal and Amit Sharma. Large language models as annotators: Enhancing generalization of nlp models at minimal cost. arXiv preprint arXiv:2306.15766, 2023.\\n\\nSiqi Bao, Huang He, Fan Wang, Hua Wu, and Haifeng Wang. Plato: Pre-trained dialogue generation model with discrete latent variable. arXiv preprint arXiv:1910.07931, 2019.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.\\n\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1\u201353, 2024.\\n\\nJacob Cohen. A coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1):37\u201346, 1960.\\n\\nEthan Fast and Eric Horvitz. Identifying dogmatism in social media: Signals and models. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 690\u2013699, 2016.\\n\\nJoseph L Fleiss. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378, 1971.\\n\\nFabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. Chatgpt outperforms crowd workers for text-annotation tasks. Proceedings of the National Academy of Sciences, 120(30):e2305016120, 2023.\\n\\nOr Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14409\u201314428, 2023.\\n\\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021.\\n\\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.\\n\\nDong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen White, and Sujay Jauhar. Making large language models better data creators. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 15349\u201315360, 2023.\"}"}
{"id": "aTXhTD44nF", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"London Lowmanstone, Ruyuan Wan, Risako Owan, Jaehyung Kim, and Dongyeop Kang. Annota-\\nalization imputation to individualize predictions: Initial studies on distribution dynamics and model\\npredictions. arXiv preprint arXiv:2305.15070, 2023.\\n\\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. Generating training data with language models:\\nTowards zero-shot language understanding. Advances in Neural Information Processing Systems,\\n35:462\u2013477, 2022.\\n\\nR OpenAI. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2(5), 2023.\\n\\nMilton Rokeach. The nature and meaning of dogmatism. Psychological Review, 61(3), 1954.\\n\\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle\\nOtt, Eric Michael Smith, Y-Lan Boureau, and Jason Weston. Recipes for building an open-domain\\nchatbot. In Proceedings of the 16th Conference of the European Chapter of the Association for Com-\\nputational Linguistics: Main Volume, pp. 300\u2013325, Online, 2021. Association for Computational\\nLinguistics.\\n\\nFlora Sakketou, Allison Lahnala, Liane Vogel, and Lucie Flek. Investigating user radicaliza-\\ntion: A novel dataset for identifying fine-grained temporal shifts in opinion. arXiv preprint\\narXiv:2204.10190, 2022.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\\nsystems, 30, 2017.\\n\\nManya Wadhwa, Jifan Chen, Junyi Jessy Li, and Greg Durrett. Using natural language explanations\\nto rescale human judgments. arXiv preprint arXiv:2305.14770, 2023.\\n\\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, and Lingpeng\\nKong. Zerogen: Efficient zero-shot learning via dataset generation. In Proceedings of the 2022\\nConference on Empirical Methods in Natural Language Processing, pp. 11653\u201311669, 2022a.\\n\\nJiacheng Ye, Jiahui Gao, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. Progen:\\nProgressive zero-shot dataset generation via in-context feedback. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2022, pp. 3671\u20133683, 2022b.\\n\\nRuoyu Zhang, Yanzeng Li, Yongliang Ma, Ming Zhou, and Lei Zou. Llmaaa: Making large language\\nmodels as active annotators. arXiv preprint arXiv:2310.19596, 2023.\\n\\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng\\nGao, Jingjing Liu, and Bill Dolan. DIALOGPT : Large-scale generative pre-training for con-\\nversational response generation. In Proceedings of the 58th Annual Meeting of the Associa-\\ntion for Computational Linguistics: System Demonstrations, pp. 270\u2013278, Online, 2020. As-\\nsociation for Computational Linguistics. doi: 10.18653/v1/2020.acl-demos.30. URL\\nhttps://aclanthology.org/2020.acl-demos.30.\\n\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\\nchatbot arena. Advances in Neural Information Processing Systems, 36, 2024.\\n\\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,\\nand Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint\\narXiv:2211.01910, 2022.\"}"}
{"id": "aTXhTD44nF", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"NeurIPS Paper Checklist\\n\\n1. Claims\\n   Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\\n   Answer: [Yes]\\n   Justification: We have ensured that the main claims made in the abstract and introduction are directly correlating to the research findings and the methods we have employed.\\n   Guidelines:\\n   \u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.\\n   \u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.\\n   \u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.\\n   \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.\\n\\n2. Limitations\\n   Question: Does the paper discuss the limitations of the work performed by the authors?\\n   Answer: [Yes]\\n   Justification: The paper discusses the main limitations of the work performed by the authors in the discussion section.\\n   Guidelines:\\n   \u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\\n   \u2022 The authors are encouraged to create a separate \u201cLimitations\u201d section in their paper.\\n   \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\\n   \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\\n   \u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.\\n   \u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\\n   \u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.\\n   \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.\\n\\n3. Theory Assumptions and Proofs\\n   Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\\n\\n12\"}"}
{"id": "aTXhTD44nF", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"501 Guidelines:\\n\\n\u2022 The answer NA means that the paper does not include theoretical results.\\n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.\\n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.\\n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.\\n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.\\n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.\\n\\n4. Experimental Result Reproducibility\\n\\nQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\\n\\nAnswer: [Yes]\\n\\nJustification: The paper has delineated all the information related to the experimental setup in the experimental setup section.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not include experiments.\\n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.\\n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.\\n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.\\n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example\\n  (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.\\n  (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.\\n  (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).\\n  (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.\\n\\n5. Open access to data and code\"}"}
{"id": "aTXhTD44nF", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?\\n\\nAnswer: [Yes]\\n\\nJustification: We have released the code and dataset, making the dataset publicly available under a license.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that paper does not include experiments requiring code.\\n\\n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\\n\\n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).\\n\\n\u2022 The instructions should contain the exact command and environment needed to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\\n\\n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.\\n\\n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.\\n\\n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).\\n\\n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.\\n\\n6. Experimental Setting/Details\\n\\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?\\n\\nAnswer: [Yes]\\n\\nJustification: We provided all the training and testing details in the experimental setup.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not include experiments.\\n\\n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.\\n\\n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material.\\n\\n7. Experiment Statistical Significance\\n\\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?\\n\\nAnswer: [Yes]\\n\\nJustification: We conducted our experiments on all LLM-generated annotations across zero-shot, one-shot, and few-shot settings, using majority voting as labels. Our reported results represent the average performance across all test samples.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not include experiments.\\n\\n\u2022 The authors should answer \u201cYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.\\n\\n14\"}"}
{"id": "aTXhTD44nF", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).\\n\\nThe method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.).\\n\\nThe assumptions made should be given (e.g., Normally distributed errors).\\n\\nIt should be clear whether the error bar is the standard deviation or the standard error of the mean.\\n\\nIt is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.\\n\\nFor asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).\\n\\nIf error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.\\n\\n8. Experiments Compute Resources\\n\\nQuestion: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?\\n\\nAnswer: [Yes]\\n\\nJustification: We have included the specifications of the hardware and software environments to ensure the reproducibility of our results.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not include experiments.\\n\\n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.\\n\\n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.\\n\\n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).\\n\\n9. Code Of Ethics\\n\\nQuestion: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\\n\\nAnswer: [Yes]\\n\\nJustification: The research conducted in this paper fully conforms with the NeurIPS Code of Ethics in every respect.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\\n\\n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.\\n\\n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).\\n\\n10. Broader Impacts\\n\\nQuestion: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?\\n\\nAnswer: [Yes]\\n\\nJustification: The paper explores how advancements and applications of our findings could benefit society by capturing opinions of users in conversation benefit interpersonal skills. Specifically, we investigate the effectiveness of current state-of-the-art large language models in this context.\"}"}
{"id": "aTXhTD44nF", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Guidelines:\\n\\n\u2022 The answer NA means that there is no societal impact of the work performed.\\n\\n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.\\n\\n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.\\n\\n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.\\n\\n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.\\n\\n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).\\n\\n11. Safeguards\\n\\nQuestion: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?\\n\\nAnswer: [NA]\\n\\nJustification: Our research does not pose any risks for misuse.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper poses no such risks.\\n\\n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.\\n\\n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.\\n\\n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.\\n\\n12. Licenses for existing assets\\n\\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?\\n\\nAnswer: [Yes]\\n\\nJustification: We have explicitly cited the crawled websites, code and models used.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not use existing assets.\\n\\n\u2022 The authors should cite the original paper that produced the code package or dataset.\\n\\n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.\\n\\n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.\"}"}
{"id": "aTXhTD44nF", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.\\n\\n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.\\n\\n\u2022 For existing datasets that are repackaged, both the original license and the license of the derived asset (if it has changed) should be provided.\\n\\n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset's creators.\\n\\n13. New Assets\\n\\nQuestion: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?\\n\\nAnswer: [Yes]\\n\\nJustification: We open-source the code and the new USDC dataset, and we provide complete documentation on how the dataset was created.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not release new assets.\\n\\n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.\\n\\n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.\\n\\n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.\\n\\n14. Crowdsourcing and Research with Human Subjects\\n\\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?\\n\\nAnswer: [Yes]\\n\\nJustification: We provide full instructions on how we surveyed our LLM generated annotations using human participants in the Results section.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\\n\\n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.\\n\\n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.\\n\\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects\\n\\nQuestion: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?\\n\\nAnswer: [NA]\\n\\nJustification: We use publicly available Reddit user conversations to create the USDC dataset, and we do not collect any new data that would require IRB approval.\\n\\nGuidelines:\\n\\n17\"}"}
{"id": "aTXhTD44nF", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\\n\\n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.\\n\\n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.\\n\\n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.\"}"}
{"id": "aTXhTD44nF", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4 shows the detailed statistics of our USDC dataset at the sub-reddit level. Fig. 5 shows the distribution of stance labels across LLM annotations across zero-shot, one-shot and few-shot settings. Fig. 6 shows the distribution of dogmatism labels across LLM annotations across zero-shot, one-shot and few-shot settings.\\n\\nTable 4: Statistics of the User Conversation Dataset.\\n\\n| subreddit         | num_conversations | min_total_token_count | max_total_token_count |\\n|-------------------|-------------------|-----------------------|-----------------------|\\n| DebateCommunism   | 73                | 529                   | 11557                 |\\n| Abortiondebate    | 70                | 1271                  | 7401                  |\\n| CapitalismVSocialism | 61        | 665                   | 16927                 |\\n| prochoice         | 60                | 582                   | 7278                  |\\n| brexit            | 56                | 637                   | 4553                  |\\n| climateskeptics   | 56                | 734                   | 7550                  |\\n| prolife           | 54                | 672                   | 13342                 |\\n| gunpolitics       | 52                | 683                   | 7889                  |\\n| MensRights        | 52                | 623                   | 5774                  |\\n| climatechange     | 49                | 520                   | 7427                  |\\n| nuclear           | 41                | 572                   | 5282                  |\\n| progun            | 39                | 436                   | 3632                  |\\n| NuclearPower      | 23                | 629                   | 4589                  |\\n| Vegetarianism     | 22                | 627                   | 3958                  |\\n| AntiVegan         | 20                | 351                   | 5052                  |\\n| climate           | 13                | 701                   | 4678                  |\\n| Egalitarianism    | 10                | 665                   | 4060                  |\\n| VeganActivism     | 8                 | 460                   | 3685                  |\\n| Veganism          | 2                 | 1332                  | 1738                  |\\n| AnimalRights      | 1                 | 845                   | 845                   |\\n| animalwelfare     | 1                 | 1363                  | 1363                  |\\n| GunsAreCool       | 1                 | 2945                  | 2945                  |\\n\\nFigure 5: Distribution of Stance labels across LLM annotations.\"}"}
{"id": "aTXhTD44nF", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We used the following prompt for getting annotations from LLMs as well as for instruction-tuning of SLMs.\\n\\n### Introduction\\n\\n**Objective**: Analyze Reddit conversations to identify the stance of specific authors on sociopolitical topics and determine their level of dogmatism.\\n\\n**Stance Definition**: Stance is defined as the expression of the author\u2019s standpoint and judgement towards a given topic.\\n\\n**Dogmatism Definition**: Dogmatism is an opinion strongly believed as a fact to support a stance without a question or allowance for conversation.\\n\\n**Task**: Given a JSON formatted Reddit submission and its comment thread, classify the stance of text segments related to 'author1' and 'author2' by assigning one of the following five predefined stance labels: 'strongly_against', 'somewhat_against', 'somewhat_in_favor', 'strongly_in_favor', 'stance_not_inferrable'. Also, assign a dogmatism score.\"}"}
{"id": "aTXhTD44nF", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "aTXhTD44nF", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Instructions for Effective Annotation\\n\\n1. **Labeling Stance**: For each segment (including the original Reddit submission, comments, or replies) where \\\"author1\\\" or \\\"author2\\\" is mentioned, assign a stance label that best represents the stance expressed towards the discussed topic in the submission. This comprehensive approach ensures no relevant contribution by \\\"author1\\\" or \\\"author2\\\" is overlooked. Evaluate the stance based on the content's tone, argumentation, and engagement level with the topic.\\n\\n2. **Providing Justification**: For each label assigned, include a concise reason, aiming for less than 50 words. Focus on the stance and argumentative indicators present in the text.\\n\\n3. **Dogmatism Assessment**: After reviewing all segments from \\\"author1\\\" and \\\"author2\\\", assign a single dogmatism label reflecting the overall tone and approach in their contributions.\"}"}
{"id": "aTXhTD44nF", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Stance Classification\\n\\nAnalyze the stance of the post enclosed in square brackets.\\n\\nCategorize each post into one of the following categories based on its stance:\\n\\n\u2022 Somewhat In Favor\\n\u2022 Somewhat Against\\n\u2022 Stance Not inferrable\\n\u2022 Strongly In Favor\\n\u2022 Strongly Against\\n\\nand return the answer as one of the corresponding stance labels.\\n\\nUser Dogmatism Identification\\n\\nAnalyze the comments of a user in conversation enclosed in square brackets.\\n\\nCategorize the opinion fluctuation of the user into one of the following categories based on its change:\\n\\n\u2022 Open to Dialogue\\n\u2022 Firm but Open\\n\u2022 Deeply Rooted\\n\u2022 Flexible\\n\\nReturn the answer as one of the corresponding dogmatism labels.\\n\\nD Sample of User Input Prompt\\n\\n\\\"### User Prompt\\n\\nNow complete the given task for the respective authors i.e., author1 name is \\\"rookerin0\\\" and respective ids are [dhoxyz], [f3pghji], [f3tywb4], [f3uomn2]. author2 name is \\\"MikeWillTerminate\\\" and respective ids are [f3rt0bf], [f3rqu2u] for the data in json format{\\\"id\\\": \\\"dhoxyz\\\", \\\"author1\\\": \\\"rookerin0\\\", \\\"title\\\": \\\"This sub should encourage anti vs. pro-gun discussions instead of shutting them down instantly\\\", \\\"content\\\": \\\"Honestly, I followed this sub specifically to take part in these discussions, but everytime I see a comment that even remotely suggests anti-gun ideals or a discussion on the subject just gets ignored and downvoted to hell. Kind of expecting this to go the same way (my karma anus is ready, downvotes), but I have to hope for healthy discussions on the subject.\\\" , \\\"comments\\\": [\\\"id\\\": \\\"f3p9n2c\\\", \\\"body\\\": \\\"I think the problem now is the two sides are at an impasse. Everytime there is a \\\"compromise\\\" pro-gun loses\\\"]}\\\"\"}"}
{"id": "aTXhTD44nF", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"something. Nowadays pro-gun is interpreting the Constitution more literally, which leaves even the most mild policies of anti-gun as infringements. To further compound this anti-gun is only considering the most extreme measures. \\\"Assault weapons\\\" bans, mandatory buybacks, red flag laws, etc. I think at this point there is just nothing left to talk about. The middle ground is gone.\\\"\\n\\n\\\"replies\\\":\\n\\n{ \\\"id\\\": \\\"f3pati9\\\", \\\"replies\\\": [ { \\\"id\\\": \\\"f3pdu44\\\", \\\"body\\\": \\\"You are exactly right. I'm done with the idea that there can be real compromise. We should have at least gotten national reciprocity and shall-issue in every state in exchange for what we've given up. Now you have to be a goddamn lawyer to exercise your rights without violating the law.\\\"\\n\\n}, { \\\"author\\\": \\\"MikeWillTerminate\\\", \\\"id\\\": \\\"f3rt0bf\\\", \\\"body\\\": \\\"I am prepared for UBCs, if they do this: 1. Lower the age to buy handguns to 18, nationwide. 2. Repeal the Hughes Amendment. 3. A FOPA-like ban on assault weapon bans (what the FOPA did with a registry) 4. The punishment for violation is a monetary fine only 5. A repeal of the GCA ban on foreign NFA weapons 6. A repeal of the National Minimum Drinking Age Act of 1984\\\"\\n\\n}, { \\\"id\\\": \\\"f3pd55z\\\", \\\"body\\\": \\\"Everytime there is a \\\"compromise\\\" pro-gun loses something. That and today's compromise is tomorrow's loophole to be closed. All such compromises do is push that policy off until the next round.\\\"\\n\\n}, { \\\"id\\\": \\\"f3paf0j\\\", \\\"body\\\": \\\"Yeah this sub it's not conducive to conversion. It's quickly devolving to little more than \\\"Boogaloo\\\" memes and shouting \\\"SHALL NOT.\\\" at each other. However, as far as I know, the mods won't delete your thread and ban you from the sub for trying to have a good faith discussion, like some of the gun control subs will.\\\"\\n\\n}, { \\\"id\\\": \\\"f3pusbm\\\", \\\"body\\\": \\\"Unfortunately this sub's mod team takes a very passive approach to moderation. With very little effort they could make this sub into a quality pro-gun meeting ground *without having to resort to censorship*. Instead they promote low-effort memes and endless duplication of posts through their.\\\"\\n\\n} ] }\"}"}
{"id": "aTXhTD44nF", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"inaction. Whubbard has the chops to resurrect this sub. Let's see if he's up to the challenge.\\\"\\n\\n\\\"replies:\\n\\n\\\"id\\\": \\\"f3q8xj6\\\",\\n\\\"body\\\": \\\"We voted to ban memes last week. All about rolling it out now.\\\"\\n\\n\\\"replies:\\n\\n\\\"id\\\": \\\"f3qn4p8\\\",\\n\\\"body\\\": \\\"Damn I might have to eat some crow here then...\\\"\\n\\n\\\"id\\\": \\\"f3pafqa\\\",\\n\\\"body\\\": \\\"Found the gun grabber!\\\"\\n\\n\\\"replies:\\n\\n\\\"id\\\": \\\"f3pcw4h\\\",\\n\\\"body\\\": \\\"Witch hunter.\\\"\\n\\n\\\"id\\\": \\\"f3pal5l\\\",\\n\\\"body\\\": \\\"I see people have discussions when it makes sense to. Not much reason to spend time responding to the same gun control measures over and over though.\\\"\\n\\n\\\"id\\\": \\\"f3paw3h\\\",\\n\\\"body\\\": \\\"I get where you're coming from, but people's ability to protect themselves and own their own property isn't something that is compromisable. Anything less, and they cease to own their own property. It's like breathing, there can be nothing less than total ability to breath when and how someone wants. It's just that simple.\\\"\\n\\n\\\"id\\\": \\\"f3pax9m\\\",\\n\\\"body\\\": \\\"My take on this, What kind of open discussion is possible for a right that is guaranteed and most importantly, not to be infringed upon? They're making all these unlawful laws to portray it as it's somehow legitimate. They are not, We are at an apex, to which both political spectrums and even us to a degree are liable for. I certainly believe both sides are waiting for this to boil over so each can finger point. I just speculate it's going to be the hell humanity been whispering about but never thought it would ever occur.\\\"\\n\\n\\\"id\\\": \\\"f3pb6ny\\\",\\n\\\"body\\\": \\\"The time for discussion is over.\\\"\\n\\n\\\"id\\\": \\\"f3pfqwq\\\",\\n\\\"body\\\": \\\"I don't know what you're talking about. Sure people downvote, but they also talk. We get \\\"why do you need 25\"}"}
{"id": "aTXhTD44nF", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"guns \\\" posts at least weekly, and several people will engage in actual conversation with them, citing facts, clearing up statistics, and telling stories to illustrate why this is important to them, but they are usually met with \\\"you stupid @#$%, you think you're Rambo\\\" or something equally clever. People who come here to discuss and learn will be treated well. People who are just trolling are treated like trolls.\\\"\\n\\n\\\"replies\\\":\\n\\n{\\\"id\\\": \\\"f3pghji\\\",\\n\\\"body\\\": \\\"I made this post because I'm always seeing rational, conversation seeking comments getting blown to downvote hell.\\\"\\n}\\n\\n\\\"replies\\\":\\n\\n{\\\"id\\\": \\\"f3pi9xv\\\",\\n\\\"body\\\": \\\"[ Like this one?](https://www.reddit.com/r/progun/comments/dhcu92/yup/f3p75tg) > One smart man in a sub full of ... welp ... \\\" strong opinions \\\". You start off with arrogance, as the sole arbiter of what constitutes a \\\"smart man\\\". Then you back it up with a dismissive swipe at what you term \\\"strong opinions\\\". > Every other country can see that PROPER gun control reduces gun violence by a ton, More arrogance. False equivalence. Unsupported claims.> but the US refuses to let go of it. In a shocking turn of events, more arrogance.> Fully aware that this is a fully pro gun sub, willing to take the downvotes in order to spark a discussion and crack some heads. You aren't the first arrogant asshole to grace this sub with posts like this. Try bringing something other than your own self-importance to the discussion.\\n\\nEdit: And then there's [ this gem](https://www.reddit.com/r/unpopularopinion/comments/d3w5z1/people_living_in_the_us_are_living_in_one_of_the/f06r3sg/) > Wanna feel like you could be shot at every single moment? Move to the US, it'll prob happen to you either as a bystander, or you'd be shot by a random citizen (sometimes police).\\\"\\n\\n{\\\"id\\\": \\\"f3pj8k0\\\",\\n\\\"body\\\": \\\"As is tradition. We're done with that condescending bullshit from antis, you don't come here for good faith discussion and whether you get a reasonable response or not, nothing ever changes, easier to downvote you and move on because we get the same treatment anytime we attempt to speak out in anti subs.\\\"\\n\\n{\\\"id\\\": \\\"f3plgf4\\\",\\n\\\"body\\\": \\\"If downvotes hurt your feelings, you shouldn't be on reddit. People tend to downvote anything they disagree with (which is why some subs specifically ask you to only downvote things that contribute nothing to the discussion). It's a bad habit, but that's the way it is. People downvote and * still * engage. You want to post a view contrary to...\\\"}\"}"}
{"id": "aTXhTD44nF", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the prevailing view of the sub, take your lumps and participate in what conversation you are offered. But if you're only here to preach about how stupid, misguided, unevolved, uneducated, irrational, and/or violent we are, don't expect a polite response.\\n\\nAn arrogant Israeli trying to tell another nation how they should be run. You're just a walking stereotype aren't you? And before you say anything, I popped into your comment history. That's where the calling you Israeli comes from.\\n\\nI thought that trying to tell other nations how they should run was your guys's stereotype.\\n\\nNo we go in and try to make them work our way.\\n\\nIt's a little unfortunate but the grabbers who come on here tend to be intellectually dishonest and/or uninformed. There was some Australian post a few days ago that pretty much asked why we like our guns more than children. No discussion to be had there. There's also some posts that clearly demonstrate the poster should inform himself or herself a little.\\n\\nActually, do that. It shows everyone that they tend to be crazy, unstable, ignorant, stereotyping, arrogant bastards who hate black people with a hair trigger.\\n\\nWelcome to reddit, home of every single safe place for anything that doesn't violate the TOS. At least it's slightly better than r/politics.\\n\\nThis isn't r/gundebate. This is a pro gun subreddit. That said, we do allow some debate provided it remains civil.\"}"}
{"id": "aTXhTD44nF", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sadly tho, r/gundebate is pretty dead...\\n\\nAnything else you want to give away for free?\"}"}
{"id": "aTXhTD44nF", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"{\"author1\": {\"name\": \"rookerin0\", \"stance_annotations\": [{\"id\": \"dhoxyz\", \"label\": \"somewhat_in_favor\", \"reason\": \"Advocates for open discussions, indicating a pro-discussion stance.\"}, {\"id\": \"f3pghji\", \"label\": \"somewhat_in_favor\", \"reason\": \"Expresses desire for rational conversation, showing a supportive stance for dialogue.\"}, {\"id\": \"f3tywb4\", \"label\": \"somewhat_against\", \"reason\": \"Uses sarcasm to counter a stereotype, indicating a stance against the opposing view.\"}, {\"id\": \"f3uomn2\", \"label\": \"somewhat_in_favor\", \"reason\": \"Expresses disappointment in lack of active debate, supporting open discussion.\"}]}, \"dogmatism_label\": \"Open to Dialogue\", \"dogmatism_reason\": \"Shows a consistent preference for discussion and engagement, without dogmatic assertions.\"}, {\"author2\": {\"name\": \"MikeWillTerminate\", \"stance_annotations\": [{\"id\": \"f3rt0bf\", \"label\": \"somewhat_in_favor\", \"reason\": \"Lists conditions for agreement, showing openness to compromise.\"}, {\"id\": \"f3rqu2u\", \"label\": \"strongly_against\", \"reason\": \"Uses strong language to describe opponents, indicating a firm stance against them.\"}]}\"}"}
{"id": "aTXhTD44nF", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E.3 Sample JSON Output from LLM: GPT-4 large\\n\\n```json\\n{\\n    \\\"author1\\\": {\\n        \\\"name\\\": \\\"rookerin0\\\",\\n        \\\"stance_annotations\\\": [\\n            {\\n                \\\"id\\\": \\\"dhoxyz\\\",\\n                \\\"label\\\": \\\"somewhat_in_favor\\\",\\n                \\\"reason\\\": \\\"Expresses desire for open discussions, indicating a stance in favor of dialogue.\\\"\\n            },\\n            {\\n                \\\"id\\\": \\\"f3pghji\\\",\\n                \\\"label\\\": \\\"somewhat_in_favor\\\",\\n                \\\"reason\\\": \\\"Advocates for rational conversation despite observing negative reactions.\\\"\\n            },\\n            {\\n                \\\"id\\\": \\\"f3tywb4\\\",\\n                \\\"label\\\": \\\"somewhat_against\\\",\\n                \\\"reason\\\": \\\"Uses sarcasm to counter a stereotype, showing a stance against it.\\\"\\n            },\\n            {\\n                \\\"id\\\": \\\"f3uomn2\\\",\\n                \\\"label\\\": \\\"stance_not_inferrable\\\",\\n                \\\"reason\\\": \\\"Comment on the state of another subreddit does not reveal a stance on the gun debate.\\\"\\n            }\\n        ],\\n        \\\"dogmatism_label\\\": \\\"Open to Dialogue\\\",\\n        \\\"dogmatism_reason\\\": \\\"Shows a consistent preference for discussion over confrontation.\\\"\\n    },\\n    \\\"author2\\\":{\\n        \\\"name\\\": \\\"MikeWillTerminate\\\",\\n        \\\"stance_annotations\\\": [\\n            {\\n                \\\"id\\\": \\\"f3rt0bf\\\",\\n                \\\"label\\\": \\\"somewhat_in_favor\\\",\\n                \\\"reason\\\": \\\"Lists conditions for accepting UBCs, indicating some openness to compromise.\\\"\\n            },\\n            {\\n                \\\"id\\\": \\\"f3rqu2u\\\",\\n                \\\"label\\\": \\\"strongly_against\\\",\\n                \\\"reason\\\": \\\"Uses derogatory language to describe opponents, showing strong opposition.\\\"\\n            }\\n        ],\\n        \\\"dogmatism_label\\\": \\\"Firm but Open\\\",\\n        \\\"dogmatism_reason\\\": \\\"While showing strong opinions, also suggests conditions for policy acceptance.\\\"\\n    }\\n}\\n```\"}"}
{"id": "aTXhTD44nF", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E.4 Sample JSON Output from LLM: Mistral Large Zero-shot\\n\\n```json\\n{\\n  \\\"author1\\\": {\\n    \\\"name\\\": \\\"rookerin0\\\",\\n    \\\"stance_annotations\\\": [\\n      {\\n        \\\"id\\\": \\\"dhoxyz\\\",\\n        \\\"label\\\": \\\"somewhat_in_favor\\\",\\n        \\\"reason\\\": \\\"Author expresses desire for open discussions on anti-gun ideals.\\\"\\n      },\\n      {\\n        \\\"id\\\": \\\"f3pghji\\\",\\n        \\\"label\\\": \\\"stance_not_inferrable\\\",\\n        \\\"reason\\\": \\\"Author is commenting on downvotes, not expressing stance.\\\"\\n      },\\n      {\\n        \\\"id\\\": \\\"f3tywb4\\\",\\n        \\\"label\\\": \\\"stance_not_inferrable\\\",\\n        \\\"reason\\\": \\\"Author is making a stereotypical comment, not expressing stance.\\\"\\n      },\\n      {\\n        \\\"id\\\": \\\"f3uomn2\\\",\\n        \\\"label\\\": \\\"stance_not_inferrable\\\",\\n        \\\"reason\\\": \\\"Author is commenting on the state of another subreddit, not expressing stance.\\\"\\n      }\\n    ],\\n    \\\"dogmatism_label\\\": \\\"Open to Dialogue\\\",\\n    \\\"dogmatism_reason\\\": \\\"Author expresses desire for open discussions and does not dismiss opposing views.\\\"\\n  },\\n  \\\"author2\\\": {\\n    \\\"name\\\": \\\"MikeWillTerminate\\\",\\n    \\\"stance_annotations\\\": [\\n      {\\n        \\\"id\\\": \\\"f3rt0bf\\\",\\n        \\\"label\\\": \\\"strongly_in_favor\\\",\\n        \\\"reason\\\": \\\"Author lists conditions for accepting UBCs, showing strong pro-gun stance.\\\"\\n      },\\n      {\\n        \\\"id\\\": \\\"f3rqu2u\\\",\\n        \\\"label\\\": \\\"strongly_in_favor\\\",\\n        \\\"reason\\\": \\\"Author\u2019s comment is dismissive of anti-gun advocates, showing strong pro-gun stance.\\\"\\n      }\\n    ],\\n    \\\"dogmatism_label\\\": \\\"Deeply Rooted\\\",\\n    \\\"dogmatism_reason\\\": \\\"Author\u2019s comments are dismissive of opposing views and show a strong, unchangeable belief in pro-gun ideals.\\\"\\n  }\\n}\\n```\\n\\nE.5 Sample JSON Output from LLM: Mistral Large One-shot\\n\\n```json\\n{\\n  \\\"author1\\\": {\\n    \\\"name\\\": \\\"rookerin0\\\",\\n    \\\"stance_annotations\\\": [\\n      {\\n        \\\"id\\\": \\\"dhoxyz\\\",\\n        \\\"label\\\": \\\"somewhat_in_favor\\\",\\n        \\\"reason\\\": \\\"Author expresses desire for open discussions on anti-gun ideals.\\\"\\n      },\\n      {\\n        \\\"id\\\": \\\"f3pghji\\\",\\n        \\\"label\\\": \\\"stance_not_inferrable\\\",\\n        \\\"reason\\\": \\\"Author is commenting on downvotes, not expressing stance.\\\"\\n      },\\n      {\\n        \\\"id\\\": \\\"f3tywb4\\\",\\n        \\\"label\\\": \\\"stance_not_inferrable\\\",\\n        \\\"reason\\\": \\\"Author is making a stereotypical comment, not expressing stance.\\\"\\n      },\\n      {\\n        \\\"id\\\": \\\"f3uomn2\\\",\\n        \\\"label\\\": \\\"stance_not_inferrable\\\",\\n        \\\"reason\\\": \\\"Author is commenting on the state of another subreddit, not expressing stance.\\\"\\n      }\\n    ],\\n    \\\"dogmatism_label\\\": \\\"Open to Dialogue\\\",\\n    \\\"dogmatism_reason\\\": \\\"Author expresses desire for open discussions and does not dismiss opposing views.\\\"\\n  }\\n}\\n```\"}"}
{"id": "aTXhTD44nF", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"{\"stance_annotations\": [{\"id\":\"dhoxyz\",\"label\":\"somewhat_in_favor\",\"reason\":\"Expresses desire for open discussions on gun control, but follows pro-gun subreddit.\",\"author\":null,\"dogmatism_label\":\"Open to Dialogue\",\"dogmatism_reason\":\"Seeks open discussions and does not express strong dogmatic views.\"},{\"id\":\"f3pghji\",\"label\":\"stance_not_inferrable\",\"reason\":\"Comments on downvotes, but no clear stance on gun control.\",\"author\":null,\"dogmatism_label\":\"Open to Dialogue\",\"dogmatism_reason\":\"No clear stance expressed.\",\"id\":\"f3tywb4\",\"label\":\"stance_not_inferrable\",\"reason\":\"Comment about stereotypes, but no clear stance on gun control.\",\"author\":null,\"dogmatism_label\":\"Open to Dialogue\",\"dogmatism_reason\":\"No clear stance expressed.\",\"id\":\"f3uomn2\",\"label\":\"stance_not_inferrable\",\"reason\":\"Mentions r/gundebate being dead, but no clear stance on gun control.\",\"author\":null,\"dogmatism_label\":\"Open to Dialogue\",\"dogmatism_reason\":\"No clear stance expressed.\"}],\"dogmatism_label\":\"Open to Dialogue\",\"dogmatism_reason\":\"Seeks open discussions and does not express strong dogmatic views.\"},\\n\\n{\"author2\": {\"name\":\"MikeWillTerminate\",\"stance_annotations\": [{\"id\":\"f3rt0bf\",\"label\":\"somewhat_in_favor\",\"reason\":\"Lists conditions for supporting UBCs, showing openness to compromise.\",\"author\":null,\"dogmatism_label\":\"Firm but Open\",\"dogmatism_reason\":\"Expresses strong pro-gun views but open to compromise under certain conditions.\"},{\"id\":\"f3rqu2u\",\"label\":\"strongly_in_favor\",\"reason\":\"Strong negative language against gun control advocates, suggesting strong pro-gun stance.\",\"author\":null,\"dogmatism_label\":\"Firm but Open\",\"dogmatism_reason\":\"Expresses strong pro-gun views but open to compromise under certain conditions.\"}],\"dogmatism_label\":\"Firm but Open\",\"dogmatism_reason\":\"Expresses strong pro-gun views but open to compromise under certain conditions.\"}}\"}"}
{"id": "aTXhTD44nF", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"|   | DR  | FBO | FX  | OTD |\\n|---|-----|-----|-----|-----|\\n| 0 | 32  | 0   | 9   |     |\\n| 0 | 119 | 0   | 77  |     |\\n| 0 | 3   | 1   | 17  |     |\\n| 0 | 69  | 0   | 131 |     |\\n\\n|   | DR  | FBO | FX  | OTD |\\n|---|-----|-----|-----|-----|\\n| 0 | 28  | 0   | 12  |     |\\n| 0 | 92  | 0   | 97  |     |\\n| 0 | 0   | 0   | 23  |     |\\n| 0 | 74  | 0   | 134 |     |\\n\\n**Figure 10:** Confusion matrix for LLaMa-3-8B Dogmatism detection models on USDC test set: finetuning (left) and instruction-tuning (right). Here, DR: Deeply Rooted, FX: Flexible, FBO: Firm but Open, OTD: Open to Dialogue.\\n\\n```json\\n[{\\n    \\\"id\\\": \\\"f3pghji\\\",\\n    \\\"label\\\": \\\"stance_not_inferrable\\\",\\n    \\\"reason\\\": \\\"Comments on downvotes, but no clear stance on the topic.\\\"\\n},\\n{\\n    \\\"id\\\": \\\"f3tywb4\\\",\\n    \\\"label\\\": \\\"stance_not_inferrable\\\",\\n    \\\"reason\\\": \\\"Makes a stereotypical comment, but no clear stance on the topic.\\\"\\n},\\n{\\n    \\\"id\\\": \\\"f3uomn2\\\",\\n    \\\"label\\\": \\\"stance_not_inferrable\\\",\\n    \\\"reason\\\": \\\"Mentions r/gundebate being dead, but no clear stance on the topic.\\\"\\n}]\\n```\\n\\n**Dogmatism Label:** Open to Dialogue\\n**Dogmatism Reason:** Seeks open discussions and engages in conversation without strong dogmatic language.\\n\\n**Author:** MikeWillTerminate\\n**Stance Annotations:**\\n- **Somewhat in Favor:** Lists conditions for supporting UBCs, showing openness to discussion.\\n- **Strongly Against:** Uses derogatory language to express strong opposition to 'grabbers'.\\n\\n**Dogmatism Label:** Firm but Open\\n**Dogmatism Reason:** Expresses strong opinions but also shows willingness to consider certain conditions for compromise.\"}"}
{"id": "aTXhTD44nF", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fig. 10 illustrates the confusion matrix for dogmatism detection for LLaMa-3-8B finetuning and instruction-tuning. We make the following observations from Fig. 10: 1) For both finetuning and instruction-tuning, there is significant misclassifications, especially for the \\\"Deeply Rooted\\\" and \\\"Flexible\\\" labels, with both having zero accuracy and F1-scores. While \\\"Firm but Open\\\" and \\\"Open to Dialogue\\\" perform moderately better, with accuracies of 48.7% and 64.4% respectively.\\n\\nThe confusion matrix indicates substantial confusion to distinguish between intermediate levels of dogmatism, such as \\\"Firm but Open\\\" and \\\"Open to Dialogue. We further reports the ROC curve shows the trade-off between the true positive rate (TPR) and false positive rate (FPR) for each class for stance and dogmatism tasks, in Figs. 11 and 12. The area under the ROC curve (AUC) is a measure of the model's ability to distinguish between classes.\"}"}
{"id": "aTXhTD44nF", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 12: LLaMa-3-8B finetuning for dogmatism task: Visualize the ROC curves for each class along with their AUC values for GPT-4 Annotations across zero-shot, one-shot, few-shot and majority labels.\"}"}
{"id": "aTXhTD44nF", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 13: LLaMa-3-8B instruction-tuning for stance detection task: Visualize the ROC curves for each class along with their AUC values for GPT-4 Annotations across zero-shot, one-shot, few-shot and majority labels.\"}"}
