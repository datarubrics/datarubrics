{"id": "E18kRXTGmV", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The question must be answerable even without the multiple-choice. Example of the invalid question: (\u201cWhat song is not performed by this musician\u201d \u2013 not answerable if you don\u2019t know the choices)\\n\\nMake sure the questions are written fluently in both the local language and English. Use a grammar checker if needed i.e. if you are not fluent in English.\\n\\nBe mindful of cultural sensitivities and avoid stereotyping or misrepresenting cultural aspects.\\n\\nEnsure there are variations on your question. Identity questions are fine, eg \u201cWhat is this\u201d, or \u201cwhere is this\u201d. But additionally adding more complex/difficult questions would be great. For example, multi-hop reasoning, counting, referencing, or questions that require local commonsense knowledge to be answered.\\n\\nCategory Definition\\n\\n- Vehicles and Transportation: Local public transport, local vehicles.\\n- Objects, Materials, Clothing: Questions about local/traditional clothes. Unique/local tools or items.\\n- Cooking and Food: Local dishes and food/drink. This category includes native fruits in the context of the image if that fruit is served as a food/drink.\\n- Geography, Buildings, Landmarks: Popular/common landmarks, local architecture/buildings. Local monuments.\\n- Plants and Animals: Plants and animals commonly found in the region.\\n- Brands, Products, and Companies: Questions about understanding local yet popular brands or companies. Even if the brand is about food/transportation, if the main focus of the question is the brand recognition itself, then it should be under this category.\\n- Sports & Recreation: Local sports and fun activities. Focuses on the activity itself rather than the location (in that case, it goes to the \u2018landmark\u2019 category).\\n- Tradition, Art, History: Local ceremonies/festivals/events, local dance/music, folklores. Historical artifacts.\\n- People & Everyday Life: Focuses on the people themselves: i.e., common habits/customs, common occupations and jobs, routine religious activities, everyday activities/routines.\"}"}
{"id": "E18kRXTGmV", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Public Figures & Pop Culture:\\nQuestions on the understanding of common public figures (e.g., politicians, artists, musicians, etc.). Common pop culture such as movies and games.\\n\\nIf the category is still ambiguous to you, pick the one you think is the most appropriate.\\n\\nExamples that can be improved\\n1) \u00bfEn qu\u00e9 mes se celebra esta fiesta? (In which month is this celebration held?)\\n   Correct\\n2) \u00bfEn qu\u00e9 mes se celebra la fiesta de la \u201cMama Negra\u201d? (In which month is the celebration of the \u201cMama Negra\u201d held?)\\n   Wrong\u2013As this question can be answered without looking at the image.\\n\\nMake sure the question is not ambiguous:\\n1) Where is this monument located?\\n   Wrong\u2013Not specific, the answer could be a city, country, province, etc.\\n2) In which city is this monument located?\\n   Correct\u2013specifically asking about the city\\n\\nMake sure the question is not too vague:\\n1) What is this?\\n   Question wording can be more specific\\n2) What is the name of this vehicle?\\n   Correct\u2013specifically asking about the vehicle name.\\n\\nAcceptable examples\\nCategory: Tradition / Art / History \u2013 Spanish/Mexico\\n\u00bfQu\u00e9 se muestra en la imagen? (What is shown in the image?)\\nA. el calendario azteca/ piedra del sol (the aztec calendar/ aztec sun stone)\\nB. una serpiente azteca (an aztec serpent)\\nC. coatlicue (coatlicue)\\nD. tl\u00e1loc (tlaloc)\\n\\n\u00bfEn d\u00f3nde se exhibe esta pieza? (Where is this piece exhibited?)\\nA. En el museo nacional de antropolog\u00eda (In the National Museum of Antropology)\\nB. en el castillo de Chapultepec (In the Chapultepec Castle)\\nC. En el z\u00f3calo de la ciudad de Mexico (In the Mexico City zocalo)\\nD. En Teotihuacan (In Teotihuacan)\"}"}
{"id": "E18kRXTGmV", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ked\u1ee5 mmemme nd\u1ecb a na-eme? (Which ceremony are they doing?)\\n\\nA. \u1eca gba nkw\u1ee5 (Traditional marriage)\\nB. Ncheta \u1eccm\u1ee5m\u1ee5 (Birthday)\\nC. Emume cheiftaincy (Chieftaincy ceremony)\\nD. Emume iri ji \u1ecdh\u1ee5r\u1ee5 (New yam festival)\\n\\nKed\u1ee5 ebe a na-eme mmemme a? (Where is this ceremony held?)\\n\\nA. \u1ee4l\u1ecd nna nwunye (The home of the bride's father)\\nB. Ahia (The market)\\nC. \u1ee4l\u1ecd nna di (The home of the groom's father)\\nD. \u1ee4l\u1ecd ns\u1ecd (The church)\\n\\nPada tahun berapakah foto ini diambil? (In what year is this photo taken?)\\n\\nA. 2015 (2015)\\nB. 2020 (2020)\\nC. 2023 (2023)\\nD. 2010 (2010)\\n\\nApa nama pasukan yang ada di foto ini? (What is the name of the squad in this photo?)\\n\\nA. Paskibraka (Paskibraka)\\nB. Brimob (Brimob)\\nC. TNI (TNI)\\nD. ABRI (ABRI)\\n\\nApa tugas utama pasukan ini? (What is the main purpose of this squad?)\\n\\nA. Mengibarkan bendera (Hoisting the flag)\\nB. Mengawal presiden (Escorting president)\\nC. Menjaga keamanan (Maintaining security)\\nD. Mengiringi pengantin (Accompanying the bride and groom)\\n\\nNaon kagunaan ieu hiji alat? (What is the use of this tool?)\\n\\nA. Alat musik (Musical instrument)\\nB. Alat pertahanan diri (Self defence tool)\\nC. Jemuran (Clothes drying equipment)\\nD. Alat masak (Cooking tool)\\n\\nIeu hiji alat teh asalna ti propinsi mana di Indonesia? (This tool comes from which province in Indonesia?)\\n\\nA. Jawa Barat (West Java)\\nB. Bali (Bali)\"}"}
{"id": "E18kRXTGmV", "page_num": 20, "content": "{\"primary_language\":\"id\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Apakah nama bunga dalam gambar ini?\\n\\nA. Pakma (Rafflesia)\\nB. Bunga raya (Hibiscus)\\nC. Anggerik (Orchid)\\nD. Bunga kertas (Bougainvillea)\\n\\nDi rantau Asia manakah bunga itu boleh ditemui?\\n\\nA. Asia Tenggara (Southeast Asia)\\nB. Asia Timur (East Asia)\\nC. Asia Selatan (South Asia)\\nD. Asia Tengah (Central Asia)\\n\\nOpo arane wong seng nang tengah embong iki?\\n\\nA. Polisi cepek (Polisi cepek)\\nB. Tukang parkir (Parking assistance man)\\nC. Mlijo (Grocery man)\\nD. Tukang becak (Pedicab man)\\n\\nOpo seng dilakukno wong seng nang tengah dalan iku?\\n\\nA. Ngatur prapatan (Managing the intersection)\\nB. Njaluk donasi (Asking for donations)\\nC. Ngawasi pelanggaaran lalu lintas (Looking out for traffic violations)\\nD. Nunjukno arah (Showing directions)\\n\\nRoh manakah yang disembah dengan altar ini?\\n\\nA. Datuk Gong (Na Tuk Kong)\\nB. Buddha (Buddha)\\nC. Brahma (Brahma)\\nD. Vishnu (Vishnu)\\n\\nApakah agama yang diamalkan oleh pengguna altar ini?\\n\\nA. Taoism (Taoisme)\\nB. Buddha (Buddhisme)\\nC. Islam (Islam)\\nD. Hindu (Hinduisme)\"}"}
{"id": "E18kRXTGmV", "page_num": 21, "content": "{\"primary_language\":\"id\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Apa yang orang-orang ini lakukan?\\n\\nA. Berwudhu (Performing ablution)\\nB. Mandi (Taking a bath)\\nC. Yoga (Yoga)\\nD. Beribadah (Praying)\\n\\nDimana biasanya orang-orang melakukan aktivitas di foto ini?\\n\\nA. Masjid (Mosque)\\nB. Gereja (Church)\\nC. Pemandian umum (Public bath)\\nD. Gym (Gym)\\n\\nAnong tawag sa kakanin na ito?\\n\\nA. Puto Bumbong (Puto Bumbong)\\nB. Suman (Suman)\\nC. Kutsinta (Kutsinta)\\nD. Sapin-Sapin (Sapin-Sapin)\\n\\nTuwing kailan ito madalas tinitinda sa Pilipinas?\\n\\nA. Christmas Season (Christmas Season)\\nB. Independence Day (Independence Day)\\nC. Labor Day (Labor Day)\\nD. National Heroes Day (National Heroes Day)\\n\\nAno tawag dun sa brown?\\n\\nA. Muscovado (Muscovado)\\nB. Latik (Toasted coconut)\\nC. Chocolate (Chocolate)\\nD. Caramel (Caramel)\"}"}
{"id": "E18kRXTGmV", "page_num": 22, "content": "{\"primary_language\":\"ko\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\uc774\ub7f0 \uc885\ub958\uc758 \uc694\ub9ac\uc5d0 \uc0ac\uc6a9\ub418\ub294 \uadf8\ub987\uc744 \ubb34\uc5c7\uc774\ub77c\uace0 \ubd80\ub974\ub098\uc694?\\n\\nA. \ub3cc\uc1a5 (Dolsot)\\nB. \ubcf5\uc8fc\uba38\ub2c8 (Bokjumeoni)\\nC. \ub0c4\ube44 (Pot)\\nD. \ud32c (Pan)\\n\\n\uadf8\ub987\uc758 \uc7ac\uc9c8\uc740 \ubb34\uc5c7\uc778\uac00\uc694?\\n\\nA. \ub3cc (Stone)\\nB. \ub3c4\uc790\uae30 (Ceramic)\\nC. \uc720\ub9ac (Glass)\\nD. \uc2a4\ud14c\uc778\ub9ac\uc2a4 \uc2a4\ud2f8 (Stainless Steel)\\n\\n\u00bfC\u00f3mo se llama este monumento ubicado en Quito?\\n\\nA. Virgen de El Panecillo (The Virgin of El Panecillo)\\nB. Manto de Mar\u00eda (Manto de Mar\u00eda)\\nC. Mitad del mundo (Middle of the world)\\nD. Cristo de la concordia (Christ of peace)\\n\\nC\u00e9n cathair ina bhfuil na dealbha seo?\\n\\nA. Cathair Bhaile \u00c1tha Cliath (Dublin City)\\nB. P\u00e1ras (Paris)\\nC. Cathair Corcaigh (Cork City)\\nD. Beirl\u00edn (Berlin)\\n\\nC\u00e9n eachtra stairi\u00fail at\u00e1 l\u00e9irithe sna dealbha seo?\\n\\nA. An Ghorta Mh\u00f3r (The Great Famine)\\nB. \u00c9ir\u00ed Amach 1916 (The 1916 Rising)\\nC. Teitheadh na n-Iarla\u00ed (The flight of the Earls)\\nD. Cogadh 1835 (The 1835 war)\\n\\nC\u00e9n abhainn at\u00e1 le taobh na ndealbh seo?\\n\\nA. An Life (The Liffey)\\nB. An tSionann (The Shannon)\\nC. Abhainn an R\u00ed (King's River)\\nD. An Thames (The Thames)\"}"}
{"id": "E18kRXTGmV", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use JotForm as our annotation platform. For question entry, contributors can upload and write questions in both languages in the form. The interface can be seen in Figure 4. During validation, contributors can see all the data submitted by other contributors (Figure 5). They can select the entry to see detailed preview of the entry (Figure 6). They can then either edit the data directly, provide comments, or confirm the data by starring the entry.\\n\\nMost-Frequent Words in the Questions\\n\\nFigure 7 shows word clouds for the most frequent words in CVQA per category. We exclude stopwords as well as 'picture', 'photo', and 'image' from the list, since most questions contain these words. In this VQA context, we can treat them as stopwords.\"}"}
{"id": "E18kRXTGmV", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: During validation, contributors can preview the submission from other contributors.\\n\\nFigure 7: Word Cloud in CVQA per category.\\n\\nFigure 8 illustrates the demographic statistics of the annotators, based on an anonymous questionnaire we provided. At the time of writing, we have information for 36 out of 76 annotators. As such, this breakdown is a rough representation of the annotation group.\\n\\nFigure 8: Annotator demographic statistics.\\n\\nE Country-Language Pairs and Scripts\\n\\nIn Table 8, we provide information on the script used in each Country-Language pair.\"}"}
{"id": "E18kRXTGmV", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Country | Language | Script |\\n|---------|----------|--------|\\n| Africa  |          |        |\\n|         | Egypt    |        |\\n|         | Egyptian Arabic | Arabic |\\n|         | Ethiopia  |        |\\n|         | Oromo    | Latin  |\\n|         |         |        |\\n|         | Kenya    |        |\\n|         | Swahili  | Latin  |\\n|         | Nigeria  |        |\\n|         | Igbo     | Latin  |\\n|         | Rwanda   |        |\\n|         | Kinyarwanda | Latin  |\\n| Asia    |          |        |\\n|         | China    |        |\\n|         | Chinese  |        |\\n|         | India    |        |\\n|         | Bengali  |        |\\n|         | Hindi    | Devanagari |\\n|         | Marathi  | Devanagari |\\n|         | Tamil    | Tamil  |\\n|         | Telugu   | Telugu |\\n|         | Urdu     | Perso-Arabic  |\\n|         | Indonesia |        |\\n|         | Indonesian |        |\\n|         | Javanese | Latin  |\\n|         | Minangkabau | Latin  |\\n|         | Sundanese | Latin  |\\n|         | Japan    |        |\\n|         | Japanese | Japanese  |\\n|         | South Korea |        |\\n|         | Korean   | Hangul |\\n|         | Malaysia |        |\\n|         | Malay    | Latin  |\\n|         | Mongolia | Cyrillic |\\n|         | Pakistan |          |\\n|         | Urdu     | Perso-Arabic |\\n|         | Philippines |        |\\n|         | Filipino | Latin  |\\n|         | Singapore | Chinese |\\n|         | Sri Lanka |        |\\n|         | Sinhala  | Sinhalese |\\n| Europe  |          |        |\\n|         | Bulgaria | Cyrillic |\\n|         | France   | Latin  |\\n|         | Ireland  | Latin  |\\n|         | Norway   | Latin  |\\n|         | Romania  | Latin  |\\n|         | Russia   | Cyrillic |\\n|         | Spain    | Latin  |\\n| Latin America |          |        |\\n|         | Argentina | Latin  |\\n|         | Brazil   | Latin  |\\n|         | Chile    | Latin  |\\n|         | Colombia | Latin  |\\n|         | Ecuador  | Latin  |\\n|         | Mexico   | Latin  |\\n|         | Uruguay  | Latin  |\\n\\nTable 8: The list of Country-Language pairs covered in CVQA and their corresponding scripts.\\n\\nTable 9 lists the authors and their respective affiliations.\"}"}
{"id": "E18kRXTGmV", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Author Affiliation                      |\\n|----------------------------------------|\\n| David Romero MBZUAI                    |\\n| Chenyang Lyu MBZUAI                    |\\n| Haryo Akbarianto Wibowo MBZUAI         |\\n| Teresa Lynn MBZUAI                     |\\n| Injy Hamed MBZUAI                      |\\n| Aditya Nanda Kishore IIT Madras        |\\n| Aishik Mandal TU Darmstadt             |\\n| Alina Draggonetti Universidad de la Rep\u00fablica |\\n| Artem Abzaliev University of Michigan  |\\n| Atnafu Lambebo Tonja Independent       |\\n| Bontu Fufa Balcha Independent          |\\n| Chenxi Whitehouse University of Cambridge |\\n| Christian Salamea Universidad Polit\u00e9cnica Salesiana |\\n| Dan John Velasco Samsung Research      |\\n| David Ifeoluwa Adelani Independent     |\\n| David Le Meur Bretagne num\u00e9rique       |\\n| Fajri Koto MBZUAI                      |\\n| Fauzan Farooqui Independent            |\\n| Frederico Belcavello Federal University of Juiz de Fora |\\n| Ganzorig Batnasan United Arab Emirates University / MBZUAI |\\n| Gisela Vallejo The University of Melbourne |\\n| Grainne Caulfield Dublin City University |\\n| Guido Ivetta Universidad Nacional de C\u00f3rdoba |\\n| Haiyue Song NICT                        |\\n| Henok Biadgilign Ademtew EAII         |\\n| Hern\u00e1n Maina Universidad Nacional de C\u00f3rdoba/CONICET |\\n| Holy Lovenia AI Singapore                |\\n| Israel Abebe Azime Saarland University |\\n| Jan Christian Blaise Cruz Samsung Research Philippines |\\n| Jay Gala MBZUAI                         |\\n| Jesus-German Ortiz-Barajas MBZUAI      |\\n| Jiahui Geng MBZUAI                     |\\n| Jinheon Baek KAIST                     |\\n| Jocelyn Dunstan Escudero Pontificia Universidad Cat\u00f3lica de Chile |\\n| Kumaranage Ravindu Yasas Nagasinghe MBZUAI |\\n| Laura Alonso Alemany Universidad Nacional de C\u00f3rdoba |\\n| Luciana Benotti Universidad Nacional de C\u00f3rdoba/CONICET |\\n| Luis Fernando D'Haro Universidad Polit\u00e9cnica de Madrid |\\n| Marcelo Viridiano                  Federal University of Juiz de Fora |\\n| Marcos Estechaga Garitagoitia Universidad Polit\u00e9cnica de Madrid |\\n| Maria Camila Buitrago Cabrera University of Stuttgart |\\n| Mario Rodr\u00edg\u00edguez-Cantelar Universidad Polit\u00e9cnica de Madrid |\\n| M\u00e9lanie Jouitteau IKER, CNRS           |\\n| Mihail Mihaylov MBZUAI                 |\\n| Mohamed Fazli Mohamed Imam MBZUAI     |\\n| Muhammad Farid Adilazuarda MBZUAI      |\\n| Munkh-Erdene Otgonbold United Arab Emirates University |\"}"}
{"id": "E18kRXTGmV", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Author | Affiliation                                      |\\n|--------|-------------------------------------------------|\\n| Munkhjargal Gochoo | United Arab Emirates University                  |\\n| Naome A. Etori      | Independent Researcher                          |\\n| Olivier NIY-OMUGISHA | Independent Researcher                          |\\n| Paula M\u00f3nica Silva  | Millennium Institute Foundation Research         |\\n| Pranjal Chitale     | Independent Researcher                          |\\n| Raj Dabre           | IIT Madras                                       |\\n| Rendi Chevi         | MBZUAI                                           |\\n| Ruochen Zhang       | Brown University                                 |\\n| Ryandito Dianadaru  | ITB                                              |\\n| Samuel Cahyawijaya  | HKUST                                            |\\n| Santiago Gonzaga    | Universidad de la Rep\u00fablica                      |\\n| Soyeong Jeong       | KAIST                                            |\\n| Sukannya Purkayastha| TU Darmstadt                                      |\\n| Tatsuki Kuribayashi | MBZUAI                                           |\\n| Thanmay Jayakumar   | IIT Madras                                       |\\n| Tiago Timponi Torrent| Federal University of Juiz de Fora, CNPq         |\\n| Toqeer Ehsan        | MBZUAI                                           |\\n| Vladimir Araujo     | KU Leuven                                        |\\n| Yova Kementchedjieva| MBZUAI                                           |\\n| Zara Burzo          | Skyline High-School                              |\\n| Zheng Wei Lim       | The University of Melbourne                      |\\n| Zheng-Xin Yong      | Brown University                                 |\\n| Oana Ignat          | University of Michigan                           |\\n| Joan Nwatu          | University of Michigan                           |\\n| Rada Mihalcea       | University of Michigan                           |\\n| Thamar Solorio      | MBZUAI                                           |\\n| Alham Fikri Aji     | MBZUAI                                           |\"}"}
{"id": "E18kRXTGmV", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark\\n\\nAbstract\\n\\nVisual Question Answering (VQA) is an important task in multimodal AI, and it is often used to test the ability of vision-language models to understand and reason on knowledge present in both visual and textual data. However, most of the current VQA models use datasets that are primarily focused on English and a few major world languages, with images that are typically Western-centric. While recent efforts have tried to increase the number of languages covered on VQA\"}"}
{"id": "E18kRXTGmV", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"datasets, they still lack diversity in low-resource languages. More importantly, although these datasets often extend their linguistic range via translation or some other approaches, they usually keep images the same, resulting in narrow cultural representation. To address these limitations, we construct CVQA, a new culturally-diverse multilingual Visual Question Answering (VQA) benchmark, designed to cover a rich set of languages and cultures, where we engage native speakers and cultural experts in the data collection process. As a result, CVQA includes culturally-driven images and questions from across 30 countries on four continents, covering 31 languages with 13 scripts, providing a total of 10k questions. We then benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and show that the dataset is challenging for the current state-of-the-art models. This benchmark can serve as a probing evaluation suite for assessing the cultural capability and bias of multimodal models and hopefully encourage more research efforts toward increasing cultural awareness and linguistic diversity in this field.\\n\\n1 Introduction\\n\\nVisual Question Answering (VQA) [2, 43, 50] is a task that requires AI systems to answer textual questions based on a given context image. VQA serves as an essential measure for assessing the understanding and reasoning capabilities of Multimodal Large Language Models (MLLMs) across diverse images and texts. With the rapid development of MLLMs, significant improvements have been observed, including support for multiple languages [12, 5, 27, 45, 53]. However, there is still a lack of VQA benchmarks that capture a diverse set of languages and cultural contexts. Specifically, most VQA benchmarks only cover the English language [2, 33]. While some work has been undertaken on multilingual VQA, it either covers a limited set of popular languages or is producing questions via translation/generation of text from the original Western-centric images, thus failing to capture cultural nuances inherent in different languages [6, 44].\\n\\nTo address these limitations, we propose CVQA: a novel, large-scale, multilingual, culturally nuanced VQA benchmark that includes a diverse set of languages, including many that are underrepresented and understudied. CVQA follows the grassroots crowd-sourcing collaboration approaches taken by Masakhane for Africa [37], NusaCrowd for Indonesia [4], and AI4Bharat for India [20]. In our case, however, we collaborate across communities, rather than within one particular community, in order to maximize cultural and linguistic representation. Consequently, our data consists of 10k questions across 30 countries, covering 31 languages. We also sub-categorize CVQA based on Country-Language pairs, resulting in 39 distinct pairs, which is substantially more extensive than existing VQA benchmarks. Furthermore, each sample in CVQA falls into one of 10 diverse categories (see Table 1) and is annotated and validated by fluent speakers and those familiar with the respective cultures, ensuring high quality and diversity. Lastly, CVQA is written in both English and local languages, enabling us to benchmark multilingual MLLMs and English-only MLLMs.\\n\\nIn this study, we benchmark CVQA across various MLLMs and find that it presents a significant challenge for open MLLMs, which most of the time achieve no more than 50% accuracy. Additionally, we observe a notable degradation in model performance when questions are asked in native languages, particularly those in understudied languages such as Breton from France and Javanese from Indonesia, highlighting a significant gap in understanding multilingual prompts. We further conduct several ablation studies to analyze the models' performance across different question categories, regions, languages, and image sources. Our contributions can be summarised as follows:\\n\\n\u2022 First, we introduce CVQA, a new culturally diverse multilingual visual question answering dataset consisting of over 10,000 questions from across 30 countries and 31 languages.\\n\u2022 Second, we provide extensive documentation on our process to crowdsource such large dataset across numerous communities, including annotation guidelines.\\n\u2022 Finally, we provide an initial set of evaluations on this benchmark, to serve as a baseline for future research on vision-language models that are culturally diverse.\"}"}
{"id": "E18kRXTGmV", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We note that efforts to enhance cultural awareness in models are increasingly gaining attention. As such, our work contributes to the growing interest within the community and can encourage further initiatives to broaden the limited world view currently captured by MLLMs.\\n\\n2. CVQA Data Collection\\n\\nThe construction of our CVQA dataset involved a detailed annotation process that aims at creating a culturally diverse and linguistically comprehensive dataset for Visual Question Answering. It is worth noting that, while defining culture is challenging, we follow Adilazuarda et al. [1] by using common-ground knowledge (e.g., information surrounding local dishes, history, places, etc. that is generally shared by the people within the region) as a proxy of culture. In this section, we now turn to outline the detailed procedures followed during the data collection and annotation phases.\\n\\n2.1. Dataset Collection Design\\n\\nCountry-Language Pair Subset\\n\\nCVQA is a multilingual, multiple-choice locally-nuanced visual question-answering dataset. The format is similar to commonly used visual QA data such as VQA [2], VQA-2 [13] or GQA [17]. Yet, in contrast to them, we gathered images and created question-answer pairs based on the cultures of various locations. Moreover, for each location, the question-answer pairs were created in their respective local languages, along with parallel English translations. Some languages are shared across different locations (e.g., Mexico-Spanish vs Spain-Spanish), and vice-versa, different languages are shared across the same location (e.g., Indonesia-Indonesian vs Indonesia-Javanese). Therefore, to capture them, we group our CVQA dataset into several subsets based on this Country-Language pair, rather than simply on language or location only.\\n\\nAnnotators\\n\\nTo elicit image collectors and annotation contributions to this project, we reached out to our network, which included both linguistic groups and NLP communities. Annotators needed to be fluent speakers of the language in question and be accustomed to the cultures of the locations for which they provided data. To promote data collection, contributors with significant contributions, either by contributing at least 100 validated question-answer pairs and/or managing several subsets, are rewarded as co-authors in this paper. The annotator demographic statistics can be seen in Figure 8, Appendix D. Our annotators are predominantly native speakers, with around 89% residing in the respective country for over 16 years. The age group distribution shows a significant concentration in the 18-30 age bracket, with about one-third female representation. Overall, the demographic profile highlights diversity in terms of age, with high levels of cultural familiarity and language proficiency.\\n\\nTable 1: Categories in our Dataset. To save space in some of our results, we might refer to them by shorthand version in brackets.\\n\\n| Category                      |\\n|-------------------------------|\\n| 1. Vehicles and Transportation (Vehicles) |\\n| 2. Cooking and Food (Food)    |\\n| 3. People and Everyday Life (People) |\\n| 4. Sports and Recreation (Sports) |\\n| 5. Plants and Animals (Plants & Animals) |\\n| 6. Objects, Materials, and Clothing (Objects) |\\n| 7. Brands and Products (Brands) |\\n| 8. Geography, Buildings, and Landmarks (Geography) |\\n| 9. Tradition, Art, and History (Tradition) |\\n| 10. Public Figure and Pop-Culture (Pop Culture) |\\n\\nCategories\\n\\nFor the categorization of questions of our CVQA dataset, we incorporate 10 diverse categories to ensure a culturally-comprehensive representative set of visual questions, which are shown in Table 1. We mainly adopt the categorization from the OK-VQA dataset [33], with some modifications to fit the theme of our project. Specifically, the categories from the OK-VQA dataset used in our CVQA dataset are 1) to 7). We split the original category of Geography, History, Language and Culture into 2 separate categories of 8) and 9). In addition, we added a new category of 10) considering the effect that cultural icons and media have on everyday life.\\n\\n2.2. Annotation Process\\n\\nWe developed concise annotation guidelines (in English) that are suitable for all Country-Language subset teams. Here we provide an overview of the key steps that annotators followed during the dataset creation process. The full guidelines are provided in Appendix A.\\n\\nImage Selection and Preparation\\n\\nFor each Country-Language pair, annotators were instructed to select images that depict diverse cultural aspects pertinent to their cultural backgrounds among one of the categories.\"}"}
{"id": "E18kRXTGmV", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Statistics of the CVQA Benchmark\\n\\nWe did not enforce balance across categories considering the different variations of cultural knowledge. We strongly recommend that annotators use their own personal images to avoid accidental data leakage from existing online sources. However, we noted that this request was not always possible, since some images are extremely hard to come by (e.g., photos of public figures or landmarks that are far from the annotator\u2019s location). Therefore, we also allowed them to use images from our pre-defined list of open-use licensing sources.\\n\\nFor self-made images, we asked the annotators whether they were willing to make the image available for commercial or research purposes. For images from existing online sources, we applied the original license. We requested annotators to avoid using sensitive images that would perpetuate stereotypes. In addition, the annotators were also requested to anonymize faces that were not public figures or fictional characters, as well as text that could reveal the answer to the accompanying questions. We also post-processed all images to remove all metadata such as geo-location, device type, and so on.\\n\\nQuestion Creation\\n\\nThe questions associated with each image had to be culturally relevant and formulated such that they would require the context of the image in order to be answerable. A maximum of three question-answer pairs could be provided for each image. Each question was accompanied by one correct answer and three distractors that were reasonably plausible, yet incorrect, thus forming a multiple-choice format.\\n\\nWhile we follow the existing VQA benchmarks in terms of using a multiple-choice format, we are also aware that multiple-choice has some flaws when used to measure a model\u2019s performance [41]. Hence, we made sure that CVQA is also convertible into free-text open-ended QA, by instructing the annotators to ensure that the question would be answerable even without the accompanying multiple choices (i.e., not through a deductive method). Moreover, to accommodate the multilingual aspect of the benchmark, each question-answer pair was created in the local language and manually translated into English.\\n\\nAnnotators were advised to create questions that promoted an understanding and appreciation of different cultures without perpetuating stereotypes. Typical questions ranged from simple identification queries (e.g., \u201cWhat is the name of this food?\u201d) to more complex ones involving multi-hop reasoning or local common-sense knowledge (e.g., \u201cWhat is the color of the t-shirt the youngest member of this group is wearing?\u201d).\\n\\nAnnotation Examples and Training\\n\\nThe annotation guidelines provided multiple examples of well-formulated questions and answers to help guide annotator efforts (See Appendix A). These examples helped clarify the level of specificity and cultural relevance expected in the annotations.\\n\\n[3] common.wikimedia.org, Flickr, GapMinder, Unsplash, Pixabay\"}"}
{"id": "E18kRXTGmV", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We provided a tutorial to annotators on how to edit and blur sensitive information in the images. To confirm understanding, we spot-checked the annotators' collected data throughout the annotation period and informed them if some of their data did not follow the guidelines.\\n\\nValidation\\n\\nThe last step in the CVQA data creation was the validation process. Each entry was validated by another annotator of the same Country-Language pair. The validators were instructed to ensure that each question followed the guidelines. Based on our spot-checking, common mistakes that we encouraged the validators to check were typos and grammatical mistakes, non-cultural questions, questions that could be answered without the image, as well as incorrectly-sourced images. More information on the annotation platform is provided in Appendix B.\\n\\n2.3 Data Statistics\\n\\nTo ensure sufficient question variation, we set the minimum number of questions to be included in CVQA to be at least 200 questions per Country-Language subset. In the end, we gathered 10,374 total questions across all subsets. Some statistics of our collected data are shown in Table 2. Our CVQA covers a diverse set of languages and locations spread across the globe. We also capture languages written in various scripts. While Latin is the dominant script (used in 22 Country-Language pairs), the remaining scripts are diverse; covering Arabic, Amharic, Bengali, Chinese, Cyrillic, Devanagari, Hangul, Japanese, Perso-Arabic, Sinhalese, Tamil, and Telugu. The Country-Language pairs and corresponding scripts are shown in Appendix E. CVQA covers several less commonly studied languages and regions, such as Ireland-Irish, Indonesia-Minangkabau, Indonesia-Javanese, France-Breton, Nigeria-Igbo and Mongolia-Mongolian.\\n\\nQuestion distribution across the subset and categories are shown in Figure 2. Whether the image is coming from an external or personal source varies depending on the subset. We also note that the category with the most personal images is Cooking and Food, which we assume is due to the ease of obtaining such images. In contrast, the category with the least amount of personal images is Public Figures and Pop Culture, as it is less likely for people to have personal photos under this category.\\n\\n| Table 2: CVQA Data Statistics |\\n|-------------------------------|\\n| No. of images  | 5,239 |\\n| No. of questions | 10,374 |\\n| No. of countries  | 30 |\\n| No. of languages | 31 |\\n| No. of country-language pairs | 39 |\\n| Avg. questions per image | 1.98 |\\n| Avg. words per question | 7.6 |\\n| Avg. words per option | 1.80 |\\n\\nTo investigate the question variations, we categorize each question into question types of \u201cwhat\u201d, \u201chow\u201d, \u201cwhy\u201d, \u201cwhere\u201d, \u201cwho\u201d, and \u201cwhich\u201d questions. We categorize the questions by simple string-matching performed on the English questions. While not perfect, we argue that this method should be able to capture the trend of the questions. As shown in Figure 2, the majority of the questions fall into \u201cwhat\u201d questions. Question distribution across different Country-Language pairs varies, with an interesting finding that India-Bengali has a lot of \u201chow\u201d questions. Across categories, perhaps unsurprisingly, the Geography and Landmark category has noticeably more \u201cwhere\u201d and \u201cwhich\u201d (e.g., in which city) questions, whereas the Public Figure and Pop Culture category has more \u201cwho\u201d questions. By looking at the most frequently used words (Figure 7) across each category, we can see the general theme of the types of questions being asked. For example, questions in the Cooking and Food category often enquire about dish names, ingredients, or tastes.\\n\\n3 Experimental Setup\\n\\nModels.\\n\\nTo evaluate performance on our CVQA benchmark, we select a range of multimodal vision-language models with multilingual and monolingual English-only capabilities. For monolingual English-only models, we test CLIP [40], a contrastive-learning-based model, trained with approximately 400 million images and English-only text pairs from the web, where we use its vit-large-patch14-336 version. We also use InstructBLIP(4.1B) [8], an English-only instruction-aware vision model based on BLIP-2 [24], trained with 13 held-in datasets covering different tasks in English. For multilingual models, we evaluate LLaVA-1.5 (7B) [22] based on Llama-2 [46], and mBLIP [12] a BLIP-2 based model that covers 96 languages (where we evaluate two model variations, mBLIP mT0-XL (4.9B) and mBLIP BLOOMZ (8.3B)). Lastly, we employ M-CLIP [5] a multilingual vision-language model.\"}"}
{"id": "E18kRXTGmV", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"gual CLIP-based model that supports 68 languages, where we use its XLM-Roberta-Large-Vit-B-32 version. We also evaluate the most advanced closed-source MLLMs, such as GPT-4o [36] and Gemini-1.5-Flash [45].\\n\\nEvaluation Framework.\\nWe perform a zero-shot evaluation with two types of prompts, as follows: a location-aware prompt, which specifies the country, the question, and the options, (e.g., \u201cLocation: {country}. Question: {question} Options: {options} Short Answer:\u201d); and a location-agnostic prompt, which follows the same template but does not specify the country in the prompt (e.g., \u201cQuestion: {question} Options: {options} Short Answer:\u201d). Additionally, due to the multilingual nature of CVQA, for each prompt, we evaluate using the English-only and local language question-option pairs. For the generative-based models, LLaVA, mBLIP and InstructBLIP, the image and the prompts are used as the input. The models then produce output probabilities and we treat the highest probability for the options (A,B,C,D) as the prediction (following MMLU [16]). On the other hand, for embedding-based models like CLIP and M-CLIP, we use the embedding-level similarity between the image and the combination of question and each answer candidate texts (Question+Option-1,...,Question+Option-4) to select the one with the highest similarity as the correct answer. We use accuracy to measure the performance, following the existing multiple-choice VQA tasks [2, 55].\\n\\n4 Results\\nIn this section, we discuss the performance of existing MLLMs on the CVQA benchmark.\\n\\nTable 3: Average performance of MLLMs on our CVQA dataset with English prompts (EN) and local language prompts (LOC).\\n\\n| Model          | EN   | LOC  | EN   | LOC  |\\n|----------------|------|------|------|------|\\n| LLaVA-1.5-7B   | 49.6 | 35.5 | 38.0 | 33.7 |\\n| M-CLIP         | 33.7 | 30.6 | 31.3 | 30.9 |\\n| CLIP           | 39.3 | 32.7 | 49.0 | 31.9 |\\n| mBLIP-mT0      |      |      | 39.3 | 32.7 |\\n| mBLIP-BLOOMZ   |      |      | 39.3 | 32.7 |\\n| InstructBLIP   |      |      | 39.3 | 32.7 |\\n| Gemini-1.5-Flash|      |      | 39.3 | 32.7 |\\n| GPT-4o         |      |      | 39.3 | 32.7 |\\n\\nTable 4: LLaVA-1.5-7B and InstructBLIP results on various VQA datasets, where the results on the other datasets are taken from Liu et al. [26].\\n\\n| Model          | VQAv2 | GQA  | VizWiz | SciQA-IMG | TextVQA |\\n|----------------|-------|------|--------|-----------|---------|\\n| LLaVA-1.5-7B   | 78.5  | 62.0 | 50.0   | 66.8      | 58.2    |\\n| InstructBLIP   |       |      | 34.5   | 60.5      | 50.1    |\\n\\nMain Results\\nThe overall performance on our CVQA dataset of various open and closed-source MLLMs are shown in Table 3. Among open models, LLaVA-1.5-7B achieves the best performance, but still significantly lagging behind closed models by more than 10%. However, Table 4 shows that LLaVA-1.5-7B indeed achieves better performance on other established English VQA benchmarks, highlighting that culturally-specific questions that we collect in CVQA are challenging even for the best-performing open model (LLaVA-1.5-7B). The performance is even worse when the question is asked in local languages, emphasizing the models\u2019 lower capability in handling non-English prompts.\\n\\nThe experimental results also highlight a substantial performance gap between open and closed-source MLLMs. Closed models like GPT-4o and Gemini-1.5-Flash demonstrate superior performance, with GPT-4o achieving the highest accuracy in both English (75.4%) and local language (74.3%) prompts. In contrast, open models like InstructBLIP and mBLIP-mT0 exhibit lower performance, particularly in local language prompts, indicating a need for more diverse training data and refined fine-tuning processes. While proprietary models show superior performance, it is hard to fully explain why, due to their closed nature. Additionally, their results are not reproducible. Therefore, we use open models in the rest of our experiments.\\n\\nPerformance per Country-Language.\\nTo see the capability of MLLMs in solving questions for each country and language, we report accuracy performance for Country-Language pairs in Figure 3. From this, we observe that all models struggle with questions in local languages, demonstrating the challenges for current MLLMs. In other words, across all models, their performance drops in local language questions compared to their performance in English questions. For instance, in the...\"}"}
{"id": "E18kRXTGmV", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Using English-only question-option pairs\\n\\nUsing local-language question-option pairs\\n\\nFigure 3: Model performance per Country-Language pair. The blue lines indicate separation by continent. All models show similar behaviour in the majority of cases, despite having different sizes.\\n\\nCase of Brazil-Portuguese, LLaV A-1.5-7B achieved a score of 60.73% for English and 51.16% for Portuguese. Moreover, in Mongolia-Mongolian, all models struggled, with LLaV A-1.5-7B reaching only 40% for English and 27.62% for Mongolian, suggesting challenges in less resource-rich language environments. It is worth noting that these multilingual MLLMs do not originally support some of the languages, which also explains the significant performance drop for those languages. In contrast, in languages that are more frequently studied in NLP and have more abundant training resources, the performance gap between English and local languages, such as Spanish, tends to be smaller [3].\\n\\nPerformance Across Categories.\\n\\nWe show the breakdown performances of models per category in Table 5, where the categories themselves are described in Section 2.1. Note that the category People and Everyday Life consistently achieves the highest accuracy across most models, with InstructBLIP obtaining 59.8% in English prompts. This can be possibly attributed to the extensive training data available for everyday human activity and interaction, which widely existed in many visual-related datasets. Conversely, the Cooking & Food and Pop Culture categories exhibit lower accuracy across models, especially in local language prompts. This demonstrates that the high diversity in food and pop culture across different cultures poses a great challenge for the generalization of MLLMs.\\n\\nTable 5: Accuracy of models across categories. Per category, the best performing models on English (EN) and local language (LOC) question-option pairs are bolded and underlined, respectively.\\n\\n| Categories               | LLaV A-1.5-7B | M-CLIP | CLIP | mBLIP-mT0 | mBLIP-BLOOMZ | InstructBLIP |\\n|-------------------------|---------------|--------|------|-----------|--------------|--------------|\\n| Brands                  | 49.9          | 37.2   | 35.7 | 36.6      | 29.7         | 40.5         |\\n| Food                    | 45.4          | 34.5   | 29.1 | 39.2      | 30.4         | 27.6         |\\n| Geography               | 47.1          | 37.1   | 34.2 | 41.8      | 31.9         | 31.6         |\\n| Objects                 | 51.8          | 39.4   | 34.5 | 39.7      | 25.4         | 43.1         |\\n| People                  | 58.9          | 45.0   | 37.8 | 46.8      | 30.9         | 46.3         |\\n| Plants & Animals        | 55.7          | 43.7   | 32.0 | 48.0      | 27.2         | 46.0         |\\n| Pop Culture             | 44.5          | 33.7   | 31.5 | 46.1      | 28.8         | 29.9         |\\n| Sports                  | 50.7          | 39.3   | 33.3 | 43.5      | 32.4         | 31.4         |\\n| Tradition               | 50.4          | 37.0   | 35.2 | 41.9      | 32.2         | 39.0         |\\n| Vehicles                | 50.6          | 39.5   | 41.1 | 44.6      | 30.5         | 42.0         |\\n\\nImpact of External Image Source.\\n\\nThe performance of various models on self-made versus web images is shown in Table 6. One of the interesting findings is the performance variability across 7\"}"}
{"id": "E18kRXTGmV", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Accuracy of different models divided by image source\\n\\n| Image Source | LLaV | A-1.5-7B | M-CLIP | CLIP | mBLIP-mT0 | mBLIP-BLOOMZ | InstructBLIP |\\n|--------------|------|----------|--------|------|-----------|--------------|--------------|\\n| Self-made    | 48.8 | 34.2     | 38.1   | 34.3 | 41.2      | 30.1         | 31.5         |\\n| Web Image    | 49.7 | 37.4     | 37.4   | 33.3 | 43.1      | 31.8         | 31.9         |\\n\\nFor self-made images, the accuracy of some models such as LLaV A-1.5-7B and CLIP tends to be lower compared to web images. For instance, LLaV A-1.5-7B achieves a 48.8% accuracy in English prompts on self-made images but slightly higher at 49.7% on web images. CLIP shows an accuracy of 43.1% in English prompts on web images compared to 41.2% on self-made images. While this trend is not consistent across the other models, the results still indicate that web images might be more representative of the data these models (such as LLaV A-1.5-7B and CLIP) were trained on, leading to better performance.\\n\\nTable 7: Location-aware and location-agnostic results\\n\\n| Prompt type   | LLaV | A-1.5-7B | M-CLIP | CLIP | mBLIP-mT0 | mBLIP-BLOOMZ | InstructBLIP |\\n|---------------|------|----------|--------|------|-----------|--------------|--------------|\\n| Location-aware| 49.6 | 35.5     | 38.0   | 33.7 | 42.7      | 30.6         | 31.3         |\\n| Location-agnostic | 48.3 | 34.7     | 38.1   | 33.8 | 43.8      | 30.8         | 34.1         |\\n\\nLocation-Aware vs Location-Agnostic Prompt.\\n\\nThe performance of the various models on location-aware versus location-agnostic prompts is shown in Table 7. While the inclusion of location information has a varied impact on different models, the overall difference between both prompt options is marginal, suggesting no significant effect of including location information on MLLMs.\\n\\nPerformance without Multiple Choice Options.\\n\\nMost of the evaluations we conduct on CVQA are under a multiple-choice setting. However, the multiple-choice setting is often brittle towards option ordering [38, 54], and not very natural with respect to real-world scenarios [30]. In this paragraph, we explore the model's performance on CVQA in an open-ended QA setting. To evaluate in this setting, we prompt the models without giving them the options (e.g., \u201cIn which city is this monument located?\u201d). Then, the answer is selected by choosing the model\u2019s highest probability of generating the full answer phrase of one of the options [11] (e.g., Jakarta, Bandung, Bali, Surabaya). This way, it is robust towards ordering unlike predicting the answer letter (e.g., A), while also not giving the model multiple-choice options that can be indirectly used for deductive reasoning. Our result shows that LLaV A-1.5-7B achieved a noticeable performance drop when prompted without multiple choice, from 49.6% to just 30% average performance. This notes that in a more practical scenario, these models might be even more unreliable in cultural understanding.\\n\\n5 Limitations\\n\\nOur new benchmark dataset represents a diverse worldview through the inclusion of different languages and regions not covered in previous datasets. But we acknowledge that even CVQA is not comprehensive, as it covers only a fraction of the world\u2019s languages and regions. CVQA also lacks an English-centric baseline, which could arguably provide an interesting comparison with the rest of the regions. Additionally, our data scale prevents using CVQA to train new models, limiting its use for benchmarking purposes only.\\n\\nWe note that each region has different characteristics of questions and difficulty\u2014some regions are more likely to provide simpler, identity \u201cwhat is\u201d questions, whereas other regions might use questions that require deeper cultural knowledge. Therefore, comparing performance across languages/countries might not always be fair.\\n\\nCulture is hard to define, and our CVQA ultimately serves only as a proxy to benchmark the model\u2019s understanding of culture through local common knowledge. However, this by no means captures all cultural nuances [1]. Additionally, our location granularity captures country-level cultural knowledge. However, it might be interesting to capture cultural awareness at a more granular level, such as city-level, since each city might have variations in cultural common knowledge. Similarly, other demographic factors such as age might play a role in common knowledge.\"}"}
{"id": "E18kRXTGmV", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section we discussed the following aspects: 1) the fact that this dataset cannot be considered as a comprehensive representation of the world languages and regions; 2) the different levels of question complexity; 3) a bounded definition of culture. While these limitations might be relevant, we consider them as plausible lines for future work and outside the scope of this initial effort.\\n\\n6 Related Work\\n\\nSubstantial progress has been made in recent years on both datasets and methodologies for VQA [42, 23]. Since the introduction of early open-ended VQA datasets [2, 13], various formats like multiple-choice [49, 28], span extraction [34], and free-text generation [27] have been developed. Among these, multiple-choice datasets [28, 29, 52] are the most commonly used, likely due to their simplicity in evaluation and comparison. The development of these datasets has significantly accelerated research progress, serving as both training data and testbeds, especially the recently introduced ScienceQA [28] and MathVista [29] designed for evaluating MLLMs. The evolution of VQA methodologies has been revolutionary, transitioning from statistical machine learning [31] to neural-based methods [32, 17, 40], and advanced MLLMs [27, 36, 45] trained on massive multimodal data. Early VQA systems often required supervised learning and were limited to specific domains, but recent models like CLIP [40], LLaVA [27], and GPT-4V [36] are capable of zero-shot or few-shot learning, demonstrating strong performance. Despite this progress, significant limitations remain. Most VQA datasets focus primarily on English and a few major world languages [28, 29, 51], leading to language bias and under-representation of many languages and cultures. Additionally, the images in these datasets predominantly reflect Western scenes and styles, lacking the diversity needed to represent real-world scenarios across different cultures [7].\\n\\nSome efforts have been made to create multilingual VQA datasets, such as FM-IQA [10], MCVQA [14], xGQA [39], MaXM [6], MTVQA [44], and MaRVL [25]. However, these datasets are still limited in terms of the number of languages and the cultural diversity of the images and questions, or being a translation of existing English data. On the other hand, there have been initiatives to create culturally-diverse datasets and benchmarks under text-only modality [35, 19, 48, 18, 9, 47, 21]. Our proposed benchmark aims to fill the gap that covers both textual and visual modality by creating a large-scale, culturally-and-linguistically diverse dataset that will enable the development of more inclusive and robust VQA models.\\n\\n7 Conclusion\\n\\nWe proposed CVQA, a novel, human-written visual QA benchmark dataset that captures cultural nuances across a diverse set of languages and locations. CVQA encompasses 10 question categories, with each question written in both English and the native language. This allowed us to benchmark both multilingual visual models and English-only models. We provided insights into our dataset\u2019s question types and commonly used terms for each category.\\n\\nWe then performed benchmarks on various visual models, including both multilingual and English-only models. Our benchmark demonstrated that CVQA presented challenges for open-source models. These models generally performed worse when queried in local languages compared to English, indicating poorer performance in handling multilingual queries. The performance is also considerably lower when we do not provide the multiple choice setting, which is a more realistic use case for this technology. We hope that publishing CVQA encourages the AI community to pay more attention to non-English-centric models and benchmarking, thereby advancing progress in multilingual, multimodal research.\\n\\nAcknowledgments\\n\\nWe thank Ananjay Goel, Aditya Rachman Putra, Radityo Eko Prasojo, Amr Keleg, and Mostafa Awad for the contributions in creating the dataset. Tiago Torrent was partially funded by CNPq Research Productivity Grant number 315749/2021-0. Luis Fernando D\u2019Haro, Mario Rodr\u00edguez-Cantelar and Marcos Estecha-Garitagoitia were supported by the European Commission through Project ASTOUND (101071191 \u2014 HORIZON-EIC-2021-PATHFINDERCHALLENGES-01), and by project BEWORD (PID2021-126061OB-C43) funded by MCIN/AEI/10.13039/501100011033.\"}"}
{"id": "E18kRXTGmV", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and, as appropriate, by \u201cERDF A way of making Europe\u201d, by the \u201cEuropean Union\u201d. We also thank the anonymous reviewers for their valuable feedback and suggestions that helped improve this paper.\\n\\nReferences\\n\\n[1] M. F. Adilazuarda, S. Mukherjee, P. Lavania, S. Singh, A. Dwivedi, A. F. Aji, J. O\u2019Neill, A. Modi, and M. Choudhury. Towards measuring and modeling \u201cculture\u201d in LLMs: A survey. arXiv preprint arXiv:2403.15412, 2024.\\n\\n[2] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh. VQA: Visual Question Answering. In Proceedings of the International Conference on Computer Vision (ICCV), pages 2425\u20132433, 2015.\\n\\n[3] Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia, Z. Ji, T. Yu, W. Chung, et al. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023, 2023.\\n\\n[4] S. Cahyawijaya, H. Lovenia, A. F. Aji, G. Winata, B. Wilie, F. Koto, R. Mahendra, C. Wibisono, A. Romadhony, K. Vincentio, J. Santoso, D. Moeljadi, C. Wirawan, F. Hudi, M. S. Wicaksono, I. Parmonangan, I. Alfina, I. F. Putra, S. Rahmadani, Y. Oenang, A. Septiandri, J. Jaya, K. Dhole, A. Suryani, R. A. Putri, D. Su, K. Stevens, M. N. Nityasya, M. Adilazuarda, R. Hadiwijaya, R. Diandaru, T. Yu, V. Ghifari, W. Dai, Y. Xu, D. Damapuspita, H. Wibowo, C. Tho, I. Karo Karo, T. Fatyanosa, Z. Ji, G. Neubig, T. Baldwin, S. Ruder, P. Fung, H. Sujaini, S. Sakti, and A. Purwarianti. NusaCrowd: Open source initiative for Indonesian NLP resources. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 13745\u201313818, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.868. URL https://aclanthology.org/2023.findings-acl.868.\\n\\n[5] F. Carlsson, P. Eisen, F. Rekathati, and M. Sahlgren. Cross-lingual and multilingual CLIP. In Proceedings of the 13th Language Resources and Evaluation Conference (LREC), pages 6848\u20136854, 2022.\\n\\n[6] S. Changpinyo, L. Xue, M. Yarom, A. Thapliyal, I. Szpektor, J. Amelot, X. Chen, and R. Soricut. MaXM: Towards multilingual visual question answering. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2667\u20132682, 2023.\\n\\n[7] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Doll\u00e1r, and C. L. Zitnick. Microsoft COCO captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.\\n\\n[8] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Proceedings of NeurIPS, 2023.\\n\\n[9] A. Dwivedi, P. Lavania, and A. Modi. Eticor: Corpus for analyzing llms for etiquettes. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6921\u20136931, 2023.\\n\\n[10] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu. Are you talking to a machine? dataset and methods for multilingual image question answering. In Proceedings of NeurIPS, 2015.\\n\\n[11] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell, N. Muenighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation, 2021. URL https://doi.org/10.5281/zenodo.5371628.\\n\\n[12] G. Geigle, A. Jain, R. Timofte, and G. Glava\u0161. mBLIP: Efficient bootstrapping of multilingual vision-LLMs. arXiv preprint arXiv:2307.06930, 2023.\\n\\n[13] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6904\u20136913, 2017.\\n\\n[14] D. Gupta, P. Lenka, A. Ekbal, and P. Bhattacharyya. A unified framework for multilingual and code-mixed visual question answering. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 900\u2013913, 2020.\\n\\n[15] D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3608\u20133617, 2018.\"}"}
{"id": "E18kRXTGmV", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.\\n\\nD. A. Hudson and C. D. Manning. GQA: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6700\u20136709, 2019.\\n\\nA. Jha, A. M. Davani, C. K. Reddy, S. Dave, V. Prabhakaran, and S. Dev. Seegull: A stereotype benchmark with broad geo-cultural coverage leveraging generative models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9851\u20139870, 2023.\\n\\nA. Kabra, E. Liu, S. Khanuja, A. F. Aji, G. Winata, S. Cahyawijaya, A. Aremu, P. Ogayo, and G. Neubig. Multi-lingual and multi-cultural figurative language understanding. In Findings of the Association for Computational Linguistics: ACL 2023, pages 8269\u20138284, 2023.\\n\\nA. Kunchukuttan, D. Kakwani, S. Golla, G. N. C., A. Bhattacharyya, M. M. Khapra, and P. Kumar. Ai4bharat-indicnlp corpus: Monolingual corpora and word embeddings for indic languages, 2020.\\n\\nW. Q. Leong, J. G. Ngui, Y. Susanto, H. Rengarajan, K. Sarveswaran, and W. C. Tjhi. Bhasa: A holistic southeast asian linguistic and cultural evaluation suite for large language models, 2023.\\n\\nC. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao. LLaV A-Med: Training a large language-and-vision assistant for biomedicine in one day. In Proceedings of NeurIPS, 2023.\\n\\nC. Li, Z. Gan, Z. Yang, J. Yang, L. Li, L. Wang, and J. Gao. Multimodal foundation models: From specialists to general-purpose assistants. Foundations and Trends\u00ae in Computer Graphics and Vision, 16(1-2):1\u2013214, 2024.\\n\\nJ. Li, D. Li, S. Savarese, and S. Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the 40th International Conference on Machine Learning (ICML), pages 19730\u201319742, 2023.\\n\\nF. Liu, E. Bugliarello, E. M. Ponti, S. Reddy, N. Collier, and D. Elliott. Visually grounded reasoning across languages and cultures. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10467\u201310485, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.818. URL https://aclanthology.org/2021.emnlp-main.818.\\n\\nH. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. In Proceedings of NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023.\\n\\nH. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. In Proceedings of NeurIPS, 2023.\\n\\nP. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In Proceedings of NeurIPS, 2022.\\n\\nP. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. MathVista: Evaluating mathematical reasoning of foundation models in visual contexts. In Proceedings of the 12th International Conference on Learning Representations (ICLR), 2023.\\n\\nC. Lyu, M. Wu, and A. F. Aji. Beyond probabilities: Unveiling the misalignment in evaluating large language models. arXiv preprint arXiv:2402.13887, 2024.\\n\\nM. Malinowski and M. Fritz. A multi-world approach to question answering about real-world scenes based on uncertain input. In Proceedings of NeurIPS, 2014.\\n\\nM. Malinowski, M. Rohrbach, and M. Fritz. Ask your neurons: A neural-based approach to answering questions about images. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 1\u20139, 2015.\\n\\nK. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi. OK-VQA: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3195\u20133204, 2019.\\n\\nM. Mathew, V. Bagal, R. P. Tito, D. Karatzas, E. Valveny, and C. V. Jawahar. InfographicVQA. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 1697\u20131706, 2022.\\n\\nM. Malinowski and M. Fritz. A multi-world approach to question answering about real-world scenes based on uncertain input. In Proceedings of NeurIPS, 2014.\"}"}
{"id": "E18kRXTGmV", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Mukherjee, C. Raj, Z. Zhu, and A. Anastasopoulos. Global voices, local biases: Socio-cultural prejudices across languages. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15828\u201315845, 2023.\\n\\nOpenAI. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023.\\n\\nI. Orife, J. Kreutzer, B. Sibanda, D. Whitenack, K. Siminyu, L. Martinus, J. T. Ali, J. Abbott, V. Marivate, S. Kabongo, M. Meressa, E. Murhabazi, O. Ahia, E. van Biljon, A. Ramkilowan, A. Akinfaderin, A. \u00d6ktem, W. Akin, G. Kioko, K. Degila, H. Kamper, B. Dossou, C. Emezue, K. Ogueji, and A. Bashir. Masakhane \u2013 machine translation for Africa, 2020.\\n\\nP. Pezeshkpour and E. Hruschka. Large language models sensitivity to the order of options in multiple-choice questions. arXiv preprint arXiv:2308.11483, 2023.\\n\\nJ. Pfeiffer, G. Geigle, A. Kamath, J.-M. Steitz, S. Roth, I. Vulic, and I. Gurevych. xGQA: Cross-lingual visual question answering. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2497\u20132511, 2022.\\n\\nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning (ICML), pages 8748\u20138763, 2021.\\n\\nJ. Robinson and D. Wingate. Leveraging large language models for multiple choice question answering. In Proceedings of the 11th International Conference on Learning Representations (ICLR), 2023.\\n\\nH. Sharma and A. S. Jalal. A survey of methods, datasets and evaluation metrics for visual question answering. Image and Vision Computing, 116:104327, 2021.\\n\\nA. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards VQA models that can read. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8317\u20138326, 2019.\\n\\nJ. Tang, Q. Liu, Y. Ye, J. Lu, S. Wei, C. Lin, W. Li, M. F. F. B. Mahmood, H. Feng, Z. Zhao, Y. Wang, Y. Liu, H. Liu, X. Bai, and C. Huang. MTVQA: Benchmarking multilingual text-centric visual question answering. arXiv preprint arXiv:2405.11985, 2024.\\n\\nG. Team. Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\\n\\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\n\\nB. Wang, Z. Liu, X. Huang, F. Jiao, Y. Ding, A. Aw, and N. F. Chen. Seaeval for multilingual foundation models: From cross-lingual alignment to cultural reasoning, 2024.\\n\\nH. A. Wibowo, E. H. Fuadi, M. N. Nityasya, R. E. Prasojo, and A. F. Aji. Copal-id: Indonesian language reasoning with local culture and nuances. arXiv preprint arXiv:2311.01012, 2023.\\n\\nL. Xu, H. Huang, and J. Liu. SUTD-TrafficQA: A question answering benchmark and an efficient network for video reasoning over traffic events. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9878\u20139888, 2021.\\n\\nA. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid. Just ask: Learning to answer questions from millions of narrated videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1686\u20131697, 2021.\\n\\nW. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. MM-Vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023.\\n\\nX. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. arXiv preprint arXiv:2311.16502, 2023.\"}"}
{"id": "E18kRXTGmV", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su. GPT-4V(ision) is a generalist web agent, if grounded.\\n\\nC. Zheng, H. Zhou, F. Meng, J. Zhou, and M. Huang. Large language models are not robust multiple choice selectors. In Proceedings of the 12th International Conference on Learning Representations (ICLR), 2024.\\n\\nY. Zhu, O. Groth, M. Bernstein, and L. Fei-Fei. Visual7W: Grounded Question Answering in Images. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.\"}"}
{"id": "E18kRXTGmV", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [TODO] to [Yes], [No], or [N/A]. You are strongly encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description. For example:\\n\\n- Did you include the license to the code and datasets? [Yes] See Section A.\\n- Did you include the license to the code and datasets? [No] The code and the data are proprietary.\\n- Did you include the license to the code and datasets? [N/A]\\n\\nPlease do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? [Yes]\\n   (b) Did you describe the limitations of your work? [Yes] See Section 5.\\n   (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section 5.\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [Yes] See Section 3.\\n   (b) Did you include complete proofs of all theoretical results? [Yes] See Section 3.\\n\\n3. If you ran experiments (e.g., for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes]\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [N/A]\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A]\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes]\\n   (b) Did you mention the license of the assets? [Yes]\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\\n   (d) Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? [Yes]\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] See Appendix A and B\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [Yes] All the major participants are co-authors of this paper. Data collected by participants are anonymous and PIIs are removed. See Section 2.\\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] The project is voluntary and all collaborators have been notified of the mechanism of being co-authors before joining.\"}"}
{"id": "E18kRXTGmV", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multilingual Multimodal Visual Question Answering Benchmark: Annotation Guidelines\\n\\nIntroduction\\nThis document provides guidelines for annotating images and corresponding questions and answers in multiple languages to create a culturally diverse and linguistically comprehensive multimodal QA benchmark.\\n\\nObjective\\nTo build a benchmark that represents a wide range of cultures and languages, to measure potential bias in visual AI models.\\n\\nGuidelines for Contributors\\nEach region and language (e.g., Ecuador-Spanish) will be represented by at most 3 annotators, in which 1 will be the team lead. Each person is expected to provide at least 100 visual questions to be considered as a co-author. The team lead will still have to provide questions, the only difference is that the team lead is responsible to find and to organize more annotators and will manage to contact and brief that annotator, if needed.\\n\\nImage Selection:\\n\u25cb Contribute images that represent diverse cultural aspects that represent the specific cultural background you\u2019re contributing to. The image must fall into one of the categories below.\\n  \u25cb Pick one of the most relevant category (more later)\\n    \u25cb Images should be relevant to your culture/country.\\n    \u25cb Ensure that images are relevant to the questions being posed. In other words, the image is needed to answer the question.\\n    \u25cb If the image contains the answer\u2019s text, you can blur/crop the image so that the image does not contain the answer.\"}"}
{"id": "E18kRXTGmV", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Self/personal picture (highly preferable). You may ask your family/friend to donate their photos, if possible.\\n\\n2. We also accept external images from:\\n   - Flickr: https://www.flickr.com/explore (please make sure the associated license to the image is Creative Commons), this can be selected at the top left of Flickr (\u201cAny License\u201d).\\n   - WikimediaCommons: https://commons.wikimedia.org/wiki/Main_Page (here you do not need to select any license for the images),\\n   - Unsplash: https://unsplash.com/ (please make sure to search the image first and they select the license: Free).\\n   - Dollar Street: https://www.gapminder.org/dollar-street (here you do not need to select any license for the images), this webpage has images only from some countries, please make sure to select your country to find images if applicable.\\n\\nMore detailed instructions for each webpage are shown at the end of this document.\\n\\nIf you use an external image, you'll need to put the URL of the original image.\\n\\nThe image must be reasonable quality (not pixelated or blurry, can be understandable). You can upload images of any ratio as long as it is not too tall or wide (e.g.: don\u2019t submit panorama pictures).\\n\\nDo not show personally identifiable information (PII) such as faces, car plates, house addresses. Faces of public figures or fictional characters are ok. Also, please be sure to blur text in the image that will leak the answer. \\\"PicdeFacer\\\" can be used for blurring: https://picdefacer.com/en/.\\n\\nTutorial on using PicdeFacer is shown at the end of this document.\\n\\nQuestion and Answer Creation:\\n\\nAfter finding the image, you must now formulate 1-3 questions + answers from that image.\\n\\nSpecifically:\\n\\n- The question must be answerable only by looking at the image.\\n- Ensure that the questions are culturally relevant and specific to the image content.\\n- Provide answers that are concise, accurate, and directly related to the question.\\n- You will also need to provide 1 correct option and 3 other incorrect options (distractors).\\n- For the distractors, choose options that are relevant, not obvious wrong answers.\"}"}
