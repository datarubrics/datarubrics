{"id": "MU2495w47rz", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OpenXAI: Towards a Transparent Evaluation of Post hoc Model Explanations\\n\\nChirag Agarwal, Satyapriya Krishna, Eshika Saxena, Martin Pawelczyk, Nari Johnson, Isha Puri, Marinka Zitnik, and Himabindu Lakkaraju\\n\\n1 Harvard University\\n2 Adobe\\n3 University of T\u00fcbingen\\n4 Carnegie Mellon University\\n\\nAbstract\\n\\nWhile several types of post hoc explanation methods have been proposed in recent literature, there is very little work on systematically benchmarking these methods. Here, we introduce OpenXAI, a comprehensive and extensible open-source framework for evaluating and benchmarking post hoc explanation methods. OpenXAI comprises of the following key components: (i) a flexible synthetic data generator and a collection of diverse real-world datasets, pre-trained models, and state-of-the-art feature attribution methods, (ii) open-source implementations of twenty-two quantitative metrics for evaluating faithfulness, stability (robustness), and fairness of explanation methods, and (iii) the first ever public XAI leaderboards to readily compare several explanation methods across a wide variety of metrics, models, and datasets. OpenXAI is easily extensible, as users can readily evaluate custom explanation methods and incorporate them into our leaderboards. Overall, OpenXAI provides an automated end-to-end pipeline that not only simplifies and standardizes the evaluation of post hoc explanation methods, but also promotes transparency and reproducibility in benchmarking these methods. While the first release of OpenXAI supports only tabular datasets, the explanation methods and metrics that we consider are general enough to be applicable to other data modalities. OpenXAI datasets and data loaders, implementations of state-of-the-art explanation methods and evaluation metrics, as well as leaderboards are publicly available at https://open-xai.github.io/. OpenXAI will be regularly updated to incorporate text and image datasets, other new metrics and explanation methods, and welcomes inputs from the community.\\n\\n1 Introduction\\n\\nAs predictive models are increasingly deployed in critical domains (e.g., healthcare, law, and finance), there has been a growing emphasis on explaining the predictions of these models to decision makers (e.g. doctors, judges) so that they can understand the rationale behind model predictions, and determine if and when to rely on these predictions. To this end, various techniques have been proposed in recent literature to generate post hoc explanations of individual predictions made by complex ML models. Several of such local explanation methods output the influence of each of the features on the model\u2019s prediction, and are therefore referred to as local feature attribution methods. Due to their generality, feature attribution methods are increasingly being utilized to explain complex models in medicine, finance, law, and science [23, 34, 78]. Thus, it is critical to ensure that the explanations generated by these methods are reliable so that relevant stakeholders and decision makers are provided with credible information about the underlying models [6].\"}"}
{"id": "MU2495w47rz", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prior works have studied several notions of explanation reliability such as faithfulness (or fidelity) \\\\[81, 51, 33\\\\], stability (or robustness) \\\\[7, 4\\\\], and fairness \\\\[18, 9\\\\], and proposed metrics for quantifying these notions. Many of these works also demonstrated through small-scale experiments or qualitative analysis that certain explanation methods are not effective w.r.t. specific notions of reliability. For instance, Alvarez-Melis and Jaakkola \\\\[7\\\\] visualized the explanations generated by some of the popular gradient based explanation methods \\\\[67, 66, 70, 72\\\\] for MNIST images, and showed that they are not robust to small input perturbations. However, it is unclear if such findings generalize beyond the settings studied. More broadly, one of the biggest open questions which has far-reaching implications for the progress of explainable AI (XAI) research is: which explanation methods are effective w.r.t. what notions of reliability and under what conditions? \\\\[43\\\\]. A first step towards answering this question involves systematically benchmarking explanation methods in a reproducible and transparent manner. However, the increasing diversity of explanation methods, and the plethora of evaluation settings and metrics outlined in existing research without standardized open-source implementations make it rather challenging to carry out such benchmarking efforts.\\n\\nIn this work, we address the aforementioned challenges by introducing OpenXAI, a comprehensive and extensible open-source framework for systematically and efficiently benchmarking explanation methods in a transparent and reproducible fashion. More specifically, our work makes the following key contributions:\\n\\n1. We introduce the OpenXAI framework, an open-source ecosystem designed to support systematic, reproducible, and efficient evaluations of post hoc explanation methods. OpenXAI unifies the existing scattered repositories of datasets, models, and evaluation metrics, and provides a simple and easy-to-use API that enables researchers and practitioners to benchmark explanation methods using just a few lines of code (Section 2).\\n\\n2. Our OpenXAI framework currently provides open-source implementations and ready-to-use API interfaces for seven state-of-the-art feature attribution methods (LIME, SHAP, Vanilla Gradients, Gradient x Input, SmoothGrad, and Integrated Gradients), and twenty-two quantitative metrics to evaluate the faithfulness, stability, and fairness of feature attribution methods. In addition, it includes a comprehensive collection of seven real-world datasets spanning diverse real-world domains, and sixteen different pre-trained models. OpenXAI also introduces a novel and flexible synthetic data generator to synthesize datasets of varying sizes, complexity, and dimensionality which facilitate the construction of reliable ground truth explanations (Section 2).\\n\\n3. As part of our OpenXAI framework, we also develop the first-ever public XAI leaderboards (shown in Figure 1) to promote transparency, and to allow users to easily compare the performance of multiple explanation methods across a wide variety of synthetic and real-world datasets, evaluation metrics, and predictive models.\\n\\n4. OpenXAI framework is easily extensible i.e., researchers and practitioners can readily incorporate custom explanation methods, datasets, predictive models, and evaluation metrics into our framework and leaderboards (Section 2).\\n\\n5. Lastly, using our proposed OpenXAI framework, we perform rigorous empirical benchmarking of the aforementioned state-of-the-art feature attribution methods to determine which methods are effective w.r.t. what notions of reliability across a wide variety of datasets and predictive models (Section 3).\\n\\nOverall, our OpenXAI framework provides an end-to-end pipeline that unifies, simplifies, and standardizes several existing workflows to evaluate explanation methods. By enabling systematic and efficient evaluation and benchmarking of existing and new explanation methods, our OpenXAI framework can inform and accelerate new research in the emerging field of XAI. OpenXAI will be regularly updated and welcomes input from the community.\\n\\nRelated Work. Our work builds on the vast literature in explainable AI. Here, we discuss closely related works and their connections to our benchmark. A more detailed discussion of the related work is included in the Appendix.\\n\\nEvaluation Metrics for Post hoc Explanations: Prior research has studied several notions of explanation reliability, namely, faithfulness (or fidelity), stability (or robustness), and fairness \\\\[51, 81\\\\]. While the faithfulness notion captures how faithfully a given explanation captures the true behavior of the underlying model \\\\[81, 51, 33\\\\], stability ensures that explanations do not change...\"}"}
{"id": "MU2495w47rz", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"drastically with small perturbations to the input. The fairness notion, on the other hand, ensures that there are no group-based disparities in the faithfulness or stability of explanations.\\n\\nTo this end, prior works proposed various evaluation metrics to quantify the aforementioned notions. For instance, Petsiuk et al. measured the change in the probability of the predicted class when important features (as identified by an explanation) are deleted from or introduced into the data instance. A sharp change in the probability implies a high degree of explanation faithfulness. Alvarez-Melis and Jaakkola loosely quantified stability as the maximum change in the resulting explanations when small perturbations are made to a given instance. Dai et al. quantified unfairness of explanations as the difference between the faithfulness (or stability) metric values averaged over instances in the majority and the minority subgroups.\\n\\nXAI Libraries and Benchmarks: Prior works have introduced a few XAI libraries and benchmarks, the most popular among them being Captum, Quantus, XAI-Bench, and SHAP Benchmark. Below, we provide a brief description of each of these, and detail how our work differs from them.\\n\\nWhile the Captum library is an open-source library which provides implementations and APIs for various state-of-the-art explanation methods, its focus is not on evaluating and/or benchmarking these methods which is the main goal of our work. Quantus library provides implementations of certain evaluation metrics to measure the faithfulness and stability/robustness of explanation methods. However, it does not focus on benchmarking explanation methods or providing public dashboards to compare the performance of these methods. Furthermore, the stability/robustness measures supported by Quantus are somewhat outdated and have been superseded by recently proposed metrics. In addition, Quantus does not support any fairness metrics to evaluate disparities in the quality of explanations which is very important in real-world settings such as healthcare, criminal justice, and policy. In contrast, OpenXAI not only subsumes popular faithfulness and stability/robustness metrics supported by Quantus but also supports 19 new metrics to measure the faithfulness, stability/robustness, as well as the fairness of explanation methods. In addition, OpenXAI focuses on systematically benchmarking state-of-the-art explanation methods and providing public dashboards to readily compare these methods.\\n\\nSHAP benchmark only focuses on evaluating and comparing different variants of SHAP via certain faithfulness metrics which are similar to the Prediction Gap on Important (PGI) and Unimportant (PGU) feature perturbation metrics outlined in our work. Note that the SHAP benchmark does not include any stability/robustness or fairness metrics. In contrast, OpenXAI not only includes 20 new metrics to evaluate the stability/robustness and fairness of explanation methods but also benchmarks various other methods (e.g., LIME, Gradient-based methods).\\n\\nXAI-Bench constructed synthetic datasets with ground truth explanations to evaluate the faithfulness of a few explanation methods (e.g., LIME, SHAP, MAPLE). However, recent research argued that their evaluation is unreliable, and predictive models learned using their synthetic datasets may not adhere to the ground truth explanations. In addition, the aforementioned evaluation is rather limited in scope as synthetic datasets may not even be representative of real-world data. In contrast, our work not only proposes a novel synthetic data generator that addresses the shortcomings of the synthetic datasets constructed in XAI-Bench but also facilitates the evaluation and benchmarking of the faithfulness, stability, as well as the fairness of 7 state-of-the-art explanation methods on 7 real-world datasets with no ground truth explanations.\\n\\nIn summary, our work is significantly different from existing libraries and benchmarks, and makes the following key contributions:\\n\\n\u2022 We provide implementations and easy-to-use API interfaces for 22 metrics to evaluate the faithfulness, stability, and fairness of explanation methods. 18 out of the 22 state-of-the-art metrics included in OpenXAI have not been implemented in any prior libraries or benchmarks \u2013 e.g., faithfulness metrics such as Feature Agreement (FA), Rank Agreement (RA), Sign Agreement (SA), Signed Rank Agreement (SRA), Pairwise Rank Agreement (PRA), stability metrics such as Relative Representation Stability (RRS), Relative Output Stability (ROS), and all fairness metrics.\\n\\n\u2022 We also introduce a novel and flexible synthetic data generator to synthesize datasets of varying sizes, complexity, and dimensionality to facilitate the construction of reliable ground truth explanations in order to evaluate state-of-the-art explanation methods. Our synthetic data generator addresses the shortcomings of the prior synthetic benchmark.\"}"}
{"id": "MU2495w47rz", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bench) by generating synthetic datasets which encapsulate certain key properties, namely, unambiguously defined local neighborhoods, a clear description of feature importances in each local neighborhood, and feature independence. These properties, in turn, allow us to theoretically guarantee that any accurate model trained on our synthetic datasets will adhere to the ground truth explanations of the underlying data.\\n\\nWe perform rigorous empirical benchmarking of 7 state-of-the-art feature attribution methods using our OpenXAI framework to determine which methods are effective w.r.t. each of the 22 evaluation metrics across 8 real-world and synthetic datasets, and 16 different predictive models. Note that none of the previously proposed libraries or benchmarks carry out such exhaustive benchmarking efforts across such a wide variety of metrics, models, and datasets. We also introduce the first ever public XAI leaderboards with such a wide variety of explanation methods, metrics, models, and datasets, to promote transparency and showcase the results of our benchmarking efforts.\\n\\n2 Overview of OpenXAI Framework\\nOpenXAI provides a comprehensive programmatic environment with synthetic and real-world datasets, data processing functions, explainers, and evaluation metrics to rigorously and efficiently benchmark explanation methods. Below, we discuss each of these components in detail.\\n\\n1) Datasets and Predictive Models.\\n\\nThe current release of our OpenXAI framework includes a collection of eight different synthetic and real-world datasets. While synthetic datasets allow us to construct ground truth explanations which can then be used to evaluate explanations output by state-of-the-art methods, real-world datasets (where it is typically hard to construct ground truth explanations) help us benchmark these methods in a more realistic manner suitable for practical applications. We would like to note that OpenXAI includes datasets that are widely employed in XAI research to evaluate the efficacy of newly proposed methods and study the behavior of existing methods.\\n\\nSynthetic Datasets\\nWhile prior research proposed methods to generate synthetic datasets and corresponding ground truth explanations, they all suffer from a significant drawback as demonstrated by Faber et al. \u2014 there is no guarantee that the models trained on these datasets will adhere to the ground truth explanations of the underlying data. This, in turn, implies that evaluating post hoc explanations using the above ground truth explanations would be incorrect since post hoc explanations are supposed to reliably explain the behavior of the underlying model, and not that of the underlying data. To illustrate, let us consider the case where we use aforementioned methods to construct a synthetic dataset with features A, B, C, and D such that the ground truth labels only depend on features A and B i.e., the ground truth explanation of the underlying data indicates that features A and B are most important. If we train a model on this data and if features A and B are correlated with C and D respectively, then the resulting model may base its predictions on C and D (and not A and B) and still be very accurate. If a post hoc explanation of this model then (correctly) indicates that the most important features of the model are C and D, this explanation may be deemed incorrect if we compare it against the ground truth explanation of the underlying data. This problem further exacerbates as we increase the complexity of the ground truth labeling function. To address the aforementioned challenges, we develop a novel synthetic data generation mechanism, SynthGauss, which encapsulates three key properties, namely, feature independence, unambiguously-defined local neighborhoods, and a clear description of feature influence in each local neighborhood. Intuitively, this approach generates K well-separated clusters where points in each cluster k 2 {1, 2, \u00b7\u00b7\u00b7 , K} are sampled from a Gaussian distribution N (\u00b5k, \u2303k) where uk 2 Rd is the mean and \u2303k 2 Rd\u21e5d is the covariance matrix. While this parameterization is general enough to support the construction of synthetic datasets of K clusters with varying means and covariances, we set the means of all the clusters such that the intracluster distances are significantly smaller than the intercluster distances, and we set the covariance matrices of all the clusters to identity. This ensures that all the features are independent, and local neighborhoods (clusters) are unambiguously defined. We then generate ground truth labels for instances by first randomly sampling feature mask vectors mk 2 {0, 1}d (vectors comprising of 0s and 1s) for each cluster k. The vector mk determines which features influence the ground truth labeling process for instances in cluster k (a value of 1 indicates that the corresponding feature is influential). We then randomly sample feature weight vectors wk for each cluster k and for each feature in the dataset. These weight vectors determine the importance of each feature for each cluster. Finally, we generate the ground truth labels for each instance by randomly sampling a feature mask vector and then assigning the instance to the cluster with the highest weighted sum of feature masks.\"}"}
{"id": "MU2495w47rz", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"vectors $w$ which capture the relative importance of each of the features in the labeling process of instances in each cluster $k$. The ground truth labels of instances in each cluster $k$ are then computed as a function (e.g., sigmoid) of the feature values of individual instances, and the dot product of the corresponding cluster's feature mask vector and weight vector i.e., $m_k w_k$. Complete pseudocode and other details of this generation process are included in the Appendix. Note that $m_k$ corresponds to the ground truth explanation for all instances in cluster $k$. Since our generation process is designed to encapsulate feature independence, unambiguous definitions of local neighborhoods, and clear descriptions of feature influences, any accurate model trained on the resulting dataset will adhere to the ground truth explanations of the underlying data (See Theorem 1 in Appendix).\\n\\nReal-world Datasets: In the current release of OpenXAI, we include seven real-world datasets that are highly diverse in terms of several key properties. They comprise of data spanning multiple real-world domains (e.g., finance, lending, healthcare, and criminal justice), varying dataset sizes (e.g., small vs. large-scale), dimensionalities (e.g., low vs. high dimensional), class imbalance ratios, and feature types (e.g., continuous vs. discrete). We focus on tabular data in this release as such data is commonly encountered in real-world applications where explainability is critical [75], and has also been widely studied in XAI literature [51]. Table 1 provides a summary of the real-world datasets currently included in OpenXAI. See Section E.1 in the Appendix for detailed descriptions of individual datasets. While these real-world datasets are primarily drawn from prior research and existing repositories, OpenXAI provides comprehensive data loading and pre-processing capabilities to make these datasets XAI-ready (more details below). We also plan to expand our collection of real-world datasets in the next iteration. Adding a new dataset into our collection is as simple as uploading a .csv file or a .zip folder. Users can also submit requests to incorporate new datasets into the OpenXAI framework by filling a simple form and providing links to the datasets (See Appendix).\\n\\nTable 1: Summary of currently available datasets in OpenXAI.\\n\\n| Dataset | Size | # features | Feature types | Feature information | Balanced |\\n|---------|------|------------|---------------|---------------------|----------|\\n| Synthetic Data | 5,000 | 20 | continuous | synthetic | 3 |\\n| German Credit [22] | 1,000 | 20 | discrete, continuous | demographic, personal, financial | 7 |\\n| HELOC [25] | 9,871 | 23 | continuous | demographic, financial | 3 |\\n| COMPAS [36] | 18,876 | 7 | discrete, continuous | demographic, personal, criminal | 7 |\\n| Adult Income [79] | 48,842 | 13 | discrete, continuous | demographic, personal, education/employment, financial | 7 |\\n| Give Me Some Credit [27] | 102,209 | 10 | discrete, continuous | demographic, personal, financial | 7 |\\n| Pima-Indians Diabetes [71] | 768 | 9 | discrete, continuous | demographic, healthcare | 7 |\\n| Framingham heart study [1] | 4,240 | 16 | continuous | demographic, healthcare | 7 |\\n\\nData loaders and pre-trained models: OpenXAI provides a Dataloader class that can be used to load the aforementioned collection of synthetic and real-world datasets as well as any other custom datasets, and ensures that they are XAI-ready. More specifically, this class takes as input the name of an existing OpenXAI dataset or a new dataset (name of the .csv file), and outputs a train set which can then be used to train a predictive model, a test set which can be used to generate local explanations of the trained model, as well as any ground-truth explanations (if and when available). If the dataset already comes with pre-determined train and test splits, this class loads train and test sets from those pre-determined splits. Otherwise, it divides the entire dataset randomly into train (70%) and test (30%) sets. Users can also customize the percentages of train-test splits. The code snippet below shows how to import the Dataloader class and load an existing OpenXAI dataset.\\n\\n```python\\nfrom OpenXAI import Dataloader\\n\\nloader_train, loader_test = Dataloader.return_loaders(data_name='german', download=True)\\ninputs, labels = iter(loader_test).next()\\n```\"}"}
{"id": "MU2495w47rz", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We also pre-trained two classes of predictive models (e.g., deep neural networks of varying degrees of complexity, logistic regression models etc.) and incorporated them into the OpenXAI framework so that they can be readily used for benchmarking explanation methods. The code snippet below shows how to load OpenXAI's pre-trained models using our `LoadModel` class.\\n\\n```python\\nfrom OpenXAI import LoadModel\\nmodel = LoadModel(data_name='german', model='ann')\\n```\\n\\nAdding additional pre-trained models into the OpenXAI framework is as simple as uploading a file with details about model architecture and parameters in a specific template. Users can also submit requests to incorporate custom pre-trained models into the OpenXAI framework by filling a simple form and providing details about model architecture and parameters (See Appendix).\\n\\n2) Explainers. OpenXAI provides ready-to-use implementations of six state-of-the-art feature attribution methods, namely, LIME, SHAP, Vanilla Gradients, Gradient x Input, SmoothGrad, and Integrated Gradients. An implementation of a random baseline which randomly assigns importance values to each of the features, and returns these random assignments as explanations is also included. Our implementations of these methods build on other open-source libraries (e.g., Captum [42]) as well as their original implementations. While methods such as LIME and SHAP leverage perturbations of data instances and their corresponding model predictions to learn a local explanation model, they do not require access to the internals of the models or their gradients. On the other hand, Vanilla Gradients, Gradient x Input, SmoothGrad, and Integrated Gradients require access to the gradients of the underlying models but do not need to repeatedly query the models for their predictions (see Table 6 in Appendix for a brief summary of these methods). These differences influence the efficiency with which explanations can be generated by these methods. OpenXAI provides an abstract `Explainer` class which enables us to load existing explanation methods as well as integrate new explanation methods.\\n\\n```python\\nfrom OpenXAI import Explainer\\nexp_method = Explainer(method='LIME')\\nexplanations = exp_method.get_explanations(model, X=inputs, y=labels)\\n```\\n\\nAll the explanation methods included in OpenXAI are readily accessible through the `Explainer` class, and users just have to specify the method name in order to invoke the appropriate method and generate explanations as shown in the above code snippet. Users can easily incorporate their own custom explanation methods into the OpenXAI framework by extending the `Explainer` class and including the code for their methods in the `get_explanations` function (see template below) of this class. They can then submit a request to incorporate their custom methods into OpenXAI library by filling a form and providing the GitHub link to their code as well as a summary of their explanation method (See Appendix).\\n\\n3) Evaluation Metrics. OpenXAI provides implementations and ready-to-use APIs for a set of twenty-two quantitative metrics proposed by prior research to evaluate the faithfulness, stability, and fairness of explanation methods. OpenXAI is the first XAI benchmark to consider all the three aforementioned aspects of explanation reliability. More specifically, we include eight different metrics to measure explanation faithfulness (both with and without ground truth explanations) [43, 59], three different metrics to measure stability [4], and eleven different metrics to measure group-based disparities (unfairness) [18] in the values of the aforementioned faithfulness and stability metrics.\\n\\nThe metrics that we choose are drawn from the latest works in explainable AI literature. Below, we briefly describe these metrics. Detailed descriptions of all the metrics along with notation and equations are included in the Appendix.\\n\\na) Ground-truth Faithfulness: Krishna et al. [43] recently proposed six evaluation metrics to capture the similarity between the top-K or a select set of features of any two feature attribution-based explanations. We leverage these metrics to capture the similarity between the explanations output by state-of-the-art methods and the ground-truth explanations constructed using our synthetic data generation process. These metrics and their definitions are given as follows: i) Feature Agreement (FA) which computes the fraction of top-K features that are common between a given post hoc explanation and the corresponding ground truth explanation, ii) Rank Agreement (RA) metric which measures the fraction of top-K features that are not only common between a given post hoc explanation and the corresponding ground truth explanation, but also have the same position in the respective rank orders, iii) Sign Agreement (SA) metric which computes the fraction of top-K features that are common between a given post hoc explanation and the corresponding ground truth explanation, but with opposite signs.\"}"}
{"id": "MU2495w47rz", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"features that are not only common between a given post hoc explanation and the corresponding ground truth explanation, but also share the same sign (direction of contribution), iv) Signed Rank Agreement (SRA) metric which computes the fraction of top-K features that are not only common between a given post hoc explanation and the corresponding ground truth explanation, but also share the same feature attribution sign (direction of contribution) and position (rank) in both the explanations, v) Rank Correlation (RC) metric which computes Spearman's rank correlation coefficient to measure the agreement between feature rankings provided by a given post hoc explanation and the corresponding ground truth explanation, and vi) Pairwise Rank Agreement (PRA) metric which captures if the relative ordering of every pair of features is the same for a given post hoc explanation as well as the corresponding ground-truth explanation.\\n\\nb) Predictive Faithfulness: We leverage the metrics outlined by [59, 18] to measure the faithfulness of an explanation when no ground truth is available. This metric, referred to as Prediction Gap on Important feature perturbation (PGI), computes the difference in prediction probability that results from perturbing the features deemed as influential by a given post hoc explanation. Higher values on this metric imply greater explanation faithfulness. We also consider the converse of this metric, Prediction Gap on Unimportant feature perturbation (PGU), which perturbs the unimportant features and measures the change in prediction probability.\\n\\nc) Stability: We consider the metrics introduced by Alvarez-Melis and Jaakkola [7], Agarwal et al. [4] to measure how robust a given explanation is to small input perturbations. More specifically, we leverage the metrics Relative Input Stability (RIS), Relative Representation Stability (RRS), and Relative Output Stability (ROS) which measure the maximum change in explanation relative to changes in the inputs, model parameters, and output prediction probabilities respectively.\\n\\nd) Fairness: Following the work by Dai et al. [18], we measure the fairness of post hoc explanations by averaging all the aforementioned metric values across instances in the majority and minority subgroups, and comparing the two estimates. If there is a huge difference in the two estimates, then we consider this to be evidence for unfairness.\\n\\nInvoking the aforementioned metrics to benchmark an explanation methods is quite simple and the code snippet below describes how to invoke the RIS metric. Users can easily incorporate their own custom evaluation metrics into OpenXAI by filling a form and providing the GitHub link to their code as well as a summary of their metric (See Appendix).\\n\\n```python\\nfrom OpenXAI import Evaluator\\nmetric_evaluator = Evaluator(inputs, labels, model, explanations)\\nscore = metric_evaluator.eval(metric='RIS')\\n```\\n\\nBenchmarking: As can be seen from the code snippets in this section, OpenXAI allows end users to easily benchmark explanation methods using just a few lines of code. To summarize the benchmarking process, let us consider a scenario where we would like to benchmark a new explanation method using OpenXAI's pre-trained neural network model and the German Credit dataset. First, we use OpenXAI's Dataloader class to load the German Credit dataset. Second, we load the neural network model ('ann') using our LoadModel class. Third, we extend the Explainer class and incorporate the code for the new explanation method in the get_explanation function of this class. Finally, we evaluate the new explanation method using various metrics from the Evaluator class.\\n\\n4) Leaderboards. OpenXAI introduces the first ever public XAI leaderboards to promote transparency, and enable users to easily compare the performance of multiple explanation methods across a variety of evaluation metrics, predictive models, and datasets. In the current release, we have six different leaderboards each corresponding to a particular dataset. A snapshot of one of our leaderboard pages is shown in Figure 1. Users can submit requests for their custom explanation methods to be featured on one of our leaderboards. To this end, they first need to following the aforementioned benchmarking process to develop and evaluate their explanation method.\"}"}
{"id": "MU2495w47rz", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: A snapshot of the leaderboard page from OpenXAI public website.\\n\\nWe also provide interactive ranking functionality (arrow mark in the figure) which allows users to rank explanation methods based on metrics of their choice. Please visit the website to see leaderboards for other datasets.\\n\\nExperimental Setup.\\n\\nWe benchmark all the six state-of-the-art feature attribution methods currently available in our OpenXAI framework along with the random baseline, using the openxai.Evaluator module (See Section 2). We use default hyperparameter settings for all these methods following the guidelines outlined in the original implementations. Details about the hyperparameters used in our experiments are discussed in Section E.3 in the Appendix. Our OpenXAI framework currently has two pre-trained models, a logistic regression model and a deep neural network model, for each dataset. The neural network models have two fully connected hidden layers with 100 nodes in each layer, and they use ReLU activation functions and an output softmax layer. See Appendix E.4 for more details on model architectures, model training, and model performance.\\n\\nTable 2: Ground-truth and predicted faithfulness results on the Heloc dataset for all explanation methods with LR model. Shown are average and standard error metric values computed across all instances in the test set.\\n\\n| Method      | PRA   | RC    | FA    | RA    | SA    | SRA   |\\n|-------------|-------|-------|-------|-------|-------|-------|\\n| Random      | 0.500 \u00b10.00 | 1.0 \u00b10.00 | 1.0 \u00b10.00 | 0.641 \u00b10.00 | 1.0 \u00b10.00 | 0.645 \u00b10.00 |\\n| VanillaGrad | 0.005 \u00b10.01 | 1.0 \u00b10.00 | 1.0 \u00b10.00 | 0.390 \u00b10.01 | 1.0 \u00b10.00 | 0.384 \u00b10.01 |\\n| IntegratedGrad | 0.043 \u00b10.00 | 0.957 \u00b10.00 | 0.957 \u00b10.00 | 0.582 \u00b10.00 | 0.957 \u00b10.00 | 0.586 \u00b10.00 |\\n| Gradient x Input | 0.022 \u00b10.00 | 0.469 \u00b10.01 | 0.469 \u00b10.01 | 0.024 \u00b10.00 | 0.274 \u00b10.00 | 0.029 \u00b10.00 |\\n| SmoothGrad | 0.049 \u00b10.00 | 0.957 \u00b10.00 | 0.957 \u00b10.00 | 0.671 \u00b10.00 | 0.469 \u00b10.01 | 0.670 \u00b10.00 |\\n| SHAP     | 0.035 \u00b10.00 | 0.034 \u00b10.00 | 0.034 \u00b10.00 | 0.033 \u00b10.00 | 0.034 \u00b10.00 | 0.036 \u00b10.00 |\\n| LIME     | 0.035 \u00b10.00 | 0.036 \u00b10.00 | 0.036 \u00b10.00 | 0.036 \u00b10.00 | 0.036 \u00b10.00 | 0.042 \u00b10.00 |\\n\\nFaithfulness.\\n\\nWe evaluate the ground-truth and predictive faithfulness of explanations generated by state-of-the-art methods using both synthetic and real-world datasets.\\n\\nGround-truth faithfulness: We evaluate ground-truth faithfulness by calculating the similarity between the generated explanations and the ground-truth explanations using the metrics discussed in Section 2. Results for various ground-truth faithfulness metrics are shown in Tables 2, 3, 16, 17. Vanilla Gradients, SmoothGrad, and Integrated Gradients produce explanations that achieve perfect scores on four ground-truth faithfulness metrics, viz. pairwise rank agreement (PRA), feature agreement (FA), rank agreement (RA), and rank correlation (RC) metrics, for all datasets. However, on average, across all datasets, LIME outperforms other methods on the signed agreement (SA) [+61.6%] and signed-rank agreement (SRA) [+65.3%] metrics, whereas gradient-based explainers achieve relatively lower values. While illustrative in nature, these findings show how OpenXAI can help identify the limitations of existing explanation methods, which in turn can inform the design of new methods.\"}"}
{"id": "MU2495w47rz", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Ground-truth and predicted faithfulness results on the Adult Income dataset for all explanation methods with LR model. Shown are average and standard error metric values computed across all instances in the test set. \\\"#\\\" indicates that higher values are better, and \\\"\\\\\\\" indicates that lower values are better. Values corresponding to best performance are bolded.\\n\\n| Method   | PRA | RC | FA | RA | SA | SRA | PGU | PGI |\\n|----------|-----|----|----|----|----|-----|-----|-----|\\n| Random   | 0.499 \u00b1 0.00 | 1.00 \u00b1 0.00 | 1.00 \u00b1 0.00 | 0.580 \u00b1 0.00 | 0.655 \u00b1 0.00 | 0.913 \u00b1 0.00 | 0.00 \u00b1 0.00 | 1.00 \u00b1 0.00 |\\n| Vanilla Gradients | 0.281 \u00b1 0.00 | 1.00 \u00b1 0.00 | 1.00 \u00b1 0.00 | 0.281 \u00b1 0.00 | 0.379 \u00b1 0.00 | 0.921 \u00b1 0.00 | 0.068 \u00b1 0.00 | 1.00 \u00b1 0.00 |\\n| Integrated Gradients | 0.496 \u00b1 0.00 | 0.923 \u00b1 0.00 | 0.923 \u00b1 0.00 | 0.567 \u00b1 0.00 | 0.601 \u00b1 0.00 | 0.869 \u00b1 0.00 | 0.037 \u00b1 0.00 | 1.00 \u00b1 0.00 |\\n| Gradient x Input | 0.068 \u00b1 0.00 | 0.921 \u00b1 0.00 | 0.923 \u00b1 0.00 | 0.075 \u00b1 0.00 | 0.105 \u00b1 0.00 | 0.697 \u00b1 0.00 | 0.037 \u00b1 0.00 | 1.00 \u00b1 0.00 |\\n| SmoothGrad | 0.250 \u00b1 0.00 | 0.138 \u00b1 0.00 | 0.138 \u00b1 0.00 | 0.070 \u00b1 0.00 | 0.741 \u00b1 0.00 | 0.133 \u00b1 0.00 | 0.003 \u00b1 0.00 | 1.00 \u00b1 0.00 |\\n| SHAP | 0.053 \u00b1 0.00 | 0.070 \u00b1 0.00 | 0.070 \u00b1 0.00 | 0.043 \u00b1 0.00 | 0.008 \u00b1 0.00 | 0.047 \u00b1 0.00 | 0.014 \u00b1 0.00 | 1.00 \u00b1 0.00 |\\n| LIME | 0.06 \u00b1 0.00 | 0.039 \u00b1 0.00 | 0.039 \u00b1 0.00 | 0.073 \u00b1 0.00 | 0.099 \u00b1 0.00 | 0.068 \u00b1 0.00 | 0.094 \u00b1 0.00 | 1.00 \u00b1 0.00 |\\n\\nPredictive faithfulness: Tables 2, 3, 16, 17 show results for the PGI and PGU metrics implemented in OpenXAI (see Section 2 and Appendix A). Overall, we find that SmoothGrad explanations are most faithful to the underlying model and, on average, across multiple datasets outperform other feature-attribution methods on PGU metric (+43.03%). However, results from the German credit dataset for the ANN model show that Gradient x Input produces considerably more faithful (+6.74%) explanations than other methods. Finally, this analysis confirms the finding by Krishna et al. [43] that explanations output by state-of-the-art methods do not necessarily align with each other. This finding further highlights the need for rigorous empirical and theoretical benchmarking of explanation methods.\\n\\nStability. Next, we examine the stability of explanation methods when the underlying models are LR models in Tables 4 and 5, and neural network models in Tables 19 and 20 in the Appendix. Due to space constraints, we focus on RIS and RRS metrics in the main paper and leave the other results to the Appendix. Overall, the relative stability varies considerably across different datasets, implying that no single explanation method is consistently the most stable. First, for the synthetic dataset in Table 4, we find that Gradient x Input, on average, outperforms feature-attribution methods in relative input stability (+93.5%, RIS) and relative representation stability (+59.2%, RRS). However, stability of Gradient x Input significantly degrades on real-world datasets (Table 5, 19, 20). Second, as shown in Tables 5, 19, and 20, there is no single explanation method that has the highest input and representation stability across all the real-world datasets. On average, across all real-world datasets, SmoothGrad achieves 63.2% higher RRS values compared to other methods, whereas no method performs consistently well when it comes to the RIS metric.\\n\\nTable 4: Stability of explanation methods on the Synthetic dataset with LR model. Shown are average and standard error values across all test set instances. Values closer to zero are desirable, and the best performance is bolded.\\n\\n| Method   | RIS   | RRS   |\\n|----------|-------|-------|\\n| Random   | 6.868 \u00b1 0.013 | 6.133 \u00b1 0.011 |\\n| Vanilla Gradients | 5.957 \u00b1 0.013 | 5.249 \u00b1 0.008 |\\n| Integrated Gradients | 0.405 \u00b1 0.015 | 5.673 \u00b1 0.012 |\\n| Gradient x Input | 9.355 \u00b1 0.008 | 9.419 \u00b1 0.037 |\\n| SmoothGrad | 6.687 \u00b1 0.015 | 6.144 \u00b1 0.006 |\\n| SHAP | 9.022 \u00b1 0.043 | 8.751 \u00b1 0.035 |\\n| LIME | 3.422 \u00b1 0.037 | 13.564 \u00b1 0.036 |\\n\\nTable 5: Stability of explanation methods on the German Credit dataset with LR model. Shown are average and standard error values across all test set instances. Values closer to zero are desirable, and the best performance is bolded.\\n\\n| Method   | RIS   | RRS   |\\n|----------|-------|-------|\\n| Random   | 6.274 \u00b1 0.104 | -1.384 \u00b1 0.112 |\\n| Vanilla Gradients | -2.004 \u00b1 0.119 | -0.906 \u00b1 0.104 |\\n| Integrated Gradients | -0.906 \u00b1 0.104 | -4.780 \u00b1 0.117 |\\n| Gradient x Input | 16.448 \u00b1 0.124 | 5.241 \u00b1 0.024 |\\n| SmoothGrad | 5.241 \u00b1 0.024 | 4.560 \u00b1 0.029 |\\n| SHAP | 9.437 \u00b1 0.124 | 4.931 \u00b1 0.122 |\\n| LIME | 10.056 \u00b1 0.115 | 9.397 \u00b1 0.119 |\\n\\nFairness. To measure fairness of explanation methods, we compute the average metric values (for each of the aforementioned faithfulness and stability metrics) for different subgroups (e.g., male and female) in the dataset and compare them. Larger gaps between the metric values for different subgroups indicates higher disparities (unfairness). Without loss of generality, we present results using the PGU (see Section 2 and Appendix A) metric. Results for LR models in Figures 2 and 3 provide two key findings. First, the fairness analysis in Figures 2 and 3 shows that there are disparities in the faithfulness of explanations (see Section 2) output by several methods (Vanilla Gradients, Integrated Gradients, and SmoothGrad). Second, Gradient x Input results in the least amount of disparity across both the datasets. These results also suggest a trade-off between various evaluation metrics. For instance, Gradient x Input method underperforms on faithfulness and stability\"}"}
{"id": "MU2495w47rz", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"metrics, but outperforms other methods (+8.9%) when it comes to fairness metrics. Given such trade-offs, practitioners can leverage the OpenXAI leaderboards (Figure 1) to select an explanation method that best meets application-specific needs. Results with NN models and other fairness metrics are included in the Appendix E.7.\\n\\nFigure 2: Fairness analysis of PGU metric on the German Credit dataset with LR model. Shown are average and standard error values for majority (male) and minority (female) subgroups. Larger gaps between the values of majority and minority subgroups (i.e, red and blue bars respectively) indicate higher disparities which are undesirable.\\n\\nFigure 3: Fairness analysis of PGU metric on the Adult Income dataset with LR model. Shown are average and standard error values for majority (male) and minority (female) subgroups. Larger gaps between the values of majority and minority subgroups (i.e, red and blue bars respectively) indicate higher disparities which are undesirable.\\n\\n4 Conclusions\\nAs post hoc explanations are increasingly being employed to aid decision makers and relevant stakeholders in various high-stakes applications, it becomes important to ensure that these explanations are reliable. To this end, we introduce OpenXAI, an open-source ecosystem comprising of XAI-ready datasets, implementations of state-of-the-art explanation methods, evaluation metrics, leaderboards and documentation to promote transparency and collaboration around evaluations of post hoc explanations. OpenXAI can readily be used to benchmark new explanation methods as well as incorporate them into our framework and leaderboards. By enabling systematic and efficient evaluation and benchmarking of existing and new explanation methods, OpenXAI can inform and accelerate new research in the emerging field of XAI. OpenXAI will be regularly updated with new datasets, explanation methods, and evaluation metrics, and welcomes input from the community.\\n\\nAcknowledgments and Disclosure of Funding\\nThe authors would like to thank the anonymous reviewers for their helpful feedback and all the funding agencies listed below for supporting this work. This work is supported in part by the NSF awards #IIS-2008461 and #IIS-2040989, and research awards from Google, JP Morgan, Amazon, Harvard Data Science Initiative, and D3 Institute at Harvard. HL would like to thank Sujatha and Mohan Lakkaraju for their continued support and encouragement. The views expressed here are those of the authors and do not reflect the official policy or position of the funding agencies.\\n\\nReferences\\n[1] Framingham heart study dataset | kaggle. https://www.kaggle.com/datasets/aasheesh200/framingham-heart-study-dataset. (Accessed on 08/15/2022).\\n[2] Shap benchmark. URL https://shap.readthedocs.io/en/latest/index.html.\\n[3] Chirag Agarwal and Anh Nguyen. Explaining image classifiers by removing input features using generative models. In ACCV, 2020.\\n[4] Chirag Agarwal, Nari Johnson, Martin Pawelczyk, Satyapriya Krishna, Eshika Saxena, Marinka Zitnik, and Himabindu Lakkaraju. Rethinking stability for attribution-based explanations. In ICLR 2022 Workshop on PAIR Struct, 2022.\\n[5] Sushant Agarwal, Shahin Jabbari, Chirag Agarwal, Sohini Upadhyay, Steven Wu, and Himabindu Lakkaraju. Towards the unification and robustness of perturbation and gradient based explanations. In ICML, 2021.\\n[6] Ulrich Aivodji, Hiromi Arai, Olivier Fortineau, S\u00e9bastien Gambs, Satoshi Hara, and Alain Tapp. Fairwashing: the risk of rationalization. In ICML, 2019.\"}"}
{"id": "MU2495w47rz", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"David Alvarez-Melis and Tommi S Jaakkola. On the robustness of interpretability methods. arXiv, 2018.\\n\\nAlejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garc\u00eda, Sergio Gil-L\u00f3pez, Daniel Molina, Richard Benjamins, et al. Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai. Information Fusion, 2020.\\n\\nAparna Balagopalan, Haoran Zhang, Kimia Hamidieh, Thomas Hartvigsen, Frank Rudzicz, and Marzyeh Ghassemi. The road to explainability is paved with bias: Measuring the fairness of explanations. arXiv, 2022.\\n\\nNaman Bansal, Chirag Agarwal, and Anh Nguyen. Sam: The sensitivity of attribution methods to hyperparameters. In CVPR, 2020.\\n\\nSolon Barocas, Andrew Selbst, and Manish Raghavan. The hidden assumptions behind counterfactual explanations and principal reasons. In FAccT, 2020.\\n\\nOsbert Bastani, Carolyn Kim, and Hamsa Bastani. Interpretability via model extraction. arXiv, 2017.\\n\\nJacob Bien and Robert Tibshirani. Classification by set cover: The prototype vector machine. arXiv, 2009.\\n\\nVadim Borisov, Tobias Leemann, Kathrin Se\u00dfler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. Deep neural networks and tabular data: A survey. arXiv, 2021.\\n\\nRich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In KDD, 2015.\\n\\nValerie Chen, Nari Johnson, Nicholay Topin, Gregory Plumb, and Ameet Talwalkar. Use-case-grounded simulations for explanation evaluation. arXiv, 2022.\\n\\nIan Covert, Scott Lundberg, and Su-In Lee. Explaining by removing: A unified framework for model explanation. JMLR, 2021.\\n\\nJessica Dai, Sohini Upadhyay, Ulrich Aivodji, Stephen H Bach, and Himabindu Lakkaraju. Fairness via explanation quality: Evaluating disparities in the quality of post hoc explanations. In AAAI Conference on AI, Ethics, and Society (AIES), 2022.\\n\\nSanjoy Dasgupta, Nave Frost, and Michal Moshkovitz. Framework for evaluating faithfulness of local explanations. arXiv, 2022.\\n\\nRicardo Dominguez-Olmedo, Amir H Karimi, and Bernhard Sch\u00f6lkopf. On the adversarial robustness of causal algorithmic recourse. In ICML. PMLR, 2022.\\n\\nFinale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning. arXiv, 2017.\\n\\nDheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/ml.\\n\\nRadwa Elshawi, Mouaz H Al-Mallah, and Sherif Sakr. On the interpretability of machine learning-based model for predicting hypertension. BMC medical informatics and decision making, 2019.\\n\\nLukas Faber, Amin K. Moghaddam, and Roger Wattenhofer. When comparing to ground truth is wrong: On evaluating gnn explanation methods. In KDD, 2021.\\n\\nFICO. Explainable machine learning challenge. https://community.fico.com/s/explainable-machine-learning-challenge?tabset-158d9=3, 2022. (Accessed on 05/23/2022).\"}"}
{"id": "MU2495w47rz", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hidde Fokkema, Rianne de Heide, and Tim van Erven. Attribution-based explanations that provide recourse cannot be robust. arXiv, 2022.\\n\\nBryce Freshcorn. Give me some credit :: 2011 competition data | kaggle. https://www.kaggle.com/datasets/brycecf/give-me-some-credit-dataset, 2022. (Accessed on 05/23/2022).\\n\\nMarzyeh Ghassemi, Luke Oakden-Rayner, and Andrew L Beam. The false hope of current approaches to explainable artificial intelligence in health care. The Lancet Digital Health, 2021.\\n\\nAmirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile. In AAAI Conference on Artificial Intelligence, 2019.\\n\\nRiccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. A survey of methods for explaining black box models. ACM computing surveys (CSUR), 2018.\\n\\nTessa Han, Suraj Srinivas, and Himabindu Lakkaraju. Which explanation should i choose? a function approximation perspective to characterizing post hoc explanations. arXiv, 2022.\\n\\nAnna Hedstr\u00f6m, Leander Weber, Dilyara Bareeva, Franz Motzkus, Wojciech Samek, Sebastian Lapuschkin, and Marina M-C H\u00f6hne. Quantus: an explainable ai toolkit for responsible evaluation of neural network explanations. arXiv, 2022.\\n\\nSara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. Evaluating feature importance estimates. arXiv, 2018.\\n\\nMark Ibrahim, Melissa Louie, Ceena Modarres, and John Paisley. Global explanations of neural networks: Mapping the landscape of predictions. CoRR, abs/1902.02384, 2019.\\n\\nS\u00e9rgio Jesus, Catarina Bel\u00e9m, Vladimir Balayan, Jo\u00e3o Bento, Pedro Saleiro, Pedro Bizarro, and Jo\u00e3o Gama. How can i choose an explainer? an application-grounded evaluation of post-hoc explanations. In FAccT, 2021.\\n\\nKareem L Jordan and Tina L Freiburger. The effect of race/ethnicity on sentencing: Examining sentence type, jail length, and prison length. In Journal of Ethnicity in Criminal Justice. Taylor & Francis, 2015.\\n\\nAmir-Hossein Karimi, Gilles Barthe, Borja Balle, and Isabel Valera. Model-agnostic counterfactual explanations for consequential decisions. arXiv, 2019.\\n\\nAmir-Hossein Karimi, Bernhard Sch\u00f6lkopf, and Isabel Valera. Algorithmic recourse: from counterfactual explanations to interventions. CoRR, abs/2002.06278, 2020.\\n\\nAmir-Hossein Karimi, Julius von K\u00fcgelgen, Bernhard Sch\u00f6lkopf, and Isabel Valera. Algorithmic recourse under imperfect causal knowledge: a probabilistic approach. CoRR, 2020.\\n\\nHarmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, and Jennifer Wortman Vaughan. Interpreting interpretability: Understanding data scientists' use of interpretability tools for machine learning. In CHI Conference on Human Factors in Computing Systems, 2020.\\n\\nJoon Sik Kim, Gregory Plumb, and Ameet Talwalkar. Sanity simulations for saliency methods. arXiv, 2021.\\n\\nNarine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-Richardson. Captum: A unified and generic model interpretability library for pytorch, 2020.\\n\\nSatyapriya Krishna, Tessa Han, Alex Gu, Javin Pombra, Shahin Jabbari, Steven Wu, and Himabindu Lakkaraju. The disagreement problem in explainable machine learning: A practitioner's perspective. arXiv, 2022.\\n\\nIsaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam Gershman, and Finale Doshi-Velez. An evaluation of the human-interpretability of explanation. arXiv, 2019.\"}"}
{"id": "MU2495w47rz", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Himabindu Lakkaraju and Osbert Bastani. \\\"How do I fool you?\\\" Manipulating user trust via misleading black box explanations. In AAAI Conference on AIES, 2020.\\n\\nHimabindu Lakkaraju, Stephen H Bach, and Jure Leskovec. Interpretable decision sets: A joint framework for description and prediction. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1675\u20131684, 2016.\\n\\nHimabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. Faithful and customizable explanations of black box models. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 131\u2013138, 2019.\\n\\nBenjamin Letham, Cynthia Rudin, Tyler H McCormick, and David Madigan. Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model. The Annals of Applied Statistics, 9(3):1350\u20131371, 2015.\\n\\nPantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. Explainable ai: A review of machine learning interpretability methods. Entropy, 23(1):18, 2021.\\n\\nZachary C Lipton. The mythos of model interpretability. CoRR, abs/1606.03490, 2016.\\n\\nYang Liu, Sujay Khandagale, Colin White, and Willie Neiswanger. Synthetic benchmarks for scientific research in explainable machine learning. In NeurIPS Datasets and Benchmarks Track, 2021.\\n\\nArnaud Looveren and Janis Klaise. Interpretable counterfactual explanations guided by prototypes. CoRR, abs/1907.02584, 2019.\\n\\nYin Lou, Rich Caruana, and Johannes Gehrke. Intelligible models for classification and regression. In KDD, 2012.\\n\\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Neural Information Processing Systems (NIPS), pages 4765\u20134774. Curran Associates, Inc., 2017.\\n\\nW James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. Definitions, methods, and applications in interpretable machine learning. Proceedings of the National Academy of Sciences, 2019.\\n\\nMartin Pawelczyk, Klaus Broelemann, and Gjergji Kasneci. Learning model-agnostic counterfactual explanations for tabular data. In WWW, 2020.\\n\\nMartin Pawelczyk, Sascha Bielawski, Johan Van den Heuvel, Tobias Richter, and Gjergji Kasneci. Carla: A python library to benchmark algorithmic recourse and counterfactual explanation algorithms. In NeurIPS Benchmark and Datasets Track, 2021.\\n\\nVitali Petsiuk, Abir Das, and Kate Saenko. Rise: Randomized input sampling for explanation of black-box models. arXiv, 2018.\\n\\nForough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Vaughan, and Hanna Wallach. Manipulating and measuring model interpretability. CoRR, 2018.\\n\\nRafael Poyiadzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and Peter Flach. FACE: Feasible and actionable counterfactual explanations. In AAAI Conference on AIES, 2020.\\n\\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \\\"Why should I trust you?\\\": Explaining the predictions of any classifier. In KDD, 2016.\\n\\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: High-precision model-agnostic explanations. In AAAI, 2018.\"}"}
{"id": "MU2495w47rz", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. (Cynthia Rudin, 2019)\\n\\nGrad-cam: Visual explanations from deep networks via gradient-based localization. (Ramprasaath R Selvaraju et al., 2017)\\n\\nLearning important features through propagating activation differences. (Avanti Shrikumar et al., 2017)\\n\\nDeep inside convolutional networks: Visualising image classification models and saliency maps. (Karen Simonyan et al., 2014)\\n\\nFooling lime and shap: Adversarial attacks on post hoc explanation methods. (Dylan Slack et al., 2020)\\n\\nReliable post hoc explanations: Modeling uncertainty in explainability. (Dylan Slack et al., 2021)\\n\\nSmoothgrad: removing noise by adding noise. (Daniel Smilkov et al., 2017)\\n\\nUsing the adap learning algorithm to forecast the onset of diabetes mellitus. (Jack W Smith et al., 1988)\\n\\nAxiomatic attribution for deep networks. (Mukund Sundararajan et al., 2017)\\n\\nTowards robust and reliable algorithmic recourse. (Sohini Upadhyay et al., 2021)\\n\\nActionable recourse in linear classification. (Berk Ustun et al., 2019)\\n\\nCounterfactual explanations for machine learning: A review. (Sahil Verma et al., 2020)\\n\\nCounterfactual explanations without opening the black box: Automated decisions and the GDPR. (Sandra Wachter et al., 2017)\\n\\nFalling rule lists. (Fulton Wang and Cynthia Rudin, 2015)\\n\\nMapping chemical performance on molecular structures using locally interpretable explanations. (Leanne S Whitmore et al., 2016)\\n\\nThe comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. (I-Cheng Yeh and Che-hui Lien, 2009)\\n\\nInterpretable classification models for recidivism prediction. (Jiaming Zeng et al., 2017)\\n\\nEvaluating the quality of machine learning explanations: A survey on methods and metrics. (Jianlong Zhou et al., 2021)\"}"}
{"id": "MU2495w47rz", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Checklist\\n\\nThe checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [TODO] to [Yes], [No], or [N/A]. You are strongly encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description. For example:\\n\\n\u2022 Did you include the license to the code and datasets?\\n  [Yes] See Section ??.\\n\\n\u2022 Did you include the license to the code and datasets?\\n  [No] The code and the data are proprietary.\\n\\n\u2022 Did you include the license to the code and datasets?\\n  [N/A]\\n\\nPlease do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\\n      [Yes] See Section 2 for a step-by-step justification of OpenXAI's contributions and Section 3 for an in-depth benchmarking analysis of explanation methods using our OpenXAI framework.\\n\\n   (b) Did you describe the limitations of your work?\\n      [Yes] In Section 2 we discuss that OpenXAI includes only six datasets and seven post-hoc explainers in its current release. However, we provide detailed instructions on how users can leverage our easy-to-use class templates and add new datasets and explainers to our pipeline. In addition, we discuss how OpenXAI ameliorates the limitation of previous benchmarks by providing the first-ever public leaderboard for comparing post-hoc explainers.\\n\\n   (c) Did you discuss any potential negative societal impacts of your work?\\n      [N/A]\\n\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them?\\n      [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results?\\n      [N/A]\\n\\n   (b) Did you include complete proofs of all theoretical results?\\n      [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?\\n      [Yes] See Appendix E.1, E.4, E.3 for a detailed description about the dataset, data splits, model architectures (number of layers, activation function, etc.), hyperparameter details of the post-hoc explainers, and metric settings. In addition, we open-source our codes in the OpenXAI GitHub repository. To improve the accessibility, interoperability, and reuse of ML tools, we apply FAIR4RS principles and implementation guidelines to all software and ML tools included in OpenXAI. We strongly believe that software and ML tools should be open and adhere to FAIR principles to encourage repeatability, reproducibility, and reuse.\\n\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?\\n      [Yes] See Section E for all training details.\\n\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?\\n      [Yes] See Figures 2-3 where we report the error bars that denote the error in the metric performance across all instances in the test set.\\n\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?\\n      [Yes] See Appendix E.4 for the compute used in benchmarking OpenXAI's post-hoc explainers.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators?\\n      [Yes] All assets, including datasets, post-hoc explainers, and evaluation metrics are properly cited.\\n\\n      15\"}"}
{"id": "MU2495w47rz", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Did you mention the license of the assets?\\n   [Yes] MIT License.\\n\\n2. Did you include any new assets either in the supplemental material or as a URL?\\n   [No]\\n\\n3. Did you discuss whether and how consent was obtained from people whose data you're using/curating?\\n   [N/A]\\n\\n4. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?\\n   [N/A]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n\\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable?\\n   [N/A]\\n\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?\\n   [N/A]\\n\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?\\n   [N/A]\"}"}
