{"id": "oIUXpBnyjv", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This approach introduces a new improved policy, $\\\\pi'$, which seamlessly integrates the original visit counts distribution with MCTS searched values. By combining these two sources of information, Gumbel MuZero efficiently exploits the knowledge acquired during the search process, leading to more informed decision-making and improved performance in a variety of environments.\\n\\nStochastic MuZero\\nThis extension enhances MuZero by enabling it to learn and plan with stochastic models. Specifically, it incorporates a stochastic model that includes afterstates and conducts stochastic tree searches using this model. Stochastic MuZero achieves competitive or superior performance compared to state-of-the-art methods in a variety of canonical board games, such as 2048 and Backgammon, while preserving the superhuman performance demonstrated by the standard MuZero in Go.\\n\\nG.2 LightZero Algorithms Overview\\nIn this section, we present a detailed summary of various MCTS/MuZero variants, encapsulated in algorithm overview diagrams. Specifically, Figure 17 provides an insight into the Monte Carlo Tree Search (MCTS), while Figure 18 illuminates the mechanics of AlphaZero, a fusion of MCTS and deep neural networks. Moreover, Figure 19 introduces MuZero, an advancement upon AlphaZero with a learned model representation. Figure 21 subsequently presents the Sampled MuZero, and finally, Figure 22 delineates the Gumbel MuZero variant. These overview diagrams serve to highlight the evolution and interconnections among MCTS, AlphaZero, MuZero, and their related algorithms.\\n\\nG.3 Model Architecture\\nWe provide the representation network structure of the MuZero series algorithms implemented in LightZero in Figure 23. The dynamics network and prediction network structures (including both value and policy networks) are presented in Figure 24. Note that the model structures we provide are based on Atari environments with image-type observations. For environments with vector-type observations, such as continuous control tasks, the overall model structure is similar, only replacing the convolutional layers in the original model with corresponding fully connected layers. For specific details, please refer to the model directory through OpenDILab at https://github.com/opendilab/LightZero.\\n\\nG.4 Hyperparameters\\nWe provide a detailed summary of the key hyperparameters for the MuZero w/ SSL algorithm, as implemented in LightZero for the Atari environment, in Table 7. The hyperparameters for other environments and algorithms are generally similar. Unless explicitly stated otherwise, all other parameters are in accordance with those in Table 7. Specifically, we highlight the primary differing hyperparameters for the Sampled EfficientZero algorithm in the Atari environment in Table 8, as well as for continuous control environments such as MuJoCo in Table 9. In addition, we explain the main distinct hyperparameters for the Gumbel MuZero algorithm in the Atari environment in Table 10, alongside the Stochastic EfficientZero algorithm in both the Atari environment, as shown in Table 11, and the 2048 environment, as presented in Table 12. Lastly, the pivotal hyperparameters for the AlphaZero/MuZero algorithm in the Gomoku environment are set out in Tables 13 and 14.\\n\\nH Comparison with Other MCTS Frameworks\\nLightZero is a comprehensive algorithm benchmark developed using PyTorch, integrating nine distinct algorithms such as AlphaZero and MuZero. It supports over twenty different environments, including Atari and Go. On the other hand, the MCTX library, primarily implemented using JAX, includes basic implementations of algorithms such as AlphaZero, MuZero, and Gumbel MuZero. However, the training process across various environments is not yet fully established.\\n\\n2 https://github.com/google-deepmind/mctx\"}"}
{"id": "oIUXpBnyjv", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To offer an intuitive comparison of the differences in integrated algorithms and supported environments between these two repositories, we present the algorithms and environments supported by LightZero and MCTX in the following Table 5 and Table 6, respectively.\\n\\nPlease note: \\\"!\\\" denotes that the corresponding item is fully implemented and thoroughly tested. \\\"\u2014\\\" signifies that the item is currently in development. \\\"\u2014\\\" indicates that the algorithm does not support the respective environment.\\n\\nRegarding the comparison table provided, we should note the following two points: Firstly, the list of algorithms and environments supported by MCTX not only encompasses MCTX itself, but also includes all derivative repositories based on MCTX. Secondly, all environments associated with Gumbel MuZero and Stochastic MuZero within the MCTX library are currently in a locked \\\"\u2014\\\" state. This is because, to our knowledge, neither MCTX nor its derivative repositories have fully implemented these algorithms and environments. Although the foundational modules for the Gumbel MuZero and Stochastic MuZero algorithms exist in the original MCTX repository, the development of a complete training pipeline for these algorithms is still in progress.\\n\\n| Algorithm | Classic | Control | Box2D | Atari | MuJoCo | Go | Bigger | Mini | Grid | Maze | Connect | Four | Gomoku | 2048 | Go |\\n|-----------|---------|---------|-------|-------|--------|----|--------|------|------|------|---------|------|--------|------|----|\\n| AlphaZero | \u2014       | \u2014       | \u2014     | \u2014     | \u2014      | \u2014  | \u2014      | \u2014    | \u2014    | \u2014    | \u2014       | \u2014    | \u2014      | \u2014    | \u2014  |\\n| Sampled  AlphaZero | \u2014       | \u2014       | \u2014     | \u2014     | \u2014      | \u2014  | \u2014      | \u2014    | \u2014    | \u2014    | \u2014       | \u2014    | \u2014      | \u2014    | \u2014  |\\n| Gumbel AlphaZero | \u2014       | \u2014       | \u2014     | \u2014     | \u2014      | \u2014  | \u2014      | \u2014    | \u2014    | \u2014    | \u2014       | \u2014    | \u2014      | \u2014    | \u2014  |\\n| MuZero    | !       | !       | !     | !     | \u2014      | \u2014  | !      | !    | !    | !    | !       | !    | !      | !    | !  |\\n| MuZero w/ SSL | !       | !       | !     | !     | \u2014      | \u2014  | !      | !    | !    | !    | !       | !    | !      | !    | !  |\\n| EfficientZero | !       | !       | !     | !     | !      | \u2014  | !      | !    | !    | !    | !       | !    | !      | !    | !  |\\n| Gumbel MuZero | !       | !       | !     | !     | \u2014      | \u2014  | !      | !    | !    | !    | !       | !    | !      | !    | !  |\\n| Sampled MuZero | !       | !       | !     | !     | !      | \u2014  | !      | !    | !    | !    | !       | !    | !      | !    | !  |\\n| Stochastic MuZero | !       | !       | !     | !     | \u2014      | \u2014  | !      | !    | !    | !    | !       | !    | !      | !    | !  |\\n\\nTable 5: Algorithms and Environments supported by LightZero as of the writing of this paper.\\n\\n| Algorithm | Classic | Control | Box2D | Atari | Go |\\n|-----------|---------|---------|-------|-------|----|\\n| AlphaZero | \u2014       | \u2014       | \u2014     | \u2014     | \u2014  |\\n| MuZero    | !       | !       | !     | !     | \u2014  |\\n| Gumbel MuZero | \u2014   | \u2014       | \u2014     | \u2014     | \u2014  |\\n| Stochastic MuZero | \u2014  | \u2014       | \u2014     | \u2014     | \u2014  |\\n\\nTable 6: Algorithms and Environments supported by MCTX as of the writing of this paper.\\n\\nI Limitations and Future Work\\n\\nDespite its considerable achievements across various benchmark environments, LightZero does bear certain limitations.\\n\\n1. Firstly, while efforts have been made to amplify the algorithm's universality through decoupling and modularization, certain specific problems may still necessitate tailored adjustments and optimizations. For instance, all current LightZero algorithms do not directly support hybrid action space environments [79]. However, through the use of action representation [80] and other techniques, they could potentially be adapted to a wider action space.\\n\\n2. Secondly, as discussed in Section 3, due to the intrinsic limitations of the MCTS algorithm, our method may encounter challenges in specific, complex real-world environments. Particularly when handling environments with high-dimensional action spaces or strong randomness, methods like Sampled MuZero and Stochastic MuZero could benefit from further optimization and improvement.\"}"}
{"id": "oIUXpBnyjv", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Finally, the high prerequisites of LightZero can pose challenges, especially for those new to decision intelligence. This is largely due to the inherent complexity of the MCTS+RL algorithms. While these algorithms are comprehensive, they require a profound understanding and significant time investment to deploy effectively. This complexity, alongside the computational demands of running these advanced algorithms, can limit their widespread application. Future enhancements should concentrate on improving the accessibility of the LightZero framework\u2019s interfaces, enriching the documentation, and cultivating a vibrant user community that encourages knowledge collaboration.\\n\\nDespite the aforementioned challenges, we remain optimistic about the potential of the LightZero framework. For future, we have identified several promising areas for further exploration:\\n\\n- **Broadening Applications**: We envision more researchers and developers applying LightZero across a wider spectrum of practical fields. These include but are not limited to natural language processing, autonomous driving, and the control and optimization of complex systems.\\n\\n- **Algorithmic Optimization**: While we have enhanced our submodules by introducing more suitable exploration mechanisms and optimizing model loss functions, there remains significant room for improvement. We invite the community to contribute new exploration and optimization strategies to further boost the performance of MCTS-style algorithms.\\n\\n- **Integration with Cutting-Edge Technologies**: We stress the importance of a seamless integration between LightZero and future research directions. Two key areas for future exploration stand out:\\n  1. **Integration with Large Language Models (LLM)**: LLM can serve as a high-level task planner, breaking down complex decision-making problems into a series of low-level instructions. LightZero can set these instructions as conditional inputs of the representation network and serve as an efficient instruction executor. Furthermore, considering the powerful reanalysis mechanism of MuZero Unplugged, it can be a viable choice for fine-tuning LLMs.\\n  2. **Clever Utilization of Model-Based RL Techniques**: LightZero has already integrated many model-based RL techniques, such as the Recurrent State-Space Models (RSSM) used in DreamerV2, latent state consistency loss used in EfficientZero, and the afterstate dynamics functions proposed in Stochastic MuZero. We aim to incorporate additional techniques within the model representation, loss function, and MCTS search process to enhance the universality and robustness of the LightZero framework.\"}"}
{"id": "oIUXpBnyjv", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The MCTS is divided into four stages, and repeated for a number of simulations. Once the search is complete, we select the action with the highest number of visits.\\n\\nA. Selection\\n\\nIn each node, select action according to the UCB score:\\n\\nThe selection function is applied recursively until a leaf node is reached.\\n\\nB. Expansion\\n\\nMonte Carlo Tree Search (MCTS) action values are updated to track the mean value of all evaluations in the subtree below that action. The leaf node is added to the search tree. Each edge from the newly expanded node is initialized to:\\n\\nC. Evaluation\\n\\nThe leaf node is evaluated by running a rollout to the end of the game with the fast rollout policy (potentially random), then computing the winner with function .\\n\\nD. Backpropagation\\n\\nLeaf node\\n\\nRollout\\n\\nFigure 17: MCTS algorithm overview.\\n\\n36\"}"}
{"id": "oIUXpBnyjv", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Planning\\nB. Acting\\nC. Training\\nD. Loss\\n\\nwhere, \\\\( r \\\\) is the observed reward, \\\\( \\\\pi \\\\) is the MCTS searched policy, and \\\\( \\\\hat{Q} \\\\) is the bootstrapped n-step target:\\n\\n\\\\[\\n\\\\hat{Q} = r + \\\\gamma \\\\max_{a} \\\\hat{Q}(s', a)\\n\\\\]\\n\\nis the MCTS searched value.\\n\\nIn Atari, \\\\( \\\\ell \\\\) is cross-entropy loss, while in board games, \\\\( \\\\ell \\\\) is MSE loss and there is no \\\\( \\\\ell \\\\) due to no intermediate reward. \\\\( \\\\ell \\\\) is cross-entopy loss for both.\\n\\nSelection:\\n\\nIn each node, the agent selects the action according to the UCB score:\\n\\n\\\\[\\n\\\\text{UCB}(a) = \\\\frac{Q(a) + \\\\sqrt{2 \\\\ln N}{\\\\Delta}^2}{N(a)}\\n\\\\]\\n\\nwhere, \\\\( N(a) \\\\) is the number of times action \\\\( a \\\\) has been taken.\\n\\nBackup:\\n\\nFor each node, we update the statistics for each edge in the simulation path as follows:\\n\\n\\\\[\\n\\\\Delta(s, a, s') = r + \\\\gamma \\\\hat{Q}(s') - Q(s, a)\\n\\\\]\\n\\nwhere, in hypothetical step \\\\( s' \\\\), we utilize the bootstrapped estimate \\\\( \\\\hat{Q} \\\\) value:\\n\\n\\\\[\\n\\\\hat{Q}(s') = r + \\\\gamma \\\\max_{a} \\\\hat{Q}(s', a)\\n\\\\]\\n\\nExpansion:\\n\\nAt the leaf node (i.e. final timestep), the reward and hidden state are computed by the dynamics and stored in the corresponding tables. The policy and value are computed by the prediction function, \\\\( \\\\pi(s) \\\\) and \\\\( \\\\hat{Q}(s) \\\\), respectively.\\n\\nA new node, corresponding to state \\\\( s_{n+1} \\\\), is added to the search tree. Each edge from the newly expanded node is initialized to \\\\( \\\\Delta(s, a, s') \\\\).\\n\\nNOTE:\\n\\n- \\\\( D \\\\) is the dynamic network (MLP, without RNN), \\\\( \\\\nu \\\\) is the value prefix, \\\\( \\\\mu \\\\) is the reward hidden state.\\n- \\\\( \\\\hat{Q}(s') \\\\) is the reanalyzed MCTS root value.\\n\\nReference: MuZero target is \\\\( \\\\mathcal{Q} \\\\).\\n\\nThe self-supervised consistency loss.\"}"}
{"id": "oIUXpBnyjv", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"NOTE:\\n\\n\\\\( \\\\pi \\\\) is predicted policy (potentially continuous) distribution, \\\\( \\\\pi_0 \\\\) is the proposal policy distribution (typically equals), \\\\( \\\\pi_{\\\\text{emp}} \\\\) is the empirical policy distribution.\\n\\n**A. Planning**\\n\\n\\\\[ \\\\text{legal_actions} = \\\\{ \\\\vdots \\\\}_{1,K}, \\\\]\\n\\nwhich are sampled according to the proposal distribution.\\n\\n**Selection:** In each node, agent selects an action according to the UCB score:\\n\\n\\\\[ \\\\text{Sampled MuZero: Learning and Planning in Complex Action Spaces (High Dimensional Discrete or Continuous)} \\\\]\\n\\nFrom left to right, current policy, the empirical distribution, the sample-based improved policy, the improved policy, respectively. As the number of samples \\\\( K \\\\) increases, \\\\( \\\\pi_{\\\\text{emp}} \\\\) converges to \\\\( \\\\pi_{\\\\text{imp}} \\\\).\\n\\n**Sampled-based Policy Improvement.**\\n\\n**B. Acting**\\n\\n\\\\[ \\\\text{Figure 21: Sampled MuZero algorithm overview.} \\\\]\\n\\n**Gumbel MuZero:** Planning with few simulations in high dimensional discrete action space\\n\\nSequential Halving is used to identify the action with the highest.\\n\\nThe key issue when the number of simulations \\\\( < \\\\) number of total actions in MCTS search is how to choose which actions to visit and how many times.\\n\\n1. We can control the number of actions sampled without replacement.\\n2. We can use a bandit algorithm to efficiently explore the set of sampled actions.\\n\\n\\\\[ \\\\text{Figure 22: Gumbel MuZero algorithm overview.} \\\\]\"}"}
{"id": "oIUXpBnyjv", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 23: The network architecture of the representation network $h$ for image input domain in LightZero. This network represents the root observation as a latent state.\"}"}
{"id": "oIUXpBnyjv", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 24:\\n\\nLeft: The network architecture of the dynamics network $g$ for image input domains in LightZero. Given a latent state and a selected action, it outputs the transitioned next latent state and the corresponding predicted reward.\\n\\nRight: The network architecture of the prediction network $f$ (encompassing both value and policy networks) for image input domains. Given a latent state, this network predicts the action probability and value.\"}"}
{"id": "oIUXpBnyjv", "page_num": 41, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Hyperparameter          | Value |\\n|-------------------------|-------|\\n| Num of frames stacked    | 4     |\\n| Num of frames skip       | 4     |\\n| Reward clipping          | True  |\\n| Optimizer type           | Adam  |\\n| Learning rate            | $3 \\\\times 10^{-3}$ |\\n| Discount factor          | 0.997 |\\n| Weight of policy loss    | 1     |\\n| Weight of value loss     | 0.25  |\\n| Weight of reward loss    | 1     |\\n| Weight of policy entropy loss | 0   |\\n| Weight of SSL loss       | 2     |\\n| Batch size               | 256   |\\n| Model update ratio       | 0.25  |\\n| Frequency of target network update | 100 |\\n| Weight decay             | $10^{-4}$ |\\n| Max gradient norm        | 10    |\\n| Length of game segment   | 400   |\\n| Replay buffer size       | $1e6$ |\\n| TD steps                 | 5     |\\n| Number of unroll steps   | 5     |\\n| Use augmentation         | True  |\\n| Discrete action encoding type | One Hot |\\n| Normalization type       | Batch Normalization |\\n| Priority exponent coefficient | 0.6 |\\n| Priority correction coefficient | 0.4 |\\n| Dirichlet noise alpha    | 0.3   |\\n| Dirichlet noise weight   | 0.25  |\\n| Number of simulations in MCTS (sim) | 50 |\\n| Reanalyze ratio          | 0     |\\n| Categorical distribution in value and reward modeling | True |\\n| The scale of supports used in categorical distribution | 300 |\\n\\nTable 7: Key hyperparameters of MuZero with/SSL on Atari environments.\\n\\n| Hyperparameter          | Value |\\n|-------------------------|-------|\\n| Num of frames stacked    | 1     |\\n| Num of frames skip       | 1     |\\n| Reward clipping          | False |\\n| Number of sampled actions (K) | Different environments have distinct values for K, shown in Figure 7 |\\n| Policy loss type         | Cross Entropy Loss |\\n| Number of simulations in MCTS (sim) | 50 |\\n| Weight of policy entropy loss | 0.005 |\\n| Length of game segment   | 200   |\\n| Use augmentation         | False |\\n| Max gradient norm        | 0.5   |\\n\\nTable 8: Key hyperparameters of Sampled EfficentZero on Atari environments.\\n\\n| Hyperparameter          | Value |\\n|-------------------------|-------|\\n| Num of frames stacked    | 1     |\\n| Num of frames skip       | 1     |\\n| Reward clipping          | False |\\n| Number of sampled actions (K) | 20 |\\n| Policy loss type         | Cross Entropy Loss |\\n| Weight of policy entropy loss | 0.005 |\\n| Length of game segment   | 200   |\\n| Use augmentation         | False |\\n| Max gradient norm        | 0.5   |\\n\\nTable 9: Key hyperparameters of Sampled EfficentZero used in continuous control environments, such as, MuJoCo. 41\"}"}
{"id": "oIUXpBnyjv", "page_num": 42, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 10: Key hyperparameters of Gumbel MuZero on Atari environments.\\n\\n| Hyperparameter Value | Action Space Size |\\n|----------------------|-------------------|\\n| Gumbel Scale         | 10                |\\n| Max visit init       | 50                |\\n| Value Scale          | 0.1               |\\n\\n### Table 11: Key hyperparameters of Stochastic MuZero on Atari environments.\\n\\n| Hyperparameter Value | Action Space Size |\\n|----------------------|-------------------|\\n| Chance space size    | $16 \\\\times \\\\text{num of possible chance tile}$ |\\n| Afterstate Dynamics Network | Similar with Dynamics Network in Figure 24 |\\n| Afterstate Prediction Network | Similar with Prediction Network in Figure 24 |\\n| Chance Encoder       | Two Layer MLP     |\\n\\n### Table 12: Key hyperparameters of Stochastic MuZero on 2048 environments.\\n\\n| Hyperparameter Value | Action Space Size |\\n|----------------------|-------------------|\\n| Chance space size    | $16 \\\\times \\\\text{num of possible chance tile}$ |\\n| Afterstate Dynamics Network | Similar with Dynamics Network in Figure 24 |\\n| Afterstate Prediction Network | Similar with Prediction Network in Figure 24 |\\n| Chance Encoder       | Two Layer MLP     |\\n| Num of frames stacked | 1                |\\n| Num of frames skip   | 1                |\\n| Reward clipping      | False             |\\n| Discount factor      | 0.999             |\\n| Batch size           | 512               |\\n| Length of game segment | 200             |\\n| TD steps             | 10                |\\n| Use augmentation     | False             |\\n| Number of simulations in MCTS (sim) | 100               |\\n\\n### Table 13: Key hyperparameters of AlphaZero on Gomoku environments.\\n\\n| Hyperparameter Value | Action Space Size |\\n|----------------------|-------------------|\\n| Board size           | 6                 |\\n| Num of frames stacked | 1                |\\n| Discount factor      | 1                 |\\n| Weight of policy loss | 1              |\\n| Weight of value loss  | 1                 |\\n| Number of simulations in MCTS (sim) | 100               |\\n| Categorical distribution in value modeling | False          |\\n\\n### Table 14: Key hyperparameters of MuZero on Gomoku environments.\\n\\n| Hyperparameter Value | Action Space Size |\\n|----------------------|-------------------|\\n| Board size           | 6                 |\\n| Num of frames stacked | 1                |\\n| Discount factor      | 1                 |\\n| Weight of SSL (self-supervised learning) loss | 0          |\\n| Length of game segment | 18               |\\n| TD steps             | 18                |\\n| Use augmentation     | False             |\\n| Number of simulations in MCTS (sim) | 100               |\\n| The scale of supports used in categorical distribution | 10         |\"}"}
{"id": "oIUXpBnyjv", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Environment\\n\\nEnvironment Attributes Possible Categories Environment Instances\\nObservation Space\\n\u2022 Vector Observation Space\\n\u2022 Image Observation Space\\n\u2022 Structured Observation Space\\nClassical Control, Bsuite, Box2D, DMControl, Gym-Hybrid\\nBoard Games, Atari, Bsuite, MiniGrid, ProcGen, 2048\\n\\nAction Space\\n\u2022 Discrete Action Space\\n\u2022 Continuous Action Space\\n\u2022 Hybrid Action Space\\nClassical Control, Box2D, DMControl, MetaDrive, Gym-Hybrid, GoBigger\\n\\nReward Space\\n\u2022 Sparse Reward\\n\u2022 Normal Dense Reward\\n\u2022 Multi-Objective Dense Reward\\nMiniGrid, Board Games, Atari, ProcGen, Bsuite, Classical Control, Bsuite, Box2D, DMControl, Gym-Hybrid, Atari, ProcGen, 2048, MetaDrive, GoBigger\\n\\nTransition Function\\n\u2022 Stochastic\\n\u2022 Back-traceable\\n2048, Atari, Bsuite, Board Games, 2048, MiniGrid\\n\\nTable 1: Overview of various attributes associated with decision-making environments, their respective potential categories crucial for the design of MCTS-style methods, and specific instances of environments that exemplify each category. Please note that each environment can encompass a series of different sub-environments (e.g., Atari typically consists of 57 distinct video games), thus some environments can simultaneously belong to multiple categories. While most categories are self-explanatory, there are two special cases worth noting: Multi-Objective Dense Reward indicates that there are multiple, distinct rewards to be balanced for varying decision targets (e.g., balancing route optimization, speed, and stability in autonomous driving tasks); Back-traceable is a critical property for AlphaZero-like methods that requires a perfect simulator to return to the original state after each simulation. While Board Games can easily store and restore old states, most control tasks and video games are practically impossible to revert to their previous states.\\n\\nIn this section, we introduce various types of RL environments integrated in LightZero and their respective characteristics. These environments encompass a wide range, including Board Games, Classical Control, Atari, MuJoCo, the sparse reward example environment (MiniGrid), the structured action space example environment (GoBigger). We categorize these environments based on different attributes in Table 1, followed by a detailed description of each environment.\\n\\nBoard Games\\nThis types of environment includes TicTacToe, Connect4, Gomoku, Chess, Go, where uniquely marked boards and explicitly defined rules for placement, movement, positioning, and attacking are employed to achieve the game's ultimate objective. These environments feature a variable discrete action space, allowing only one player's piece per board position. In practice, algorithms utilize action mask to indicate reasonable actions.\\n\\nClassic Control\\nIn the reinforcement learning domain, classical control environments like Cartpole and Pendulum are built upon physical principles and provide explicit action and state spaces. They are commonly used as benchmark environments to evaluate the initial performance of reinforcement learning algorithms in discrete or continuous action spaces.\\n\\nBox2D\\nAll of these environments in this suite feature toy games centered around physics control (e.g., LunarLander and BipedalWalker), utilizing Box2D-based physics and PyGame-based rendering.\"}"}
{"id": "oIUXpBnyjv", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Their straightforward simulation mechanics and meaningful visual outputs have made them popular benchmarks for RL beginners. Moreover, they are suitable for visualizing the tree search process. Atari This category includes sub-environments like Pong, Qbert, Ms.Pacman, Breakout, UpNDown, and Seaquest. In these environments, agents control game characters and perform tasks based on pixel input, such as hitting bricks in Breakout. With their high-dimensional visual input and discrete action space features, Atari environments are widely used to evaluate the capability of reinforcement learning algorithms in handling visual inputs and discrete control problems. MuJoCo This category includes sub-environments such as Hopper-v3, HalfCheetah-v3, Walker2d-v3, and Humanoid-v3. MuJoCo is a precise and efficient physics simulation engine capable of simulating intricate digital mechanical systems, such as bipedal robots and robotic arms. These environments serve as testbeds for assessing the performance of reinforcement learning in control tasks like robot balancing or robotic arm object grasping. MuJoCo environments are characterized by continuous state and action spaces, and real-world physical laws, commonly used to evaluate the ability of reinforcement learning algorithms to handle continuous control and high-dimensional input problems. MiniGrid In addition to the difficulty levels of action and observation spaces, the sparsity or density of reward spaces is another major consideration. For example, MiniGrid series environments, like MiniGrid-Empty-8x8-v0 and MiniGrid-FourRooms-v0, provide simple and scalable sparse reward environments. In these environments, agents navigate in a grid world and complete various tasks. Sparse reward environments pose challenges to the exploration capabilities of reinforcement learning algorithms, as agents receive limited useful feedback most of the time. GoBigger GoBigger is a multi-agent reinforcement learning environment that emphasizes cooperation and competition. It features structured observation spaces where each agent is represented by one or more clone balls. The goal of the agents is to collide and merge with other balls within a limited time, thereby increasing their size. The observation space includes all unit information within the agent's local view, and rewards depend on the size difference between consecutive time steps. This environment offers diverse sub-environments for different tasks, such as t2p2, t3p2, t4p3, etc. Here, the number following t represents the number of teams, and the number following p indicates the number of agents per team. In our experiments, we set $t = p = 2$. 2048 is a numerical game in which players need to merge adjacent blocks with the same number in a 4x4 grid by swiping up, down, left, or right. The randomness of the game lies in the fact that after each swipe, a block with a value of 2 or 4 is randomly generated. Theoretically, there are up to 32 possibilities after each move, which poses challenges for model-based RL algorithms. For algorithms in the MCTS family, solving such problems requires modeling the randomness of the environment and considering various possibilities during the search process. Details about Main Experiment B.1 Benchmark Settings Environments In Table 1, we provide a categorization of common sequential decision-making environments, each presenting distinct challenges. These environments encompass the majority of the cases previously discussed in Section 3.2. We have undertaken a thorough and impartial evaluation of the algorithms incorporated into LightZero, across these varied environments. Algorithms In our main benchmark experiments, we assessed a series of algorithm variants, including AlphaZero, MuZero, MuZero w/ SSL (MuZero with Self-Supervised Learning Consistency Loss), EfficientZero, Sampled EfficientZero, Stochastic MuZero, and Gumbel MuZero. Note that in this context, MuZero w/ SSL represents the original MuZero algorithm augmented with the self-supervised loss proposed in [17]. EfficientZero refers to the MuZero algorithm enhanced with the self-supervised loss and value_prefix from [17]. We decided not to open the third improvement discussed in [17], as it was shown to have limited performance benefits and be highly time-consuming. Consequently, it was omitted from our primary benchmark experiments.\"}"}
{"id": "oIUXpBnyjv", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"EfficientZero is based on the previously described EfficientZero, incorporating sampling-related modifications from [45]. This enables the algorithm to simultaneously handle environments with discrete and continuous action spaces.\\n\\n**Metrics**\\n\\nIn all our experimental evaluations, the primary metric employed to gauge performance was the mean Return calculated over the evaluated episodes for each algorithm. Each algorithm was independently executed five times with unique seeds to ensure the integrity and reliability of our results. In the results graphs, the shaded regions illustrate the standard deviation in different runs, indicating the variability or spread of the runs under different seeds. On the other hand, the solid lines represent the mean, providing the average performance across the different runs.\\n\\nThe horizontal axis, labeled as Env Steps (environment steps), represents the number of steps taken in the environment during each run. The vertical axis represents the average Return over the evaluated 20 episodes, serving as a measure of the algorithm's effectiveness in each run.\\n\\n**B.2 Main Benchmark**\\n\\nIn this section, we provide a comprehensive benchmark results for LightZero. This encompasses baseline performances in discrete action spaces detailed in Section B.2.1, and continuous action spaces in Section B.2.2. In addition, we include comparative analyses of Gumbel MuZero and Stochastic MuZero, delineated in Sections B.2.3 and B.2.4 respectively. Furthermore, we extend our investigation to present the baseline evaluations in sparse reward scenarios like MiniGrid (Section B.2.5), and in multi-agent environments exemplified by GoBigger (Section B.2.6).\\n\\nFor a thorough examination and in-depth discussion of these benchmark results, we encourage readers to refer to Section 4.2, where we provide key observations and insights derived from our experiments.\\n\\n**B.2.1 Discrete Decision Benchmark**\\n\\nIn Figure 7, we present a comparative performance evaluation of four key algorithms\u2014MuZero, MuZero with SSL, EfficientZero, and Sampled EfficientZero\u2014across a selection of six representative environments from the classic Atari image input domain.\\n\\nFigure 8 showcases a performance comparison of AlphaZero and MuZero on two representative board games\u2014Connect4 and Gomoku (board_size=6). What stands out is that while AlphaZero's sample efficiency significantly surpasses that of MuZero, the latter still manages to produce satisfactory results, even in the absence of a simulator.\\n\\n**B.2.2 Continuous Control Benchmark**\\n\\nIn the upper of Figure 9, we present the performance of Sampled EfficientZero with different policy modeling approaches on the classical continuous benchmark environment, Pendulum-v1 and LunarlanderContinuous-v2. In the bottom of Figure 9, we present the performance of Sampled EfficientZero with different policy modeling approaches on the conventional continuous benchmark environment, MuJoCo, exemplified by Hopper-v3 and Walker2d-v3.\\n\\nThe dimensions of the action space for the four aforementioned environments are 1, 2, 3, and 6, respectively, with the number of discrete bins set to 11, 7, and 5. Consequently, the final dimensions of the factored categorical distributions [65] stand at $11^1$, $7^2$, $5^3$, and $5^6$, in the same order. Notably, the dimensionality of the discretized action space undergoes an exponential increase with the dimension of the original continuous action space. Under conditions of a fixed simulation cost, specifically with the number of sampled actions $K$ set to 20, and the number of simulations per search $\\\\text{sim}$ set to 50, we witness a steady decline in the performance of the factored policy representation as the dimensionality increases. Conversely, the Gaussian policy representation maintains a relatively stable performance, demonstrating its robustness in higher-dimensional spaces.\\n\\nHowever, even the Gaussian policy representation does not achieve the performance of model-free methods in MuJoCo. We speculate that this is due to the unique reward mechanism of the\"}"}
{"id": "oIUXpBnyjv", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 7: Performance comparison of algorithms integrated in LightZero on six representative image-based Atari environments. The horizontal axis represents Env Steps (environment steps), while the vertical axis indicates the average Return over 20 assessed episodes. In this context, **MuZero w/ SSL** denotes the original MuZero algorithm augmented with self-supervised loss. **EfficientZero** refers to the MuZero algorithm enhanced with self-supervised loss and value_prefix. **Sampled EfficientZero** builds upon EfficientZero with additional modifications related to sampling.\\n\\nFigure 8: Performance comparison of **AlphaZero** and **MuZero** on Connect4 and Gomoku (board_size=6). AlphaZero exhibits significantly higher sample efficiency compared to MuZero. This suggests that deploying AlphaZero directly could be advantageous when an environment simulator is readily available. However, even in the absence of a simulator, MuZero can still yield comparable results, which underscores its broad applicability.\"}"}
{"id": "oIUXpBnyjv", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 9: Upper: Performance comparison of Sampled EfficientZero utilizing different policy modeling techniques in continuous action space environments. Bottom: Performance comparison of Sampled EfficientZero using various policy modeling methods in MuJoCo continuous action space environments. Performance of the factored policy representation declines gradually with increasing action space size, while the Gaussian policy representation exhibits more stable performance.\\n\\nMuJoCo environment, where the optimal policy actions are likely extreme values of $-1$ or $1$ in most times. Sampling from a Gaussian distribution makes it challenging to obtain these extreme actions, which leads to suboptimal performance. To enhance performance in MuJoCo environments, a straightforward idea for future work is to augment extreme actions during the sampling process.\\n\\nB.2.3 Gumbel MuZero Benchmark\\n\\nIn Figure 10, we present the performance comparison of Gumbel MuZero and MuZero under different simulation costs (i.e. the number of simulations in one search) on Gomoku (board_size=6), Lunarlander-v2, PongNoFrameskip-v4 and Ms.PacmanNoFrameskip-v4. Original Monte Carlo Tree Search methods have to visit almost all the possible nodes to ensure stable optimization. The demands of high simulation counts have become the main time-consuming source of MCTS-style methods. In our benchmark, we validate that Gumbel MuZero can achieve notably better performance than MuZero when the number of simulations is limited. Besides, we don't tune the balance weight of the completedQ proposed in Gumbel MuZero, it may significantly influence the final performance in some environments, which maybe the reason why Gumbel MuZero shows lower episode return in the high simulation times case.\\n\\nB.2.4 Stochastic MuZero Benchmark\\n\\nIn Figure 11, we present a comparative performance analysis between Stochastic MuZero and MuZero, conducted within 2048 environments that exhibit intrinsic randomness. Specifically, our attention is focused on the environments typified by standard 2048 settings, where num_chances=2 (potential random tiles being {2, 4}), and a variant environment with increased stochasticity, num_chances=5 (potential random tiles are {2, 4, 8, 16, 32}). Stochastic MuZero exhibits a distinct performance advantage.\"}"}
{"id": "oIUXpBnyjv", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: Performance comparison of Gumbel MuZero and MuZero under varying simulation costs. Gumbel MuZero demonstrates significantly better performance than MuZero when the number of simulations is limited, showcasing its potential for designing low time-cost MCTS agents. For Gomoku (board_size=6), we evaluate sim={20, 10}. For LunarLander-v2, we assess sim={20, 10, 5}. For Atari Games, we examine sim={50, 16, 2}.\\n\\nFigure 11: Performance comparison between Stochastic MuZero and MuZero in 2048 environments with varying levels of chance (num_chances=2 and 5). In environments characterized by stochastic transition dynamics, Stochastic MuZero marginally surpasses MuZero. However, as the level of stochasticity escalates, the performance of Stochastic MuZero begins to face limitations. This implies that intricate randomness remains a substantial challenge for MCTS algorithms.\\n\\nB.2.5 MiniGrid Benchmark\\n\\nIn Figure 12, we present a performance comparison using the example environment MiniGrid-KeyCorridorS3R3-v0 from the MiniGrid suite. The figure on the left demonstrates the application of\"}"}
{"id": "oIUXpBnyjv", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 12: Left: Performance comparison of various exploration strategies applied to the MiniGrid-KeyCorridorS3R3-v0 environment (Return during the collection phase). The IntrinsicExploration strategy, which leverages curiosity mechanisms to explore the state space, demonstrates superior sample efficiency. Right: Performance comparison of algorithms implemented in LightZero on MiniGrid-KeyCorridorS3R3-v0. In environments characterized by high-dimensional vector observations and sparse rewards, self-supervised learning loss assists model alignment. However, predicting the value may pose challenges and potentially impede learning.\\n\\nB.2.6 Multi Agent Benchmark\\nCompared to single-agent environments, multi-agent environments confront more complex challenges, including, but are not limited to, joint state-action spaces, multi-dimensional optimization objectives, non-stationarity and credit assignment issues [66]. To tackle these challenges, our preliminary experiments in multi-agent environments mainly adopted the independent learning paradigm [67]. In this paradigm, we regard other agents as part of the environment, with each agent making decisions independently, and all agents sharing a common policy-value network. During the Monte Carlo Tree Search process, each agent also conducts searches independently.\\n\\nTo investigate the impact of the number of agents on algorithm performance, we conducted experiments in the GoBigger environment [23] using our custom T2P2 and T2P3 scenarios (all other environment parameters remained the same except for the number of agents (P)). As demonstrated in Figure 13, in both two scenarios, our algorithm can achieve stable convergence in confrontations with built-in bots, and its sample efficiency has improved about six-fold compared to non-MCTS methods. For comparison, in the T2P2 scenario, the MuZero algorithm requires approximately 400K Env Steps to reach a return of 150K, while algorithms such as MAPPO [68] necessitate about 3M Env Steps to achieve the same level of performance according to the paper [23]. This evidence proves that, aside from increased time expenditure, the performance of the MCTS algorithm is not significantly impacted when the number of agents is not very large. These initial attempts underscore the vast potential of sample-efficient MCTS-like algorithms in multi-agent environments.\\n\\nIn the future, we aim to provide baseline results of various algorithms integrated into LightZero on more multi-agent baseline environments (such as PettingZoo [69], MAMuJoCo [70]). Additionally, we plan to delve deeper into how to leverage the unique characteristics of multi-agent environments and multi-agent reinforcement learning (MARL) techniques such as Centralized Training Decentralized Control (CTDC) [71].\"}"}
{"id": "oIUXpBnyjv", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13: Performance comparison between MuZero and EfficientZero, both trained in the independent learning paradigm on the representative multi-agent environment GoBigger in T2P2 and T2P3 scenarios. Both algorithms demonstrate the ability to achieve stable convergence when pitted against built-in bots, and their sample efficiency showcases about a six-fold improvement over other non-MCTS methods [23].\\n\\nC Details about Two Case Studies\\n\\nC.1 Exploration Mechanism in MCTS\\n\\nC.1.1 Details about Motivation\\n\\nStriking the right balance between exploration and exploitation [26] [27] [28] is a fundamental challenge in reinforcement learning. For the MCTS algorithms integrated into LightZero, determining the optimal balance is also crucial. However, there has been limited research on the performance of MCTS algorithms in sparse reward environments. Recently, [72] investigated the combination of adversarial imitation learning and MCTS in the DeepMind Control Suite [59]; however, they did not delve into performance in sparse reward environments like MiniGrid [44].\\n\\nOur preliminary experiments revealed that in some simple sparse reward environments in MiniGrid, MuZero struggles to make substantial progress within 1M steps. We observed that while board games also exhibit sparse rewards, each game has a clear win/draw/loss outcome, providing a supervisory signal for assessing the quality of the game state. In contrast, in other sparse reward environments like MiniGrid, non-zero rewards are only obtained upon reaching the goal, with all other states yielding zero rewards. This makes it challenging to collect trajectories with supervisory signals. As searching in some intermediate states of MiniGrid may be of limited value due to zero rewards, we hypothesize that increasing the number of simulations may not significantly improve performance.\\n\\nIn Table 2, we provide a comprehensive comparison of exploration strategies employed in MCTS algorithms as part of the LightZero framework. In this study, our main emphasis is on the general exploration strategies applied to decision-making algorithms. As for the unique exploration parameters specific to MCTS, previous research has predominantly used default values, which are likely near-optimal settings. A detailed examination of these unique parameters falls outside the scope of our current work and will be addressed in future research efforts.\\n\\nC.1.2 Details about Settings\\n\\nWe performed experiments in the MiniGrid environment, focusing on the MiniGrid-KeyCorridorS3R3-v0 and MiniGrid-FourRooms-v0 scenarios. We set up the two environments with a maximum step limit of 300 and used the Adam optimizer with a learning rate of $3 \\\\times 10^{-3}$. The other parameter settings are consistent with those in Table 7.\"}"}
{"id": "oIUXpBnyjv", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Here, we provide detailed explanations of the various enhanced exploration strategies used in Figure 5 of Section 5.1.\\n\\n- **Naive**: This is the default exploration strategy, which includes a manual temperature decay mechanism. The initial temperature is 1, and the temperature decays twice during training, to 0.5 and 0.25, respectively. The decay timings are controlled by a parameter. At $50\\\\%$ of this parameter's value multiplied by the training steps, the temperature decays to 0.5. At the $75\\\\%$ point, the temperature decays to 0.25. Afterward, the temperature remains fixed at 0.25.\\n\\n- **NaiveDoubleSimulation**: Same configuration as Naive except use double number of simulations.\\n\\n- **FixedTemperature-1**: A fixed temperature parameter was utilized throughout the training period, with the specific value set to $x=1$.\\n\\n- **PolicyEntropyRegularization-x**: An additional policy entropy regularization term was added to the original loss of MuZero [14] algorithm, with policy entropy loss weights of 0.05 or 0.005.\\n\\n- **EpsGreedy**: Inspired by common exploration settings in value-based RL methods [4], actions were selected uniformly with probability $\\\\epsilon$, and with probability $1-\\\\epsilon$ with the argmax operation.\\n\\n- **IntrinsicExploration**: An additional fixed target network and predictor network were introduced. The predictor network was employed to predict the output of the target network, and the normalized MSE loss was used to obtain the intrinsic reward. For specific details, please refer to Section C.1.3.\\n\\n**C.1.3 Details about Intrinsic Exploration**\\n\\nThe core idea of intrinsic rewards is to encourage the agent to visit novel states as much as possible throughout the learning process or from a life-long perspective. In theory, any long-term novelty estimator can be used to generate this incentive. The RND (Random Network Distillation) [27] algorithm demonstrates good performance, is easy to implement, and can be parallelized, so we use it as an example to generate intrinsic rewards, denoted as $r_i$. \\n\\n- Adding Dirichlet noise at the root node (also known as the decision node).\\n- Balancing the weight of prior probability $P$ and MCTS-derived $Q$ estimates at intermediate nodes using the PUCT formula [31].\\n- The alpha value in the Dirichlet distribution is 0.3. The noise weight is 0.25.\\n- The base constant $c_1=1.25$, the initialization constant $c_2=19652$.\\n\\n**General Exploration Strategies in RL**\\n\\n- **Data collection phase**: Decaying temperature coefficient for visit count distribution.\\n- Epsilon-greedy exploration strategy.\\n\\n- **Learning phase**: Policy entropy regularization.\\n- Intrinsic exploration, e.g., NGU [28].\\n- Imitation learning techniques, e.g., Efficient Imitate [72].\\n\\n- The temperature transitions from 1 to 0.5 and then to 0.25 in a fixed training steps.\\n- By default, not used.\\n- Default policy entropy loss weight is 0.\\n\\n**Table 2: Overview of Exploration Strategies in MCTS algorithms implemented in LightZero**\\n\\n| Strategy                                      | Description                                                                 |\\n|-----------------------------------------------|-----------------------------------------------------------------------------|\\n| Naive                                         | Default exploration strategy with manual temperature decay mechanism.        |\\n| NaiveDoubleSimulation                         | Same as Naive except double number of simulations.                           |\\n| FixedTemperature-1                           | Fixed temperature parameter throughout training, $x=1$.                      |\\n| PolicyEntropyRegularization-x                | Additional policy entropy regularization term added to MuZero loss.          |\\n| EpsGreedy                                     | Exploration strategy inspired by value-based RL methods, $\\\\epsilon$ selection.|\\n| IntrinsicExploration                          | Additional fixed target and predictor networks for intrinsic reward.         |\\n\\nHere, we provide detailed explanations of the various enhanced exploration strategies used in Figure 5 of Section 5.1.\\n\\n- **Naive**: This is the default exploration strategy, which includes a manual temperature decay mechanism. The initial temperature is 1, and the temperature decays twice during training, to 0.5 and 0.25, respectively. The decay timings are controlled by a parameter. At $50\\\\%$ of this parameter's value multiplied by the training steps, the temperature decays to 0.5. At the $75\\\\%$ point, the temperature decays to 0.25. Afterward, the temperature remains fixed at 0.25.\\n\\n- **NaiveDoubleSimulation**: Same configuration as Naive except use double number of simulations.\\n\\n- **FixedTemperature-1**: A fixed temperature parameter was utilized throughout the training period, with the specific value set to $x=1$.\\n\\n- **PolicyEntropyRegularization-x**: An additional policy entropy regularization term was added to the original loss of MuZero [14] algorithm, with policy entropy loss weights of 0.05 or 0.005.\\n\\n- **EpsGreedy**: Inspired by common exploration settings in value-based RL methods [4], actions were selected uniformly with probability $\\\\epsilon$, and with probability $1-\\\\epsilon$ with the argmax operation.\\n\\n- **IntrinsicExploration**: An additional fixed target network and predictor network were introduced. The predictor network was employed to predict the output of the target network, and the normalized MSE loss was used to obtain the intrinsic reward. For specific details, please refer to Section C.1.3.\\n\\n**C.1.3 Details about Intrinsic Exploration**\\n\\nThe core idea of intrinsic rewards is to encourage the agent to visit novel states as much as possible throughout the learning process or from a life-long perspective. In theory, any long-term novelty estimator can be used to generate this incentive. The RND (Random Network Distillation) [27] algorithm demonstrates good performance, is easy to implement, and can be parallelized, so we use it as an example to generate intrinsic rewards, denoted as $r_i$. \\n\\n- Adding Dirichlet noise at the root node (also known as the decision node).\\n- Balancing the weight of prior probability $P$ and MCTS-derived $Q$ estimates at intermediate nodes using the PUCT formula [31].\\n- The alpha value in the Dirichlet distribution is 0.3. The noise weight is 0.25.\\n- The base constant $c_1=1.25$, the initialization constant $c_2=19652$. \\n\\n**General Exploration Strategies in RL**\\n\\n- **Data collection phase**: Decaying temperature coefficient for visit count distribution.\\n- Epsilon-greedy exploration strategy.\\n\\n- **Learning phase**: Policy entropy regularization.\\n- Intrinsic exploration, e.g., NGU [28].\\n- Imitation learning techniques, e.g., Efficient Imitate [72].\\n\\n- The temperature transitions from 1 to 0.5 and then to 0.25 in a fixed training steps.\\n- By default, not used.\\n- Default policy entropy loss weight is 0.\\n\\n**Table 2: Overview of Exploration Strategies in MCTS algorithms implemented in LightZero**\\n\\n| Strategy                                      | Description                                                                 |\\n|-----------------------------------------------|-----------------------------------------------------------------------------|\\n| Naive                                         | Default exploration strategy with manual temperature decay mechanism.        |\\n| NaiveDoubleSimulation                         | Same as Naive except double number of simulations.                           |\\n| FixedTemperature-1                           | Fixed temperature parameter throughout training, $x=1$.                      |\\n| PolicyEntropyRegularization-x                | Additional policy entropy regularization term added to MuZero loss.          |\\n| EpsGreedy                                     | Exploration strategy inspired by value-based RL methods, $\\\\epsilon$ selection.|\\n| IntrinsicExploration                          | Additional fixed target and predictor networks for intrinsic reward.         |\"}"}
{"id": "oIUXpBnyjv", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Original MSE loss\\n\\nSpecifically, it utilizes two neural networks: (1) Random Network $g: \\\\mathbf{O} \\\\rightarrow \\\\mathbf{R}_k$, a fixed network with randomly initialized parameters that takes the observed observation $x_t$ as input and outputs its encoding $g(x_t)$. (2) Prediction Network $\\\\hat{g}: \\\\mathbf{O} \\\\rightarrow \\\\mathbf{R}_k$, a network that takes the observation $x_t$ as input and outputs the predicted value $\\\\hat{g}(x_t; \\\\theta)$ for the observation encoding $g(x_t)$. The networks are trained on the data collected by the agent using stochastic gradient descent, updating the parameters $\\\\theta$ by minimizing the mean squared error $\\\\text{err}(x_t) = \\\\|\\\\hat{g}(x_t; \\\\theta) - g(x_t)\\\\|^2$. The fundamental concept of this approach is to leverage the neural networks' ability to model dataset distributions in a manner akin to supervised learning. A larger prediction error for a particular observation suggests that the agent has visited the surrounding observations in the observation space less frequently, indicating a higher degree of novelty for that observation.\\n\\nReward normalization\\n\\nTo mitigate the impact of significant magnitude variations in the mean squared error across training and different environments, the final intrinsic reward is defined using min-max normalization. The combined reward at time $t$ is then defined as:\\n\\n$$r_t = r_{et} + \\\\beta r_{it},$$\\n\\nwhere $r_{et}$ denotes the external reward provided by the environment at time $t$, $r_{it}$ represents the normalized intrinsic reward generated by the exploration module, and $\\\\beta$ is a positive number serving as the weight factor for the intrinsic reward. In this paper, we set $\\\\beta$ to $\\\\frac{1}{300}$ to ensure that the sum of intrinsic rewards in a single episode is less than the maximum original external reward of 1, thus preventing intrinsic rewards from dominating the original objective.\\n\\nKey designs\\n\\nIn the specific implementation, a critical aspect is the selection of the input $x_t$. In our preliminary experiments, we used the latent state from the MuZero model but observed subpar performance. We hypothesize that this is due to the distribution of latent states changing throughout the training process, causing the target values obtained by the random network to continuously shift and resulting in intrinsic rewards that essentially resemble noise. Ultimately, we chose to use the environment's original observation $o_t$ as the input observed state and achieved satisfactory results.\\n\\nIt is worth mentioning that combining efficient exploration methods, such as intrinsic rewards, with the unique MCTS exploration strategies detailed in Table 2, represents an intriguing research direction. We leave this integration as a potential area for future investigation.\\n\\nC.2 Alignment in Environment Model Learning\\n\\nAlignment\\n\\nIn this section, we examine case studies centered on the essential objective functions. Drawing inspiration from the paper [25], the importance of alignment in training the representation, policy and model is emphasized. The policy should only access accurate model states, while the representation must encode task-relevant and predictable information. The consistency loss proposed in [17] serves as a candidate technique: at each unroll step, the latent state generated by the dynamic model should be aligned (similar/consistent) to the latent state directly obtained from the original observation via the representation network.\\n\\nSetting\\n\\nTo investigate the impact of self-supervised consistency loss on different types of observation data, we use the MuZero algorithm as our baseline. The Adam optimizer is employed with a learning rate of $3 \\\\times 10^{-3}$. The notation \\\"MuZero w/ SSL\\\" signifies that the consistency loss weight is 2, while \\\"MuZero\\\" means the consistency loss weight is 0. The other hyperparameters remain the same as those described in Section G.4.\\n\\nSpecial case in board games\\n\\nAs shown in Figure 14, for special image input environments (checkerboard with discrete values) like TicTacToe, the cosine similarity relatively low, around 0.662. This highlights the challenge of achieving consistency between the latent state output from the dynamic model and the latent state derived from the observation via the representation network. Since adjacent observations differ only in one position, adding consistency loss with inappropriate optimization settings might hinder the learning progress of other parts of the algorithm. To verify this, we conducted an experiment on TicTacToe using the SGD optimizer and compared the performance with and without the consistency loss, as shown in the right panel of Figure 14. We observed that the addition of consistency loss hampers the learning progress in the early stages, which further supports our conjecture. That is to say, our experiments demonstrate that the consistency loss proposed in\"}"}
{"id": "oIUXpBnyjv", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 14: Left: The change of cosine similarity (i.e. negative consistency loss, \\\\( \\\\cos = 1.0 \\\\) indicates that the two vectors have the same direction.) for three input types throughout the MuZero training. In special image input environments such as TicTacToe, characterized by checkerboard patterns with discrete values, the cosine similarity between the latent state output from the dynamic model and the latent state derived from the observation via the representation network remains relatively low. This emphasizes the challenge of attaining consistency between these two latent states.\\n\\nRight: Effects of self-supervised consistency loss on board games when using SGD [73] optimizer with momentum. The inclusion of consistency loss impedes the learning progress during the initial stages. [17] is suitable only for environments with specific attributes. For board games, devising a suitable alignment constraint to ensure consistent model learning remains a topic for future research.\\n\\nD Explanation for Figure 2\\n\\nIn this section, we will introduce the score rules in Figure 2. We categorize the critical capabilities of general decision solvers into the following dimensions: multi-modal observation space, complex action space, inherent stochasticity, reliance on prior knowledge, simulation cost, hard exploration, and data efficiency. We then compare four algorithms including Model-free RL (e.g. PPO), AlphaZero, MuZero, and LightZero. Please note that in this section, the term LightZero refers to the algorithm that embodies the optimal combination of techniques and hyperparameter settings within our framework.\\n\\nWe will elaborate on the specific interpretation of each algorithm depicted in Figure 2.\\n\\nModel-free RL (PPO)\\n\\nWe use the standard on-policy PPO implementation and DI-engine [74] benchmark results. This implementation includes optimized hyper-parameters and incorporates various techniques to enhance performance across different environments. The list of typical hyperparameters for PPO is presented in Table 3.\\n\\n| Hyperparameter Value |\\n|----------------------|\\n| Epoch per collect    | 10       |\\n| Num of samples per collect | 3200    |\\n| Batch size           | 320      |\\n| Discount factor      | 0.99     |\\n| GAE lambda           | 0.95     |\\n| Recompute advantage  | True     |\\n| Entropy weight       | 0.001    |\\n| Dual clip            | True     |\\n| Gradient norm        | 0.5      |\\n| PopArt               | True     |\\n\\nTable 3: Key hyperparameters of model-free PPO methods.\\n\\nAlphaZero\\n\\nWe use the default implementation of AlphaZero provided by LightZero, following the settings from the original AlphaZero paper [29].\"}"}
{"id": "oIUXpBnyjv", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We employ the default implementation of MuZero provided by LightZero, following the settings described in the original MuZero paper [14] and the public codebase MuZero-General [51].\\n\\nWe have incorporated the relevant MCTS/MuZero algorithm techniques, each tailored for diverse environments with unique challenges. This integration has given rise to a specialized algorithm version, dubbed LightZero, within our framework.\\n\\nThe score assigned to each algorithm in the distinct dimensions indicates their performance and applicability. A score of 1 suggests poor performance and limited applicability, while a higher score implies a broader application scope and better performance. That is to say, a score of 2 means that this algorithm can deal with some parts of envionments or problems in this dimension. A score of 3 means that it can tackle more than a half of issues while 4 indicates it has already been capable of many challenges but there is still some improved space. The highest score 5 is assigned to those methods that could be state-of-the-art results in this dimension or don't need to care about this problem. We conduct a qualitative analysis to compare the following algorithm versions.\\n\\n**Multi-modal Observation Space**\\n\\nAlphaZero is customized to model the 2D image-like observation in board games, receiving a score of 2. MuZero extends this capability and is able to handle more general 2D image observations including video games. Model-free RL methods and LightZero utilize extra self-supervised learning representation learning techniques, enabling them to work effectively with high-dimensional and complex states. However, challenges still exist in learning abstract decision behaviors within multi-modal representations.\\n\\n**Complex Action Space**\\n\\nIn terms of action spaces, AlphaZero is limited to discrete actions in board games. MuZero extends it to various discrete action spaces. Model-free RL methods are also able to handle both discrete and continuous actions. However, in more complex hybrid action space, these methods often require special mechanisms, such as the autogressive action prediction used in AlphaStar [3]. Sampled MuZero represents a significant endeavor to implement MCTS in continuous action spaces, employing finite action samples as the primary basis for action selection. However, when the number of samples is insufficient, performance may suffer due to inadequate approximation. Despite this, LightZero, which incorporates this sampling mechanism, achieves a score of 4, effectively showcasing its ability to address these challenges.\\n\\n**Inherent Stochasticity**\\n\\nThe stochasticity of environment dynamics poses a significant challenge for RL algorithms. Conventional MCTS algorithms face planning issues due to inconsistent state transitions. Value-based RL methods, such as DQN, often face difficulty adapting to variable reward signals for identical actions within a particular state, leading to suboptimal performance. Actor-Critic algorithms like PPO somewhat mitigate this issue. LightZero adopts insights from [16] and [75], employing an enhanced model that simultaneously learns the distribution of latent chances for various scenarios. These learned probabilities are subsequently incorporated into the tree search nodes. While these adjustments have bolstered MCTS's ability to handle inherent dynamics randomness and the partially observable state, the challenge remains substantial when addressing real-world decision-making scenarios.\\n\\n**Reliance on Prior Knowledge**\\n\\nOne key limitation of AlphaZero is its reliance on prior knowledge of environments, such as the perfect simulator or game rules and records. MuZero addresses this limitation by designing a dynamics model that learns the reward and transition function, significantly relaxing this requirement. For model-free RL methods and LightZero, they are more flexible but still require some prior-oriented operations like observation pre-processing and reward shaping, to transform the original problem into a more standard MDP. Therefore, a score of 4 is suitable for them.\\n\\n**Simulation Cost**\\n\\nAlthough MCTS-style methods show great data efficiency, it is important to acknowledge that the wall-time of related training programs can be astonishing due to the simulation cost. These methods necessitate the establishment of comprehensive search trees to assure adequate visit counts and precise value estimation. LightZero, by integrating core ideas from the Gumbel MuZero paper and utilizing other optimization techniques, effectively minimizes this overhead. However, due to its inherent model-based nature, it still demands a longer runtime compared to model-free RL methods.\"}"}
{"id": "oIUXpBnyjv", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tree-search-based planning methods enhance exploration capabilities by reducing the search space for the optimal policy. Conversely, traditional model-free RL methods often struggle in sparse reward environments without specific techniques. AlphaZero gains a slight advantage in this dimension by utilizing a perfect simulator rather than models that are incrementally trained over time. LightZero incorporates additional exploration strategies at minimal cost and successfully tackles the hard exploration task efficiently (such as MiniGrid).\\n\\nData Efficiency\\nMCTS-style methods excel in data efficiency compared to model-free RL methods, offering impressive performance on academic benchmarks like Atari-100K and DMControl-500K. LightZero implements specialized data sampling with staleness and reuse limitation and management mechanisms (e.g., throughput limiter) to further improve data efficiency.\\n\\nEfficiency Analysis in LightZero\\nThis section first lists the computational hardware configurations of all the experiments. For nearly all basic experiments and ablations, we aim to validate the runtime efficiency on a small resource budget, thus we deploy each experiment instance on the Kubernetes cluster with computational resource of 1 A100 40G GPU, 24 CPU cores and 100GB RAM. Given these settings, LightZero can train Atari agents for 100K steps in 4 hours, and it can conduct 100K steps of self-play on Gomoku in 5 hours. Furthermore, we profile the entire training pipeline of these tiny instances. Based on these analysis, we can identify several bottlenecks in the runtime of the MCTS/MuZero training pipeline and found some practical, albeit imperfect, solutions to these bottlenecks.\\n\\nEnvironment Latency\\nRL environments usually vary in the execution time of reset and step methods. For AlphaZero-style methods, environment\u2019s step method are called many times (e.g., more than 50 in one action selection) and lead to the huge time cost. Particularly, AlphaZero deployed on board games may spend more than a half of the whole training time in environments. In practice, we use the vmap and jit mechanisms provided by JAX and the LRU (Least Recently Used) cache trick to accelerate these methods. Thanks to many for-loop and duplicate operations in classic board game implementations, we can significantly reduce the associated overhead and efficiently implement self-play training.\\n\\nTree Search\\nAlthough the tree search process doesn\u2019t contain many complex mathematical operations, it is still a non-neglected component in the efficiency optimization. Moreover, the naive vectorized environment scheme shows no obvious gain in tree search methods because each environment needs a unique search tree, which is not suitable to use the batch processing to speed up. On the other hand, due to the large number of call times, some basic python primitives like getattr and setattr have become a drawback of this module, thus LightZero implements a variety of tree search operations in MCTS/MuZero algorithm variants with cpp and Cython extension for Python, and it offers obvious improvements in various fine-grained code blocks.\\n\\nModel Inference\\nAs MuZero-style methods utilize a learned model in place of the perfect simulator, they spend more time on the inference procedure of neural network models. To tackle the efficiency issues associated with more complex network architectures and larger network parameters, LightZero leverages various tools from the open-source community, including some features of PyTorch 2.0, some tricks about mixed precision training and large batch training in RL.\\n\\nParallel MCTS\\nUtilizing a large CPU cluster and enabling multiple parallel data collectors has proven successful in many decision-making tasks. When it comes to MCTS-style methods, due to the difficulty mentioned in tree search part, previous methods mainly use the collector-parallel scheme, each collector is composed of an environment, a search tree and a RL agent. Naive batch processing inside the collector can lead to significant waiting time between the three components: env step time, model inference time and tree search time (shown in Figure 15(d)), which severely underutilized computational resources. As suggested in Figure 15(e), LightZero launches multiple (k) environments and search trees in a collector, but divides these modules into several groups, the number of group should be determined by three concrete time counts for a specific environment.\"}"}
{"id": "oIUXpBnyjv", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 15: Comparison of various data collection pipelines. a), b), c) illustrate simple serial pipelines for different types of algorithms. d) and e) depict two distinct parallel MCTS schemes, the former will cause the obvious waiting time while the latter can utilize all types of computation resources as much as possible, which makes three different parts overlapped with each other.\\n\\nSpecifically, data collector alternate between different groups to execute corresponding steps. For example, even when the first group of environments has completed its steps, the corresponding CPUs are not idle as they can execute environment simulation steps for the second group. Correspondingly, RL agents can use the similar strategies to improve the utilization on GPU. Although the ideal case illustrated in Figure 15(e) is challenging to achieve perfectly, this grouping and buffering mechanism can significantly enhance the parallel capabilities of MCTS in practical data collection.\\n\\nMulti-GPU Training\\n\\nLeveraging the Distributed Data Parallel module from PyTorch, we've implemented multi-GPU training capabilities into the LightZero codebase. This functionality has been rigorously tested with the EfficientZero model within the Atari PongNoFrameskip-v4 environment, using 1, 2, and 4 GPUs. The results, depicted in Table 4, demonstrate that by training with 4 GPUs, we observe an approximate five-fold increase in speed, while maintaining similar levels of performance. Consequently, with access to additional GPU computing resources, we anticipate an even greater acceleration of the training process.\\n\\nReanalyze\\n\\nFrom the viewpoint of system design, data reanalyze is the most special component in MuZero-style methods. SpeedyZero [34] explores the necessity and best practice of separate reanalysis nodes in distributed training. They use different model replicas to compute priority for sampling and new targets for training. LightZero optimizes the cost of reanalyze modules with a\"}"}
{"id": "oIUXpBnyjv", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Approximate training times for EfficientZero on the PongNoFrameskip-v4 environment with different numbers of GPUs. The results show that training with 4 GPUs can improve the speed by about 5 times with similar performance, which is in line with our expectations.\\n\\nSimple yet effective selection mechanism: only the data with high training potential (e.g., appropriate state-action novelty and high temporal difference error) and sufficient training stability require a high-frequency reanalysis ratio; other data can undergo fewer reanalysis operations to save time.\\n\\nCommunication: Transporting model and data is two main communications in the distributed RL training. To ensure the stable convergence of agents, the entire training pipeline cannot collect excessive data or train the network too many times. The former shows few performance gain or even lead to some harmful training batch, while the latter can result in severe over-fitting. According to the actual speed of sending models (from agent learner to data collector and data arranger) and sending collected data, LightZero designs a throughput limiter to control the ratio between generating new data and sample training batch. By using a fixed batch size, we can adjust the ratio within a reasonable range (such as 0.8x-1.2x of original settings). Besides, there is a unnecessary dataflow of the RL training pipeline, which is described in Figure 16. In classic RL frameworks, the latest generated data in data collector is located in the inference GPU, however, it has to go through many steps to become serialized bytes data and then it is sent to agent learner by normal communication techniques like socket. To address this problem, we utilize the RDMA (Remote Direct Memory Access) technique to send the training data from inference GPU to training GPU directly. With the aid of this improvement, LightZero can expedite data collection in the P2P (Peer-to-Peer) communication mechanism, bypassing a series of previously complicated operations.\\n\\nFigure 16: Comparison of various algorithms during the data collection phase. Utilizing Peer-to-Peer (P2P) communication with the Remote Direct Memory Access (RDMA) technique significantly reduces the time required in the data collection process, bypassing the complex operations typically employed in conventional communication methods.\"}"}
{"id": "oIUXpBnyjv", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Data Collector\\n\\nMuZero is representative of online Reinforcement Learning (RL) algorithms, demanding substantial interactions between the agent and the environment to amass training data. Within this sub-module, we leverage a vectorized environment manager and a deeply optimized batch-parallel search tree, facilitated by Cython/C++ extensions. These elements work collaboratively to ensure a high-throughput data collection process.\\n\\nData Arranger\\n\\nThe function of the Data Arranger sub-module is to maximize the utility of different stale data. In the context of MuZero, the first step entails storing complete trajectories or episodes in a prioritized replay buffer to maintain their temporal sequence. Subsequently, we institute a priority recomputer mechanism to periodically determine the sampling priority of the stored data. The calculated priority is proportional to the likelihood of the data being sampled for training. Furthermore, to rectify any off-policy bias and enhance the stability of training, a data reanalyzer updates the target value stored in data, utilizing the latest network parameters. Recognizing the intricate \u201cproducer-consumer\u201d relationship between the Data Collector and Agent Learner, MuZero incorporates a throughput limiter to monitor the number of push/pop data operations and regulate the allocation of computational resources.\\n\\nAgent Learner\\n\\nThe Agent Learner sub-module amalgamates an array of deep learning and reinforcement learning techniques to train a set of neural networks as defined in MuZero. Components of this sub-module include the distributional RL module, which models the inherent randomness of environmental reward, and the data parallel and mixed precision training utilities that expedite per-iteration time. These features can be conveniently enabled or disabled via the corresponding configuration fields.\\n\\nAgent Evaluator\\n\\nThroughout the training process, MuZero necessitates the evaluation of the performance of the newly trained network. The Agent Evaluator sub-module encompasses several evaluation strategies such as low-temperature sampling and beam search to enhance results. This sub-module also employs a broad spectrum of metrics and visualizations for agents.\\n\\nContext Exchanger\\n\\nTo achieve asynchronous execution across four sub-modules and to efficiently scale the entire training pipeline, we employ a Context Exchanger. This component incorporates novel communication elements to facilitate the efficient transfer of necessary context information.\\n\\nBy dissecting the implementation of the MuZero algorithm into these sub-modules, we demonstrate the flexibility and modularity of LightZero, and how it can be effectively employed in the design and execution of complex RL algorithms.\\n\\nG Details about Algorithm Implementation\\n\\nIn this section, we first provide an overview of the subsequent algorithm extensions of MuZero, followed by a comprehensive algorithm overview diagram illustrating the implementation of various integrated algorithms in LightZero (e.g., Figure 19). Next, we introduce the model's network architecture, including the representation, dynamics, and prediction network. Finally, we present the hyperparameter settings for each algorithm and environment used in the baseline results.\\n\\nG.1 MuZero's Extensions\\n\\nIn recent years, MuZero has been extended through various algorithmic innovations to enhance its efficiency and stability across different scenarios. These extensions mainly include:\\n\\nSampled MuZero\\n\\nThis approach introduces a general framework called the sample-based policy iteration, which can be theoretically applied to any types of action spaces. The core idea is to compute improved policies within a subset of the original action space. As the number of sampled actions gradually increases towards the size of the entire action space, the sampled improved policy converges probabilistically to the improved policy over the entire action space.\\n\\nGumbel MuZero\\n\\nis an extension designed to enhance performance in environments with low simulation costs by leveraging the Gumbel-Top-k trick to select actions that guarantee policy...\"}"}
{"id": "oIUXpBnyjv", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LightZero: A Unified Benchmark for Monte Carlo Tree Search in General Sequential Decision Scenarios\\n\\nYazhe Niu, Yuan Pu, Zhenjie Yang, Xueyan Li, Tong Zhou, Jiyuan Ren, Shuai Hu, Hongsheng Li, Yu Liu\\n\\n1 SenseTime Group LTD\\n2 Shanghai Artificial Intelligence Laboratory\\n3 The Chinese University of Hong Kong\\n4 Centre for Perceptual and Interactive Intelligence\\n\\nAbstract\\n\\nBuilding agents based on tree-search planning capabilities with learned models has achieved remarkable success in classic decision-making problems, such as Go and Atari. However, it has been deemed challenging or even infeasible to extend Monte Carlo Tree Search (MCTS) based algorithms to diverse real-world applications, especially when these environments involve complex action spaces and significant simulation costs, or inherent stochasticity. In this work, we introduce LightZero, the first unified benchmark for deploying MCTS/MuZero in general sequential decision scenarios. Specifically, we summarize the most critical challenges in designing a general MCTS-style decision-making solver, then decompose the tightly-coupled algorithm and system design of tree-search RL methods into distinct sub-modules. By incorporating more appropriate exploration and optimization strategies, we can significantly enhance these sub-modules and construct powerful LightZero agents to tackle tasks across a wide range of domains, such as board games, Atari, MuJoCo, MiniGrid and GoBigger. Detailed benchmark results reveal the significant potential of such methods in building scalable and efficient decision intelligence. The code is available as part of OpenDILab at https://github.com/opendilab/LightZero.\\n\\n1 Introduction\\n\\nGeneral decision intelligence needs to solve tasks in many distinct domains. Recent advances in reinforcement learning (RL) algorithms have addressed several challenging decision-making problems [1, 2] and even surpassed top-level human experts in performance [3]. However, these state-of-the-art RL agents often exhibit poor data efficiency and face significant challenges when handling a wide range of diverse problems. Different environments present specific learning requirements and difficulties that prompted currently various algorithms (e.g. DQN [4], PPO [5], R2D2 [6], SAC [7]) and system architectures such as IMPALA [8] and others [9, 10, 11]. Designing a general and data-efficient decision solver needs to tackle various challenges, while ensuring that the proposed algorithm can be universally deployed anywhere without domain-specific knowledge requirements.\\n\\nMonte Carlo Tree Search (MCTS) is a powerful approach that utilizes a search tree with simulation and backpropagation mechanisms to train agents with a small data budget [12]. To model high-dimensional observation spaces and complex policy behaviour, AlphaGo [13] enhances MCTS with deep neural networks and designs the policy and value network that identify optimal actions and winning rates respectively, which was the first to defeat the strongest professional human player in Go. Despite the impressive results, MCTS-style algorithms rely on a series of necessary conditions, such as knowledge of game rules and simulators, discrete action space and deterministic state transition, which severely restrict the application scope of these methods. In recent years, several successors\\n\\n*Corresponding Author\\n\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\"}"}
{"id": "oIUXpBnyjv", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to AlphaGo have attempted to extend its capabilities in various directions. MuZero [14] relaxes the requirements for prior knowledge of environments by training a set of neural networks to reconstruct reward, value and policy. Sampled MuZero [15] successfully applies MCTS to various complex action space with a novel planning mechanism based on sampled actions. [16, 17, 18] improve MuZero in terms of planning stochasticity, representation learning effectiveness and simulation efficiency respectively. These emerging algorithm insights and techniques have contributed to the development of more general MCTS algorithms and toolchains.\\n\\nIn this paper, we present a unified algorithm benchmark named LightZero that first comprehensively integrates different MCTS/MuZero algorithm branches, including 9 algorithms and more than 20 decision environments with detailed evaluation. To better understand the potential of MCTS as an efficient general-purpose sequential decision solver, we revisit the development history of MCTS methods [19] and the diverse criterions of newly proposed RL environments [20, 21, 22]. As shown in Figure 2, we outline the six most challenging dimensions in developing LightZero as a general method, including multi-modal and high-dimensional observation space [23], complex action space, reliance on prior knowledge, inherent stochasticity, simulation cost, and hard exploration.\\n\\nFurthermore, highly coupled algorithm and system architectures greatly increase the cost and barriers of migrating and improving MCTS-style methods. Some special mechanisms like tree search and data reanalyze [24] seriously hinder the simplification and parallel acceleration of code implementation. To overcome these difficulties, LightZero designs a modularly pipeline to enable distinct algorithm components as plug-ins. For example, the chance node planning for modelling stochasticity can also be used in continuous control or hybrid action environments. From the unified viewpoint provided by LightZero, we can systematically divide the whole training scheme of MCTS-style methods into four sub-modules: data collector, data arranger, agent learner, and agent evaluator. LightZero's decoupled architecture empowers developers to focus intensively on the customization of environments and algorithms. Meanwhile, some techniques like off-policy correction and data throughput limiter can ensure the steady convergence of the algorithm while achieving runtime speedups.\\n\\nBased on these supports, LightZero also explores the advantages of combining some novel insights from model-based RL with MCTS approaches. In particular, the misalignment problem [25] of state representation learning and dynamics learning can result in the problematic optimization for MuZero, thus a simple self-consistency loss can significantly speed up convergence without special tuning. Besides, intrinsic reward mechanism [26] [27] [28] can address the exploration deficiency of tree-search methods with hand-crafted noises. Subsequently, we evaluate the ability of LightZero as a general solver for various decision problems. Experiments on different types of environments demonstrate LightZero's rich application ranges and data efficiency regimes with few hyper-parameter adjustments. At last, we provide discussions on the future optimization directions of each sub-module.\\n\\nIn general, we summarize the three key contributions of this paper as follows:\\n\\n\u2022 We present LightZero, the first general MCTS/MuZero algorithm benchmark that systematically evaluates related algorithms and system designs.\\n\\n\u2022 We outline the most critical challenges of real-world decision applications. To address these issues, we decouple the algorithm and system design of MCTS methods and design a modular training pipeline, which can easily integrate novel insights for better scalability.\\n\\n\u2022 We demonstrate the capability and future potential of LightZero as a general sequential decision solver, which can be trained and deployed across diverse domains.\\n\\n2 Background\\n\\nReinforcement Learning models a decision-making problem as a Markov Decision Process (MDP) $M = (S, A, P, R, \u03b3, \u03c1_0)$, where $S$ and $A$ denote the state space and action space, respectively. The transition function $P$ maps $S \\\\times A$ to $S$, while the expected reward function $R$ maps $S \\\\times A$ to $R$. The discount factor $\u03b3 \\\\in [0, 1)$ determines the importance of future rewards, and $\u03c1_0$ represents the initial state distribution. The goal of RL is to learn a policy $\u03c0: S \\\\rightarrow A$ that maximizes the expected discounted return over the trajectory distribution $J(\u03c0) = \\\\mathbb{E}_{\u03c0, \u03c1_0, P, R}[^{\u221e}_{t=0}\u03b3^t r_t]$. \\n\\n2\"}"}
{"id": "oIUXpBnyjv", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"EfficientZero[22] is a generalized version of AlphaGo[13], eliminating the reliance on supervised learning from game records. It is trained entirely through unsupervised self-play and achieves superhuman performance in various board games, such as chess, shogi, and Go. This approach replaces the handcrafted features and heuristic priors commonly used in traditional intelligent programs. Specifically, EfficientZero employs a deep neural network parameterized by $\\\\theta$, represented as $(p, v) = f_\\\\theta(s)$. Given a board position $s$, the network produces an action probability $p_a = P_r(a|s)$ for each action $a$ and a scalar value $v$ to predict the expected return $z$, i.e. $v \\\\rightarrow z$.\\n\\nMuZero[14] achieves superhuman performance in more complex domains with visual input[30], without knowledge of the environment's transition rules. It combines tree search with a learned model, using three networks:\\n\\n1. **Representation Network:** $s_0 = h_\\\\theta(o_1, \\\\ldots, o_t)$. This network represents the root node (at time $t$) as a latent state, obtained by processing past observations $o_1, \\\\ldots, o_t$.\\n2. **Dynamics Network:** $r_k, s_k = g_\\\\theta(s_k-1, a_k)$. This network simulates the dynamics of the environment. Given a state and selected action, it outputs the transitioned next state and corresponding reward.\\n3. **Prediction Network:** $p_k, v_k = f_\\\\theta(s_k)$. Given a latent state, this network predicts the action probability and value. Notably, MuZero searches within the learned latent space. For the MCTS process in MuZero, assume the initial root node $s_0$ is generated from the original board state through the representation network, each edge stores the following information:\\n\\n$$\\nN(s, a), P(s, a), Q(s, a), R(s, a), S(s, a),\\n$$\\n\\nrespectively representing visit counts, policy, mean value, reward, and state transition. The MCTS process in the latent space can be divided into three phases:\\n\\n- **Selection**: Actions are chosen according to the Upper Confidence Bound (UCB) formula:\\n\\n$$\\na^* = \\\\arg\\\\max_a Q(s, a) + P(s, a) \\\\frac{p_{\\\\pi}(N(s, b))}{1 + N(s, a)} [c_1 + \\\\log(P_{\\\\pi}(N(s, b)) + c_2 + 1)]\\n$$\\n\\nwhere, $N$ represents the visit count, $Q$ is the estimated average value, and $P$ is the policy's prior probability. $c_1$ and $c_2$ are constants that control the relative weight of $P$ and $Q$.\\n\\n- **Expansion**: The selected action is executed in the learned model, continuing until a leaf node is encountered. At this point, a new state node $s_l$ is generated, and its associated predicted reward $r_l$ is determined. Utilizing the prediction function, we obtain the predicted values $p_l$ and $v_l$.\\n\\n- **Backup**: The estimated cumulative reward at step $k$ is calculated based on $v_l$, denoted as:\\n\\n$$\\nG_k = l - 1 - k \\\\sum_{\\\\tau=0}^{\\\\infty} \\\\gamma^\\\\tau r_{k+1+\\\\tau} + \\\\gamma^{l-k} v_l.\\n$$\\n\\nSubsequently, $Q$ and $N$ are updated along the search path.\\n\\nAfter the search is completed, the visit count set $N(s, a)$ is returned at the root node $s_0$. These visit counts are normalized to obtain the improved policy:\\n\\n$$\\n\\\\tilde{\\\\pi}(a|s) = \\\\frac{N(s, a)}{T} / \\\\sum_b N(s, b)\\n$$\\n\\nwhere, $T$ is the total number of visits.\"}"}
{"id": "oIUXpBnyjv", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we will first introduce the overview of LightZero, followed by a comprehensive analysis of challenges in various decision environments. Additionally, we propose a specific training pipeline design for a modular and scalable MCTS toolchain. We will conclude this section with two algorithm insights inspired by the decoupled design of LightZero.\\n\\n3.1 Overview\\n\\nAs is shown in Figure 1, LightZero is the first benchmark that integrates almost all recent advances in the MCTS/MuZero sub-domain. Specifically, LightZero incorporates nine key algorithms derived from the original AlphaZero [29], establishing a standardized interface for training and deployment across diverse decision environments. Unlike the original versions of these derived algorithms, which focused on specific avenues of improvement, LightZero provides a unified viewpoint and interface. This unique feature enables exploration and comparison of all possible combinations of these techniques, offering a comprehensive baseline for reproducible and accessible research. The concrete experimental results are thoroughly described in Section 4 and Appendix B.\"}"}
{"id": "oIUXpBnyjv", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"issues in the design of general and efficient MCTS algorithms. In order to systematically complement this endeavor, we conducted an analysis of a set of classic and newly proposed RL environments to identify common characteristics. Based on this analysis, we have summarized six core challenging dimensions, which are presented in a radar plot depicted in Figure 2. Concretely, The intentions and goals of six types of environmental capabilities are:\\n\\n1) Multi-modal observation spaces pose a challenge for agents as they must be able to extract different representation modalities (e.g., low-dimensional vectors, visual images, and complex relationships) while effectively fusing distinct embeddings.\\n\\n2) Complex action space necessitates the agent's proficiency in generating diverse decision signals, encompassing discrete action selection, continuous control, and hybrid structured action space.\\n\\n3) Reliance on prior knowledge is a major drawback of methods like AlphaZero. These approaches inherently require accessibility to a perfect simulator and specific rules of the environment. In contrast, MuZero and its derived methods address this limitation by learning an environment model to substitute the simulator and related priors.\\n\\n4) Inherent stochasticity presents a fundamental challenge in tree-search-based planning methods. The uncertainty of environment dynamics and partially observable state spaces both can lead to misalignment of planning trajectories, resulting in a large number of useless or conflicting search results.\\n\\n5) Simulation cost stands as the primary contributor to wall-time consumption for MCTS-style methods. At the same time, the algorithm performance will degrade a lot if the algorithm fails to visit all the necessary actions during the simulation process.\\n\\n6) Hard exploration represents a crucial challenge that is often overlooked. While search trees can enhance efficiency by reducing the scope of exploration, MCTS-style methods are susceptible to difficulties in environments with numerous non-terminating cases, such as mazes.\\n\\n3.3 How to Simplify A General MCTS Algorithm: Decouple Pipeline into 4 Sub-Modules\\n\\nThe impressive performance of MCTS-style methods is often accompanied by a notable drawback: the complexity of implementations, which greatly restricts their applicability. In contrast to some classic model-free RL algorithms like DQN [32] and PPO [5], MCTS-style methods require multi-step simulations using search trees at each agent-environment interaction. Also, to improve the quality of training data, MuZero Unplugged [24] introduce a data reanalyze mechanism that uses the newly obtained model to compute improved training targets on old data. However, both of these techniques require multiple calls to simulators or neural networks, increasing the complexity across various aspects of the overall system, including code, distributed training, and communication topology.\"}"}
{"id": "oIUXpBnyjv", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Therefore, it is necessary to simplify the whole framework based on the integration of algorithms. Figure 3 presents a depiction of the complete pipeline of LightZero with four core sub-modules.\\n\\nFirstly, LightZero offers support for both online and offline RL [33] training schemes. The distinction between them lies in the utilization of either an online interaction data collector or direct usage of an offline dataset. Secondly, LightZero restructures its components and organizes them into four main sub-modules, based on the principle of high cohesion and low coupling. Data collector is responsible for efficient action selection using policy network and search tree. It also contains various exploration strategies, data pre-processing and packaging operations.\\n\\nData arranger plays a unique role in MCTS by effectively storing and preparing valuable data for training purposes. This sub-module involves the data reanalyze technique [24] to correct off-policy and even offline data. Furthermore, the modified priority sampling [34] ensures training mini-batches have both sufficient variety and high learning potential. To balance these tricks with efficiency, the throughput limiter controls the ratio of adding and sampling data to ensure optimal data utilization within a fixed communication bandwith.\\n\\nAgent learner is responsible for training multiple networks. It can be enhanced through various optimization techniques, such as self-supervised representation learning [35, 36], model-based rollout [37, 38], distributional prediction [39] and normalization [40, 41]. These techniques contribute to the policy improvement and further enhance the overall performance of the agent.\\n\\nAgent evaluator periodically provides the diverse evaluation metrics [42] to monitor the training procedure and assess policy behaviour. It also integrates some inference-time tricks like beam search [43] to enhance test performance. We provide a detailed analysis of how these sub-modules are implemented in specific algorithms in Appendix F. Built upon these abstractions, LightZero serves as a valuable toolkit, enabling researchers and engineers to develop enhanced algorithms and optimize systems effectively.\\n\\n3.4 How to Improve A General MCTS Algorithm: 2 Examples\\nIn this section, we present two algorithm improvement examples inspired by LightZero. The below dimensions pose necessary challenges in designing a comprehensive MCTS solver. LightZero addresses these challenges through various improvements, resulting in superior performance compared to individual algorithm variants across different domains (Section 4 and 5).\\n\\nIntrinsic Exploration\\nWhile tree-search-based methods perform well in board games with only eventual reward, they may encounter challenges or perform poorly in other environments with sparse rewards, such as MiniGrid [44]. One crucial distinction between these two problems is that in the former, the search tree can always reach several deterministic final states, whereas in the latter, it may encounter various non-termination states due to the limitation of maximum episode length. To address this issue, LightZero incorporates the idea of intrinsic reward methods [28] and implement it efficiently within MuZero's learned models. Further details can be found in Section 5.1.\\n\\nAlignment in Environment Model Learning\\nMuZero employs a representation network to generate latent states and a dynamics network to predict next latent states. However, there is no explicit supervision guiding the desired properties of the latent space. Traditional self-supervised representation learning methods often fail to align these proxy tasks with RL objectives. The difference of rollouts between the perfect simulator and the learned model is also a problem that cannot be ignored. Further exploration of misalignments across different environments are discussed in Section 5.2.\\n\\n4 Experiments\\nIn Section 4.1, we initially present some representative experiments of LightZero, with detailed experimental settings and more comprehensive results outlined in the Appendix B. Subsequently, in Section 4.2, we delve into key observations and reflections based on these benchmark results, introducing some critical insights. Particularly regarding the exploration and the alignment issues of environment model learning, we conduct an in-depth experimental analysis in Section 5.\"}"}
{"id": "oIUXpBnyjv", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.1 Benchmark Results\\nTo benchmark the difference among distinct algorithms and the capability of LightZero as a general decision solver, we conduct an extensive comparisons across a diverse range of RL environments. The algorithm variants list contains AlphaZero [29], MuZero [14], EfficientZero [17], Sampled MuZero [15], Stochastic MuZero [16], Gumbel MuZero [18] and other improved versions of LightZero. For each scenario, we evaluate all the possible variants on corresponding environments. In Figure 4, we show some selected results as examples. For detailed settings, metrics, comprehensive benchmark results and related analysis, please refer to Appendix B.\\n\\n4.2 Key Observations and Insights\\nBuilding on the unified design of LightZero and the benchmark results, we have derived some key insights about the strengths and weaknesses of each algorithm, providing a comprehensive understanding of these algorithms' performance and potential applications.\\n\\nO1: In board game environments, AlphaZero's sample efficiency greatly exceeds that of MuZero. This suggests that employing AlphaZero directly is advantageous when an environment simulator is available; however, MuZero can still achieve satisfactory results even in the absence of a simulator.\\n\\nO2: Self-supervised loss substantially enhances performance in most Atari environments with image inputs. Figure 7 demonstrates that MuZero with SSL performs similarly to MuZero in MsPacman, while outperforming it in the other five environments. This result highlights the importance of SSL for aligning the model and accelerating the learning process in image input environments.\\n\\nO3: Predicting value instead of reward does not guarantee performance enhancement. For example, in Figure 7, EfficientZero outperforms MuZero with SSL only in the MsPacman and Breakout environments, while showing similar performance in the other environments. In certain specific scenarios, such as the sparse reward environments depicted in Figure 12, EfficientZero's performance is significantly inferior to that of MuZero with SSL. Therefore, we should prudently decide whether to predict value, taking into account the attributes of the environment.\\n\\nO4: MuZero with SSL and EfficientZero demonstrate similar performance across most Atari environments and in complex structured observation settings, such as GoBigger. This observation suggests...\"}"}
{"id": "oIUXpBnyjv", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"that environments with complex structured observations can benefit from representation learning and contrastive learning techniques [35] to enhance sample efficiency and robustness.\\n\\nIn discrete action spaces, Sampled EfficientZero's performance is correlated with action space dimensions. For instance, Sampled EfficientZero performs on par with EfficientZero in Breakout (action space dimension of 4), but its performance decreases in MsPacman (dimension of 9).\\n\\nSampled EfficientZero with Gaussian policy representation is more scalable in continuous action spaces. The Gaussian version performs well in traditional continuous control and MuJoCo environments, while factored discretization [45] is limited to low-dimensional actions.\\n\\nGumbel MuZero achieves notably better performance than MuZero when the number of simulations is limited, which exhibits its potential in designing low time-cost MCTS agent.\\n\\nIn environments with stochastic state transitions or partial observable states (such as Atari without stacked frames), Stochastic MuZero can obtain slightly better performance than MuZero.\\n\\nThe self-supervised loss proposed in [17], sampling-related techniques in Sampled MuZero [45], computational improvements in Gumbel MuZero [18] for utilizing MCTS searched information, and environment stochasticity modeling in Stochastic MuZero [16] are orthogonal to each other, exhibiting minimal interference. LightZero is exploring and developing ways to seamlessly integrate these characteristics to design a universal decision-making algorithm.\\n\\n### 5 Two Algorithm Case Studies for LightZero\\n\\n#### 5.1 Exploration Strategies in MCTS\\n\\n**Motivation**\\n\\nFinding the optimal trade-off between exploration and exploitation is a fundamental challenge in RL. It is well-known that MCTS can reduce the policy search space and facilitate exploration. However, there exists limited research on the performance of MCTS algorithms in hard-exploration environments. Based on the above benchmark results, we conduct a detailed analysis of the algorithm behaviours between challenging sparse reward environments and board games, as well as insights behind the selection of exploration mechanisms in this section and Appendix C.1.\\n\\n**Settings**\\n\\nWe performed experiments in MiniGrid environment, mainly on the KeyCorridorS3R3 and FourRooms scenarios. Expanding upon the naive setting (handcrafted temperature decay), we conducted a comprehensive investigation of six distinct exploration strategies in LightZero. A detailed description of each exploration mechanism is provided in Appendix C.1.\\n\\n![Figure 5: Performance of various MCTS exploration mechanisms in MiniGrid environment (Return during the collection phase). Under the naive setting, the agent fails due to inadequate exploration. Merely increasing search budgets with the NaiveDoubleSimulation approach does not yield any significant improvement. EpsGreedy, FixedTemperature and PolicyEntropyRegularization display higher variance as they cannot guarantee enough exploration. IntrinsicExploration effectively explores the state space by leveraging curiosity mechanisms, resulting in the highest sample efficiency.](image-url)\\n\\n**Analysis**\\n\\nFigure 5 indicate that simply increasing search budgets does not yield improved performance in challenging exploration environments. Instead, implementing a larger temperature and\"}"}
{"id": "oIUXpBnyjv", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Impact of self-supervised consistency loss across different environments with various types of observations. From left to right, performance comparisons involve standard image input, compact vector input, and unique board image input, considering cases with and without consistency loss. Experiments show that the consistency loss proves to be critical only for standard image input. Incorporating policy entropy bonus can enhance action diversity during data collection, albeit at the cost of increased variance. However, theoretically, they cannot guarantee sufficient exploration, often resulting in mediocre performance and a higher likelihood of falling into local optima due to policy collapse. Epsilon-greedy exploration ensures a small probability of uniform sampling, which aids in exploring areas with potentially high returns. EpsGreedy has varying effects in different environments in early stages, but theoretically, due to its ability to ensure sufficient exploration, it may achieve good results in the long run. A more effective strategy involves curiosity-driven techniques, such as RND [27], which assigns higher intrinsic rewards to novel state-action pairs, bolstering the efficiency of exploration. The performance of the IntrinsicExploration method supports this assertion, and it can be integrated into MuZero with minimal overhead (Appendix C.1.3).\\n\\n5.2 Alignment in Environment Model Learning\\n\\nMotivation\\n\\nAligned and scalable [25] environment models are vital for MuZero-style algorithms, with factors such as model structure, objective functions, and optimization techniques contributing to their success. The consistency loss proposed in [17] could serve as an approach for aligning the latent state generated by the dynamics model with the state obtained from the observation. In this section, we investigate the impact of consistency loss on learning dynamic models and final performance in environments with diverse observations (vector, standard images, special checkerboard images).\\n\\nSettings\\n\\nTo study the impact of the consistency loss on various types of observation data, we employ the MuZero algorithm as our baseline. To ensure the reliability of our experimental results, we maintain the same configurations across other settings, with additional experimental details provided in Appendix C.2. In the experiments, we use Pong as the environment for image input, LunarLander for continuous vector input, and TicTacToe for special image input (checkerboard) environments.\\n\\nAnalysis\\n\\nIn Figure 6, consistency loss is critical for standard image input. Removing the consistency loss results in significant decline in performance, indicating the challenge of learning a dynamic model for high-dimensional inputs. For vector input environments like LunarLander, consistency loss provides a minor advantage, suggesting that learning a dynamic model is relatively easier on the compact vector observations. In special two dimension input environments like TicTacToe, the consistency loss remains large, highlighting the difficulty of achieving consistency between latent state outputs. Additionally, adding consistency loss with inappropriate hyper-parameters may lead to non-convergence (Appendix C.2). In conclusion, our experiments demonstrate that the effectiveness of the consistency loss depends on the special observation attributes. For board games, a future research direction involves investigating suitable loss functions to ensure alignment during training.\\n\\n6 Related Work\\n\\nSequential Decision-making Problems\\n\\nIn the domain of sequential decision-making problems, intelligent agents aim to make optimal decisions over time, taking into account observed states and prior actions [46]. However, these problems are often compounded by the presence of continuous action spaces, dynamic transitions, and exploration difficulties. To address such problems, model-free RL methods [5, 7, 32] focus on learning expected state rewards, optimizing actions, or combining...\"}"}
{"id": "oIUXpBnyjv", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"both strategies to achieve optimal policy learning. Conversely, model-based RL [25] incorporates the environment's transition into its optimization objective, aiming to maximize the expected return on trajectory distribution. MCTS [19] is a modeling approach derived from search planning algorithms such as minimax [47] and alpha-beta pruning [48]. Unlike these algorithms, which recursively search decision paths and evaluate their returns, MCTS employs a heuristic search on prior-guided simulations, effectively addressing excessive search consumption in complex decision spaces.\\n\\nMCTS Algorithms and Toolkits\\n\\nDespite the impressive performance and efficiency of the MCTS+RL approach, constructing the training system and dealing with intricate algorithmic details pose significant challenges when applying these algorithms to diverse decision intelligence domains. Recent research has made progress in this direction. MuZero Unplugged [24] introduced the Reanalyse technique, a simple and efficient enhancement that achieves good performance both online and offline. ROSMO [49] investigated potential issues with MuZero in offline RL and suggested a regularized one-step lookahead approach.\\n\\nThe lack of comprehensive open-source implementations remains a challenge within the research community. For example, Sampled MuZero [15] lacks a public implementation. AlphaZero-General [50] and MuZero-General [51] each support only a single algorithm, and neither offers distributed implementations. Although EfficientZero [17], does support multi-GPU implementation, it is limited to the single algorithms. KataGo [52], while primarily focused on the AlphaZero and Go game, requires substantial computational resources during training, potentially posing hardware barriers for ordinary users. As a result, the research community continues to seek more efficient and enhanced open-source tools.\\n\\nStandardization and Reproducibility\\n\\nIn the realm of Deep RL, the quest for standardizing algorithm coupled with the creation of unified benchmarks has ascended as focal points of growing significance. [53] emphasize the critical necessity of not only replicating existing work but also accurately assessing the advancements brought about by new methodologies. However, the process of reproducing extant Deep RL methods is far from straightforward, largely due to the non-determinism inherent in environments and the variability innate to the methods themselves, which can render reported results challenging to interpret. [54] proposed a set of metrics for quantitatively measuring the reliability of RL algorithms. These metrics, focusing on variability and risk both during and after training, are intended to equip researchers and production users with tools to evaluate and enhance the reliability of RL algorithms. In [55], a large-scale empirical study was undertaken to identify the factors that significantly influence the performance of on-policy RL algorithms within continuous control tasks. The insights garnered from this research offer valuable, practical suggestions for the training of on-policy RL algorithms. Despite these advancements, there remains a noticeable dearth of work specifically investigating benchmarks and the details of reproducing studies in the domain of MCTS.\\n\\n7 Conclusion\\n\\nIn this paper, we introduce LightZero, the first unified algorithm benchmark to modularly integrates various MCTS-style RL methods, systematically analyze and address the challenges and opportunities for deploying MCTS as a general and efficient decision solver. Through the incorporation of decoupled system design and novel algorithm insights, we conduct detailed benchmarks and demonstrate the potential of LightZero as scalable and efficient decision-making problem toolchains for the research community. Besides, based on this benchmark and related case studies, we also discuss existing limitations and valuable topics for future work in Appendix I.\"}"}
{"id": "oIUXpBnyjv", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\n\\n[1] Adri\u00e0 Puigdom\u00e8nech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Zhaohan Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark. In International conference on machine learning, pages 507\u2013517. PMLR, 2020.\\n\\n[2] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. arXiv preprint arXiv:2206.08853, 2022.\\n\\n[3] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets, R\u00e9mi Leblond, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario W\u00fcnsch, Katrina McK-inney, Oliver Smith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver. Grandmaster level in starcraft II using multi-agent reinforcement learning. Nat., 575(7782):350\u2013354, 2019.\\n\\n[4] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\\n\\n[5] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017.\\n\\n[6] Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent experience replay in distributed reinforcement learning. In International conference on learning representations, 2019.\\n\\n[7] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018.\\n\\n[8] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International conference on machine learning, pages 1407\u20131416. PMLR, 2018.\\n\\n[9] Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado Van Hasselt, and David Silver. Distributed prioritized experience replay. arXiv preprint arXiv:1803.00933, 2018.\\n\\n[10] Lasse Espeholt, Rapha\u00ebl Marinier, Piotr Stanczyk, Ke Wang, and Marcin Michalski. Seed rl: Scalable and efficient deep-rl with accelerated central inference. arXiv preprint arXiv:1910.06591, 2019.\\n\\n[11] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys\u0142aw D\u02db ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\\n\\n[12] David Silver and Joel Veness. Monte-carlo planning in large pomdps. Advances in neural information processing systems, 23, 2010.\\n\\n[13] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Masuring the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.\"}"}
{"id": "oIUXpBnyjv", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap, and David Silver. Mastering atari, go, chess and shogi by planning with a learned model. CoRR, abs/1911.08265, 2019.\\n\\nThomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon Schmitt, and David Silver. Learning and planning in complex action spaces. In International Conference on Machine Learning, pages 4476\u20134486. PMLR, 2021.\\n\\nIoannis Antonoglou, Julian Schrittwieser, Sherjil Ozair, Thomas K Hubert, and David Silver. Planning in stochastic environments with a learned model. In International Conference on Learning Representations, 2021.\\n\\nWeirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games with limited data. Advances in Neural Information Processing Systems, 34:25476\u201325488, 2021.\\n\\nIvo Danihelka, Arthur Guez, Julian Schrittwieser, and David Silver. Policy improvement by planning with gumbel. In International Conference on Learning Representations, 2022.\\n\\nMaciej \u00b4Swiechowski, Konrad Godlewski, Bartosz Sawicki, and Jacek Ma\u00b4ndziuk. Monte carlo tree search: A review of recent modifications and applications. Artificial Intelligence Review, 56(3):2497\u20132562, 2023.\\n\\nIan Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesvari, Satinder Singh, et al. Behaviour suite for reinforcement learning. arXiv preprint arXiv:1908.03568, 2019.\\n\\nOpen Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, et al. Open-ended learning leads to generally capable agents. arXiv preprint arXiv:2107.12808, 2021.\\n\\nJoshua Albrecht, Abraham Fetterman, Bryden Fogelman, Ellie Kitanidis, Bartosz Wr\u00f3blewski, Nicole Seo, Michael Rosenthal, Maksis Knutins, Zack Polizzi, James Simon, et al. Avalon: A benchmark for rl generalization using procedurally generated worlds. Advances in Neural Information Processing Systems, 35:12813\u201312825, 2022.\\n\\nMing Zhang, Shenghan Zhang, Zhenjie Yang, Lekai Chen, Jinliang Zheng, Chao Yang, Chuming Li, Hang Zhou, Yazhe Niu, and Yu Liu. Gobigger: A scalable platform for cooperative-competitive multi-agent interactive simulation. In The Eleventh International Conference on Learning Representations, 2023.\\n\\nJulian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou, and David Silver. Online and offline reinforcement learning by planning with a learned model. Advances in Neural Information Processing Systems, 34:27580\u201327591, 2021.\\n\\nRaj Ghugare, Homanga Bharadhwaj, Benjamin Eysenbach, Sergey Levine, and Ruslan Salakhutdinov. Simplifying model-based rl: Learning representations, latent-space models, and policies with one objective. arXiv preprint arXiv:2209.08466, 2022.\\n\\nDeepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning, pages 2778\u20132787. PMLR, 2017.\\n\\nYuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018.\\n\\nAdri\u00e0 Puigdom\u00e8nech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven Kapturowski, Olivier Tieleman, Mart\u00edn Arjovsky, Alexander Pritzel, Andrew Bolt, et al. Never give up: Learning directed exploration strategies. arXiv preprint arXiv:2002.06038, 2020.\"}"}
{"id": "oIUXpBnyjv", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017.\\n\\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279, 2013.\\n\\nChristopher D Rosin. Multi-armed bandits with episode context. Annals of Mathematics and Artificial Intelligence, 61(3):203\u2013230, 2011.\\n\\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. A. Riedmiller, A. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.\\n\\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\\n\\nYixuan Mei, Jiaxuan Gao, Weirui Ye, Shaohuai Liu, Yang Gao, and Yi Wu. Speedyzero: Mastering atari with limited data and time. In The Eleventh International Conference on Learning Representations, 2023.\\n\\nMichael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning, pages 5639\u20135650. PMLR, 2020.\\n\\nIlya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020.\\n\\nJacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-efficient reinforcement learning with stochastic ensemble value expansion. Advances in neural information processing systems, 31, 2018.\\n\\nMichael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. Advances in neural information processing systems, 32, 2019.\\n\\nWill Dabney, Mark Rowland, Marc Bellemare, and R\u00e9mi Munos. Distributional reinforcement learning with quantile regression. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018.\\n\\nHado P van Hasselt, Arthur Guez, Matteo Hessel, Volodymyr Mnih, and David Silver. Learning values across many orders of magnitude. Advances in neural information processing systems, 29, 2016.\\n\\nTobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan, David Budden, Gabriel Barth-Maron, Hado Van Hasselt, John Quan, Mel Ve\u02c7cer\u00edk, et al. Observe and look further: Achieving consistent performance on atari. arXiv preprint arXiv:1805.11593, 2018.\\n\\nStephanie CY Chan, Samuel Fishman, John Canny, Anoop Korattikara, and Sergio Guadarrama. Measuring the reliability of reinforcement learning algorithms. arXiv preprint arXiv:1912.05663, 2019.\\n\\nSam Wiseman and Alexander M Rush. Sequence-to-sequence learning as beam-search optimization. arXiv preprint arXiv:1606.02960, 2016.\"}"}
{"id": "oIUXpBnyjv", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment for openai gym. https://github.com/maximecb/gym-minigrid, 2018.\\n\\nThomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon Schmitt, and David Silver. Learning and planning in complex action spaces. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 4476\u20134486. PMLR, 2021.\\n\\nR. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. IEEE Transactions on Neural Networks, 16:285\u2013286, 1988.\\n\\nRonald L Rivest. Game tree searching by min/max approximation. Artificial Intelligence, 34(1):77\u201396, 1987.\\n\\nDonald E Knuth and Ronald W Moore. An analysis of alpha-beta pruning. Artificial intelligence, 6(4):293\u2013326, 1975.\\n\\nZichen Liu, Siyi Li, Wee Sun Lee, Shuicheng Yan, and Zhongwen Xu. Efficient offline policy optimization with a learned model. In International Conference on Learning Representations, 2023.\\n\\nShantanu Thakoor, Surag Nair, and Megha Jhunjhunwala. Learning to play othello without human knowledge, 2016.\\n\\nAur\u00e8le Hainaut Werner Duvaud. Muzero general: Open reimplementation of muzero. https://github.com/werner-duvaud/muzero-general, 2019.\\n\\nDavid J Wu. Accelerating self-play learning in go. arXiv preprint arXiv:1902.10565, 2019.\\n\\nPeter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Proceedings of the AAAI conference on artificial intelligence, 2018.\\n\\nStephanie CY Chan, Samuel Fishman, John Canny, Anoop Korattikara, and Sergio Guadarrama. Measuring the reliability of reinforcement learning algorithms. arXiv preprint arXiv:1912.05663, 2019.\\n\\nMarcin Andrychowicz, Anton Raichuk, Piotr Sta\u00b4nczyk, Manu Orsini, Sertan Girgin, Raphael Marinier, L\u00e9onard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly, and Olivier Bachem. What matters in on-policy reinforcement learning? a large-scale empirical study, 2020.\\n\\nG. Brockman, V . Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. CoRR, abs/1606.01540, 2016.\\n\\nIan Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesv\u00e1ri, Satinder Singh, Benjamin Van Roy, Richard Sutton, David Silver, and Hado van Hasselt. Behaviour suite for reinforcement learning. In International Conference on Learning Representations, 2020.\\n\\nErin Catto. Box2D. https://github.com/erincatto/Box2D, 2017.\\n\\nSaran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa. dm control: Software and tasks for continuous control. Software Impacts, 6:100022, 2020.\\n\\nThomas Hirtz. gym-hybrid. https://github.com/thomashirtz/gym-hybrid, 2020.\"}"}
{"id": "oIUXpBnyjv", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning. arXiv preprint arXiv:1912.01588, 2019.\\n\\nQuanyi Li, Zhenghao Peng, Lan Feng, Qihang Zhang, Zhenghai Xue, and Bolei Zhou. Metadrive: Composing diverse driving scenarios for generalizable reinforcement learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\\n\\nGabriele Cirulli. 2048. https://github.com/gabrielecirulli/2048, 2014.\\n\\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033. IEEE, 2012.\\n\\nYunhao Tang and Shipra Agrawal. Discretizing continuous action space for on-policy optimization. In Proceedings of the aaai conference on artificial intelligence, 2020.\\n\\nPeter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017.\\n\\nChristian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020.\\n\\nChao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative multi-agent games. Advances in Neural Information Processing Systems, 35:24611\u201324624, 2022.\\n\\nJ Terry, Benjamin Black, Nathaniel Grammel, Mario Jayakumar, Ananth Hari, Ryan Sullivan, Luis S Santos, Clemens Dieffendahl, Caroline Horsch, Rodrigo Perez-Vicente, et al. Pettingzoo: Gym for multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 34:15032\u201315043, 2021.\\n\\nBei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr, Wendelin B\u00f6hmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy gradients. Advances in Neural Information Processing Systems, 34:12208\u201312221, 2021.\\n\\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Monotonic value function factorisation for deep multi-agent reinforcement learning. The Journal of Machine Learning Research, 21(1):7234\u20137284, 2020.\\n\\nZhao-Heng Yin, Weirui Ye, Qifeng Chen, and Yang Gao. Planning for sample efficient imitation learning. arXiv preprint arXiv:2210.09598, 2022.\\n\\nSebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016.\\n\\nDI engine Contributors. DI-engine: OpenDILab decision intelligence engine. https://github.com/opendilab/DI-engine, 2021.\\n\\nSherjil Ozair, Yazhe Li, Ali Razavi, Ioannis Antonoglou, Aaron Van Den Oord, and Oriol Vinyals. Vector quantized models for planning. In International Conference on Machine Learning, pages 8302\u20138313. PMLR, 2021.\\n\\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018.\"}"}
{"id": "oIUXpBnyjv", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Peng Wu. Pytorch 2.0: The journey to bringing compiler technologies to the core of pytorch (keynote). In Christophe Dubach, Derek Bruening, and Ben Hardekopf, editors, Proceedings of the 21st ACM/IEEE International Symposium on Code Generation and Optimization, CGO 2023, Montr\u00e9al, QC, Canada, 25 February 2023-1 March 2023, page 1. ACM, 2023.\\n\\nWouter Kool, Herke Van Hoof, and Max Welling. Stochastic beams and where to find them: The gumbel-top-k trick for sampling sequences without replacement. In International Conference on Machine Learning, pages 3499\u20133508. PMLR, 2019.\\n\\nMatthew Hausknecht and Peter Stone. Deep reinforcement learning in parameterized action space. arXiv preprint arXiv:1511.04143, 2015.\\n\\nBoyan Li, Hongyao Tang, Yan Zheng, Jianye Hao, Pengyi Li, Zhen Wang, Zhaopeng Meng, and Li Wang. Hyar: Addressing discrete-continuous action reinforcement learning via hybrid action representation. arXiv preprint arXiv:2109.05490, 2021.\\n\\nBen Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\n\\nAndreas Doerr, Christian Daniel, Martin Schiegg, Nguyen-Tuong Duy, Stefan Schaal, Marc Toussaint, and Trimpe Sebastian. Probabilistic recurrent state-space models. In International conference on machine learning, pages 1280\u20131289. PMLR, 2018.\\n\\nDanijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020.\"}"}
