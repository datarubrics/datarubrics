{"id": "aXeiCbMFFJ", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"If your work uses existing assets, did you cite the creators? [Yes]\\n\\nDid you mention the license of the assets? [Yes]\\n\\nDid you include any new assets either in the supplemental material or as a URL? [Yes] We provide our code, training data, and checkpoints in this GitHub repo: https://github.com/deepcs233/Visual-CoT\\n\\nDid you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] See Appendix H.\\n\\nDid you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See Appendix H.\\n\\nIf you used crowdsourcing or conducted research with human subjects...\\n\\nDid you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\\n\\nDid you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n\\nDid you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\"}"}
{"id": "aXeiCbMFFJ", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Overview\\n\\nOur supplementary includes the following sections:\\n\\n\u2022 Section B: Framework details. Details for model design, implementation and training data.\\n\u2022 Section C: Detection performance of the visual CoT bboxes. Details for detection performance for the intermediate visual CoT bounding boxes.\\n\u2022 Section D: More experiment results. Additional performance evaluation and performance analysis.\\n\u2022 Section E: Prompt design. Prompt for generating the visual CoT dataset and evaluating the performance.\\n\u2022 Section F: Limitations. Discussion of limitations of our work.\\n\u2022 Section G: Potential negative societal impacts. Discussion of potential negative societal impacts of our work.\\n\u2022 Section F: More visualization. More Visualization of our dataset and demos.\\n\u2022 Section I: Disclaimer. Disclaimer for the visual CoT dataset and the related model.\\n\\nFollowing NeurIPS Dataset and Benchmark track guidelines, we have shared the following artifacts:\\n\\n| Artifact       | Link                        | License       |\\n|----------------|-----------------------------|---------------|\\n| Code Repository| https://github.com/deepcs233/Visual-CoT | Apache-2.0   |\\n| Data           | https://huggingface.co/datasets/deepcs233/Visual-CoT | CC BY 4.0    |\\n| Model Weights  | https://huggingface.co/collections/deepcs233/viscot-65fe883e2a0cdd3c59fc5d63 | Apache-2.0   |\\n\\nThe authors are committed to ensuring its regular upkeep and updates.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B Framework details\\n\\nB.1 Model details\\nWe choose the pre-trained ViT-L/14 of CLIP [57] as the vision encoder and Vicuna-7/13B [13] as our LLM, which has better instruction following capabilities in language tasks compared to LLaMA [64]. Consider an input original image, we take the vision encoder to obtain the visual feature. Similar to LLaV A [40, 39], we use a simple linear layer to project the image features into the word embedding space to obtain the visual tokens $H_0$ which share the same dimensionality of the LLM.\\n\\nB.2 Implementation details\\nFollowing the setup described by Vicuna [13], our model undergoes a two-stage training process. In the first stage, we pre-train the model for 1 epoch using a learning rate of 2e-3 and a batch size of 128. For the second stage, we fine-tune the model for 1 epoch on our visual CoT dataset, employing a learning rate of 2e-5 and a batch size of 128. The Adam optimizer with zero weight decay and a cosine learning rate scheduler are utilized. To conserve GPU memory during fine-tuning, we employ FSDP (Full Shard Data Parallel) with ZeRO3-style. All models are trained using 32 \u00d7 A100s. In the case of training the setting with a 7B LLM and a resolution of 224, the first/second pre-training stage completes within 1/16 hours.\\n\\nB.3 Training data details\\nWe train the model on a reorganized Vision-Language dataset. The training data is a composite of three sources: the second stage data from LLaV A, data from Shikra\u2019s [6] second stage, and our visual CoT data. The inclusion of data from Shikra, which features various datasets with positional annotations, such as RefCOCO [24] for REC, visual gemone [27] for grounding caption. These datasets can enhance VisCoT's ability to accurately identify and understand locations within images. This enhancement is crucial for tasks requiring precise spatial awareness. We listed all training data in Table 6. We removed the images from the training set that are the same as those in the testing or validation set to prevent potential data leakage. Our training data includes three parts, and they are from LLaV A-1.5, a subset of Shikra, and our proposed visual CoT dataset separately.\\n\\n| Dataset          | Size  | Source Datasets                                                                 |\\n|------------------|-------|--------------------------------------------------------------------------------|\\n| LLaV A-1.5       | 665K  | LLaV A, ShareGPT, VQAv2, GQA, OKVQA, OCRVQA, A-OKVQA, TextCaps, RefCOCO, VG    |\\n| Shikra           | 1.4M  | RefCOCO(+/g), VG, PointQA-Local/Twice, Visual-7W, Flickr30K                   |\\n| Visual CoT dataset | 376K | TextVQA, TextCaps, DocVQA, Birds-200-2011, Flickr30K, InfographicsVQA, VSR, GQA, Open images |\\n\\nC Detection performance of the visual CoT bboxes\\nIn Table 7, we present the detection performance based on the predicted CoT bounding boxes. A higher performance indicates that our VisCoT identifies the key regions with greater accuracy.\\n\\nD More experiment results\\n\\nD.1 Performance evaluation\\nIn Tab. 8 and Tab. 9, we showcase the baseline performance of our model, where it directly answers questions without employing the visual CoT process.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Detection performance (Top-1 Accuracy@0.5) on the visual CoT benchmark. The ground truth bounding boxes used for computing the metric are the intermediate CoT bounding boxes annotated in our CoT benchmark.\\n\\n| Method       | LLM Res. | VisCoT-7B Size | MLLM | Res. | Doc/Text | Chart | MLLM | VisCoT-7B | 224 | 2 | 13.6 | 41.3 | 46.8 | 5.0 | 15.7 | 7.2 |\\n|--------------|----------|---------------|------|------|----------|-------|------|-----------|-----|---|------|------|------|-----|------|-----|\\n| VisCoT-7B    | 224      | 2             | 20.4 | 46.3 | 57.6     | 9.6   | 18.5 | 10.0      |     |   |      |      |      |     |      |     |\\n| VisCoT-7B    | 336      | 2             | 51.3 | 29.4 | 49.5     | 59.3  | 54.0 | 47.1      |     |   |      |      |      |     |      |     |\\n\\nTable 8: Comparison with SoTA methods on 8 benchmarks. VisCoT achieves the best performance on the most of benchmarks, and ranks second on the other. For a fair comparison, VisCoT generates responses directly, without the visual CoT process. SQA \\\\[43\\\\]; VQA T: TextVQA \\\\[61\\\\]; MME P: MME-Preception \\\\[16\\\\]; MME C: MME-Cognition \\\\[16\\\\]; POPE \\\\[36\\\\]; MMB: MMBench \\\\[42\\\\]; MMB CN: MMBench-Chinese \\\\[42\\\\].\\n\\n| Method         | LLM Res. | Flickr30k | Visual7W | GQA | Open images | VSR | Birds-200-2011 |\\n|----------------|----------|-----------|----------|-----|-------------|-----|----------------|\\n| CLIP-2         | Vicuna-13B | 224      | 49.6     | 31.1 | 42.0        | 57.6 | 69.6           |\\n| InstructCLIP   | Vicuna-7B | 224      | 51.3     | 29.4 | 49.5        | 59.3 | 54.0           |\\n| InstructCLIP   | Vicuna-13B | 224     | 51.3     | 29.4 | 49.5        | 59.3 | 54.0           |\\n| Shikra         | Vicuna-13B | 224     | \u2013        | \u2013   | \u2013           | \u2013    | 58.8           |\\n| IDEFICS-9B     | LLaMA-7B  | 224      | 44.2     | 38.4 | 25.9        | \u2013    | \u2013              |\\n| IDEFICS-80B    | LLaMA-65B | 224     | 68.9     | 45.2 | 30.9        | \u2013    | \u2013              |\\n| Qwen-VL        | Qwen-7B   | 448      | 67.1     | 59.3 | 63.8        | \u2013    | 38.2           |\\n| Qwen-VL-Chat   | Qwen-7B   | 448      | 68.2     | 57.5 | 61.5        | 1487.5 | 360.7        |\\n| LLaVA1.5       | Vicuna-7B | 336     | 66.8     | 62.0 | 58.2        | 85.9 | 1510.7        |\\n| LLaVA1.5       | Vicuna-13B | 336     | 71.6     | 63.3 | 61.3        | 85.9 | 1531.3        |\\n| SPHINX         | LLaMA-13B | 224     | 69.3     | 62.6 | 51.6        | 80.7 | 1476.1        |\\n| VisCoT         | Vicuna-7B | 224     | 68.2     | 63.1 | 55.4        | 86.0 | 1453.6        |\\n| VisCoT         | Vicuna-13B | 224     | 71.6     | 64.2 | 57.8        | 85.6 | 1480.0        |\\n| VisCoT         | Vicuna-7B | 336     | 68.3     | 62.0 | 61.0        | 86.5 | 1514.4        |\\n| VisCoT         | Vicuna-13B | 336     | 73.6     | 63.3 | 62.3        | 83.3 | 1535.7        |\\n\\nTextVQA \\\\[61\\\\], GQA \\\\[21\\\\]. Our model still achieves comparative results across all benchmarks. This performance indicates that the visual CoT data we proposed not only enhances visual comprehension in CoT-specific scenarios but also boosts the model's overall visual understanding in standard inference setups. As demonstrated in Tab. 10, the implementation of visual CoT enables our model to achieve superior performance even with a lower resolution and a reduced number of visual tokens. This finding highlights the efficiency and effectiveness of the visual CoT approach in enhancing model accuracy.\\n\\nD.2 Performance analysis\\nTab. 4 shows that our baseline with visual CoT performs better than the model without CoT. We further investigate whether different bounding box sizes affect performance improvement. In Fig. 6, visual grounding. Furthermore, we evaluate VisCoT on REC benchmarks with RefCOCO \\\\[24\\\\], RefCOCO+ \\\\[48\\\\], and RefCOCOg \\\\[48\\\\] datasets. Our model outperforms the previous state-of-the-art models, including the specialist models such as G-DINO-L \\\\[41\\\\] and UNINEXT \\\\[78\\\\]. Notably, even with a minimal setup (7B LLM & 224 resolution), our approach outperforms methods that utilize higher resolutions or larger LLM models. This demonstrates that our dataset, enhanced with intermediate bounding boxes, significantly improves the model's precision in locating and understanding referred objects or regions. \\\"Top-1 Accuracy@0.5\\\" refers to the accuracy of a model in predicting the correct bounding box as the top prediction when the Intersection over Union (IoU) between the predicted and ground truth bounding boxes meets or exceeds 50%.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 9: Performance (Top-1 Accuracy@0.5) on Referring Expression Comprehension (REC) tasks.\\n\\n| Method       | Res. | VisCoT | VisCoT-7B | VisCoT-7B (w/o COT) | VisCoT-7B | VisCoT-7B (w/o COT) | VisCoT-7B | VisCoT-7B (w/o COT) |\\n|--------------|------|--------|-----------|---------------------|-----------|---------------------|-----------|---------------------|\\n| Specialist models |      |        |           |                     |           |                     |           |                     |\\n| UNINEXT [78]  | 384  | 2      | 85.24     | 92.64               | 94.33     | 91.46               | 88.73     | 89.37               |\\n| G-DINO-L [41] | 256  | 2      | 82.75     | 90.56               | 93.19     | 88.24               | 86.13     | 87.02               |\\n| Generalist models |      |        |           |                     |           |                     |           |                     |\\n| VisionLLM-H [69] |     | -      | -         | -                   | 86.70     | -                   | -         | -                   |\\n| OFA-L [67]    | 480  | 2      | 68.29     | 79.96               | 83.67     | 76.39               | 67.57     | 67.58               |\\n| Shikra 7B [6] | 224  | 2      | 81.60     | 87.01               | 90.61     | 80.24               | 82.27     | 82.19               |\\n| Shikra 13B [6]| 224  | 2      | 82.89     | 87.83               | 91.11     | 81.81               | 82.64     | 83.16               |\\n| MiniGPT-v2-7B [5] | 448  | 2      | 79.97     | 88.69               | 91.65     | 85.33               | 84.44     | 84.66               |\\n| MiniGPT-v2-7B-Chat [5] | 448  | 2      | 79.58     | 88.06               | 91.29     | 84.30               | 84.19     | 84.31               |\\n| Qwen-VL-7B [3] | 448  | 2      | 83.12     | 89.36               | 92.26     | 85.34               | 85.58     | 85.48               |\\n| Qwen-VL-7B-Chat [3] | 448  | 2      | 82.82     | 88.55               | 92.27     | 84.51               | 85.96     | 86.32               |\\n| Ferret-7B [82] | 336  | 2      | 80.78     | 87.49               | 91.35     | 82.45               | 83.93     | 84.76               |\\n| u-LLaVA-1B [77] | 224  | 2      | 72.21     | 80.41               | 82.73     | 77.82               | 74.77     | 75.63               |\\n| SPHINX-13B [37] | 224  | 2      | 82.77     | 89.15               | 91.37     | 85.13               | 84.87     | 83.65               |\\n\\nTable 10: Performance on VQA benchmarks.\\n\\n| Model          | DocVQA | TextVQA | ChartQA |\\n|----------------|--------|---------|---------|\\n| LLaVA-1.5-7B  | 21.6   | 58.2    | 17.7    |\\n| VisCoT-7B      | 14.4   | 55.5    | 14.2    |\\n| VisCoT-7B (w/o COT) | 39.0   | 62.9    | 19.2    |\\n\\nwe divide each evaluation dataset into five equal parts based on their relative bounding box sizes. We observe that the visual CoT usually achieve greater improvement when the corresponding bounding box is relative smaller.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 6: Visualization of performance improvement across different bounding box relative sizes for different source datasets. We find that visual CoT shows a larger improvement in cases where the queried object is relatively small. Red bars represent evaluation data samples where the model with CoT outperforms the model without CoT. Green bars indicate the opposite. The y-axis represents different ranges of relative sizes of bboxes $R$. For example, the 20-40% range indicates that the bboxes in this range occupy the relatively small 20-40% quantile within the entire dataset. For clarity, samples where both models achieve the same scores are omitted.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E.1 Generating the dataset for TextCaps\\n\\nYou are an AI visual assistant, and you are seeing a single image. What you see is provided with several sentences and Ocr_tokens, describing the same image you are looking at. Ocr_tokens indicates the text in the image. Answer all questions as you are seeing the image. Design a conversation between you and a person asking about this photo. The answers should be in a tone that a visual AI assistant is seeing the image and answering the question. Ask THREE diverse questions and give corresponding answers. Again, do not ask about uncertain details. Do not just make up questions and answers based on Ocr_tokens. Your response should include questions asking about the textual information of the image, the object types, counting the objects, object actions, object locations, relative positions between objects, etc. Please only ask questions that have definite answers:\\n\\n\u2022 One can see the content in the image that the question asks about and can answer confidently;\\n\u2022 One can determine confidently from the image that it is not in the image. Do not ask any questions that cannot be answered confidently.\\n\\nCraft Questions Around Ocr_tokens: Create questions that directly pertain to these identified words or phrases. Ensure that the question is structured in a way that the answer MUST be a word or phrase directly from the Ocr_tokens. Your answer cannot contain words outside of Ocr_tokens. The answers must be within three words.\\n\\nPlease follow the provided format:\\n\\nQuestion: [question]\\nAnswer: [answer]\\n\\nHere is the context you need to process:\\nImage description: { }\\nOcr_tokens: { }\"}"}
{"id": "aXeiCbMFFJ", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E.2 Generating the dataset for Flickr30k\\n\\nYou are an AI visual assistant, and you are seeing a single image. What you see are provided with five sentences, describing the same image you are looking at. Each sentence includes specific objects mentioned and their corresponding locations within the image (e.g., [a peach] is located at [area: 95162]). Answer all questions as you are seeing the image. Design a conversation between you and a person asking about this photo. The answers should be in a tone that a visual AI assistant is seeing the image and answering the question. Ask diverse questions and give corresponding answers. The generated questions need closer examination of specific regions in the image to gather detailed information for answering. The generated answers must be based on the corresponding area. When creating your questions, keep the following considerations in mind:\\n\\n\u2022 Direct Alignment: Ensure the \u201cFocus Area\u201d specified in each question directly corresponds to the content of the question. For instance, if the question refers to \u201ctwo women\u201d, the focus area should align with the portion described as \u201c[Two women]\u201d in the image description.\\n\\n\u2022 Image-Only Basis: Respondents will only have access to the image itself and will NOT see the provided descriptions or area details. Ensure your questions can be answered by viewing the image alone.\\n\\n\u2022 Avoid Repetition: Each question should be distinctive without overlapping content.\\n\\n\u2022 Clarity and Precision: The answers to your questions should be both lucid and exact. Evade vagueness.\\n\\n\u2022 Restricted Question Formats: Refrain from phrasing questions like \u201cWhat\u2019s in region xx?\u201d or \u201cWhat happens in description 1?\u201d. The terms \u201cdescription\u201d and \u201cregion\u201d should not appear in your questions & answers.\\n\\n\u2022 MUST: The \u201cFocus Area\u201d you provide can answer the question you provide.\\n\\nPlease follow the provided format, area_id is a number:\\n\\nQuestion: [question]\\nFocus Area: [area: area_id]\\nAnswer: [answer]\\n\\nDescribe 1: With a barn in the background a child puts her head through a hole in a cow cutout and smiles for the camera.\\n\\n\u2022 [a barn] is located at [area: 62407]\\n\u2022 [a child] is located at [area: 62402]\\n\u2022 [a hole] is located at [area: 62405]\\n\\nE.3 Generating the dataset with detailed reasoning steps for GQA\\n\\nYou are an AI visual assistant, and you are seeing a single image. I will provide a question-answer pair along with the corresponding reasoning steps. The question and answer are based on an image. You need to generate the pure reasoning text in a step-by-step format, with each step clearly numbered (1. 2. 3. ... etc). The reasoning text should help solve the question and reach the final answer without including or hinting at the answer itself. The reasoning text must not include any ID numbers.\\n\\nQuestion: What appliance is to the right of the cabinet?\\nAnswer: The appliance is a microwave.\\n\\nReasoning steps: \\n\\n1. [{\"operation\": \"select\", \"dependencies\": [], \"argument\": \"cabinet (3588933)\"}]\\n2. [{\"operation\": \"relate\", \"dependencies\": [0], \"argument\": \"appliance, to the right of, s (1564001)\"}]\\n3. [{\"operation\": \"query\", \"dependencies\": [1], \"argument\": \"name\"}]\"}"}
{"id": "aXeiCbMFFJ", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What does the top post it have written on it? Please provide the bounding box coordinate of the region that can help you answer the question better.\\n\\nGroundtruth:\\n\\nFigure 7: Visualization results of the VisCoT. Model-generated bounding boxes are shown in red, while ground truth (GT) bounding boxes are in blue. In this case, our model incorrectly predicts the CoT region, leading to a wrong answer.\\n\\nE.4 Evaluation for the visual CoT benchmark using the ChatGPT\\n\\nYou are responsible for proofreading the answers, you need to give a score to the model's answer by referring to the standard answer, based on the given question. The full score is 1 point and the minimum score is 0 points. Please output the score in the form \u201cscore: <score>\u201d. The evaluation criteria require that the closer the model's answer is to the standard answer, the higher the score.\\n\\nQuestion: \\n\\nStandard answer: \\n\\nModel's answer: \\n\\nF Limitations\\n\\nIn scenarios where the input image contains extensive information or the question is particularly complex, VisCoT may struggle to identify the most relevant region for answering the question. As shown in Figure 7, this challenge can sometimes result in the model being misled and producing incorrect responses.\\n\\nOur data pipeline inherits the limitations of utilizing GPT-4 API. (1) Accuracy and Misinformation: Generated content may not always be accurate, which could lead to the spread of misinformation. To mitigate this, we have designed a comprehensive filtering script as a post-process to improve content quality. (2) Bias and Fairness: Since we do not have access to the training data of GPT-4, the generated instructional data might reflect inherent biases, potentially reinforcing social or cultural inequalities present in the base model training. In terms of data usage, we explicitly state that OpenAI's terms must be adhered to, and the data can only be used for research purposes.\\n\\nG Potential negative societal impacts\\n\\nThe potential negative societal impacts of our work are similar to other MLLMs and LLMs. The development of Visual CoT and MLLMs, while advancing AI, poses societal risks like increased privacy invasion, the perpetuation of biases, the potential for misinformation, job displacement, and ethical concerns regarding accountability and consent.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: Examples in the visual CoT dataset, with corresponding question-answer annotations and visual CoT bboxes. The red bounding boxes in the images highlight the critical image regions that provide necessary and related information for answering the questions.\\n\\nH More visualization\\n\\nWe provide more visualization results of our proposed visual CoT dataset in Fig. 8, Fig. 9.\\n\\nWe provide more visualization results of our VisCoT baseline in Fig. 10, Fig. 11, Fig. 12, Fig. 13.\\n\\nI Disclaimer\\n\\nThis dataset was collected and released solely for research purposes, with the goal of making the MLLMs dynamically focus on visual inputs and provide intermediate interpretable thoughts. The authors are strongly against any potential harmful use of the data or technology to any party.\\n\\nIntended Use.\\n\\nThe data, code, and model checkpoints are intended to be used solely for (I) future research on visual-language processing and (II) reproducibility of the experimental results reported.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: Does the bird in the picture have iridescent underparts and white crown?\\nAnswer: No\\n\\nQuestion: Is the truck part of the cake?\\nAnswer: No\\n\\nQuestion: Why was the picture taken?\\nAnswer: To show the cake.\\n\\nQuestion: Who is dressed in blue?\\nAnswer: Boy\\n\\nFigure 9: Examples in the visual CoT dataset, with corresponding question-answer annotations and visual CoT bboxes. The red bounding boxes in the images highlight the critical image regions that provide necessary and related information for answering the questions.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is the team name on the jersey?\\n\\nPlease provide the bounding box coordinate of the region that can help you answer the question better.\\n\\norioles\\n\\n[331.5, 385.9, 562.5, 522.3]\\n\\nFirst word in green?\\n\\nPlease provide the bounding box coordinate of the region that can help you answer the question better.\\n\\nfagiolina\\n\\n[326.9, 434.1, 490.3, 492.7]\\n\\nWhat is the team name on the jersey?\\n\\nPlease provide the bounding box coordinate of the region that can help you answer the question better.\\n\\nazkaban\\n\\n[321.9, 547.8, 424.6, 571.3]\\n\\nFigure 10: Visualization results of the VisCoT. Model-generated bounding boxes are shown in red, while ground truth (GT) bounding boxes are in blue.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"What brand radio is this? Please provide the bounding box coordinate of the region that can help you answer the question better. What is the licence plate number? Please provide the bounding box coordinate of the region that can help you answer the question better. Who is the first reference? Please provide the bounding box coordinate of the region that can help you answer the question better. William R. Beisel, M.D.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Which department is shown on page 10 top left corner? Please provide the bounding box coordinate of the region that can help you answer the question better.\\n\\nHe is reading cards.\\n\\nFigure 12: Visualization results of the VisCoT. Model-generated bounding boxes are shown in red, while ground truth (GT) bounding boxes are in blue.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What color is the toy that the little girl is carrying while walking? Please provide the bounding box coordinate of the region that can help you answer the question better. The toy is pink and blue.\\n\\nWhat kind of furniture is left of the ladder? Please provide the bounding box coordinate of the region that can help you answer the question better. The furniture is chairs.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning\\n\\nHao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, Hongsheng Li\\n\\n1 The Chinese University of Hong Kong\\n2 SenseTime Research\\n3 University of Toronto\\n4 HKGAI under InnoHK\\n\\nAbstract\\n\\nMulti-Modal Large Language Models (MLLMs) have demonstrated impressive performance in various VQA tasks. However, they often lack interpretability and struggle with complex visual inputs, especially when the resolution of the input image is high or when the interested region that could provide key information for answering the question is small. To address these challenges, we collect and introduce the large-scale Visual CoT dataset comprising 438k question-answer pairs, annotated with intermediate bounding boxes highlighting key regions essential for answering the questions. Additionally, about 98k pairs of them are annotated with detailed reasoning steps. Importantly, we propose a multi-turn processing pipeline that dynamically focuses on visual inputs and provides interpretable thoughts. We also introduce the related benchmark to evaluate the MLLMs in scenarios requiring specific local region identification. Extensive experiments demonstrate the effectiveness of our framework and shed light on better inference strategies. The Visual CoT dataset, benchmark, and pre-trained models are available on this webpage to support further research in this area.\\n\\n1 Introduction\\n\\nWith the success of large language models (LLMs) like GPT-4 [1] and Gemini [63], researchers are enhancing these models by incorporating visual understanding capabilities. This enthusiasm has led to the emergence of multi-modal large language models (MLLM), such as LLaV A [39, 40], SPHINX [17, 37], and Qwen-VL [3]. Involving the extraction of visual tokens from input images, these MLLMs mostly follow a two-stage schedule: first the alignment of these tokens with linguistic modalities, and then the joint processing in LLMs. MLLMs have demonstrated viability in various scenarios, such as image captioning, visual question answering, and optical character recognition, owing to their ability to generate plausible outputs and leverage the extensive knowledge of LLMs.\\n\\nHowever, many popular MLLMs [47, 58, 23, 85, 7, 9, 76, 75, 83] and related benchmarks [35, 8, 22, 73, 74] are primarily trained to respond to instructions based on visual inputs, employing a decoder-only autoregressive design as a single black box. While these models exhibit impressive generation capabilities, they suffer from inaccurate information [36] and even hallucinations [18]. Moreover, the black-box design hinders the interpretability of visual-language models. Additionally, the potential of multi-turn in-context capability and the advantages of chain-of-thought [70, 89, 81] for LLMs have not been extensively explored in MLLMs. Some recent works, such as multimodal-CoT [90],\"}"}
{"id": "aXeiCbMFFJ", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and have shown improvements by incorporating text-level chain-of-thought reasoning or in-context learning. However, it remains uncharted whether existing MLLMs can benefit from chain-of-thought reasoning in the visual understanding process, along with their interpretability remains largely unexplored.\\n\\nFurthermore, humans comprehend intricate visual information differently, often by focusing on specific image regions or details within a given sample. For instance, when asked for a detailed regional description, humans tend to scan the entire image first, locate the references, and then focus on the targets. In contrast, most MLLMs process aligned image contexts in a fixed-grain manner (e.g., CLIP [57], EV A2-CLIP [62], InternVL [12]). To mimic human-like efficient reasoning behaviors, models need to identify image regions containing essential visual details and dynamically zoom in to capture adjusted context, which current MLLMs struggle with, leading them to seek information primarily from the text domain.\\n\\nTherefore, there is a pressing need to develop methods that can handle multi-turn, dynamic focused visual inputs, while providing more interpretable stages of reasoning to enhance the efficacy and applicability of MLLMs. However, two significant challenges hinder the design of such pipelines: the lack of intermediate visual chain-of-thought supervision in existing visual question-answering (VQA) datasets, and the reliance of popular MLLM pipelines on static image context inputs.\\n\\nTo address these challenges, we develop and release a 438k visual chain-of-thought dataset by annotating each visual question-answer pair with a bounding box. The bounding box highlights the key image region essential for answering the question. We suppose that accurately locating and comprehending this key region will significantly improve MLLM\u2019s response accuracy and relevance. Notably, about 98k question-answer pairs include extra detailed reasoning steps. These annotations are designed to instruct the MLLM in a logical, step-by-step process to identify the final bbox and generate the answer. Building on the dataset, we propose a novel pipeline that unleashes the visual CoT reasoning capability of MLLMs, which is designed to identify and output key regions in an image that provides detailed information relevant to the given question. It integrates the understanding of both the original image and detailed local image to generate the final answer. Besides, we provide the corresponding visual CoT benchmark and pre-trained models for reproducibility, aiming to foster further research in the visual chain-of-thought for MLLMs.\\n\\nTo summarize, this paper makes the following contributions:\\n\\n\u2022 We present a visual chain-of-thought dataset comprising 438k data items, each consisting of a question, an answer, and an intermediate bounding box as CoT contexts. Some items also contain detailed reasoning steps. The dataset spans across five distinct domains.\\n\u2022 We propose a novel multi-turn processing pipeline for MLLMs that can dynamically focus on visual inputs and provide intermediate interpretable thoughts.\\n\u2022 We introduce the visual chain-of-thought benchmark for evaluating MLLMs in scenarios where they need to focus on specific local regions or reasons to identify objects.\\n\\n2 Related Works\\n\\nMulti-modal LLMs. Since the advent of large language models (LLMs), their success in various language applications has paved the way for the development of multi-modal large language models (MLLMs), which integrate vision and language modalities. Initially, MLLMs were treated as dispatch schedulers to connect vision expert models, such as VisualChatGPT [71], HuggingGPT [59], and MM-REACT [80], in order to extend language models to other tasks and modalities. More recently, MLLMs have focused on aligning these modalities through extensive training on image-caption pairs or image-question conversations. Notable methods like LLaV A [40] train a projector that maps image tokens to aligned representations of pre-trained LLMs. Other approaches, such as BLIP-2 [32, 31], adopt a query transformer (Q-Former) to learn image embeddings using learnable queries after obtaining image features. MoV A [96] designs an adaptive router to fuse task-specific vision experts with a coarse-to-fine mechanism. In terms of training strategy, recent works [40, 3, 68, 94, 10, 44] commonly employ a 2-stage framework. The first stage involves pre-training on image-caption pairs, while the second stage focuses on alignment by using question-answering triplets. MLLMs have also been extended to various applications, including fine-grained localization [69, 29] such as object detection [86], video understanding [84, 34, 11], and image generation [25, 56].\"}"}
{"id": "aXeiCbMFFJ", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Examples of five domains covered in the visual CoT dataset, with corresponding question-answer annotations and visual CoT bboxes: chart, text/doc, general VQA, fine-grained understanding, and relation reasoning. The red bounding boxes in the images highlight the critical image regions that provide necessary and related information for answering the questions.\\n\\nReasoning Capability of LLMs and MLLMs. LLMs have demonstrated impressive reasoning capabilities, enabled by in-context learning (ICL) [4], which allows feeding prompted samples and context. This capability has been further enhanced by chain-of-thought (CoT) [70] prompting, which enables LLMs to generate coherent intermediate reasoning steps toward the final answer. Previous studies have shown that LLMs benefit from manually written demonstrations [70] as well as zero-shot prompting outputs [26]. Trar [92] proposes a routing module to dynamically select informative regions based on the attention map. However, due to the domain gap between vision and text data, MLLMs fail to naturally inherit this reasoning capability. To address this limitation, researchers have focused on enhancing the reasoning capability of MLLMs in both the training and prompting paradigms. For instance, Flamingo [2] bridges the gap between these two modalities by pre-training on interleaved visual and textual data. Similarly, other works leverage visual grounded-reasoning [45, 93] data in training, such as Shikra [6] and KOSMOS-2 [53]. More recently, V\u2217 [72] and CogCoM[55] modify the general mechanism in MLLMs and collect a series of visual reasoning steps as training data. On the other hand, studies have also explored prompting models [19, 87, 88, 51, 91] to understand complex visual scenes and tasks, focusing on the details of prompting techniques in MLLMs.\\n\\n3 Visual CoT Dataset\\n\\nThere is a shortage of multimodal datasets for training multi-modal large language models (MLLMs) that require to identify specific regions in an image for additional attention to improve response performance. This type of dataset with grounding bbox annotations could possibly help the MLLM output intermediate interpretable attention area and enhance performance. To fill the gap, we curate a visual CoT dataset, as illustrated in Fig. 1 and Tab. 1. This dataset specifically focuses on identifying critical regions within images, a feature essential for models to concentrate on relevant visual elements.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: One data example with detailed reasoning steps, of which we have collected about 98k of this type. The red bounding box shows the important image region for answering the question.\\n\\nAn example of detailed reasoning steps in GQA dataset\\n\\nQuestion: What appliance is to the right of the cabinet?\\n\\n### Please think step by step and provide the bounding box coordinate of the region that can help you answer the question better. ###\\n\\nReasoning steps:\\n1. Identify the cabinet in the image.\\n2. Observe the area to the right of the identified cabinet.\\n3. Look for any appliance located to the right side of the cabinet.\\n4. Determine the name of the appliance found in this location\\n\\n**CoT BBox:** [163, 44, 206, 67]\\n\\n**Answer**\\nThe appliance is a microwave.\\n\\nTable 2: The overview of the visual CoT dataset. The dataset spans five distinct domains and includes various source datasets, ensuring a broad representation of visual data styles.\\n\\n| Domain                  | Source Dataset | Size | Used GPT-4? | Dataset Description |\\n|-------------------------|----------------|------|-------------|---------------------|\\n| Text/Doc                | TextVQA [61]   | 16k  | No          | Images with text    |\\n|                         | TextCaps [60]  | 32k  | Yes         | Images with text    |\\n|                         | DocVQA [50]    | 33k  | No          | Doc Images          |\\n|                         | DUDE [65]      | 15k  | No          | Doc Images          |\\n|                         | SROIE [20]     | 4k   | No          | Invoice Images      |\\n| Fine-Grained Understanding| Birds-200-2011 [66] | 10k | No | Images of birds |\\n| General VQA             | Flickr30k [54] | 136k | Yes | Images |\\n|                         | Visual7W [95]  | 43k  | No          | Images              |\\n| Charts                  | InfographicsVQA [49] | 15k | No | Infographic |\\n| Relation Reasoning      | VSR [38]       | 3k   | No          | Images              |\\n|                         | GQA [21]       | 88k  | Yes         | Images (with detailed reasoning steps) |\\n|                         | Open images [28] | 43k | No | Images |\\n\\nTo improve response accuracy. Each data sample consists of a question, answer, and a corresponding visual bounding box across five domains, as shown in Tab. 2. Some data samples also include extra detailed reasoning steps.\\n\\nTo ensure a robust foundation for detailed visual and textual analysis, our dataset deliberately integrates a diverse selection of data including text/doc, fine-grained understanding, charts, general VQA, and relation reasoning. These data domains are deliberately chosen to cultivate a comprehensive skill set across varied analytical tasks: 1) Text/doc enhances MLLM's capabilities on OCR and contextual understanding, crucial for applications requiring text interpretation in complex environments. 2) Fine-grained understanding aids in identifying and distinguishing subtle differences in visual appearance and patterns. 3) Charts foster the ability to interpret graphical data, which are essential for business and scientific applications. 4) General VQA exposes models to a wide array of visual queries, improving their general usability. 5) Relation reasoning data develops spatial and contextual awareness of MLLMs, vital for interactive and navigational tasks. Together, these modalities ensure the dataset not only fills existing gaps but also enhances the versatility and contextual awareness of MLLMs across varied scenarios.\\n\\n3.1 Data Generation\\n\\nTo collect and build a diverse and comprehensive Visual CoT dataset, we select twelve source datasets across five distinct domains, primarily consisting of Visual Question Answering (VQA) and Image Captioning datasets. We reuse their images and useful annotations, such as question-answer pairs, image captions, and object relations, to aid in building our dataset. The data construction process involves both linguistic and visual annotators to create question-answer pairs, and provide intermediate chain-of-thought bounding boxes indicating the crucial image region for answering the question. For the linguistic annotations, we employ GPT-4 [1], known for its robust language model. The text generation is designed to provide context and ensure the answers are relevant and accurate. Additionally, the dataset includes a variety of question types, from simple to complex, to ensure a comprehensive skill set for the models.\\n\\nFor the visual annotations, we carefully select image regions that are critical for answering the questions. This involves understanding the context and ensuring that the regions are relevant to the question. The dataset is designed to be diverse, covering a wide range of visual and textual data styles, ensuring that the models are trained on a variety of scenarios.\\n\\nThe dataset is also designed to be comprehensive, covering a wide range of visual and textual data styles, ensuring that the models are trained on a variety of scenarios. This includes data from different domains, such as text and images, to ensure that the models can handle a wide range of tasks.\\n\\nFor the text generation, we employ GPT-4 [1], known for its robust language model. The text generation is designed to provide context and ensure the answers are relevant and accurate. Additionally, the dataset includes a variety of question types, from simple to complex, to ensure a comprehensive skill set for the models.\\n\\nFor the visual annotations, we carefully select image regions that are critical for answering the questions. This involves understanding the context and ensuring that the regions are relevant to the question. The dataset is designed to be diverse, covering a wide range of visual and textual data styles, ensuring that the models are trained on a variety of scenarios.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Statistics of the proposed visual CoT dataset. We visualize the CoT bbox distribution, average bbox size, and average relative size of bbox area for each source dataset.\\n\\nFor the visual annotations, we choose PaddleOCR [15], an efficient and accurate tool for optical character recognition. In the following sections, we elaborate on the generation methods employed for each domain-specific dataset.\\n\\nText/Doc. We choose five text-related datasets to create data in this domain: TextVQA [61], DocVQA [50], DUDE [65], TextCaps [60], SROIE [20]. The five datasets focus on text recognition and comprehension in a variety of images and documents. TextVQA, DocVQA, DUDE and SROIE have already provided question-answer pairs, which we directly adopt. TextCaps, providing only captions and OCR tokens, required us to employ a linguistic annotator to create corresponding questions and answers (see further details in Appendix E.1). For the visual CoT bboxes, we then apply PaddleOCR [15] to detect OCR-identified regions in the image, and specify the CoT bounding boxes as the region that consists of words and sentences aligning with the answer. Furthermore, we also design a filtering pipeline to improve content quality. This process ensures that the areas highlighted by the bounding boxes are directly relevant to the questions.\\n\\nFine-Grained Understanding. For this domain, we use Birds-200-2011 [66], which is a widely-used dataset for fine-grained visual categorization. This dataset is not only rich in visual data but also includes detailed annotations about various bird parts and their attributes, along with bird bounding boxes in each picture. To leverage this dataset for our MLLM, we have formulated questions that challenge the model to identify specific characteristics or features present in the birds. These questions are designed to test the MLLM's ability to discern and recognize fine-grained details in the images.\\n\\nGeneral VQA. We use Flickr30k [54] and Visual7W [95] as the dataset for general VQA tasks. In Flickr30k, each image encompassed five captions and the bounding boxes of most objects mentioned in the captions. Employing a similar approach to TextCaps, we use GPT-4 to generate questions that require focusing on small objects in the images. The visual CoT bounding boxes in our proposed dataset correspond to the bboxes of objects identified and annotated in the official dataset. Visual7W has already provided the question-answer pairs with object-level grounding annotations.\\n\\nCharts. We select the InfographicsVQA [49] dataset for its high-resolution infographics, which are advantageous for training MLLMs to pinpoint answer locations. Like in our Text/Doc data, we apply OCR techniques to identify regions containing the answers, using these identified areas as the CoT bounding boxes for more precise model training.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We select the Visual Spatial Reasoning (VSR) [38], GQA [21], and Open Images [28] datasets to construct data focusing on relation-reasoning. These datasets are rich in spatial relational information among objects in images. For our chain-of-thought (CoT) bounding boxes, we use the bounding boxes surrounding the objects relevant to the question. For instance, if the question is \u201cWhat is the material of the desk left to the woman?\u201d, the bounding box of the desk to the woman's left is designated as the visual CoT bounding box, providing more visual context for the MLLM's reasoning process. In GQA [21] each image is associated with a scene graph of objects and relations. Each question comes with a structured representation of its semantics. With these annotations, we utilize GPT-4 to generate detailed reasoning steps, as illustrated in Tab. 1. The related prompt is available in Appendix E.3.\\n\\n3.2 Dataset Analysis\\n\\nWe provide a visualization of the data statistics in Fig. 2. We partition the bboxes in each dataset into three groups (large, medium, small) based on the relative bounding box size $R$, which is the ratio of the CoT bbox size relative to the total image size. The visualization reveals that the majority of the annotated key regions, particularly in text-oriented datasets, occupy only a small portion of the entire image, highlighting the importance of identifying these crucial areas to enhance performance. Specifically, the average bounding box size is 247.8 pixels, which well aligns with the common input resolution for a vision encoder ranges between 224 and 336 pixels, while the original image size is usually too large and needs down-sampling that loses information. These regions account for only about 13.2% of the image area. This highlights the necessity for MLLMs to accurately pinpoint these crucial areas to enhance processing efficiency and effectiveness. If the model fails to correctly identify and focus on these key regions, the majority of the image processed could be irrelevant, leading to inefficient computation, hallucination, and potential degradation in performance.\\n\\n4 Enhancing MLLMs with Chain-of-Thought Capabilities\\n\\nAlong with the visual CoT dataset, we also propose a visual CoT MLLM framework named VisCoT, which employs standard models without specialized modifications, serving as a baseline to enhance MLLMs with visual CoT capabilities. In this section, we briefly introduce the framework, and illustrate the pipeline in Fig. 3. Readers are referred to Appendix B for more details.\\n\\nVisCoT Pipeline.\\n\\nTo train the MLLM baseline with visual CoT data, we add a CoT prompt (\u201cPlease provide the bounding box coordinate of the region that can help you answer the question better.\u201d) to the question, asking the model to identify the most informative region of the image. VisCoT then determines this region and generates its bounding box. During the training phase, we utilize the ground truth bounding box to extract visual information rather than a predicted one in the following steps. With the original image $X_0$ and the bbox, a visual sampler extracts the localized image $X_1$ containing detailed information. The same vision encoder and projector are then used to extract visual tokens $H_1$. The MLLM then integrates visual tokens from both the original and localized images $\\\\{H_0, H_1\\\\}$ to provide more precise and comprehensive answers. For data without visual CoT annotations, this procedure is omitted as indicated by the dashed box in Fig. 3. Here, the MLLM directly answers based on the input image alone. Our VisCoT baseline is thus adaptable to data in both annotated and non-annotated formats simultaneously.\\n\\nVisual Sampler.\\n\\nGiven the original image and the predicted bbox, the visual sampler's role is to accurately select the relevant region that considers the visual encoder requirement and bbox corner cases. We first calculate the center point $[x_0, y_0]$, half-width $w_{\\\\text{half}}$, and half-height $h_{\\\\text{half}}$ of the bounding box predicted by VisCoT. To capture more context and meet the square receptive field requirement of the CLIP model, $\\\\max\\\\{\\\\max\\\\{w_{\\\\text{half}}, h_{\\\\text{half}}\\\\}, \\\\text{res}_{\\\\text{half}}\\\\}$ is chosen as the sample size $s$. $\\\\text{res}_{\\\\text{half}}$ is the half input size of the vision encoder. Consequently, the visual sampler crops the region $[x_0-s, y_0-s, x_0+s, y_0+s]$ for further processing. During inference, if the calculated cropped box extends beyond the image boundaries, the center point is adjusted towards the center of the image to ensure the box remains within the image frame. This adjustment is important for improving the overall performance, as it can mitigate the impact of any detection inaccuracies.\\n\\nInference.\\n\\nVisCoT offers two options to generate answers: with or without the visual CoT process. If the CoT feature is not needed, users can simply provide the MLLM with the image and question. To engage the CoT feature, users can append the additional visual CoT prompt after the question.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In which country is this event taking place?\\n\\nPlease provide the bounding box coordinate of the region that can help you answer the question better.\\n\\nIt's Germany, as the 'Vorsicht' sign suggests, which is German for 'Caution'.\\n\\nFigure 3: VisCoT first extracts visual tokens from an image and pinpoints the key region relevant to the question. Then, it processes the localized visual information. Finally, the MLLM integrates the information from the overall and localized images to construct a comprehensive and accurate answer.\\n\\nTable 3: Performance on the Visual CoT benchmark. Datasets highlighted in grey indicate their training splits were not used in our model's training phase. Res indicates input image resolution.\\n\\n| Doc/Text | Chart | MLLM | Res. | DocVQA | TextCaps | TextVQA | DUDE | SROIE | InfographicsVQA |\\n|----------|-------|------|------|--------|----------|---------|------|-------|-----------------|\\n| LLaVA-1.5-7B [39] | 336 | 2 | 0.244 | 0.597 | 0.588 | 0.290 | 0.136 | 0.400 |\\n| LLaVA-1.5-13B [39] | 336 | 2 | 0.268 | 0.615 | 0.617 | 0.287 | 0.164 | 0.426 |\\n| SPHINX-13B [37] | 224 | 2 | 0.198 | 0.551 | 0.532 | 0.000 | 0.071 | 0.352 |\\n| VisCoT-7B | 224 | 2 | 0.355 | 0.610 | 0.719 | 0.279 | 0.341 | 0.356 |\\n| VisCoT-7B | 336 | 2 | 0.476 | 0.675 | 0.775 | 0.386 | 0.470 | 0.324 |\\n\\n5 Experiments\\n\\nFirstly, we provide an overview of the construction and evaluation of the CoT benchmark. Subsequently, in the evaluation phase, we begin by accessing VisCoT on the proposed benchmark (refer to Sec. 5.2). Additionally, we conduct further experiments to analyze the impact of essential components within VisCoT through an ablation study in Sec. 5.3. Finally, we showcase the capabilities of VisCoT in engaging complex multimodal conversations in Sec. 5.4. The training details and detection performance of the visual CoT bboxes can be found in Appendix B & C.\\n\\n5.1 Visual CoT Benchmark\\n\\nIn this section, we provide an overview of our visual CoT benchmark, which primarily focuses on scenarios where the MLLM needs to concentrate on specific regions within a complete image. We utilize 12 source datasets, as shown in Fig. 1, and when an official training/evaluation split exists, we adopt it. In cases where such a split does not exist, we randomly divide the dataset. Additionally, we incorporate the test split of SROIE, DUDE, and Visual7W to evaluate the model's zero-shot visual CoT capabilities. Following the methodology of previous MLLM studies [33, 46], we employ...\"}"}
{"id": "aXeiCbMFFJ", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Ablation study on the different BBox selection strategies. \u2018w/o CoT\u2019 indicates a standard, non-CoT-based inference process. \u2018GT BBox\u2019 uses annotated ground truth bboxes. \u2018Random\u2019 and \u2018Center\u2019 refer to using random and center bboxes instead of model predictions.\\n\\n| BBox Strategy | Doc/Text | Chart | General VQA | Relation Reasoning | Fine-grained | Average |\\n|---------------|----------|-------|-------------|-------------------|-------------|---------|\\n| Baseline      | 0.355    | 0.610 | 0.719       | 0.279             | 0.341       | 0.356   |\\n| w/o CoT       | 0.170    | 0.502 | 0.463       | 0.175             | 0.044       | 0.332   |\\n| GT BBox       | 0.774    | 0.827 | 0.840       | 0.718             | 0.633       | 0.778   |\\n| Random        | 0.208    | 0.463 | 0.495       | 0.157             | 0.146       | 0.378   |\\n| Center        | 0.220    | 0.533 | 0.558       | 0.204             | 0.205       | 0.366   |\\n\\nTable 5: Ablation study on the visual sampler design.\\n\\n| Expanded Cropping | Centered Cropping | Doc/Text | Chart | General VQA | Relation Reasoning | Fine-grained | Average |\\n|-------------------|-------------------|----------|-------|-------------|-------------------|-------------|---------|\\n| \u2713                 |                   | 0.399    | 0.321 | 0.621       | 0.668             | 0.509       | 0.496   |\\n|                  | \u2713                 | 0.410    | 0.328 | 0.625       | 0.678             | 0.531       | 0.506   |\\n|                  | \u2713 \u2713               | 0.434    | 0.331 | 0.641       | 0.677             | 0.521       | 0.518   |\\n\\nChatGPT [52] and ask it to assign a numerical score between 0 and 1, where a higher score indicates better prediction accuracy. For detailed information on the prompt used for ChatGPT-based evaluation, please refer to Appendix E.4.\\n\\n5.2 Performance Evaluation\\n\\nIn this section, we comprehensively evaluate VisCoT across various multi-modal tasks to thoroughly assess our model\u2019s visual understanding ability. Tab. 3 highlights the enhancements through the visual CoT benchmark. We also showcase the baseline performance of our model on other benchmarks in Appendix D, where it directly answers questions without employing the visual CoT process.\\n\\nIn Tab. 3, we test our model and LLaVA-1.5 on the proposed visual CoT benchmark as detailed in Sec. 5.1. To demonstrate the impact of the chain-of-thought process, we also include the ablation study that removes this reasoning process and directly generates the response in a standard, direct manner. Notably, our pipeline shows significant improvement in the doc/text-related tasks and high-resolution image processing, even when the training splits from corresponding datasets are not utilized for the model training. For instance, SROIE [20] is a dataset that involves extracting key information from scanned receipts, such as the company name and the total price. Our model achieves 8\u00d7 performance compared to the standard pipeline without a chain-of-thought process. Furthermore, the visual CoT pipeline also shows superior results in other benchmark tasks, showing its efficacy in enhancing the model\u2019s comprehensive visual and textual interpretation abilities.\\n\\n5.3 Ablation Study\\n\\nIn the ablation studies below, in default, we ablate VisCoT-7B with a resolution of 224 and mainly evaluate in the proposed visual CoT benchmark.\\n\\nVisual CoT BBox Selection Strategies. Tab. 4 showcases the performance of our model on the visual CoT benchmark using different strategies for bbox selection. As anticipated, employing ground truth annotated bounding boxes instead of model predictions yields the highest performance, surpassing the baseline by a significant margin. This can be considered the upper bound of our model\u2019s potential.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Token Efficiency. The visual CoT pipeline utilizes double the visual tokens for answer generation, leading us to assess its performance at various resolutions: 224, 336, and 448. As depicted in Fig. 4, the visual CoT pipeline exhibits improved token efficiency in our model. For instance, when equipped with the visual CoT, our model's accuracy at 224 resolution surpasses that of the standard pipeline at 448 resolution, while only using half the visual tokens.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.4 Visualization\\n\\nThis section displays VisCoT\u2019s qualitative performance through Fig. 5, highlighting its visual CoT ability to identify critical regions in images that aid in answering questions and synthesizing the combined contexts of both original and zoomed-in images. We also provide comparative results with different configurations: VisCoT (GT BBox), and VisCoT (w/o CoT). The accuracy of detection and depth of understanding directly contribute to the quality of the generated answers.\\n\\n6 Conclusion\\n\\nIn this paper, we introduced VisCoT, a pioneering approach that enhances multi-modal large language models with visual chain-of-thought reasoning. This methodology addresses critical gaps in MLLMs, particularly in interpretability and processing dynamic visual inputs. Our visual CoT dataset offers 438k annotated question-answer pairs for detailed visual analysis. Our novel multi-turn processing pipeline allows MLLMs to dynamically focus and interpret visual data, mirroring human cognition. VisCoT provides more interpretable reasoning stages, and the visual CoT benchmark advances the evaluation of MLLMs\u2019 focus on specific image areas. Extensive experiments validate the framework\u2019s effectiveness, offering a promising starting point for further exploration in visual CoT.\\n\\nAcknowledgement.\\nThis project is funded in part by National Key RD Program of China Project 2022ZD0161100, by the Hong Kong Generative AI Research and Development Center (HKGAI) Ltd under the Innovation and Technology Commission (ITC)\u2019s InnoHK, by General Research Fund of Hong Kong RGC Project 14204021. Hongsheng Li is a PI of HKGAI under the InnoHK.\\n\\nReferences\\n\\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\\n\\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.\\n\\n[3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.\\n\\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\n[5] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023.\\n\\n[6] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm\u2019s referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023.\\n\\n[7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023.\\n\\n[8] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[11] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024.\\n\\n[10] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022.\\n\\n[11] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023.\\n\\n[12] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023.\\n\\n[13] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna.lmsys.org (accessed 14 April 2023), 2023.\\n\\n[14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.\\n\\n[15] Yuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei Liu, Jun Zhou, Yifan Bai, Zilin Yu, Yehua Yang, Qingqing Dang, et al. Pp-ocr: A practical ultra lightweight ocr system. arXiv preprint arXiv:2009.09941, 2020.\\n\\n[16] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023.\\n\\n[17] Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, et al. Sphinx-x: Scaling data and parameters for a family of multi-modal large language models. arXiv preprint arXiv:2402.05935, 2024.\\n\\n[18] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models. arXiv preprint arXiv:2308.06394, 2023.\\n\\n[19] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: A visual language model for gui agents. arXiv preprint arXiv:2312.08914, 2023.\\n\\n[20] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1516\u20131520. IEEE, 2019.\\n\\n[21] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700\u20136709, 2019.\\n\\n[22] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Guanglu Song, Peng Gao, et al. Mmsearch: Benchmarking the potential of large models as multi-modal search engines. arXiv preprint arXiv:2409.12959, 2024.\\n\\n[23] Yang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, Lin Ma, and Yu-Gang Jiang. Lumen: Unleashing versatile vision-centric capabilities of large multimodal models. arXiv preprint arXiv:2403.07304, 2024.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "aXeiCbMFFJ", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023.\\n\\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023.\\n\\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022.\\n\\nGen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, and Rongrong Ji. Cheap and quick: Efficient vision-language instruction tuning for large language models. Advances in Neural Information Processing Systems, 36, 2024.\\n\\nGen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji. Multi-task collaborative network for joint referring expression comprehension and segmentation. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 10034\u201310043, 2020.\\n\\nRuipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability. arXiv preprint arXiv:2306.07207, 2023.\\n\\nBingqi Ma, Zhuofan Zong, Guanglu Song, Hongsheng Li, and Yu Liu. Exploring the role of large language models in prompt encoding for diffusion models. arXiv preprint arXiv:2406.11831, 2024.\\n\\nJunhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11\u201320, 2016.\\n\\nMinesh Mathew, Viraj Bagal, Rub\u00e8n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1697\u20131706, 2022.\\n\\nMinesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 2200\u20132209, 2021.\\n\\nChancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig. Compositional chain-of-thought prompting for large multimodal models. arXiv preprint arXiv:2311.17076, 2023.\\n\\nOpenAI. Chatgpt. https://openai.com/blog/chatgpt/, 2023.\\n\\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023.\\n\\nBryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 2641\u20132649, 2015.\\n\\nJi Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, et al. Cogcom: Train large vision-language models diving into details through chain of manipulations. arXiv preprint arXiv:2402.04236, 2024.\\n\\nShengju Qian, Huiwen Chang, Yuanzhen Li, Zizhao Zhang, Jiaya Jia, and Han Zhang. Strait: Non-autoregressive generation with stratified image transformer. arXiv preprint arXiv:2303.00750, 2023.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "aXeiCbMFFJ", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Penghao Wu and Saining Xie. V*: Guided visual search as a core mechanism in multimodal \\\\textsc{llms}. \\\\textit{arXiv} preprint \\\\textit{arXiv:2312.14135}, 17, 2023.\\n\\nPeng Xia, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, et al. Cares: A comprehensive benchmark of trustworthiness in medical vision language models. \\\\textit{arXiv} preprint \\\\textit{arXiv:2406.06007}, 2024.\\n\\nPeng Xia, Siwei Han, Shi Qiu, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, Zhaorun Chen, Chenhang Cui, Mingyu Ding, Linjie Li, et al. Mmie: Massive multimodal interleaved comprehension benchmark for large vision-language models. \\\\textit{arXiv} preprint \\\\textit{arXiv:2410.10139}, 2024.\\n\\nPeng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, and Huaxiu Yao. Mmed-rag: Versatile multimodal rag system for medical vision language models. \\\\textit{arXiv} preprint \\\\textit{arXiv:2410.13085}, 2024.\\n\\nPeng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, and Huaxiu Yao. Rule: Reliable multimodal rag for factuality in medical vision language models. \\\\textit{arXiv} preprint \\\\textit{arXiv:2407.05131}, 2024.\\n\\nJinjin Xu, Liwu Xu, Yuzhe Yang, Xiang Li, Yanchun Xie, Yi-Jie Huang, and Yaqian Li. u-llava: Unifying multi-modal tasks via large language model. \\\\textit{arXiv} preprint \\\\textit{arXiv:2311.05348}, 2023.\\n\\nBin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance perception as object discovery and retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15325\u201315336, 2023.\\n\\nXu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, and Xin Geng. Exploring diverse in-context configurations for image captioning. \\\\textit{Advances in Neural Information Processing Systems}, 36, 2024.\\n\\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. \\\\textit{arXiv} preprint \\\\textit{arXiv:2303.11381}, 2023.\\n\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. \\\\textit{Advances in Neural Information Processing Systems}, 36, 2024.\\n\\nHaoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. \\\\textit{arXiv} preprint \\\\textit{arXiv:2310.07704}, 2023.\\n\\nGe Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, et al. Map-neo: Highly capable and transparent bilingual large language model series. \\\\textit{arXiv} preprint \\\\textit{arXiv:2405.19327}, 2024.\\n\\nHang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. \\\\textit{arXiv} preprint \\\\textit{arXiv:2306.02858}, 2023.\\n\\nJiacheng Zhang, Yang Jiao, Shaoxiang Chen, Jingjing Chen, and Yu-Gang Jiang. Eventhallusion: Diagnosing event hallucinations in video llms. \\\\textit{arXiv} preprint \\\\textit{arXiv:2409.16597}, 2024.\\n\\nShilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. \\\\textit{arXiv} preprint \\\\textit{arXiv:2307.03601}, 2023.\\n\\nYuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. What makes good examples for visual in-context learning? \\\\textit{Advances in Neural Information Processing Systems}, 2023.\\n\\nYuechen Zhang, Shengju Qian, Bohao Peng, Shu Liu, and Jiaya Jia. Prompt highlighter: Interactive control for multi-modal llms. \\\\textit{arXiv} preprint \\\\textit{arXiv:2312.04302}, 2023.\\n\\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. \\\\textit{arXiv} preprint \\\\textit{arXiv:2210.03493}, 2022.\"}"}
{"id": "aXeiCbMFFJ", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023.\\n\\nGe Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang. Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models. Advances in Neural Information Processing Systems, 36:5168\u20135191, 2023.\\n\\nYiyi Zhou, Tianhe Ren, Chaoyang Zhu, Xiaoshuai Sun, Jianzhuang Liu, Xinghao Ding, Mingliang Xu, and Rongrong Ji. Trar: Routing the attention spans in transformer for visual question answering. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2074\u20132084, 2021.\\n\\nChaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo, Xingjia Pan, Mingbao Lin, Chao Chen, Liujuan Cao, Xiaoshuai Sun, and Rongrong Ji. Seqtr: A simple yet universal network for visual grounding. In European Conference on Computer Vision, pages 598\u2013615. Springer, 2022.\\n\\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\\n\\nYuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4995\u20135004, 2016.\\n\\nZhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang, Hongsheng Li, and Yu Liu. Mova: Adapting mixture of vision experts to multimodal context. arXiv preprint arXiv:2404.13046, 2024.\\n\\nChecklist\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\\n   (b) Did you describe the limitations of your work? [Yes] See Appendix F.\\n   (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Appendix G.\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n   (b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] We have provided the related details in Appendix. The code, training data, benchmark, and checkpoints can be found in this GitHub repo: https://github.com/deepcs233/Visual-CoT\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See 'Training Details' in Section Experiments. We also provide reproducible scripts that contain all hyperparameters in this GitHub repo: https://github.com/deepcs233/Visual-CoT\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No]\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix B.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\"}"}
