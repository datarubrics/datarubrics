{"id": "7AjdHnjIHX", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Original Counterfactual\\n\\nHuman annotation error\\n\\nTwo people dressed in red skiing across a snowy landscape\\n\\nTwo people dressed in red race across a snowy landscape\\n\\nFailure to accurately depict spatial relationships\\n\\nA woman lies on the ground under a suitcase. A man lies on the ground under a suitcase.\\n\\nFailure to generate correct number of objects\\n\\nA bathroom sink with two toothbrush holders on it\\n\\nA bathroom sink with two cup holders on it\\n\\nTable 9: Additional examples of failure cases identified by manual error analysis\"}"}
{"id": "7AjdHnjIHX", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: Frequency of altered subjects which appeared at least 20 times in errors identified by human annotators\\n\\nIn many cases, these failures do not negatively impact the depiction of the counterfactual change in the two images because the inaccuracies pertain to details other than the altered subjects. For example, the first row of Table 8 shows the counterfactual pair associated with an image which was categorized as a failure to generate a subject/object; in this case, the altered subjects (kitchen \u2192 field) are depicted correctly, but both images lack the eating tray described in the prompt. Similarly, the counterfactual pair shown in the second row of Table 8 lacks fine-grained details in the prompt (e.g., dirty room), but still depicts the altered subjects correctly (man \u2192 kid).\\n\\nWe found that 15% of the sampled errors could be attributed to a hyponymy relationship between the altered subjects which caused both captions to be equally valid for a given image. For example, the third row of Table 8 shows a counterfactual pair where the counterfactual image was incorrectly labeled by the human annotator because both captions were valid descriptions of the image (i.e., girls can also be referred to as kids). Nevertheless, this example is still a valid counterfactual pair considering that the counterfactual caption does not accurately describe the original image and is more descriptive of the counterfactual image than the original caption.\\n\\nAn additional 15% of the sampled errors appeared to be valid image-text pairs without any significant deficiencies. We therefore concluded that such cases were human annotation errors (see Table 9 row 1 for an example). Finally, 4% of the sampled images had equally valid caption choices because both of the altered subjects appeared in the image that was annotated.\\n\\nThe results of this error analysis suggest that the quality of counterfactuals produced by our approach may improve as the capabilities of text-to-image diffusion models advance. New models which overcome known limitations of existing models could be used as a substitute for Stable Diffusion in our approach to produce higher-quality counterfactuals. Additionally, errors associated with hyponymy relationships could be addressed in future work through a refinement of our subject alteration process. For example, ontologies could be used to avoid noun substitutions where it can be determined that a hyponymy relationship exists between the noun candidates. Finally, additional constraints on the image generation process could be explored to prevent both altered subjects from appearing in the same image.\\n\\nA.3.2 Taxonomic Analysis of Errors\\n\\nTo better understand the relationship between the altered subjects in our counterfactuals and potential failure cases, we conducted a taxonomic analysis of the altered subjects which occurred most frequently among errors identified by human annotators. Table 10 provides the frequency of altered subject pairs which occurred at least 20 times in the error cases identified by human annotators.\\n\\nInterestingly, we observe that 19 of these 20 most frequent altered subject pairs belong to the human taxonomy. We further analyzed this human taxonomy in COCO-Counterfactuals by constructing a list of human-related words, which consists of 'girl', 'boy', 'man', 'men', 'woman', 'guy', 'kid', 'person', 'people', 'child', 'children', 'couple', 'group', and 'lady'. An image-text pair is said to be related to this human taxonomy if the altered subject of its caption belongs to this list. We find that there are 4117 image-text pairs in COCO-Counterfactuals that are related to the human taxonomy, among which 1864 were identified as errors by human annotators. The corresponding error rate for altered subjects related to the human taxonomy is 44.3%, which indicates that generating counterfactual pairs involving human altered subjects is more challenging for our approach. This suggests that a\"}"}
{"id": "7AjdHnjIHX", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Training dataset | D\\\\textsubscript{train} | D\\\\textsubscript{CF\\\\textsubscript{train}} | R@1 | R@5 | R@10 | R@1 | R@5 | R@10 | Mean |\\n|------------------|---------------------|---------------------|------|-----|------|-----|-----|------|------|\\n| None (pre-trained CLIP) | 0 | 0 | 50.12 | 75.04 | 83.6 | 30.73 | 56.28 | 67.18 | 60.49 |\\n| MS-COCO | 13,928 | 0 | 57.33 | 0.3 | 81.28 | 0.2 | 88.71 | 0.2 | 41.13 | 0.1 | 68.46 | 0.1 |\\n| MS-COCO + COCO-CFs | 13,928 | 6,939 | 56.91 | 0.3 | 80.7 | 0.2 | 87.82 | 0.2 | 39.92 | 0.1 | 67.01 | 0.1 |\\n| MS-COCO + COCO-CFs | 34,820 | 20,894 | 58.06 | 0.3 | 81.39 | 0.2 | 88.78 | 0.2 | 41.82 | 0.1 | 68.79 | 0.1 |\\n\\nTable 11: Mean image-text retrieval performance on the OOD Flickr30k test set using only COCO-Counterfactuals which were correctly labeled by humans, measured across 25 different random seeds.\\n\\nTable 12: Image-text retrieval performance on the in-domain MS-COCO test set. All other settings are identical to Table 3.\\n\\nA.4 Training Data Augmentation with Only Correctly-annotated COCO-Counterfactuals\\nWe investigate the potential impact of COCO-Counterfactuals which were incorrectly labeled by humans on training data augmentation. Table 11 provides the OOD image-text retrieval performance in this setting, where COCO-Counterfactuals were filtered to only include those which were correctly labeled by the human annotators. Overall we find similar performance as our previous experiments using the full COCO-Counterfactuals dataset (Table 3), suggesting that filtering our synthetic data using human evaluations is not necessary for data augmentation applications.\\n\\nA.5 COCO-Counterfactuals Improve In-domain Performance\\nWe evaluate the same models trained with counterfactual data augmentation described in Section 5 on the MS-COCO test set. The results of this in-domain evaluation are provided in Table 12. Similar to the OOD image-text retrieval setting, we find that data augmentation with COCO-Counterfactuals provides statistically significant performance improvements relative to training without counterfactual data augmentations. Notably, previous work has observed that counterfactual data augmentation can degrade performance on withheld in-domain test sets (Wang and Culotta, 2021; Howard et al., 2022), whereas data augmentation with our COCO-Counterfactuals actually increases in-domain performance on MS-COCO.\\n\\nA.6 COCO-Counterfactuals for Model Evaluation Experiments\\nWe further investigate whether our COCO-Counterfactuals (COCO-CFs) can serve as a challenging test set for state-of-the-art multimodal vision-language models such as CLIP, Flava (Singh et al., 2022), BridgeTower (Xu et al., 2022) and ViLT (Kim et al., 2021) for the zero-shot image-text retrieval and image-text matching tasks. We employed the following HuggingFace implementations of these models via the transformers library:\\n\\n- **CLIP**: We used the pre-trained model clip-vit-base-patch32\\n- **Flava**: We used the pre-trained model flava-full\\n- **BridgeTower**: We used the pre-trained model bridgetower-large-itm-mlm-itc\\n- **ViLT**: We used the pre-trained model vilt-b32-finetuned-coco\\n\\nZero-shot Image-text Retrieval. In Section 4, we evaluated the zero-shot image-text retrieval (ITR) performance of pre-trained Flava and BridgeTower models on COCO-CFs and human-evaluated...\"}"}
{"id": "7AjdHnjIHX", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 13 reports ITR performance (i.e., Recall at 1, 5, and 10) on COCO-CFs and human-evaluated COCO-CFs for the pre-trained CLIP model. Similar to Table 2, the percentages enclosed within parentheses indicate the change in performance of the CLIP model on an evaluated dataset versus the performance of that model on MS-COCO (baseline).\\n\\nWe observe that on both COCO-CFs and human-evaluated COCO-CFs datasets, while the performance of the pre-trained CLIP model degrades marginally on Text Retrieval task, its performance increases for Image Retrieval task. We attribute this to potential data contamination due to how we employed a pre-trained CLIP model in our counterfactual image generation process (see Section 3.2). As a result, COCO-Counterfactuals includes image-text pairs for which CLIP achieves high image-text retrieval performance.\\n\\nB Dataset and Experiment Details\\n\\nB.1 Hyper-parameter Selection and Models Used to Generate COCO-Counterfactuals\\n\\nIn this section, we will detail hyper-parameters and pre-trained models used to our generate COCO-Counterfactuals dataset.\\n\\nB.1.1 Creating Counterfactual Captions\\n\\nGiven an original caption from the MS-COCO dataset, we use Natural Language Toolkit (NLTK) (Bird et al., 2009) modules:\\n\\n- punkt for sentence tokenizer, and\\n- averaged_perceptron_tagger for part-of-speech (POS) tagger to identify all nouns as candidate words for substitution.\\n\\nFor each of the identified nouns, we create 10 candidate counterfactual captions by replacing only one noun with the [MASK] token and retrieving the top-10 most probable replacements via masked language modeling (MLM). For MLM, we used the pre-trained model roberta-base (Liu et al., 2019) implemented in the library transformers (Wolf et al., 2019).\\n\\nA motivation for our use of noun substitutions is the desire to produce minimal-edit counterfactuals. This is a common strategy for NLP counterfactuals (Kaushik et al., 2019; Wang and Culotta, 2021; Yang et al., 2021) because the high degree of similarity between the original and counterfactual text preserves spurious correlations that models might rely on for discernment. Furthermore, our use of perplexity filtering mitigates the potential for such word substitutions to produce unrealistic counterfactual captions.\\n\\nIn order to measure similarity between each candidate counterfactual caption and an original caption, we used the pre-trained model all-MiniLM-L6-v2, which is implemented within the library sentence-transformers (Reimers and Gurevych, 2019).\\n\\nAmong generated candidate counterfactual captions, we kept only those candidates which have a sentence similarity within the range (0.8, 0.91). We selected this similarity range heuristically, observing that it produced best results after extensive experimentation.\"}"}
{"id": "7AjdHnjIHX", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Finally, we employed the pre-trained model gpt2-large, a GPT-2 (Radford et al., 2018) model implemented in the transformers library, to score the perplexity and choose the candidate having the lowest perplexity as our counterfactual caption.\\n\\nB.1.2 Generating Counterfactual Images\\n\\nAfter creating a counterfactual caption, our next task is to generate synthetic images from the corresponding original caption and counterfactual caption, respectively. In order to do so, we have adopted an implementation from Instruct-Pix2Pix (Brooks et al., 2023) in which all hyper-parameters are set to their default values. Specifically, we over-generate 100 image pairs with Prompt-to-Prompt by randomly sampling values of the parameter $p \\\\sim U(0.1, 0.9)$ (i.e., parameter $p$ indicates the portion of denoising for which to fix self attention maps). The resulting 100 image pairs are filtered using CLIP (Radford et al., 2021) to ensure:\\n\\ni. a minimum cosine similarity of 0.2 between the encoding of each caption and its corresponding generated image, and\\nii. a minimum cosine similarity of 0.7 between the encoding of the two respective images in each generated image pair.\\n\\nFrom remaining image pairs, the best image pair is chosen such that it has the highest directional similarity $CLIP_{dir}$ score. Selecting images with the highest $CLIP_{dir}$ improves the overall quality of our generated counterfactuals via greater consistency between the alterations made in both modalities.\\n\\nB.2 Human Annotation Study\\n\\nProfessional annotation services for our human study were provided by Mindy Support. The total cost of this study was $1068.59 for 218 annotation hours. The instructions provided to annotators are depicted in Figure 4. We are unable to provide the hourly wages paid to workers as this is considered proprietary information by Mindy Support. However, the following statement was provided by the vendor regarding compensation:\\n\\n\\\"We prioritize compliance with all standards of local and international legislation, ensuring fair treatment and equal opportunities for individuals of various backgrounds, ages, and other characteristics. We are committed to upholding the principles of fair wages, non-discrimination, and labor standards, including the prohibition of child labor. As an organization, we strictly adhere to legal requirements and strive to create an inclusive and ethical working environment for all. Rest assured that our compensation rates reflect market demands and provide fair remuneration for the work performed by our participants. We remain dedicated to abiding by all labor regulations and social and economic standards.\\\"\\n\\nB.3 Training Data Augmentation Experiments\\n\\nIn this section, we detail how we constructed our training datasets and how we finetuned the pre-trained CLIP model for experiments described in Section 5.\\n\\nB.3.1 Training Dataset Preparation\\n\\nOur training data augmentation experiments utilize various combinations of the MS-COCO validation set and our COCO-Counterfactuals dataset. For simplicity, a caption-image pair is referred to as a sample. We define a counterfactual sample as following. Given a sample $(C, I)$ (i.e., caption $C$ and image $I$) from our COCO-Counterfactuals dataset, a sample $(C', I')$ from COCO-Counterfactuals dataset is called a counterfactual sample of $(C, I)$ iff $C'$ and $C$ are counterfactual captions of each other. By this definition, COCO-Counterfactuals dataset includes 34,820 samples that correspond to 17,410 paired counterfactual samples.\\n\\nFor experiments in Section 5, we have prepared the following 4 datasets:\"}"}
{"id": "7AjdHnjIHX", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Instructions provided to data annotators\\n\\n(a.) MS-COCO dataset. This is a subset of the 5K validation split of the 2017 MS-COCO dataset, achieved by filtering out all samples with captions which are not included in our COCO-Counterfactuals. This results in a dataset (referred to as the MS-COCO dataset used in experiments in Section 5) of 17,410 captions and their paired original images.\\n\\n(b.) [MS-COCO + COCO-CFs] base dataset. This dataset is a combination of:\\n- 50% random sampling (i.e., 8,705 caption-image pairs) of the MS-COCO dataset constructed in (a.).\\n- 25% random sampling of paired counterfactual samples from our COCO-Counterfactuals dataset. This results in a total of 4,353 pairs of samples with their corresponding counterfactuals, for a total of 8,706 caption-image samples from our COCO-Counterfactuals dataset.\\n\\n[13] https://cocodataset.org/#download\"}"}
{"id": "7AjdHnjIHX", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Overall, the [MS-COCO + COCO-CFs] base dataset consists of 17,411 captions and their paired original images, which is approximately equal in size to the MS-COCO dataset constructed in (a.). This dataset is a combination of:\\n\\n- all samples (i.e., 17,410 caption-image pairs) from the MS-COCO dataset constructed in (a.).\\n- 75% random sampling (i.e., 26,115 caption-image pairs) from our COCO-Counterfactuals dataset.\\n\\nOverall, dataset [MS-COCO + COCO-CFs] medium consists of 43,525 captions and their paired original images.\\n\\n(d.) [MS-COCO + COCO-CFs] all dataset. This dataset is a combination of:\\n\\n- all samples (i.e., 17,410 caption-image pairs) from the MS-COCO dataset constructed in (a.).\\n- all samples (i.e., 34,820 caption-image pairs) from our COCO-Counterfactuals dataset.\\n\\nOverall, dataset [MS-COCO + COCO-CFs] all consists of 52,230 captions and their paired original images.\\n\\nEach of the datasets described above is split into a training set (80%) and a validation set (20%). In each experiment, the validation set is used to pick the best model checkpoint at the conclusion of training. Tables 3, 4, and 12 report experimental results for models trained using the train split of these four datasets.\\n\\n| D_train | indicates the total number of samples (i.e., image-text pairs) included in the respective training set, while | D_CF_train | indicates how many of those image-text pairs were sampled from the COCO-Counterfactuals dataset.\\n\\nB.3.2 Finetuning CLIP with Data Augmentation\\n\\nWe use each of the four training sets constructed in Section B.3.1 to finetune the CLIP model clip-vit-base-patch32. We adopted a publicly-available finetuning script provided by HuggingFace.\\n\\nWe repeat each of our training experiments with 25 different seeds and data_seed from the ranges [107, 131] and [108, 132], respectively. In each experiment, we use a learning rate to 5e-7, weight decay of 0.001, training batch size of 128, and evaluation batch size of 128.\\n\\nB.4 Compute Infrastructure Used In this Study\\n\\nOur experiments were conducted using an Intel AI supercomputing cluster comprised of Intel Xeon processors and 512 Intel Gaudi\u00ae AI accelerators, as well as an internal Slurm linux cluster with Nvidia RTX 3090 GPUs. Our dataset generation pipeline was parallelized across this compute infrastructure and took approximately 3 days to complete. Our training data augmentation experiments varied in running time depending upon the size of the dataset, ranging between 2 to 10 hours.\\n\\nB.5 License Information of Assets Employed in This Study\\n\\n\u2022 NLTK is open source software distributed under the terms of the Apache License Version 2.0.\\n\u2022 Transformers is released under the Apache License Version 2.0 and is available on GitHub at https://github.com/huggingface/transformers.\\n\u2022 Pre-trained model Roberta-base is released under the MIT License.\\n\u2022 Library sentence-transformers is licensed under the Apache License Version 2.0 and is available on GitHub at https://github.com/UKPLab/sentence-transformers.\\n\u2022 Pre-trained model all-MiniLM-L6-v2 is licensed under the Apache License Version 2.0.\\n\u2022 Pre-trained gpt2-large model is license under the MIT License.\\n\\n14 The finetuning script can be accessed at https://github.com/huggingface/transformers/blob/main/examples/pytorch/contrastive-image-text/run_clip.py.\"}"}
{"id": "7AjdHnjIHX", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Instruct-Pix2Pix is licensed under the MIT License and is available on GitHub at https://github.com/timothybrooks/instruct-pix2pix.\\n\\nInstruct-Pix2Pix further employs stable-diffusion-v1-5 that is released under CreativeML-Open-RAIL-M License.\\n\\nFor the MS-COCO dataset:\\n\u2013 The annotations in the dataset are released under the Creative Commons Attribution 4.0 License.\\n\u2013 The use of the images in the dataset must abide by the Flickr Terms of Use.\\n\\nPre-trained model clip-vit-base-patch32 is licensed under the MIT License.\\n\\nPre-trained model flava-full is licensed under the 3-Clause BSD License.\\n\\nPre-trained model BridgeTower large-itm-mlm-itc is released under the MIT License.\\n\\nPre-trained vilt-b32-finetuned-coco model is license under the Apache License Version 2.0.\\n\\nC.1 Motivation\\n\\nFor what purpose was this dataset created?\\n\\nThis dataset was created for the purpose of exploring the relevancy of counterfactual examples for multimodal vision-language models. Specifically, our aim was to create a dataset which can serve both as a challenging evaluation dataset for existing models and as a resource for training data augmentation to improve multimodal models on downstream tasks.\\n\\nFor additional discussion of our motivation and the intuition behind counterfactual examples, see Section 1.\\n\\nWho created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\\n\\nThe dataset was created by the authors of this paper who are affiliated with Intel Labs, a research and development organization within Intel Corporation.\\n\\nWho funded the creation of the dataset?\\n\\nThe creation of this dataset was funded by Intel Corporation.\\n\\nC.2 Composition\\n\\nWhat do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?\\n\\nThe instances represent synthetically-generated images and accompanying text captions. The images depict a variety of different everyday scenarios.\\n\\nHow many instances are there in total (of each type, if appropriate)?\\n\\nCOCO-Counterfactuals contains a total of 34,820 image-caption pairs.\\n\\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?\\n\\nYes, it contains all possible instances per our filtering criteria.\\n\\nWhat data does each instance consist of?\\n\\nEach instance consists of a synthetically-generated image and an accompanying text caption.\\n\\nIs there a label or target associated with each instance?\\n\\nNo\\n\\nIs any information missing from individual instances?\\n\\nNo\\n\\nAre relationships between individual instances made explicit (e.g., users' movie ratings, social network links)?\\n\\nYes, instances which correspond to a single counterfactual pair are annotated as such in our dataset. Otherwise, there are no other relationships between individual instances.\\n\\nAre there recommended data splits (e.g., training, development/validation, testing)?\\n\\nNo\\n\\nAre there any errors, sources of noise, or redundancies in the dataset?\\n\\nThe automated methodology used to generate COCO-Counterfactuals introduces the possibility of noise and errors in the dataset. See Section ?? for additional discussion.\\n\\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?\\n\\nYes\"}"}
{"id": "7AjdHnjIHX", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor\u2013patient confidentiality, data that includes the content of individuals\u2019 non-public communications)?\\n\\nNo\\n\\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?\\n\\nYes, the dataset may contain offensive material due to the manner in which it was automatically constructed. See Section ?? for additional discussion.\\n\\nDoes the dataset identify any subpopulations (e.g., by age, gender)?\\n\\nNo\\n\\nIs it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset?\\n\\nNo\\n\\nDoes the dataset contain data that might be considered sensitive in any way (e.g., data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)?\\n\\nNo\\n\\nC.3 Collection Process\\n\\nHow was the data associated with each instance acquired?\\n\\nThe data associated with each instance was acquired via our data generation methodology (see Section 3 for a detailed description).\\n\\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)?\\n\\nPlease see Section 3 for a complete description of our data generation methodology.\\n\\nIf the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?\\n\\nNot applicable\\n\\nWho was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?\\n\\nThe COCO-Counterfactuals dataset was collected automatically, as detailed in Section 3. Human evaluation of COCO-Counterfactuals involved paid professional annotators employed by Mindy Support (see Appendix B.2 for details).\\n\\nOver what timeframe was the data collected?\\n\\nThe data was generated and evaluated over the course of approximately three months.\\n\\nWere any ethical review processes conducted (e.g., by an institutional review board)?\\n\\nNo, institutional review was not required.\\n\\nDid you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?\\n\\nNo, the dataset was generated automatically and was not collected directly from individuals.\\n\\nWere the individuals in question notified about the data collection?\\n\\nNot applicable\\n\\nDid the individuals in question consent to the collection and use of their data?\\n\\nNot applicable\\n\\nIf consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses?\\n\\nNot applicable\\n\\nHas an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted?\\n\\nNo, not applicable\\n\\nC.4 Preprocessing/cleaning/labeling\\n\\nWas any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?\\n\\nYes, we apply extensive filtering to various stages of our data generation pipeline in order to improve the quality of the dataset. See Section 3 for a complete description of these methods.\"}"}
{"id": "7AjdHnjIHX", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Was the \u201craw\u201d data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?\\n\\nNo. However, due to how our dataset is automatically constructed, raw data can be reproduced by running our code.\\n\\nIs the software that was used to preprocess/clean/label the data available?\\n\\nYes, we will make our code publicly available upon publication.\\n\\nC.5 Uses\\n\\nHas the dataset been used for any tasks already?\\n\\nYes, we applied COCO-Counterfactuals to the task of model evaluation in Section 4 and to the task of training data augmentation in Section 5.\\n\\nIs there a repository that links to any or all papers or systems that use the dataset?\\n\\nOur GitHub repository will contain links to papers and systems used by our data generation methodology. Additionally, this paper contains references to all such papers and systems that we utilized.\\n\\nWhat (other) tasks could the dataset be used for?\\n\\nCOCO-Counterfactuals is broadly applicable to tasks which require multimodal inputs consisting of images with paired text. One potential use case not explored during this study is large-scale pre-train of multimodal models, which could be improved through counterfactual data augmentation.\\n\\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?\\n\\nDue to the way in which COCO-Counterfactuals was generated automatically, it may contain errors, offensive material, or biases which are present in the models employed by our pipeline.\\n\\nWe used Stable Diffusion to collect image data, which has well-known limitations that should be considered when utilizing datasets which are derived from them. These limitations include unrealistic depictions of hands, palms, and other fine-grained objects (Samuel et al., 2023); failures to generate one or more of the subjects in a prompt and correctly bind attributes such as color (Chefer et al., 2023); difficulties with object counting and spatial relationship understanding (Cho et al., 2022); and challenges associated with the composition of concepts (Liu et al., 2022). While our experiments suggest that COCO-Counterfactuals is relatively robust to generation failures when used for training data augmentation, future applications of our methodology should consider the risks associated with these limitations relative to the intended use of the generated dataset.\\n\\nStable diffusion and other text-to-image diffusion models have been shown to exhibit biases associated with race and gender, including over-representation of masculinity and whiteness (Luccioni et al., 2023); racial and gender disparities in depictions of certain occupations (Bianchi et al., 2023); and preferences for certain genders or skin tones (Cho et al., 2022). Consequently, models trained on COCO-Counterfactuals may learn similar social biases as those expressed in synthetic images generated by Stable Diffusion. While some recent work has investigated approaches for mitigating biases in diffusion models, further investigation is needed into the de-biasing of datasets on which these models are trained in order to fully eliminate them (Schramowski et al., 2023). Users of the dataset should carefully consider how these limitations may impact their potential use case.\\n\\nAre there tasks for which the dataset should not be used?\\n\\nThe dataset should not be used for a task if the limitations discussed above are unacceptable or potentially problematic for the intended use case.\\n\\nC.6 Distribution\\n\\nWill the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?\\n\\nYes, the dataset will be made open source and publicly available.\\n\\nHow will the dataset will be distributed (e.g., tarball on website, API, GitHub)?\\n\\nThe dataset will be distributed via the Hugging Face Hub.\\n\\nWhen will the dataset be distributed?\\n\\nThe dataset will be made available publicly upon publication of this paper.\"}"}
{"id": "7AjdHnjIHX", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?\\n\\nThe dataset will be distributed under the CC BY 4.0 license.\\n\\nHave any third parties imposed IP-based or other restrictions on the data associated with the instances?\\n\\nNo\\n\\nDo any export controls or other regulatory restrictions apply to the dataset or to individual instances?\\n\\nNo\\n\\nC.7 Maintenance\\n\\nWho will be supporting/hosting/maintaining the dataset?\\n\\nThe dataset will be hosted on the Hugging Face Hub. The authors of this paper will support and maintain the dataset via our public GitHub repository.\\n\\nHow can the owner/curator/manager of the dataset be contacted (e.g., email address)?\\n\\nThe corresponding author can be contacted via the e-mail address listed on the first page of this paper. Alternatively, an issue can be raised on our GitHub repository.\\n\\nIs there an erratum?\\n\\nNo\\n\\nWill the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?\\n\\nAlthough we do not anticipate the need to update this dataset in the future, we will respond to issues which are raised on our public GitHub repository for this project.\\n\\nIf the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)?\\n\\nNot applicable\\n\\nWill older versions of the dataset continue to be supported/hosted/maintained?\\n\\nYes. If the dataset is updated in the future, older versions will remain available.\\n\\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?\\n\\nYes, we make our dataset open source and welcome others to build on it. This can be done by making contributions to our GitHub repository and/or citing our dataset as appropriate when used in future work.\"}"}
{"id": "7AjdHnjIHX", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs\\n\\nTiep Le\\nIntel Labs\\ntiep.le@intel.com\\n\\nVasudev Lal\\nIntel Labs\\nvasudev.lal@intel.com\\n\\nPhillip Howard\\nIntel Labs\\nphillip.r.howard@intel.com\\n\\nAbstract\\nCounterfactual examples have proven to be valuable in the field of natural language processing (NLP) for both evaluating and improving the robustness of language models to spurious correlations in datasets. Despite their demonstrated utility for NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counterfactual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of multimodal vision-language models via training data augmentation. We make our code and the COCO-Counterfactuals dataset publicly available.\\n\\n1 Introduction\\nWhile vision and language models have achieved remarkable performance improvements in recent years, out-of-domain (OOD) generalization remains a challenge for even the best models, which typically exhibit much lower performance in zero-shot evaluation settings than on withheld in-domain test sets. This has often been attributed to spurious correlations between non-causal features and labels in datasets which can be exploited during training as shortcuts to achieving artificially high in-domain performance (Geirhos et al., 2020). For example, image recognition models often learn to utilize spurious features in the backgrounds of images when trained for classification on datasets such as ImageNet (Singla and Feizi, 2021; Xiao et al., 2020).\\n\\nAugmenting training datasets with counterfactuals, which study the impact on a response variable following a change to a causal feature, has been previously proposed as a strategy for countering this effect in NLP models (Levesque et al., 2012; Kaushik et al., 2019). Motivated by concepts in causal learning (Feder et al., 2022), these methods typically form counterfactual examples by making minimal edits to an input text such that a corresponding label or attribute of the text (e.g., sentiment)\"}"}
{"id": "7AjdHnjIHX", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Two men work in the kitchen of a restaurant.\\n\\nTwo men work in the kitchen of a house.\\n\\nA large black ball sitting next to a glass of milk.\\n\\nA large black ball sitting next to a glass of water.\\n\\nA white dog sitting on a table next to a paper.\\n\\nA white dog sitting on a table next to a book.\\n\\nA meal of sandwiches, potatoes, and Red Stripe beer.\\n\\nA meal of sandwiches, pizza, and Red Stripe beer.\\n\\nFigure 1: Examples of COCO-Counterfactuals, our minimal-edit counterfactuals dataset for images with paired text captions.\\n\\nis changed. Training models with counterfactual examples therefore provides a strong inductive bias against learning spurious correlations in datasets, leading to greater robustness and improved generalization on OOD data (Eisenstein, 2022; Vig et al., 2020) as well as enabling better domain adaptation in low resource settings (Calderon et al., 2022).\\n\\nDespite its success in the realm of NLP, the application of counterfactual data augmentation to multimodal vision-language models has largely remained unexplored, mainly due to low-resource settings involving multimodal data and challenges associated with creating paired counterfactual examples spanning multiple modalities. For example, consider the task of creating counterfactual examples for a multimodal dataset containing images with associated text captions. Creating a counterfactual to a given image-text example requires not only minimally editing a casual feature in the text caption, but also making a corresponding minimal edit to the image which ideally modifies only the changed causal feature while preserving other spurious features from the original image. Collecting such counterfactual examples from existing image datasets is infeasible due to the massive variation in natural images that can accurately depict even identical text captions. While manual creation of counterfactual examples by humans is an option that has been employed previously for NLP (Kaushik et al., 2019; Gardner et al., 2020), this approach suffers from a lack of scalability due to the high cost of human labor, which would be compounded even further for multimodal counterfactuals due to the need for both text and image editing skills. Given these challenges, how can paired image-text counterfactual examples be created at the scale needed for effective model evaluation and data augmentation?\\n\\nWe address this problem by introducing a novel data generation pipeline for automatically creating multimodal counterfactual examples using text-to-image diffusion models. Our approach minimally edits captions from an existing image-text dataset and then leverages Stable Diffusion (Rombach et al., 2021) with cross-attention control (Hertz et al., 2022) to generate pairs of images with minimal differences (i.e., isolated to the counterfactual change). We employ our data generation pipeline to create at scale COCO-Counterfactuals (Figure 1), a counterfactual variant of the MS-COCO dataset (Lin et al., 2014).\\n\\nWe validate the quality of COCO-Counterfactuals using human evaluations and conduct zero-shot experiments showing that state-of-the-art multimodal models are challenged by our generated counterfactual examples. Our additional experiments show that training CLIP (Radford et al., 2021)\"}"}
{"id": "7AjdHnjIHX", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"COCO-Counterfactuals improves its performance on multiple OOD datasets, including zero-shot tasks not seen during training. We make COCO-Counterfactuals and the code for our counterfactual data generation pipeline publicly available under the CC BY 4.0 License.\\n\\n2 Related Work\\n\\n2.1 Counterfactual Examples for NLP\\n\\nCounterfactual data augmentation has been shown to improve the robustness of models across a wide range of problem domains in NLP. Kaushik et al. (2019) demonstrated that human-authored counterfactuals pose a significant challenge for existing models and that augmenting training datasets with counterfactual examples improves sentiment analysis and Natural Language Inference classifiers. Gardner et al. (2020) similarly used human experts to author minimally-edited contrast example sets for 10 NLP datasets and showed that model performance evaluated on them drops substantially. A number of approaches have been proposed to move beyond reliance on human authors towards automated methods for generating counterfactual examples. Wang and Culotta (2021) and Yang et al. (2021) automatically construct counterfactual examples by identifying and removing or replacing potentially causal words. Howard et al. (2022) introduce a framework for generating looser counterfactuals which allow larger edits of original examples, resulting in more natural and linguistic counterfactual examples. Other semi-automated methods have been proposed to generate counterfactual examples while still relying on human input or labeling (Wu et al., 2021). To the best of our knowledge, none of these existing approaches for automatic counterfactual generation have been extended to multimodal image-text datasets.\\n\\n2.2 Image Benchmarks for Measuring Spurious Correlations\\n\\nSeveral image datasets have been proposed as benchmarks for measuring the degree to which models have learned to rely on spurious correlations during training. CelebA Hair Color (Liu et al., 2015) is a binary image classification dataset that labels whether a person depicted has a blonde hair color, which is spuriously correlated with gender. Sagawa et al. (2019) constructed the Waterbirds dataset by cropping images of landbirds or seabirds onto land and sea backgrounds, resulting in a binary classification task for bird type (i.e., landbird or seabird) in the presence of spurious correlations with the background. Colored MNIST (Arjovsky et al., 2019) artificially imposes colors on the MNIST handwritten digits dataset, where the color is spuriously correlated with the class label. Lynch et al. (2023) uses text-to-image models to generate Spurious, an image classification dataset of four dog breeds spuriously correlated with six background locations. Unlike COCO-Counterfactuals, these datasets are limited only to image classification over a small number of labels and are primarily suited for evaluating model robustness as opposed to training data augmentation.\\n\\nThrush et al. (2022) introduced Winoground, an image-text dataset aimed at measuring visio-linguistic compositionality. Given two images and two captions which have the same words but in different order, the task is to correctly match each caption to its corresponding image. While their paired image-text examples can be viewed as counterfactuals, they focus only on edits to word order and rely on humans to create a dataset aimed specifically at evaluating compositionality. In contrast, our method automatically generates counterfactual examples with word content changes while also preserving non-causal spurious features across paired counterfactual images.\\n\\n2.3 Data Augmentation with Synthetic Images Generated from Text-to-image Models\\n\\nMotivated by recent advances in text-to-image diffusion models (Nichol et al., 2021; Rombach et al., 2021; Saharia et al., 2022; Ramesh et al., 2022), data augmentation with synthetically-generated images has emerged as a growing topic of interest. He et al. (2022) showed that images generated by GLIDE (Nichol et al., 2021) for specific classes in image recognition datasets can be used for...\"}"}
{"id": "7AjdHnjIHX", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"training to improve performance on the corresponding image classification tasks. Trabucco et al. (2023) perform image-to-image transformations for data augmentation using text-to-image diffusion models, observing improvements in few-shot image classification performance. Vendrow et al. (2023) represent class labels from image recognition datasets as custom tokens in the vocabulary of a text-to-image diffusion model, enabling them to generate images of objects from the original dataset under different domain shifts. While our data generation pipeline also leverages text-to-image diffusion models, our approach differs from prior work in our focus on producing minimal changes to paired image-text data in both the vision and language modalities.\\n\\nCOCO-Counterfactuals\\n\\nWe detail our data generation methodology for creating COCO-Counterfactuals (COCO-CFs), a synthetic multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset (Lin et al., 2014). While we showcase our methodology by generating and releasing the COCO-Counterfactuals dataset, our approach can be applied to automatically construct multimodal counterfactuals for any dataset containing image captions.\\n\\n3.1 Creating Counterfactual Captions\\n\\nGiven an original image caption $C_o$, our first task is to create a corresponding counterfactual caption $C_c$ which alters a subject of $C_o$ while preserving most of its original details. The altered subject represents the changed causal feature in our counterfactual example while the remaining preserved details from the original caption can be viewed as potentially spurious correlated features.\\n\\nTo alter a subject of $C_o$, we first identify all nouns using NLTK (Bird et al., 2009) as candidate words for substitution. For each of the $i \\\\in \\\\{1, \\\\ldots, n\\\\}$ identified nouns, we create 10 candidate counterfactual captions by replacing only the $i$-th noun in $C_o$ with the [MASK] token and retrieving the top-10 most probable replacements via masked language modeling (MLM). This produces a total of $n \\\\times 10$ candidate counterfactual captions, which we then filter to retain only those in which the substituted word is also a noun.\\n\\nOur aim is to substitute nouns with alternative words that represent different subjects, and yet still maintain ontological similarity to the original noun. Hence, we use a pre-trained sentence similarity model to measure the similarity between each candidate counterfactual caption and the original caption $C_o$, keeping only those candidates which have a sentence similarity within the range $(0.8, 0.91)$. Finally, we use GPT-2 to score the perplexity of all candidates which remain after filtering and choose the candidate having the lowest perplexity as our counterfactual caption $C_c$.\\n\\n3.2 Generating Counterfactual Images\\n\\nAfter creating a counterfactual caption $C_c$, our next task is to generate synthetic images $I_{so}$ and $I_{sc}$ from the original caption $C_o$ and counterfactual caption $C_c$ (respectively). Ideally we would like $I_{so}$ and $I_{sc}$ to differ only in terms of the noun which was replaced in $C_o$ to produce $C_c$, thereby enabling the changed causal feature to be learned in the presence of other potentially spurious correlated features (i.e., the unchanged details between $C_o$ and $C_c$). However, this is a challenge for existing text-to-image generation models as minor changes to a text prompt can produce significantly different images. For instance, prompting Stable Diffusion with the captions \u201cA small child lounges with a remote in his hand\u201d and \u201cA small child lounges with a toy in his hand\u201d may produce images that differ not only in the object that the child is holding, but also in other details such as his facial features, the manner in which he is laying, the color of his clothes, and the image background.\\n\\nTo address this issue, Hertz et al. (2022) proposed a methodology called Prompt-to-Prompt which injects cross-attention maps during the diffusion process to control the attention between certain pixels and tokens of the prompt during denoising steps. This enables separate generations from\\n\\nAppendix B.1 details hyper-parameters and pre-trained models used to generate COCO-Counterfactuals.\\n\\nIn this work, we focus on counterfactual captions that are derived from altering a noun from original captions. We leave the investigation of altering words of other types such as verbs and adjectives for future work.\\n\\nWe use RoBERTa-base for MLM.\"}"}
{"id": "7AjdHnjIHX", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A small child lounges with a remote in his hand.\\n\\nA small child lounges with a toy in his hand.\\n\\nFigure 2: Examples of COCO-Counterfactuals generated with Prompt-to-Prompt (left) and without (right). Prompt-to-prompt enables us to extend the principle of minimal-edit text counterfactuals to the visual domain, isolating image differences to only the changed causal feature.\\n\\nBrooks et al. (2023) noted that making different changes to images may require varying the parameter $p$ in Prompt-to-Prompt, which controls the number of denoising steps with shared attention weights. For example, changes that require more substantial structural modifications to the image may necessitate less overall similarity between the resulting images and thus fewer shared attention weights. We therefore adopt their proposed approach of over-generating 100 image pairs with Prompt-to-Prompt by randomly sampling values of the parameter $p \\\\sim U(0.1, 0.9)$.\\n\\nThe resulting 100 image pairs are filtered using CLIP (Radford et al., 2021) to ensure a minimum cosine similarity of 0.2 between the encoding of each caption and its corresponding generated image, with the best image pair $(I_{so}, I_{sc})$ chosen from those which remain according to the directional similarity in CLIP space (Gal et al., 2022):\\n\\n$$\\\\text{CLIP}_{\\\\text{dir}} = (E_T(C_c) - E_T(C_o)) \\\\cdot (E_I(I_{sc}) - E_I(I_{so}))$$\\n\\nwhere $E_T$ and $E_I$ are CLIP's text and image encoders (respectively). The CLIP$_{\\\\text{dir}}$ metric measures the consistency in changes between the two images $(I_{so}, I_{sc})$ and their corresponding captions $(C_o, C_c)$. Thus, selecting images with a higher CLIP$_{\\\\text{dir}}$ improves the overall quality of our generated counterfactuals via greater consistency between the alterations made in both modalities.\\n\\n3.3 Generating COCO-Counterfactuals from MS-COCO\\n\\nWe apply our counterfactual caption and image generation pipeline described above to create the COCO-Counterfactuals dataset. Specifically, we generate candidate counterfactual captions for 25,014 original MS-COCO captions, keeping only the best candidate counterfactual for each original caption that meets our filtering criteria. This produced a total of 24,508 original & counterfactual caption pairs $(C_o, C_c)$ after filtering and selection. Our image over-generation pipeline produced 2.45 million candidate image pairs $(I_{so}, I_{sc})$ for these 24.5k caption pairs, of which 17,410 had at least one generated image pair which met our filtering criteria. After selection according to the CLIP$_{\\\\text{dir}}$ metric, a total of 34,820 image-caption pairs remain, comprising our COCO-Counterfactuals dataset.\\n\\nWe use the implementation from Instruct-Pix2Pix (Brooks et al., 2023).\\n\\n9 We use the 5K validation split of the 2017 dataset from https://cocodataset.org/#download.\\n\\n10 While we use MS-COCO in this study as the source of our original captions, one advantage of our counterfactual generation approach is that the input dataset itself does not require paired image and text data.\\n\\n11 While the MS-COCO 5K validation split has 25,014 captions, COCO-Counterfactuals includes only 17,410 of them due to our filtering criteria. Thus, for a fair comparison in our experiments, hereafter we refer to this subset of the 5K validation split including only those 17,410 captions and their paired original images as the MS-COCO dataset.\\n\\n5\"}"}
{"id": "7AjdHnjIHX", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This section aims to show that, in addressing the challenges associated with low-resource settings involving multimodal data (see Section 1), our proposed novel data generation pipeline can serve as an efficient and scalable framework to automatically create high quality multimodal counterfactual examples in COCO-Counterfactuals (COCO-CFs). Toward this goal, we first employ human evaluation to analyze COCO-CFs. We then show that COCO-CFs can be used as a challenging dataset for model evaluation on zero-shot image-text retrieval and image-text matching tasks.\\n\\n4.1 Human Evaluation of COCO-Counterfactuals\\n\\nWe employ professional data annotators to conduct a human study on the quality of COCO-CFs. For each of the 34,820 images in COCO-CFs, we have at least one annotator choose whether the corresponding original or counterfactual caption best fits the image. Annotators can also choose \\\"both\\\" if both captions describe the image equally well, or \\\"neither\\\" if neither caption accurately describes the image. 10% of the images are labeled by 3 different individuals to estimate inter-annotator agreement, with the remaining images each labeled by a single annotator (see Appendix B.2 for additional details).\\n\\nTable 1 provides the percentage of images from COCO-CFs which were matched to their correct caption by the human annotators. We also report the percentage of incorrect matches (i.e., the wrong caption was picked as best describing the image) as well as the percentage of \\\"both\\\" and \\\"neither\\\" labels. Overall, 73% of images were correctly matched to their corresponding caption (see Appendix A.3 for an analysis of incorrect matches). Images generated from the counterfactual caption had a 10% greater incidence of incorrect caption selections than those generated from the original caption. This could be due to the constraints imposed on the counterfactual image by Prompt-to-Prompt (i.e., shared attention weights with the original image), which increases the likelihood that the generated image lacks some of the details in its corresponding caption.\\n\\nThe Fleiss' kappa coefficient for the 10% of images labeled by three annotators was 0.74, indicating strong agreement among the annotators who participated in this study. Among those images which had label disagreement, 47.4% of the labels were correct, 27.3% were incorrect, and 18.6% selected \\\"neither.\\\" This suggests that many of the disagreements are associated with images for which the correct caption choice is more ambiguous.\\n\\nWhile we employed human annotators to validate the quality of COCO-Counterfactuals for this analysis, our automated counterfactual generation approach does not require the use of human annotators to produce a new dataset. Indeed, our experiments described subsequently in Section 5.1 show that COCO-Counterfactuals which were labeled as incorrect by humans have no negative impact on training data augmentation.\\n\\n4.2 COCO-Counterfactuals for Model Evaluation\\n\\nMotivated by prior work which has proposed using counterfactuals as challenging test sets in NLP (Kaushik et al., 2019; Gardner et al., 2020), we further investigate whether our COCO-CFs can serve a similar purpose for state-of-the-art multimodal vision-language models such as CLIP, Flava (Singh et al., 2022), BridgeTower (Xu et al., 2022) and ViLT (Kim et al., 2021) for the zero-shot image-text retrieval and image-text matching tasks. We employed the HuggingFace implementations of these model in our experiments (see Appendix A.6 for more detail).\"}"}
{"id": "7AjdHnjIHX", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2: Image-text retrieval performance on COCO-CFs and human-evaluated COCO-CFs for BridgeTower and Flava models.\\n\\n| Model          | COCO-CFs | Human-evaluated COCO-CFs |\\n|----------------|----------|--------------------------|\\n|                | Recall R@1 | Recall R@5 | Recall R@10 | Recall R@1 | Recall R@5 | Recall R@10 |\\n| BridgeTower    | 21.72 (-51%) | 46.94 (-35%) | 58.65 (-29%) | 17.93 (-47%) | 38.94 (-35%) | 49.95 (-30%) |\\n| Flava          | 26.36 (-41%) | 54.1 (-25%) | 66.13 (-20%) | 21.44 (-37%) | 45 (-25%) | 56.39 (-21%) |\\n\\nLargest drops of performance against the baseline are in boldface.\\n\\n---\\n\\n### Figure 3: ITM score differences computed for existing multimodal models on COCO-Counterfactuals dataset.\\n\\n### 4.2.1 Zero-shot Image-text Retrieval\\n\\nWe evaluate the zero-shot image-text retrieval (ITR) performance of pre-trained Flava and BridgeTower models on COCO-CFs as well as human-evaluated COCO-CFs, which consists of only image-text pairs that were correctly matched in our human evaluation study (Section 4.1). Since pre-trained CLIP was employed in our counterfactual image generation process (see Section 3.2), it is not suitable for zero-shot ITR setting. Thus, we only report its evaluation in Appendix A.6 for completeness. For baselines, we evaluate ITR performance of these models on the MS-COCO dataset.\\n\\nTable 2 reports ITR performance (i.e., Recall at 1, 5, and 10) on COCO-CFs and human-evaluated-COCO-CFs for pre-trained BridgeTower and Flava models. The percentages enclosed within parentheses indicate the change in performance of a model on an evaluated dataset versus the performance of that model on MS-COCO (baseline). We observe that the performance of BridgeTower and Flava decreases significantly (up to 51% and 57%, respectively) compared to the baseline's performance on both COCO-CFs and human-evaluated-COCO-CFs. These results demonstrate that COCO-Counterfactuals can serve as a challenging test set for SOTA multimodal vision-language models.\\n\\n### 4.2.2 Image-text Matching\\n\\nTypically, during pre-training for image-text matching (ITM), multimodal models learn to differentiate actual image-text pairs from alternative images or captions which are randomly sampled from a dataset. By design, our COCO-CFs have the potential to make this task significantly more challenging by requiring models to also differentiate between minimally-edited image or text candidates. We measure the magnitude of this increased difficulty by comparing the difference in ITM scores between actual image-text pairs and their corresponding counterfactual or randomly sampled alternatives. Let \\\\((C_o, I_o)\\\\) denote an original image-text pair from MS-COCO, \\\\(I_s\\\\) denote our synthetically-generated image corresponding to \\\\(C_o\\\\), and \\\\((C_c, I_s)\\\\) denote our corresponding synthetically-generated counterfactual image-text pair in COCO-Counterfactuals. We further denote \\\\((C_r, I_r)\\\\) as a different original image-text pair randomly sampled from MS-COCO such that \\\\(I_o \\\\neq I_r\\\\). For a given pre-trained...\"}"}
{"id": "7AjdHnjIHX", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"multimodal model, we compute the following metrics using its ITM scoring function $G$:\\n\\n$$IR_r = G(C_r, I_r) - G(C_r, I_o)$$\\n\\n$$IR_c = G(C_c, I_{sc}) - G(C_o, I_{so})$$\\n\\n$$TR_r = G(C_r, I_r) - G(C_o, I_r)$$\\n\\n$$TR_c = G(C_c, I_{sc}) - G(C_o, I_{so})$$\\n\\n$IR_r$ and $TR_r$ scores can be viewed as measuring the confidence of a model's image or text retrieval (respectively) over two real image-text pairs from MS-COCO. Similarly, $IR_c$ and $TR_c$ scores measure image or text retrieval confidence, but using matched image-text pairs from COCO-Counterfactuals dataset. For all metrics, values greater than zero indicate that a model scores the correct image-text pair as more similar than its random or counterfactual alternative. Larger positive values can be viewed as indicating greater confidence in the model's correct discernment between the alternatives.\\n\\nFigure 3 plots the distribution of these four metrics for three pre-trained multimodal models: CLIP, BridgeTower, and ViLT. All three models exhibit a significant negative distribution shift when presented with counterfactual alternatives rather than random alternatives, demonstrating the increased difficulty of COCO-Counterfactuals for existing models. A significant number of COCO-Counterfactuals are also incorrectly scored (i.e., have ITM score difference less than zero) by all three models. Even in cases where the counterfactual alternatives can be correctly discerned, we posit that the much smaller values of $IR_c$ and $TR_c$ may improve the efficiency of training through the increased difficulty of the ITM task.\\n\\n4.2.3 Discussion\\n\\nWhen used as a test set, COCO-Counterfactuals by design evaluate the robustness of models to minimal changes in paired image-text data. Table 2 and Figure 3 show that existing models perform significantly worse when evaluated on COCO-Counterfactuals. Additionally, we find that training these same models on COCO-Counterfactuals produces an average relative improvement of 24.3% in image-text retrieval performance on withheld counterfactual examples (see Table 5 of Appendix A.1). These results point to the usefulness our dataset for evaluating and improving the robustness of multimodal models to counterfactual changes.\\n\\n5. COCO-Counterfactuals for Training Data Augmentation\\n\\nThis section aims to evaluate whether COCO-Counterfactuals can serve as an alternative to real data for training data augmentation in low-resource scenarios. We train a fully unfrozen pre-trained CLIP model with its contrastive loss using various combinations of real data from MS-COCO and COCO-CFs datasets (see Appendix B.3 for additional training details). In order to investigate the robustness of models trained on COCO-CFs, we evaluate them on OOD datasets for image-text retrieval and image recognition. For baselines, we report the performance of pre-trained CLIP (i.e., without any additional training) as well as a CLIP model which has been additionally trained using only real data from MS-COCO. We repeat each of our training experiments with 25 different random seeds and report both the mean and standard deviation of performance measured across all random seeds. We also validate the statistical significance of performance improvements obtained by models trained on COCO-CFs using one-tailed t-tests.\\n\\n5.1 Image-text Retrieval\\n\\nTo evaluate OOD performance on the image-text retrieval task that CLIP was trained for, we use the 1K test set of Flickr30k dataset (Young et al., 2014). Table 3 reports the zero-shot performance of the baselines as well as CLIP trained with varying amounts of the original MS-COCO and COCO-CFs datasets. We observe that all CLIP models trained with COCO-Counterfactuals outperform pre-trained CLIP by an average of 5 points, based on the mean performance across text and image retrieval settings. Additionally, our best model trained with 20,894 COCO-Counterfactuals provides statistically significant improvements relative to training only on the real MS-COCO dataset across all settings. We also found that COCO-Counterfactuals improve in-domain performance on the MS-COCO test set, which we detail in Appendix A.5.\\n\\nTo investigate the potential impact of COCO-Counterfactuals which were labeled incorrectly by human annotators, we repeated our training data augmentation experiments using only image-text pairs which were correctly matched in our human evaluation study (Section 4.1). Overall, we found...\"}"}
{"id": "7AjdHnjIHX", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Table 3: Image-text retrieval performance on the OOD Flickr30k 1K test set for pre-trained CLIP and CLIP models trained on varying amounts of data from MS-COCO and COCO-CFs datasets.\\n\\n| Training dataset | $D_{train}$ | % CFs | CIFAR10 | CIFAR100 | Food101 | Caltech101 | Caltech256 | ImageNet | Mean  |\\n|------------------|-------------|-------|---------|----------|---------|------------|------------|----------|-------|\\n| None (pre-trained CLIP) | 0 | 0% | 88.8 | 64.17 | 59.25 | 78.36 | 83.43 | 90.32 | 84.13 |\\n| MS-COCO | 13,928 | 0% | 89.21 | 0.3 | 63.89 | 0.4 | 82.67 | 0.2 | 92.77 | 0.1 | 85.05 | 0.2 | 59.55 | 0.2 | 78.85 | 0.2 |\\n| MS-COCO + COCO-CFs | 13,928 | 50% | 89.45 | 0.3 | 66.67 | 0.4 | 83.13 | 0.2 | 92.63 | 0.1 | 85.21 | 0.2 | 59.66 | 0.2 | 79.46 | 0.2 |\\n| MS-COCO + COCO-CFs | 34,820 | 60% | 89.16 | 0.3 | 66.88 | 0.4 | 82.12 | 0.2 | 92.87 | 0.1 | 84.95 | 0.2 | 59.22 | 0.3 | 79.20 | 0.2 |\\n| MS-COCO + COCO-CFs | 41,784 | 67% | 88.51 | 0.5 | 65.97 | 0.5 | 82.06 | 0.2 | 92.77 | 0.1 | 84.59 | 0.2 | 58.88 | 0.2 | 78.80 | 0.2 |\\n\\nBest results are in boldface. Results which use COCO-CFs are underlined when a one-tailed t-test indicates that their improvement over training only on MS-COCO is statistically significant ($p \\\\leq 0.05$).\\n\\n### Table 4: Zero-shot classification accuracy of pre-trained CLIP and CLIP models trained on varying amounts of data from MS-COCO and COCO-CFs datasets. All other settings are identical to Table 3.\\n\\n| Training dataset | $D_{train}$ | % CFs | CIFAR10 | CIFAR100 | Food101 | Caltech101 | Caltech256 | ImageNet | Mean  |\\n|------------------|-------------|-------|---------|----------|---------|------------|------------|----------|-------|\\n| None (pre-trained CLIP) | 0 | 0% | 88.8 | 64.17 | 59.25 | 78.36 | 83.43 | 90.32 | 84.13 |\\n| MS-COCO | 13,928 | 0% | 89.21 | 0.3 | 63.89 | 0.4 | 82.67 | 0.2 | 92.77 | 0.1 | 85.05 | 0.2 | 59.55 | 0.2 | 78.85 | 0.2 |\\n| MS-COCO + COCO-CFs | 13,928 | 50% | 89.45 | 0.3 | 66.67 | 0.4 | 83.13 | 0.2 | 92.63 | 0.1 | 85.21 | 0.2 | 59.66 | 0.2 | 79.46 | 0.2 |\\n| MS-COCO + COCO-CFs | 34,820 | 60% | 89.16 | 0.3 | 66.88 | 0.4 | 82.12 | 0.2 | 92.87 | 0.1 | 84.95 | 0.2 | 59.22 | 0.3 | 79.20 | 0.2 |\\n| MS-COCO + COCO-CFs | 41,784 | 67% | 88.51 | 0.5 | 65.97 | 0.5 | 82.06 | 0.2 | 92.77 | 0.1 | 84.59 | 0.2 | 58.88 | 0.2 | 78.80 | 0.2 |\\n\\n### 5.2 Image Recognition\\n\\nDespite being trained for image-text retrieval, CLIP has exhibited impressive performance at zero-shot image recognition. Using the same approach as Radford et al. (2021) for the image recognition task (i.e., form a sentence \\\"A photo of a \\\\{c\\\\}\\\" for each class label c to obtain image-text matching scores), we evaluate whether CLIP models trained on COCO-Counterfactuals exhibit competitive OOD performance improvement to baselines' performance in this zero-shot classification setting.\\n\\nUsing the same CLIP models trained on varying amounts of MS-COCO and COCO-CFs, Table 4 reports their zero-shot classification accuracy on six image recognition datasets. We observe that training with an approximately 50-50 split of MS-COCO & COCO-CFs provides the best overall performance, offering improvements over pre-trained CLIP (without any additional training) on all datasets except Food101 and outperforming training with only MS-COCO on most datasets (see Appendix A.2 for additional analysis of performance differences).\\n\\n### 5.3 Discussion\\n\\nRecent work investigating the suitability of synthetic training data for image recognition tasks has found that synthetic image data is much less efficient than real data, requiring 5x more synthetic training samples to achieve similar performance as models trained on real data (He et al., 2022). In contrast, our results show that training data augmentation with COCO-Counterfactuals is at least as efficient (Table 3) and sometimes more efficient (Table 4) than data augmentation with an identical amount of real data ($|D_{train}| = 13,928$). This finding suggests that our approach could be particularly valuable in low-resource settings where paired image-text data is scarce.\"}"}
{"id": "7AjdHnjIHX", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Consistent with prior work on training data augmentation with NLP counterfactuals (Howard et al., 2022; Joshi and He, 2022), Tables 3 and 4 show that improvements in OOD performance with increasing amounts of counterfactual examples reaches a saturation point, beyond which additional data augmentation does not lead to further improvements. For image-text retrieval on Flickr30k (Table 3), this saturation point is reached with a 40 / 60% mixture of MS-COCO / COCO-Counterfactuals in the training dataset. In contrast, Table 4 shows that the saturation point for the OOD image recognition datasets is reached with a 50 / 50% split based on the mean of the six datasets. These results suggest that the optimal mixture of real examples and synthetically generated counterfactual examples may differ depending on the evaluation task and dataset.\\n\\nWhile training data augmentation with COCO-Counterfactuals produces statistically significant performance improvements relative to training with only real data, the overall magnitude of these improvements is limited and varies by evaluation setting. COCO-Counterfactuals produce the largest improvements on zero-shot image recognition tasks, where its overall mean improvement over pre-trained CLIP is twice as large as that achieved by training on an equivalent amount of real data from MS-COCO. However, OOD generalization performance varies by dataset, which further analysis suggests, is related to domain gaps between altered subjects in COCO-Counterfactuals and the domain of the evaluation dataset (see Appendix A.2 for details).\\n\\n6 Conclusion\\n\\nWe proposed an automated data generation methodology for creating counterfactual examples from image-text pairs to address the challenge of low-resource settings involving multimodal data. This approach was used to create COCO-Counterfactuals (COCO-CFs), a high-quality synthetic dataset of paired image-text counterfactuals derived from MS-COCO captions. COCO-CFs are challenging for existing pre-trained multimodal models and significantly increase the difficulty of the zero-shot image-text retrieval and image-text matching tasks. Our experiments demonstrate that augmenting training data with COCO-CFs improves OOD generalization on multiple downstream tasks.\\n\\nIn this work, we focused on the creation of task-agnostic counterfactual examples. A promising direction for future research is the adaptation of our approach to produce task-specific counterfactuals. For example, in the case of image recognition, the counterfactual changes could be limited to a targeted label distribution to produce counterfactual examples more tailored to the end task. Alternatively, task-specific model failures or spurious correlations could be diagnosed and used as a basis for determining which counterfactual changes to consider when creating the counterfactual captions. We believe that such approaches have the potential to produce counterfactuals which are more targeted for improving specific model deficiencies.\\n\\nAnother opportunity for future work is larger-scale automatic generation of counterfactual examples to enable full counterfactual pre-training of multimodal models. Additionally, we believe that extending our image-text counterfactuals to the video domain could be a promising path towards improving video transformers through counterfactual data augmentation.\\n\\nLimitations & Ethical Concerns\\n\\nMotivated by the desire to produce minimal-edit counterfactuals, we only considered changes to nouns. This is a common strategy for NLP counterfactuals (see Appendix B.1.1 for discussion), but alternative generation strategies such as controlled text decoding (Howard et al., 2022) could be used to enable a larger range of counterfactual changes, in addition to alterations of adjectives or verbs. We leave investigation of these directions to future studies.\\n\\nDue to a limited compute budget, we only explored generating COCO-Counterfactuals using Stable Diffusion. Additionally, our training data augmentation experiments were limited to a single model (CLIP). It is possible that other text-to-image generation models may exhibit better performance for generating counterfactual image-text data. Additionally, the benefits of counterfactual data augmentation may vary for different multimodal vision-language models.\\n\\nDespite the impressive recent improvements in text-to-image generation capabilities, models such as Stable Diffusion have well-known limitations that should be considered when utilizing datasets which are derived from them (see Appendix C.5 for a detailed discussion). We do not foresee significant risks of security threats or human rights violations in our work. However, the automated nature of our image generation process may introduce the possibility of our COCO-Counterfactuals dataset containing images that some individuals may consider inappropriate or offensive.\"}"}
{"id": "7AjdHnjIHX", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\nMartin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. 2019. Invariant risk minimization. arXiv preprint arXiv:1907.02893.\\n\\nFederico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal Ladhak, Myra Cheng, Debora Nozza, Tatsunori Hashimoto, Dan Jurafsky, James Zou, and Aylin Caliskan. 2023. Easily accessible text-to-image generation amplifies demographic stereotypes at large scale. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pages 1493\u20131504.\\n\\nSteven Bird, Ewan Klein, and Edward Loper. 2009. Natural language processing with Python: analyzing text with the natural language toolkit. O\u2019Reilly Media, Inc.\\n\\nTim Brooks, Aleksander Holynski, and Alexei A Efros. 2023. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18392\u201318402.\\n\\nNitay Calderon, Eyal Ben-David, Amir Feder, and Roi Reichart. 2022. Docogen: Domain counterfactual generation for low resource domain adaptation. arXiv preprint arXiv:2202.12350.\\n\\nHila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. 2023. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42(4):1\u201310.\\n\\nJaemin Cho, Abhay Zala, and Mohit Bansal. 2022. Dall-eval: Probing the reasoning skills and social biases of text-to-image generative models. arXiv preprint arXiv:2202.04053.\\n\\nJacob Eisenstein. 2022. Uninformative input features and counterfactual invariance: Two perspectives on spurious correlations in natural language. arXiv preprint arXiv:2204.04487.\\n\\nAmir Feder, Katherine A Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret E Roberts, et al. 2022. Causal inference in natural language processing: Estimation, prediction, interpretation and beyond. Transactions of the Association for Computational Linguistics, 10:1138\u20131158.\\n\\nRinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2022. Stylegan-nada: Clip-guided domain adaptation of image generators. ACM Transactions on Graphics (TOG), 41(4):1\u201313.\\n\\nMatt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, et al. 2020. Evaluating models' local decision boundaries via contrast sets. arXiv preprint arXiv:2004.02709.\\n\\nRobert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. 2020. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665\u2013673.\\n\\nRuifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and Xiaojuan Qi. 2022. Is synthetic data from generative models ready for image recognition? arXiv preprint arXiv:2210.07574.\\n\\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 2022. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626.\\n\\nPhillip Howard, Gadi Singer, Vasudev Lal, Yejin Choi, and Swabha Swayamdipta. 2022. Neurocounterfactuals: Beyond minimal-edit counterfactuals for richer data augmentation. arXiv preprint arXiv:2210.12365.\\n\\nNitish Joshi and He He. 2022. An investigation of the (in) effectiveness of counterfactually augmented data. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3668\u20133681.\\n\\nDivyansh Kaushik, Eduard Hovy, and Zachary C Lipton. 2019. Learning the difference that makes a difference with counterfactually-augmented data. arXiv preprint arXiv:1909.12434.\"}"}
{"id": "7AjdHnjIHX", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583\u20135594. PMLR.\\n\\nHector Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, pages 740\u2013755. Springer.\\n\\nNan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. 2022. Compositional visual generation with composable diffusion models. In European Conference on Computer Vision, pages 423\u2013439. Springer.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.\\n\\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015. Deep learning face attributes in the wild. In Proceedings of the IEEE international conference on computer vision, pages 3730\u20133738.\\n\\nAlexandra Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and Yacine Jernite. 2023. Stable bias: Analyzing societal representations in diffusion models. arXiv preprint arXiv:2303.11408.\\n\\nAengus Lynch, Gb\u00e8tondji JS Dovonon, Jean Kaddour, and Ricardo Silva. 2023. Spawrious: A benchmark for fine control of spurious correlation biases. arXiv preprint arXiv:2303.05470.\\n\\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2021. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2018. Language models are unsupervised multitask learners. Technical Report, OpenAI.\\n\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125.\\n\\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.\\n\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2021. High-resolution image synthesis with latent diffusion models.\\n\\nShiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. 2019. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731.\\n\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022. Photo-realistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494.\\n\\nDvir Samuel, Rami Ben-Ari, Simon Raviv, Nir Darshan, and Gal Chechik. 2023. It is all about where you start: Text-to-image generation with seed selection. arXiv preprint arXiv:2304.14530.\"}"}
{"id": "7AjdHnjIHX", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Patrick Schramowski, Manuel Brack, Bj\u00f6rn Deiseroth, and Kristian Kersting. 2023. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22522\u201322531.\\n\\nRavi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Aur\u00e9lie Herbelot, Moin Nabi, Enver Sangineto, and Raffaella Bernardi. 2017. Foil it! find one mismatch between image and language caption. arXiv preprint arXiv:1705.01359.\\n\\nAmanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. 2022. FLA V A: A foundational language and vision alignment model. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 15617\u201315629. IEEE.\\n\\nSahil Singla and Soheil Feizi. 2021. Salient imagenet: How to discover spurious features in deep learning? arXiv preprint arXiv:2110.04301.\\n\\nTristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. 2022. Winoground: Probing vision and language models for visio-linguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5238\u20135248.\\n\\nBrandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov. 2023. Effective data augmentation with diffusion models. arXiv preprint arXiv:2302.07944.\\n\\nJoshua Vendrow, Saachi Jain, Logan Engstrom, and Aleksander Madry. 2023. Dataset interfaces: Diagnosing model failures using controllable counterfactual generation. arXiv preprint arXiv:2302.07865.\\n\\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Simas Sakenis, Jason Huang, Yaron Singer, and Stuart Shieber. 2020. Causal mediation analysis for interpreting neural nlp: The case of gender bias. arXiv preprint arXiv:2004.12265.\\n\\nZhao Wang and Aron Culotta. 2021. Robustness to spurious correlations in text classification via automatically generated counterfactuals. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14024\u201314031.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface's transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771.\\n\\nTongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel S Weld. 2021. Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models. arXiv preprint arXiv:2101.00288.\\n\\nKai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. 2020. Noise or signal: The role of image backgrounds in object recognition. arXiv preprint arXiv:2006.09994.\\n\\nXiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, and Nan Duan. 2022. Bridge-tower: Building bridges between encoders in vision-language representation learning. arXiv preprint arXiv:2206.08657.\\n\\nLinyi Yang, Jiazheng Li, Padraig Cunningham, Yue Zhang, Barry Smyth, and Ruihai Dong. 2021. Exploring the efficacy of automatically generated counterfactuals for sentiment analysis. arXiv preprint arXiv:2106.15231.\\n\\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378.\"}"}
{"id": "7AjdHnjIHX", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 COCO-Counterfactuals\\n\\nImprove Model Robustness to Counterfactual Changes\\n\\nBy design, COCO-Counterfactuals may offer greater improvements to the robustness of models to minimal or counterfactual changes in images. Such examples are unlikely to be present in the datasets used previously to evaluate OOD generalization. Therefore, we also evaluate the performance of models on a withheld test set of COCO-Counterfactuals to determine their image-text retrieval capabilities on in-domain counterfactual examples. Specifically, we withhold 30% of the original-counterfactual paired examples in COCO-Counterfactuals for testing and train the pre-trained CLIP, BridgeTower, and Flava models on the remainder, with 56% of the total dataset used for training and 14% used as a development set.\\n\\nTable 5 compares the performances of CLIP, BridgeTower, and Flava models trained on COCO-Counterfactuals to those trained on an equivalent amount of real examples from MS-COCO and to their pre-trained versions. We observe that training on COCO-Counterfactuals results in a mean improvement of 11.83, 21.55, and 11.47 relative to the pre-trained CLIP, BridgeTower, and Flava models, respectively. This represents an average relative improvement of 24.3% for each model over the performance of its pre-trained version. In addition, the CLIP, BridgeTower, and Flava models that were trained on COCO-Counterfactuals achieve a mean absolute improvement of 6.06, 10.08, and 5.28, respectively, relative to those that were trained on MS-COCO. The greater magnitude of these performance gains relative to our OOD image-text retrieval evaluations (Table 3) suggests that training on COCO-Counterfactuals improves model robustness to counterfactual changes, which are not present in our (non-counterfactual) OOD evaluation datasets.\\n\\n| Text Retrieval | Image Retrieval |\\n|---------------|-----------------|\\n| Pre-trained Models | Training dataset | R@1 | R@5 | R@10 | R@1 | R@5 | R@10 | Mean |\\n| CLIP None (pre-trained CLIP) | 50.96 | 79.33 | 86.45 | 47.89 | 77.19 | 85.73 | 71.26 |\\n| MS-COCO | 57.17 | 84.23 | 90.66 | 55.45 | 84.00 | 90.65 | 77.03 |\\n| COCO-CFs | 65.03 | 90.26 | 94.99 | 64.09 | 89.52 | 94.62 | 83.09 |\\n| BridgeTower None (pre-trained BridgeTower) | 35.26 | 65.31 | 76.73 | 28.77 | 56.63 | 68.46 | 55.19 |\\n| MS-COCO | 41.78 | 71.78 | 81.88 | 44.68 | 75.38 | 84.48 | 66.66 |\\n| COCO-CFs | 54.37 | 83.08 | 90.53 | 56.63 | 84.48 | 91.36 | 76.74 |\\n| Flava None (pre-trained Flava) | 34.40 | 66.63 | 78.02 | 51.55 | 80.64 | 88.24 | 66.58 |\\n| MS-COCO | 46.70 | 76.36 | 85.68 | 52.55 | 81.08 | 88.43 | 71.80 |\\n| COCO-CFs | 54.39 | 83.35 | 90.27 | 57.97 | 85.11 | 91.38 | 77.08 |\\n\\nTable 5: Image-text retrieval performance on a withheld COCO-CFs test set.\\n\\nA.2 Analysis of Differences in OOD Generalization on Image Recognition Datasets\\n\\nTo better understand the differences in OOD generalization performance across datasets, we measured the frequency in which the altered subjects used to produce COCO-Counterfactuals overlapped with class labels. Specifically, we define the COCO-CFs Label Frequency for each image recognition dataset as the total number of COCO-Counterfactuals in which one or more of the dataset's labels matched one of the two altered subjects used to produce the counterfactual pair. Table 6 provides the COCO-CFs Label Frequency for each image recognition dataset along with the change in OOD performance relative to pre-trained CLIP after training on various sizes of COCO-CFs (see Appendix B.3.1 for a definition of dataset sizes). We observe that datasets having a higher COCO-CFs Label Frequency generally achieve larger improvements in OOD generalization.\\n\\nNote that the image-text retrieval performance of the three pre-trained models (CLIP, BridgeTower, and Flava) on the in-domain COCO-Counterfactuals test set in Table 5 are higher than the respective values on the entire COCO-Counterfactuals dataset provided in Tables 2 and 13. This is expected because the retrieval space of the in-domain COCO-Counterfactuals test set is only 30% of the entire COCO-Counterfactuals dataset.\"}"}
{"id": "7AjdHnjIHX", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Frequency of class label occurrence in COCO-CFs and absolute change ($\\\\Delta$) in performance relative to pre-trained CLIP after training on various sizes of COCO-CFs.\\n\\n| Dataset | CIFAR100 | Caltech101 | Caltech256 | CIFAR10 | ImageNet | Food101 |\\n|---------|----------|------------|------------|---------|----------|---------|\\n| Errors  | 3446     | 354        | 744        | 398     | 887      | 28      |\\n| Frequency | 2.50     | 2.31       | 1.78       | 0.65    | 0.41     | -1.04   |\\n| $\\\\Delta$ | 2.63     | 2.55       | 1.52       | 0.36    | -0.03    | -2.05   |\\n| $\\\\Delta$ | 1.80     | 2.45       | 1.16       | -0.29   | -0.37    | -2.11   |\\n\\nTable 7: Image-text retrieval performance on the in-domain COCO-CFs test set.\\n\\nThe Pearson correlation coefficient between COCO-CFs Label Frequency and the 18 performance change measurements in Table 6 is 0.522 with a p-value of 0.026, indicating statistically significant positive correlation.\\n\\nThese results suggest that a major contributor to the variation in OOD generalization performance across datasets is the overlap between the evaluation dataset domain and the set of subjects which are altered in COCO-Counterfactuals. Food101, the only dataset which saw no improvement in performance on our best-performing COCO-CFs training dataset, had only 28 cases of overlap between its label set and the subject alterations in COCO-CFs. In contrast, the greatest performance improvements were achieved on CIFAR100, for which 3446 COCO-CFs had subject alterations matching at least one label from the dataset. These findings point to the potential usefulness of targeting counterfactual changes for task-specific datasets.\\n\\nA.3 Analysis of Errors in COCO-Counterfactuals Identified by Human Annotators\\n\\nIn this section, we analyze errors in COCO-Counterfactuals using the labels assigned by human annotators (Section 4.1). Specifically, we consider an error to be any image-text pair from the COCO-Counterfactuals dataset for which the human annotator did not select the correct caption for the corresponding image.\\n\\nA.3.1 Manual Categorization of Errors\\n\\nTo investigate potential failure cases in our counterfactual generation approach, we randomly sampled and categorized 100 image-text pairs which were identified as errors by the human annotators. Table 7 provides the percentage of sampled COCO-Counterfactuals which were assigned to various error categories. Additionally, Tables 8 and 9 provide examples of counterfactual pairs which were assigned to the top-six most frequent error categories.\\n\\nWe found that 66% of the sampled errors can be attributed to known limitations of existing text-to-image diffusion models (Chefer et al., 2023; Samuel et al., 2023; Cho et al., 2022), which include the categories for failure to generate a subject or object (e.g., Table 8, row 1), failure to generate fine-grained details (e.g., Table 8, row 2), failure to accurately depict spatial relationships (e.g., Table 9, row 2), failure to generate the correct number of objects described in the prompt (e.g., Table 9, row 3), and failure to bind attributes such as color.\"}"}
{"id": "7AjdHnjIHX", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Failure to generate subject/object\\n\\nA cat walking through a kitchen\\n\\nA cat walking through a field\\n\\nFailure to generate fine-grained details\\n\\nA man playing Wii in a dirty room\\n\\nA kid playing Wii in a dirty room\\n\\nHyponymy relationship between altered subjects\\n\\nTwo kids in pink and purple jackets standing by a fence\\n\\nTwo girls in pink and purple jackets standing by a fence\\n\\nTable 8: Examples of failure cases identified by manual error analysis\"}"}
