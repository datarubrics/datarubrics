{"id": "3BxYAaovKr", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ego4D Goal-Step: Toward Hierarchical Understanding of Procedural Activities\\n\\nYale Song, Eugene Byrne, Tushar Nagarajan, Huiyu Wang, Miguel Martin, Lorenzo Torresani\\n\\nFundamental AI Research (FAIR), Meta\\n\\nhttps://github.com/facebookresearch/ego4D-goalstep\\n\\nAbstract\\n\\nHuman activities are goal-oriented and hierarchical, comprising primary goals at the top level, sequences of steps and substeps in the middle, and atomic actions at the lowest level. Recognizing human activities thus requires relating atomic actions and steps to their functional objectives (what the actions contribute to) and modeling their sequential and hierarchical dependencies towards achieving the goals. Current activity recognition research has primarily focused on only the lowest levels of this hierarchy, i.e., atomic or low-level actions, often in trimmed videos with annotations spanning only a few seconds. In this work, we introduce Ego4D Goal-Step, a new set of annotations on the recently released Ego4D with a novel hierarchical taxonomy of goal-oriented activity labels. It provides dense annotations for 48K procedural step segments (430 hours) and high-level goal annotations for 2,807 hours of Ego4D videos. Compared to existing procedural video datasets, it is substantially larger in size, contains hierarchical action labels (goals - steps - substeps), and provides goal-oriented auxiliary information including natural language summary description, step completion status, and step-to-goal relevance information. We take a data-driven approach to build our taxonomy, resulting in dense step annotations that do not suffer from poor label-data alignment issues resulting from a taxonomy defined a priori. Through comprehensive evaluations and analyses, we demonstrate how Ego4D Goal-Step supports exploring various questions in procedural activity understanding, including goal inference, step prediction, hierarchical relation learning, and long-term temporal modeling.\\n\\n1 Introduction\\n\\nRecognizing complex patterns of human activities has been the subject of extensive research in computer vision and the broader AI community [2, 25, 46, 8, 23, 22]. However, progress has been relatively slow compared to object and scene understanding [59, 32, 36, 62]. One of the main obstacles has been the scarcity of large-scale datasets annotated with a comprehensive taxonomy representing complex human activities. While object recognition benefits from WordNet [15] that provides an extensive taxonomy of objects found in everyday scenarios, activity recognition is presented with unique difficulties because there is currently no established taxonomy in place that encompasses the broadly varying granularities of activities, from atomic actions (e.g., pick-up cup, sit down) to procedural sequences (e.g., make lasagna).\\n\\nIn our quest to build a new dataset for human activity recognition, we draw inspiration from the psychology literature. Studies have shown the inherent hierarchical nature of human behavior [5, 12], comprising the primary goals at the highest level, intermediate steps and their substeps in the middle, and atomic actions at the lowest level. Social cognitive theories [3] suggest that this hierarchy is formed by human agents deliberately setting goals, anticipating potential consequences of different actions.\"}"}
{"id": "3BxYAaovKr", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Ego4D Goal-Step offers hierarchical procedural activity labels with three distinct levels: goal - step - substep. Each annotation comes with a time interval, categorical label, and natural language description. It also provides auxiliary information including step summaries, task completion status (is_continued), task relevance (is_relevant), and procedural activity indicator (is_procedural).\\n\\nactions, and planning out a sequence of steps and their substeps to achieve the desired goal in a hierarchical manner. Although the planned sequence of actions may not necessarily align with the actual execution order [5], inferring and reasoning over the hierarchical representations has been shown to be crucial for understanding human behavior [12, 41].\\n\\nMost existing activity datasets have focused on the lowest levels of this hierarchy \u2013 i.e., atomic or low-level actions \u2013 often in trimmed videos and with annotations spanning only a few seconds [25, 46, 8, 23]. This focus on atomic actions has even raised questions about the necessity of temporal modeling in existing video tasks [40, 11, 44], and its suitability for studying real-world videos containing higher-level activities over longer temporal extents.\\n\\nIn response to this, procedural activities \u2013 those that involve performing a series of steps to achieve predetermined goals \u2013 have recently gained particular attention [29, 60, 49, 61, 48, 34, 4, 43, 55]. Recognizing goal-oriented steps that unfold over a long time horizon requires modeling long-term temporal context, making it a challenging long-form video understanding task. However, existing datasets are either small-scale [29, 3, 43], do not model high-level goals, or ignore the hierarchical relationship between steps [49, 61, 4]. Furthermore, the step taxonomy is commonly built from external sources detached from videos (e.g., text articles from wikiHow [60, 49, 61]), resulting in misalignment between the constructed label space and the observed data. Consequently, a significant portion of the video is left unlabeled, offering an incomplete record of activities.\\n\\nTo address these limitations, we introduce Ego4D Goal-Step, a new set of annotations on Ego4D [22] with a newly developed hierarchical taxonomy for procedural activities. It contains two main components: (1) The goal annotation set consists of 7,353 videos, totaling 2,807 hours, labeled with a taxonomy of 319 goals grouped into 34 scenarios. This is a focused set covering the 72% of Ego4D videos and is intended to provide a large-scale training and evaluation dataset for goal inference. (2) The step annotation set focuses on the cooking scenario portion of Ego4D, and is intended specifically for procedural activity recognition. It consists of 47,721 densely labeled step and substep segments, amounting to 430 hours in total, annotated based on a taxonomy of 86 goals and 514 fine-grained steps commonly performed in diverse home kitchen environments across many countries.\\n\\nEgo4D Goal-Step stands out from existing procedural video datasets for its several unique features. (1) We develop our taxonomy in a data-driven manner to accurately represent the activities, rather than...\"}"}
{"id": "3BxYAaovKr", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Dataset statistics.\\n\\nWe report \u201cEgo4D Goal-Step\u201d \u2014 the subset that comes with hierarchical step labels, and \u201cEgo4D Goal-Step (goal labels)\u201d \u2014 the full set which includes videos with goal labels but no step labels. \u201cHier\u201d indicates datasets with hierarchical label spaces. Breakfast [29] and Assembly101 [43] provide both coarse- and fine-level segments, analogous to our step and substep segments; we report their combined numbers. Assembly101 [43] provides 12 synchronized views per recording; we report single view statistics to make the numbers compatible with other datasets.\\n\\n1. The annotations form a three-level hierarchy (goal - steps - substeps), a unique feature that is unavailable in most existing datasets. When combined with existing Ego4D labels that are action-centric (e.g., narrations, FHO, and moments), it creates an attractive multi-level hierarchical label space.\\n2. Every annotation includes a time interval, a categorical label, and a natural language description, enabling both detection and language grounding tasks.\\n3. Procedural segments also come with step summary descriptions, useful for video summarization [38].\\n4. Additional goal-oriented information, such as task relevance and completion status signals, supports novel research directions such as task graph inference and progress tracking [33].\\n5. Our dataset inherits the large scale and diversity of Ego4D, capturing immersive views of procedural activities from a first-person perspective, and featuring long untrimmed videos that reveal the unfolding of individual goals over time. Together, these strengths make Ego4D Goal-Step a significant step forward in procedural activity understanding. See Figure 1 for an illustration of these features.\\n\\nIn summary, we introduce Ego4D Goal-Step, the largest available egocentric dataset for procedural activity understanding, with 2,807 hours of videos with specific goal labels and 430 hours of segments with fine-grained step/substep labels. In what follows, we describe our annotation and taxonomy development process. We also provide a comprehensive analysis of new annotations and compare them with existing procedural video datasets. Finally, we demonstrate the value of our annotations for temporal detection and grounding tasks, and analyze the results in the context of the unique properties of our dataset.\\n\\n2 Related Work\\n\\nActivity recognition has a two-decade history in computer vision. Early works have tackled atomic action classification using datasets of seconds-long video clips at relatively small scales [42, 20, 35, 30, 47]. Following the success of deep neural networks, several datasets have focused on scaling up by leveraging online videos [27, 1, 8, 28, 21, 37]. Recognizing the need for long-form video modeling, several datasets have also been proposed for action detection in untrimmed videos [6, 46, 26, 54, 23]. Recently, the community has expanded the scope by developing datasets for procedural activities. The typical dataset construction process involves selecting procedural tasks beforehand, e.g., various recipes in cooking, then collecting data for the pre-selected tasks through participant recordings or by mining online video repositories. Participant-recorded datasets like Breakfast [29] and Assembly101 [43] benefit from a controlled collection setting, enabling the development of a taxonomy aligned with the data, and resulting in dense annotations and hierarchical step segments similar to ours. However, they capture limited diversity (e.g., 10 cooking recipes) and are smaller in scale.\"}"}
{"id": "3BxYAaovKr", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On the other hand, Cross-Task and COIN are Internet-mined datasets that benefit from the scalability. However, they rely on external resources to develop a taxonomy (e.g., wikiHow), leading to label spaces that do not capture precisely and comprehensively the activities represented in the videos. Consequently, a large portion of videos remains unlabeled, with densities of labeled segments in the low 40% (see Table 1). Furthermore, annotated segments often only partially match with step labels (e.g., due to subtle variations in objects used, step ordering, etc.), resulting in weakly-labeled data. These datasets are also often non-hierarchical and represent steps at a single level.\\n\\nCompared to existing procedural video datasets, Ego4D Goal-Step provides dense annotations at scale, a well-aligned taxonomy, and a hierarchical label space \u2013 all at the same time \u2013 thanks to our hierarchical partitioning approach to data annotation and taxonomy development. With a wide range of procedural videos sourced from Ego4D, accompanied by dense annotations with a comprehensive taxonomy, our dataset enables procedural activity understanding at scale.\\n\\n3 Ego4D Goal-Step\\n\\n3.1 Ego4D: The Status Quo\\n\\nEgo4D is annotated in a variety of ways. All videos are annotated with scenarios and narrations which provide high-level and low-level descriptions of actions, respectively. Scenarios provide coarse categorization of activities (e.g., construction, arts and crafts), while narrations describe a camera-wearer's action at a specific time. The narrations are interaction-centric, focusing on individual, atomic actions that a camera-wearer performs over a short time period. For example \u201cC picks up the spoon,\u201d \u201cC pets the dog,\u201d \u201cC unscrews the screw,\u201d where C represents the camera-wearer.\\n\\nWhile the narrations offer valuable information to understand simple movements and hand-object interactions, in the broader context of activity understanding, they are limited. Human actions are not performed arbitrarily \u2014 they are intentional and are done to accomplish a particular goal [3]. For example, C picks up the spoon to add sugar to coffee; C unscrews the screw to detach a bicycle wheel. These goals are left hidden in existing narrations. Moreover, these goals themselves are part of more structured activities. \u201cAdding sugar\u201d is a step in the process of making coffee, and \u201cDetaching the bicycle wheel\u201d is a step towards replacing a punctured tire tube. While narrations explicitly capture what is immediately happening, they do not reveal why, or more broadly, to what end.\\n\\nNarrations form the scaffolding for various other annotations on smaller subsets of Ego4D. Forecasting hands and objects (FHO) annotations involve atomic actions parsed into simpler (verb, noun) tuples. For example, the long-term forecasting task involves predicting the sequence of future atomic actions, without capturing the overarching goal. Moments query annotations go one step higher, representing composite actions that involve sequences of atomic actions like \u201cwash dishes in the sink\u201d or \u201cput on safety equipment.\u201d While they are higher-level, they are still short-term activities and are not connected by their long-term task structure. Moreover, they cover a small set of categories (roughly 100), span an inconsistent set of granularities (e.g., the atomic action \u201ccut dough\u201d, and the high level \u201coperate the dough mixing machine\u201d), and are not intended to cover complete activities/goals.\\n\\nCollectively, these annotations inherit the narrow scope of narrations and offer only a short-term understanding of human activity, limiting their value for procedural activity understanding in intentional, long-form and structured human activity.\\n\\n3.2 Ego4D Goal-Step: Annotation and Taxonomy Development\\n\\nTo address this gap, we annotate Ego4D for procedural video understanding. Ego4D videos are collected without prearranged scripts or step-by-step instructions. As a consequence, the complete set of activities present in the dataset is unknown and a taxonomy cannot be established beforehand. We overcome this in a data-driven manner, using a hierarchical partitioning approach for annotation and taxonomy development. In short, we first ask annotators to identify the primary goals depicted in each video. Next, they delve into each goal segment to identify the individual steps and their corresponding action sequences. Then, again, annotators recursively analyze each action segment to further annotate steps at lower levels to construct a complete step hierarchy. Throughout this process, we present the annotators with an incomplete and evolving taxonomy and encourage them to...\"}"}
{"id": "3BxYAaovKr", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"suggest missing categories. We review them periodically and update our taxonomy over the course of annotation. The whole process involves five stages:\\n\\nStage 1: Goal taxonomy initialization\\nWe initialize a goal taxonomy with Ego4D scenario labels and manually subdivide them into specific goal categories. For example, we expand the \\\"cooking\\\" scenario into popular dishes (e.g., \\\"make pasta,\\\" \\\"make omelet\\\") and the \\\"construction site\\\" scenario into specific construction tasks (e.g., \\\"paint drywall,\\\" \\\"build wooden deck\\\").\\n\\nStage 2: Goal annotation and taxonomy update\\nAnnotators watch full-length videos (not clips cut to shorter lengths) and identify distinctive goal segments, assigning a goal category and providing free-form text description, e.g., \\\"make_omelet\\\" and \\\"making omelet with toasted bread\\\" as shown in Figure 1. We emphasize the importance of providing full-length videos to the annotators because short-length clips will inherently lack the overall goal of actions (e.g., a clip capturing boiling salted water will not give any clue that it was part of making pasta). For missing categories, annotators choose \\\"other\\\" and describe it in text. Auxiliary information, including procedural activity indicator (is_procedural), task continuity from previous segments (is_continued), and bullet-point summaries of procedural steps, are also annotated.\\n\\nWhile the annotation is in progress, we iteratively refine our goal taxonomy. This process is largely manual. It involves mapping keywords in descriptions to existing goal categories and adding new categories to the taxonomy when necessary. We also manually verify the correctness of keyword mapping by visual inspection. This process is done periodically in batches of annotations.\\n\\nStage 3: Step taxonomy initialization\\nWe prompt a large language model, LLaMA-7B [50], to generate step-by-step instructions for each goal. This provides a concise, but potentially incomplete, list of step candidates per goal category. Moreover, it often misses routine steps performed in a given environment, e.g., washing hands in a home kitchen. To represent missing steps, we create a \\\"catch-all\\\" bucket for each scenario and use the same model to generate commonly occurring steps.\\n\\nStage 4: Step annotation and taxonomy update\\nAnnotators review full-length videos with goal annotations and identify step segments, assigning a step category and natural language description. To ensure annotators capture step-level granularity that reveals intentions and not low-level physical movements, we provide a specific template, \\\"the camera wearer did X in order to Y,\\\" and ask them to prioritize intentions (Y) over physical movements (X) in their annotations. As shown in Figure 1, this allows us to collect labels that reveal the functional objectives behind actions (e.g., \\\"toast bread,\\\" \\\"crack eggs,\\\" \\\"beat eggs with chopsticks\\\") rather than just the description of low-level actions. Similar to stage 2, annotators indicate the procedural nature of step segments and describe substeps in bullet-points. Additionally, they provide task-relevance information (is_relevant) for each step segment using a multiple-choice question (\\\"essential,\\\" \\\"optional,\\\" and \\\"irrelevant\\\"). They are further requested to find a relevant wikiHow article and provide its URL, for annotation accuracy and educational resources. The taxonomy update follows the same process as stage 2.\\n\\nStage 5: Substep annotation and taxonomy update\\nFinally, we ask annotators to further partition step segments into individual substeps. This process is largely the same as stage 4, and it shares the same step taxonomy. The only difference is that we provide annotators with both the corresponding goal and step annotations, and that we encourage annotators to focus on lower levels of granularity.\\n\\n3.3 Dataset Analysis\\nStatistics\\nTable 1 compares Ego4D Goal-Step with existing procedural activity datasets. As shown, Ego4D Goal-Step represents the largest available procedural video dataset in terms of total number of hours annotated with step labels. Goal annotations are available for a subset covering 72% of Ego4D after discarding non-procedural or uninteresting videos, totaling 2,807 hours of videos. At the segment level, there are a total of 47,721 segments for steps and substeps combined. On average, each goal segment has 23.82 step segments, and each step segment has 4.6 substep segments.\\n\\nAnnotation\\nWe contracted a third party vendor to manage the annotation process, and checked privacy and ethical compliance through rigorous reviews. We completed 10 iterations for goal annotation (stage 2), 10 iterations for step annotation (stage 4), and 8 iterations for substep annotation (stage 5). Each iteration involved annotators providing labels for a batch of videos, us reviewing them, and updating the taxonomy. On average, annotators took 2\u00d7 the video duration to annotate.\"}"}
{"id": "3BxYAaovKr", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2 illustrates the distribution of goal, step, and substep segment durations. On average, goal segments span 19.64 minutes, while step and substep segments last 50.03 seconds and 19.49 seconds, respectively. The average substep duration aligns with datasets that derive their taxonomy from wikiHow [4, 60, 49], which suggests that our data-driven taxonomy has granularity similar to that found in annotations based on fixed step-by-step instructions. The average duration of our steps/substeps is 32.5 seconds, which is 6 times longer than Breakfast [29] and 22 times longer than Assembly101 [43]. This shows that most of the step/substep segments capture longer-duration actions than [30, 43] without sacrificing annotation density or capturing short-term atomic actions.\\n\\nOur taxonomy contains 319 goal categories grouped into 34 scenarios. Among these, the cooking scenario provides procedural step annotations, comprising 86 goal categories and 514 step categories. We use these step categories from the taxonomy to annotate both step and substep segments. The dataset exhibits a long-tail distribution, with 125 goal categories representing 90% of labeled goal segments and 209 step categories covering 90% of labeled step/substep segments.\\n\\nEgo4D Goal-Step comes with various goal-oriented activity labels. About 92% of the goal segments are annotated with descriptions (3.03 words per sentence) and 63% with summaries (5.43 sentences per summary, 4.92 words per sentence). 100% of the step and substep segments come with descriptions (4.44 words per sentence) and 15% of them include summaries (4.38 sentences per summary, 3.1 words per sentence). Figure 2 shows the distributions of auxiliary labels.\\n\\nWe provide data splits for training (70%), validation (15%), and testing (15%) purposes. The splits were made at the video level, ensuring balanced inclusion of step categories across the splits. We release full annotations for the training and validation splits while withholding the test split.\\n\\n4 Experiments\\n4.1 Tasks\\n\\nGoal/step localization\\nLocalizing temporal action segments within a long, untrimmed video has numerous applications ranging from activity recognition to automatic chapterization for efficient navigation of instructional videos. Following temporal action localization [45], we formulate the task as predicting tuples of (start_time, end_time, class_category) encompassing goals and individual steps given a long untrimmed video. We use ActionFormer [56] and EgoOnly [51] as our baseline models for their superior performance demonstrated in various action localization benchmarks. We evaluate the results using the detection mAP averaged over goal or step categories, and over temporal IoUs {0.1, 0.2, 0.3, 0.4, 0.5}.\\n\\nOnline goal/step detection\\nWhile the localization task above focuses on offline inference scenarios, here we consider its online counterpart. The task is formulated as predicting goal/step categories at each timestamp as they unfold in a streaming input video. Models must utilize information from the\"}"}
{"id": "3BxYAaovKr", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Main results on tasks supported by our dataset. We report standard deviation over 8 runs.\\n\\nFor all experiments except for the EgoOnly [51] baseline, we use pre-computed clip-level features extracted densely from each video using Omnivore [19], which are publicly available for download on the official Ego4D repository (we used \\\\texttt{omnivore_video_swinl}). The Omnivore model has been pretrained on a combination of multiple modalities (images, videos, and 3D data) in a supervised fashion and has been demonstrated to achieve strong generalization ability across a wide variety of vision tasks. EgoOnly [51] pretrains the ViT [14] backbone from scratch on the raw frames of Ego4D [22] using the MAE [24] objective, then further finetunes it on a combination of four existing action recognition datasets (Kinetics-600 [7], Ego4D Moments [22], EPIC-Kitchens-100 [13], COIN [49]) in a supervised fashion. For EgoOnly on the online detection task, we attach a single-layer linear prediction head on top of the pretrained ViT backbone and train the entire model end-to-end. Each prediction is made on input frames with 2 second temporal context. For EgoOnly on the offline localization task, we take the ViT backbone finetuned on the online detection task and attach the ActionFormer head [56] on top, and train just the prediction head while keeping the ViT backbone frozen throughout training. For all tasks, we use open source baseline implementations and adapt hyperparameters for the proposed dataset (details in Supp.).\\n\\n4.2 Main results\\n\\nTable 2 reports the main results for all the tasks on both the validation and test sets. EgoOnly [51] achieves strong performance on offline step localization and online step detection, demonstrating the effectiveness of its egocentric pretraining strategy and corroborating the strong empirical evidence presented in their paper. Note that, compared to ActionFormer [56] and LSTR [53] that leverage precomputed features from Omnivore [19], EgoOnly finetunes its ViT backbone directly on our dataset by solving the temporal segmentation task, providing further performance improvement over the two other baselines.\\n\\n1 Actionformer (https://github.com/happyharrycn/actionformer_release), LSTR (https://github.com/amazon-science/long-short-term-transformer), and VSLNet (https://github.com/EGO4D/episodic-memory/tree/main/NLQ/VSLNet).\"}"}
{"id": "3BxYAaovKr", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Validation set results on hierarchical goal-step relationship.\\n\\nLeft: localization and online detection. Combining goal and step/substep instances does not help each other, likely due to the large difference in segment lengths and the limited number of goal samples. Metric: detection/per-frame mAP.\\n\\nRight: grounding. Training jointly on steps and substeps outperforms other alternatives, highlighting the benefit of the hierarchical nature of our step annotations. Metric: Recall@1, IoU=0.3.\\n\\nThese results suggest that exploiting the step-substep hierarchy is clearly beneficial (right table), while effectively leveraging the goal-step hierarchy needs further investigation (left table).\\n\\nCompared to the step prediction tasks, goal prediction requires a much longer temporal context. For instance, step segments have an average duration of 32.5 seconds, whereas goal segments have an average duration of 1946.9 seconds. This makes it a challenging long-form video understanding task.\\n\\nWe report ActionFormer and LSTR baseline results but omit EgoOnly results due to its focus on relatively shorter-term temporal context. Specifically, EgoOnly's ViT backbone is fine-tuned with 2-second clips from our dataset, providing limited long-term context to the learned representations and, as a result, achieving inferior results compared to baselines that leverage Omnivore features. This highlights the challenging nature of our dataset and warrants further investigation into refinements in training and modeling approaches to achieve a more balanced performance across goal and step prediction tasks.\\n\\nOn the step grounding task, models achieve roughly similar recall scores on the validation and test splits. Note that a subset of text queries in each split belong to step classes that are not seen during training. For example, the step category label \\\"Add the melted chocolate and blend until smooth\\\" does not occur in the training set, but its corresponding natural language description \\\"Add chocolate balls to blender jar\\\" is still groundable. The results on this zero-shot subset are understandably lower \u2013 4.3 and 3.8 Recall@1, mIoU=0.3 on validation and test splits, respectively.\\n\\n4.3 How do models exploit the hierarchical goal-step-substep relationship?\\n\\nNext, we study the hierarchical relationship among goals, steps, and substeps in our dataset. First, we explore the goal-step hierarchy by comparing models trained on goals only, steps and substeps only, and all combined. Table 3 (left) shows that simply combining all instances does not help the individual tasks of localization and online detection, likely due to the non-overlapping taxonomy, the large difference in segment lengths, as well as limited number of goal samples.\\n\\nHowever, once we switch to step-substep hierarchy, in Table 3 (right), we find that jointly training on steps and substeps (last row) is superior to models trained on substeps only (A) or steps only (B). This joint training is significantly more effective than pairing steps with their parent descriptions (C).\\n\\nImportantly, the gain over (A) and (B) is not simply an effect of dataset size. In (D-E), we replace steps/substeps with an equivalent amount of Ego4D narration data from the same videos to match the total training set size of the result in the last row. The narrations capture low-level actions performed by the camera-wearer (e.g., \\\"camera-wearer picks up a pan\\\") and have been shown to improve performance in egocentric grounding models [39]. Our results show that while adding narrations can offer a small improvement, jointly training on steps and substeps remains a superior strategy.\\n\\nPut together, the results suggest that jointly training with steps and substeps leads to models that are aware of the sequential and hierarchical relationships between steps, leading to stronger performance across both levels.\\n\\nThis was done by concatenating the step and substep descriptions and using that as our hierarchy-aware query. More sophisticated, embedding-based step and substep fusion performed worse in our experiments.\"}"}
{"id": "3BxYAaovKr", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.4 Which types of steps are easier to recognize?\\n\\nWe evaluate grounding models based on the role of steps in the overall procedure: essential, optional, or irrelevant. Essential steps are necessary for completing the task (e.g., cracking eggs for cooking an omelet), optional steps are relevant but not essential (e.g., adding sriracha sauce to an omelet), and irrelevant steps are unrelated (e.g., answering phone while cooking). Figure 3 shows that essential steps are the most difficult to recognize. This is somewhat counterintuitive since they occur frequently within the task (as evidenced by the largest number of instances reported in the figure) and are contextual with the activity. However, we found that they are difficult to distinguish from other steps of the related tasks as they tend to involve the same set of objects, scenes, and actions, e.g., flatten dough on table, knead dough, coat dough with flour in the task \u201cMake bread,\u201d requiring fine-grained recognition. Conversely, optional steps (e.g., dispose eggshells, wear apron) and irrelevant steps (e.g., use phone, drink tea) are visually distinct in the context of the procedure and thus are easier to recognize, overcoming the disadvantage of relatively low training data (number above each bar) and despite their loose (or missing) connection to the goal.\\n\\n4.5 Can long-term temporal context benefit procedural activity recognition?\\n\\nRecognizing procedural activities in egocentric videos, especially in the online inference regime, requires long-term temporal context to accumulate enough past history [52]. For example, recognizing different but similar recipes may become possible only after observing completion of certain steps. We quantitatively study this property by exploiting the hierarchical structure in Ego4D Goal-Step. In the localization task, this is achieved by adjusting the feature sampling stride before feeding features into the ActionFormer [56] that localizes goals and/or steps. This changes the effective temporal context consumed, especially when the theoretical receptive field does not cover the full video. Results are visualized in Figure 4 (left) with error bars representing standard deviation of 8 runs. We find that a larger temporal context is indeed required to disambiguate goals, while steps favor a much smaller context for finer temporal details. Additionally, we find goal-step joint training with a small context adversely affects goal localization due to the dominance of shorter steps in the training signal. As a result, the step results with and without goal instances are almost overlapping.\"}"}
{"id": "3BxYAaovKr", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In online goal/step detection tasks, we follow the experimental setting of LSTR [53] that varies both short-term and long-term memory lengths. For online goal detection (Figure 4 middle), we find that increasing the long-term memory to 256 seconds and longer leads to noticeable gains. Furthermore, a larger short term memory consistently leads to better results. These findings demonstrate that predicting goals indeed requires long-term temporal context. This aligns with our intuition that disambiguating the goals requires piecing together evidence that needs to be accumulated over a long temporal span. For instance, in the case of preparing a recipe, it may only become fully recognizable after several ingredients have been used and multiple steps have been completed.\\n\\nOn the other hand, for online step detection (Figure 4 right), performance increases when we switch the short term memory length from 4 seconds to 8 seconds, but plateaus when we further push to 16 seconds. Also, longer long-term memory does not seem to improve performance; in fact, when a 4 second short-term memory is used, performance decreases when the long-term memory exceeds 64 seconds, which is twice as long as the average step segment duration of 32.5 seconds. This pattern is consistent with our observation on step localization, suggesting that step detection favors fine-grained information captured in short-term temporal context over longer context.\\n\\n5 Conclusion\\nWe presented Ego4D Goal-Step, a new set of annotations with a hierarchical taxonomy of procedural activity labels on the recently released Ego4D. It is larger than existing procedural video datasets, contains hierarchical action labels (goals - steps - substeps), and provides various auxiliary information useful for procedural activity understanding. We demonstrated three distinct task scenarios supported by our dataset \u2013 temporal localization, online detection, and grounding \u2013 and analyzed how various research questions can be explored such as hierarchical learning and long-term video modeling. The comprehensive nature of our dataset calls for future investigations into other aspects of procedural activity understanding. One promising direction is incorporating existing Ego4D annotations for more comprehensive analyses of procedural activities. For instance, narrations and FHO/Moments provide atomic action labels that can be combined with our dataset to form a 4-level hierarchy (goals - steps - substeps - actions). Furthermore, hand & object interaction annotations provide object state information that can enable object state-based progress monitoring. We eagerly anticipate the emergence of active research threads pursuing these directions.\\n\\nLimitations and societal impact\\nWe acknowledge that Ego4D Goal-Step is intended for research purposes and should not be regarded as a comprehensive dataset encompassing the full range of daily human activities. Models trained on our dataset may exhibit biases towards the specific activities included in the dataset, resulting in a limited coverage of our everyday living scenarios.\\n\\nReferences\\n[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici, B. Varadarajan, and S. Vijayanarasimhan. Youtube-8m: A large-scale video classification benchmark. arXiv preprint arXiv:1609.08675, 2016.\\n[2] J. K. Aggarwal and M. S. Ryoo. Human activity analysis: A review. Acm Computing Surveys (Csur), 43(3):1\u201343, 2011.\\n[3] A. Bandura. Social cognitive theory: An agentic perspective. Asian journal of social psychology, 2(1):21\u201341, 1999.\\n[4] S. Bansal, C. Arora, and C. Jawahar. My view is the best view: Procedure learning from egocentric videos. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XIII, pages 657\u2013675. Springer, 2022.\\n[5] M. Botvinick and D. C. Plaut. Doing without schema hierarchies: a recurrent connectionist approach to normal and impaired routine sequential action. Psychological review, 111(2):395, 2004.\\n[6] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision and pattern recognition, pages 961\u2013970, 2015.\\n[7] J. Carreira, E. Noland, A. Banki-Horvath, C. Hillier, and A. Zisserman. A short note about kinetics-600. arXiv preprint arXiv:1808.01340, 2018.\"}"}
{"id": "3BxYAaovKr", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] J. Carreira and A. Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299\u20136308, 2017.\\n\\n[2] C.-Y. Chang, D.-A. Huang, D. Xu, E. Adeli, L. Fei-Fei, and J. C. Niebles. Procedure planning in instructional videos. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XI, pages 334\u2013350. Springer, 2020.\\n\\n[3] A. Chevan and M. Sutherland. Hierarchical partitioning. The American Statistician, 45(2):90\u201396, 1991.\\n\\n[4] J. Choi, C. Gao, J. C. Messou, and J.-B. Huang. Why can't i dance in the mall? learning to mitigate scene bias in action recognition. Advances in Neural Information Processing Systems, 32, 2019.\\n\\n[5] R. P. Cooper and T. Shallice. Hierarchical schemas and goals in the control of sequential behavior. 2006.\\n\\n[6] D. Damen, H. Doughty, G. M. Farinella, A. Furnari, E. Kazakos, J. Ma, D. Moltisanti, J. Munro, T. Perrett, W. Price, et al. Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. International Journal of Computer Vision, pages 1\u201323, 2022.\\n\\n[7] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Ruder, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n\\n[8] C. Fellbaum. Wordnet. In Theory and applications of ontology: computer applications, pages 231\u2013243. Springer, 2010.\\n\\n[9] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. D. Iii, and K. Crawford. Datasheets for datasets. Communications of the ACM, 64(12):86\u201392, 2021.\\n\\n[10] R. Ghoddoosian, S. Sayed, and V. Athitsos. Hierarchical modeling for task recognition and action segmentation in weakly-labeled instructional videos. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1922\u20131932, 2022.\\n\\n[11] R. Girdhar and K. Grauman. Anticipative video transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 13505\u201313515, 2021.\\n\\n[12] R. Girdhar, M. Singh, N. Ravi, L. van der Maaten, A. Joulin, and I. Misra. Omnivore: A single model for many visual modalities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16102\u201316112, 2022.\\n\\n[13] L. Gorelick, M. Blank, E. Shechtman, M. Irani, and R. Basri. Actions as space-time shapes. IEEE transactions on pattern analysis and machine intelligence, 29(12):2247\u20132253, 2007.\\n\\n[14] R. Goyal, S. Ebrahimi Kahou, V. Michalski, J. Materzynska, S. Westphal, H. Kim, V. Haenel, I. Fruend, P. Yianilos, M. Mueller-Freitag, et al. The \\\"something something\\\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 5842\u20135850, 2017.\\n\\n[15] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18995\u201319012, 2022.\\n\\n[16] C. Gu, C. Sun, D. A. Ross, C. Vonondruck, C. Pantofaru, Y. Li, S. Vijayanarasimhan, G. Toderici, S. Ricco, R. Sukthankar, et al. Ava: A video dataset of spatio-temporally localized atomic visual actions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6047\u20136056, 2018.\\n\\n[17] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000\u201316009, 2022.\\n\\n[18] F. C. Heilbron, V. Escorcia, B. Ghanem, and J. C. Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In 2015 IEEE conference on computer vision and pattern recognition (CVPR), pages 961\u2013970. IEEE, 2015.\\n\\n[19] H. Idrees, A. R. Zamir, Y.-G. Jiang, A. Gorban, I. Laptev, R. Sukthankar, and M. Shah. The thumos challenge on action recognition for videos \\\"in the wild\\\". Computer Vision and Image Understanding, 155:1\u201323, 2017.\"}"}
{"id": "3BxYAaovKr", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei. Large-scale video classification with convolutional neural networks. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 1725\u20131732, 2014.\\n\\nW. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.\\n\\nH. Kuehne, A. Arslan, and T. Serre. The language of actions: Recovering the syntax and semantics of goal-directed human activities. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 780\u2013787, 2014.\\n\\nH. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. Hmdb: a large video database for human motion recognition. In 2011 International conference on computer vision, pages 2556\u20132563. IEEE, 2011.\\n\\nA. Liu, S. Sohn, M. Qazwini, and H. Lee. Learning parameterized task structure for generalization to unseen entities. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 7534\u20137541, 2022.\\n\\nL. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, and M. Pietik\u00e4inen. Deep learning for generic object detection: A survey. International journal of computer vision, 128:261\u2013318, 2020.\\n\\nL. Logeswaran, S. Sohn, Y. Jang, M. Lee, and H. Lee. Unsupervised task graph generation from instructional video transcripts. arXiv preprint arXiv:2302.09173, 2023.\\n\\nZ. Luo, W. Xie, S. Kapoor, Y. Liang, M. Cooper, J. C. Niebles, E. Adeli, and F.-F. Li. Moma: Multi-object multi-actor activity parsing. Advances in Neural Information Processing Systems, 34:17939\u201317955, 2021.\\n\\nM. Marsza\u0142ek, I. Laptev, and C. Schmid. Actions in context. In IEEE Conference on Computer Vision & Pattern Recognition, 2009.\\n\\nS. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, and D. Terzopoulos. Image segmentation using deep learning: A survey. IEEE transactions on pattern analysis and machine intelligence, 44(7):3523\u20133542, 2021.\\n\\nM. Monfort, A. Andonian, B. Zhou, K. Ramakrishnan, S. A. Bargal, T. Yan, L. Brown, Q. Fan, D. Gutfreund, C. von Drick, et al. Moments in time dataset: one million videos for event understanding. IEEE transactions on pattern analysis and machine intelligence, 42(2):502\u2013508, 2019.\\n\\nM. Otani, Y. Song, Y. Wang, et al. Video summarization overview. Foundations and Trends\u00ae in Computer Graphics and Vision, 13(4):284\u2013335, 2022.\\n\\nS. K. Ramakrishnan, Z. Al-Halah, and K. Grauman. Naq: Leveraging narrations as queries to supervise episodic memory. In Computer Vision and Pattern Recognition (CVPR), 2023 IEEE Conference on. IEEE, 2023.\\n\\nK. Schindler and L. Van Gool. Action snippets: How many frames does human action recognition require? In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20138. IEEE, 2008.\\n\\nD. W. Schneider and G. D. Logan. Retrieving information from a hierarchical plan. Journal of Experimental Psychology: Learning, Memory, and Cognition, 33(6):1076, 2007.\\n\\nC. Schuldt, I. Laptev, and B. Caputo. Recognizing human actions: a local svm approach. In Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004., volume 3, pages 32\u201336. IEEE, 2004.\\n\\nF. Sener, D. Chatterjee, D. Shelepov, K. He, D. Singhania, R. Wang, and A. Yao. Assembly101: A large-scale multi-view video dataset for understanding procedural activities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21096\u201321106, 2022.\\n\\nL. Sevilla-Lara, S. Zha, Z. Yan, V. Goswami, M. Feiszli, and L. Torresani. Only time can tell: Discovering temporal data for temporal modeling. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 535\u2013544, 2021.\\n\\nZ. Shou, D. Wang, and S.-F. Chang. Temporal action localization in untrimmed videos via multi-stage cnns. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1049\u20131058, 2016.\"}"}
{"id": "3BxYAaovKr", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[46] G. A. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev, and A. Gupta. Hollywood in homes: Crowdsourcing data collection for activity understanding. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part I 14, pages 510\u2013526. Springer, 2016.\\n\\n[47] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.\\n\\n[48] H. L. Tan, H. Zhu, J.-H. Lim, and C. Tan. A comprehensive survey of procedural video datasets. Computer Vision and Image Understanding, 202:103107, 2021.\\n\\n[49] Y. Tang, D. Ding, Y. Rao, Y. Zheng, D. Zhang, L. Zhao, J. Lu, and J. Zhou. Coin: A large-scale dataset for comprehensive instructional video analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1207\u20131216, 2019.\\n\\n[50] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\\n\\n[51] H. Wang, M. K. Singh, and L. Torresani. Ego-only: Egocentric action detection without exocentric transferring. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023.\\n\\n[52] C.-Y. Wu and P. Krahenbuhl. Towards long-form video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1884\u20131894, 2021.\\n\\n[53] M. Xu, Y. Xiong, H. Chen, X. Li, W. Xia, Z. Tu, and S. Soatto. Long short-term transformer for online action detection. Advances in Neural Information Processing Systems, 34:1086\u20131099, 2021.\\n\\n[54] S. Yeung, O. Russakovsky, N. Jin, M. Andriluka, G. Mori, and L. Fei-Fei. Every moment counts: Dense detailed labeling of actions in complex videos. International Journal of Computer Vision, 126:375\u2013389, 2018.\\n\\n[55] A. Zala, J. Cho, S. Kottur, X. Chen, B. O\u02d8guz, Y. Mehdad, and M. Bansal. Hierarchical video-moment retrieval and step-captioning. arXiv preprint arXiv:2303.16406, 2023.\\n\\n[56] C.-L. Zhang, J. Wu, and Y. Li. Actionformer: Localizing moments of actions with transformers. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part IV, pages 492\u2013510. Springer, 2022.\\n\\n[57] H. Zhang, A. Sun, W. Jing, and J. T. Zhou. Span-based localizing network for natural language video localization. arXiv preprint arXiv:2004.13931, 2020.\\n\\n[58] H. Zhang, A. Sun, W. Jing, and J. T. Zhou. Temporal sentence grounding in videos: A survey and future directions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.\\n\\n[59] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu. Object detection with deep learning: A review. IEEE transactions on neural networks and learning systems, 30(11):3212\u20133232, 2019.\\n\\n[60] L. Zhou, C. Xu, and J. Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018.\\n\\n[61] D. Zhukov, J.-B. Alayrac, R. G. Cinbis, D. Fouhey, I. Laptev, and J. Sivic. Cross-task weakly supervised learning from instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3537\u20133545, 2019.\\n\\n[62] Z. Zou, K. Chen, Z. Shi, Y. Guo, and J. Ye. Object detection in 20 years: A survey. Proceedings of the IEEE, 2023.\"}"}
{"id": "3BxYAaovKr", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix A Implementation details\\n\\nFor all the baselines except for EgoOnly [51], we use pre-computed clip-level features extracted densely from each video using Omnivore [19], which are publicly available for download on the official Ego4D repository (we used \\\"omnivore_video_swinl\\\"). These features are extracted with a stride of 16 frames, which is 0.533s or 1.875 features per second on Ego4D videos of 30 fps.\\n\\nFor EgoOnly [51], we use the checkpoint which was pretrained on the raw frames of Ego4D [22] using the MAE [24] objective and then further finetuned with the temporal segmentation objective on a combination of four popular action recognition datasets: Kinetics-600 [7], Ego4D MQ [22], EPIC-Kitchens-100 [13], COIN [49]. In our experiments, we take this checkpoint and further finetune it on the temporal segmentation objective with our step annotations, for 10 epochs. We use the base learning rate of 2e-4 and train the model for 32 epochs, with linear warm-up for 16 epochs. We denote the raw feature stride of 0.533s (i.e. 1.875 fps) by the \\\"effective temporal context\\\" (ETC) reported in Figure 4 left of the main paper. Based on the best results reported in Figure 4 (left), we set the ETC for step localization to be 1\u00d7, for both step-only and goal-step joint experiments. For goal localization, we set it to 8\u00d7 (feature stride of 4.266s) for goal-only experiments and 16\u00d7 (feature stride of 8.533s) for goal-step joint experiments.\\n\\nOnline goal/step detection.\\n\\nWe use LSTR [53] and EgoOnly [51] as our baseline approaches. LSTR combines an encoder that captures the long-term coarse-scale information and a decoder that models short-term information. We use the official implementation of LSTR [53], including the default short-term memory of 8 seconds and the long-term memory of 512 seconds, unless noted otherwise. In our experiments, we found that training the model for 4 epochs yields the best performance on the validation split. We train with the base learning rates of 4e-5 for goals, and 2e-4 for steps. We follow the default hyperparameters provided in the official implementation, and apply the linear learning rate warm-up for the first 40% of the training iterations. Training each model takes less than 4 hours (4 epochs) on a single V100 GPU.\\n\\nFor EgoOnly, we attach a single-layer classification head on top of the pretrained EgoOnly backbone and train the entire network end-to-end with the temporal segmentation objective with the default temporal context of 2 seconds. For online step detection inference and evaluation, we take only the last frame classification output as the prediction at the current timestamp. We use the same model to extract features for offline step localization experiments.\\n\\nStep grounding.\\n\\nFor this task, we use all annotated instances of steps and substeps. We ignore goal segment annotations as they do not inform model selection. This is because many videos contain 3... We note that here the notion of epoch is different from ActionFormer/LSTR because of the differences in training settings; we refer to EgoOnly [51] for details.\"}"}
{"id": "3BxYAaovKr", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a single goal that spans a large portion of the video, which results in trivially high recall scores (as mentioned in Section 4.1).\\n\\nFor our baseline model, we use VSLNet [57]. The model works as follows. First, a sequence of features are extracted for the video and text query. A cross-attention mechanism then aggregates information across modalities to produce one feature per time-step. Finally, two LSTM models aggregate these features to predict the start / end time probabilities at each time-step. We refer the reader to the original paper for full model details [57].\\n\\nEach video is re-sampled to 128 features for training. We train models for 200 epochs with learning rate 1e-3, batch size 32, and select the model with the highest validation set performance for testing. The remaining hyperparameters follow the default for Ego4D NLQ [22]. Training takes approximately 6 hours on two V100 GPUs. We report an average of 8 runs for each result (Table 2 in the main paper).\\n\\nAppendix B Additional dataset statistics\\n\\nSunburst Chart\\n\\n(a) Goal descriptions\\n\\n(b) Step descriptions (cooking scenarios)\\n\\nFigure 5: Distribution of natural language descriptions for goals and steps/sub-steps combined. To create the plot, we removed stop words and stemmed the remaining words. For the goal descriptions, we display the first two words, while for the step/sub-step descriptions, we display the first three words.\\n\\nAppendix C Datasheet for Ego4D Goal-Step\\n\\nWe provide the datasheet following the format suggested by Gebru et al. [16]. We would like to note that Ego4D Goal-Step contains new annotations on existing videos in Ego4D. No new videos were collected or recorded during the annotation process. In all our responses below, the term \u201cdata\u201d specifically refers to the annotations, not the videos associated with Ego4D Goal-Step, unless otherwise noted.\\n\\nC.1 Motivation\\n\\na) For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.\\n\\nEgo4D Goal-Step was created to support research on human activity understanding in long-form egocentric videos. Specifically, it provides procedural activity labels organized in a hierarchical\"}"}
{"id": "3BxYAaovKr", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"structure, capturing three distinct granularities of human actions (goals - steps - substeps). It supports various tasks in procedural activity understanding, including temporal action localization [56] and temporal grounding of step descriptions [58] \u2013 both of which we demonstrate in our paper \u2013 as well as other related tasks including hierarchical action segmentation [17], action anticipation [18], procedure planning [9], and task graph learning [31].\\n\\nb) Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\\n\\nEgo4D Goal-Step was created by Fundamental AI Research (FAIR) at Meta. The authors of this paper are also part of the Ego4D consortium.\\n\\nc) Who funded the creation of the dataset?\\n\\nIf there is an associated grant, please provide the name of the grantor and the grant name and number.\\n\\nThis project is funded by Meta.\\n\\nC.2 Composition\\n\\na) What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.\\n\\nEach instance is a set of activity annotations on a particular video in Ego4D [22], organized in a hierarchical form. We show an example annotation in Listing 1.\\n\\nb) How many instances are there in total (of each type, if appropriate)?\\n\\nEgo4D Goal-Step is comprised of two main components.\\n\\n\u2022 The goal annotation set consists of 7,353 videos, totaling 2,807 hours. This set covers 72% of Ego4D after discarding non-procedural or uninteresting videos, and is intended to provide a large-scale training and evaluation dataset for goal inference.\\n\\n\u2022 The step annotation set focuses on the cooking scenario portion of Ego4D, and is intended specifically for procedural activity recognition. It consists of 47,721 densely labeled segments, amounting to 430 hours in total.\\n\\nListing 1: Example annotation in a JSON format\\n\\n```json\\n{\\n  \\\"9 b58e3ab -7 b6d -4 e79 -9 eea - c21420b0eedc\\\": {\\n    \\\"start_time\\\": 0.0210286458333333,\\n    \\\"end_time\\\": 510.1876953125,\\n    \\\"goal_category\\\": \\\"COOKING: MAKE_OMELET\\\",\\n    \\\"goal_description\\\": \\\"Make omelette\\\",\\n    \\\"goal_wikihow_url\\\": \\\"https://www.wikihow.com/Cook-a-Basic-Omelette\\\",\\n    \\\"summary\\\": [\\n      \\\"Toasting bread on a pan\\\",\\n      \\\"Making omelet\\\",\\n      \\\"Serving omelet with ketchup\\\"\\n    ],\\n    \\\"is_procedural\\\": true,\\n    \\\"segments\\\": [\\n      {\\n        \\\"start_time\\\": 0,\\n        \\\"end_time\\\": 56.99209,\\n        \\\"step_category\\\": \\\"General cooking activity: Toast bread\\\",\\n        \\\"step_description\\\": \\\"Toast bread\\\",\\n        \\\"is_continued\\\": false,\\n        \\\"is_procedural\\\": true,\\n        \\\"is_relevant\\\": \\\"essential\\\",\\n        \\\"summary\\\": [\\n          \\\"heat skillet\\\",\\n          \\\"toast bread\\\",\\n          \\\"trash kitchen waste\\\"\\n        ]\\n      }\\n    ]\\n  }\\n}\\n```\"}"}
{"id": "3BxYAaovKr", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).\\n\\nThe goal annotation set covers 72% of Ego4D. We filtered out videos based on scenarios that are deemed non-procedural or uninteresting. The discarded scenarios are:\\n\\n- **Eating**: \u201cEating\u201d, \u201cEating at a restaurant\u201d, \u201cEating in a canteen\u201d, \u201cEating in hawker center\u201d\\n- **Talking**: \u201cTalking with family members\u201d, \u201cTalking with friends/housemates\u201d, \u201cTalking to colleagues\u201d, \u201cTalking on the phone\u201d\\n- **Attending a meeting**: \u201cAttending a TA session\u201d, \u201cParticipating in a meeting\u201d\\n- **Watching something**: \u201cWatching tv\u201d, \u201cWorking at desk\u201d, \u201cReading books\u201d, \u201cVideo call\u201d, \u201cPlaying games/video games\u201d, \u201cOn a screen (phone/laptop)\u201d, \u201cPlay with cellphone\u201d\\n- **Commuting / moving around**: \u201cBike\u201d, \u201cCycling/jogging\u201d, \u201cCar-commuting, road trip\u201d, \u201cSkateboard/scooter\u201d, \u201cWalking the dog/pet\u201d, \u201cWalking on street\u201d, \u201cClothes, other shopping\u201d\\n\\nThe step annotation set contains only cooking scenario videos due to their strong procedural characteristics.\\n\\nEgo4D metadata includes information about collection sites (university names), which contains geographical information. This allows us to analyze the geographical diversity of the goal and step annotation sets. The distribution of collection sites are (see Figure 6):\\n\\n- **Goal annotation set**: 'cmu': 1675, 'unict': 1214, 'utokyo': 840, 'iiith': 832, 'minnesota': 701, 'frl_track_1_public': 606, 'bristol': 529, 'kaust': 524, 'cmu_africa': 144, 'uniandes': 74, 'nus': 115, 'indiana': 62, 'georgiatech': 37.\\n- **Step annotation set**: 'utokyo': 237, 'iiith': 204, 'bristol': 101, 'minnesota': 96, 'unict': 77, 'kaust': 77, 'cmu_africa': 41, 'nus': 7.\\n\\nWe can see that the goal annotation set closely follows the distribution of the entire Ego4D dataset. The step annotation set is different from those distributions. We note that this is because cooking\"}"}
{"id": "3BxYAaovKr", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"scenario videos were collected only in those collection sites, rather than us intentionally selecting videos from certain collection sites.\\n\\nd) What data does each instance consist of? \\\"Raw\\\" data (e.g., unprocessed text or images) or features? In either case, please provide a description.\\n\\nEach instance consists of time interval, goal/step category and description, text summary, and various other auxiliary information. For full details of the information contained in each instance, please see Listing 1.\\n\\ne) Is there a label or target associated with each instance? If so, please provide a description.\\n\\nThere are various labels associated with each instance. Below shows the definition of each field.\\n\\n\u2022 start_time and end_time: Time interval\\n\u2022 goal_category and step_category: goal and step category label\\n\u2022 goal_description and step_description: goal and step description in natural language\\n\u2022 goal_wikihow_url: A wikiHow URL that best describes the activity captured in a video\\n\u2022 summary: A bullet-pointed summary of steps contained in each segment\\n\u2022 segments: A list of step segments (or substep segments) under a given goal (or step) segment\\n\u2022 is_procedural: A boolean flag indicating whether this segment contains procedural activity.\\n\u2022 is_continued: A boolean flag indicating whether this segment is a continuation of an activity (goal, step, or substep) from the most recent earlier segment of the same activity.\\n\u2022 is_relevant: One of \\\"essential\\\", \\\"optional\\\", \\\"irrelevant\\\" indicating how relevant this segment is to its parent segment.\\n\\nf) Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.\\n\\nEverything that the annotators have provided is included. No data is missing.\\n\\ng) Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)? If so, please describe how these relationships are made explicit.\\n\\nEach individual video in Ego4D contains metadata about collection site (university name), from which relationships can be established across individual instances. This information is available in the Ego4D dataset. In Ego4D Goal-Step, to avoid redundancy, we do not explicitly provide the collection site information, but this can be easily retrieved by using the unique video identifier (the keys in the JSON dictionary, e.g., \\\"9b58e3ab-7b6d-4e79-9eea-c21420b0eedc\\\" in Listing 1).\\n\\nh) Are there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.\\n\\nWe provide data splits for training, validation, and testing purposes. We release full annotations for the training and validation splits while withholding the test split. To facilitate evaluation, we will set up and maintain a test server on EvalAI. Participants can upload their results to the server, where they will be evaluated automatically.\\n\\nWe divide the data into training (70%), validation (15%), and test (15%) splits. The split is performed at the video level to ensure no information leakage across splits. To achieve this, we employed stratified sampling over videos using the train_test_split function from the sklearn Python package. Each video is assigned a single category label. We adopted a greedy assignment approach, iterating over step categories sorted by their frequency. Step labels are assigned to videos until a minimum number is reached, which we set at a minimum of 4 instances per step category.\\n\\ni) Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.\\n\\nAs described in Section 3.2 of the main paper, annotators provide both category labels and natural language descriptions for segments of goals, steps, and substeps identified in a video. When annotators cannot find a category label in the taxonomy, they select \\\"other\\\" and suggest a new category in the description. We periodically review those descriptions labeled with \\\"other\\\" and either map them to existing categories or add them to the taxonomy. This is done through keyword mapping and manual verification. Due to the manual mapping process and the inherent complexity of human activity, it is possible that the process is noisy.\"}"}
{"id": "3BxYAaovKr", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a dataset consumer? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.\\n\\nEgo4D Goal-Step is a new set of annotations on existing videos in Ego4D, and will be hosted directly on the official Ego4D repository alongside other annotations. The repository itself is supported by Meta and the 15 university Ego4D consortium, and the board and consortium exist independent of Meta. There is funded work already underway for future versions of the dataset and no doubts about the long-term persistence of the consortium or the flagship video dataset itself which these annotations are based upon.\\n\\nEach version of the dataset that is updated is marked as an individual update, and prior versions are always available in their original form via the CLI. The dataset will also include appropriate metadata to confirm the correct versions and videos are downloaded and used beyond that guarantee.\\n\\nEgo4D Goal-Step annotations are available via the CLI and do not require a license or any restrictions (once publicly released). For the Ego4D videos themselves, there is no cost, but an individual or entity must sign the license and receive approval to download the access keys as detailed above. Requests are only rejected for incorrect submissions or from being from a country with comprehensive US, UK or EU trade restrictions (currently Crimea, Donetsk, and Luhansk regions of Ukraine, Russia, Cuba, North Korea, Iran, and Syria - though that is subject to change).\\n\\nk) Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor\u2013patient confidentiality, data that includes the content of individuals' non-public communications)? If so, please provide a description.\\n\\nNo information contained in Ego4D Goal-Step is considered confidential.\\n\\nl) Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why.\\n\\nNo.\\n\\nm) Does the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset.\\n\\nNo explicit process was in place to identify any subpopulations during the annotation process. The activity labels were indiscriminate of the identity of the camera wearer who recorded the videos.\\n\\nn) Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? If so, please describe how.\\n\\nIt is not possible to identify individuals from the labels provided in Ego4D Goal-Step. All videos in Ego4D went through a set of de-identification processes to ensure a high standard of data privacy, see Appendix B of Grauman et al. [22] for details.\\n\\no) Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description.\\n\\nNo.\\n\\nC.3 Collection Process\\n\\na) How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If the data was...\"}"}
{"id": "3BxYAaovKr", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.\\n\\nThe videos were directly observable by the annotators, through a specialized tool with a video player and input fields tailored for our annotation (see the annotation user interface in Figure 7).\\n\\nb) What mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)? How were these mechanisms or procedures validated?\\n\\nWe used an internal tool developed at Meta (Figure 7) for web-based annotation. The same tool was used for the creation of the Ego4D dataset, validating its robustness and scalability.\\n\\nc) If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?\\n\\nSee our response in D.2.c.\\n\\nd) Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?\\n\\nWe contracted a third party vendor to manage the annotation process, and checked privacy and ethical compliance through rigorous reviews. The amount of compensation were determined via contract, and the payments were made on a monthly basis. Because of multiple annotation projects involved with the vendor, the exact breakdown for how much they were paid is unknown to us.\\n\\ne) Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.\\n\\nThe entire annotation took 6 months to collect, from late 2022 to mid 2023. The annotation timeframe does not match the creation timeframe of video recordings, which was made in year 2021 through 2022.\\n\\nf) Were any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.\\n\\nYes. This annotation project went through a rigorous internal review process at Meta for privacy and ethical compliance.\\n\\ng) Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?\\n\\nCollection or recording of new videos was not as part of Ego4D Goal-Step annotation. The annotations were provided through a third party vendor who managed contracted workers. The vendor managed all contractors, applying their well-established processes for annotator recruitment, training, auditing, and management. The contractors use our web-based annotation tool (Figure 7) to annotate videos.\\n\\nh) Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.\\n\\nYes, our annotators are contracted workers who fully comprehend the nature of their tasks. They have been duly informed that we are gathering annotations from them, and these annotations will be made available to the public.\\n\\ni) Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.\\n\\nYes, see above.\\n\\nj) If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).\\n\\n...\"}"}
{"id": "3BxYAaovKr", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 6: Comparison of geographical distributions of videos on three different sets: the original Ego4D [22], our goal annotation set, and our step annotation set. The goal annotation set follows the original distribution, while the step annotation set differs because of its focuses cooking scenarios.\\n\\nFigure 7: The user interface used for data annotation.\"}"}
{"id": "3BxYAaovKr", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"No.\\n\\nk) Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation. No.\\n\\nC.4 Processing, cleaning, labeling\\n\\na) Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remaining questions in this section.\\n\\nWe mapped natural language descriptions of goals, steps, and substeps to their corresponding categories via keyword mapping followed by manual verification; see D.2.i and Section 3.2 of the main paper for details.\\n\\nb) Was the \\\"raw\\\" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the \\\"raw\\\" data.\\n\\nYes. We also release the \\\"raw\\\" natural language descriptions in in Ego4D Goal-Step; see \\\"goal_description\\\" and \\\"step_description\\\" in Listing 1.\\n\\nc) Is the software that was used to preprocess/clean/label the data available? If so, please provide a link or other access point. N/A.\\n\\nC.5 Uses\\n\\na) Has the dataset been used for any tasks already? If so, please provide a description.\\n\\nEgo4D Goal-Step is not public yet, so no other papers have used it for their tasks. However, our main paper demonstrates how it can be used in three scenarios: temporal goal/step localization, online goal/step detection, and step grounding. See Section 4 for details.\\n\\nb) Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point. N/A.\\n\\nc) What (other) tasks could the dataset be used for? Besides the tasks we demonstrate in the paper, Ego4D Goal-Step can support other tasks in procedural video understanding, such as hierarchical action segmentation [17], action anticipation [18], procedure planning [9], and task graph learning [31], and video summarization [38].\\n\\nd) Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a dataset consumer might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other risks or harms (e.g., legal risks, financial harms)? If so, please provide a description. Is there anything a dataset consumer could do to mitigate these risks or harms? No.\\n\\ne) Are there tasks for which the dataset should not be used? We acknowledge that Ego4D Goal-Step is intended for research purposes and should not be regarded as a comprehensive dataset encompassing the full range of daily human activities. Similar to other large-scale real-world datasets, Ego4D Goal-Step exhibits skewed distribution of activities, subjects, and settings, despite the best effort in Ego4D to capture real-world authenticity and comprehensive diversity. As such, models trained on our dataset may exhibit biases towards the specific activities included in the dataset, resulting in a limited coverage of our everyday living scenarios.\"}"}
{"id": "3BxYAaovKr", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.6 Distribution\\n\\na) Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.\\n\\nYes. The dataset will be publicly available on the internet.\\n\\nb) How will the dataset be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?\\n\\nEgo4D Goal-Step will be distributed through the official Ego4D download protocol, which can be found in https://ego4d-data.org. There will be no DOI assigned specifically for Ego4D Goal-Step.\\n\\nc) When will the dataset be distributed?\\n\\nWe will release Ego4D Goal-Step around before December 2023.\\n\\nd) Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.\\n\\nEgo4D Goal-Step will be distributed under the standard Ego4D license agreement, which can be found in https://ego4ddataset.com/.\\n\\ne) Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.\\n\\nNo.\\n\\nf) Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.\\n\\nNo.\\n\\nC.7 Maintenance\\n\\na) Who will be supporting/hosting/maintaining the dataset?\\n\\nThe Ego4D consortium will support/host/maintain it.\\n\\nb) How can the owner/curator/manager of the dataset be contacted (e.g., email address)?\\n\\nThe primary contact is Yale Song (yalesong@meta.com).\\n\\nc) Is there an erratum? If so, please provide a link or other access point.\\n\\nNot so far, it has not been released yet.\\n\\nd) Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to dataset consumers (e.g., mailing list, GitHub)?\\n\\nYes, we plan to update the dataset with labeling corrections and newly annotated labels in the future. The updates will be made on the official Ego4D website, https://ego4d-data.org/docs/updates/.\\n\\ne) If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.\\n\\nNo.\\n\\nf) Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers.\\n\\n23\"}"}
{"id": "3BxYAaovKr", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yes, older versions will continue to be supported/hosted/maintained through the Ego4D repository.\\n\\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to dataset consumers? If so, please provide a description.\\n\\nYes, we welcome community contributions through Github and any other forms of communication. These contributions will be validated and verified by us before merged into the dataset. We acknowledge community contributors in our updates, e.g., see an example of our acknowledgement in https://ego4d-data.org/docs/updates/#nlq-annotation-updates.\"}"}
