{"id": "L5aY1mWvXQ", "page_num": 1, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rethinking Evaluation Strategy for Temporal Link Prediction through Counterfactual Analysis\\n\\nAniq Ur Rahman\\\\textsuperscript{1}  Alexander Modell\\\\textsuperscript{2}  Justin P. Coon\\\\textsuperscript{1}\\n\\\\textsuperscript{1}University of Oxford, U.K.  \\\\textsuperscript{2}Imperial College London, U.K.\\naniq.rahman@eng.ox.ac.uk, a.modell@imperial.ac.uk, justin.coon@eng.ox.ac.uk\\n\\nAbstract\\n\\nIn response to critiques of existing evaluation methods for Temporal Link Prediction (TLP) models, we propose a novel approach to verify if these models truly capture temporal patterns in the data. Our method involves a sanity check formulated as a counterfactual question: \u201cWhat if a TLP model is tested on a temporally distorted version of the data instead of the real data?\u201d Ideally, a TLP model that effectively learns temporal patterns should perform worse on temporally distorted data compared to real data. We provide an in-depth analysis of this hypothesis and introduce two data distortion techniques to assess well-known TLP models. Our contributions are threefold: (1) We introduce simple techniques to distort temporal patterns within a graph, generating temporally distorted test splits of well-known datasets for sanity checks. These distortion methods are applicable to any temporal graph dataset. (2) We perform counterfactual analysis on TLP models such as JODIE, TGAT, TGN, and CAWN to evaluate their capability in capturing temporal patterns across different datasets. (3) We propose an alternative evaluation strategy for TLP, addressing the limitations of binary classification and ranking methods, and introduce two metrics \u2013 average time difference (ATD) and average count difference (ACD) \u2013 to provide a comprehensive measure of a model\u2019s predictive performance. The code and datasets are available at: \\\\url{https://github.com/Aniq55/TLPCF.git}\\n\\n1 Introduction\\n\\nIn static graphs, link prediction refers to the task of predicting whether an edge exists between two nodes after having observed other edges in the graph. Temporal link prediction (TLP) is a dynamic extension of link prediction wherein the task is to predict whether a link (edge) exists between any two nodes in the future based on the historical observations (Qin and Yeung, 2023). The predictive capability of TLP models make them useful in applications pertaining to dynamic graphs, such as product recommendations (Qin et al., 2024; Fan et al., 2021), social network content or account recommendation (Fan et al., 2019; Daud et al., 2020), fraud detection in financial networks (Kim et al., 2024), and resource allocation, to name a few.\\n\\nIn the TLP literature (Kumar et al., 2019; Trivedi et al., 2019; Xu et al., 2020; Rossi et al., 2020; Wang et al., 2020; Cong et al., 2023), the TLP task is treated as a binary classification problem where the query\\n\\n\\\\[ q_1 : \\\\text{\u201cDoes an edge exist between the nodes } u \\\\text{ and } v \\\\text{ at time } t?\\\\]\\n\\nis processed by a model and then compared with the ground truth following which metrics such as area under the receiver operating characteristic curve (AU-ROC), and average precision (AP) are reported. The ground truth consists of positive samples, and a fixed number of random negative samples. There are a couple of issues in the binary classification approach. Firstly, the timestamps in the query are restricted to the timestamps present in the ground truth, which makes the evaluation\"}"]}
{"id": "L5aY1mWvXQ", "page_num": 2, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"biased and does not test the model\u2019s performance in the continuous time range. Secondly, checking for the existence of an edge at a specific timestamp is an ill-posed question, and instead the existence of an edge should be queried within a finite time-interval. Lastly, the negative edge sampling strategy, and the number of negative samples per positive sample impact the performance metrics as seen in EXH (Poursafaei and Rabbany 2023).\\n\\nAlternatively, in a rank-based approach, the query is formulated as:\\n\\n\\\\[ q_2 : \\\\text{\u201cWhich nodes are likely to have an edge with node } u \\\\text{ at time } t?\\\\]\\n\\nIn this case, the model returns an ordered list of nodes arranged from most likely to least likely. Then, the rank of the ground truth edge is returned if a match is found, and if not, a high number is reported. For all the edges in the test data, metrics such as Mean Average Rank (M\u00c1R) or Mean Reciprocal Rank (MRR) can be reported to assess the performance of the model (Huang et al. 2024). While the rank-based metrics are more intuitive than AU-ROC and AP, the issues regarding binary classification mentioned above still remain unaddressed. To give a true picture of the predictive power of the TLP models, a penalty term should be introduced to account for the nodes that are incorrectly estimated to form an edge with node \\\\( u \\\\) at time \\\\( t \\\\).\\n\\nIn a recent work, Poursafaei et al. (2022) highlighted that the state-of-the-art (SoTA) performance of some TLP models on the standard benchmark datasets is near-perfect. This is counterintuitive because TLP is a challenging task, even more challenging than link prediction of static graphs, due to the additional degree of freedom in the data induced by the temporal dimension. The flaw in the evaluation method is attributed to the limited negative sampling strategy, and the authors propose a new negative edge sampling strategy which results in a different ranking of the baselines.\\n\\nInspired by the critique of the evaluation method, we propose a method to conduct sanity check of the TLP models to determine if they truly capture the temporal patterns in the data. The sanity check is formulated as the counterfactual question (Pearl 2019):\\n\\n\u201cWhat if a TLP model which is trained on a temporal graph is tested on temporally distorted version of the data instead of the real data?\u201d\\n\\nIdeally, a TLP model which is capable of learning the temporal patterns should perform worse on temporally distorted data compared to the real data. We conduct an in-depth analysis of this argument and introduce various data distortion techniques to assess well-known TLP models.\\n\\n**Contributions** The contributions of our work can be summarised as follows:\\n\\n- We introduce simple **techniques** to distort the temporal patterns within a graph. These techniques are then used to generate temporally distorted version of the test split of some famous datasets which can be used for **sanity check**. Moreover, the distortion methods can be applied to any temporal graph dataset.\\n\\n- We perform **counterfactual analysis** on TLP models such as JODIE (Kumar et al. 2019), TGAT (Xu et al. 2020), TGN (Rossi et al. 2020), and CAWN (Wang et al. 2020) to check whether they are capable of capturing the temporal patterns within various datasets.\\n\\n- We propose an alternative **evaluation strategy** for TLP through which the existing pitfalls of binary classification and ranking methods can be avoided. We also propose two **metrics**: average time difference (ATD), and average count difference (ACD) to measure the performance of TLP models. These metrics can provide a holistic picture of a model\u2019s predictive performance.\\n\\n**Organization** In Sec. 2 we define temporal graphs and the associated notations. We also provide a brief overview of interpreting temporal graphs as point processes, which forms the theoretical foundation of TLP. In Sec. 3 we formalize the counterfactual analysis through logical arguments, and also propose data distortion techniques. The results of the counterfactual analysis are presented in Sec. 4 along with the details of the datasets and TLP models used for evaluation. In Sec. 5 we suggest a generative evaluation approach for TLP, and discuss the broader impact and limitations of our work.\"}"]}
{"id": "L5aY1mWvXQ", "page_num": 3, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2 Preliminaries\\n\\n2.1 Definitions\\n\\nIn TLP literature, continuous-time temporal graphs with ephemeral edges are often considered, where edges represent interaction events between two nodes at a specific point in time. Alternatively, temporal graphs can be defined with edges that appear at a certain time and either persist for a duration (Celikkanat et al., 2024; Farzaneh and Coon, 2023) or accumulate indefinitely. In this work, we focus on the ephemeral edge temporal graph, also known as interaction graphs (Qin et al., 2024) or unevenly sampled edge sequence (Qin and Yeung, 2023).\\n\\n**Definition 2.1.** A temporal graph with \\\\( m \\\\in \\\\mathbb{N} \\\\) ephemeral edges formed between nodes in \\\\( \\\\mathcal{U} \\\\) and \\\\( \\\\mathcal{V} \\\\) is defined as \\\\( \\\\mathcal{G} = (\\\\mathcal{U}, \\\\mathcal{V}, \\\\mathcal{E}) \\\\), where \\\\( \\\\mathcal{E} \\\\triangleq \\\\{(u_i, v_i, t_i) : i \\\\in [m], u_i \\\\in \\\\mathcal{U}, v_i \\\\in \\\\mathcal{V}, t_i \\\\in \\\\mathbb{R}\\\\} \\\\) denotes the set of edges. The tuple \\\\((u, v, t)\\\\) is referred to as an edge event.\\n\\nWhile the definition caters to bipartite structure, with \\\\( \\\\mathcal{U} = \\\\mathcal{V} \\\\), it can also represent general graphs.\\n\\n**Definition 2.2.** The occurrences of a particular edge \\\\((u, v)\\\\) in \\\\( \\\\mathcal{E} \\\\) is denoted as \\\\( \\\\mathcal{E}_{(u,v)} \\\\) and defined as \\\\( \\\\mathcal{E}_{(u,v)} \\\\triangleq \\\\{(u, v, t) : (u, v, t) \\\\in \\\\mathcal{E}\\\\} \\\\).\\n\\n**Definition 2.3.** The slice of edges in \\\\( \\\\mathcal{E} \\\\) with timestamps in the range \\\\((t_1, t_2)\\\\) is denoted as \\\\( \\\\mathcal{E}(t_1, t_2) \\\\) and defined as \\\\( \\\\mathcal{E}(t_1, t_2) \\\\triangleq \\\\{(u, v, t) : (u, v, t) \\\\in \\\\mathcal{E}, t \\\\in (t_1, t_2)\\\\} \\\\).\\n\\n**Definition 2.4.** The timestamps in \\\\( \\\\mathcal{E} \\\\) consisting of \\\\( m \\\\in \\\\mathbb{N} \\\\) edges can be extracted through a function \\\\( \\\\mathcal{T} : (\\\\mathcal{U} \\\\times \\\\mathcal{V} \\\\times \\\\mathbb{R})^m \\\\rightarrow \\\\mathbb{R}^m \\\\) as \\\\( \\\\mathcal{T}(\\\\mathcal{E}) \\\\triangleq \\\\{t : (u, v, t) \\\\in \\\\mathcal{E}\\\\} \\\\).\\n\\n2.2 Point Process\\n\\nPerry and Wolfe (2013) modelled the interaction events of a directed edge \\\\((u, v)\\\\) as an inhomogenous Poisson point process. In a recent work on continuous-time representation learning on temporal graphs, Modell et al. (2024) followed suit, and assumed \\\\( \\\\mathcal{E}_{(u,v)} \\\\) to be sampled from an independent inhomogenous Poisson point process with intensity \\\\( \\\\lambda_{(u,v)}(t) \\\\). The number of edge events \\\\((u, v)\\\\) between timestamps \\\\( t_1 \\\\) and \\\\( t_2 \\\\) follow a Poisson distribution with rate \\\\( \\\\int_{t_1}^{t_2} \\\\lambda_{(u,v)}(t) \\\\, dt \\\\), i.e.,\\n\\n\\\\[\\n|\\\\mathcal{E}_{(u,v)}(t_1, t_2)| \\\\sim \\\\text{Poisson} \\\\left( \\\\int_{t_1}^{t_2} \\\\lambda_{(u,v)}(t) \\\\, dt \\\\right). \\\\tag{1}\\n\\\\]\\n\\nTo connect the present to the past, Du et al. (2016) view the intensity function \\\\( \\\\lambda^*_{(u,v)}(t) \\\\) as a nonlinear function of the sample history, where \\\\( \\\\ast \\\\) indicates that the function is conditioned on the history. The conditional density function for edge \\\\((u, v)\\\\) is written as\\n\\n\\\\[\\np^*_{(u,v)}(t) = \\\\lambda^*_{(u,v)}(t) \\\\exp \\\\left( - \\\\int_{t'}^{t} \\\\lambda^*_{(u,v)}(\\\\tau) \\\\, d\\\\tau \\\\right), \\\\tag{2}\\n\\\\]\\n\\nwhere \\\\( t' < t \\\\) is the last time when edge \\\\((u, v)\\\\) was observed. The goal is to find the parameters \\\\( \\\\lambda^*_{(u,v)}(t) : 0 < t \\\\leq T \\\\) which can describe the observation \\\\( \\\\mathcal{E}_{(u,v)} \\\\). This is done by minimizing the negative log likelihood (NLL) at the timestamps of edge occurrence (Shchur et al., 2021):\\n\\n\\\\[\\n\\\\min_{\\\\lambda^*_{(u,v)}(t) : 0 < t \\\\leq T} \\\\sum_{t \\\\in \\\\mathcal{T}(\\\\mathcal{E}_{(u,v)})} \\\\log \\\\left( \\\\lambda^*_{(u,v)}(t) \\\\right) + \\\\int_{0}^{T} \\\\lambda^*_{(u,v)}(\\\\tau) \\\\, d\\\\tau, \\\\quad T = \\\\max \\\\mathcal{T}(\\\\mathcal{E}_{(u,v)}). \\\\tag{3}\\n\\\\]\\n\\nIn (Shchur et al., 2021), the operation of a neural temporal point process is summarized as:\\n\\n- The edge events in \\\\( \\\\{(u, v, t_i) : i \\\\in [m]\\\\} \\\\) are represented as feature vectors \\\\( x_i = f_e(u, v, t_i) \\\\),\\n- The historical feature vectors are encoded into a state vector \\\\( h_i = f_h(x_1, \\\\cdots, x_{i-1}) \\\\),\\n- The distribution of \\\\( t_i \\\\) conditioned on the past is simply conditioned on \\\\( h_i \\\\).\\n\\nThe functions \\\\( f_e \\\\) and \\\\( f_h \\\\), as well as the conditioning on \\\\( h_i \\\\), can be implemented using neural networks.\\n\\n**Conjecture 2.1.** The samples from a neural temporal point process are learnable, i.e., a model exists which can perform temporal link predictions based on the past observations.\"}"]}
{"id": "L5aY1mWvXQ", "page_num": 4, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3 Counterfactual Analysis\\n\\nExperiment Setup  A model $f$ is trained on a temporal graph $\\\\mathcal{E}_{\\\\text{train}}$ and tested on $\\\\mathcal{E}_{\\\\text{test}}$ through the binary classification approach resulting in metrics such as AU-ROC, and AP. In general, $\\\\mathcal{E}_{\\\\text{train}} = \\\\mathcal{E}(0, \\\\tau_0)$, and $\\\\mathcal{E}_{\\\\text{test}} = \\\\mathcal{E}(\\\\tau_0, T)$, i.e., the train and test data are chronologically split from the same temporal graph which is assumed to be generated through a common causal mechanism.\\n\\nIn light of the experimental setup, we ask the question: \u201cWould the model $f$ which is trained on $\\\\mathcal{E}_{\\\\text{train}}$ perform well if tested on a distorted version of $\\\\mathcal{E}_{\\\\text{test}}$ instead of $\\\\mathcal{E}_{\\\\text{test}}$?\u201d To formalise the question in the counterfactual framework proposed by [Pearl (2019)], we consider the following statements:\\n\\n$x'$: The test data is $\\\\mathcal{E}_{\\\\text{test}}$.\\n\\n$x$: The test data is a temporally distorted version of $\\\\mathcal{E}_{\\\\text{test}}$.\\n\\n$y'$: The performance metric is in the range $(\\\\alpha - \\\\epsilon, \\\\min\\\\{1, \\\\alpha + \\\\epsilon\\\\})$.\\n\\n$y$: The performance metric is strictly less than $\\\\alpha - \\\\epsilon$.\\n\\nThen, the counterfactual question can be framed as $P(y_x \\\\mid x', y')$ which stands for:\\n\\nThe probability that the prediction accuracy would be less than $\\\\alpha - \\\\epsilon$ had the test data been a temporally distorted version of $\\\\mathcal{E}_{\\\\text{test}}$, given the prediction accuracy was observed to be approximately $\\\\alpha$ when the model was tested on $\\\\mathcal{E}_{\\\\text{test}}$.\\n\\nWe link the counterfactual question to our hypothesis in the following proposition:\\n\\n**Proposition 3.1.** If $P(y_x \\\\mid x', y') \\\\approx 0 \\\\implies$ model $f$ cannot learn the temporal patterns in $\\\\mathcal{E}_{\\\\text{train}}$.\\n\\n**Proof.** Consider the set of logical statements:\\n\\n$s_1$: The temporal graph $\\\\mathcal{E}$ contains patterns that allow future edge predictions to be made based on past information, i.e., $\\\\mathcal{G}$ is learnable.\\n\\n$s_2$: The model $f$ is capable of learning the patterns in a learnable temporal graph.\\n\\n$s_3$: $\\\\mathcal{E}_{\\\\text{train}} = \\\\mathcal{E}(0, \\\\tau_0), \\\\mathcal{E}_{\\\\text{test}} = \\\\mathcal{E}(\\\\tau_0, T)$.\\n\\n$s_4$: $\\\\mathcal{G}' = \\\\mathcal{D}(\\\\mathcal{E}_{\\\\text{test}})$, where $\\\\mathcal{D}(\\\\cdot)$ is the temporal distortion function.\\n\\n$s_5$: The model $f$ is trained on $\\\\mathcal{E}_{\\\\text{train}}$.\\n\\n$s_6$: The prediction metric reported by $f$ on the real test data $\\\\mathcal{E}_{\\\\text{test}}$ is higher than the prediction metric on the distorted data $\\\\mathcal{G}'$.\\n\\n$$s_1 \\\\land s_2 \\\\land s_3 \\\\land s_4 \\\\land s_5 \\\\implies s_6$$\\n\\n$$\\\\neg s_6 \\\\implies \\\\neg s_1 \\\\lor \\\\neg s_2 \\\\lor \\\\neg s_3 \\\\lor \\\\neg s_4 \\\\lor \\\\neg s_5$$\\n\\n(contraposition)\\n\\nFor the experimental setup $s_3 = 1$, and $s_5 = 1$. Assuming that the temporal graph $\\\\mathcal{G}$ is learnable $s_1 = 1$, and that the function $\\\\mathcal{D}(\\\\mathcal{E}_{\\\\text{test}})$ results in a temporally distorted version of $\\\\mathcal{E}_{\\\\text{test}}$, i.e., $s_4 = 1$, we get $\\\\neg s_6 \\\\implies \\\\neg s_2$. Alternatively, $\\\\neg s_6 \\\\equiv \\\\mathbb{I}(P(y_x \\\\mid x', y') \\\\approx 0)$, and $\\\\neg s_2$ is interpreted as \u201cmodel $f$ is incapable of learning the temporal patterns in $\\\\mathcal{G}$\u201d.\\n\\n**Example** In Fig. 1, we show that $\\\\mathcal{E}_{\\\\text{train}} \\\\cup \\\\mathcal{E}_{\\\\text{test}}$ is sampled from a point process with intensity $\\\\lambda^*(t), t \\\\in [0, T]$. We generate $\\\\mathcal{E}'$ from another point process with intensity $\\\\lambda'(t), t \\\\in [\\\\tau_0, T]$. We depict the intensity functions as two sinusoidal waves with different frequency and phase. If a model $f$ learns this intensity function by observing $\\\\mathcal{E}_{\\\\text{train}}$, and then generates samples for prediction, they would be more similar to $\\\\mathcal{E}_{\\\\text{test}}$ than $\\\\mathcal{E}'$.\\n\\n![Figure 1: Example of Temporal distortion.](image-url)\"}"]}
{"id": "L5aY1mWvXQ", "page_num": 5, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.1 Temporal Distortion Techniques\\n\\nLet $\\\\mathcal{E}$ be a temporal graph sampled from a temporal point process with intensity $\\\\lambda^*(t)$ for $t \\\\in [0, T]$. Let $\\\\mathcal{E}'$ be data sampled from another point process with intensity $\\\\lambda'(t)$ for $t \\\\in [0, T]$.\\n\\n**Definition 3.1.** The temporal graph $\\\\mathcal{E}'$ is $\\\\delta$-temporally distorted w.r.t. $\\\\mathcal{E}$ if for some $\\\\delta > 0$,\\n\\n$$\\\\frac{1}{T} \\\\int_0^T |\\\\lambda^*(t) - \\\\lambda'(t)| \\\\, dt > \\\\delta.$$  \\\\hfill (5)\\n\\nIn practice, we do not have access to the true intensity functions, and have to compare the realisations instead. Let $\\\\mathcal{E}$ and $\\\\mathcal{E}'$ be two temporal graphs, then we measure the difference in their characteristics through the following two metrics.\\n\\n**Definition 3.2.** The average time difference (ATD) between $\\\\mathcal{E}$ and $\\\\mathcal{E}'$ is defined as:\\n\\n$$\\\\text{ATD}(\\\\mathcal{E}, \\\\mathcal{E}') \\\\triangleq \\\\frac{1}{|\\\\mathcal{E}|} \\\\sum_{(u,v,t) \\\\in \\\\mathcal{E}} \\\\min_{t' \\\\in \\\\mathcal{T}(\\\\mathcal{E}'(u,v)) \\\\cup \\\\{T\\\\}} |t - t'|,$$  \\\\hfill (6)\\n\\nwhere $T = \\\\max \\\\mathcal{T}(\\\\mathcal{E}) - \\\\min \\\\mathcal{T}(\\\\mathcal{E})$.\\n\\n**Definition 3.3.** The average count difference (ACD) between $\\\\mathcal{E}$ and $\\\\mathcal{E}'$ is defined as:\\n\\n$$\\\\text{ACD}(\\\\mathcal{E}, \\\\mathcal{E}') \\\\triangleq \\\\frac{1}{|\\\\mathcal{E}|} \\\\sum_{(u,v,t) \\\\in \\\\mathcal{E}} \\\\left| |\\\\mathcal{E}(u,v)(t - \\\\bar{\\\\tau}, t + \\\\bar{\\\\tau})| - |\\\\mathcal{E}'(u,v)(t - \\\\bar{\\\\tau}, t + \\\\bar{\\\\tau})| \\\\right|,$$  \\\\hfill (7)\\n\\nwhere $\\\\bar{\\\\tau} = \\\\frac{\\\\max \\\\mathcal{T}(\\\\mathcal{E}) - \\\\min \\\\mathcal{T}(\\\\mathcal{E})}{|\\\\mathcal{E}|}$.\\n\\nNow that we are equipped with metrics to measure the difference between two temporal graphs, we devise distortion functions $\\\\mathcal{D}(\\\\cdot)$ which can enable us to investigate the counterfactual question posed earlier. We propose two distortion techniques $\\\\mathcal{D}_{\\\\text{INTENSE}}(\\\\cdot, K)$ which creates $K$ time-perturbed copies of each edge events, and $\\\\mathcal{D}_{\\\\text{SHUFFLE}}(\\\\cdot)$ wherein the timestamps of different edge events are shuffled.\\n\\n**Algorithm 1 $\\\\mathcal{D}_{\\\\text{INTENSE}}$**\\n\\n**Input** $\\\\mathcal{E}, K \\\\in \\\\mathbb{N}$\\n\\n**Output** $\\\\mathcal{E}'$\\n\\n1: $\\\\mathcal{E}' = \\\\emptyset$\\n2: $\\\\tau_0 \\\\leftarrow \\\\min \\\\mathcal{T}(\\\\mathcal{E})$\\n3: $T \\\\leftarrow \\\\max \\\\mathcal{T}(\\\\mathcal{E})$\\n4: $\\\\bar{\\\\tau} \\\\leftarrow \\\\frac{T - \\\\tau_0}{|\\\\mathcal{E}|}$\\n5: for $(u, v, t) \\\\in \\\\mathcal{E}$ do\\n6:     for $k \\\\in [K]$ do\\n7:         $\\\\tau \\\\sim \\\\text{Uniform}(-\\\\bar{\\\\tau}, \\\\bar{\\\\tau})$\\n8:         $\\\\mathcal{E}' \\\\leftarrow \\\\mathcal{E}' \\\\cup \\\\{(u, v, t + \\\\tau)\\\\}$\\n9:     end for\\n10: end for\\n\\n**Algorithm 2 $\\\\mathcal{D}_{\\\\text{SHUFFLE}}$**\\n\\n**Input** $\\\\mathcal{E}$\\n\\n**Output** $\\\\mathcal{E}'$\\n\\n1: $\\\\mathcal{E}' = \\\\emptyset$\\n2: $\\\\mathcal{T} \\\\leftarrow \\\\mathcal{T}(\\\\mathcal{E})$\\n3: for $(u, v, t) \\\\in \\\\mathcal{E}$ do\\n4:     $\\\\tau \\\\sim \\\\mathcal{T}$\\n5:     $\\\\mathcal{E}' \\\\leftarrow \\\\mathcal{E}' \\\\cup \\\\{(u, v, \\\\tau)\\\\}$\\n6:     $\\\\mathcal{T} \\\\leftarrow \\\\mathcal{T} \\\\setminus \\\\{\\\\tau\\\\}$\\n7: end for\\n\\n**INTENSE** Let the real temporal graph data be denoted by $\\\\mathcal{E} = \\\\bigcup_{(u,v) \\\\in \\\\mathcal{U} \\\\times \\\\mathcal{V}} \\\\mathcal{E}(u,v)$, and the distorted version be denoted by $\\\\mathcal{E}' = \\\\bigcup_{(u,v) \\\\in \\\\mathcal{U} \\\\times \\\\mathcal{V}} \\\\mathcal{E}'(u,v)$. then, for each edge event $(u, v, t)$ in the real data $\\\\mathcal{E}$, we create $K$ edge events $(u, v, t + \\\\tau)$ with $\\\\tau$ sampled uniformly from $(-\\\\bar{\\\\tau}, \\\\bar{\\\\tau})$ for some $\\\\bar{\\\\tau} > 0$. Alternatively, if it is known that $\\\\mathcal{E}(u,v)$ is sampled from a point process with intensity $\\\\lambda^*_*(u,v)(t)$, then we can generate $\\\\mathcal{E}'(u,v)$ by sampling from another point process with intensity $\\\\lambda'_*(u,v)(t)$, such that\\n\\n$$\\\\lambda'_*(u,v)(t) = K \\\\lambda^*_*(u,v)(t), \\\\forall (u,v) \\\\in \\\\mathcal{U} \\\\times \\\\mathcal{V}.$$  \\n\\nThe operation of $\\\\mathcal{D}_{\\\\text{INTENSE}}$ is described in Algorithm 1.\\n\\n**SHUFFLE** For any two edge events $(u, v, t), (u', v', t') \\\\in \\\\mathcal{E}$, we shuffle the timestamps in the distorted version, i.e. $(u, v, t'), (u', v', t) \\\\in \\\\mathcal{E}'$. The shuffling process is also called label permutation (Chatterjee, 2018). In terms of the point process, we can explain shuffling as follows. If $\\\\mathcal{E}(u,v)$ is known to be sampled from a point process with intensity $\\\\lambda^*_*(u,v)(t)$, then $\\\\mathcal{E}'(u,v)$ can be generated by sampling from an inhomogenous Poisson point process with intensity $\\\\lambda'_*(u,v)(t)$, where\\n\\n$$\\\\lambda'_*(u,v)(t) = \\\\frac{\\\\int_0^T \\\\lambda^*_*(u,v)(t) \\\\, dt}{\\\\int_0^T \\\\lambda^*(t) \\\\, dt} \\\\lambda^*(t), \\\\forall (u,v) \\\\in \\\\mathcal{U} \\\\times \\\\mathcal{V}.$$  \\n\\nWe describe the operation of $\\\\mathcal{D}_{\\\\text{SHUFFLE}}$ in Algorithm 2.\"}"]}
{"id": "L5aY1mWvXQ", "page_num": 6, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4 Experiment\\n\\nDatasets We use the following datasets\\\\footnote{The datasets can be downloaded from https://zenodo.org/records/7213796} to perform counterfactual analysis:\\n\\n- wikipedia (Kumar et al. 2019) describes a dynamic graph of interaction between the editors and Wikipedia pages over a span of one month. The entries consist of the user ID, page ID, and timestamp. The edge features are LIWC-feature vectors (Pennebaker et al. 2001) of the edit text. The edge feature dimension is 172.\\n\\n- reddit (Kumar et al. 2019) describes a bipartite interaction graph between the users and subreddits. The interaction event is recorded with the IDs of the user, subreddit and timestamp. Similar to wikipedia, the post content is converted into a LIWC-feature vector of dimension 172 which serves as the edge feature.\\n\\n- uci (Panzarasa et al. 2009) is a dynamic graph describing message-exchange among the students at University of California at Irvine (UCI) from April to October 2004. The interaction event consists of the user IDs, and timestamp.\\n\\nThe scale of the datasets are presented in Table\\\\ref{tab:datasets}. The datasets are chronologically split in the ratio 0.7 : 0.15 : 0.15 into train, validation, and test sets, respectively.\\n\\nNext, we use $\\\\mathcal{D}_{\\\\text{INTENSE}}(\\\\cdot, 5)$ and $\\\\mathcal{D}_{\\\\text{SHUFFLE}}(\\\\cdot)$ to create 10 temporally distorted samples of the test splits of each dataset. In Table\\\\ref{tab:distortion}, we present the ATD, and ACD by comparing the distorted samples with the original test data of different datasets. Through $\\\\mathcal{D}_{\\\\text{INTENSE}}(\\\\cdot, 5)$, the ATD is negligible, however, the ACD is close to 5. Through $\\\\mathcal{D}_{\\\\text{SHUFFLE}}(\\\\cdot)$, the ACD is approximately 1 for wikipedia and reddit, and close to 2 for uci. We also see an increase in ATD which is close to 0.1 for all datasets. Therefore, the metrics ATD and ACD should be considered in conjunction to measure the dissimilarity of two temporal graphs.\\n\\n| Dataset | $|U \\\\cup V|$ | $|E|$ |\\n|---------|-------------|-------------|\\n| wikipedia | 9227 | 157474 |\\n| reddit | 10984 | 672447 |\\n| uci | 1899 | 59835 |\\n\\nTable 1: Number of nodes and edges in temporal graph datasets.\\n\\n| Dataset | ATD | ACD |\\n|---------|-----|-----|\\n| wikipedia | 6.9e-6 \u00b1 2e-8 | 4.479 \u00b1 1.9e-3 |\\n| reddit | 1.6e-6 \u00b1 2e-9 | 4.112 \u00b1 3.9e-4 |\\n| uci | 1.6e-5 \u00b1 1.2e-7 | 7.214 \u00b1 1.2e-2 |\\n\\nTable 2: Distortion measures on different datasets.\\n\\nModels We evaluate\\\\footnote{GPU: NVIDIA GeForce RTX\u2122 3060. CPU: 12th Gen Intel\u00ae Core\u2122 i7-12700 \u00d7 20; 16.0 GiB.} the performance of the following TLP models\\\\footnote{The optimal hyper-parameters reported by the models are used.} in light of Proposition\\\\ref{prop:3.1}:\\n\\n- JODIE (Kumar et al. 2019) uses a recurrent neural network (RNN) to generate node embeddings for each interaction event. The future embedding of a node is estimated through a novel projection operator which is turn in used to predict future edge events.\\n\\n- TGAT (Xu et al. 2020) relies on self-attention mechanism to generate node embeddings to capture the temporal evolution of the graph structure.\\n\\n- TGN (Rossi et al. 2020) combine memory modules with graph-based operators to create an encoder-decoder pair capable of creating temporal node embeddings.\\n\\n- CAWN (Wang et al. 2020) propose a novel strategy based on the law of triadic closure, where temporal walks retrieve the dynamic graph motifs without explicitly counting and selecting the motifs. The node IDs are replaced with the hitting counts to facilitate inductive inference.\\n\\nFor all the models we have forked the main branch of their original Github repositories, and added additional arguments to account for the distortion technique, as well as more focused logging. We wanted to evaluate GraphMixer (Cong et al. 2023) as it claims superior performance, however the distorted datasets we generated were not compatible with the dataloader used in their codebase.\"}"]}
{"id": "L5aY1mWvXQ", "page_num": 7, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Results  The models are evaluated under two settings: transductive, and inductive. In transductive TLP, the nodes \\\\( u, v \\\\) in the positive sample \\\\((u, v, t) \\\\in \\\\mathcal{E}_{\\\\text{test}}\\\\) were observed during training. In contrast, in inductive TLP, at least one node in \\\\( u, v \\\\) is novel, and was not observed during training.\\n\\nTable 3: Performance of the models JODIE, TGAT, TGN, and CAWN on three datasets, and their temporally distorted versions denoted as INTENSE, and SHUFFLE. For each metric, we report the mean, and the 95% confidence interval (CI) as mean \u00b1 CI. We have marked the metrics in blue for distortions that showed that a model was incapable of learning on a certain dataset as per Proposition 3.1, and orange otherwise, with \\\\( \\\\epsilon = 0.05 \\\\).\\n\\n| JODIE | wikipedia | | reddit | | uci |\\n|-------|-----------|-----------|-----------|-----------|\\n|       | AU-ROC    | AP        | AU-ROC    | AP        | AU-ROC    | AP        |\\n| transductive | 0.9170 \u00b1 3e-3 | 0.9137 \u00b1 5e-3 | 0.9679 \u00b1 4e-3 | 0.9654 \u00b1 5e-3 | 0.8950 \u00b1 3e-3 | 0.8726 \u00b1 5e-3 |\\n| INTENSE | 0.9177 \u00b1 7e-3 | 0.9078 \u00b1 1e-2 | 0.9619 \u00b1 9e-3 | 0.9567 \u00b1 1e-2 | 0.9244 \u00b1 2e-3 | 0.9129 \u00b1 5e-3 |\\n| SHUFFLE | 0.9097 \u00b1 2e-2 | 0.8962 \u00b1 4e-2 | 0.9661 \u00b1 1e-2 | 0.9613 \u00b1 4e-2 | 0.8852 \u00b1 3e-3 | 0.8509 \u00b1 3e-3 |\\n| inductive | 0.8941 \u00b1 4e-3 | 0.8970 \u00b1 5e-3 | 0.9343 \u00b1 9e-3 | 0.9138 \u00b1 2e-2 | 0.7546 \u00b1 8e-3 | 0.7310 \u00b1 2e-2 |\\n| INTENSE | 0.9036 \u00b1 1e-2 | 0.8972 \u00b1 1e-2 | 0.9457 \u00b1 3e-2 | 0.9308 \u00b1 4e-2 | 0.8384 \u00b1 3e-3 | 0.8332 \u00b1 8e-3 |\\n| SHUFFLE | 0.9157 \u00b1 1e-2 | 0.9078 \u00b1 2e-2 | 0.9419 \u00b1 3e-2 | 0.9251 \u00b1 6e-3 | 0.7368 \u00b1 5e-3 | 0.6994 \u00b1 8e-3 |\\n\\n| TGAT | wikipedia | | reddit | | uci |\\n|-------|-----------|-----------|-----------|-----------|\\n|       | AU-ROC    | AP        | AU-ROC    | AP        | AU-ROC    | AP        |\\n| transductive | 0.9499 \u00b1 2e-3 | 0.9528 \u00b1 2e-3 | 0.9806 \u00b1 6e-4 | 0.9818 \u00b1 6e-4 | 0.7885 \u00b1 1e-2 | 0.7694 \u00b1 7e-3 |\\n| INTENSE | 0.9680 \u00b1 2e-3 | 0.9691 \u00b1 2e-3 | 0.9821 \u00b1 6e-4 | 0.9825 \u00b1 6e-4 | 0.8707 \u00b1 1e-2 | 0.8637 \u00b1 2e-2 |\\n| SHUFFLE | 0.9492 \u00b1 5e-3 | 0.9532 \u00b1 5e-3 | 0.9814 \u00b1 7e-3 | 0.9826 \u00b1 6e-3 | 0.7719 \u00b1 1e-2 | 0.7336 \u00b1 2e-2 |\\n| inductive | 0.9353 \u00b1 2e-3 | 0.9401 \u00b1 2e-3 | 0.9641 \u00b1 1e-3 | 0.9658 \u00b1 1e-3 | 0.7020 \u00b1 8e-3 | 0.7008 \u00b1 1e-2 |\\n| INTENSE | 0.9604 \u00b1 2e-3 | 0.9621 \u00b1 2e-3 | 0.9676 \u00b1 8e-4 | 0.9676 \u00b1 1e-3 | 0.8019 \u00b1 2e-2 | 0.8095 \u00b1 2e-2 |\\n| SHUFFLE | 0.9257 \u00b1 7e-3 | 0.9304 \u00b1 7e-3 | 0.9644 \u00b1 7e-3 | 0.9664 \u00b1 3e-3 | 0.6558 \u00b1 7e-3 | 0.6324 \u00b1 1e-2 |\\n\\n| TGN | wikipedia | | reddit | | uci |\\n|-------|-----------|-----------|-----------|-----------|\\n|       | AU-ROC    | AP        | AU-ROC    | AP        | AU-ROC    | AP        |\\n| transductive | 0.9370 \u00b1 1e-3 | 0.9472 \u00b1 1e-3 | 0.9545 \u00b1 1e-3 | 0.9578 \u00b1 1e-3 | 0.7826 \u00b1 1e-2 | 0.7975 \u00b1 1e-2 |\\n| INTENSE | 0.9898 \u00b1 1e-3 | 0.9911 \u00b1 6e-4 | 0.9723 \u00b1 2e-3 | 0.9744 \u00b1 2e-3 | 0.9653 \u00b1 3e-3 | 0.9709 \u00b1 3e-3 |\\n| SHUFFLE | 0.8310 \u00b1 3e-2 | 0.8487 \u00b1 3e-2 | 0.9533 \u00b1 2e-3 | 0.9563 \u00b1 2e-3 | 0.6722 \u00b1 6e-2 | 0.6520 \u00b1 4e-2 |\\n| inductive | 0.9374 \u00b1 1e-3 | 0.9463 \u00b1 1e-3 | 0.9299 \u00b1 1e-3 | 0.9346 \u00b1 1e-3 | 0.7714 \u00b1 6e-3 | 0.7948 \u00b1 6e-3 |\\n| INTENSE | 0.9903 \u00b1 1e-3 | 0.9908 \u00b1 6e-4 | 0.9617 \u00b1 3e-3 | 0.9645 \u00b1 3e-3 | 0.9592 \u00b1 3e-3 | 0.9650 \u00b1 2e-3 |\\n| SHUFFLE | 0.8194 \u00b1 2e-2 | 0.8376 \u00b1 3e-2 | 0.9266 \u00b1 4e-3 | 0.9299 \u00b1 3e-3 | 0.6245 \u00b1 2e-2 | 0.6193 \u00b1 9e-3 |\\n\\n| CAWN | wikipedia | | reddit | | uci |\\n|-------|-----------|-----------|-----------|-----------|\\n|       | AU-ROC    | AP        | AU-ROC    | AP        | AU-ROC    | AP        |\\n| transductive | 0.9886 \u00b1 1e-4 | 0.9901 \u00b1 1e-4 | 0.9864 \u00b1 4e-3 | 0.9884 \u00b1 3e-3 | 0.9162 \u00b1 9e-4 | 0.9397 \u00b1 8e-4 |\\n| INTENSE | 0.9977 \u00b1 9e-5 | 0.9975 \u00b1 8e-5 | 0.9931 \u00b1 8e-5 | 0.9942 \u00b1 7e-5 | 0.9848 \u00b1 6e-4 | 0.9889 \u00b1 7e-4 |\\n| SHUFFLE | 0.9868 \u00b1 3e-4 | 0.9887 \u00b1 3e-4 | 0.9859 \u00b1 6e-4 | 0.9880 \u00b1 2e-3 | 0.8495 \u00b1 7e-3 | 0.8866 \u00b1 2e-3 |\\n| inductive | 0.9877 \u00b1 5e-4 | 0.9896 \u00b1 4e-4 | 0.9833 \u00b1 5e-3 | 0.9859 \u00b1 3e-3 | 0.9052 \u00b1 1e-2 | 0.9273 \u00b1 2e-3 |\\n| INTENSE | 0.9972 \u00b1 6e-4 | 0.9971 \u00b1 1e-5 | 0.9929 \u00b1 8e-5 | 0.9938 \u00b1 8e-5 | 0.9810 \u00b1 3e-3 | 0.9857 \u00b1 2e-3 |\\n| SHUFFLE | 0.9876 \u00b1 1e-2 | 0.9896 \u00b1 6e-3 | 0.9826 \u00b1 8e-4 | 0.9851 \u00b1 1e-3 | 0.8383 \u00b1 3e-2 | 0.8783 \u00b1 3e-2 |\\n\\nFrom Table 3 it is evident that none of the models are capable of distinguishing between the real data, and data sampled from a five-times more intense version. However, we see that TGN is fairly robust when the timestamps of the test data are shuffled, as its performance worsens the most compared to other models. The performance gap between the real and distorted versions decrease as the dataset size increases (see Table. 1).\\n\\nIn Fig. 2 and Fig. 3 we present the metric gap \\\\( \\\\mathbb{E}[y_x - y'] \\\\) for \\\\( x \\\\sim \\\\mathcal{D}_{\\\\text{INTENSE}}(x', 5) \\\\), and \\\\( x \\\\sim \\\\mathcal{D}_{\\\\text{SHUFFLE}}(x') \\\\), respectively, for different models in categorical bar plots grouped by the dataset. We\"}"]}
{"id": "L5aY1mWvXQ", "page_num": 8, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"check whether \\\\( \\\\max y_x < \\\\min y' \\\\) in an empirical way by checking if \\\\( \\\\mathbb{E}[y_x] + \\\\text{CI} < \\\\mathbb{E}[y'] - \\\\text{CI}' \\\\implies \\\\mathbb{E}[y_x] - \\\\mathbb{E}[y'] < -(\\\\text{CI} + \\\\text{CI}') \\\\). Therefore, we plot \\\\( \\\\mathbb{E}[y_x] - \\\\mathbb{E}[y'] \\\\) as coloured bars, and \\\\(- (\\\\text{CI} + \\\\text{CI}')\\\\) as black diamonds. Moreover, we indicate \\\\( \\\\epsilon = 0.05 \\\\) as the dashed black line passing through \\\\(-0.05\\\\).\\n\\n\\\\[\\n\\\\mathbb{E}_{x \\\\sim \\\\mathcal{D}_{\\\\text{INTENSE}}(x',5)}[y_x - y']\\n\\\\]\\n\\nFigure 2: \\\\( \\\\mathbb{E}_{x \\\\sim \\\\mathcal{D}_{\\\\text{INTENSE}}(x',5)}[y_x - y'] \\\\).\\n\\n\\\\[\\n\\\\mathbb{E}_{x \\\\sim \\\\mathcal{D}_{\\\\text{SHUFFLE}}(x')}[y_x - y']\\n\\\\]\\n\\nFigure 3: \\\\( \\\\mathbb{E}_{x \\\\sim \\\\mathcal{D}_{\\\\text{SHUFFLE}}(x')}[y_x - y'] \\\\).\\n\\nThe models evaluated in this work form the set of baselines to validate the performance of new models. However, as we demonstrate, a higher metric alone is not indicative of good performance without sanity checks. The counterfactual question helps make the evaluation more explainable, as models that perform worse on temporally distorted data with high ATD and ACD can claim superiority over modes that do not. An ideal TLP model should be able to capture the difference in the count of edge events, as well as temporal shifts in the edge events.\"}"]}
{"id": "L5aY1mWvXQ", "page_num": 9, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5 Discussion\\n\\nMoving away from the binary classification approach to assess the performance of temporal link prediction, the research should explore a generative approach where after observing a temporal graph from time $t \\\\in (0, \\\\tau_0)$, the model can generate a temporal graph in $t \\\\in (\\\\tau_0, T)$. This generated temporal graph should be compared with the ground truth for similarity to assess the performance of the model. The metrics ATD and ACD can be used to measure the difference in the timestamps, as well as the edge counts along the time axis.\\n\\nWe showed that the performance gap in light of Proposition 3.1 decreases with increasing size of the temporal graph, focus should be establish TLP models on smaller datasets, first in the transductive setting, and then progress to inductive setting. In the generative method of evaluation, we can also make use of other metrics that characterise a network, or a point process to add additional constraints.\\n\\n**Broader Impact** We presented a framework, wherein we asked a counterfactual question, and then designed intervention mechanisms by generating temporally distorted test sets. In the future, researchers can devise their own temporal distortion techniques to assess the performance of a TLP model, if they follow the binary classification approach to evaluation. Our aim is also to encourage researchers to explore the generative evaluation strategy, and design TLP models which can generate temporal graphs after observing the edge events in the past. While our work focused on temporal graphs with ephemeral edges (see Definition 2.1), distortion techniques can also be designed for interval graphs, where the edge events persist for a duration. In this work, rather than introducing novel datasets, we present techniques for generating temporally distorted versions of any temporal graph dataset. This makes the contribution relevant even for datasets which will be introduced in the future.\\n\\n**Limitations** Due to resource constraints, we could not evaluate the models on more datasets. However, we aim to get additional results by the rebuttal period on the datasets used in [Poursafaei and Rabbany (2023)]. We also wanted to measure the performance of the models through ranking metrics like MRR or MAR, but the distorted datasets were not compatible with the dataloader used by Temporal Graph Benchmark (TGB) [Huang et al. (2024)].\\n\\n**References**\\n\\nA. Celikkanat, N. Nakis, and M. M\u00f8rup. Continuous-time graph representation with sequential survival process. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 38, pages 11177\u201311185, 2024.\\n\\nS. Chatterjee. Learning and memorization. In *International conference on machine learning*, pages 755\u2013763. PMLR, 2018.\\n\\nW. Cong, S. Zhang, J. Kang, B. Yuan, H. Wu, X. Zhou, H. Tong, and M. Mahdavi. Do We Really Need Complicated Model Architectures For Temporal Networks? In *The Eleventh International Conference on Learning Representations*, Sept. 2023.\\n\\nN. N. Daud, S. H. Ab Hamid, M. Saadoon, F. Sahran, and N. B. Anuar. Applications of link prediction in social networks: A review. *Journal of Network and Computer Applications*, 166:102716, 2020.\\n\\nN. Du, H. Dai, R. Trivedi, U. Upadhyay, M. Gomez-Rodriguez, and L. Song. Recurrent marked temporal point processes: Embedding event history to vector. In *Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining*, pages 1555\u20131564, 2016.\\n\\nW. Fan, Y. Ma, Q. Li, Y. He, E. Zhao, J. Tang, and D. Yin. Graph neural networks for social recommendation. In *The world wide web conference*, pages 417\u2013426, 2019.\\n\\nZ. Fan, Z. Liu, J. Zhang, Y. Xiong, L. Zheng, and P. S. Yu. Continuous-time sequential recommendation with temporal graph collaborative transformer. In *Proceedings of the 30th ACM international conference on information & knowledge management*, pages 433\u2013442, 2021.\\n\\nA. Farzaneh and J. P. Coon. An information-theoretic analysis on temporal graph evolution. In *Temporal Graph Learning Workshop@ NeurIPS 2023*, 2023.\"}"]}
{"id": "L5aY1mWvXQ", "page_num": 10, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"S. Huang, F. Poursafaei, J. Danovitch, M. Fey, W. Hu, E. Rossi, J. Leskovec, M. Bronstein, G. Rabusseau, and R. Rabbany. Temporal graph benchmark for machine learning on temporal graphs. *Advances in Neural Information Processing Systems*, 36, 2024.\\n\\nY. Kim, Y. Lee, M. Choe, S. Oh, and Y. Lee. Temporal graph networks for graph anomaly detection in financial networks. *arXiv preprint arXiv:2404.00060*, 2024.\\n\\nS. Kumar, X. Zhang, and J. Leskovec. Predicting dynamic embedding trajectory in temporal interaction networks. In *Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining*, pages 1269\u20131278, 2019.\\n\\nA. Modell, I. Gallagher, E. Ceccherini, N. Whiteley, and P. Rubin-Delanchy. Intensity profile projection: A framework for continuous-time representation learning for dynamic networks. *Advances in Neural Information Processing Systems*, 36, 2024.\\n\\nP. Panzarasa, T. Opsahl, and K. M. Carley. Patterns and dynamics of users\u2019 behavior and interaction: Network analysis of an online community. *Journal of the American Society for Information Science and Technology*, 60(5):911\u2013932, 2009.\\n\\nJ. Pearl. The seven tools of causal inference, with reflections on machine learning. *Communications of the ACM*, 62(3):54\u201360, 2019.\\n\\nJ. W. Pennebaker, M. E. Francis, and R. J. Booth. Linguistic inquiry and word count: Liwc 2001. *Mahway: Lawrence Erlbaum Associates*, 71(2001):2001, 2001.\\n\\nP. O. Perry and P. J. Wolfe. Point process modelling for directed interaction networks. *Journal of the Royal Statistical Society Series B: Statistical Methodology*, 75(5):821\u2013849, 2013.\\n\\nF. Poursafaei and R. Rabbany. Exhaustive Evaluation of Dynamic Link Prediction. In *2023 IEEE International Conference on Data Mining Workshops (ICDMW)*, pages 1121\u20131130, Shanghai, China, Dec. 2023. IEEE. ISBN 9798350381641. doi: 10.1109/ICDMW60847.2023.00147.\\n\\nF. Poursafaei, S. Huang, K. Pelrine, and R. Rabbany. Towards better evaluation for dynamic link prediction. *Advances in Neural Information Processing Systems*, 35:32928\u201332941, 2022.\\n\\nM. Qin and D.-Y. Yeung. Temporal Link Prediction: A Unified Framework, Taxonomy, and Review, June 2023.\\n\\nY. Qin, W. Ju, H. Wu, X. Luo, and M. Zhang. Learning graph ode for continuous-time sequential recommendation. *IEEE Transactions on Knowledge and Data Engineering*, 2024.\\n\\nE. Rossi, B. Chamberlain, F. Frasca, D. Eynard, F. Monti, and M. Bronstein. Temporal graph networks for deep learning on dynamic graphs. *arXiv preprint arXiv:2006.10637*, 2020.\\n\\nO. Shchur, A. C. T\u00fcrkmen, T. Januschowski, and S. G\u00fcnnemann. Neural temporal point processes: A review. *arXiv preprint arXiv:2104.03528*, 2021.\\n\\nR. Trivedi, M. Farajtabar, P. Biswal, and H. Zha. Dyrep: Learning representations over dynamic graphs. In *International conference on learning representations*, 2019.\\n\\nY. Wang, Y.-Y. Chang, Y. Liu, J. Leskovec, and P. Li. Inductive representation learning in temporal networks via causal anonymous walks. In *International Conference on Learning Representations*, 2020.\\n\\nD. Xu, C. Ruan, E. Korpeoglu, S. Kumar, and K. Achan. Inductive representation learning on temporal graphs. In *International Conference on Learning Representations*, 2020.\"}"]}
{"id": "L5aY1mWvXQ", "page_num": 11, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Checklist\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? [Yes] We have mentioned the contributions of our work in the Contributions paragraph, and also in the abstract.\\n   (b) Did you describe the limitations of your work? [Yes] In Sec. 5 we have discussed the limitations of our work.\\n   (c) Did you discuss any potential negative societal impacts of your work? [NA]\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [Yes] See Sec. 3\\n   (b) Did you include complete proofs of all theoretical results? [Yes] The proof of Proposition 3.1 is given in Sec. 3\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] The URL is provided in the abstract.\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] We have mentioned this as a footnote in Sec. 4\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] We reported the 95% confidence intervals for the metrics along with the means, which were run multiple times for random seeds, and on multiple samples.\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] This is mentioned as a footnote in Sec. 4\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes] We have cited the original sources, as well as the link where they are available.\\n   (b) Did you mention the license of the assets? [Yes] The license of the previous datasets can be found at the link.\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We have provided our code as a link in the abstract.\\n   (d) Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? [NA] The datasets are already released with a public license.\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [NA] The datasets do not contain any personally identifiable information or offensive content.\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [NA]\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [NA]\\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [NA]\"}"]}
