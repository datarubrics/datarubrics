{"id": "loJM1acwzf", "page_num": 1, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MMLONGBENCH-DOC: Benchmarking Long-context Document Understanding with Visualizations\\n\\nYubo Ma\u00b9, Yuhang Zang\u00b2*, Liangyu Chen\u00b9, Meiqi Chen\u00b3, Yizhu Jiao\u2074, Xinze Li\u00b9, Xinyuan Lu\u2075, Ziyu Liu\u2076, Yan Ma\u2077, Xiaoyi Dong\u00b2, Pan Zhang\u00b2, Liangming Pan\u2078, Yu-Gang Jiang\u2079, Jiaqi Wang\u00b2, Yixin Cao\u2079*, Aixin Sun\u00b9\\n\\n\u00b9 S-Lab, Nanyang Technological University, \u00b2 Shanghai AI Laboratory, \u00b3 Peking University, \u2074 University of Illinois Urbana-Champaign, \u2075 National University of Singapore, \u2076 Wuhan University, \u2077 Singapore Management University, \u2078 University of Arizona, \u2079 Fudan University\\n\\nAbstract\\n\\nUnderstanding documents with rich layouts and multi-modal components is a long-standing and practical task. Recent Large Vision-Language Models (LVLMs) have made remarkable strides in various tasks, particularly in single-page document understanding (DU). However, their abilities on long-context DU remain an open problem. This work presents MMLONGBENCH-DOC, a long-context, multi-modal benchmark comprising 1,082 expert-annotated questions. Distinct from previous datasets, it is constructed upon 135 lengthy PDF-formatted documents with an average of 47.5 pages and 21,214 textual tokens. Towards comprehensive evaluation, answers to these questions rely on pieces of evidence from (1) different sources (text, image, chart, table, and layout structure) and (2) various locations (i.e., page number). Moreover, 33.7% of the questions are cross-page questions requiring evidence across multiple pages. 20.6% of the questions are designed to be unanswerable for detecting potential hallucinations. Experiments on 14 LVLMs demonstrate that long-context DU greatly challenges current models. Notably, the best-performing model, GPT-4o, achieves an F1 score of only 44.9%, while the second-best, GPT-4V, scores 30.5%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse performance than their LLM counterparts which are fed with lossy-parsed OCR documents. These results validate the necessity of future research toward more capable long-context LVLMs.\\n\\n1 Introduction\\n\\nDocuments are one of the fundamental forms of information preservation and exchange. In each year, tens of millions of documents are created, read, saved, and dispatched [1]. Beyond unstructured pure-text, documents feature both complicated layout structures and information across distinct modalities such as text, table, chart, image, etc. Accordingly, the automatic understanding of documents (Document Understanding; DU) stands as a long-standing task in urgent and practical needs.\\n\\nRecently, a number of LVLMs, both closed-source ones (GPT-4o [2], Gemini-1.5 [3], Claude-3 [4], etc.) and open-source ones (InternLM-XC2-4KHD [5], InternVL-Chat [6], Otter [7], LLaVA-NeXT [8], CogVLM [9], mPLUG-DocOwl 1.5 [10], TextMonkey [11], etc.) have been developed and presented the great potential to handle documents. Most of them have achieved promising performance on single-page DU datasets like DocVQA [12], ChartQA [13], InfoVQA [14], TAT-DQA [15], etc. However, considerable amounts of documents in the real world are long-context\\n\\n*Corresponding Authors.\\nProject Page: https://mayubo2333.github.io/MMLongBench-Doc\\n\\n38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.\"}"]}
{"id": "loJM1acwzf", "page_num": 2, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"documents with tens or even hundreds of pages. The understanding of these lengthy documents brings new challenges for LVLMs from at least two aspects: (1) **Localization**: identify and retrieve information from massive, heterogeneous information (similar to the *needle in a haystack* task); (2) **Cross-page comprehension**: collect and reason over multi-source information across different pages. These two kinds of abilities are beyond the evaluation scopes of the aforementioned single-page DU datasets. Some recent DU datasets [16, 17, 18] feature multiple-page DU, but almost all their documents are either as short of only several pages or of low information density, making the localization-related questions over-simple. Additionally, few (if any) questions in these datasets necessitate cross-page comprehension. See more detailed related work in Section 2. In summary, there lacks a unified and high-quality benchmark on lengthy documents, leaving the evaluation of long-context DU largely unexplored.\\n\\nIn this paper, we present MMLONGBENCH-DOC, a benchmark designed to evaluate the Multi-Modality Long-context Document understanding abilities of LVLMs. Towards a comprehensive benchmark, it incorporates lengthy documents from both four existing datasets [13, 17, 18, 19] and other various papers, brochures, etc. Consequently, our benchmark includes 135 PDF-formatted documents spanning across 7 diverse domains, with each document averaging 47.5 pages and 21,214.1 textual tokens. Regarding the questions, we employ ten expert-level annotators to (1) edit questions associated with documents from existing datasets to meet our benchmark\u2019s standard and (2) create new questions for all collected documents to expand the scale of the benchmark. Then a three-round, semi-automatic reviewing process ensures the benchmark\u2019s annotation quality. As a result, MMLONGBENCH-DOC comprises 1,082 human-annotated questions, with 184 sourced from four existing datasets and 898 newly annotated. Being a multi-modal benchmark, the answer to each question requires evidence from one or more of these five in-document sources: *text, layout, chart, table*, and *image*. Questions are categorized into three types based on the number of evidence pages\\\\(^1\\\\) with examples illustrated in Figure 1(a): (1) 494 *single-page* questions (with one evidence page) mainly to evaluate localization abilities, (2) 365 *cross-page* questions (with multiple evidence pages) to assess cross-page comprehension, and (3) 223 *unanswerable* questions (no evidence for answering it, *i.e.*, no evidence pages) to reduce shortcuts and measure LVLMs\u2019 potential hallucinations. Meta-information including evidence pages, sources, and answer formats, is preserved for fine-grained evaluation and analysis. Detailed descriptions of the annotation pipeline and statistics can be found in Section 3.\\n\\nWe conduct extensive experiments on MMLONGBENCH-DOC to evaluate the long-context DU abilities of 14 LVLMs, including 4 proprietary and 10 open-source ones. Given a document, we screenshot each page and feed all of these PNG-formatted images to LVLMs in an end-to-end approach. For comparison, we also convert the documents to textual format by optical character recognition (OCR) and evaluate another 6 proprietary and 4 open-source 10 LLMs (6 proprietary and\\n\\n---\\n\\n\\\\(^1\\\\)Given a document \\\\(D\\\\) and a question \\\\(q\\\\) upon \\\\(D\\\\), We call page \\\\(P\\\\) (in document \\\\(D\\\\)) an *evidence page* of \\\\(q\\\\) if the answer of \\\\(q\\\\) necessitates one or more pieces of evidence in page \\\\(P\\\\).\"}"]}
{"id": "loJM1acwzf", "page_num": 3, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison between our benchmark and previous DU datasets. **Unans.**: unanswerable question. **TXT/L/C/TAB/I**: pure text/generalized layout/chart/table/image. **Doc. Rel.**: document relevance. Whether document information is indispensable for the answer. **Avg. Position**: the average page index on which the answer evidence is located. *:Statistics from [20].\\n\\n| Benchmarks       | Document | Question type | Answer Evidence | Avg. Position |\\n|------------------|----------|---------------|-----------------|---------------|\\n|                  | # Pages  | # Tokens      | Cross-page (%)  | Unans. (%)    | Doc. Rel. | Source | Avg. Position |\\n| DocVQA [12]      | 1.0      | 151.5         | \u2717               | \u2717             | \u2714         | TXT/L/C/TAB/I | -            |\\n| ChartQA [13]     | 1.0      | 236.9         | \u2717               | \u2717             | \u2714         | C      | -            |\\n| InfoVQA [14]     | 1.2      | 288.0         | \u2717               | \u2717             | \u2714         | L/C/TAB/I | -            |\\n| TAT-DQA [15]     | 1.1      | 577.0         | \u2717               | \u2717             | \u2714         | TXT/TAB | -            |\\n| VisualWebBench [21] | 1.0      | 452.4         | \u2717               | \u2717             | \u2714         | LAY/I  | -            |\\n| MP-DocVQA [16]   | ~12*     | ~7000*        | \u2717               | \u2717             | \u2714         | TAB    | -            |\\n| DUDE [17]        | 5.7      | 1831.5        | \u2714(2.1%)         | \u2714(12.7%)      | \u2714         | TXT/L/C/TAB/I | 2.5         |\\n| SlideVQA [18]    | 20.0     | 2030.5        | \u2714(13.9%)        | \u2717             | \u2714         | TXT/L/C/TAB/I | 9.1          |\\n| MMLONGBENCH-DOC  | 47.5     | 21214.1       | \u2714(33.0%)        | \u2714(22.5%)      | \u2714         | TXT/L/C/TAB/I | 23.6         |\\n\\n4 open-source ones). The results in Figure 1(c) highlight the challenges that current LVLMs face with long-context DU. The best-performing LVLM, GPT-4o, achieves an overall F1 score of only 44.9%, while the second-best LVLM, GPT-4V, scores 30.5%. Moreover, all the remaining LVLMs tested with multi-modal documents performed worse than single-modal LLMs handling lossy, OCR-parsed texts. Specifically, the Gemini-1.5-Pro and Claude-3-Opus present 4.2% and 6.4% absolute decrease when the inputs change from document screenshots to OCR-parsed texts. Regarding open-source models, the best-performing LVLM lags behind the best-performing LLM by 11.7%. These results reveal that long-context DU is a far-from-resolved task for current LVLMs.\\n\\n2 Related Work\\n\\n**Benchmarks for Document Understanding.** A great amount of datasets have emerged to evaluate the DU capabilities of LVLMs. Many datasets focus exclusively on either a single component (e.g., table, chart) [13; 15; 21; 22] or a single page [12; 14] from the full documents. Some recent DU datasets [16; 17; 18; 23; 19] attempt to assess multi-page documents, but still exhibit shortcomings in terms of document length (page number), information density (token number) and the construction approaches. Specifically, MP-DocVQA [16] is an extension of DocVQA [12] and inherently absent of both cross-page and unanswerable questions. Annotating from scratch, DUDE [17] includes a small percentage of cross-page questions (2.1%) and unanswerable questions (12.7%). However, due to the relatively short context length (5.3 pages on average) and the use of crowd-sourced annotations, questions in DUDE tend to be less challenging and somewhat less rigorous. SlideVQA features 20-page documents and cross-page questions (12.9%). Nevertheless, the documents in SlideVQA are in slide-deck format and of relatively low information density. Moreover, these cross-page questions are HotpotQA-style [24] created by instantiating entity graphs and co-referencing in-graph entities across multiple pages. The entity graph from a closed document tends to be sparse and has significant shortcuts (see examples in Appendix A.4). These shortcuts sometimes lead to false cross-page questions that actually do not require answer evidence across different pages. The recent FinanceBench [19] features both extremely long-context documents and practical, scalable cross-page questions. However, its documents are exclusively financial reports. Additionally, the reference answers are in open-ended formats, making the expert-level manual evaluation indispensable. The above reasons limit the broader applicability of FinanceBench. To our best knowledge, MMLONGBENCH-DOC is the first comprehensive, qualified, and easy-to-use benchmark on the long-context DU task. More detailed descriptions and comparisons are presented in Table 1.\\n\\n**Models for Document Understanding.** There are two main branches of models for automatic DU tasks. The first approach employs two-stream, OCR-dependent architectures to separately encode textual information (parsed via OCR) and visual information (images and/or layout structures) [25; 26; 27]. In contrast, the second approach develops OCR-free models that understand documents.\\n\\n---\\n\\n2 We view website screenshots and posters as generalized documents and define *equivalent page number* (EPN) to measure their context lengths: \\\\( \\\\text{EPN}(D) = \\\\text{ceil}(\\\\frac{\\\\text{Pixel}(D)}{P}) \\\\). Here \\\\( \\\\text{Pixel}(D) \\\\) is the pixel number of generalized document \\\\( D \\\\), and \\\\( P \\\\) is the average pixel numbers of each page (converting from .pdf to .png format with resolution 240) in MMLONGBENCH-DOC.\"}"]}
{"id": "loJM1acwzf", "page_num": 4, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"in an end-to-end manner \\\\cite{28,29}. With the rapid advancement of LVLMs, the latter approach has dominated the current DU solutions. As mentioned above, a range of LVLMs demonstrate promising performance on single-page DU datasets. However, as shown in Section 4 even the most advanced LVLMs fall significantly short of achieving satisfactory performance on our benchmark. It reveals that understanding lengthy documents still poses great challenges to current LVLMs.\\n\\n**Long-context LVLMs and LLMs.** Lengthy documents necessitate the use of LVLMs or LLMs with extended context sizes. Several benchmarks \\\\cite{30,31,32,33} and solutions \\\\cite{34,35,36,37} have been proposed to evaluate and develop long-context LLMs. However, there exists limited related work for long-context LVLMs, leaving this area largely unexplored. Until very recently, contemporary studies \\\\cite{38,39,40} assess and/or improve LVLMs\u2019 multi-image understanding capabilities. Evaluations on both MMLONGBENCH-Doc and these works indicate that current LVLMs are still not fully equipped to handle long-context DU and many other practical tasks that require extensive contextual comprehension.\\n\\n## 3 MMLONGBENCH-Doc\\n\\nWe design a three-stage annotation pipeline for the construction of our benchmark. The three stages will be introduced in Section 3.1, Section 3.2, and Section 3.3 respectively. We also provide key statistics of our benchmark in Section 3.4.\\n\\n### 3.1 Document Collection\\n\\nAs a long-context DU benchmark, the documents shall be of diverse topics and lengthy enough. To this end, we crawl a great amount of documents from various sources. Then we select the lengthy ones from these documents. Specifically, we encompass a diverse array of documents from two approaches. (1) **Existing documents** from four previous datasets: DUDE \\\\cite{17}, SlideVQA \\\\cite{18}, ChartQA \\\\cite{13}, and FinanceBench \\\\cite{19}. (2) **Newly-collected documents** from Arxiv \\\\cite{3}, ManualsLib \\\\cite{4} and Google Search \\\\cite{5}. Then we (1) filter out the documents with fewer than 15 pages or license restrictions and (2) down-sample documents from DUDE, SlideVQA, and FinanceBench for a more balanced distribution. Detailed descriptions of our selection and processing procedure can be found in Appendix A.1 and Appendix A.2.\\n\\nIn summary, we collect a total of 135 documents. Among them, 76 documents are from existing datasets and incorporate previously annotated questions (represented as triangles). The remaining 59 documents are newly collected and incorporate no existing questions. We manually categorize them into 7 types: Research Report, Financial Report, Academic Paper, Brochure, Guideline, Administration & Industry File, Tutorial / Workshop. We showcase some instances of these documents in Appendix A.3.\\n\\n### 3.2 Question and Answer Collection\\n\\nTo serve as a high-quality and comprehensive benchmark, the question annotation of our benchmark adheres to the following standards: (1) All questions shall be neither over-easy nor over-difficult. (2) Questions are not repetitively derived from the same page or the same pattern. (3) The distribution of evidence numbers, evidence sources, and evidence locations for the questions shall be balanced. (4) No questions shall be answered correctly without accessing the relevant documents.\\n\\nTen authors serve as expert-level annotators for the question-and-answer collection. All of them are doctors or Ph.D. students proficient in English reading and writing. Before formal annotation, they undergo a training session and pre-annotate three documents for practice. We iteratively review their annotation results and provide personalized feedback until their annotations meet the standards mentioned above. Regarding the formal annotation, we divide 135 documents into 54 batches (each having 2-4 documents) and dispatch these batches to annotators. We then ask the annotators to submit their results in units of batches and set reasonable time intervals for each batch\u2019s submission. We\\n\\n---\\n\\n\\\\(^3\\\\)https://arxiv.org\\n\\n\\\\(^4\\\\)https://www.manualslib.com\\n\\n\\\\(^5\\\\)https://www.google.com.sg\"}"]}
{"id": "loJM1acwzf", "page_num": 5, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"timely evaluate their annotations after each submission and remind the annotators if their questions in this turn diverge from the standards. It avoids the annotators rushing all assignments in a short time and benefits the annotation quality. We recommend the annotators take 60-90 minutes on each document. Specifically, the annotators shall rapidly read through the whole document in the first 15-30 minutes. For the remaining time, they shall dive deep into specific components to modify existing annotations and/or add new annotations as detailed below.\\n\\n**Modify Existing Questions.** Documents collected from existing datasets had been annotated with some questions and answers from previous work. However, their crowd-sourcing annotations inevitably make some questions, answers, and other meta information unqualified. Therefore, we edit their annotations before including them as a component of our benchmark.\\n\\nSpecifically, we classify six potential problems in original annotations: *Wrong Answers or Evidence Pages, Repetitive Question, Ambiguous Question, Decontextualization-required Question, Low Document-relevant Question* and *Potential Shortcut*. See detailed explanations and examples about these problems in Appendix [A.4](#). Given an existing document, the annotators are tasked to evaluate each existing question\u2019s quality according to whether they have one or more above problems and assign a label from {Retain, Revise, Remove} for each question. Then the annotators would revise the Revise questions to meet our quality criteria and remove the Remove questions. Among all 425 original questions from 76 existing documents, 32.2% of them are revised and 46.1% are removed. We finally collect 211 questions in this procedure. The corresponding GUI is shown in Appendix [A.7](#).\\n\\n**Add New Questions.** We newly annotate questions on both existing and newly collected documents to expand the questions in our benchmark. Specifically, we ask annotators to add about 3 questions on existing documents, and 6 questions on newly-collected documents. Given most existing questions (even after editing) are single-page ones and sourced from texts, we put more focus on (1) cross-page and unanswerable questions and (2) questions sourced from tables, charts, and images for newly added questions to balance the distribution. We detail the quantitative requirements in Appendix [A.5](#). Associated with questions, annotators also provide reference answers and meta-information (*i.e.*, evidence sources, answer format, evidence locations) for all samples. We finalized a collection of 965 samples in this procedure. The corresponding GUI is shown in Appendix [A.7](#).\\n\\n### 3.3 Quality Control\\n\\nCombining the merits of humans and LVLMs, we adopt a three-round, semi-automatic quality control procedure to improve the annotation quality of our benchmark. We detail each round in the following components and leave the discussion of potential bias in Appendix [A.6](#).\\n\\n**Document-relevant Detection.** Our benchmark is designed to evaluate LVLMs\u2019 long-context document understanding abilities. All questions are expected to be unanswerable without access to corresponding documents. To remove low document-relevant questions (*i.e.*, questions not relying on documents), we feed each annotated question WITHOUT documents to GPT-4o. A question will be identified as *low document-relevant* question if GPT-4o correctly predicts under this case. Ultimately, 94 samples are identified as low document-relevant questions and removed in this round.\\n\\n**Self-reflection.** We draw inspirations from MMBench [41](#) and leverage LVLMs to reduce the wrongly-annotated samples. Specifically, we feed the remaining questions from the last round WITH their documents to GPT-4o. Samples whose model predictions are inconsistent with the reference answers are sent back to corresponding annotators. The annotators are asked to check each question and identify whether the inconsistency is caused by *problematic annotation* or not. As a result, 13.8% of the samples are identified as problematic annotations. The annotators revise them accordingly.\\n\\n**Cross-checking.** In parallel, annotators cross-check the annotated samples from other annotators and determine the inconsistency reasons the same as described above. We calculate Cohen\u2019s kappa value of their identifications as 0.42 (17.5% inconsistent samples), showing a moderate agreement. Regarding the 17.5% inconsistent samples, two primary authors serve as meta-annotators and make final decisions on them (and if necessary, revise accordingly).\\n\\n### 3.4 Dataset Overview and Analysis\\n\\nThe main statistics of MMLONGBENCH-DOC are presented in Table [2](#). Overall, our benchmark consists of 1,082 questions. These questions are constructed upon 135 lengthy documents across 7\"}"]}
{"id": "loJM1acwzf", "page_num": 6, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Statistic                  | Number |\\n|---------------------------|--------|\\n| **Documents**             | 135    |\\n| - Type                    | 7      |\\n| - Average/Medium pages    | 47.5 / 28 |\\n| - Average/Medium length   | 21,214.1 / 12,179 |\\n| **Total questions**       | 1,082  |\\n| - Single-page question    | 494 (45.7%) |\\n| - Cross-page questions    | 365 (33.7%) |\\n| - Unanswerable questions  | 223 (20.6%) |\\n| - Derived questions       | 184 (17.0%) |\\n| - Newly-annotated questions | 898 (83.0%) |\\n| **(Evidence source)**     |        |\\n| - Pure-text               | 305 (35.5%) |\\n| - Layout                  | 119 (13.9%) |\\n| - Table                   | 218 (25.4%) |\\n| - Chart                   | 178 (20.7%) |\\n| - Image                   | 304 (35.4%) |\\n| **(Answer Format)**       |        |\\n| - String                  | 250 (29.1%) |\\n| - Integer                 | 299 (34.8%) |\\n| - Float                   | 159 (18.5%) |\\n| - List                    | 151 (17.6%) |\\n| **Avg./Max. question length** | 16.4 / 60 |\\n| **Avg./Max. answer length** | 2.8 / 54 |\\n\\nTable 2: Dataset Statistics\\n\\nFigure 2: Detailed distribution of documents. **Top:** Document type. **Middle:** Page Number. **Bottom:** Token Number.\\n\\nFigure 3: Detailed distribution of questions & answers. **Left:** Absolute position of answer evidences (the page index). **Middle:** Relative position (the page index/document page number). **Right:** Evidence page number of each question. (0: unanswerable question; >2: cross-page question).\\n\\ndocument types, with an average of 47.5 pages and 21,214.1 tokens. Please see detailed distributions of these documents in Figure 2. Regarding the questions, there are 494 single-page questions (1 evidence page), 365 cross-page questions (2+ evidence pages), and 223 unanswerable questions (no evidence page). These three types of questions evaluate the LVLMs\u2019s long-context DU capabilities from complementary aspects: the localization ability, the cross-page comprehension ability, and the hallucination severity, respectively. For single-page and cross-page questions, their answer evidence is scattered among different context sources (i.e., text, layout, table, chart, image) and evenly distributed across different locations of the documents (see Table 2, Figure 3 Left and Middle). Also notably, 28.6% of cross-page questions have more than two evidence pages, which further enhances the challenge of our benchmark.\\n\\n4 Evaluation\\n\\n4.1 Evaluation Protocol\\n\\nWe follow MATHVISTA [56] to conduct a three-step evaluation protocol: response generation, answer extraction, and score calculation. We adopt such a protocol out of three considerations: (1) Current LVLMs are instructed to generate long responses, rather than short-form answers, in conventional settings. (2) The evaluation of long responses, however, remains an open and challenging problem. (3) We focus on the document understanding (not instruction following) abilities of LVLMs.\"}"]}
{"id": "loJM1acwzf", "page_num": 7, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: **Evaluation of various models on MMLONGBENCH-DOC.** We report the generalized accuracy of five types of evidence sources including pure text (TXT), layout (LAY), chart (CHA), table (TAB), and image (IMG). We also present the generalized accuracy of questions categorized by the number of evidence pages: single-page (SIN), cross-page (MUL), and unanswerable (UNA) questions. The **best** and **second-best** performance in each section are highlighted.\\n\\n| Model                                      | #Param | Context Window | Evidence Source | Evidence Page | ACC  | F1  |\\n|--------------------------------------------|--------|----------------|-----------------|---------------|------|-----|\\n| **OCR (Tesseract [42]) + Large Language Models (LLMs)** |        |                |                 |               |      |     |\\n| ChatGLM-128k [37]                          | 6B     | 128k           | TXT             | 23.4          | 18.8 | 16.3|\\n| Mistral-Instruct-v0.2 [43]                  | 7B     | 32k            | LAY             | 19.9          | 16.9 | 16.4|\\n| Mixtral-Instruct-v0.1 [44]                  | 8x7B   | 32k            | CHA             | 24.2          | 21.3 | 21.0|\\n| Mixtral-Instruct-v0.1 [44]                  | 8x22B  | 64k            | TAB             | 34.2          | 21.3 | 21.0|\\n| **Proprietary Models**                      |        |                | FIG             | 12.7          | 11.5 | 11.3|\\n| QWen-Plus [45]                              | -      | 32k            | SIN             | 17.4          | 14.2 | 18.9|\\n| DeepSeek-V2 [46]                            | -      | 32k            | MUL             | 27.8          | 20.2 | 24.9|\\n| Claude-3 Opus [4]                           | -      | 32k            | UNA             | 30.8          | 32.0 | 26.9|\\n| Gemini-1.5-Pro [3]                          | -      | 32k            |                 | 29.3          | 21.2 | 17.0|\\n| GPT-4-turbo [47]                            | -      | 128k           |                 | 36.5          | 28.7 | 27.6|\\n| GPT-4o [2]                                  | -      | 128k           |                 | 41.1          | 35.4 | 30.1|\\n| **Large Visual Language Models (LVLMs)**    |        |                |                 |               |      |     |\\n| DeepSeek-VL-Chat [48]                       | 7.3B   | 4k             | TXT             | 7.2           | 5.2  | 7.4 |\\n| Idefics2 [49]                               | 8B     | 8k             | LAY             | 9.0           | 8.7  | 7.7 |\\n| MiniCPM-Llama3-V2.5 [50, 51]                | 8B     | 2k             | CHA             | 11.9          | 9.5  | 8.5 |\\n| InternLM-XC2-4KHD [5]                       | 8B     | 16k            | TAB             | 9.9           | 12.6 | 10.3|\\n| mPLUG-DocOwl 1.5 [52]                       | 8.1B   | 4k             | FIG             | 8.2           | 7.4  | 6.9 |\\n| Qwen-VL-Chat [53]                           | 9.6B   | 6k             | SIN             | 5.5           | 5.2  | 6.1 |\\n| Monkey-Chat [54]                            | 9.8B   | 2k             | MUL             | 6.8           | 6.6  | 6.2 |\\n| **Open-source, >14B Models**                |        |                | UNA             | 7.7           | 5.7  | 8.3 |\\n| CogVLM2-LLaMA3-Chat [9]                     | 19B    | 8k             |                 | 3.7           | 3.9  | 4.4 |\\n| InternVL-Chat-v1.5 [6]                      | 26B    | 4k             |                 | 14.0          | 14.9 | 14.6|\\n| EMU2-Chat [55]                              | 37B    | 2k             |                 | 6.1           | 5.7  | 8.3 |\\n| **Proprietary Models**                      |        |                |                 |               |      |     |\\n| Claude-3 Opus [4]                           | -      | 200k           | TXT             | 24.9          | 25.6 | 17.4|\\n| Gemini-1.5-Pro [3]                          | -      | 128k           | LAY             | 21.0          | 21.1 | 28.2|\\n| GPT-4V(ision) [47]                          | -      | 128k           | CHA             | 34.4          | 36.4 | 32.4|\\n| GPT-4o [2]                                  | -      | 128k           | TAB             | 46.3          | 54.5 | 42.8|\\n| **Human Baseline**                          |        |                | FIG             | 46.0          | 41.5 | 44.9|\\n\\nSpecifically, we impose no limitations on *response generation* stage to encourage LVLMs to answer the questions in a freestyle. Then we propose a unified LLM-based *answer extractor* (GPT-4o under our setting) to convert their long responses to short-form answers. Finally, we use a rule-based *score calculator* to evaluate the converted short answers. We report both generalized accuracy and generalized F1 score to balance the answerable (positive) and unanswerable (negative) questions. The used prompt, the high correlation between our automatic *answer extractor* and human evaluation, and the detailed rules of our *score calculation* are described in Appendix B.\\n\\n### 4.2 Experimental Setup\\n\\nWe evaluate 14 LVLMs on MMLONGBENCH-DOC, including 4 proprietary LVLMs and 10 open-source LVLMs. To purely evaluate LVLMs\u2019 long-context DU abilities, we screenshot each page of the PDF-formatted document with 144 DPI and feed all these PNG-formatted images to LVLMs in an end-to-end approach. Notably, all evaluated open-source LVLMs do not support multi-image inputs or present significant performance drops when fed with excessive images (*e.g.*, more than 10 or 20 images). Therefore, we employ a concatenation strategy that combines all screenshot pages into 1 or 5 images and feeds these concatenated images to open-source LVLMs. Regarding proprietary LVLMs, we adopt the same concatenation strategy and reduce the image number to 20 for Claude-3-Opus to fit its maximum image threshold. For GPT-4o, GPT-4V, and Gemini-1.5-Pro, we directly send all original screenshots to them (*i.e.*, the image number equals the page number).\\n\\nFor comparison, we also use the Tesseract [42] OCR model to recognize and extract texts from the documents and feed the parsed documents to 10 LLMs, including 6 proprietary and 4 open-source\"}"]}
{"id": "loJM1acwzf", "page_num": 8, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ones. Texts exceeding their context lengths are truncated. Notably, as a key component of the classical solution for the DU task, the OCR model can handle most flattened texts and some structured tables in the document. However, it cannot perceive the information from the charts or images. Thus the TXT-formatted, OCR-parsed documents are lossy documents in which the information is not fully preserved. More detailed hyperparameters are introduced in Appendix B.5. Additionally, we also conduct manual evaluation on a subset of our datasets (238 questions from 29 documents) to indicate the difficulty of this task for humans.\\n\\n4.3 Main Results\\n\\nWe compare the performance of different LVLMs and LLMs in Table 3, reporting their generalized accuracy and F1 scores (shown in the last two columns). Regarding LVLMs, we draw several conclusions as below: (1) The performance demonstrates that long-context DU is still a challenging and unsolved task for current LVLMs. The best-performing LVLM, GPT-4o, merely achieves a 44.9% F1 score. The second best-performing LVLM, GPT-4V, lags behind by over 10% percent and presents a 31.4% F1 score. All other LVLMs only achieve about 20% or even lower F1 scores. (2) Though far from satisfactory, GPT-4o performs much better than all other models (including GPT-4V). Thus we speculate that the multi-modal pre-training paradigm significantly benefits LVLMs\u2019 cross-modality understanding capabilities. (3) Proprietary LVLMs perform better than open-source LVLMs by a large margin. We attribute it to the difference of acceptable image numbers: open-source LVLMs only support single-image or several-image inputs, while proprietary LVLMs can be fed with at least 20 images or even more. Given that lengthy documents have tens of even hundreds of pages, it is impractical for open-source LVLMs to accurately perceive the information in the documents from the excessively concatenated images. (4) The performances of different models are highly correlated with their acceptable image numbers and maximum image resolutions. Notably, open-source LVLMs that support high-resolution images (i.e., InternLM-XC2-4KHD and InternVL-Chat-v1.5) exhibit superior performance compared to those with lower resolution limits.\\n\\nSurprisingly, LVLMs even demonstrate overall worse performance than LLMs, even LLMs are fed with lossy OCR-parsed documents. Specifically, Gemini-1.5-Pro and Claude-3 Opus have 4.2% and 6.4% absolute F1-score degradations on vision versions. And the best-performing LLM (Mixtral) also surpasses the best-performing LVLM (InternVL-v1.5) by 11.7%. The above results clearly reveal that most current LVLMs are still not proficient in cross-modality, long-context document understandings. It is promising that GPT-4o and GPT-4-turbo achieve better performance when seeing multi-modality PDF documents than parsed text by 14.4% and 5.3% F1-score, respectively. Their performances validate the feasibility, benefit, and necessity of understanding documents in an end-to-end, cross-modality approach. We speculate that the scarce related pre-training corpus (i.e., extremely multi-image or lengthy documents) hinders the long-context DU capabilities of other LVLMs. We will leave related explorations for future work.\\n\\nRegarding the human evaluation, we observe 66.0% F1-score from our annotators and a significant performance gap (exceeding 20% in absolute) between the current LVLMs and humans. This gap highlights the challenges of document understanding for LVLMs and the necessity of our benchmark.\\n\\n4.4 Fine-grained Results.\\n\\n**Document Type.** As illustrated in Figure 4, LVLMs and LLMs exhibit distinct performance patterns across various document types. Our findings include: (1) All evaluated models demonstrate decent performance on industrial documents, which tend to have more standardized formats and less non-textual information. (2) The GPT series and Mixtral (i.e., the SoTA open-source LLM) show relatively balanced performance across different document types. In contrast, other models perform significantly worse in specialized domains such as academic papers and financial reports. (3) When equipped with OCR, LLM-based models like GPT-4 and Mixtral achieve comparable or even superior performance on industrial documents, academic papers, and brochures. Conversely, end-to-end LVLMs outperform OCR+LLMs in areas such as tutorials, research reports, and guidelines. We speculate that comprehending these latter document types requires more extensive multi-modal information, from which LVLMs significantly benefit.\\n\\n**Evidence Source.** We categorize questions based on their evidence sources and present fine-grained results in Figure 4 and Table 3. Our observations reveal that only GPT-4o exhibits relatively balanced\"}"]}
{"id": "loJM1acwzf", "page_num": 9, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"performance across the different sources. Other LVLMs, however, show inferior performance on questions related to charts and/or images compared to those related to text and/or layout. Additionally, LLMs generally demonstrate better or comparable performance to LVLMs on text- and table-related questions but show worse performance on questions involving other elements. This highlights the limitations of OCR (and other PDF parsers) when dealing with charts and images, as well as the gap in OCR capabilities between LVLMs and pure-text LLMs.\\n\\n**Evidence Position.** We also examine how the evidence locations (i.e., the page indexes where the answer evidence is found) affect model performance. The results shown in Figure 5 reinforce that MMLONGBENCH-DOC poses significant challenges for current models, at least partially due to the extended length of the documents. Almost all models (except InternVL-v1.5) exhibit their best performance on questions derived from the initial pages, while their performance declines progressively as the page index increases. Interestingly, two proprietary models, Gemini-Pro-1.5 and Claude-3-Opus, experience particularly sharp declines in performance.\\n\\n**Number of Evidence Page.** We observe a consistent trend that all models achieve higher scores on single-page questions than cross-page questions. It reveals that gathering and reasoning over all necessary information across different pages is not trivial for current LVLMs and LLMs. More interestingly, evaluated LVLMs behave differently on unanswerable questions. GPT-4o and Claude-3 Opus adopt more aggressive strategies and usually tend to provide some answers. It makes their answers more likely helpful, but also increases the risk of hallucination and unfaithfulness (see their scores on unanswerable questions are much lower than answerable questions). On the contrary, Gemini-1.5-Pro, DeepSeek-VL-Chat, and EMU2-Chat are much more cautious and tend to refuse to answer questions about which they are uncertain. It makes their answers safer but less helpful (with large amounts of responses like *I don\u2019t know*).\\n\\n## 5 Analysis & Discussion\\n\\n### 5.1 Oracle Setting\\n\\nWe conduct additional experiments to explore to what extent the challenges of MMLONGBENCH-DOC are caused by the long-context lengths of documents. Specifically, we feed 820 answerable questions along with their oracle evidence pages (instead of the whole documents) to three representative LVLMs and show results in Figure 6. On one hand, it indicates that long-context length is a\"}"]}
{"id": "loJM1acwzf", "page_num": 10, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"significantly challenging factor for document understanding. Compared with the oracle-page setting, lengthy documents lead to more than 20% absolute performance degradation on Gemini-1.5-Pro and InternLM-XC2-4KHD. Regarding the single-page questions, the performance difference even achieves up to 30%. On the other hand, the overall performance achieves only about 40% and 30% for Gemini-1.5-Pro and InternLM-XC2-4KHD even under oracle-page setting. And the improvement for GPT-4o is much less (about 10%). It demonstrates that the development of long-context LVLMs can largely facilitate, though still can not fully solve, the long-context DU task.\\n\\n![Figure 6: Performance comparisons between normal setting (feeding models with the whole documents) and oracle setting (feeding models only with the evidence pages) among three LVLMs.](image)\\n\\n5.2 Error Analysis\\n\\nWe further conduct error analysis to understand the bottleneck of current LVLMs in a qualitative approach. Specifically, we randomly select 72 error predictions from GPT-4o\u2019s responses and manually check their error reasons. These errors are categorized into seven types: Perceptual Error, Irrelevant Answer, Incomplete Evidence, Hallucinated Evidence, Extractor Error, Reasoning Error and Knowledge Lacking. The distribution of these errors is illustrated in Figure 7. It indicates that most errors come from the model\u2019s hallucination (i.e., wrong explanations and answers to unanswerable questions) and perceptual errors (mainly in visual contexts). Additionally, GPT-4o sometimes misunderstands the intent of questions and provides irrelevant responses. The errors caused by collecting incomplete evidence (for cross-page questions) are also unignorable. The descriptions and examples of these error types are detailed in Appendix C.1.\\n\\n6 Conclusion\\n\\nIn this work, we present MMLONGBENCH-DOC to evaluate the long-context DU capabilities of LVLMs. Extensive experiments on 14 LVLMs (and 10 LLMs for comparison) reveal that the understanding of lengthy documents poses great challenges to current LVLMs. Even though the performance of GPT-4o proves the benefit of end-to-end, multi-modality perception for DU tasks, most LVLMs struggle on long visual contexts (i.e., extremely multiple images) and show inferior performance compared to OCR+LLM pipelines. We hope that the construction of our benchmark could push forward the development of more powerful LVLMs on lengthy document understanding.\\n\\nAcknowledgements\\n\\nThis study is supported under the RIE2020 Industry Alignment Fund \u2013 Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). This work is also supported by Shanghai Artificial Intelligence Laboratory, the National Key R&D Program of China (2022ZD0160201).\"}"]}
{"id": "loJM1acwzf", "page_num": 11, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Lutz Bornmann and R\u00fcdiger Mutz. Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references. *Journal of the Association for Information Science and Technology*, 66, 2014.\\n\\n[2] Open AI. Hello gpt-4o, 2024.\\n\\n[3] Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024.\\n\\n[4] Anthropic. Introducing the next generation of claude, 2024.\\n\\n[5] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et al. Internlm-Xcomposer2-4KHD: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. *ArXiv preprint*, abs/2404.06512, 2024.\\n\\n[6] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. *ArXiv preprint*, abs/2404.16821, 2024.\\n\\n[7] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning, 2023.\\n\\n[8] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. LLaVA-NeXT: Stronger llms supercharge multimodal capabilities in the wild, 2024.\\n\\n[9] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. CogVLM: Visual expert for pretrained language models, 2023.\\n\\n[10] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding, 2024.\\n\\n[11] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai. Textmonkey: An ocr-free large multimodal model for understanding document, 2024.\\n\\n[12] Minesh Mathew, Dimosthenis Karatzas, R. Manmatha, and C. V. Jawahar. Docvqa: A dataset for vqa on document images. *2021 IEEE Winter Conference on Applications of Computer Vision (WACV)*, pages 2199\u20132208, 2020.\\n\\n[13] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: A benchmark for question answering about charts with visual and logical reasoning. In *Findings of the Association for Computational Linguistics: ACL 2022*, pages 2263\u20132279, Dublin, Ireland, 2022. Association for Computational Linguistics.\\n\\n[14] Minesh Mathew, Viraj Bagal, Rub\u00e8n P\u00e9rez Tito, Dimosthenis Karatzas, Ernest Valveny, and C.V. Jawahar. Infographicvqa. *2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)*, pages 2582\u20132591, 2021.\\n\\n[15] Fengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang, and Tat-Seng Chua. Towards complex document understanding by discrete reasoning. In *Proceedings of the 30th ACM International Conference on Multimedia*, pages 4857\u20134866, 2022.\\n\\n[16] Rub\u00e8n Tito, Dimosthenis Karatzas, and Ernest Valveny. Hierarchical multimodal transformers for multi-page docvqa, 2023.\\n\\n[17] Jordy Van Landeghem, Rub\u00e8n P\u00e9rez Tito, \u0141ukasz Borchmann, Michal Pietruszka, Pawel J\u2019oziak, Rafal Powalski, Dawid Jurkiewicz, Micka\u00ebl Coustaty, Bertrand Ackaert, Ernest Valveny, Matthew B. Blaschko, Sien Moens, and Tomasz Stanislawek. Document understanding dataset and evaluation (DUDE). In *ICCV*, 2023.\\n\\n[18] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. SlideVQA: A dataset for document visual question answering on multiple images. In *AAAI*, 2023.\\n\\n[19] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. FinanceBench: A new benchmark for financial question answering, 2023.\"}"]}
{"id": "loJM1acwzf", "page_num": 12, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[20] \u0141ukasz Borchmann, Michal Pietruszka, Tomasz Stanislawek, Dawid Jurkiewicz, Micha\u0142 Turski, Karolina Szyndler, and Filip Gralinski. Due: End-to-end document understanding benchmark. In NeurIPS Datasets and Benchmarks, 2021.\\n\\n[21] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. VisualWebBench: How far have multimodal llms evolved in web page understanding and grounding?, 2024.\\n\\n[22] Marcin Kardas, Piotr Czapla, Pontus Stenetorp, Sebastian Ruder, Sebastian Riedel, Ross Taylor, and Robert Stojnic. AxCell: Automatic extraction of results from machine learning papers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8580\u20138594, Online, 2020. Association for Computational Linguistics.\\n\\n[23] Jon Saad-Falcon, Joe Barrow, Alexa Siu, Ani Nenkova, David Seunghyun Yoon, Ryan A. Rossi, and Franck Dernoncourt. Pdftriage: Question answering over long, structured documents, 2023.\\n\\n[24] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369\u20132380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.\\n\\n[25] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash, editors, KDD \u201920: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 1192\u20131200. ACM, 2020.\\n\\n[26] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou. LayoutLMv2: Multi-modal pre-training for visually-rich document understanding. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2579\u20132591, Online, 2021. Association for Computational Linguistics.\\n\\n[27] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for document ai with unified text and image masking. In Proceedings of the 30th ACM International Conference on Multimedia, MM \u201922, page 4083\u20134091, New York, NY, USA, 2022. Association for Computing Machinery.\\n\\n[28] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision (ECCV), 2022.\\n\\n[29] Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: screenshot parsing as pretraining for visual language understanding. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023.\\n\\n[30] Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long language sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 12007\u201312021, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics.\\n\\n[31] Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-eval: Instituting standardized evaluation for long context language models, 2023.\\n\\n[32] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: A bilingual, multitask benchmark for long context understanding. ArXiv preprint, abs/2308.14508, 2023.\\n\\n[33] Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. \u221ebench: Extending long context evaluation beyond 100k tokens, 2024.\\n\\n[34] Szymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Milo\u2019s. Focused transformer: Contrastive training for context scaling. ArXiv preprint, abs/2307.03170, 2023.\\n\\n[35] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. ArXiv preprint, abs/2309.12307, 2023.\"}"]}
{"id": "loJM1acwzf", "page_num": 13, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[36] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. *ArXiv preprint*, abs/2309.00071, 2023.\\n\\n[37] Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. LongAlign: A recipe for long context alignment of large language models. *ArXiv preprint*, abs/2401.18058, 2024.\\n\\n[38] Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang. Milebench: Benchmarking mllms in long context. *ArXiv preprint*, abs/2404.18532, 2024.\\n\\n[39] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning, 2024.\\n\\n[40] Yujie Lu, Xiujun Li, Tsu-Jui Fu, Miguel Eckstein, and William Yang Wang. From text to pixel: Advancing long-context understanding in mllms, 2024.\\n\\n[41] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. MMbench: Is your multi-modal model an all-around player? *ArXiv preprint*, abs/2307.06281, 2023.\\n\\n[42] Ray Smith. An overview of the tesseract ocr engine. In *ICDAR*, 2007.\\n\\n[43] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b, 2023.\\n\\n[44] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mixtral of experts, 2024.\\n\\n[45] Qwen Team. Introducing qwen1.5, 2024.\\n\\n[46] DeepSeek-AI. DeepSeek-V2: A strong, economical, and efficient mixture-of-experts language model, 2024.\\n\\n[47] OpenAI. GPT-4 technical report, 2024.\\n\\n[48] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. DeepSeek-VL: towards real-world vision-language understanding. *ArXiv preprint*, abs/2403.05525, 2024.\\n\\n[49] Hugo Lauren\u00e7on, L\u00e9o Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?, 2024.\\n\\n[50] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. RLAIF-V: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. *ArXiv preprint*, abs/2405.17220, 2024.\\n\\n[51] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, and Gao Huang. LLaVA-UHD: an lmm perceiving any aspect ratio and high-resolution images. *ArXiv preprint*, abs/2403.11703, 2024.\\n\\n[52] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, et al. mPLUG-DocOwl 1.5: Unified structure learning for ocr-free document understanding. *ArXiv preprint*, abs/2403.12895, 2024.\\n\\n[53] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: A frontier large vision-language model with versatile abilities. *ArXiv preprint*, abs/2308.12966, 2023.\\n\\n[54] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. *ArXiv preprint*, abs/2311.06607, 2023.\\n\\n[55] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. *ArXiv preprint*, abs/2312.13286, 2023.\"}"]}
{"id": "loJM1acwzf", "page_num": 14, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[56] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In *International Conference on Learning Representations (ICLR)*, 2024.\\n\\n[57] Tomasz Stanislawek, Filip Grali\u0144ski, Anna Wr\u00f3blewska, Dawid Lipi\u0144ski, Agnieszka Kaliska, Paulina Rosalska, Bartosz Topolski, and P. Biecek. Kleister: Key information extraction datasets involving long documents with complex layouts. In *IEEE International Conference on Document Analysis and Recognition*, 2021.\\n\\n[58] S. Svetlichnaya. DeepForm: Understand structured documents at scale., 2020.\\n\\n[59] Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. Funsd: A dataset for form understanding in noisy scanned documents. *2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)*, 2:1\u20136, 2019.\\n\\n[60] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and C. V. Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. *2019 International Conference on Document Analysis and Recognition (ICDAR)*, pages 1516\u20131520, 2019.\\n\\n[61] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension. In *2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 5376\u20135384, 2017.\\n\\n[62] Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In *The IEEE Winter Conference on Applications of Computer Vision (WACV)*, March 2020.\\n\\n[63] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. Visualmrc: Machine reading comprehension on document images. *ArXiv*, abs/2101.11272, 2021.\\n\\n[64] Xingyu Chen, Zihan Zhao, Lu Chen, JiaBao Ji, Danyang Zhang, Ao Luo, Yuxuan Xiong, and Kai Yu. WebSRC: A dataset for web-based structural reading comprehension. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages 4173\u20134185, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\"}"]}
{"id": "loJM1acwzf", "page_num": 15, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Benchmark Construction Details\\n\\nA.1 Existing Document Collection\\n\\nAlthough previous datasets contain a relatively small proportion of lengthy documents, their absolute quantity should not be disregarded. Therefore, we compile lengthy documents from various datasets to include them as part of the documents in this benchmark. Specifically, we review and consider 21 previous document understanding (DU) datasets, and ultimately select 4 of them for further document selection. The selection reasons are shown in Table 4. All of these four datasets are licensed under the Creative Commons license (CC-BY) or other open-source licenses. Regarding the 4 selected datasets: DUDE [17], SlideVQA [18], ChartQA [13] and FinanceBench [19], we collect a total of 76 documents and detail our collection procedures as below.\\n\\nTable 4: Comparison of selected and considered datasets for our benchmark.\\n\\n| Dataset          | Selected | Comment                                                                 |\\n|------------------|----------|-------------------------------------------------------------------------|\\n| DUDE [17]        | \u2713        |                                                                          |\\n| SlideVQA [18]    | \u2713        |                                                                          |\\n| ChartQA [13]     | \u2713        |                                                                          |\\n| FinanceBench [19]| \u2713        |                                                                          |\\n| DocVQA [12]      | \u2717        | Repetitive with some documents/questions in DUDE; Single-page documents only |\\n| MP-DocVQA [16]   | \u2717        | Repetitive with some documents/questions in DUDE; Single-page questions only |\\n| Kleister Charity [57]| \u2717    | Repetitive with some documents/questions in DUDE; Over-simple            |\\n| Kleister NDA [57]| \u2717        | Repetitive with some documents/questions in DUDE; Over-simple            |\\n| DeepForm [58]    | \u2717        | Repetitive with some documents/questions in DUDE; Over-simple            |\\n| FUNSD [59]       | \u2717        | Repetitive with some documents/questions in DUDE; Over-simple            |\\n| SROIE [60]       | \u2717        | Repetitive with some documents/questions in DUDE; Over-simple            |\\n| Infographics VQA [14]| \u2717    | Infographs are not long-context documents                                |\\n| TAT-QA [15]      | \u2717        | Repetitive with some documents/questions in FinanceBench                 |\\n| PWC [22]         | \u2717        | Repetitive with our self-annotated questions from academic papers        |\\n| PaperQA [56]     | \u2717        | Repetitive with our self-annotated questions from academic papers        |\\n| TextbookQA [61]  | \u2717        | Low document-relevance; Over-simple                                     |\\n| PlotQA [62]      | \u2717        | Repetitive with our self-annotated questions from academic papers        |\\n| VisualMRC [63]   | \u2717        | Human performance reached; Website screenshots are not long-context documents |\\n| WebSRC [64]      | \u2717        | Human performance reached; Website screenshots are not long-context documents |\\n| VisualWebBench [21]| \u2717    | Human performance reached; Website screenshots are not long-context documents |\\n| PDFTriage [23]   | \u2717        | Not publicly available                                                  |\\n\\nDUDE: We first filter all documents over 15 pages in the validation set of the original dataset, resulting in 87 documents. From these, we randomly sample 23 to include as a component of our benchmark documents.\\n\\nSlideVQA: We download slide decks in the test set by following the instructions in the original repository [6]. Pursuing lengthy documents, we slightly modified the code to remove the 20-page truncation procedure. Then we randomly select 27 slide decks for our benchmark documents.\\n\\nFinanceBench: We randomly sample 5 financial reports from the test set.\\n\\nChartQA: Different from the above three datasets, ChartQA only contains chart screenshots cropped from documents. We take the following steps to recover these original documents: (1) We use the Tesseract OCR model [42] to recognize the text within the charts. (2) We use these texts as keywords to search for related documents on Google Search. (3) We manually identify these documents and remove all those that are less than 15 pages. From the ChartQA test set, we finalize a collection of 53 research reports from the Pew Research Center. We randomly sample 18 of these documents to include as a component of our benchmark documents.\\n\\n---\\n\\n[6]https://github.com/nttmdlab-nlp/SlideVQA\"}"]}
{"id": "loJM1acwzf", "page_num": 16, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.2 Newly-annotated Document Collection\\n\\nMost documents collected from previous datasets are *Industrial Files, Tutorial & Workshop, Finance Report* and *Research Report*. To diversify our benchmark, we additionally collect 59 documents including *Academic Paper, Brochure*, and *Guideline*. We detail the collection procedures as below.\\n\\n**Academic Paper** We collect 24 academic papers from Arxiv. All selected papers are over 15 pages (including references and appendix). To ensure annotation quality, each paper is either written or thoroughly read by at least one of the annotators.\\n\\n**Guideline and Brochure** We collect 21 guidelines and 14 brochures from either ManualsLib or Google Search, covering diverse topics such as school, company, institution, products, service *etc.*. Each document is manually reviewed by one corresponding annotator and other primary authors to ensure its availability for academic use.\\n\\nA.3 Document Examples\\n\\nAs stated in Section 2.1, the documents in MMLONGBENCH-DOC can be categorized into seven types. We show the examples of each type as below.\\n\\n![Figure 8: Document example about Administration & Industrial File](image)\\n\\n![Figure 9: Document example about Tutorial & Workshop (only show first 50 pages)](image)\\n\\n---\\n\\n7 Should any authors request the removal of their documents, we will promptly comply.\"}"]}
{"id": "loJM1acwzf", "page_num": 17, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: Document example about Research Report\\n\\nFigure 11: Document example about Financial Report (only show first 50 pages)\"}"]}
{"id": "loJM1acwzf", "page_num": 18, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":null}"]}
{"id": "loJM1acwzf", "page_num": 19, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.4 Existing Question Editing\\n\\nDocuments collected from existing datasets had been annotated with some questions and answers. However, their crowd-sourcing annotations inevitably make some questions, answers, and other meta information unqualified. So we conduct a systematic and manual pipeline to edit their annotations. Specifically, we classify six potential problems in original annotations. The definitions and examples of these problems are shown below.\\n\\n1. **Wrong Answer or Evidence Pages**: The reference answers and/or evidence pages in original datasets are wrongly annotated.\\n\\n![Figure 15: Example of the original annotation with Wrong Answer or Evidence Pages.](image)\\n\\n**Original Question:**\\nWhy is the service not safe?\\n\\n**Original Answer:**\\nMedicines were not managed safely.\\n\\n**Corrected Answer:**\\n1. Medicines were not managed safely\\n2. There were insufficient staff to care for people\u2019s needs during the evening and at night.\\n3. Some risks to people\u2019s health and wellbeing were not assessed and action was not taken to reduce the risk.\\n4. Safeguarding incidents were not investigated or reported appropriately\\n\\n**Error type:**\\nWrong Answer or Evidence\\n\\n**Comment:**\\nThe original answer only mentions single point of the safety problem and is incomplete: in fact, there are a total of four points.\"}"]}
{"id": "loJM1acwzf", "page_num": 20, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Repetitive Question: Too many questions with the same types (e.g., key information extraction) occur in a single document (or even on the same page or point).\\n\\nFigure 16: Example of the original annotation with Repetitive Question.\"}"]}
{"id": "loJM1acwzf", "page_num": 21, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. **Ambiguous Question**: The question is ambiguous at the document level (e.g., the absence of entity, period, exact section or page, *etc.*), or too broad to exactly answer.\\n\\n![Figure 17: Example of the original annotation with Ambiguous Question.](image)\\n\\n**Original Question**: What is the telephone no?\\n\\n**Original Answer**: 01983 873655\\n\\n**Error type**: Ambiguous Question\\n\\n**Comment**: The are two main entities occurred in this report: (1) The Limes Residential Home, and (2) Care Quality Commission. There is neither explicit statement nor the implicit coreference about any entity in this question, making it ambiguous.\\n\\n**Revised Question**: What is the telephone no for The Limes Residential Home?\\n\\n**Revised Answer**: 01983 873655\"}"]}
{"id": "loJM1acwzf", "page_num": 22, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"4. **Potential Shortcut:** The resolution of the question does not rely on two entities (across different pages) but only one of them, *i.e.*, there exists a shortcut for this question.\\n\\n**Original Question:**\\nWhy did the company which Mamoon Hamid is affiliated with invest $1M in the seed for greenhouse?\\n\\n**Original Answer:**\\nStrong conviction in team, market and early customer validation.\\n\\n**Error Type:**\\nPotential Shortcut\\n\\n**Comment:**\\nThe coreference of Adventure Capital circled in white in the left slide, *i.e.*, the company which Mammon Hamid is affiliated with, makes no sense for answering this question. It is because that the words circled in blue in the right slide, *i.e.*, invest $1M in the seed, is a potentially a strong shortcut for answering this question. Though seemingly relying on the information across two pages, it is still likely a single-page question.\\n\\n**Revised Question:**\\nWhy did greenhouse invest $1M in the seed for greenhouse?\\n\\n**Revised Answer:**\\nStrong conviction in team, market and early customer validation.\\n\\nFigure 18: Example of the original annotation with Potential Shortcut.\"}"]}
{"id": "loJM1acwzf", "page_num": 23, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"5. **Low Document-relevant Question:** The resolution of the question does not rely on the information from the document. It can be solved by the parametric knowledge in the LVLMs.\\n\\n![Map of Nepal](image)\\n\\n**Original Question:**\\nKailali is in which region of Nepal?\\n\\n**Original Answer:**\\nFar-western region\\n\\n**Error type:**\\nLow document-relevant Question\\n\\n**Comment:**\\nThis question is originally intended to evaluate the model\u2019s understanding ability on this map. However, the development of LVLMs makes this question not relying on the information of this document anymore (can be answered by model\u2019s parametric knowledge).\\n\\n**Revised Question:**\\nWhat is the color of Kailali in the map of Page 12?\\n\\n**Revised Answer:**\\nYellow\\n\\nFigure 19: Example of the original annotation with *Low Document-relevant Question*. \"}"]}
{"id": "loJM1acwzf", "page_num": 24, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. **Decontextulization-required Question**: The understanding of the question is conditioned on a single page or even a single component of the document.\\n\\n![Figure 20: Example of the original annotation with Decontextulization-required Question.](image)\\n\\nWhen dealing with questions categorized under any of these six problem types, annotators are instructed to either revise or remove them. Typically, repetitive questions and those with potential shortcuts are removed. In contrast, wrongly-annotated or decontextualization-required questions are generally revised. For ambiguous and low document-relevant questions, the course of action depends more on the annotators\u2019 discretion.\"}"]}
{"id": "loJM1acwzf", "page_num": 25, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.5 New Question Annotation\\n\\nWe annotate new questions on both existing and newly-collected documents. To ensure a diverse range of questions, we impose limitations on the question distributions categorized by their types (i.e., single-page, cross-page or unanswerable) and evidence sources (i.e., table, chart, image). To balance existing questions which are mostly single-page and text-based, we place greater emphasis on cross-page, unanswerable, table-related, chart-related, and image-related questions. The detailed standards are as follows:\\n\\n| Document Type       | Evidence Page | Evidence Source | All |\\n|---------------------|---------------|-----------------|-----|\\n|                     | Cross-page    | Unanswerable    | Table | Chart | Image |     |\\n| Industrial File     | \u2265 2           | -               | -    | -     | -     | \u2265 3 |\\n| Workshop & Tutorial | \u2265 2           | \u2265 1             | -    | -     | -     | \u2265 6 |\\n| Research Report     | \u2265 3           | \u2265 1             | \u2265 2  | \u2265 2   | -     | \u2265 5 |\\n| Financial Report    | \u2265 5           | \u2265 2             | \u2265 7  | -     | -     | \u2265 10|\\n| Academic Paper      | \u2265 3           | \u2265 1             | \u2265 2  | \u2014     | \u2265 3   | \u2265 6 |\\n| Guidebook           | \u2265 3           | \u2265 1             | -    | -     | \u2265 4   | \u2265 7 |\\n| Brochure            | \u2265 2           | \u2265 1             | -    | -     | \u2265 3   | \u2265 7 |\\n\\nTable 5: The minimum requirements for the number and distribution of questions, categorized by the evidence page numbers and evidence sources. We have set varying requirements for different document types based on their specific characteristics.\\n\\nA.6 Potential Bias for LVLM-based Quality Checking\\n\\nAs described in Section 3.3, we employ GPT-4o to remove document-agnostic (i.e., can be correctly answer without documents) samples and review potential wrongly-labeled samples. A reasonable speculation raises that our final benchmark can be biased toward GPT-4o\u2019s answers, especially when GPT-4o outperforms others by a large margin. We discuss this potential bias as follows.\\n\\nWe check the effect of GPT-4o\u2019s involvement in the quality control step-by-step. Specifically, we compare the performance of samples remained after each step across GPT-4o and two other competitive models (GPT-4V and Gemini-1.5-Pro). We show their results in the table below.\\n\\n| Step Description                                      | GPT-4o | GPT-4V | Gemini-1.5-Pro |\\n|-------------------------------------------------------|--------|--------|---------------|\\n| No quality control                                    | 43.1%  | 35.2%  | 23.3%         |\\n| + document-relevance detection                        | 41.2%  | 31.0%  | 20.5%         |\\n| + document-relevance detection + self-reflection / cross-checking | 42.7%  | 31.4%  | 20.9%         |\\n\\nTable 6: Step-wise performance comparison with and without LVLM-based quality checking\\n\\nThe results illustrate that the potential bias in step 1 (document-relevance detection) actually reduce, rather than increase, the performance gap between GPT4o and other models. It is because that we filter out all samples correctly answered by GPT4o without the access to documents. Under this case, the more significant performance drop of GPT-4V and Gemini-1.5-Pro can only be attributed to their limited document understanding and over-reliance on their internal knowledge. Regarding the step 2 and 3 (self-reflection and cross-checking), we provide inconsistent answers between human annotations and GPT4o\u2019s predictions to annotators and ask them to check and revise accordingly. The potential bias of this step does lead to a slight performance bias (1.1% absolute difference at maximum). We believe that such bias is NOT the main cause of GPT4o\u2019s significantly best performance. Without the involvement of GPT-4o in the quality control process, GPT-4o still significantly outperforms GPT-4V by 7.9% (43.1% - 35.2%) and Gemini-1.5-Pro by 19.8% (43.1% - 23.3%). Accordingly, all primary conclusions in our paper still hold.\"}"]}
{"id": "loJM1acwzf", "page_num": 26, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.7 GUI Screenshots\\n\\nWe present the screenshots for editing existing questions and annotating new questions (along with their reference answers and meta-data) in Figure 21 and Figure 22 respectively.\\n\\n![Figure 21: GUI screenshot for editing existing questions (along with reference answers and meta-data)](image1)\\n\\n![Figure 22: GUI screenshot for annotating new questions (along with reference answers and meta-data)](image2)\\n\\nA.8 Annotation Cost\\n\\nThis benchmark is annotated by the authors of this paper. Therefore, the data collection does not need compensation. And we count the time cost of our benchmark as below.\\n\\n**Pre-annotation** (about 45h): the development of annotation interface (10h), the writing of annotation guideline (5h), training session (10h), preliminary annotation and personalized feedback (20h).\\n\\n**Annotation** (about 150h): It takes about 60-90 minutes for the annotation of each document. And all of the 130 documents take about 150 hours.\\n\\n**Post-annotation** (about 45h): quality checking (30h), data processing and release preparation (15h).\\n\\nIn summary, our benchmark annotation approximately takes a total of 45+150+45=240 hours (1.36 man months).\"}"]}
{"id": "loJM1acwzf", "page_num": 27, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B Experimental Details\\n\\nB.1 Prompt for Response Generation\\n\\nListing 1: Prompt used for response generation. The [Document] is in PNG format (page screenshots) for LVLMs, and TXT format for LLMs.\\n\\nRead the above documents and answer this question:\\n[question]\\n\\nPlease make your answer as concise as possible.\\n\\nB.2 Prompt for Answer Extraction\\n\\nListing 2: Prompt used for answer extraction.\\n\\nGiven the question and analysis, you are tasked to extract answers with required formats from the free-form analysis.\\n\\n- Your extracted answers should be one of the following formats: (1) Integer, (2) Float, (3) String and (4) List. If you find the analysis the question can not be answered from the given documents, type \\\"Not answerable\\\". Exception: If the analysis only tells you that it can not read/understand the images or documents, type \\\"Fail to answer\\\".\\n\\n- Please make your response as concise as possible. Also note that your response should be formatted as below:\\n\\n```\\nExtracted answer: [answer]\\nAnswer format: [answer format]\\n```\\n\\nPlease read the following example, then extract the answer from the model response and type it at the end of the prompt.\\n\\n---\\n\\nQuestion: List the primary questions asked about the services in this report.\\nAnalysis: The primary questions asked about the services in the report for The Limes Residential Home are:\\n1. Is the service safe?\\n2. Is the service effective?\\n3. Is the service caring?\\n4. Is the service responsive?\\n5. Is the service well-led?\\n\\nExtracted answer: ['Is the service safe?', 'Is the service effective', 'Is the service caring?', 'Is the service responsive?', 'Is the service well-led?']\\nAnswer format: List\\n\\n---\\n\\nQuestion: How many regulations of the HSCA 2008 are breached in all according to this report?\\nAnalysis: According to the report, the provider breached 10 Health and Social Care Act 2008 (Regulated Activities) Regulations in total. Here are the specifics:\\n\\n1. Regulation 13: Safeguarding service users from abuse and improper treatment\\n2. Regulation 12: Safe care and treatment\\n3. Regulation 18: Staffing\\n4. Regulation 11: Need for consent\\n5. Regulation 10: Dignity and respect\\n6. Regulation 9: Person-centred care\\n7. Regulation 17: Good governance\\n8. Regulation 18 (CQC Registration Regulations 2009): Notification of other incidents\\n9. Regulation 18: Failure to maintain an accurate and up-to-date care plan\"}"]}
{"id": "loJM1acwzf", "page_num": 28, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"10. Regulation 11: Failure to implement the Mental Capacity Act 2005 code of practice effectively\\n\\nThese breaches involve issues concerning staffing, safeguarding, medicines management, dignity and respect, consent, care planning, governance, and failure to notify the CQC of incidents.\\n\\nExtracted answer: 10\\nAnswer format: Integer\\n\\n---\\n\\nQuestion: According to the survey that is the percentage of Chinese who are paying more or about the same attention to politics after Trump\u2019s election?\\nAnalysis: The survey provided does not specify the percentage of Chinese individuals specifically who are paying more or about the same attention to politics after Trump\u2019s election. The report focuses primarily on American demographics and does not include specific details about the Chinese population in relation to this question. If you need information about a different demographic or a summary of the findings from the American demographic, I can certainly help with that!\\n\\nExtracted answer: Not answerable\\nAnswer format: None\\n\\n---\\n\\nQuestion: How many quotations from male respondent over 50 years old are included in this report?\\nAnalysis: The image you\u2019ve provided appears to be a screenshot of a document with multiple charts. However, the text is too small and blurry to read accurately. If you can provide a clearer image or more context, I might be able to help you with your question.\\n\\nExtracted answer: Fail to answer\\nAnswer format: None\\n\\n---\\n\\nB.3 Rules for Score Calculation\\n\\nWe evaluate the model\u2019s responses by scoring the extracted answers against the reference answers. The scorer is rule-based and employs different strategies according to the format of the reference answer. We detail its rules as below:\\n\\n**String:** We firstly use a series of regular expressions to determine whether the answers require exact matching (e.g., telephone numbers, email addresses, website addresses, file names, times, dates, etc.). If an exact match is needed, we perform a straightforward string comparison and score the answer either 0 or 1. Otherwise, we follow previous work [17] and calculate the ANLS (Average Normalized Levenshtein Similarity) with a pre-defined threshold ($\\\\tau = 0.5$).\\n\\n**Integer:** We perform an exact match comparison and score the answer either 0 or 1.\\n\\n**Float:** We view the prediction and reference answers as equal if they fall within a 1% relative tolerance.\\n\\n**List:** We adopt a relatively strict rule for scoring answers in list format: predictions that do not have the same number of elements as the reference receive a score of 0. For the remaining predictions, as Eq. [1] indicates, we score each element in order and use the minimum element-wise score as the score for the entire list. The element-wise scoring strategies is determined by the formats of elements (i.e., string, integer or float).\\n\\n\\\\[\\n\\\\text{pred_list, ref_list} = \\\\text{sorted(pred_list), sorted(ref_list)}\\n\\\\]\\n\\\\[\\n\\\\text{Score(pred_list, ref_list)} = \\\\min(\\\\text{[Score(pred, ref) for pred, ref in zip(pred_list, ref_list)]})\\n\\\\] (1)\"}"]}
{"id": "loJM1acwzf", "page_num": 29, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluation detailed in the Appendix B.4 shows that while this scorer is not perfect, it aligns well with human judgment. We will continue refining these rules to cover more corner cases and enhance their accuracy.\\n\\n### B.4 Human Evaluation on the Automatic Evaluation Pipeline\\n\\nWe conduct human evaluations to assess the performance of our automatic evaluation pipeline, which includes the answer extractor and the score calculator. Specifically, we randomly select 100 questions and review their responses from two representative LVLMs: GPT-4o and Gemini-1.5-Pro. We manually evaluate the correctness of each response and compare the results between human evaluation and automatic evaluation. The performance, as shown in Table 7, indicates a high correlation between human judgment and our automatic pipeline.\\n\\n| Model       | Inconsistent Evaluation |\\n|-------------|-------------------------|\\n|             | Ans. Extractor | Scorer | Overall |\\n| GPT-4o      | 4             | 2      | 6       |\\n| Gemini-1.5-Pro | 2             | 2      | 4       |\\n\\nTable 7: We manually check 100 responses from GPT-4o and Gemini-1.5-Pro, and compare the evaluation results between humans and our automatic pipeline.\\n\\n### B.5 Model Hyperparameters\\n\\nThe hyperparameters of used LVLMs and LLMs in Section 3.3 are detailed in Table 8. The temperature is set as 0.0, and the max_new_tokens is set as 1024 for all the models. The \u2018concatenated_images\u2019 parameter determines the maximum number of images that can be combined into a single input for LVLMs. By concatenating multiple images, we can meet the minimum context window requirements. The \u2018max_pages\u2019 parameter specifies the maximum number of images that can be directly input into the LVLMs without concatenation.\\n\\n| Model                  | Hyperparameters                                      |\\n|------------------------|------------------------------------------------------|\\n| **LLM**                |                                                      |\\n| ChatGLM-128k           | max_input_words=60000                                |\\n| Mistral-Instruct-v0.2-7B | max_input_words=20000                               |\\n| Mixtral-Instruct-v0.1-8x7B | max_input_words=20000                               |\\n| Mixtral-Instruct-v0.1-8x22B | max_input_words=40000                               |\\n| QWen-Plus              | max_input_words=16000                                |\\n| DeepSeek-V2            | max_input_words=20000                                |\\n| **LVLM**               |                                                      |\\n| DeepSeek-VL-Chat       | concatenated_images=5                                |\\n| Qwen-VL-Chat           | concatenated_images=5                                |\\n| Idefics2               | concatenated_images=5                                |\\n| MiniCPM-Llama3-V2.5    | concatenated_images=2                                |\\n| InternLM-XC2-4KHD      | concatenated_images=2                                |\\n| Monkey-Chat            | concatenated_images=1                                |\\n| CogVLM2-Llama3-Chat    | concatenated_images=1                                |\\n| InternVL-Chat-v1.5     | concatenated_images=5                                |\\n| EMU2-Chat              | concatenated_images=5                                |\\n| **LLM & LVLM**         |                                                      |\\n| Claude-3 Opus          | version=claude-3-opus-20240229, concatenated_images=20 |\\n| Gemini-1.5-Pro         | max_pages=120, version=gemini-1.5-pro-latest         |\\n| GPT-4-turbo            | max_pages=120, version=gpt-4-turbo-2024-04-09        |\\n| GPT-4o                 | max_pages=120, version=gpt-4o-2024-05-13             |\\n\\nTable 8: Model Hyperparameters\"}"]}
{"id": "loJM1acwzf", "page_num": 30, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C Qualitative Study\\n\\nC.1 Error Analysis\\n\\nWe delve into the analysis of error by GPT-4o to further understand its bottlenecks and potentials on long-context document understanding. We manually check 72 incorrect responses and categorized their error reasons into 7 types. Except for the Extraction Error caused by our automatic evaluation pipeline (see Appendix B.4), we detail and showcase another six reasons as below:\\n\\n**Perceptual Error:** GPT-4o sometimes struggles to extract or understand visual information from document screenshots. For instance, it misinterprets the axes and colored circles in the charts shown in Figure 23. Additionally, it inaccurately counts the number of green bars in Figure 24. They demonstrate that even the cutting-edge LVLMs still fall short in fundamental perceptual capabilities.\\n\\n**Incomplete Evidence:** Though GPT-4o has achieved significantly better global searching abilities compared to other models when dealing with lengthy, multi-modal documents, it sometimes still omits certain information. For example, GPT-4o misses one chapter author from Columbia University in the full list (Figure 25). Additionally, it overlooks an app that appears across two pages (Figure 26).\\n\\n**Hallucinated Evidence:** As stated in Section 3.4, GPT-4o adopts more aggressive strategies and tends to provide more false-positive answers. It sometimes even fabricates non-existent evidence in documents to support its incorrect responses. For example, it references a non-existent page in Figure 27 and fabricates the content of a page in Figure 28. The above examples clearly reveal the importance of further research on LVLMs\u2019 hallucination and safety.\\n\\n**Knowledge Lacking:** Resolving certain questions requires both information from the documents and the parametric knowledge within LVLMs. We have observed error cases stemming from the absence of specific knowledge. For example, GPT-4o overlooks details about the fixed asset turnover ratio and uses the single-point value instead of the average value to calculate this metric (Figure 29). Additionally, it misidentifies buildings at Tsinghua University in Figure 30.\\n\\n**Reasoning Error:** Though not a primary cause, flawed reasoning based on correctly collected evidence and information from documents can sometimes lead to wrong answers. For example, GPT-4o correctly gathers all data but calculates a relative percentage instead of an absolute percentage in Figure 31. Additionally, as shown in Figure 32, it correctly lists all quizzes but inaccurately counts them in the final step.\\n\\n**Irrelevant Answer:** GPT-4o sometimes misunderstands the intent of questions and provides irrelevant responses. For instance, in Figure 33, GPT-4o answers about button operations when the question asks about button functions. Similarly, in Figure 34, where the question asks for the MOST discrimination type, GPT-4o summarizes all types instead.\"}"]}
{"id": "loJM1acwzf", "page_num": 31, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Perceptual Error: Case 1\\n\\nviews toward the U.S. found in Pew Research Center\u2019s 2019 Global Attitudes survey, especially among people who place themselves on the ideological right in Germany, even as favorable opinions of the U.S. remain low.\\n\\nDespite these divergences in opinion, young people in both countries have more positive views of the U.S.-German relationship. In the U.S., for example, 62% of people ages 18 to 29 say the relationship is good, compared with 75% of those ages 65 and older. Similarly, in Germany, four-in-ten young people say relations with the U.S. are good, compared with only 31% of those 65 and older.\\n\\nThese are among the major findings from a Pew Research Center survey of 1,004 adults conducted in the U.S. from Sept. 17-22, 2019, and a K\u00f6rber-Stiftung survey of 1,000 adults conducted in Germany from Sept. 9-28, 2019. This analysis also includes results from Pew Research Center\u2019s Spring 2019 Global Attitudes survey, conducted among 1,503 adults in the U.S. from May 13-June 18, 2019, and 2,015 adults in Germany from May 31-July 25, 2019.\\n\\n**Error Type:** Perceptual Error\\n\\n**Analysis:** The model wrongly identifies the percentages of people aged 30-49 who have a positive view of the U.S.-Germany relationship from the chart. It should be 35% and 72%, rather than 31% and 40%.\\n\\n**Question:** Among people who age 30-49, what is the difference of percentage value between Americans and Germen having positive view on their bilateral relationship?\\n\\n**Response:** The difference in the percentage of people aged 30-49 who have a positive view of the U.S.-Germany relationship between Americans and Germans is 9% points. According to data on page 5, 40% of Americans aged 30-49 view the relationship positively, while only 31% of Germans in the same age group share this view.\\n\\nFigure 23: Error example about Perceptual Error\"}"]}
{"id": "loJM1acwzf", "page_num": 32, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Perceptual Error: Case 2\\n\\nQuestion: How many green bars appear in Figure 1?\\nResponse: In Figure 1, there are four green bars.\\n\\nError Type: Perceptual Error\\nAnalysis: The model wrongly counts 4 green bars in Figure 1. Obviously, there are 6 green bars.\\n\\nFigure 24: Error example about Perceptual Error\"}"]}
{"id": "loJM1acwzf", "page_num": 33, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: How many chapter authors of this report come from Columbia University?\\n\\nResponse: The report lists the chapter authors on page xii. From Columbia University, the chapter authors are: 1. Radley Horton, 2. Patrick Kinney. Therefore, there are two chapter authors from Columbia University.\\n\\nError Type: Incomplete Evidence\\n\\nAnalysis: The model only finds out 2 of 3 chapter authors from Columbia University, leaving out Garcia-Pando shown in the top-left of Page 15.\"}"]}
{"id": "loJM1acwzf", "page_num": 34, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Question: What are the overlapped apps between page 21 and 62?\\nResponse: The overlapped apps between page 21 and page 62 are:\\n- LinkedIn\\n- SlideShare\\n\\nError Type: Incomplete Evidence\\nAnalysis: The model correctly identifies the two shared apps (LinkedIn and SlideShare) between two pages. But it leaves out another one app: Podcast.\\n\\nFigure 26: Error example about Incomplete Evidence\"}"]}
{"id": "loJM1acwzf", "page_num": 35, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"**Hallucinated Evidence: Case 1**\\n\\n**Question:** How many sampled questions illustrated in this slide?\\n\\n**Response:** There are five sample questions illustrated in this slide deck. They are shown on the following slides:\\n\\n1. Slide 10: \\\"The following search string will produce which results according to the Venn diagram?\\\"\\n2. Slide 11: \\\"A role you were previously recruiting for just re-opened. Which Talent Pipeline feature should you utilize to source talent already in your pipeline?\\\"\\n3. Slide 12: \\\"Based on the report below, which recruiter should you go to for advice on InMail best practices?\\\"\\n4. Slide 13: \\\"You recruit for Retail Bankers in high volume. Which efficiency tool(s) will help you constantly uncover new leads?\\\"\\n5. Slide 14: \\\"What happens to my certification with the rollout of Next-Gen Recruiter?\\\"\\n\\n**Error Type:** Hallucinated Evidence\\n\\n**Analysis:** The model totally fabricates the contents of Slide 14 (Page 14). This slide is not about sampled question.\\n\\nFigure 27: Error example about *Hallucinated Evidence*\"}"]}
{"id": "loJM1acwzf", "page_num": 36, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hallucinated Evidence: Case 2\\n\\nQuestion: Is there blue color handwritten words in page 30?\\nResponse: Yes, there are blue handwritten words on page 30.\\n\\nError Type: Hallucinated Evidence\\nAnalysis: This document has only 20 pages. There is no \u201cso-called\u201d page 30 in the response.\\n\\nFigure 28: Error example about Hallucinated Evidence\"}"]}
{"id": "loJM1acwzf", "page_num": 37, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Question:** What is the FY2019 fixed asset turnover ratio for Activision Blizzard?\\n\\n**Response:** The fixed asset turnover ratio is calculated by dividing net sales by net fixed assets. The formula is: Fixed Asset Turnover Ratio = Net Sales / Net Fixed Assets\\n\\nFrom the provided document, we can find the necessary values for the fiscal year 2019:\\n\\n- Net Sales (Net Revenues) for 2019: $6,489 million\\n- Net Fixed Assets (Property and Equipment, net) at the end of 2019: $253 million\\n\\nUsing these values, the fixed asset turnover ratio is:\\n\\n$$\\\\frac{6,489}{253} \\\\approx 25.65$$\\n\\nTherefore, the fixed asset turnover ratio for Activision Blizzard for the fiscal year 2019 is approximately 25.65.\\n\\n**Error Type:** Knowledge Lacking\\n\\n**Analysis:** The model uses a wrong formula to calculate the financial metric. To calculate the Fixed Asset Turnover Ratio, the denominator should be the average value of net fixed assets in the past two years, instead of the value of a single year.\"}"]}
{"id": "loJM1acwzf", "page_num": 38, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Knowledge Lacking: Case 2\\n\\nQuestion: How many figures in this document show the old gate of Tsinghua (\\\"Er Xiao Men\\\" in Chinese)?\\nResponse: The document contains two figures showing the old gate of Tsinghua (\\\"Er Xiao Men\\\" in Chinese). The first one is on page 2, and the second one is on page 27\\n\\nError Type: Knowledge Lacking\\nAnalysis: Though it has similar appearance, the building shown in Page 27 (i.e. the right figure) is not \u201cEr Xiao Men\u201d but \u201cQing Hua Xue Tang\u201d.\\n\\nFigure 30: Error example about Knowledge Lacking\"}"]}
{"id": "loJM1acwzf", "page_num": 39, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Reasoning Error: Case 1\\n\\nFigure 6: The overall architecture of our adaptive filter-then-rerank paradigm. We color easy samples in orange and hard samples in pink. For easy samples, the final predictions are exactly from the SLM-based methods. For hard samples, the top-N predictions from SLMs are fed into LLMs as the input of multiple-choice questions (pink box). The question is paired with demos (green box). LLMs rerank these N candidates and generate the final prediction.\\n\\nfilter, and Vicuna-13B, InstructGPT or GPT-4 as the reranker. The threshold \u03c4 to determine sample difficulty is optimized on the valid set. For hard sample, the top-3 SLM predictions and none (if not included) are fed to LLMs for reranking. Each LLM prompt has 4-shot demos. See demo examples in Appendix G.1. We follow templates in Lu et al. (2022a) for TACREV and carefully design others. See these templates in Appendix G.2. We adopt chain-of-thought reasoning (Wet et al., 2022b), i.e., prefacing the answer with an explanation, to facilitate LLMs\u2019 reranking procedure.\\n\\nBaseline We compare our method with two kinds of baselines to validate its effectiveness.\\n\\n(1) LLMs with ICL: We follow the prompts in Section 3.3 and conduct experiments on three LLMs. (2) Supervised SLMs: We follow previous SoTA methods shown in Section 3.4 (FSL-S or Know-Prompt). We additionally combine two SLMs with ensemble or reranking approach (i.e., replace the LLM with another SLM as the reranker) to verify that improvements from our SLM-LLM integrated system are not solely due to the ensemble effects.\\n\\n5.3 Main Results\\n\\nTable 3 shows that our filter-then-rerank method consistently improves performance across three datasets and nine settings. For instance, with InstructGPT, reranking provides an average F1 gain of 2.4% without SLM ensemble (Lines 4 vs. 7). Based on ensemble SLMs as the filter, our method still achieves 2.1% (Lines 5 vs. 8) gains on average. This confirms (1) the effectiveness of the LLM reranking and (2) its gains are different and (almost) orthogonal to the SLM ensemble.\\n\\n5.4 Analysis\\n\\nFew makes big difference Our method selectively reranks hard samples. Table 4 shows that (1) only a minor fraction (0.5%\u201310%) of samples are deemed hard and are reranked by LLMs. (2) Despite their limited quantity, reranking results in a substantial performance boost on these samples (10%\u201325% absolute F1 gains). This split on a small subset significantly enhances the overall performance.\\n\\nGPT-4 is more aggressive From Tables 3 and 4, GPT-4 generally improves more on hard samples, yet InstructGPT surpasses GPT-4 in NER and RE tasks when evaluated overall. This discrepancy arises from GPT-4\u2019s aggressive reranking which introduces more true positives, InstructGPT, however, focuses more on reducing false positives.\\n\\nFew makes small cost Figure 7 demonstrates that our method impressively reduces budget and latency by approximately 80%\u201390% compared to direct ICL. This reduction is due to (1) fewer LLM callings (only for hard samples) and (2) shorter prompts (fewer candidate labels and demos).\\n\\n5.5 Ablation Study\\n\\nWe investigate the effectiveness of the modules in adaptive filter-then-rerank system by removing each of them in turn: (1) CoT: We exclude the explanation for each examples in demo. (2) Demo:\\n\\nTable 3: Overall results of LLM-based ICL methods, SLM-based supervised methods, and our proposed filter-then-rerank (SLM-LLM) methods. The best results are in bold face and the second best are underlined. All results except InstructGPT and GPT-4 are averaged over 5 runs, and sample standard deviations are in the round bracket.\\n\\n| Method | F1 (NER) | TACREV (RE) | ACR (ED) |\\n|--------|----------|-------------|----------|\\n| 5-shot | 20-shot  | 100-shot    | 5-shot   | 20-shot  |\\n| CODEX  | 53.4\u00b10.5 | 54.6\u00b10.5 | 55.0\u00b10.5 | 53.4\u00b10.5 | 54.6\u00b10.5 | 55.0\u00b10.5 | 53.4\u00b10.5 | 54.6\u00b10.5 | 55.0\u00b10.5 |\\n| InteractGPT | 53.4\u00b10.5 | 54.6\u00b10.5 | 55.0\u00b10.5 | 53.4\u00b10.5 | 54.6\u00b10.5 | 55.0\u00b10.5 | 53.4\u00b10.5 | 54.6\u00b10.5 | 55.0\u00b10.5 |\\n| GPT-4  | 53.4\u00b10.5 | 54.6\u00b10.5 | 55.0\u00b10.5 | 53.4\u00b10.5 | 54.6\u00b10.5 | 55.0\u00b10.5 | 53.4\u00b10.5 | 54.6\u00b10.5 | 55.0\u00b10.5 |\\n| Previous SLM + Rerank (S) | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 |\\n| Previous SLM + Rerank (L) | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 |\\n| InstructGPT | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 |\\n| GPT-4  | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 |\\n\\nWe remove all examples, rendering the reranking a zero-shot problem. (3) LF (label filtering): We retain all labels as candidate choices for reranking, instead of only the top-N labels from the SLMs. (4) AD (adaptive): We feed all samples, not just hard ones, to the LLMs.\\n\\nWe show their results in Table 5 and see that (1) Demos with explanations consistently enhance the reranking ability of LLMs across all datasets. (2) Demos without explanations also contribute to performance improvement. (3) Label filtering results in gains and notably reduces the demo length.\\n\\nTable 5: Ablation study on three datasets. The filter is ensemble SLMs and the reranker is GPT-4.\\n\\n| Method | F1 (NER) | TACREV (RE) | ACR (ED) |\\n|--------|----------|-------------|----------|\\n| 5-shot | 20-shot  | 100-shot    | 5-shot   | 20-shot  |\\n| CODEX  | 53.4\u00b10.5 | 54.6\u00b10.5 | 55.0\u00b10.5 | 53.4\u00b10.5 | 54.6\u00b10.5 | 55.0\u00b10.5 | 53.4\u00b10.5 | 54.6\u00b10.5 | 55.0\u00b10.5 |\\n| InteractGPT | 53.4\u00b10.5 | 54.6\u00b10.5 | 55.0\u00b10.5 | 53.4\u00b10.5 | 54.6\u00b10.5 | 55.0\u00b10.5 | 53.4\u00b10.5 | 54.6\u00b10.5 | 55.0\u00b10.5 |\\n| GPT-4  | 53.4\u00b10.5 | 54.6\u00b10.5 | 55.0\u00b10.5 | 53.4\u00b10.5 | 54.6\u00b10.5 | 55.0\u00b10.5 | 53.4\u00b10.5 | 54.6\u00b10.5 | 55.0\u00b10.5 |\\n| Previous SLM + Rerank (S) | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 |\\n| Previous SLM + Rerank (L) | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 |\\n| InstructGPT | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 |\\n| GPT-4  | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 | 59.4\u00b11.1 | 61.0\u00b11.1 | 61.0\u00b11.1 |\\n\\nFigure 7: The financial and time cost over 500 sentences. InstructGPT as the reranker.\\n\\n6 Conclusion\\n\\nThrough an extensive empirical study on nine datasets spanning four IE tasks, we find that LLMs, despite their superiority in extreme low-resource scenarios, are not effective few-shot information extractors in general. They struggle with ill-related prompts, have limited demonstration capacity, and incur high inference costs. However, LLMs significantly improve the performance on hard samples when combined with SLM. Building on these insights, we propose an adaptive filter-then-rerank paradigm to leverage the strengths of SLMs and LLMs and mitigate their limitations. This approach consistently achieves promising results, with an average 2.4% F1 gain across multiple few-shot IE tasks, while minimizing latency and budget costs.\\n\\n**Question:** If I drop the green module shown in Figure 6, what is the absolute percentage drop on 100-shot TACREV setting?\\n\\n**Response:** The absolute percentage drop on the 100-shot TACREV setting when the green module is dropped can be found in Table 4. For the 100-shot TACREV setting, the F1 score before reranking is 72.0, and after reranking, it is 74.3. The absolute percentage drop is calculated as follows:\\n\\n\\\\[\\n\\\\text{Absolute Percentage Drop} = \\\\left( \\\\frac{\\\\text{F1 score before reranking} - \\\\text{F1 score after reranking}}{\\\\text{F1 score after reranking}} \\\\right) \\\\times 100\\n\\\\]\\n\\nAbsolute Percentage Drop = ((74.3 - 72.0) / 74.3) \u00d7 100\\n\\nAbsolute Percentage Drop = (2.3 / 74.3) \u00d7 100\\n\\nAbsolute Percentage Drop \u2248 3.1%\\n\\nSo, the absolute percentage drop on the 100-shot TACREV setting when the green module is dropped is approximately 3.1%.\\n\\n**Error Type:** Reasoning Error\\n\\n**Analysis:** The formula listed in model response is the relative percentage drop, instead of the absolute percentage drop.\\n\\nFigure 31: Error example about Reasoning Error\"}"]}
{"id": "loJM1acwzf", "page_num": 40, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: How many quizzes are there in units 4, 5, and 6 combined?\\nResponse: There are three quizzes in units 4, 5, and 6 combined:\\n- Unit 4: Quiz 2: Twenty Core Concepts on Unit 3 & 4\\n- Unit 5: No quiz listed\\n- Unit 6: Quiz #3: Comprehension of Twenty Core Concepts from Unit 5 & 6\\nSo, the total number of quizzes in units 4, 5, and 6 is 3.\\n\\nError Type: Reasoning Error\\nAnalysis: The response analyze correctly the quizzes in each unit, but count their number wrongly at the last step.\"}"]}
{"id": "loJM1acwzf", "page_num": 41, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: How many distinct button functions are there during a workout?\\nResponse: During a workout, the HONOR Watch GS Pro has the following distinct button functions:\\n1. Press the Up button: Lock, mute, or end workout.\\n2. Press the Down button: Switch screen.\\n3. Press and hold the Up button: Finish the workout.\\n4. Press and hold the Down button: Wake up the voice assistant.\\n5. Swipe up or down on the screen: Switch screen.\\n6. Swipe left or right on the screen: Switch screen.\\nSo, there are 6 distinct button functions during a workout.\\n\\nError Type: Irrelevant Answer\\nAnalysis: The question asks the button functions. The response, however, answers the button operations and outputs an irrelevant answer.\\n\\nFigure 33: Error example about Irrelevant Answer\"}"]}
{"id": "loJM1acwzf", "page_num": 42, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Question:** What types of discrimination are most commonly reported by women in STEM jobs compared to men in the survey of Pew Research Center, January 2018.\\n\\n**Response:** In the Pew Research Center survey titled \\\"Women and Men in STEM Often at Odds Over Workplace Equity,\\\" published in January 2018, several types of discrimination were reported by women in STEM jobs compared to men, and by women in workplaces with a higher proportion of women. Here are the key findings:\\n\\n1. **Gender Discrimination**:  \\n   - **Overall Experience**: 50% of women in STEM jobs reported experiencing gender discrimination at work, compared to 19% of men in STEM jobs.  \\n   - **Specific Types of Discrimination**: Women in STEM jobs reported various forms of gender discrimination, including earning less than a man doing the same job (29%), being treated as if they were not competent (29%), experiencing repeated small slights at work (20%), and receiving less support from senior leaders (18%).\\n\\n5. **Advanced Degrees**:  \\n   - **Higher Discrimination**: 62% of women in STEM jobs with a postgraduate degree reported experiencing gender discrimination, compared to 41% of women with some college or less education.  \\n   - **Impact on Fair Treatment**: Women with advanced degrees were less likely to believe that women are treated fairly in opportunities for promotion and advancement.\\n\\nThese findings highlight the significant gender disparities and challenges that women in STEM fields face, particularly in male-dominated environments and specific job roles like computer-related positions.\\n\\n**Error Type:** Irrelevant Answer  \\n**Analysis:** The question asks about the MOST discrimination types, while response summarizes various types of discriminations towards women in STEM jobs.\"}"]}
{"id": "loJM1acwzf", "page_num": 43, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.2 Case Study\\n\\n**Question:** What is Interest Coverage Ratio for Activision Blizzard in F2019?\\n\\n**Reference Answer:** 17.85\\n\\n**GPT-4a \u2705**\\n\\n**Answer:** Interest Coverage Ratio (ICR) for Activision Blizzard in Fiscal Year 2019 can be calculated using the formula:\\n\\n\\\\[\\n\\\\text{Interest Coverage Ratio} = \\\\frac{\\\\text{EBIT}}{\\\\text{Interest Expense}}\\n\\\\]\\n\\nFrom the provided financial statements for the year ended December 31, 2019:\\n- Operating Income (EBIT) = $1,607 million\\n- Interest Expense = $90 million (from Interest and other expense (income), net)\\n\\nThus, the Interest Coverage Ratio is:\\n\\n\\\\[\\n\\\\text{ICR} = \\\\frac{1,607}{90} = 17.86\\n\\\\]\\n\\nThis means that Activision Blizzard earned about 17.86 times its interest expenses before taxes and interest charges in 2019.\\n\\n**OCR + Mixtral-8x22B \u274c**\\n\\n**Answer:** The Interest Coverage Ratio for Activision Blizzard in F2019 is not explicitly stated in the provided document. However, it can be calculated using the information provided in the document. The Interest Coverage Ratio is calculated by dividing Earnings Before Interest and Taxes (EBIT) by Interest Expense. According to the document, Activision Blizzard had an EBIT of $1,633 million and an Interest Expense of $28 million in F2019. Therefore, the Interest Coverage Ratio for Activision Blizzard in F2019 is approximately 58.32.\\n\\n**Gemini-1.5-Pro \u274c**\\n\\n**Answer:** The provided document is the 2019 10-K report for Activision Blizzard, but it does not contain the specific term \\\"Interest Coverage Ratio\\\". To calculate this ratio, you need both the company's earnings before interest and taxes (EBIT) and its interest expense. You can find the following information within the document to calculate the Interest Coverage Ratio:\\n\\n- **Operating income:** Page 47 (Consolidated Statements of Operations Data)\\n- **Interest expense (income), net:** Page 47 (Consolidated Statements of Operations Data)\\n\\nPlease note:\\n- You will need to use the provided figures to calculate the Interest Coverage Ratio using the formula: **Interest Coverage Ratio = EBIT / Interest expense**.\\n- The document may refer to the components of the ratio using slightly different terminology. Once you have located the relevant figures, you can calculate the Interest Coverage Ratio for Activision Blizzard in F2019.\\n\\n**InternVL-v1.5 \u274c**\\n\\n**Answer:** The Interest Coverage Ratio for Activision Blizzard in F2019 is 1.9.\\n\\n**OCR + GPT-4 \u274c**\\n\\n**Answer:** The Interest Coverage Ratio for Activision Blizzard in Fiscal Year 2019 is not explicitly stated in the provided text. To calculate the Interest Coverage Ratio, you would typically use the formula:\\n\\n\\\\[\\n\\\\text{Interest Coverage Ratio} = \\\\frac{\\\\text{EBIT}}{\\\\text{Interest Expense}}\\n\\\\]\\n\\nWhere:\\n- EBIT (Earnings Before Interest and Taxes) is typically operating income or earnings before the deduction of interest and tax expenses.\\n- Interest Expense is the cost incurred by an entity for borrowed funds.\\n\\nFrom the provided text, specific figures for EBIT and Interest Expense for Activision Blizzard in Fiscal Year 2019 are needed to calculate the Interest Coverage Ratio, and these figures are not provided in the text excerpts. Therefore, it is not possible to calculate the Interest Coverage Ratio without additional financial data from the company's financial statements or reports.\\n\\nFigure 35: Case Study. Evidence source: table. The evidence pages are zoomed in. The correct extracted information and reasoning are colored in green, and the wrong ones are colored in red.\"}"]}
{"id": "loJM1acwzf", "page_num": 44, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: What devices other than phone are introduced for setting device+?\\n\\nReference Answer: 1. Vision 2. Bluetooth devices\\n\\nGPT-4o \u2705\\nAnswer: Other than phones, the devices introduced for setting Device+ in the Huawei Nova Y70 User Guide include:\\n\\n1. **Vision**: Ensure that it is connected to the same LAN and logged in to the same HUAWEI ID as your phone.\\n2. **Bluetooth devices**: Some Bluetooth devices (such as Bluetooth headsets) can be linked with your phone via Device+ after establishing a Bluetooth connection.\\n\\nThese devices can be linked to your phone to enable seamless collaboration and task transfer.\\n\\nGPT-4v \u274c\\nAnswer: The images you provided are pages from the user guide for the Huawei Nova Y70 smartphone. This guide includes detailed instructions on various features and settings of the device, such as basic gestures, camera usage, app management, and settings adjustments. If you have specific questions about any of the features or need further details on how to use the device, feel free to ask!\\n\\nGemini-1.5-Pro \u274c\\nAnswer: The document you provided is a user guide for the Huawei nova Y70 phone and it doesn't contain information on \\\"device+\\\". Therefore, I can't answer your question.\\n\\nInternVL-v1.5 \u274c\\nAnswer: The devices introduced for setting device+ are Bluetooth devices, such as Bluetooth headphones, and other devices that support Bluetooth.\\n\\nOCR + GPT-4 \u2705\\nAnswer: The devices other than phones that are introduced for setting up Device+ include:\\n\\n1. **Vision**: This refers to devices like smart TVs or displays that can be linked with your phone for a seamless multi-device experience.\\n2. **Bluetooth devices**: Such as Bluetooth headsets, which can be connected to your phone via Device+ for audio streaming or calls.\\n\\nThese devices can be controlled and interacted with through your phone, allowing for tasks like transferring MeeTime calls, audio, or video content to your Vision device, or managing audio playback through connected Bluetooth devices.\\n\\nOCR + Mixtral-8x22B \u2705\\nAnswer: For setting Device+, ensure that devices to be connected support Device+. Before you get started, enable Bluetooth and Wi-Fi and log in to your HUAWEI ID on your phone. For other devices, perform the following settings:\\n\\n+ **Vision**: Ensure that it is connected to the same LAN and logged in to the same HUAWEI ID as your phone.\\n+ **Bluetooth device**: Some Bluetooth devices (such as Bluetooth headsets) can be linked with your phone via Device+ after establishing a Bluetooth connection.\\n\\n@ Device+ does not support collaboration between phones.\\n\\nIf Device+ is hidden, access Control Panel and go to > Show Device+.\\n\\nFigure 36: Case Study. Evidence source: text. The evidence pages are zoomed in. The correct extracted information and reasoning are colored in green, and the wrong ones are colored in red.\"}"]}
{"id": "loJM1acwzf", "page_num": 45, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: At NTU, how many types of Field Sports can students participate in?\\n\\nReference Answer: 4\\n\\nAnswer: At Nanyang Technological University (NTU), students can participate in four types of field sports: Cricket, Football, Rugby, and Touch Football.\\n\\nAnswer: At Nanyang Technological University (NTU), students can participate in several types of field sports. According to the guidebook, the field sports available include:\\n\\n1. Cricket\\n2. Football\\n3. Rugby\\n4. Touch Football\\n\\nThese sports provide a variety of options for students interested in field sports, catering to different interests and skills.\\n\\nAnswer: The provided pamphlet does not list the number of Field Sports students can participate in. Therefore, I cannot answer your question.\\n\\nAnswer: Based on the image, students at NTU can participate in 5 types of Field Sports. These are listed as Football, Netball, Rugby, Volleyball, and Ultimate Frisbee.\\n\\nAnswer: Students at NTU can participate in 10 types of Field Sports: Badminton, Cricket, Football, Rugby, Touch Football, Ultimate Frisbee, Golf, Mind Games, International Chess, and Snooker and Pool.\\n\\nFigure 37: Case Study. Evidence source: layout. The evidence page is zoomed in. The correct extracted information and reasoning are colored in green, and the wrong ones are colored in red.\"}"]}
{"id": "loJM1acwzf", "page_num": 46, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: How many children are being read to in the photograph of Page 3?\\nReference Answer: 2\\n\\nGPT-4o \u2705\\nAnswer: In the photograph on Page 3, there are two children being read to.\\n\\nGPT-4v \u2705\\nAnswer: In the photograph on Page 3, there are two children being read to by an adult.\\n\\nGemini-1.5-Pro \u2705\\nAnswer: There are **two** children being read to in the photograph on page 3.\\n\\nInternVL-v1.5 \u274c\\nAnswer: In the photograph of Page 3, there are three children being read to by the woman.\\n\\nOCR + GPT-4 \u274c\\nAnswer: The text provided does not contain any information about a photograph or the number of children being read to in a photograph on Page 3. Therefore, I'm unable to answer the question based on the given text. If you have access to the photograph or additional details, please share them, and I would be happy to help with your query.\\n\\nOCR + Mixtral-8x22B \u274c\\nAnswer: There are three children being read to in the photograph on Page 3.\\n\\nFigure 38: Case Study. Evidence source: image. The evidence page is zoomed in. The correct extracted information and reasoning are colored in green, and the wrong ones are colored in red.\"}"]}
{"id": "loJM1acwzf", "page_num": 47, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D Limitations\\n\\nMMLONGBENCH-DOC is the first comprehensive benchmark designed to evaluate the long-context document understanding capabilities of LVLMs. While our benchmark addresses significant gaps in the previous datasets, we acknowledge several limitations.\\n\\nOne primary limitation is the scale of the benchmark. Currently, our benchmark includes a test set comprising 135 documents and 1,082 questions. It is much smaller compared to previous datasets. The complexity and difficulty of annotations limit the scale of our benchmark. As a long-context benchmark, our documents average about 50 pages and 20,000 tokens. And most questions require either complicated reasoning or cross-page comprehension. It takes more than one hour for an expert-level annotator to read through a single document, and then edit existing instances and create new instances on this document. Given the purpose of MMLONGBENCH-DOC as an evaluation benchmark, we prioritize annotation quality over quantity. Moreover, the results presented in Sections 3.3 and 3.4 confirm that the scale of our benchmark is sufficient for fine-grained evaluations across different document types, evidence sources, evidence pages, etc. Additionally, we plan to expand our benchmark by adding more documents and questions in future iterations.\\n\\nWe roughly categorize these questions into three types, i.e., single-page, cross-page, and unanswerable questions, based on whether evidence can be found in the documents and the number of evidence pages. However, unlike MMBench [41] or MathVista [56], we provide no further taxonomy to classify some (e.g., 7 or 20) fine-grained, evaluated reasoning or perception capabilities out of two main reasons: (1) Prior (i.e., pre-annotation) taxonomy limits the diversity of the questions. Therefore we provide no predefined classifications in our guideline and encourage the expert-level annotators to freely write questions without constraints. (2) The intrinsic complexity of document understanding presents significant challenges for establishing a posterior (i.e., post-annotation) taxonomy.\\n\\nWhile there exist limitations in our benchmark, MMLONGBENCH-DOC surely represents a significant step forward in this field. We would iteratively maintain and refine this benchmark and hope it could push forward the development of long-context document understanding.\\n\\nE Social Impacts\\n\\nThe development and use of MMLONGBENCH-DOC may have potential societal implications. For instance, biased or inaccurate outputs from benchmarked models could perpetuate harmful stereotypes or reinforce existing social inequalities. Additionally, the ability to process and analyze long documents could potentially be used to surveil or monitor individuals\u2019 personal information. Developers and users of MMLONGBENCH-DOC benchmark must be aware of these potential consequences and take steps to ensure responsible development and deployment of AI models.\\n\\nF Author Statement\\n\\nThe authors state that all of the previous datasets that we collected are licensed under the Creative Commons license (CC-BY) or other open-source licenses. Using this dataset should abide by the policy of OpenAI. Regarding the newly collected documents, we manually check them to ensure their availability for academic use. Should any authors request the removal of their documents, we will promptly comply.\"}"]}
{"id": "loJM1acwzf", "page_num": 48, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Checklist\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? [Yes]\\n   (b) Did you describe the limitations of your work? [Yes] See Appendix D\\n   (c) Did you discuss any potential negative societal impacts of your work? [Yes] See supplemental material E\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A] We didn\u2019t involve theory in this benchmark.\\n   (b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] https://mayubo2333.github.io/MMLongBench-Doc\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [N/A]\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A]\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 4\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes]\\n   (b) Did you mention the license of the assets? [Yes] See Appendix F\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] https://mayubo2333.github.io/MMLongBench-Doc\\n   (d) Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? [Yes] See Appendix A.1 and A.2\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\"}"]}
