{"id": "ScPgzCZ6Lo", "page_num": 1, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"GC-Bench: An Open and Unified Benchmark for Graph Condensation\\n\\nQingyun Sun\\\\textsuperscript{1,*}, Ziying Chen\\\\textsuperscript{1,*}, Beining Yang\\\\textsuperscript{2}, Cheng Ji\\\\textsuperscript{1}, Xingcheng Fu\\\\textsuperscript{3}, Sheng Zhou\\\\textsuperscript{4}, Hao Peng\\\\textsuperscript{1}, Jianxin Li\\\\textsuperscript{1}, Philip S. Yu\\\\textsuperscript{5}\\n\\n\\\\textsuperscript{1}Beihang University, \\\\textsuperscript{2}University of Edinburgh, \\\\textsuperscript{3}Guangxi Normal University, \\\\textsuperscript{4}Zhejiang University, \\\\textsuperscript{5}University of Illinois, Chicago\\n\\n\\\\{sunqy,chanztuying\\\\}@buaa.edu.cn\\n\\nAbstract\\n\\nGraph condensation (GC) has recently garnered considerable attention due to its ability to reduce large-scale graph datasets while preserving their essential properties. The core concept of GC is to create a smaller, more manageable graph that retains the characteristics of the original graph. Despite the proliferation of graph condensation methods developed in recent years, there is no comprehensive evaluation and in-depth analysis, which creates a great obstacle to understanding the progress in this field. To fill this gap, we develop a comprehensive Graph Condensation Benchmark (GC-Bench) to analyze the performance of graph condensation in different scenarios systematically. Specifically, GC-Bench systematically investigates the characteristics of graph condensation in terms of the following dimensions: effectiveness, transferability, and complexity. We comprehensively evaluate 12 state-of-the-art graph condensation algorithms in node-level and graph-level tasks and analyze their performance in 12 diverse graph datasets. Further, we have developed an easy-to-use library for training and evaluating different GC methods to facilitate reproducible research. The GC-Bench library is available at https://github.com/RingBDStack/GC-Bench.\\n\\n1 Introduction\\n\\nData are the driving force behind advancements in machine learning, especially with the advancement of large models. However, the rapidly increasing size of datasets presents challenges in management, storage, and transmission. It also makes model training more costly and time-consuming. This issue is particularly pronounced in the graph domain, where larger datasets mean more large-scale structures, making it challenging to train models in environments with limited resources. Compared to graph coarsening, which groups nodes into super nodes, and sparsification, which selects a subset of edges, graph condensation \\\\cite{39, 10} synthesizes a smaller, informative graph that retains enough data for models to perform comparably to using the full dataset.\\n\\nGraph condensation. Graph condensation aims to learn a new small but informative graph. A general framework of GC are shown in Figure 1. Given a graph dataset $G$, the goal of Graph condensation is to achieve comparable results on synthetic condensed graph dataset $G'$ as training on the original $G$. For Node-level condensation, the original dataset $G = \\\\{G\\\\} = \\\\{X \\\\in \\\\mathbb{R}^{N \\\\times d}, A \\\\in \\\\mathbb{R}^{N \\\\times N}\\\\}$ and the condensed dataset $G' = \\\\{G'\\\\} = \\\\{X \\\\in \\\\mathbb{R}^{N' \\\\times d'}, A \\\\in \\\\mathbb{R}^{N' \\\\times N'}\\\\}$, where $N' \\\\ll N$. For Graph-level condensation, the original dataset $G = \\\\{G_1, G_2, \\\\cdots, G_n\\\\}$ and the condensed dataset $G' = \\\\{G'_1, G'_2, \\\\cdots, G'_{n'}\\\\}$, where $n' \\\\ll n$. The condensation ratio $r$ can be calculated by condensed dataset size / whole dataset size.\\n\\n\\\\*Equal contribution.\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 2, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 1: The GC methods can be broadly divided into two categories: The first category depends on the backbone model, refining the condensed graph by aligning it with the backbone\u2019s gradients (a), trajectories (b), and output distributions (c) trained on both original and condensed graphs. The second category, independent of the backbone, optimizes the graph by matching its distribution with that of the original graph data (d) or by identifying frequently co-occurring computation trees (e).\\n\\n**Research Gap.** Although several studies aim to comprehensively discuss existing GC methods [39, 10], they either overlook graph-specific properties or lack systematic experimentation. This discrepancy highlights a significant gap in the literature, partly due to limitations in datasets and evaluation dimensions. A concurrent work [23] analyzed the performance of node-level GC methods but it included only a subset of representative methods, lacking an analysis of graph-level methods, a deep structural analysis, and an assessment of the generalizability of the methods. To bridge this gap, we introduce GC-Bench, an open and unified benchmark to systematically evaluate existing graph condensation methods focusing on the following aspects: \u2776 **Effectiveness:** the progress in GC, and the impact of structure and initialization on GC; \u2777 **Transferability:** the transferability of GC methods across backbone architectures and downstream tasks; \u2778 **Efficiency:** the time and space efficiency of GC methods. The contributions of GC-Bench are as follows:\\n\\n- **Comprehensive benchmark.** GC-Bench systematically integrated 12 representative and competitive GC methods on both node-level and graph-level by unified condensation and evaluation, giving multi-dimensional analysis in terms of effectiveness, transferability, and efficiency.\\n\\n- **Key findings.** (1) Graph-level GC methods is still far from achieving the goal of lossless compression. A large condensation ratio does not necessarily lead to better performance with current methods. (2) GC methods can retain semantic information from the graph structure in a condensed graph, but there is still significant improvement room in preserving complex structural properties. (3) All condensed datasets struggle to perform well outside the specific tasks they were condensed, leading to limited applicability. (4) Backbone-dependent GC methods embed model-specific information in the condensed datasets, and popular graph transformers are not compatible with current GC methods as backbones. (5) The initialization mechanism affects both the performance and convergence according to the characteristics of the dataset and the GC method. (6) Most GC methods coupled with backbones and whole dataset training have poor time and space efficiency, contradicting the initial purpose of using GC for efficient training.\\n\\n- **Open-sourced benchmark library and future directions.** GC-Bench is open-sourced and easy to extend to new methods and datasets, which can help identify directions for further exploration and facilitate future endeavors.\\n\\n## 2 Overview of GC-Bench\\n\\nWe introduce **Graph Condensation Benchmark (GC-Bench)** in terms of datasets (\u25b7 Section 2.1), algorithms (\u25b7 Section 2.2), research questions that guide our benchmark design (\u25b7 Section 2.3) and the comparison with related benchmarks (\u25b7 Section 2.4). The overview of GC-Bench is shown in Table 1. More details can be found in the Appendix provided in the **Supplementary Material**.\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 3, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: An overview of GC-Bench\\n\\n| Methods                  | Traditional core-set methods | Gradient matching | Trajectory matching | Distribution matching | Kernel Ridge Regression | Computation Tree Compression |\\n|--------------------------|------------------------------|-------------------|---------------------|-----------------------|-------------------------|-----------------------------|\\n|                          | Random, Herding [36], K-Center [30] | GCond [15], DosCond [14], SGDD [42] | SFGC [47], GEOM [45] | GCDM [20], DM [24, 22] | KiDD [41] | Mirage [11] |\\n\\n| Datasets                  | Homogeneous datasets | Heterogeneous datasets | Graph-level datasets |\\n|---------------------------|----------------------|------------------------|----------------------|\\n|                           | Cora [16], Citeseer [16], ogbn-arxiv [13], Flickr [43], Reddit [12] | ACM [46], DBLP [7] | NCI1 [32], DD [4], ogbg-molbace [13], ogbg-molbbbp [13], ogbg-molhiv [13] |\\n\\n| Downstream Tasks          | Node-level task | Graph-level task |\\n|---------------------------|-----------------|------------------|\\n|                           | Node classification, Link prediction, Anomaly detection | Graph classification |\\n\\n| Evaluations               | Effectiveness | Transferability | Efficiency |\\n|---------------------------|---------------|-----------------|------------|\\n|                           | Performance under different condensation ratios, Impact of structural properties, Impact of initialization mechanism | Different downstream tasks, Different backbone model architectures | Time and memory consumption |\\n\\n2.1 Benchmark Datasets\\n\\nRegarding evaluation datasets, we adapt the 12 widely-used datasets in the current literature. The node level dataset include 5 homogeneous dataset (Cora [16], Citeseer [16], ogbn-arxiv [13], Flickr [43], Reddit [12]) and 2 heterogeneous datasets (ACM [46] and DBLP [7]). The graph-level dataset include NCI1 [32], DD [4], ogbg-molbace [13], ogbg-molbbbp [13], ogbg-molhiv [13]. We leverage the public train, valid, and test split of these datasets. We report the dataset statistics in Appendix A.1.\\n\\n2.2 Benchmark Algorithms\\n\\nWe selected 12 representative and competitive GC methods across 6 categories for evaluation. The main ideas of these methods are shown in Figure 1. The evaluated methods include: (1) traditional core-set methods including Random, Herding [36], K-Center [30], (2) gradient matching methods including DosCond [14], GCond [15] and SGDD [42], (3) trajectory matching methods including SFGC [47] and GEOM [45], (4) distribution matching methods including GCDM [20] and DM [22, 24], (5) Kernel Ridge Regression (KRR) based method KiDD [41], and (6) Computation Tree Compression (CTC) method Mirage [11]. The details of evaluated methods are in Appendix A.2.\\n\\n2.3 Research Questions\\n\\nWe systematically design the GC-Bench to comprehensively evaluate the existing GC algorithms and inspire future research. In particular, we aim to investigate the following research questions.\\n\\n**RQ1: How much progress has been made by existing GC methods?**\\n\\n**Motivation and Experiment Design.** Previous GC methods always adopt different experimental settings, making it difficult to compare them fairly. Given the unified settings of GC-Bench, the first question is to revisit the progress made by existing GC methods and provide potential enhancement directions. A good GC method is expected to perform well consistently under different datasets and different condensation ratios. To answer this question, we evaluate GC methods\u2019 performance on 7 node-level datasets and 5 graph-level datasets with a broader range of condensation ratio $r$ than previous works. The results are shown in Sec. 3.1 and Appendix B.1.\\n\\n**RQ2: How do the potential flaws of the structure affect the graph condensation performance?**\\n\\n**Motivation and Experiment Design.** Most of the current GC methods borrow the idea of image condensation and overlook the specific non-IID properties of irregular graph data. The impact of structural properties in GC is still not thoroughly explored. On one hand, the structure itself possesses various characteristics such as homogeneity and heterogeneity, as well as homophily and heterophily. It remains unclear whether these properties should be preserved in GC and how to preserve them. On the other hand, some structure-free GC methods [47] suggest that the condensed dataset may not\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 4, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"need to explicitly preserve graph structure, and preserving structural information in the generated samples is sufficient. To answer this question, we evaluated GC methods on both homogeneous and heterogeneous as well as homophilic and heterophilic datasets to further explore the impact of structural properties. The results are shown in Sec. 3.2 and Appendix B.2.\\n\\n**RQ3: Can the condensed graphs be transferred to different types of tasks?**\\n\\n**Motivation and Experiment Design.** Most existing GC methods are primarily designed for node classification and graph classification tasks. However, there are numerous downstream tasks on graph data, such as link prediction and anomaly detection, which focus on different aspects of graph data. The transferability of GC across various graph tasks has yet to be thoroughly explored. To answer this question, we perform condensation guided by node classification and use the condensed dataset to train models for 3 classic downstream tasks: link prediction, node clustering, and anomaly detection. The evaluation results are shown in Sec. 3.3 and Appendix B.3.\\n\\n**RQ4: How does the backbone model architecture used for condensation affect the performance?**\\n\\n**Motivation and Experiment Design.** The backbone model plays an important role in extracting the critical features of the original dataset and guiding the optimization process of generating the condensed dataset. Most GC methods choose a specific graph neural network (GNN) as the backbone. The impact of the backbone model architecture and its transferability is under-explored. A high-quality condensed dataset is expected to be used for training models with not only the specific one used for condensation but also various architectures. To answer this question, we evaluate the transferability performance for 5 representative GNN models (SGC [37], GCN [16], GraphSAGE [12], APPNP [18], and ChebyNet [2]) and 2 non-GNN models (the popular Graph Transformer [31]) and simple MLP). We also investigated the performance variation of different backbones with the number of training steps. The evaluation results are shown in Sec. 3.4 and Appendix B.4.\\n\\n**RQ5: How does the initialization mechanism affect the performance of graph condensation?**\\n\\n**Motivation and Experiment Design.** The initialization mechanism of the condensed dataset is crucial for convergence and performance in image dataset condensation but remains unexplored for irregular graph data. To answer this question, we adopt 5 distinct initialization strategies (Random Noise, Random Sample, Center, K-Center, and K-Means) to evaluate their impact on condensation performance and converge speed. The results are shown in Sec. 3.5 and Appendix B.5.\\n\\n**RQ6: How efficient are these GC methods in terms of time and space?**\\n\\n**Motivation and Experiment Design.** As the GC methods aim to achieve comparable performance on the condensed dataset and the original dataset, they always rely on the training process on the original datasets. The efficiency and scalability of GC methods are overlooked by existing methods, which is crucial in practice since the original intent of GC is to reduce computation and storage costs for large graphs. To answer this question, we evaluate the time and memory consumption of these GC methods. Specifically, we record the overall time when achieving the best result, the peak CPU memory, and the peak GPU memory. The results are shown in Sec. 3.6 and Appendix B.6.\\n\\n### 2.4 Discussion on Existing Benchmarks\\n\\nTo the best of our knowledge, GC-Bench is the first comprehensive benchmark for both node-level and graph-level graph condensation. There are a few image dataset condensation benchmark works for image classification task [1] and condensation adversarial robustness [38]. A recent work GCondenser [23] evaluates some node-level GC methods for node classification on homogeneous graphs with limited evaluation dimensions in terms of performance and time efficiency. Our GC-Bench analyzes more GC methods on a wider variety of datasets (both homogeneous and heterogeneous) and tasks (node classification, graph classification), encompassing both node-level and graph-level methods. In addition to performance and efficiency analysis, we further explore the transferability across different tasks (link prediction, node clustering, anomaly detection) and backbones. With GC-Bench covering more in-depth investigation over a wider scope, we believe it will provide valuable insights into existing works and future directions. A comprehensive comparison with GCondenser can be found in Appendix A.5.\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 5, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: **Node classification accuracy (%)** (mean\u00b1std) across datasets with varying condensation ratios $r$. $\\\\mathcal{H}$ denotes the homophily ratio [29]. The best results are shown in **bold** and the runner-ups are shown in *underlined*. Red color highlights entries that exceed the whole dataset performance.\\n\\n| Dataset | Ratio($r$) | Traditional Core-set Methods | Distribution | Gradient | Trajectory | Whole Dataset |\\n|---------|------------|-------------------------------|--------------|----------|------------|---------------|\\n|         |            | Random | Herding | K-Center | GCDM | DM | DosCond | GCond | SGDD | SFGC | GEOM |               |\\n| Cora ($\\\\mathcal{H}=0.81$) | 0.26% | 31.6\u00b11.2 | 48.6\u00b11.4 | 48.6\u00b11.4 | 39.0\u00b10.4 | 35.6\u00b13.9 | 78.7\u00b11.3 | **79.8\u00b10.7** | 78.6\u00b12.7 | 78.8\u00b11.2 | 50.1\u00b11.2 | 80.8\u00b10.3 |\\n|         | 0.52% | 47.1\u00b11.7 | 56.0\u00b10.6 | 44.7\u00b12.7 | 42.3\u00b10.7 | 38.5\u00b14.4 | **81.1\u00b10.1** | 80.2\u00b10.7 | **81.2\u00b10.5** | 79.2\u00b11.8 | 70.5\u00b10.9 |\\n|         | 1.30% | 62.3\u00b11.0 | 69.9\u00b10.8 | 62.0\u00b11.3 | 63.4\u00b10.1 | 63.2\u00b10.5 | **81.2\u00b10.4** | 80.2\u00b12.2 | **81.6\u00b10.5** | 79.4\u00b10.7 | 78.5\u00b11.9 |\\n|         | 2.60% | 72.4\u00b10.5 | 74.2\u00b10.6 | 73.5\u00b10.7 | 73.4\u00b10.2 | 72.6\u00b10.7 | 79.6\u00b10.2 | 80.5\u00b11.3 | **81.8\u00b10.2** | 81.7\u00b10.0 | 76.1\u00b14.7 |\\n|         | 3.90% | 74.6\u00b10.6 | 76.0\u00b10.6 | 77.4\u00b10.3 | 76.4\u00b10.5 | 75.3\u00b10.1 | 78.9\u00b10.1 | 79.8\u00b10.4 | **82.1\u00b10.8** | 81.5\u00b10.5 | 78.3\u00b12.0 |\\n|         | 5.20% | 77.1\u00b10.6 | 76.7\u00b10.4 | 76.5\u00b10.6 | 78.4\u00b10.0 | 77.9\u00b10.2 | 79.7\u00b10.6 | 77.8\u00b13.6 | **82.0\u00b11.4** | 81.4\u00b10.0 | 80.4\u00b10.8 |\\n| Reddit ($\\\\mathcal{H}=0.78$) | 0.05% | 45.2\u00b11.7 | 54.6\u00b12.4 | 50.7\u00b12.2 | 61.1\u00b10.4 | 60.6\u00b10.7 | 50.1\u00b11.8 | 84.4\u00b12.6 | 83.6\u00b11.7 | **87.2\u00b12.1** | 73.6\u00b11.2 |\\n|         | 0.10% | 53.3\u00b11.6 | 62.3\u00b11.6 | 50.1\u00b11.3 | 72.3\u00b10.5 | 68.7\u00b11.2 | 59.5\u00b12.8 | 84.7\u00b11.9 | 83.8\u00b12.2 | 81.5\u00b14.2 | 76.2\u00b13.9 |\\n|         | 0.20% | 65.1\u00b11.2 | 69.4\u00b12.1 | 54.2\u00b11.9 | 80.8\u00b10.6 | 76.7\u00b10.1 | 65.8\u00b10.4 | 86.3\u00b12.4 | 88.3\u00b10.5 | **90.4\u00b10.9** | 89.9\u00b10.6 |\\n|         | 0.50% | 76.1\u00b11.8 | 81.4\u00b10.7 | 67.8\u00b11.3 | 86.4\u00b10.1 | 84.2\u00b10.2 | 65.8\u00b10.4 | 87.9\u00b12.7 | 91.0\u00b10.1 | **91.7\u00b10.2** | 91.7\u00b10.3 |\\n|         | 1.00% | 84.9\u00b10.5 | 85.9\u00b10.3 | 74.6\u00b10.8 | OOM | OOM | 78.3\u00b11.3 | 87.2\u00b11.2 | 85.9\u00b10.9 | **91.8\u00b10.2** | OOM |\\n|         | 5.00% | **92.3\u00b10.1** | **91.9\u00b10.1** | 88.4\u00b10.3 | OOM | OOM | OOM | OOM | OOM | OOM | OOM |\\n| Citeseer ($\\\\mathcal{H}=0.74$) | 0.18% | 39.3\u00b12.0 | 36.6\u00b12.6 | 36.0\u00b12.6 | 30.6\u00b12.5 | 32.1\u00b11.5 | **71.8\u00b10.1** | 71.3\u00b10.6 | 71.7\u00b13.2 | 70.7\u00b11.1 | 63.7\u00b10.4 |\\n|         | 0.36% | 44.1\u00b12.0 | 41.5\u00b12.6 | 38.7\u00b13.2 | 36.2\u00b12.0 | 43.3\u00b11.2 | **73.1\u00b10.3** | 70.4\u00b11.8 | **73.3\u00b13.3** | 70.9\u00b11.6 | 69.8\u00b10.1 |\\n|         | 0.90% | 49.3\u00b11.0 | 56.5\u00b11.3 | 51.4\u00b11.4 | 52.6\u00b11.1 | 53.7\u00b10.3 | **73.0\u00b10.4** | 70.7\u00b10.9 | 72.8\u00b10.6 | 70.7\u00b10.5 | 70.5\u00b10.3 |\\n|         | 1.80% | 57.4\u00b10.7 | 68.4\u00b10.8 | 61.4\u00b11.2 | 64.7\u00b10.6 | 66.4\u00b10.2 | 71.6\u00b10.6 | 70.2\u00b11.4 | **73.6\u00b10.6** | 70.4\u00b11.2 | 67.5\u00b10.6 |\\n|         | 2.70% | 66.2\u00b11.0 | 68.1\u00b10.8 | 67.5\u00b10.8 | 69.2\u00b11.6 | 68.5\u00b10.0 | 71.7\u00b11.0 | 67.7\u00b10.2 | **72.1\u00b10.5** | 71.0\u00b11.0 | 70.9\u00b10.5 |\\n|         | 3.60% | 69.2\u00b10.3 | 69.1\u00b10.7 | 69.3\u00b10.7 | 69.3\u00b10.4 | 70.4\u00b10.4 | **72.4\u00b10.3** | 68.4\u00b11.6 | **73.3\u00b11.2** | 70.6\u00b10.1 | 70.7\u00b10.1 |\\n| ogbn-arxiv ($\\\\mathcal{H}=0.65$) | 0.05% | 10.7\u00b10.9 | 32.7\u00b10.9 | 36.9\u00b11.2 | 51.1\u00b10.2 | 40.4\u00b13.0 | 59.5\u00b11.0 | 60.0\u00b10.0 | 59.7\u00b10.2 | **68.2\u00b12.3** | 64.5\u00b10.9 |\\n|         | 0.10% | 36.5\u00b11.1 | 41.6\u00b11.0 | 40.3\u00b10.9 | 55.9\u00b11.5 | 47.7\u00b12.0 | 60.9\u00b10.8 | 59.5\u00b11.1 | 56.1\u00b13.4 | **69.1\u00b10.7** | 64.6\u00b11.4 |\\n|         | 0.20% | 43.2\u00b10.8 | 48.9\u00b10.8 | 42.7\u00b11.0 | 59.3\u00b10.8 | 47.1\u00b10.7 | 62.2\u00b10.4 | 61.4\u00b10.7 | 60.9\u00b10.9 | **69.0\u00b10.4** | 64.3\u00b11.0 |\\n|         | 0.50% | 48.3\u00b10.4 | 51.4\u00b10.4 | 48.3\u00b10.5 | 61.0\u00b10.1 | 57.7\u00b10.2 | 62.1\u00b10.3 | 63.0\u00b10.9 | 63.2\u00b10.2 | **67.1\u00b10.0** | 67.2\u00b10.0 |\\n|         | 1.00% | 51.2\u00b10.4 | 52.0\u00b10.4 | 50.3\u00b10.5 | 61.0\u00b10.0 | 59.8\u00b10.2 | 61.8\u00b11.3 | 62.8\u00b10.0 | 62.0\u00b12.8 | **68.4\u00b11.3** | 69.1\u00b10.3 |\\n|         | 5.00% | 57.2\u00b10.2 | 56.1\u00b10.2 | 56.5\u00b10.4 | OOM | OOM | OOM | OOM | OOM | OOM | OOM |\\n| Flickr ($\\\\mathcal{H}=0.24$) | 0.05% | 40.5\u00b10.4 | 41.8\u00b10.6 | 43.9\u00b10.7 | 45.8\u00b10.4 | 44.7\u00b10.5 | 40.7\u00b11.1 | 44.2\u00b12.3 | **46.6\u00b10.1** | 44.3\u00b10.4 | 44.8\u00b10.8 |\\n|         | 0.10% | 42.0\u00b10.6 | 41.4\u00b10.6 | 41.4\u00b10.8 | **47.5\u00b10.2** | 45.4\u00b10.1 | 43.2\u00b10.0 | 44.2\u00b11.4 | **46.8\u00b10.2** | 44.0\u00b11.5 | 45.5\u00b10.7 |\\n|         | 0.20% | 43.1\u00b10.2 | 41.5\u00b10.7 | 41.4\u00b10.8 | **48.8\u00b10.1** | 42.8\u00b10.6 | 43.9\u00b10.6 | 44.4\u00b11.4 | **46.8\u00b10.3** | 41.4\u00b15.6 | 46.1\u00b10.6 |\\n|         | 0.50% | 43.1\u00b10.3 | 44.4\u00b10.3 | 42.6\u00b10.7 | **49.1\u00b10.3** | **48.6\u00b10.3** | 45.0\u00b10.3 | 44.7\u00b10.6 | 45.5\u00b10.9 | **46.5\u00b10.1** | 47.1\u00b10.1 |\\n|         | 1.00% | 43.0\u00b10.4 | 44.5\u00b10.6 | 43.2\u00b10.2 | **49.3\u00b10.1** | **49.6\u00b10.5** | 46.0\u00b10.3 | 45.1\u00b10.5 | 47.2\u00b10.9 | 46.6\u00b10.0 | 47.0\u00b10.4 |\\n|         | 5.00% | 45.2\u00b10.3 | 44.7\u00b10.4 | 45.5\u00b10.2 | OOM | OOM | OOM | OOM | OOM | OOM | OOM |\\n| ACM | .003% | 83.9\u00b11.7 | 82.5\u00b11.6 | 76.3\u00b12.4 | 84.8\u00b10.3 | 84.8\u00b10.3 | 89.3\u00b10.6 | 80.6\u00b10.8 | 90.3\u00b12.3 | **92.2\u00b10.2** | 73.4\u00b10.5 |\\n|         | .007% | 84.7\u00b11.0 | 84.5\u00b10.8 | 80.9\u00b11.6 | 87.1\u00b10.2 | 87.1\u00b10.2 | 89.8\u00b10.0 | 81.9\u00b11.6 | 91.2\u00b11.9 | **91.6\u00b11.2** | 73.4\u00b10.5 |\\n|         | .013% | 86.8\u00b10.9 | 88.6\u00b10.6 | 87.1\u00b11.2 | 90.4\u00b10.0 | 90.4\u00b10.0 | 89.9\u00b10.1 | 80.4\u00b13.4 | 91.9\u00b13.4 | **92.0\u00b10.4** | 79.3\u00b13.0 |\\n|         | .033% | 87.7\u00b11.4 | 88.3\u00b11.0 | 88.6\u00b10.7 | 91.6\u00b10.2 | 91.6\u00b10.2 | 90.9\u00b10.1 | 89.0\u00b11.2 | 91.2\u00b14.8 | **92.2\u00b10.2** | 82.8\u00b11.0 |\\n|         | .066% | 89.1\u00b10.6 | 87.9\u00b11.1 | 88.8\u00b11.0 | 91.6\u00b10.2 | 91.6\u00b10.2 | 91.4\u00b10.2 | 86.6\u00b12.1 | **91.9\u00b12.0** | 91.8\u00b11.0 | 71.1\u00b10.6 |\\n|         | .332% | 89.3\u00b10.9 | 89.4\u00b10.6 | 88.7\u00b10.9 | 91.3\u00b10.2 | 91.3\u00b10.2 | 91.0\u00b10.5 | 89.3"]}
{"id": "ScPgzCZ6Lo", "page_num": 6, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: **Graph classification performance on GIN** (mean\u00b1std) across datasets with varying condensation ratios $r$. The best results are shown in **bold** and the runner-ups are shown in **underlined**. Red color highlights entries that exceed the whole dataset values.\\n\\n| Dataset | Graph /Cls | Ratio($r$) | Traditional Core-set methods | Gradient | KRR | CTC | Whole Dataset |\\n|---------|------------|------------|-----------------------------|----------|-----|-----|---------------|\\n|         |            |            | Random | Herding | K-Center | DosCond | KiDD | Mirage |               |\\n| NCII Acc. (%) | 1 | 0.06% | 50.90\u00b12.10 | 51.90\u00b11.60 | 51.90\u00b11.60 | 49.20\u00b11.10 | 61.40\u00b10.50 | 50.80\u00b12.20 | 80.0\u00b11.8 |\\n|          | 5 | 0.24% | 52.10\u00b11.00 | 60.50\u00b12.40 | 47.00\u00b11.10 | 51.10\u00b10.80 | 63.20\u00b10.20 | 51.30\u00b11.10 |               |\\n|          | 10 | 0.49% | 55.60\u00b11.90 | 61.80\u00b11.50 | 49.40\u00b11.80 | 50.30\u00b11.30 | 64.20\u00b10.10 | 51.70\u00b11.40 |               |\\n|          | 20 | 0.97% | 58.70\u00b11.40 | 60.90\u00b11.90 | 55.20\u00b11.60 | 50.30\u00b11.30 | 60.90\u00b10.70 | 52.10\u00b12.20 |               |\\n|          | 50 | 2.43% | 61.10\u00b11.20 | 59.00\u00b11.50 | 62.70\u00b11.50 | 50.30\u00b11.30 | 65.40\u00b10.60 | 52.40\u00b12.70 |               |\\n| DD Acc. (%) | 1 | 0.21% | 49.70\u00b111.30 | 58.80\u00b16.10 | 58.80\u00b16.10 | 46.30\u00b18.50 | 71.30\u00b11.50 | 74.00\u00b10.40 | 70.1\u00b12.2 |\\n|          | 5 | 1.06% | 40.80\u00b14.30 | 58.70\u00b15.80 | 51.30\u00b15.30 | 57.50\u00b15.60 | 70.90\u00b11.10 | - |               |\\n|          | 10 | 2.12% | 63.10\u00b15.20 | 64.10\u00b15.80 | 53.40\u00b13.10 | 46.30\u00b18.50 | 71.50\u00b10.50 | - |               |\\n|          | 20 | 4.25% | 56.40\u00b14.30 | 67.00\u00b12.60 | 58.50\u00b15.70 | 40.70\u00b10.00 | 71.20\u00b10.90 | - |               |\\n|          | 50 | 10.62% | 58.90\u00b16.30 | 68.40\u00b14.00 | 62.30\u00b12.50 | 44.00\u00b16.70 | 71.80\u00b11.00 | - |               |\\n| ogbg-molbace ROC-AUC | 1 | 0.17% | 0.468\u00b10.045 | 0.486\u00b10.035 | 0.486\u00b10.035 | 0.512\u00b10.092 | 0.706\u00b10.000 | 0.590\u00b10.004 | 0.763\u00b10.020 |\\n|          | 5 | 0.83% | 0.312\u00b10.019 | 0.470\u00b10.042 | 0.553\u00b10.024 | 0.555\u00b10.079 | 0.562\u00b10.000 | 0.419\u00b10.010 |               |\\n|          | 10 | 1.65% | 0.442\u00b10.028 | 0.532\u00b10.031 | 0.594\u00b10.019 | 0.536\u00b10.072 | 0.594\u00b10.000 | 0.419\u00b10.010 |               |\\n|          | 20 | 3.31% | 0.510\u00b10.023 | 0.509\u00b10.052 | 0.512\u00b10.031 | 0.484\u00b10.080 | 0.640\u00b10.011 | 0.423\u00b10.011 |               |\\n|          | 50 | 8.26% | 0.486\u00b10.020 | 0.625\u00b10.026 | 0.595\u00b10.026 | 0.503\u00b10.084 | 0.723\u00b10.011 | - |               |\\n| ogbg-molbbbp ROC-AUC | 1 | 0.12% | 0.510\u00b10.013 | 0.532\u00b10.015 | 0.532\u00b10.015 | 0.546\u00b10.026 | 0.616\u00b10.000 | 0.592\u00b10.004 | 0.635\u00b10.017 |\\n|          | 5 | 0.61% | 0.522\u00b10.014 | 0.546\u00b10.020 | 0.581\u00b10.022 | 0.519\u00b10.041 | 0.607\u00b10.005 | 0.431\u00b10.013 |               |\\n|          | 10 | 1.23% | 0.508\u00b10.018 | 0.578\u00b10.017 | 0.619\u00b10.027 | 0.505\u00b10.028 | 0.663\u00b10.000 | 0.465\u00b10.036 |               |\\n|          | 20 | 2.45% | 0.567\u00b10.010 | 0.533\u00b10.009 | 0.546\u00b10.012 | 0.493\u00b10.031 | 0.677\u00b10.001 | 0.610\u00b10.022 |               |\\n|          | 50 | 6.13% | 0.595\u00b10.014 | 0.552\u00b10.018 | 0.594\u00b10.016 | 0.509\u00b10.015 | 0.684\u00b10.009 | 0.590\u00b10.031 |               |\\n| ogbg-molhiv ROC-AUC | 1 | 0.01% | 0.366\u00b10.087 | 0.462\u00b10.072 | 0.462\u00b10.072 | 0.674\u00b11.31 | 0.664\u00b10.16 | 0.710\u00b10.009 | 0.701\u00b10.028 |\\n|          | 5 | 0.03% | 0.501\u00b10.051 | 0.496\u00b10.044 | 0.519\u00b10.096 | 0.369\u00b11.75 | 0.657\u00b10.005 | 0.703\u00b10.012 |               |\\n|          | 10 | 0.06% | 0.554\u00b10.031 | 0.458\u00b10.058 | 0.471\u00b10.054 | 0.457\u00b12.14 | 0.632\u00b10.000 | 0.513\u00b10.055 |               |\\n|          | 20 | 0.12% | 0.621\u00b10.022 | 0.582\u00b10.027 | 0.627\u00b10.050 | 0.281\u00b10.007 | 0.648\u00b10.025 | 0.633\u00b10.048 |               |\\n|          | 50 | 0.30% | 0.625\u00b10.062 | 0.600\u00b10.034 | 0.680\u00b10.049 | 0.455\u00b12.14 | 0.587\u00b10.038 | 0.588\u00b10.067 |               |\\n\\n*Mirage cannot directly generate graphs with the required ratio. Parameter search aligns generated graphs with DosCond disk usage (see Appendix B.1). '-' denotes results unavailable due to recursive limits reached in MP Tree search.\\n\\npoorly with GCN. This is because KiDD does not rely on the backbone and depends solely on the structure. Consequently, the stronger the downstream model\u2019s expressive ability, the better the results.\\n\\nFrom both node-level and graph-level results, we observe that as the condensation ratio increases, traditional core-set methods improve, narrowing the performance gap with deep methods. However, deep GC methods show a saturation point or even a decline in performance beyond a certain threshold, suggesting that larger condensed data may introduce noise and biases that degrade performance.\\n\\n**Key Takeaways 1:** Current node-level GC methods can achieve nearly lossless condensation performance. However, there is still a significant gap between graph-level GC and whole dataset training, indicating there is substantial room for improvement.\\n\\n**Key Takeaways 2:** A large condensation ratio does not necessarily lead to better performance with current methods.\\n\\n### 3.2 Structure in Graph Condensation (RQ2)\\n\\nWe analyze the impact of structure in terms of heterogeneity and heterophily. Experimental settings and additional results can be found in Appendix B.2.\\n\\n(1) **Heterogeneity v.s. Homogeneity.** For the heterogeneous datasets ACM and DBLP, we convert the heterogeneous graphs into homogeneous ones for evaluation. From the results in Table 2, we observe that GC methods designed for homogeneous graphs preserve most of the semantic information and perform comparably to models training on the whole dataset.\\n\\n(2) **Heterophily v.s. Homophily.** From the results of the heterophilous dataset Flickr (with homophily ratio $H = 0.24$) in Table 2, we can observe that current GC methods can achieve almost the same accuracy as models training on the whole dataset. However, there is still a significant gap compared to the state-of-the-art results of the model designed for heterophilic graphs.\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 7, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: **Cross-task performance on Citeseer.** For all downstream tasks, the models are trained solely using data of graphs condensed by node classification. For anomaly detection (c, d), structural and contextual anomalies [3] are injected into both the condensed graph and the original graph.\\n\\n**Key Takeaways 3:** Existing GC methods primarily address simple graph data. However, the conversion process to specific data types is non-trivial, leaving significant room for improvement in preserving complex structural properties.\\n\\n### 3.3 Transferability on Different Tasks (RQ3)\\n\\nTo evaluate the transferability of GC methods, we condense the dataset by node classification (NC) and use the condensed dataset to train models for link prediction (LP), node clustering (NClu), and anomaly detection (AD) tasks. The results on Citeseer are shown in Figure 2. Settings and additional results can be found in Appendix B.3.\\n\\nAs shown in Figure 2, performance with condensed datasets was significantly lower compared to original datasets in all transferred tasks. This decline may be due to the task-specific nature of the condensation process, which retains only task-relevant information while ignoring other semantically rich details. For instance, AD task prioritizes high-frequency graph signals more than NC and LP tasks, leading to poor performance when transferring condensed datasets from NC to AD tasks. Among the methods, gradient matching methods (GCond, DosCond, and SGDD) demonstrated better transferability in downstream tasks. In contrast, while structure-free methods (SFGC and GEOM) perform well in node classification (Section 3.1), they show a significant performance gap in AD tasks compared to gradient matching methods.\\n\\n**Key Takeaways 4:** All condensed datasets struggle to perform well outside the context of the specific tasks for which they were condensed, leading to limited applicability.\\n\\n### 3.4 Transferability of Backbone Model Architectures (RQ4)\\n\\nWe adopt one model (SGC or Graph Transformer) as the backbone for condensation and use the various models in downstream tasks evaluation. Details and additional results are in Appendix B.4.\\n\\nAs shown in Figure 3(a) and 3(b), each column shows the generalization performance of a condensed graph generated by different methods for various downstream models. We can observe that datasets condensed with SGC generally maintain performance when transferred across models. However, datasets condensed with Graph Transformer (GTrans) consistently underperform across various methods, and other models also exhibit reduced performance when adapted to Graph Transformer. Intuitively, SGC\u2019s basic neighbor message-passing strategy may overlook global dependencies critical to more complex models, and similarly, complex models may not perform well when adapted to simpler models. As we can observe, DosCond exhibits generally better transferability compared to other gradient-matching methods. Since it can be regarded as the one-step gradient matching variant of GCond, we further test the impact of gradient matching steps on transferability (Figure 3(c)). Increasing the number of matching steps was found to correlate with reduced performance across architectures, indicating that extensive gradient matching may encode model-specific biases.\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 8, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: **Cross-architecture performance.** Using SGC and Graph Transformer (GTrans) to condense Cora with a 2.6% ratio, we then test the accuracy on various downstream architectures (a, b). Furthermore, we evaluate the influence of gradient matching steps on GCond (c).\\n\\n![Cross-architecture performance](image)\\n\\n(a) Cross-arch. Acc. from SGC  (b) Cross-arch. Acc. from GTrans  (c) GCond across different steps.\\n\\nFigure 4: **The impact of initialization** under different condensation ratios (a, b) and the impact across different datasets Cora (a, b) and ogbn-arxiv (c).\\n\\n![Impact of initialization](image)\\n\\n(a) GCond on Cora (2.60%)  (b) GCond on Cora (0.26%)  (c) GCond on ogbn-arxiv (0.50%)\\n\\n**Key Takeaways 5:** Current GC methods exhibit significant performance variability when transferred to different backbone architectures. Involving the entire training process potentially may lead to encoding backbone-specific details in the condensed datasets.\\n\\n**Key Takeaways 6:** Despite their strong performance in general graph learning tasks, transformers surprisingly yield suboptimal results in graph condensation.\\n\\n### 3.5 Initialization Impact (RQ5)\\n\\nWe evaluate 5 distinct initialization strategies, namely: Random Noise, Random Sample, Center, K-Center, and K-Means. The results of GCond on Cora and ogbn-arxiv are shown in Figure 4. Detailed settings and additional results can be found in Appendix B.5.\\n\\nAs shown in Figure 4(a) and Figure 4(b), the choice of the initialization method can significantly influence the efficiency of the condensation process but with little impact on the final accuracy. For instance, using Center on Cora reduces the average time to reach the same accuracy by approximately 25% compared to Random Sample and 71% compared to Random Noise. However, this speed advantage diminishes as the scale of the condensed graph increases. Additionally, different datasets have their preferred initialization methods for optimal performance. For example, Center is generally faster for Cora condensed by GCond while K-Means performs better on ogbn-arxiv.\\n\\n**Key Takeaways 7:** Different datasets have their preferred initialization methods for optimal performance even for the same GC method.\\n\\n**Key Takeaways 8:** The initialization mechanism primarily affects the convergence speed with little impact on the final performance. The smaller the condensed graph, the greater the influence of different initialization strategies on the convergence speed.\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 9, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.6 Efficiency and Scalability (RQ6)\\n\\nIn this subsection, we evaluate the condensation time and memory consumption of GC methods. The results on ogbn-arxiv are shown in Figure 5, where the x-axis denotes the overall condensation time (min) when achieving the best validation performance, the y-axis denotes the test accuracy (%), the inner size of the marker represents the peak CPU memory usage (MB), while the outer size represents the peak GPU memory usage (MB). As we can observe, the gradient matching methods have higher time and space consumption compared to other types of methods. However, Table 2 shows that current gradient and distribution matching GC methods may trigger OOM (Out of Memory) errors on large datasets with high condensation ratios, making them unsuitable for large-scale scenarios, which contradicts the goal of applying graph condensation to extremely large graphs. More detailed results in Appendix B.6.\\n\\n**Key Takeaways 9**: GC methods that rely on backbones and full-scale data training have large time and space consumption.\\n\\n4 Future Directions\\n\\nNotwithstanding the promising results, there are some directions worthy to explore in the future:\\n\\n**Theory of optimal condensation.** According to our findings, GC methods are striving to achieve better performance with smaller condensed dataset sizes but it\u2019s not necessarily true that larger compressed datasets lead to better results. How to trade off between dataset size, information condensation, and information preservation, and whether there exists a theory of Pareto-optimal condensation in the graph condensation process, are future research directions.\\n\\n**Condensation for more complex graph data.** Current GC methods are predominantly tailored to the simplest types of graphs, overlooking the diversity of graph structures such as heterogeneous graphs, directed graphs, hypergraphs, signed graphs, dynamic graphs, text-rich graphs, etc. There is a pressing need for research on graph condensation methods that cater to more complex graph data.\\n\\n**Task-Agnostic graph condensation.** Task-agnostic GC methods could greatly enhance flexibility and utilization in graph data analysis, promoting versatility across various domains. Current methods often depend on downstream labels or task-specific training. Future research should focus on developing task-agnostic, unsupervised, or self-supervised GC methods that preserve crucial structural and semantic information independently of specific tasks or datasets.\\n\\n**Improving the efficiency and scalability of graph condensation methods.** Efficient and scalable GC methods are crucial yet challenging to design. Most current methods combine condensation with full training, making them resource-heavy and less scalable. Decoupling these processes could significantly enhance GC\u2019s efficiency and scalability, broadening its use across various domains.\\n\\n5 Conclusion and Future Works\\n\\nThis paper introduces a comprehensive graph condensation benchmark, GC-Bench, by integrating and comparing 12 methods across 12 datasets covering varying types and scopes. We conduct extensive experiments to reveal the performance of GC methods in terms of effectiveness, transferability, and efficiency. We implement an library (https://github.com/RingBDStack/GC-Bench) that incorporates all the aforementioned protocols, baseline methods, datasets, and scripts to reproduce the results in this paper. The GC-Bench library offers a comprehensive and unbiased platform for evaluating current methods and facilitating future research. In this study, we mainly evaluate the performance of GC methods for the node classification and graph classification task, which is widely adopted in the previous literature. In the future, we plan to extend the GC-Bench with broader coverage of datasets and tasks, providing further exploration of the generalization ability of GC methods. We will update the benchmark regularly to reflect the most recent progress in GC methods.\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 10, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nThe corresponding author is Jianxin Li. This work is supported by the NSFC through grants No.62225202 and No.62302023, the Fundamental Research Funds for the Central Universities, CAAI-MindSpore Open Fund, developed on OpenI Community. This work is also supported in part by NSF under grants III-2106758, and POSE-2346158.\\n\\nReferences\\n\\n[1] Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Dc-bench: Dataset condensation benchmark. In NeurIPS, 2022.\\n\\n[2] Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. arXiv: Learning, arXiv: Learning, 2016.\\n\\n[3] Kaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu. Deep Anomaly Detection on Attributed Networks, page 594\u2013602. 2019.\\n\\n[4] Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. Journal of molecular biology, 2003.\\n\\n[5] Junfeng Fang, Xinglin Li, Yongduo Sui, Yuan Gao, Guibin Zhang, Kun Wang, Xiang Wang, and Xiangnan He. Exgc: Bridging efficiency and explainability in graph condensation. arXiv preprint arXiv:2402.05962, 2024.\\n\\n[6] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. arXiv preprint arXiv:1903.02428, 2019.\\n\\n[7] Tao-yang Fu, Wang-Chien Lee, and Zhen Lei. Hin2vec: Explore meta-paths in heterogeneous information networks for representation learning. In CIKM, pages 1797\u20131806, 2017.\\n\\n[8] Jian Gao and Jianshe Wu. Multiple sparse graphs condensation. Knowledge-Based Systems, 278:110904, 2023.\\n\\n[9] Xinyi Gao, Tong Chen, Yilong Zang, Wentao Zhang, Quoc Viet Hung Nguyen, Kai Zheng, and Hongzhi Yin. Graph condensation for inductive node representation learning. In ICDE, 2024.\\n\\n[10] Xinyi Gao, Junliang Yu, Wei Jiang, Tong Chen, Wentao Zhang, and Hongzhi Yin. Graph condensation: A survey. arXiv preprint arXiv:2401.11720, 2024.\\n\\n[11] Mridul Gupta, Sahil Manchanda, Sayan Ranu, and Hariprasad Kodamana. Mirage: Model-agnostic graph distillation for graph classification. In ICLR, 2024.\\n\\n[12] William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In NeurIPS, 2017.\\n\\n[13] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In NeurIPS, 2020.\\n\\n[14] Wei Jin, Xianfeng Tang, Haoming Jiang, Zheng Li, Danqing Zhang, Jiliang Tang, and Bing Yin. Condensing graphs via one-step gradient matching. In SIGKDD, 2022.\\n\\n[15] Wei Jin, Lingxiao Zhao, Shi-Chang Zhang, Yozen Liu, Jiliang Tang, and Neil Shah. Graph condensation for graph neural networks. In ICLR, 2021.\\n\\n[16] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.\\n\\n[17] Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308, 2016.\\n\\n[18] Johannes Klicpera, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In ICLR, 2018.\\n\\n[19] Xinglin Li, Kun Wang, Hanhui Deng, Yuxuan Liang, and Di Wu. Attend who is weak: Enhancing graph condensation via cross-free adversarial training. arXiv preprint arXiv:2311.15772, 2023.\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 11, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[20] Mengyang Liu, Shanchuan Li, Xinshi Chen, and Le Song. Graph condensation via receptive field distribution matching. *arXiv preprint arXiv:2206.13697*, 2022.\\n\\n[21] Yang Liu, Deyu Bo, and Chuan Shi. Graph condensation via eigenbasis matching. In *ICML*, 2024.\\n\\n[22] Yilun Liu, Ruihong Qiu, and Zi Huang. Cat: Balanced continual graph learning with graph condensation. In *ICDM*, pages 1157\u20131162. IEEE, 2023.\\n\\n[23] Yilun Liu, Ruihong Qiu, and Zi Huang. Gcondenser: Benchmarking graph condensation. *arXiv preprint arXiv:2405.14246*, 2024.\\n\\n[24] Yilun Liu, Ruihong Qiu, Yanran Tang, Hongzhi Yin, and Zi Huang. Puma: Efficient continual graph learning with graph condensation. *arXiv preprint arXiv:2312.14439*, 2023.\\n\\n[25] Qingsong Lv, Ming Ding, Qiang Liu, Yuxiang Chen, Wenzheng Feng, Siming He, Chang Zhou, Jianguo Jiang, Yuxiao Dong, and Jie Tang. Are we really making much progress?: Revisiting, benchmarking and refining heterogeneous graph neural networks. In *SIGKDD*, 2021.\\n\\n[26] Runze Mao, Wenqi Fan, and Qing Li. Gcare: Mitigating subgroup unfairness in graph condensation through adversarial regularization. *Applied Sciences*, 13(16):9166, 2023.\\n\\n[27] Christopher J. Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. *arXiv: Learning, arXiv: Learning*, Jul 2020.\\n\\n[28] Qiying Pan, Ruofan Wu, Tengfei Liu, Tianyi Zhang, Yifei Zhu, and Weiqiang Wang. Fedgkd: Unleashing the power of collaboration in federated graph neural networks. *arXiv preprint arXiv:2309.09517*, 2023.\\n\\n[29] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks. In *ICLR*, 2019.\\n\\n[30] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In *ICLR*, 2018.\\n\\n[31] Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjin Wang, and Yu Sun. Masked label prediction: Unified message passing model for semi-supervised classification, 2021.\\n\\n[32] Nikil Wale, Ian A Watson, and George Karypis. Comparison of descriptor spaces for chemical compound retrieval and classification. *Knowledge and Information Systems*, 2008.\\n\\n[33] Chun Wang, Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, and Chengqi Zhang. Attributed graph clustering: A deep attentional embedding approach. In *IJCAI*, 2019.\\n\\n[34] Lin Wang, Wenqi Fan, Jiatong Li, Yao Ma, and Qing Li. Fast graph condensation with structure-based neural tangent kernel. In *The Web Conference*, 2024.\\n\\n[35] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu. Heterogeneous graph attention network. In *The Web Conference*, 2019.\\n\\n[36] Max Welling. Herding dynamical weights to learn. In *ICML*, pages 1121\u20131128, 2009.\\n\\n[37] Felix Wu, Tianyi Zhang, Amauri H. Souza, Christopher Fifty, Tao Yu, and Kilian Q. Weinberger. Simplifying graph convolutional networks. *arXiv: Learning, arXiv: Learning*, 2019.\\n\\n[38] Yifan Wu, Jiawei Du, Ping Liu, Yuewei Lin, Wenqing Cheng, and Wei Xu. Dd-robustbench: An adversarial robustness benchmark for dataset distillation. *arXiv preprint arXiv:2403.13322*, 2024.\\n\\n[39] Hongjia Xu, Liangliang Zhang, Yao Ma, Sheng Zhou, Zhuonan Zheng, and Bu Jiajun. A survey on graph condensation. *arXiv preprint arXiv:2402.02000*, 2024.\\n\\n[40] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In *ICLR*, 2018.\\n\\n[41] Zhe Xu, Yuzhong Chen, Menghai Pan, Huiyuan Chen, Mahashweta Das, Hao Yang, and Hanghang Tong. Kernel ridge regression-based graph dataset distillation. In *SIGKDD*, pages 2850\u20132861, 2023.\\n\\n[42] Beining Yang, Kai Wang, Qingyun Sun, Cheng Ji, Xingcheng Fu, Hao Tang, Yang You, and Jianxin Li. Does graph distillation see like vision dataset counterpart? In *NeurIPS*, 2023.\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 12, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[43] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna. Graphsaint: Graph sampling based inductive learning method. In *ICLR*, 2020.\\n\\n[44] Tianle Zhang, Yuchen Zhang, Kun Wang, Kai Wang, Beining Yang, Kaipeng Zhang, Wenqi Shao, Ping Liu, Joey Tianyi Zhou, and Yang You. Two trades is not baffled: Condense graph via crafting rational gradient matching. *arXiv preprint arXiv:2402.04924*, 2024.\\n\\n[45] Yuchen Zhang, Tianle Zhang, Kai Wang, Ziyao Guo, Yuxuan Liang, Xavier Bresson, Wei Jin, and Yang You. Navigating complexity: Toward lossless graph condensation via expanding window matching. *CoRR*, abs/2402.05011, 2024.\\n\\n[46] Jianan Zhao, Xiao Wang, Chuan Shi, Zekuan Liu, and Yanfang Ye. Network schema preserving heterogeneous information network embedding. In *IJCAI*, 2020.\\n\\n[47] Xin Zheng, Miao Zhang, Chunyang Chen, Quoc Viet Hung Nguyen, Xingquan Zhu, and Shirui Pan. Structure-free graph condensation: From large-scale graphs to condensed graph-free data. In *NeurIPS*, 2023.\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 13, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Checklist\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? [Yes]\\n   (b) Did you describe the limitations of your work? [Yes] See Sec. 5.\\n   (c) Did you discuss any potential negative societal impacts of your work? [No]\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n   (b) Did you include complete proofs of all theoretical results? [N/A]\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Appendix C.\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Appendix A.\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] See Section 3.\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix A.4.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes] See Section 2.1 and Appendix A.2.\\n   (b) Did you mention the license of the assets? [Yes] See Appendix C.\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\\n   (d) Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? [Yes] See Appendix C.\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See Appendix C.\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 14, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Details of GC-Bench\\n\\nA.1 Datasets\\n\\nThe evaluation node-level datasets include 5 homogeneous datasets (3 transductive datasets, i.e., Cora, Citeseer [16] and ogbn-arxiv [13], and 2 inductive datasets, i.e., Flickr [43] and Reddit [12]) and 2 heterogeneous datasets (ACM [46] and DBLP [7]). The evaluation graph-level datasets include 5 datasets (NCII [32], DD [4], ogbg-molbace [13], ogbg-molhiv [13], ogbg-molbbbp [13]).\\n\\nWe utilize the standard data splits provided by PyTorch Geometric [6] and the Open Graph Benchmark (OGB) [13] for our experiments. For datasets in TUDataset [27], we split the data into 10% for testing, 10% for validation, and 80% for training. For ACM and DBLP datasets, we follow the settings outlined in [25]. Dataset statistics are shown in Table A1.\\n\\nTable A1: Dataset statistics. For heterogeneous datasets, the features are from the target nodes (papers in ACM and authors in DBLP).\\n\\n| Dataset       | #Nodes / #Avg. Nodes | #Edges / #Avg. Edges | #Classes | #Features / Graphs |\\n|---------------|----------------------|----------------------|----------|--------------------|\\n| Node-level    |                      |                      |          |                    |\\n| Cora          | 2,708                | 5,429                | 7        | 1,433              |\\n| Citeseer      | 3,327                | 4,732                | 6        | 3,703              |\\n| ogbn-arxiv    | 169,343              | 1,166,243            | 40       | 128                |\\n| Flickr        | 89,250               | 899,756              | 7        | 500                |\\n| Reddit        | 232,965              | 57,307,946           | 210      | 602                |\\n| ACM           | 10,942               | 547,872              | 3        | 1,902              |\\n| DBLP          | 37,791               | 170,794              | 4        | 334                |\\n| Graph-level   |                      |                      |          |                    |\\n| ogbg-molhiv   | 25.5                 | 54.9                 | 2        | 41,127             |\\n| ogbg-molbace  | 34.1                 | 36.9                 | 2        | 1,513              |\\n| ogbg-molbbbp  | 24.1                 | 26.0                 | 2        | 2,039              |\\n| NCII          | 29.8                 | 32.3                 | 2        | 4,110              |\\n| DD            | 284.3                | 715.7                | 2        | 1,178              |\\n\\nA.2 Algorithms\\n\\nWe summarize the current GC algorithms in Table A2. We choose 12 representative ones for evaluation in this paper including Random, K-Center [30], Herding [36], GCond [15], DosCond [14], SGDD [42], GCDM [20], DM [24], SFGC [47], GEOM [45], KiDD [41], Mirage [11]. We will continue to update and improve the benchmark to include more algorithms. Here we introduce the GC algorithms in detail:\\n\\n- **Traditional Core-set Methods**\\n  - **Random**: For node classification tasks, nodes are randomly selected to form a new subgraph. For graph classification, the graphs are randomly selected to create a new subset.\\n  - **Herding [36]**: The nodes or graphs are selected samples that are closest to the cluster center.\\n  - **K-Center [30]**: Nodes or graphs are chosen such that they have the minimal distance to the nearest cluster center, which is generated using the K-Means Clustering method.\\n\\n- **Gradient Matching Methods**\\n  - **GCond [15]**: In GCond, the optimization of the synthetic dataset is framed as a bi-level problem. It adapts a gradient matching scheme to match the gradients of GNN parameters between the condensed and original graphs, while optimizing the model\u2019s performance on the datasets. For generating the synthetic adjacency matrix, GCond employs a Multi-Layer Perceptron (MLP) to model the edges by using node features as input, maintaining the correlations between node features and graph structures.\\n  - **DosCond [14]**: In DosCond, the gradient matching scheme only matches the network gradients for model initialization $\\\\theta_0$ while discarding the training trajectory of $\\\\theta_t$, which accelerated the entire condensation process by only informing the direction to update the\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 15, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table A2: Summary of Graph Condensation (GC) algorithms. We also provide public access to the official algorithm implementations. \u201cKRR\u201d is short for Kernel Ridge Regression and \u201cCTC\u201d is short for computation tree compression. \u201cGNN\u201d is short for Graph, \u201cGNTK\u201d is short for graph neural tangent kernel, \u201cSD\u201d is short for spectral decomposition. \u201cNC\u201d is short for node classification, \u201cLP\u201d is short for link prediction, \u201cAD\u201d is short for anomaly detection, and \u201cGC\u201d is short for graph classification.\\n\\n| Taxonomy          | Method       | Initialization | Backbone Model | Downstream Task | Code   | Venue       |\\n|-------------------|--------------|----------------|----------------|-----------------|--------|-------------|\\n| Traditional       | Random       | \u2014              | \u2014              | \u2014               | \u2014      | \u2014           |\\n| Methods           | Herding [36] | \u2014              | \u2014              | \u2014               | link   | ICLR, 2009  |\\n|                   | K-Center [30]| \u2014              | \u2014              | \u2014               | link   | ICLR, 2018  |\\n|                   | GCond [15]   | Random Sample  | GNN            | NC              | link   | ICLR, 2021  |\\n|                   | DosCond [14] | Random Sample  | GNN            | NC, GC          | link   | SIGKDD, 2022|\\n|                   | MSGC [8]     | Zero Matrix    | GNN            | NC              | \u2014      | KBS, 2023   |\\n|                   | SGDD [42]    | Random Sample  | GNN            | NC, LP, AD      | link   | NeurIPS, 2023|\\n|                   | GCARe [26]   | \u2014              | GNN            | NC              | \u2014      | Appl. Sci. 2023|\\n|                   | CTRL [44]    | K-Means        | GNN            | NC, GC          | link   | arXiv, 2024 |\\n|                   | GroC [19]    | Random Sample  | GNN            | NC, GC          | \u2014      | arXiv, 2023 |\\n|                   | EXGC [5]     | Random Sample  | GNN            | NC              | link   | WWW, 2024   |\\n|                   | MCond [9]    | Random Sample  | GNN            | NC              | \u2014      | ICDE, 2024  |\\n| Distribution      | GCDM [20]    | Random Sample  | GNN            | NC              | \u2014      | arXiv, 2022 |\\n| Matching          | DM [22, 24]  | Random Sample  | GNN            | NC              | \u2014      | ICDM, 2023  |\\n|                   | GDEM [21]    | Eigenbasis Approximation | SD | NC | link | ICML, 2024 |\\n|                   | FedGKD [28]  | Random Noise   | GNN            | NC              | \u2014      | arXiv, 2023 |\\n|                   | SFGC [47]    | K-Center       | GNN            | NC              | link   | NeurIPS, 2023|\\n|                   | GEOM [45]    | K-Center       | GNN            | NC              | link   | ICML, 2024  |\\n| KRR               | GC-SNTK [34] | Random Noise   | GNTK           | NC              | link   | WWW, 2024   |\\n|                   | KiDD [41]    | Random Sample  | GNTK           | GC              | link   | SIGKDD, 2023|\\n| CTC               | Mirage [11]  | \u2014              | GNN            | GC              | link   | ICLR, 2024  |\\n\\n1 The code repository for EXGC is not fully developed.\\n\\nsynthetic dataset. DosCond also modeled the discrete graph structure as a probabilistic model and each element in the adjacency matrix follows a Bernoulli distribution.\\n\\n- **MSGC [8]**: MSGC condenses a large-scale graph into multiple small-scale sparse graphs, leveraging neighborhood patterns as substructures to enable the construction of various connection schemes. This process enriches the diversity of embeddings generated by GNNs, enhances the representation power of GNNs on complex graphs.\\n\\n- **SGDD [42]**: SGDD uses graphon approximation to ensure that the structural information of the original graph is retained in the synthetic, condensed graph. The condensed graph structure is optimized by minimizing the optimal transport (OT) distance between the original structure and the condensed structure.\\n\\n- **GCARe [26]**: GCARe addresses biases in condensed graphs by regularizing the condensation process, ensuring that the knowledge of different subgroups is distilled fairly into the resulting graphs.\\n\\n- **CTRL [44]**: CTRL clusters each class of the original graph into sub-clusters and uses these as initial value for the synthetic graph. By considering both the direction and magnitude of gradients during gradient matching, it effectively minimizes matching errors during the condensation phase.\\n\\n- **GroC [19]**: GroC uses an adversarial training (bi-level optimization) framework to explore the most impactful parameter spaces and employs a Shock Absorber operator to apply targeted adversarial perturbation.\\n\\n- **EXGC [5]**: EXGC leverages Mean-Field variational approximation to address inefficiency in the current gradient matching schemes and uses the Gradient Information Bottleneck objective to tackle node redundancy.\\n\\n- **MCond [9]**: MCond addresses the limitations of traditional condensed graphs in handling unseen data by learning a one-to-many node mapping from original nodes to synthetic nodes and uses an alternating optimization scheme to enhance the learning of synthetic graph and mapping matrix.\\n\\n- **Distribution Matching Methods**\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 16, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"- **GCDM** [20]: GCDM synthesizes small graphs with receptive fields that share a similar distribution to the original graph, achieved through a distribution matching loss quantified by maximum mean discrepancy (MMD).\\n\\n- **DM** [22, 24]: DM can be regarded as a one-step variant of GCDM. In DM, the optimization is centered on the initial parameters. Notably, in [22] and [24], DM does not learn any structures for efficiency. However, for better comparison in our experiments, we continue to learn the adjacency matrix.\\n\\n- **FedGKD** [28]: FedGKD trains models on condensed local graphs within each client to mitigate the potential leakage of the training set membership. FedGKD features a Federated Graph Neural Network framework that enhances client collaboration using a task feature extractor for graph data distillation and a task relator for globally-aware model aggregation.\\n\\n**\u2022 Trajectory Matching Methods**\\n\\n- **SFGC** [47]: SFGC uses trajectory matching instead of a gradient matching scheme. It first trains a set of GNNs on original graphs to acquire and store an expert parameter distribution offline. The expert trajectory guides the optimization of the condensed graph-free data. The generated graphs are evaluated using closed-form solutions of GNNs under the graph neural tangent kernel (GNTK) ridge regression, avoiding iterative GNN training.\\n\\n- **GEOM** [45]: GEOM makes the first attempt toward lossless graph condensation using curriculum-based trajectory matching. A homophily-based difficulty score is assigned to each node and the easy nodes are learned in the early stages while more difficult ones are learned in the later stages. On top of that, GEOM incorporated a Knowledge Embedding Extraction (KEE) loss into a matching loss.\\n\\n**\u2022 Kernel Ridge Regression Methods**\\n\\n- **GC-SNTK** [34]: GC-SNTK introduces a Structure-based Neural Tangent Kernel (SNTK) to capture graph topology, replacing the inner GNNs training in traditional GC paradigm, avoiding multiple iterations.\\n\\n- **KiDD** [41]: KiDD uses kernel ridge regression (KRR) with a graph neural tangent kernel (GNTK) for graph-level tasks. To enhance efficiency, KiDD introduces LiteGNTK, a simplified GNTK, and proposes KiDD-LR for faster low-rank approximation and KiDD-D for handling discrete graph topology using the Gumbel-Max reparameterization trick. We use KiDD-LR for experiments as it has generally demonstrated better performance compared to KiDD-D.\\n\\n**\u2022 Computation Tree Compression Methods**\\n\\n- **Mirage** [11]: Mirage decomposes graphs in datasets into a collection of computation trees and then mines frequently co-occurring trees from this set. Mirage then uses aggregation functions (MEANPOOL, SUMPOOL, etc.) on the embeddings of the root node of each tree to approximate the graph embedding.\\n\\n### A.3 Hyper-Parameter Setting\\n\\nFor the implementation of various graph condensation methods, we adhere to the default parameters as specified by the authors in their respective original implementations. This approach ensures that our results are comparable to those reported in the foundational studies. For condensation ratios that were not explored in the original publications, we employ a grid search strategy to identify the optimal hyperparameters within the predefined search space. This includes experimenting with various combinations, such as differing learning rates for the feature optimizer and the adjacency matrix optimizer. The corresponding hyperparameter space are shown in Table A3.\\n\\n### A.4 Computation resources\\n\\nAll experiments were conducted on a high-performance GPU cluster to ensure a fair comparison. The cluster consists of 32 identical dell-GPU nodes, each featuring 256GB of memory, 2 Intel Xeon processors, and 4 NVIDIA Tesla V100 GPUs, with each GPU having 64 GB of GPU memory. If any experiment setting exceeds the GPU memory limit, it is reported as out-of-memory (OOM).\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 17, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table A3: Hyperparameter search space of different methods\\n\\n| Method          | Hyperparameter | Values                                      |\\n|-----------------|----------------|---------------------------------------------|\\n| General Settings| Learning Rate  | 0.1, 0.01, 0.001, 0.0001, 0.00001           |\\n|                 | Epochs         | 300, 400, 500, 800, 1000, 2000, 3000, 4000, 5000 |\\n|                 | Layers         | 2, 3                                        |\\n|                 | Dropout Rate   | 0, 0.05, 0.1, 0.5, 0.6, 0.7, 0.8            |\\n|                 | Weight Decay   | 0, 0.0005                                   |\\n|                 | Hidden Units   | 128, 256                                    |\\n|                 | Pooling        | sum, mean                                   |\\n|                 | Activation     | LeakyReLU, ReLU, Sigmoid, Softmax           |\\n|                 | Batch Size     | (16,6000)                                   |\\n| SGDD            | mx_size        | 50, 100                                     |\\n|                 | opt_scale      | 5, 10                                       |\\n| GCond, DosCond, SGDD, GCDM, DM | outer loop | 1, 2, 5, 10, 15, 20 |\\n| GCond, SGDD, GCDM | inner loop | 1, 5, 10, 15, 20 |\\n| SFGC, GEOM      | expert_epochs  | 50, 70, 100, 350, 600, 800, 1000, 1500, 1600, 1900 |\\n|                 | start_epoch    | 10, 20, 50, 100, 200, 300                   |\\n|                 | teacher_epochs | 800, 1000, 1200, 2400, 3000                 |\\n| GEOM            | lam            | 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95        |\\n|                 | T              | 250, 500, 600, 800, 1000, 1200               |\\n|                 | scheduler      | linear, geom, root                          |\\n| KiDD            | scale          | uniform, degree                             |\\n|                 | rank           | 8, 16, 32                                   |\\n|                 | orth_reg       | 0.01, 0.001, 0.0001                          |\\n\\nA.5 Discussion on Existing Benchmarks\\n\\nTo the best of our knowledge, the only concurrent work is GCondenser [23]. The comparison of GCondenser and our GC-Bench are list in Table A4. GCondenser [23] focus the node-level GC methods for node classification on homogeneous graphs with limited evaluation dimensions in terms of performance and time efficiency. Our GC-Bench analyzes more GC methods on a wider variety of datasets (both homogeneous and heterogeneous) and tasks (node classification, graph classification), encompassing both node-level and graph-level methods. In addition to performance and efficiency analysis, we further explore the transferability across different tasks (link prediction, node clustering, anomaly detection) and backbones (GNN models and the popular Graph Transformer). With GC-Bench covering more in-depth investigation over a wider scope, we believe it will provide valuable insights into existing works and future directions.\\n\\nB Settings and Additional Results\\n\\nIn this section, we provide more details of the experimental settings and the additional results for the proposed 6 research questions, respectively.\\n\\nB.1 Settings and Additional Results of Performance Comparison (RQ1)\\n\\nB.1.1 Comparison Setting\\n\\nNode Classification Graph Dataset Setting. We compared ten state-of-the-art GC methods. The selection of the condensation ratio $r$ is based on the labeling rates of different datasets. For datasets like Cora and Citeseer, the labeling rates are less than 50%, we select $r$ as a proportion of the labeling rate, specifically at $\\\\{5\\\\%, 10\\\\%, 25\\\\%, 50\\\\%, 75\\\\%, 100\\\\%\\\\}$. For datasets like ogbn-arxiv, and inductive datasets where all nodes in the training graphs are labeled, with a relatively higher labeling rate, $r$ is chosen to be $\\\\{5\\\\%, 10\\\\%, 25\\\\%, 50\\\\%, 75\\\\%, 100\\\\%\\\\}$. Corresponding condensation rates are shown in Table B2.\\n\\nGraph Classification Graph Dataset Setting. We compared three state-of-the-art GC algorithms on graph classification datasets: DosCond [14], KiDD [41], and Mirage [11]. Mirage [11] does not condense datasets into unified graphs measurable by Graphs per Class (GPC) as DosCond [14] and\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 18, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table A4: Comparison of GCondenser and GC-Bench\\n\\n| Algorithms          | GCondenser                  | GC-Bench                  |\\n|---------------------|-----------------------------|---------------------------|\\n| Traditional Core-set Methods | Random, K-Center            | Random, K-Center, Herding |\\n| Gradient Matching   | GCond, DosCond, SGDD        | GCond, DosCond, SGDD      |\\n| Distribution Matching | GCDM, DM                    | GCDM, DM                  |\\n| Trajectory Matching | SFGC                        | SFGC, GEOM                |\\n| KRR                 | \u2014                           | KiDD                      |\\n| CTC                 | \u2014                           | Mirage                    |\\n\\n| Datasets            | GCondenser                  | GC-Bench                  |\\n|---------------------|-----------------------------|---------------------------|\\n| Node-level Homogeneous | Cora, Citeseer, ogbn-arxiv  | Cora, Citeseer, ogbn-arxiv |\\n|                     | Flickr, Reddit, PubMed      | Flickr, Reddit            |\\n| Node-level Heterogeneous | \u2014                           | ACM, DBLP                 |\\n| Graph-level         | \u2014                           | NCII, DD, ogbg-molbace    |\\n|                     |                             | ogbg-molbbp, ogbg-molhiv  |\\n\\n| Tasks               | GCondenser                  | GC-Bench                  |\\n|---------------------|-----------------------------|---------------------------|\\n| Nodel-level         | node classification         | node classification       |\\n|                     |                             | link prediction           |\\n|                     |                             | node clustering           |\\n|                     |                             | anomaly detection         |\\n| Graph-level         | \u2014                           | graph classification      |\\n\\n| Evaluation Dimensions | GCondenser                  | GC-Bench                  |\\n|-----------------------|-----------------------------|---------------------------|\\n| Condensation Ratios   | \u2713                           | \u2713                         |\\n| Impact of Structure   | \u2713                           | \u2713                         |\\n| Impact of Initialization | \u2713                           | \u2713                         |\\n| Backbone Trans.       | SGC and GCN transfer to SGC, GCN, GraphSAGE, APPNP, ChebyNet, MLP | SGC, GCN and Graph Transformer transfer to SGC, GCN, GraphSAGE, APPNP, ChebyNet, MLP, Graph Transformer |\\n| Task Trans.           | \u2014                           | \u2014                         |\\n| Time                  | \u2713                           | \u2713                         |\\n| Space                 | \u2014                           | \u2713                         |\\n\\nKiDD [41] do. Therefore, we measure the condensed dataset size by storing its elements in .pt format, similar to DosCond [14] and KiDD [41]. We select the Mirage-condensed dataset size closest to DosCond\u2019s as the corresponding GPC. KiDD [41] generally occupies more disk space than DosCond under the same GPC. The size of Mirage datasets is determined by two parameters: the number of GNN layers ($L$) and the frequency threshold $\\\\Theta$. We fix $L = 2$, consistent with the 2-layer model used for validation, and employ a grid search strategy to identify the threshold combination that yields a dataset size closest to the targeted GPC. The corresponding disk space, GPC, and threshold choices are presented in Table B1. Note that for small thresholds, the MP Tree search algorithms used in Mirage [11] may reach recursive limits. Consequently, in DD and ogbg-molbace, certain GPCs lack corresponding threshold values.\\n\\n**Heterogeneous Graph Dataset Setting.** Due to the absence of condensation methods specifically for heterogeneous graphs, we convert heterogeneous datasets into homogeneous graphs for condensation, focusing on target nodes. We uniformly summed the adjacency matrices corresponding to various meta-paths as in [25], and applied zero-padding to match the maximum feature dimension as well as one-hot encoding for nodes without features. Specifically, in GEOM [45], when calculating heterophily, all nodes without labels (non-target nodes) are assigned the same distinct label, ensuring a consistent heterophily calculation.\\n\\n**B.1.2 Additional Results**\\n\\nThe graph classification performance on GCN is shown in Table B3. DosCond [14] with GCN demonstrates significant advantages in 12 out of 25 cases, while KiDD [41] underperforms in most scenarios. Notably, DosCond [14] and Mirage [11] even outperform the results of the whole dataset.\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 19, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table B1: Comparison of Disk Size and Graph per Class (GPC) for condensed datasets between *Mirage* and *DosCond*.\\n\\n| Dataset          | Graph/Cls | Mirage Disk Size (Bytes) | DosCond Disk Size (Bytes) | Class 0 Threshold | Class 1 Threshold |\\n|------------------|-----------|--------------------------|---------------------------|-------------------|-------------------|\\n|                  |           |                          |                           |                   |                   |\\n| *NCI1* [32]      | 1         | 14,455                   | 18,425                    | 451               | 441               |\\n|                  | 5         | 81,622                   | 82,745                    | 351               | 381               |\\n|                  | 10        | 142,228                  | 162,301                   | 301               | 291               |\\n|                  | 20        | 195,609                  | 324,035                   | 251               | 231               |\\n|                  | 50        | 995,277                  | 806,403                   | 201               | 171               |\\n| *DD* [4]         | 1         | 38,352                   | 855,077                   | 15                | 9                 |\\n|                  | 5         | \u2014                        | 4,265,957                 | \u2014                 | \u2014                 |\\n|                  | 10        | \u2014                        | 8,529,583                 | \u2014                 | \u2014                 |\\n|                  | 20        | \u2014                        | 17,056,751                | \u2014                 | \u2014                 |\\n|                  | 50        | \u2014                        | 42,638,383                | \u2014                 | \u2014                 |\\n| *ogbg-molbace* [13] | 1         | 13,836                   | 14143                     | 120               | 90                |\\n|                  | 5         | 60,047                   | 60,927                    | 230               | 80                |\\n|                  | 10        | 106,077                  | 119,497                   | 120               | 80                |\\n|                  | 20        | 232,191                  | 236,489                   | 140               | 70                |\\n|                  | 50        | \u2014                        | 587,337                   | \u2014                 | \u2014                 |\\n| *ogbg-molbbbp* [13] | 1         | 8,817                    | 8,831                     | 29                | 198               |\\n|                  | 5         | 34,699                   | 34,175                    | 49                | 109               |\\n|                  | 10        | 66,433                   | 65,929                    | 30                | 90                |\\n|                  | 20        | 104,091                  | 129,289                   | 20                | 80                |\\n|                  | 50        | 324,425                  | 319,369                   | 17                | 87                |\\n| *ogbg-molhiv* [13] | 1         | 9,606                    | 9,717                     | 8,000             | 250               |\\n|                  | 5         | 54,669                   | 38,837                    | 1,760             | 170               |\\n|                  | 10        | 74,524                   | 75,263                    | 1,680             | 130               |\\n|                  | 20        | 148,028                  | 148,095                   | 1,420             | 110               |\\n|                  | 50        | 330,498                  | 366,463                   | 800               | 110               |\\n\\nTable B2: Different condensation ratios of transductive datasets. For heterogeneous datasets, the number of nodes in the original graph is the sum of all types of nodes.\\n\\n| Ratio (r) | Cora | Citeseer | ACM | DBLP |\\n|-----------|------|----------|-----|------|\\n| 5%        | 0.26%| 0.18%    | 0.003%| 0.002%|\\n| 10%       | 0.52%| 0.36%    | 0.007%| 0.004%|\\n| 25%       | 1.30%| 0.90%    | 0.013%| 0.007%|\\n| 50%       | 2.60%| 1.80%    | 0.033%| 0.019%|\\n| 75%       | 3.90%| 2.70%    | 0.066%| 0.037%|\\n| 100%      | 5.20%| 3.60%    | 0.332%| 0.186%|\\n\\non *ogbg-molbace*. For *Mirage* [11], due to the algorithm\u2019s recursive depth under low threshold parameters, we have only one result corresponding to GPC 1 on *DD*. However, this single result already surpasses all datasets condensed by *KiDD* [41] and the dataset with GPC 1 condensed by *DosCond*.\\n\\n### B.2 Settings and Additional Results of Structure in Graph Condensation (RQ2)\\n\\n#### B.2.1 Experimental Settings\\n\\nThe homophily ratio we use is the edge homophily ratio, which represents the fraction of edges that connect nodes with the same labels. It can be calculated as:\\n\\n$$\\n\\\\mathcal{H}(G) = \\\\frac{1}{|\\\\mathcal{E}|} \\\\sum_{(j,k) \\\\in \\\\mathcal{E}} 1(y_j = y_k), \\\\quad i \\\\in \\\\mathcal{V},\\n$$\\n\\n(A.1)\\n\\nwhere $\\\\mathcal{V}$ is the node set, $\\\\mathcal{E}$ is the edge set, $|\\\\mathcal{E}|$ is the number of edges in the graph, $y_i$ is the label of node $i$ and $1(\\\\cdot)$ is the indicator function. A graph is typically considered to be highly homophilous when $\\\\mathcal{H}$ is large (typically, $0.5 \\\\leq \\\\mathcal{H} \\\\leq 1$), such as *Cora* and *Reddit*. Conversely, a graph with a low edge homophily ratio is considered to be heterophilous, such as *Flickr*. \\n\\n19\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 20, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table B3: **Graph classification performance on GCN** (mean\u00b1std) across datasets with varying condensation ratios $r$. The best results are shown in **bold** and the runner-ups are shown in **underlined**. Red color highlights entries that exceed the whole dataset values.\\n\\n| Dataset | Graph /Cls | Ratio($r$) | Traditional Core-set methods | Gradient | KRR | CTC | Whole Dataset |\\n|---------|------------|------------|-----------------------------|----------|-----|-----|---------------|\\n|         |            |            | Random | Herding | K-Center | DosCond | KiDD | Mirage |               |\\n| NCII Acc. (%) | 1 | 0.06% | 53.30\u00b10.6 | 55.20\u00b12.6 | 55.20\u00b12.6 | **57.30\u00b10.9** | 49.30\u00b11.1 | 49.10\u00b10.9 | 71.1\u00b10.8 |\\n|          | 5 | 0.24% | 55.00\u00b11.4 | 56.50\u00b10.9 | 53.20\u00b10.6 | **58.40\u00b11.4** | 56.10\u00b11.0 | 49.60\u00b12.2 |               |\\n|          | 10 | 0.49% | **58.10\u00b12.2** | **58.60\u00b10.8** | 57.00\u00b12.6 | 57.80\u00b11.6 | 57.50\u00b11.1 | 48.60\u00b10.1 |               |\\n|          | 20 | 0.97% | 54.40\u00b10.8 | 59.10\u00b11.1 | **60.10\u00b11.3** | **60.10\u00b13.2** | 56.40\u00b10.6 | 48.70\u00b10.0 |               |\\n|          | 50 | 2.43% | 56.80\u00b11.1 | 58.70\u00b11.1 | **64.40\u00b10.9** | 58.20\u00b12.8 | 59.90\u00b10.6 | 48.60\u00b10.1 |               |\\n| DD Acc. (%) | 1 | 0.21% | 59.70\u00b11.5 | 66.90\u00b12.8 | 66.90\u00b12.8 | 68.30\u00b16.6 | 58.60\u00b12.4 | **71.20\u00b16.6** | 78.4\u00b11.7 |\\n|          | 5 | 1.06% | 61.90\u00b11.1 | 66.20\u00b12.5 | 62.00\u00b11.7 | **73.10\u00b12.2** | 58.60\u00b11.1 | - |               |\\n|          | 10 | 2.12% | 63.70\u00b12.8 | 68.00\u00b13.6 | 62.50\u00b12.3 | **71.30\u00b18.3** | 61.60\u00b13.8 | - |               |\\n|          | 20 | 4.25% | 64.70\u00b15.3 | 69.70\u00b18.8 | 63.10\u00b11.9 | **73.00\u00b15.8** | 62.60\u00b11.4 | - |               |\\n|          | 50 | 10.62% | 66.60\u00b12.1 | 68.50\u00b11.4 | **68.90\u00b11.8** | **74.20\u00b13.6** | 59.30\u00b10.0 | - |               |\\n| ogbg-molbace ROC-AUC | 1 | 0.17% | 0.510\u00b10.03 | 0.515\u00b10.04 | 0.517\u00b10.04 | **0.658\u00b10.06** | 0.568\u00b10.04 | **0.733\u00b10.02** | 0.711\u00b10.019 |\\n|          | 5 | 0.83% | 0.612\u00b10.03 | 0.653\u00b10.03 | 0.508\u00b10.08 | **0.691\u00b10.06** | 0.356\u00b10.02 | **0.760\u00b10.02** |               |\\n|          | 10 | 1.65% | 0.620\u00b10.05 | 0.658\u00b10.04 | 0.646\u00b10.04 | **0.702\u00b10.05** | 0.542\u00b10.02 | **0.759\u00b10.02** |               |\\n|          | 20 | 3.31% | 0.642\u00b10.03 | 0.631\u00b10.05 | 0.575\u00b10.03 | **0.659\u00b10.04** | 0.526\u00b10.01 | **0.761\u00b10.03** |               |\\n|          | 50 | 8.26% | **0.677\u00b10.01** | **0.629\u00b10.03** | 0.576\u00b10.08 | **0.714\u00b10.03** | 0.446\u00b10.02 | - |               |\\n| ogbg-molbbbp ROC-AUC | 1 | 0.12% | 0.534\u00b10.04 | 0.560\u00b10.01 | 0.560\u00b10.01 | **0.600\u00b10.02** | 0.504\u00b10.04 | **0.600\u00b10.02** | 0.646\u00b10.013 |\\n|          | 5 | 0.61% | 0.561\u00b10.01 | 0.574\u00b10.02 | **0.585\u00b10.05** | 0.579\u00b10.05 | 0.561\u00b10.04 | **0.609\u00b10.01** |               |\\n|          | 10 | 1.23% | 0.566\u00b10.01 | **0.590\u00b10.02** | **0.598\u00b10.02** | 0.550\u00b10.03 | 0.550\u00b10.05 | 0.517\u00b10.028 |               |\\n|          | 20 | 2.45% | 0.593\u00b10.03 | 0.568\u00b10.01 | 0.545\u00b10.09 | 0.590\u00b10.05 | **0.594\u00b10.02** | **0.626\u00b10.03** |               |\\n|          | 50 | 6.13% | 0.587\u00b10.007 | 0.579\u00b10.022 | **0.621\u00b10.011** | 0.598\u00b10.024 | **0.603\u00b10.01** | 0.602\u00b10.018 |               |\\n| ogbg-molhiv ROC-AUC | 1 | 0.01% | 0.733\u00b10.008 | 0.727\u00b10.012 | 0.727\u00b10.012 | **0.734\u00b10.002** | 0.725\u00b10.007 | 0.728\u00b10.012 | 0.750\u00b10.010 |\\n|          | 5 | 0.03% | 0.729\u00b10.006 | 0.720\u00b10.018 | **0.739\u00b10.01** | 0.730\u00b10.008 | 0.738\u00b10.003 | 0.717\u00b10.003 |               |\\n|          | 10 | 0.06% | 0.724\u00b10.011 | 0.726\u00b10.014 | 0.723\u00b10.012 | **0.730\u00b10.007** | 0.731\u00b10.008 | **0.735\u00b10.028** |               |\\n|          | 20 | 0.12% | 0.723\u00b10.015 | **0.726\u00b10.015** | 0.724\u00b10.01 | 0.733\u00b10.007 | 0.703\u00b10.097 | 0.710\u00b10.016 |               |\\n|          | 50 | 0.30% | 0.712\u00b10.014 | **0.723\u00b10.019** | 0.721\u00b10.012 | **0.731\u00b10.011** | 0.723\u00b10.011 | 0.718\u00b10.022 |               |\\n\\n*Mirage* cannot directly generate graphs with the required ratio. Thus, we search the parameter space and aligned the generated graph to match *DosCond*\u2019s disk usage as substitution (see Appendix B.1).\\n\\nWe also calculate the homophily ratio of condensed datasets. Since the condensed datasets have weighted edges, we first sparsify the graph by removing all edges with weights less than 0.05, then calculate the homophily ratio by adjusting the fraction to a weighted fraction, which can be represented as:\\n\\n$$\\n\\\\mathcal{H}(G) = \\\\frac{\\\\sum_{(j,k) \\\\in E} w_{jk} \\\\mathbf{1}(y_j = y_k)}{\\\\sum_{(j,k) \\\\in E} w_{jk}}, \\\\quad i \\\\in \\\\mathcal{V},\\n$$\\n\\n(A.2)\\n\\nwhere $w_{jk}$ is the weight of the edge between nodes $j$ and $k$.\\n\\n### B.2.2 Additional Results\\n\\nThe results of homophily ratios of condensed datasets are shown in Table B4. It appears that condensed datasets often struggle to preserve the homophily properties of the original datasets. For instance, in the case of the heterophilous dataset *Flickr*, an increase in the homophily rate is observed under most methods and ratios.\\n\\n| Dataset | Whole Dataset | Ratio ($r$) | GCDM | DM | DosCond | GCond | SGDD |\\n|---------|---------------|------------|------|----|---------|-------|------|\\n| Cora    | 0.81          | 1.30%      | 0.76 | 0.88 | 0.20    | 0.64  | 0.19 |\\n|         |               | 2.60%      | 0.11 | 0.74 | 0.16    | 0.55  | 0.19 |\\n|         |               | 5.20%      | 1.00 | 0.21 | 0.15    | 0.62  | 0.15 |\\n| Citeseer| 0.74          | 0.90%      | 0.16 | 0.75 | 0.19    | 0.57  | 0.14 |\\n|         |               | 1.80%      | 0.08 | 0.30 | 0.20    | 0.36  | 0.19 |\\n|         |               | 3.60%      | 1.00 | 0.34 | 0.15    | 0.22  | 0.15 |\\n| Flickr  | 0.24          | 0.05%      | 0.28 | 0.29 | 0.25    | 0.28  | 0.32 |\\n|         |               | 0.50%      | 0.29 | 0.22 | 0.08    | 0.28  | 0.30 |\\n|         |               | 1.00%      | 0.36 | 0.18 | 0.06    | 0.28  | 0.26 |\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 21, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We visualize the condensed datasets using force-directed graph visualization, as shown in Figure B.1, Figure B.2, and Figure B.3. Since SFGC [47] and GEOM [45] synthesize edge-free datasets, we do not visualize the datasets they condensed. As shown in the visualization, graphs condensed by different methods exhibit distinct structural characteristics. For example, distribution matching methods often result in less pronounced community structures compared to other methods.\\n\\nWe also visualize the node degree distribution of the original graph and the condensed graphs in Figure B.4. Note that the graphs condensed by GCDM [20] and DM [24] are dense and each edge has an extremely small weight under most situations, the degree of each node is also small. We observe that the degree distributions of most condensed datasets deviate significantly from the original graph. Among them, SGDD [42] demonstrates a relatively similar degree distribution to that of the original graph.\\n\\n![Figure B.1: Visualization of the Condensed Citeseer (1.80%) Dataset. Only the top 20% of edges ranked by weight are visualized.](image)\\n\\n![Figure B.2: Visualization of the Condensed Cora (2.60%) Dataset. Only the top 20% of edges ranked by weight are visualized.](image)\\n\\n![Figure B.3: Visualization of the Condensed Flickr (0.50%) Dataset. Only the top 1% of edges ranked by weight are visualized.](image)\\n\\n### B.3 Settings and Additional Results of Transferability in Different Tasks (RQ3)\\n\\n#### B.3.1 Link Prediction\\n\\nFor the link prediction task, we utilize a graph autoencoder (GAE)[17] based on Graph Convolutional Networks (GCN[16]). The GAE consists of a two-layer GCN encoder that creates node embeddings. During training, we enhance the dataset by randomly adding negative links and use a decoder to perform binary classification on edges. During evaluation, we test the model using the test set of the\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 22, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"original graph. Since trajectory matching methods do not generate any edges, we do not use them for link prediction tasks. The results of condensed datasets on the link prediction task are shown in Table B5. We observe that most condensed datasets underperform in link prediction tasks, especially on ogbn-arxiv and Flickr. Most methods\u2019 condensed datasets consistently underperform compared to traditional core-set methods, indicating room for improvement.\\n\\nB.3.2 Node Clustering\\n\\nFor the node clustering tasks on condensed datasets, we utilize DAEGC [33] to train on synthetic datasets condensed using the node classification task. We then test the trained model on the original large-scale datasets and include the results of other methods on the original graph for comprehensive comparison. Due to the performance degradation of GAT with large neighborhood sizes, we use GCN as the encoder. Performance metrics include Accuracy (Acc.), Normalized Mutual Information (NMI), F-score, and Adjusted Rand Index (ARI).\\n\\nTo fully leverage the condensed datasets, we include the results of node clustering with pertaining. In this experiment, the GCN encoder is first trained on the synthetic datasets with a node classification task, which incorporates the synthetic labels\u2019 information. Using the pre-trained GCN as an encoder, we then perform node clustering on the synthetic datasets and the original graph. Results of node clustering tasks, both without and with pertaining are shown in Table B6 and Table B7 respectively.\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 23, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table B5: **Link Prediction Accuracy (%)** of different condensed datasets. The best results are shown in **bold**.\\n\\n| Dataset | Ratio (r) | Random | Herding | K-Center | GCDM | DM | DosCond | GCond | SGDD | Whole Dataset |\\n|---------|-----------|--------|---------|----------|------|----|---------|-------|------|---------------|\\n| Citeseer | 0.90%     | 0.52   | 0.52    | 0.55     | 0.53 | 0.53| 0.50    | 0.65  | **0.69** | 0.82          |\\n|         | 1.80%     | 0.52   | 0.52    | 0.54     | 0.51 | 0.52| 0.51    | 0.51  | **0.67** | 0.78          |\\n|         | 3.60%     | 0.54   | 0.53    | 0.53     | 0.53 | 0.53| 0.53    | 0.53  | **0.61** |              |\\n| Cora    | 1.30%     | 0.58   | 0.54    | 0.58     | 0.72 | 0.71| 0.67    | 0.61  | 0.51    | 0.70          |\\n|         | 2.60%     | 0.55   | 0.55    | 0.56     | 0.69 | 0.67| 0.58    | **0.77** | 0.62   | 0.78          |\\n|         | 5.20%     | 0.57   | 0.56    | 0.58     | 0.70 | **0.71** | 0.59 | 0.65  | 0.56          |\\n| ogbn-arxiv | 0.05%   | 0.76   | 0.68    | 0.67     | 0.66 | 0.68| 0.63    | 0.60  | 0.70    |              |\\n|         | 0.20%     | 0.72   | 0.72    | **0.73** | 0.72 | 0.72| 0.69    | 0.71  | 0.51    | 0.75          |\\n|         | 0.50%     | **0.74** | 0.73    | **0.74** | 0.71 | 0.73| 0.72    | 0.72  | 0.70    |              |\\n| Flickr  | 0.05%     | 0.55   | 0.54    | 0.53     | **0.60** | 0.53| 0.52    | 0.54  | 0.51    | 0.70          |\\n|         | 0.20%     | 0.63   | 0.63    | 0.63     | 0.63 | 0.51| 0.53    | 0.57  | **0.70** | 0.75          |\\n|         | 0.50%     | **0.70** | 0.68    | **0.70** | 0.56 | 0.65| 0.62    | 0.67  | 0.61    |              |\\n\\nWe observe that most condensed datasets perform worse in the node clustering task compared to the original dataset. However, when additional information from the pretraining of the node classification task on condensed dataset is utilized, the results of node clustering significantly improve. Notably, some datasets in Table B6 exhibit identical results with the Adjusted Rand Index (ARI) being 0 or even negative. This occurs because the clustering results do not match the number of classes in the labels, requiring manual splitting of clusters in such scenarios. An ARI of 0 indicates that the clustering result is as good as random, while a negative ARI suggests it is worse than random.\\n\\n### B.3.3 Anomaly Detection\\n\\nFor the anomaly detection tasks, we generate two types of anomalies, **Contextual Anomalies** and **Structural Anomalies**, following the method described in [3]. We set the anomaly rate to 0.05; if the condensed dataset is too small, we inject one contextual anomaly and two structural anomalies.\\n\\n**Contextual Anomalies**: Each outlier is generated by randomly selecting a node and substituting its attributes with those from another node with the maximum Euclidean distance in attribute space.\\n\\n**Structural Anomalies**: Outliers are generated by randomly selecting a small group of nodes and making them fully connected, forming a clique. The nodes in this clique are then regarded as structural outliers. This process is repeated iteratively until a predefined number of cliques are generated.\\n\\nWe conduct anomaly detection by training a DOMINANT model [3], which features a shared graph convolutional encoder, a structure reconstruction decoder, and an attribute reconstruction decoder. Initially, we inject predefined anomalies into the test set of the original graph and use it for evaluation across different condensed datasets derived from this graph. The model is then trained on these condensed datasets, which were injected with specific types of anomalies before training. The DOMINANT model measures reconstruction errors as anomaly scores for both the graph structure and node attributes, combining these scores to detect anomalies. The results are evaluated using the ROC-AUC metric, as shown in Table B8 and B9.\\n\\n### B.4 Settings and Additional Results of Transferability across Backbone Model Architectures (RQ4)\\n\\n#### B.4.1 Experimental Settings\\n\\nFor transferability evaluation, we use different models as backbones to test the condensation methods. For distribution matching methods, two backbone models with shared parameters are used to generate embeddings that are matched. For trajectory matching methods, two backbone models are used to generate expert trajectories and student trajectories, respectively, to match the corresponding parameters. For gradient matching methods, two backbone models with shared parameters are used to generate gradients for real and synthetic data. Models are selected using grid-searched hyperparameters. The details of the backbone architecture are as follows:\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 24, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table B6: **Node Clustering without Pretraining Results** on *Cora* and *Citeseer* with varying condensation ratios ($r$). The best results are highlighted in **bold**, the runner-ups are underlined, and the best results of condensed datasets are shaded in gray.\\n\\n| Methods     | Ratio ($r$) | **Cora** | | **Cora** |\\n|-------------|-------------|----------|----------|----------|\\n|             |             | Acc.     | NMI      | ARI      | F1       | Acc.     | NMI      | ARI      | F1       |\\n| K-means     | Full        | 54.4     | 31.2     | 28.5     | 41.3     | 50.0     | 31.7     | 37.6     | 23.9     |\\n| DAEGC [33]  | Full        | 67.2     | 39.7     | 41.0     | 63.6     |          |          |          |          |\\n| Random      | 0.90%       | 40.6     | 19.1     | 17.5     | 36.0     | 1.30%    | 36.6     | 13.5     | 9.0      | 34.3     |\\n|             | 1.80%       | 38.3     | 14.8     | 13.6     | 34.5     | 2.60%    | 33.5     | 13.9     | 7.1      | 33.4     |\\n|             | 3.60%       | 41.8     | 18.1     | 16.9     | 39.4     | 5.20%    | 30.2     | 0.4      | 0.0      | 6.8      |\\n| Herding     | 0.90%       | 41.9     | 16.9     | 15.3     | 40.0     | 1.30%    | 37.4     | 18.2     | 11.7     | 35.0     |\\n|             | 1.80%       | 44.9     | 18.7     | 16.0     | 41.1     | 2.60%    | 36.6     | 16.4     | 11.9     | 34.0     |\\n|             | 3.60%       | 58.1     | 27.8     | 29.2     | 52.3     | 5.20%    | 26.7     | 13.7     | 2.9      | 20.6     |\\n| K-Center    | 0.90%       | 41.4     | 16.9     | 16.2     | 38.6     | 1.30%    | 30.2     | 0.4      | 0.0      | 6.8      |\\n|             | 1.80%       | 44.1     | 18.1     | 18.1     | 38.8     | 2.60%    | 30.2     | 0.4      | 0.0      | 6.8      |\\n|             | 3.60%       | 22.8     | 1.8      | 1.2      | 20.9     | 5.20%    | 30.2     | 0.4      | 0.0      | 6.8      |\\n| GCDM        | 0.90%       | 23.5     | 2.1      | 1.1      | 17.7     | 1.30%    | 30.2     | 0.4      | 0.0      | 6.8      |\\n|             | 1.80%       | 45.3     | 19.1     | 17.7     | 42.9     | 2.60%    | 30.2     | 0.4      | 0.0      | 6.8      |\\n|             | 3.60%       | 25.9     | 4.5      | 3.5      | 20.0     | 5.20%    | 30.2     | 0.4      | 0.0      | 6.8      |\\n| DM          | 0.90%       | 28.6     | 10.2     | 6.3      | 25.1     | 1.30%    | 30.2     | 0.4      | 0.0      | 6.8      |\\n|             | 1.80%       | 57.1     | 31.4     | 26.2     | 49.5     | 2.60%    | 30.2     | 0.4      | 0.0      | 6.8      |\\n|             | 3.60%       | 44.3     | 20.6     | 17.0     | 38.6     | 5.20%    | 29.6     | 16.2     | 7.7      | 23.4     |\\n| DosCond     | 0.90%       | 61.8     | 34.0     | 34.7     | 55.9     | 1.30%    | 46.6     | 36.7     | 27.3     | 41.2     |\\n|             | 1.80%       | 59.6     | 33.0     | 32.6     | 50.3     | 2.60%    | 49.9     | 39.3     | 27.9     | 44.3     |\\n|             | 3.60%       | 57.8     | 32.0     | 30.2     | 54.8     | 5.20%    | 44.6     | 40.9     | 25.1     | 37.3     |\\n| GCond       | 0.90%       | 56.5     | 27.3     | 26.8     | 50.6     | 1.30%    | 30.2     | 0.4      | 0.0      | 6.8      |\\n|             | 1.80%       | 45.4     | 24.0     | 20.0     | 43.9     | 2.60%    | 30.2     | 0.4      | 0.0      | 6.8      |\\n|             | 3.60%       | 42.5     | 23.6     | 20.8     | 38.2     | 5.20%    | 33.2     | 17.9     | 8.8      | 25.5     |\\n| SGDD        | 0.90%       | 46.7     | 19.9     | 18.8     | 43.4     | 1.30%    | 42.1     | 23.5     | 17.7     | 39.2     |\\n|             | 1.80%       | 56.8     | 27.4     | 27.6     | 52.8     | 2.60%    | 54.4     | 31.8     | 26.4     | 50.2     |\\n|             | 3.60%       | 47.7     | 19.0     | 16.9     | 45.3     | 5.20%    | 30.1     | 0.4      | -0.1     | 6.8      |\\n| GEOM        | 0.90%       | 41.4     | 16.9     | 16.2     | 38.6     | 1.30%    | 40.7     | 16.9     | 11.6     | 37.3     |\\n|             | 1.80%       | 44.1     | 18.1     | 18.1     | 38.8     | 2.60%    | 30.8     | 12.9     | 9.3      | 29.2     |\\n|             | 3.60%       | 22.8     | 1.8      | 1.2      | 20.9     | 5.20%    | 35.6     | 16.0     | 11.5     | 33.6     |\\n\\n- **MLP**: MLP is a simple neural network consisting of fully connected layers. The MLP we use is structured similarly to a GCN but without the adjacency matrix input, effectively functioning as a standard multi-layer perceptron (MLP). The MLP we adopted consists of 2 layers with 256 hidden units in each layer.\\n\\n- **GCN [16]**: GCN is the most common architecture for evaluating condensed datasets in mainstream GC methods. GCN defines a localized, first-order approximation of spectral graph convolutions, effectively aggregating and combining features from a node\u2019s local neighborhood, leveraging the graph\u2019s adjacency matrix to update node representations through multiple layers. We adhere to the setting in previous work [15] and use 2 graph convolutional layers for node classification, each followed by ReLu activation and batch normalization depending on the configuration. For graph classification, we use a 3-layer GCN with a sum pooling function. The hidden unit size is set to 256.\\n\\n- **SGC [37]**: SGC is the standardized model used for condensation in previous works. It can be regarded as a simplified version of GCN, which ignores the nonlinear activation function but still keeps two Graph Convolution layers, thereby preserving similar graph filtering behaviors. In the experiments, we use 2-layer SGC with no bias.\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 25, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table B7: **Node Clustering with Pretraining Results** on *Cora* and *Citeseer* with varying condensation ratios ($r$). The best results are highlighted in **bold** and the runner-ups are underlined.\\n\\n| Methods   | Ratio ($r$) | *Citeseer* | | | *Cora* | | |\\n|-----------|-------------|------------|---|---|---|---|---|\\n|           |             | Acc. | NMI | ARI | F1 | Acc. | NMI | ARI | F1 |\\n| Random    | 0.90%       | 27.3 | 5.5 | 4.7 | 24.6 | 1.30% | 41.7 | 15.8 | 13.5 | 37.3 |\\n|           | 1.80%       | 32.7 | 9.7 | 7.8 | 31.4 | 2.60% | 36.5 | 14.6 | 9.1 | 35.4 |\\n|           | 3.60%       | 44.6 | 16.0 | 14.1 | 43.0 | 5.20% | 44.4 | 23.5 | 14.9 | 45.7 |\\n| Herding   | 0.90%       | 36.7 | 12.8 | 11.1 | 34.4 | 1.30% | 40.7 | 18.3 | 12.9 | 40.0 |\\n|           | 1.80%       | 36.8 | 13.1 | 10.2 | 36.2 | 2.60% | 36.1 | 14.6 | 8.7 | 34.9 |\\n|           | 3.60%       | 39.4 | 16.9 | 14.1 | 38.1 | 5.20% | 35.0 | 16.6 | 10.9 | 36.3 |\\n| K-Center  | 0.90%       | 33.7 | 9.7 | 8.3 | 29.5 | 1.30% | 41.8 | 19.3 | 14.5 | 39.2 |\\n|           | 1.80%       | 37.6 | 15.6 | 13.9 | 34.9 | 2.60% | 38.5 | 20.8 | 14.8 | 38.3 |\\n|           | 3.60%       | 41.7 | 17.1 | 14.3 | 40.5 | 5.20% | 38.5 | 17.4 | 10.9 | 36.3 |\\n| GCDM      | 0.90%       | 31.1 | 9.6 | 6.6 | 27.3 | 1.30% | 21.3 | 3.7 | 1.7 | 20.1 |\\n|           | 1.80%       | 33.1 | 11.9 | 11.1 | 30.4 | 2.60% | 27.0 | 10.9 | 5.7 | 26.7 |\\n|           | 3.60%       | 39.7 | 18.0 | 15.2 | 34.4 | 5.20% | 30.0 | 12.4 | 7.0 | 29.6 |\\n| DM        | 0.90%       | 36.5 | 15.7 | 12.9 | 30.0 | 1.30% | 27.3 | 9.3 | 8.3 | 29.5 |\\n|           | 1.80%       | 37.1 | 10.6 | 8.6 | 31.4 | 2.60% | 20.8 | 3.3 | 0.9 | 19.0 |\\n|           | 3.60%       | 29.2 | 6.0 | 4.0 | 23.6 | 5.20% | 23.5 | 4.8 | 1.6 | 16.3 |\\n| DosCond   | 0.90%       | 62.7 | 35.9 | 35.1 | 60.6 | 1.30% | 60.2 | 42.5 | 29.4 | 61.2 |\\n|           | 1.80%       | 45.2 | 17.9 | 15.4 | 40.8 | 2.60% | 44.5 | 30.1 | 16.6 | 46.5 |\\n|           | 3.60%       | 58.6 | 29.6 | 28.5 | 55.8 | 5.20% | 25.4 | 9.8 | 5.0 | 25.0 |\\n| GCond     | 0.90%       | 44.0 | 22.5 | 18.7 | 40.3 | 1.30% | 67.4 | 45.1 | 40.4 | 65.8 |\\n|           | 1.80%       | 58.5 | 30.9 | 29.6 | 54.9 | 2.60% | 63.7 | 44.5 | 36.2 | 61.8 |\\n|           | 3.60%       | 52.0 | 26.8 | 22.5 | 46.6 | 5.20% | 60.9 | 47.1 | 37.1 | 56.0 |\\n| SGDD      | 0.90%       | 46.7 | 23.5 | 19.1 | 42.3 | 1.30% | 65.1 | 44.6 | 37.1 | 64.6 |\\n|           | 1.80%       | 55.4 | 28.0 | 25.8 | 50.9 | 2.60% | 35.7 | 19.2 | 11.7 | 34.8 |\\n|           | 3.60%       | 40.5 | 18.3 | 14.3 | 34.8 | 5.20% | 37.3 | 21.1 | 14.4 | 34.1 |\\n| SFGC      | 0.90%       | 34.2 | 9.8 | 8.4 | 32.2 | 1.30% | 41.2 | 21.2 | 13.9 | 40.2 |\\n|           | 1.80%       | 47.1 | 21.7 | 20.6 | 43.5 | 2.60% | 38.7 | 20.7 | 13.5 | 36.2 |\\n|           | 3.60%       | 48.5 | 23.3 | 21.5 | 44.8 | 5.20% | 37.3 | 21.1 | 14.4 | 34.1 |\\n| GEOM      | 0.90%       | 32.7 | 10.5 | 8.6 | 31.7 | 1.30% | 39.1 | 20.1 | 11.4 | 40.0 |\\n|           | 1.80%       | 48.2 | 23.6 | 22.7 | 45.2 | 2.60% | 32.2 | 14.5 | 8.9 | 29.4 |\\n|           | 3.60%       | 54.2 | 25.7 | 24.9 | 52.1 | 5.20% | 38.1 | 22.0 | 12.7 | 34.7 |\\n\\nTable B8: **Structural Anomaly Detection results (ROC-AUC)** on *Cora* and *Citeseer* with varying condensation ratios. The best results are shown in **bold** and the runner-ups are shown in underline.\\n\\n| Dataset | Ratio ($r$) | Random | Herding | K-Center | GCDM | DM | DosCond | GCond | SGDD | SFGC | GEOM |\\n|---------|-------------|--------|---------|----------|------|----|---------|-------|------|------|------|\\n| Citeseer| 0.90%       | 0.44   | 0.38    | 0.44     | 0.76 | 0.76 | 0.73    | **0.77** | 0.67 | 0.62 | 0.59 |\\n|         | 1.80%       | 0.46   | 0.45    | 0.46     | **0.78** | 0.78 | 0.66    | 0.75  | 0.68 | 0.60 | 0.56 |\\n|         | 3.60%       | 0.44   | 0.40    | 0.44     | **0.76** | 0.76 | 0.70    | 0.74  | 0.75 | 0.59 | 0.57 |\\n| Cora    | 1.30%       | 0.56   | 0.59    | 0.62     | 0.80 | 0.80 | 0.79    | **0.81** | 0.75 | 0.54 | 0.51 |\\n|         | 2.60%       | 0.50   | 0.65    | 0.67     | 0.80 | 0.80 | **0.82** | 0.79  | 0.81 | 0.53 | 0.53 |\\n|         | 5.20%       | 0.65   | 0.55    | 0.67     | **0.82** | 0.82 | 0.81    | 0.71  | 0.54 | 0.55 | 0.55 |\\n\\n- **Cheby** [2]: Cheby utilizes Chebyshev polynomials to approximate the graph convolution operations, which retains the essential graph filtering properties of GCN while reducing the computational complexity. We use a 2-layer Cheby with 256 hidden units and ReLU activation function.\\n\\n- **GraphSAGE** [12]: GraphSAGE is a spatial-based graph neural network that directly samples and aggregates features from a node\u2019s local neighborhood. In the experiments, We use a two-layer architecture and a hidden dimension size of 256 while using a mean aggregator.\\n\\n- **APPNP** [18]: APPNP leverages personalized PageRank to propagate information throughout the graph. This method decouples the neural network used for prediction from the propagation mechanism, enabling the use of personalized PageRank for message passing. In the experiments,\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 26, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table B9: **Contextual Anomaly Detection results (ROC-AUC)** on *Cora* and *Citeseer* with varying condensation ratios. The best results are shown in **bold** and the runner-ups are shown in *underline*.\\n\\n| Dataset | Ratio (r) | Random | Herding | K-Center | GCDM | DM | DosCond | GCond | SGDD | SFGC | GEOM |\\n|---------|-----------|--------|---------|----------|------|----|---------|-------|------|------|------|\\n| Citeseer | 0.90%     | 0.62   | 0.60    | 0.62     | 0.65 | 0.65| 0.55    | 0.70  | **0.74** | 0.62 | 0.59 |\\n|         | 1.80%     | 0.60   | 0.54    | 0.60     | 0.64 | 0.65| 0.58    | **0.68** | 0.68 | 0.52 | 0.59 |\\n|         | 3.60%     | 0.57   | 0.56    | 0.57     | **0.68** | **0.68** | 0.59 | **0.68** | 0.52 | 0.59 | 0.57 |\\n| Cora    | 1.30%     | 0.52   | 0.48    | 0.53     | 0.52 | 0.52| 0.45    | **0.54** | 0.41 | **0.54** | 0.51 |\\n|         | 2.60%     | 0.50   | 0.45    | 0.54     | 0.54 | 0.54| 0.56    | 0.55  | **0.57** | 0.53 | 0.53 |\\n|         | 5.20%     | 0.56   | 0.58    | **0.59** | 0.55 | 0.55| **0.55** | 0.57  | 0.57 | **0.62** | 0.54 | 0.55 |\\n\\nwe use a 2-layer model implemented with ReLU activation and sparse dropout to condense and evaluate.\\n\\n- **GIN [40]**: GIN aggregates features by linearly combining the node features with those of their neighbors, achieving classification power as strong as the Weisfeiler-Lehman graph isomorphism test. We specifically applied a 3-layer GIN with a mean pooling function to compress and evaluate graph classification datasets. For the datasets *DD* and *NCII*, we use negative log-likelihood loss function for training and softmax activation in the final layer. For *ogbg-molhiv*, *ogbg-molbbbp* and *ogbg-molbace*, we use binary cross-entropy with logits and sigmoid activation in the final layer.\\n\\n- **Graph Transformer [31]**: The Graph Transformer leverages the self-attention mechanism of the Transformer to capture long-range dependencies between nodes in a graph. It employs multi-head self-attention to dynamically weigh the importance of different nodes, effectively modeling complex relationships within the graph. We use a two-layer model with layer normalization and gated residual connections, following the settings outlined in [31].\\n\\n### B.4.2 Additional Results\\n\\nTable B10 shows the node classification accuracy of datasets condensed by traditional core-set methods, which is backbone-free, evaluated across different backbone architectures on *Cora*.\\n\\nTable B10: **Node Classification Accuracy (%)** of core-set datasets across different backbone architectures on *Cora* (2.6%).\\n\\n| Methods   | SGC | GCN | GraphSage | APPNP | Cheby | GTrans. | MLP |\\n|-----------|-----|-----|-----------|-------|-------|---------|-----|\\n| Full Dataset | 80.8 | 80.8 | 80.8 | 80.3 | 78.8 | 69.6 | 81.0 |\\n| Herding   | 74.8 | 74.0 | 74.1 | 73.3 | 69.6 | 65.4 | 74.1 |\\n| K-Center  | 72.5 | 72.4 | 71.8 | 71.5 | 63.0 | 64.3 | 72.2 |\\n| Random    | 71.7 | 72.4 | 71.6 | 71.3 | 65.3 | 62.7 | 71.6 |\\n\\n### B.5 Settings and Additional Results of Initialization Impacts (RQ5)\\n\\n#### B.5.1 Experimental Settings\\n\\nThe details of evaluated initialization mechanism are as follows:\\n\\n- **Random Sample.** We randomly select features from nodes in the original graph that correspond to the same label, using these features to initialize the synthetic nodes.\\n\\n- **Random Noise.** Consistent with prevalent dataset condensation methods, we initialize node features by sampling from a Gaussian distribution.\\n\\n- **Center.** This method involves extracting features from nodes within the same label, applying the K-Means clustering algorithm to these features while treating the graph as a singular cluster and utilizing the centroid of this cluster as the initialization point for all synthetic nodes bearing the same label.\\n\\n- **K-Center.** Similar to the Center initialization method, but employ the K-Means Clustering method on original nodes by dividing each class of the original graph nodes into n clusters,\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 27, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where \\\\( n \\\\) is the number of synthetic nodes per class. We select the center of these clusters as the initialization of synthetic nodes in this class.\\n\\n- **K-Means.** Similar to the K-Center initialization method, but instead of using the centroids of clusters to initialize the synthetic dataset, randomly select one node from each cluster to serve as the initial state for the synthetic node.\\n\\n### B.5.2 Additional Results\\n\\nThe performance of different initialization mechanism on *Cora* (2.6%) and *Cora* (0.26%) are shown in Table B11 and Table B12, respectively. It is evident that distribution matching methods are highly sensitive to the choice of initialization, especially when the dataset is condensed to a smaller scale. Additionally, trajectory matching methods perform poorly with random noise initialization and often fail to converge.\\n\\nTable B11: Performance comparison of different initialization on various methods for *Cora* (2.60%). The best results are shown in **bold**.\\n\\n| Methods   | Random Noise | Random Sample | Center | K-Center | K-Means |\\n|-----------|--------------|---------------|--------|----------|---------|\\n| GCDM      | 34.5         | 73.3          | 77.4   | **78.7** | 75.9    |\\n| DM        | 34.5         | 73.7          | 77.7   | **78.1** | 75.9    |\\n| DosCond   | 78.8         | 81.9          | 81.8   | **82.5** | 81.8    |\\n| GCond     | 74.8         | 75.1          | **76.3** | 76.2    | 75.1    |\\n| SGDD      | 81.7         | 81.8          | 82.6   | **82.7** | 82.5    |\\n| SFGC      | 52.5         | 80.7          | 79.7   | 81.5     | **81.8** |\\n| GEOM      | -            | 77.9          | 48.3   | 78.8     | **78.9** |\\n\\nTable B12: Performance comparison of different initialization on various methods for *Cora* (0.26%). The best results are shown in **bold**.\\n\\n| Methods   | Random Noise | Random Sample | Center | K-Center | K-Means |\\n|-----------|--------------|---------------|--------|----------|---------|\\n| GCDM      | 32.3         | 37.8          | **78.7** | 78.7     | 34.3    |\\n| DM        | 32.2         | 38.4          | **77.9** | 77.9     | 34.2    |\\n| DosCond   | 78.7         | **82.4**      | 80.5   | 82.0     | 81.9    |\\n| GCond     | 80.2         | **81.6**      | 80.1   | 81.2     | 80.7    |\\n| SGDD      | 82.2         | 82.2          | **82.7** | 82.7    | 81.5    |\\n| SFGC      | 79.7         | 79.7          | **79.8** | 79.8    | 81.5    |\\n| GEOM      | -            | 49.6          | 51.3   | 51.3     | **65.0** |\\n\\n### B.6 Settings and Additional Results of Efficiency and Scalability (RQ6)\\n\\n#### B.6.1 Experimental Settings\\n\\nFor a fair comparison, all the experiments are conducted on a single NVIDIA A100 GPU. Then we report the overall condensation time (min) when achieving the best validation performance, the peak CPU memory usage (MB) and the peak GPU memory usage (MB).\\n\\n#### B.6.2 Additional Results\\n\\nThe detailed time and space consumption of the node-level GC methods on *ogbn-arxiv* (0.50%) and graph-level GC methods on *ogbg-molhiv* (1 Graph/Cls) are shown in Table B13 and Table B14 respectively. For node-level methods, although trajectory matching methods (*SFGC* [47], *GEOM* [45]) may consume less time and memory due to their offline matching mechanism, the expert trajectories generated before matching can occupy up to 764 GB of space as shown in Table B15, significantly impacting storage requirements. Among all the graph-level GC methods, *Mirage* [11] stands out by not relying on any GPU resources for calculation and can condense data extremely quickly, taking only 1% of the time required by other methods.\\n\\nTable B13: Time and memory consumption of different methods on *ogbn-arxiv* (0.50%).\\n\\n| Consumption | GCDM | DM | DosCond | GCond | SGDD | SFGC | GEOM |\\n|-------------|------|----|---------|-------|------|------|------|\\n| Time (min)  | 212.90 | 57.70 | 117.38 | 266.57 | 226.62 | 245.65 | 148.37 |\\n| Acc. (%)    | 58.09 | 58.09 | 60.73 | 61.28 | 61.51 | 67.13 | 67.29 |\\n| CPU Memory (MB) | 2720.88 | 2708.70 | 5372.60 | 5270.70 | 5426.30 | 3075.30 | 3335.10 |\\n| GPU Memory (MB) | 2719.74 | 2552.63 | 3850.24 | 3850.24 | 8326.35 | 4050.12 | 5308.42 |\\n\\n### C Reproducibility and Limitations\\n\\n**Accessibility and license.** All the datasets, algorithm implementations, and experimental settings are publicly available in our open project ([https://github.com/RingBDStack/GC-Bench](https://github.com/RingBDStack/GC-Bench)). Our\"}"]}
{"id": "ScPgzCZ6Lo", "page_num": 28, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table B14: Time and memory consumption of different methods on \\\\textit{ogbg-molhiv} (1 Graph/Cls).\\n\\n| Consumption       | DosCond | KiDD   | Mirage |\\n|-------------------|---------|--------|---------|\\n| Time (min)        | 218.11  | 202.38 | 2.91    |\\n| Acc. (%)          | 67.41   | 66.44  | 71.09   |\\n| CPU Memory (MB)   | 2666.29 | 3660.79| 752.22  |\\n| GPU Memory (MB)   | 1005.98 | 6776.42| 0.00    |\\n\\nTable B15: Expert trajectory size (GB) for trajectory matching methods.\\n\\n| Citeseer | Cora | ogbn-arxiv |\\n|----------|------|------------|\\n| 129      | 152  | 15         |\\n\\n| Flickr | Reddit | ACM | DBLP |\\n|--------|--------|-----|------|\\n| 21     | 42     | 312 | 764  |\\n\\npackage (codes and datasets) is licensed under the MIT License. This license permits users to freely use, copy, modify, merge, publish, distribute, sublicense, and sell copies of the software, provided that the original copyright notice and permission notice are included in all copies or or substantial portions of the software. The MIT License is widely accepted for its simplicity and permissive terms, ensuring ease of use and contribution to the codes and datasets. We bear all responsibility in case of violation of rights, etc, and confirmation of the data license.\\n\\n**Datasets.** Cora, Citeseer, Flickr, Reddit and DBLP are publicly available online\\\\(^3\\\\) with the MIT license. \\\\textit{ogbn-arxiv}, \\\\textit{ogbg-molbace}, \\\\textit{ogbg-molbbbp} and \\\\textit{ogbg-molhiv} are released by OGB [13] with the MIT license. ACM [46] is the subset hosted in [35] with the MIT license. NCI1 [32] and DD [4] are available in TU Datasets [27] with the MIT license. All the datasets are consented to by the authors for academic usage. All the datasets do not contain personally identifiable information or offensive content.\\n\\n**Limitations.** GC-Bench has some limitations that we aim to address in future work. Our current benchmark is limited to a specific set of graph types and graph tasks and might not reflect the full potential and versatility of GC methods. We hope to implement more GC algorithms for various tasks (e.g. subgraph classification, community detection) on more types of graphs (e.g., dynamic graph, directed graph). Besides, due to resource constraints and the availability of implementations, we could not include some of the latest GC algorithms in our benchmark. We will continuously update our repository to keep track of the latest advances in the field. We are also open to any suggestions and contributions that will improve the usability and effectiveness of our benchmark, ensuring it remains a valuable resource for the IGL research community.\\n\\n\\\\(^3\\\\)https://github.com/pyg-team/pytorch_geometric\"}"]}
