{"id": "Mbd3QxXjq5", "page_num": 1, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset\\n\\nShubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, Igor Gitman\\nNVIDIA\\n\\nAbstract\\n\\nRecent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA [1] and MAmmoTH [2] are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on our proposed prompting novelty, the recent progress in open-source LLMs, and some brute-force scaling, we construct OpenMathInstruct-1, a high-quality math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using the recently released and permissively licensed Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best gpt-distilled models. To support the open-source efforts, we have released our code, models, and the OpenMathInstruct-1 dataset under a commercially permissive license.\\n\\n1 Introduction\\n\\nThe huge development and inference costs associated with general-purpose large language models (LLMs) have led to the rise of smaller, task-specific LLMs. Recent work has proposed creating these domain/task-specific LLMs by generating high-quality synthetic data using powerful closed-source models such as GPT-3.5/4 [3] and training smaller models on the generated distillation data [4, 5, 6]. For mathematical reasoning, our task of interest, all the current state-of-the-art open-source models are gpt-distilled [7, 2, 8, 9]. However, model development recipes relying on proprietary models like GPT-4 can have serious limitations: (a) legal restraints on how the finetuned models can be used [2], (b) generating data with closed-source models is typically costlier than state-of-the-art open-source models, and (c) these recipes lack reproducibility as closed-source model behaviors can vary significantly over time [10].\\n\\nFor developing mathematical reasoning models, why are open-source models not used in place of closed-source models? To answer this, we compare GPT-4 with the Mixtral 8x7B model [11], one of the best open-source LLMs at mathematical reasoning in early 2024, by generating code-interpreter style solutions for two popular mathematical reasoning benchmarks, namely GSM8K [12].\\n\\n---\\n\\n1 Data and models are available at https://huggingface.co/collections/nvidia/openmath-65c5619de2ba059be0775014\\nCode is available at https://github.com/NVIDIA/NeMo-Skills\\nhttps://openai.com/policies/terms-of-use\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 2, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Training set coverage of Mixtral model generated solutions as a function of number of solutions sampled per problem (using temperature of 1.0 and top_p = 0.95). The statistics for the training set coverage of GPT-4 are from [8].\\n\\nand MATH [13]. We use the metric training set coverage (TSC) to compare the models, where TSC measures the number of training problems for which any of the generated solutions leads to the ground truth answer (pass@k). Figure 1 shows the training set coverage (TSC) of the Mixtral model as a function of the number of sampled solutions. For the relatively easier GSM8K benchmark, the Mixtral model\u2019s coverage catches up to GPT-4\u2019s with almost 8x the number of solution samples. For the challenging MATH benchmark, even with 12x the number of solutions, the Mixtral model still has a lower TSC than GPT-4. This gap in the training set coverage reflects the distillation data quality and, hence, the quality of the final fine-tuned model. This explains the preference for GPT-4 in the current distillation pipelines for mathematical reasoning.\\n\\nBridging the coverage gap between GPT-4 and Open-source LLMs: We limit our investigation of open-source LLMs for synthesizing solutions to the Mixtral-base model due to (a) its strong performance on mathematical reasoning tasks compared to other open-source LLMs, and (b) its permissive license. As a first attempt, we use a brute-force approach of sampling several solutions per problem. However, this approach only scales logarithmically, limiting its effectiveness (Figure 1). Next, we explore the approach of targeted solution generation, where we write few-shot prompts focused on specific sections of the training data. Concretely, we write few-shot prompts for each mathematics subject in the MATH dataset and merge the synthesized solutions. The motivation is that these subject-specific few-shot prompts could better target the latent mathematical capabilities of these general-purpose LLMs. Unfortunately, we only find a marginal gain in TSC with this approach (Section 2.2.2). Finally, we utilize the fact that reference text solutions accompany mathematical benchmarks such as MATH and GSM8K. These reference solutions can aid the synthesis of code-interpreter style solutions. We show that using these reference solutions in our few-shot prompt with a slight modification substantially increases the coverage and, consequently, the performance of the fine-tuned model (Section 2.2.3 and 4.1.2).\\n\\nOur solution synthesis experiments result in OpenMathInstruct-1, a collection of 1.8M problem-solution pairs. OpenMathInstruct-1 has a training set coverage of 93% for MATH and 99.9% for GSM8K. Table 1 shows that compared to previous mathematical reasoning fine-tuning datasets, OpenMathInstruct-1 is at least four times bigger, and, even more importantly, it is permissively licensed, allowing unrestricted usage by future work. To illustrate the quality of OpenMathInstruct-1, we train and release a range of models based on Mistral-7B [14], Llama 2 [15], and CodeLlama [16]. In particular, the CodeLlama-70B model fine-tuned on a subset of OpenMathInstruct-1, referred to as OpenMath-CodeLlama-70B, achieves a score of 84.6% on GSM8K and 50.7% on MATH. These scores are competitive with the current best GPT-distilled models. Finally, to support the open-source efforts in this direction, we have publicly released all our fine-tuned models, code, and the OpenMathInstruct-1 dataset, along with a further 6.6M incorrect sampled solutions under a commercially permissive license.\\n\\nhttps://mistral.ai/news/mixtral-of-experts/\\n\\nThe incorrect solution trajectories can be used to train verifier models [12, 17, 18].\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 3, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison of OpenMathInstruct-1 with mathematical reasoning fine-tuning datasets used by current state-of-the-art open-source models. OpenMathInstruct-1 is 4x bigger than the current largest dataset, MetaMathQA, and is the only one, except Lila, with a permissive license. Datasets marked with * have not been publicly released.\\n\\n| Dataset                  | Size  | Generating LM (Permissive License) |\\n|--------------------------|-------|-----------------------------------|\\n| Lila [19]                | 272K  | - (\u2713)                             |\\n| MathInstruct [2]         | 262K  | GPT-4 (\u2717)                         |\\n| MetaMathQA [1]           | 395K  | GPT-3.5 (\u2717)                       |\\n| MathCodeInstruct [7]     | 80K   | GPT-4 + Self (\u2717)                  |\\n| WizardMath* [20]         | 96K   | GPT-3.5 (\u2717)                       |\\n| ToRA* [8]                | 16K   | GPT-4 (\u2717)                         |\\n| OpenMathInstruct-1 (Ours)| 1.8M  | Mixtral (\u2713)                       |\\n\\n2 Training Data Synthesis\\n\\n2.1 Overview\\n\\nSetup. Let \\\\( \\\\mathcal{X} = \\\\{(q_1, a_1), \\\\ldots, (q_N, a_N)\\\\} \\\\) be a typical mathematical reasoning training dataset, where \\\\( q_i \\\\) and \\\\( a_i \\\\) denote the \\\\( i \\\\)th question and answer respectively. Optionally, the training dataset may include reference text solution \\\\( t_i \\\\), which illustrates a trajectory from \\\\( q_i \\\\) to \\\\( a_i \\\\) using mathematical principles. Besides the data, we assume access to a foundation LLM like Mixtral-base. The goal is to generate diverse, high-quality solutions for the training set problems using the LLM: a popular recipe for reasoning tasks [21, 22]. Recent work has also attempted augmenting training set problems [1, 2], but we limit our exploration to solution synthesis for existing problems in the benchmark.\\n\\nSolution Format. We use the code-interpreter format for the synthesized solutions (see Figure 5 in Appendix for a sample solution). The code-interpreter format interweaves natural language reasoning with Python code blocks. It thus combines the computation precision of coding environments with the expressiveness of natural language reasoning, which is particularly suitable for mathematical reasoning tasks [8, 23]. To demarcate the start and end of a code block, we use the strings \\\\( \\\\langle \\\\text{llm-code} \\\\rangle \\\\) and \\\\( \\\\langle /\\\\text{llm-code} \\\\rangle \\\\). A code block is followed by its execution block, which is demarcated by \\\\( \\\\langle \\\\text{llm-code-output} \\\\rangle \\\\) and \\\\( \\\\langle /\\\\text{llm-code-output} \\\\rangle \\\\). During inference, the model invokes the Python interpreter to run the preceding code block after generating \\\\( \\\\langle /\\\\text{llm-code} \\\\rangle \\\\), appends the execution result in between the \\\\( \\\\langle \\\\text{llm-code-output} \\\\rangle \\\\) separators, and resumes the autoregressive model inference.\\n\\nApproach. We use few-shot prompting to synthesize solutions for the training sets of GSM8K and MATH. Formally, the prompt has the form:\\n\\n\\\\[\\n\\\\mathcal{I} (q_1, c_1), \\\\ldots, (q_K, c_K) \\\\ q'\\n\\\\]\\n\\nwhere \\\\( \\\\mathcal{I} \\\\) represents a text-based instruction for the task, \\\\( \\\\{q_1, \\\\ldots, q_K\\\\} \\\\) represent \\\\( K \\\\) problems representative of the dataset, \\\\( \\\\{c_1, \\\\ldots, c_K\\\\} \\\\) represent their respective solutions in the code-interpreter format, and \\\\( q' \\\\) represents a question from the training set. Given this prompt, the base LLM generates a candidate solution \\\\( c' \\\\) for the question \\\\( q' \\\\). If \\\\( c' \\\\) leads to the correct answer for the question \\\\( q' \\\\), we add the pair \\\\( (q', c') \\\\) to our fine-tuning set. For all our experiments, we choose \\\\( K = 5 \\\\), and the representative problems are chosen from the training set of the corresponding benchmark. In the instruction \\\\( \\\\mathcal{I} \\\\), we instruct the model to output the answer inside the \\\\( \\\\boxed{} \\\\) block. The complete instruction is in Table 13 in Appendix B.4.\\n\\nSampling Details. We sample solutions with temperature=1.0 and top_p=0.95. We use the following constraints in our generation pipeline: (a) the total number of input-output tokens is limited to 4096, (b) a maximum of 512 new tokens after each code block, (c) a maximum of 3 code blocks, and (d) the generation halts after any code execution error. We use the TensorRT-LLM toolkit.\\n\\n---\\n\\n5 Both GSM8K and MATH have these text solutions.\\n6 During training, we don\u2019t mask the code execution output surrounded by \\\\( \\\\langle \\\\text{llm-code-output} \\\\rangle \\\\) separators.\\n7 https://github.com/NVIDIA/TensorRT-LLM\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 4, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Statistics of unique solutions generated by prompts described in Section 2.2. Default prompt refers to the single prompt used for the two benchmarks, Mask-Text refers to prompting the model with masked text solution, and Subj refers to prompting with subject-specific prompts (applicable only to MATH). Coverage % refers to the percentage of problems in the training set for which there\u2019s at least one solution among the generated solutions.\\n\\n| Prompt       | MATH       | GSM8K       |\\n|--------------|------------|-------------|\\n|              | # Samples  | # Unique Solns. | Coverage (in %) | # Samples  | # Unique Solns. | Coverage (in %) |\\n| Default      | 224        | 177K        | 80.1          | 128        | 434K          | 99.1          |\\n| + Subj       | 224        | 191K        | 80.1          | -          | -             | -             |\\n| Mask-Text    | 224        | 192K        | 85.9          | 128        | 602K          | 99.9          |\\n| + Subj       | 224        | 227K        | 87.5          | -          | -             | -             |\\n| Total        | 896        | 787K        | 93.0          | 256        | 1036K         | 99.9          |\\n\\n2.2 Prompting\\n\\nIn the previous section, we described our solution generation pipeline. A key ingredient of this pipeline is the few-shot prompt examples. We next describe the different prompting strategies explored in this work.\\n\\n2.2.1 Default\\n\\nWe choose five representative examples of GSM8K and MATH to create the few-shot prompt for the respective datasets. For GSM8K, we use a mix of problems that require vanilla Python code and problems that are best solved using Python\u2019s sympy library. For MATH, we compose a 5-shot prompt with examples from different subjects. To reflect this diversity of reasoning paths required for MATH, we choose a mix of problems that require code-based solutions, text-based solutions, and a combination of both. The prompts used for the two datasets are presented in Appendix B.6.\\n\\nFor GSM8K, we sample 128 solutions per training problem, which gets a training set coverage of 99.1%. For MATH, we sample 224 solutions per training problem, which only achieves a training set coverage of 80.1%. This difference in coverage reflects the difficulty of the MATH benchmark compared to GSM8K, which has been noted in previous work as well [8, 9].\\n\\n2.2.2 Subject-specific Prompting (Subj)\\n\\nCould the diversity of mathematical topics in MATH be a reason for the low training set coverage with a single 5-shot prompt? To answer this question, we create subject-specific prompts for the seven subjects in the MATH benchmark, namely algebra, geometry, intermediate algebra, number theory, prealgebra, precalculus, and probability (See Table 11 in the appendix for the subject-wise split of MATH training data). The MATH benchmark also labels problems by their hardness level, with levels ranging from 1 to 5, where level 5 is the hardest. For creating subject-specific 5-shot prompts, we choose one example from each level for a given subject. For each of the seven prompts, we sample 32 solutions per problem and combine the data generated with all the prompts, which is equivalent to 32 x 7 = 224 solutions per problem. However, even with this fine-grained prompting, we only achieve a negligible gain in the training set coverage, though the total number of correct solutions increases by 14K (177K \u2192 191K, see Table 2).\\n\\nCombining this fine-tuning dataset with the earlier single default prompt dataset yields a training coverage of 85.1% for MATH, a boost of 5% absolute. However, achieving this coverage required sampling almost 450 solutions per problem (224 + 224 = 448). Can we make the solution generation pipeline more efficient?\\n\\n2.2.3 Masked Text Solution Prompting (Mask-Text)\\n\\nGSM8K and MATH benchmarks come with reference text solutions. Using these text solutions can, in theory, reduce the problem of code-interpreter solution generation to a translation problem from text to code. We initially experimented by prompting the LLM with:\\n\\n\\\\[ I (q_1, t_1, c_1), \\\\ldots, (q_K, t_K, c_K) q', t' \\\\]\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 5, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lynne bought 7 books about cats and 2 books about the solar system. She also bought 3 magazines. Each book cost $7 and each magazine cost $4. How much did Lynne spend in all?\\n\\n**Ground-Truth Text Solution**\\nLynne bought a total of $7 + 2 = 9$ books. The books cost Lynne $9 \\\\times 7 = 63$. For 3 magazines, Lynne spent $3 \\\\times 4 = 12$. In total, Lynne spent $63 + 12 = 75$\\n\\n**Masked Text Solution**\\nLynne bought a total of $7 + 2 = M$ books. The books cost Lynne $M \\\\times 7 = N$. For 3 magazines, Lynne spent $3 \\\\times 4 = P$. In total, Lynne spent $N + P = Q$\\n\\nFigure 2: A sample masked solution from GSM8K training set. The masked text solution only masks the intermediate computations, such as $9 \\\\rightarrow M$ and $63 \\\\rightarrow N$, and doesn\u2019t mask the amounts introduced in the question, such as $7$, $2$, and $4$.\\n\\nwhere $t_i$\u2019s represent the text solution of representative problem $q_i$\u2019s and $t'$ represents the text solution of the problem $q'$. Using the text solution in the prompt leads to a considerable increase in training set coverage. However, our manual analysis revealed that many solutions were shortcuts. E.g., trivial solutions such as `print(ANSWER)` or `The answer is ANSWER` where the ANSWER is copied from the text solution $t'$ in the prompt. Our attempts to filter out these trivial solutions proved challenging as there are many creative ways in which the generated solutions were cheating (see Figure 11 in Appendix).\\n\\nTo deter the possibility of such shortcut solutions where the results of intermediate computations or the final answer from the text solution are copied, we propose prompting with a masked text solution. Such solutions have all numbers in intermediate computations replaced with symbols. A sample masked text solution is shown in Figure 2. These masked text solutions are generated using few-shot prompting as follows:\\n\\n$$I_{\\\\text{mask}} (q_1, t_1, t_1^{\\\\text{mask}}), \\\\ldots, (q_K, t_K, t_K^{\\\\text{mask}}) q', t'$$\\n\\nwhere $I_{\\\\text{mask}}$ represents the instruction for the solution masking task, and $\\\\{t_1^{\\\\text{mask}}, \\\\ldots, t_K^{\\\\text{mask}}\\\\}$ represent masked text solutions corresponding to $\\\\{t_1, \\\\ldots, t_K\\\\}$. For a detailed overview of the masked text solution generation pipeline, we refer the reader to Appendix B.5. Using these masked text solutions in the prompts significantly boosts the training set coverage for MATH, increasing from $80.1\\\\% \\\\rightarrow 85.9\\\\%$ for the single default prompt, and $80.1\\\\% \\\\rightarrow 87.5\\\\%$ for the subject-specific prompts. For GSM8K, it leads to the coverage increasing from $99.1\\\\%$ to $99.9\\\\%$.\\n\\nTable 2 summarizes the statistics of the solutions dataset generated via different prompts. The OpenMathInstruct-1 dataset is obtained by merging and deduplicating the problem-solution pairs resulting from the above-described prompt strategies. OpenMathInstruct-1 consists of 787K unique solutions for 6978 problems (out of 7500) in MATH and 1.04M unique solutions for 7469 problems (out of 7473) in GSM8K. To get to this final dataset, we also perform a few post-processing steps, which are described next.\\n\\n### 2.3 Post-processing\\n\\nThe generated solutions can sometimes be syntactically noisy even if they lead to the right answer. We fix or remove the following solutions:\\n\\n- Remove solutions with multiple `\\\\boxed{}` blocks.\\n- Remove solutions with the `<llm-code>` string but not the `</llm-code>` string.\\n- Remove text beyond the solution line with the answer, i.e., the `\\\\boxed{}` block. See Figure 12 in the Appendix for an example solution where we perform trimming.\\n\\nWhile these post-processing steps can fix some of the syntactic errors, filtering semantically noisy, i.e., solutions that get to the right answer with flawed reasoning [12], is a much harder problem and beyond the scope of this work. Anecdotally, we find such solutions to be rare in our corpus. See Figure 13 in the Appendix for a sample semantically noisy solution.\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 6, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.4 Data Selection\\n\\nOpenMathInstruct-1 on average has hundreds of solutions per problem. These solutions can have different formats (code vs. text), and problems can have very different numbers of solutions in the dataset. Careful data selection allows for reduced training times and can also benefit performance. We detail the data selection strategies explored in this work.\\n\\n2.4.1 Fair vs. Naive Downsampling\\n\\nFor a dataset like MATH, where problems have divergent difficulty levels, our solution generation strategy leads to a corpus where easier problems have a lot of solutions and harder problems have very few solutions (see Appendix A.2 for a detailed discussion on solution count). A naive strategy for downsampling treats every instance, i.e., problem-solution pair, as an equal. This problem-agnostic sampling perpetuates the imbalance of the original corpus, as seen in Figure 3(a). We propose a fair sampling alternate in which we iterate over all the problems round-robin and sample without replacement from the remaining solutions for each problem. This problem-dependent sampling ensures a more balanced representation of each problem in the downsampled dataset (see Figure 3(b)). Experimental results show that fair downsampling outperforms naive downsampling (Section 4.1.1).\\n\\n2.4.2 Code-Preferred Solutions\\n\\nThe code-interpreter format allows for mixing code and text, and also text-based solutions without any code blocks. For GSM8K, the proportion of text-based solutions is 2%, but for MATH, their representation is 35.1%. While natural language reasoning is more expressive, it lacks the precision of code-based solutions [24]. Suppose for a problem \\\\( q \\\\) there are a total of \\\\( N_{\\\\text{total}} \\\\) correct solutions in the corpus, out of which \\\\( N_{\\\\text{code}} \\\\) represents the number of code-based solutions, and \\\\( N_{\\\\text{text}} \\\\) represents the text-based solutions. We propose the following two code-preferential data selection strategies:\\n\\n- **Majority-Code**: If \\\\( N_{\\\\text{code}} > N_{\\\\text{text}} \\\\), remove all the text-based solutions.\\n- **Any-Code**: If \\\\( N_{\\\\text{code}} > 0 \\\\), remove all the text-based solutions.\\n\\nAblation experiments over the MATH subset of OpenMathInstruct-1 show the benefit of code-preferential data selection (Section 4.1.4).\\n\\n3 Experimental Setup\\n\\n3.1 Training Details\\n\\nFor all our experiments, including ablations, models of size 34B or smaller are trained for four epochs. A global batch size of 128 is used along with the AdamW optimizer with a weight decay of 1e-8.\\n\\n---\\n\\n8We detect the presence of code by searching for \\\\( \\\\langle \\\\text{llm-code} \\\\rangle \\\\) in the solution string.\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 7, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Comparison of our OpenMath-finetuned models with their gpt-distilled counterparts. We present results on popular mathematical reasoning tasks, namely, GSM8K, MATH, GSM-Hard, SVAMP, TabMWP, ASDiv, and MAWPS. For ToRA and MAmmoTH, we report the results of their \\\"-Code(r)\\\" versions whenever available since they are always better than their non-code counterparts. SC (k=50) denotes self-consistency decoding with 50 samples. We highlight the following results for a parameter range: best with SC, best and second best with greedy decoding.\\n\\n| Size | Base Model | Model | GSM8K | MATH | GSM-Hard | SVAMP | TabMWP | ASDiv | MAWPS |\\n|------|------------|-------|-------|------|---------|-------|--------|-------|-------|\\n| -    | GPT-4 (Code Interpreter) |       | 97.0  | 69.7 | 77.6    | 94.8  | 95.9   | 92.6  | 97.7  |\\n| 7B   | Llama-2    | WizardMath | 54.9  | 10.7 | -       | 36.1  | -      | -     | -     |\\n|      |            | MetaMath   | 66.4  | 19.4 | -       | -     | -      | -     | -     |\\n|      |            | MAmmoTH    | 59.4  | 33.4 | -       | 71.4  | -      | -     | -     |\\n|      |            | ToRA       | 72.6  | 44.6 | 56.0    | 70.4  | 51.6   | 78.7  | 93.5  |\\n|      | CodeLlama  | + SC (k=50) | 76.8  | 52.5 | -       | -     | -      | -     | -     |\\n|      |            | OpenMath-CodeLlama | 75.9  | 43.6 | 60.1    | 79.6  | 56.0   | 77.7  | 93.5  |\\n|      |            | + SC (k=50) | 84.8  | 55.6 | -       | -     | -      | -     | -     |\\n|      | Mistral    | MetaMath-Mistral-7B | 77.7  | 28.2 | -       | -     | -      | -     | -     |\\n|      |            | MAmmoTH-7B-Mistral | 75.0  | 40.0 | -       | -     | -      | -     | -     |\\n|      |            | WizardMath  | 83.2  | 33.0 | -       | -     | -      | -     | -     |\\n|      |            | OpenMath-Mistral-7B | 80.2  | 44.5 | 63.7    | 82.4  | 70.0   | 82.7  | 95.4  |\\n|      |            | + SC (k=50) | 86.9  | 57.2 | -       | -     | -      | -     | -     |\\n| 13B  | Llama-2    | WizardMath | 63.9  | 14.0 | -       | 51.9  | -      | -     | -     |\\n|      |            | MetaMath   | 72.3  | 22.4 | -       | -     | -      | -     | -     |\\n|      |            | MAmmoTH    | 64.7  | 36.3 | -       | 73.7  | -      | -     | -     |\\n|      | CodeLlama  | + SC (k=50) | 80.4  | 55.1 | -       | -     | -      | -     | -     |\\n|      |            | OpenMath-CodeLlama | 78.8  | 45.5 | 61.9    | 78.8  | 59.7   | 81.2  | 93.6  |\\n|      |            | + SC (k=50) | 86.8  | 57.6 | -       | -     | -      | -     | -     |\\n| 34B  | CodeLlama  | MAmmoTH    | 72.7  | 43.6 | -       | 84.3  | -      | -     | -     |\\n|      |            | ToRA       | 80.7  | 51.0 | 63.7    | 80.5  | 70.5   | 84.2  | 93.3  |\\n|      |            | + SC (k=50) | 85.1  | 60.0 | -       | -     | -      | -     | -     |\\n|      |            | OpenMath-CodeLlama | 80.7  | 48.3 | 64.0    | 83.6  | 66.0   | 82.7  | 94.9  |\\n|      |            | + SC (k=50) | 88.0  | 60.2 | -       | -     | -      | -     | -     |\\n| 70B  | Llama-2    | WizardMath | 81.6  | 22.7 | -       | 71.8  | -      | -     | -     |\\n|      |            | MetaMath   | 82.3  | 26.6 | -       | -     | -      | -     | -     |\\n|      |            | MAmmoTH    | 76.9  | 41.8 | -       | 82.4  | -      | -     | -     |\\n|      | CodeLlama  | ToRA       | 84.3  | 49.7 | 67.2    | 82.7  | 74.0   | 86.8  | 93.8  |\\n|      |            | + SC (k=50) | 88.3  | 56.9 | -       | -     | -      | -     | -     |\\n|      |            | OpenMath-Llama2 | 84.7  | 46.3 | 65.7    | 85.0  | 70.8   | 84.3  | 95.6  |\\n|      |            | + SC (k=50) | 90.1  | 58.3 | -       | -     | -      | -     | -     |\\n|      | CodeLlama  | OpenMath-CodeLlama | 84.6  | 50.7 | 66.6    | 87.8  | 74.2   | 84.7  | 95.7  |\\n|      |            | + SC (k=50) | 90.8  | 60.4 | -       | -     | -      | -     | -     |\\n\\n2 [25] and dropout [26] of 0.1. We save one checkpoint per epoch for ablation experiments and two checkpoints per epoch for final model runs. The final checkpoint is created by averaging all the saved checkpoints. All experiments are performed using the NeMo toolkit [27]. For the full set of training hyperparameters, see Appendix B.1.\\n\\n3.2 Evaluation Setup\\n\\nWe evaluate our models on popular math reasoning benchmarks, namely GSM8K, MATH, GSM-Hard [24], SVAMP [28], TabMWP [29], ASDiv [30], and MAWPS [31]. For ablation experiments and hyperparameter selection, we create a validation set of 1K examples from the training set of GSM8K and MATH since both datasets lack an actual validation set. All the fine-tuned models are evaluated in the zero-shot setting. We use greedy decoding and self-consistency/majority voting [32] for evaluation. For majority voting, we found that using a lower temperature of 0.7 is beneficial compared to the data generation setup. We also deviate from the data generation setup by allowing the model to continue answer generation after code execution errors.\\n\\nhttps://github.com/NVIDIA/NeMo\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 8, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4 Results\\n\\nWe finetune all the models on a mixture of (a) 512K fair downsampled GSM8K instances, and (b) 512K MATH instances with any-code filtering (Section 2.4)\\\\textsuperscript{10}. Thus, the total finetuning corpus size is roughly 1.02M. We justify the data selection choices later in the ablation experiments.\\n\\nTable 3 compares the performance of OpenMath-finetuned models against their GPT-distilled counterparts. Among the 7B models, our OpenMath-Mistral-7B is competitive with all the GPT-distilled models. It is second-best to WizardMath on GSM8K, and bested by ToRA by 0.1% on MATH\\\\textsuperscript{11}. Our models easily outperform both MetaMath \\\\textsuperscript{1} and MAmmoTH \\\\textsuperscript{2}, even when controlling for the base fine-tuned model. Since WizardMath and ToRA finetuning datasets are not publicly available yet, OpenMathInstruct-1 presents a superior alternative to the publicly available MetaMathQA and MathInstruct datasets, which are used to fine-tune MetaMath and MAmmoTH, respectively.\\n\\nWith the increase in model parameters, our models continue to outperform MetaMath and MAmmoTH substantially. Compared to ToRA, with greedy decoding, we see a meaningful drop in performance on MATH, though our models are equal or better on GSM8K. With self-consistency (SC) decoding, however, our models outperform ToRA on both MATH and GSM8K. The substantial gains with SC can be attributed to the diversity of our fine-tuning data.\\n\\n4.1 Ablations\\n\\nWe perform ablation experiments with the Mistral-7B as the base model. We report results on the 1K-sized validation subsets for MATH and GSM8K created by us.\\n\\nTable 4: Comparison of performance of Fair vs Naive downsampling on our validation subset of GSM8K and MATH.\\n\\n| Sampling | GSM8K | MATH |\\n|----------|-------|------|\\n| Naive    | 74.3  | 35.0 |\\n| Fair     | 75.3  | 37.0 |\\n\\nTable 5: Comparison of Default vs Masked prompting on our validation subset of GSM8K and MATH.\\n\\n| Prompt | GSM8K | MATH |\\n|--------|-------|------|\\n| Default| 73.8  | 36.9 |\\n| Masked | 77.7  | 37.4 |\\n\\n4.1.1 Fair vs. Naive Downsampling\\n\\nWe finetune the base model on a dataset of 128K instances created by combining 64K naive or fair downsampled instances from the GSM8K and MATH portion of the data. Table 4 shows that the model fine-tuned on the data downsampled with fair sampling outperforms the one created by naive downsampling. The performance gap is particularly substantial for MATH, which suffers from a graver data imbalance than GSM8K in our corpus.\\n\\n4.1.2 Default vs Masked Prompting\\n\\nWe finetune the base model on a dataset of 128K instances created by combining 64K fair-sampled instances from the GSM8K and MATH portion of the data generated using default and masked prompting. Table 5 shows that the model fine-tuned on the data generated using masked prompting outperforms the one created by default prompting on both GSM8K and MATH. Thus, the gains in the training set coverage with masked prompting (Section 2.2.3) also translate to finetuning performance.\\n\\n4.1.3 Impact of Fine-Tuning Dataset Size\\n\\nTo determine the impact of the size of the fine-tuning dataset, we create datasets of size 128K/256K/512K by combining 64K/128K/256K fair downsampled equally-sized subsets of GSM8K and MATH. Table 6 shows that the performance increases for both GSM8K and MATH with the increase in the fine-tuning dataset size. We didn\u2019t find any benefit from training the models for more steps, so the performance gain is attributable to the increased data size.\\n\\n\\\\textsuperscript{10}The actual number of MATH instances is 511,677.\\n\\n\\\\textsuperscript{11}Our grading script scores the publicly released ToRA outputs about 2-3% lower than the reported numbers. We believe that ToRA uses some heuristics to extract answers when the model doesn\u2019t generate answers in the correct format.\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 9, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Effect of fine-tuning dataset size on performance on our validation subset of GSM8K and MATH.\\n\\n| Dataset Size | GSM8K | MATH |\\n|--------------|-------|------|\\n| 128K         | 75.3  | 37.0 |\\n| 256K         | 79.0  | 38.6 |\\n| 512K         | 81.0  | 41.6 |\\n\\nTable 7: Comparison of default vs subject-wise prompt performance on our MATH validation subset.\\n\\n| Prompt   | Pass@1 | SC (k=4) |\\n|----------|--------|----------|\\n| Default  | 39.1   | 41.7     |\\n| Subject  | 38.3   | 44.5     |\\n\\nTable 8: Impact of code-preferential data selection on our MATH validation subset performance.\\n\\n| Prompt          | Pass@1 | SC (k=4) |\\n|-----------------|--------|----------|\\n| Default         | 37.4   | 45.2     |\\n| Majority-Code   | 39.8   | 42.6     |\\n| Any-Code        | 39.4   | 42.6     |\\n\\n4.1.4 MATH-only Ablations\\n\\nThis section presents the ablation results for only the MATH portion of OpenMathInstruct-1. We finetune the base model on a 128K fair downsampled subset to control for data size.\\n\\n**Default vs Subject-Specific Prompting.** In section 2.2.2 we motivated using subject-specific prompts, which ultimately didn\u2019t result in much training set coverage difference. *But how are the solutions generated by the combination of subject-wise prompts different from a single default prompt?* To answer this, we create a subset of 128K instances generated with the default prompt/subject-specific prompts. Table 7 compares the finetuning performance on these two splits on our MATH validation subset. While the model trained on the subject-specific subset underperforms the model trained on the default subset with greedy decoding, the trend is decisively reversed for self-consistency decoding with four samples. This suggests that the subset collected with subject-specific prompts has a higher diversity of solutions than the ones collected using a single prompt.\\n\\n**Code-Preferential Subsets.** In this ablation, we determine the impact of code-preferential solution selection strategies proposed in Section 2.4.2. Table 8 shows that code-preferential solution strategies aid the greedy decoding performance. However, the reduction in solution diversity arguably results in a performance drop with self-consistency decoding (text-based solutions are about one-third of the original corpus). Based on these results and because Any-Code results in a smaller finetuning dataset (512K compared to 664K with Majority-Code), we chose to use the Any-Code subset.\\n\\n5 Analysis\\n\\nWe analyze the performance of the ablation model trained on 512K instances from Section 4.1.3. We limit the discussion to the MATH benchmark, where the model scores 41.6% on our validation subset.\\n\\n**Performance-split by Subjects and Levels.** Figure 4 presents the performance split by subjects and levels on the MATH validation subset. Among subjects, we see that the model\u2019s worst performance is on geometry, which can be attributed to the lack of multi-modality in our base models [23]. We see a monotonic decrease in performance with the increase in hardness level which is to be expected [23]. The model scores 72.4% on Level 1 problems and only 16.3% on the hardest problems, i.e., Level 5.\\n\\n**Error Analysis.** Table 9 shows that the model performs an absolute 13.3% better when using code for answering questions in comparison to when not using it. We find that some of the errors made by text-based solution could have been avoided by preferring code-based solution; see Figure 16 for a sample solution where the model makes an arithmetic calculation error. This analysis provides another support for our proposal and use of code-preferred solutions from Section 2.4.2.\\n\\nTable 10 presents the count of different error categories. For code-based solutions, we find that almost 74% of the errors in such solutions are due to reasoning errors, and the remaining 26% are attributable to execution-related issues. We present sample solutions from these error categories in Appendix B.3.\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 10, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Performance split based on solution format. Solutions without \\\\texttt{llm-code-output} string are considered to be \\\\textit{text-based}.\\n\\n| Solution Type | Accuracy (in %) | Count |\\n|---------------|----------------|-------|\\n| Text-based    | 32.0           | 278   |\\n| Code + Text   | 45.3           | 722   |\\n| Total         | 41.6           | 1000  |\\n\\nTable 10: Types of errors and their counts.\\n\\n| Error Type                  | Count |\\n|-----------------------------|-------|\\n| Text Reasoning Error        | 189   |\\n| Code Reasoning Error        | 292   |\\n| Code Execution Error        | 78    |\\n| Code timeout                | 15    |\\n| Max code executions reached | 10    |\\n| Total                       | 584   |\\n\\n6 Related Work\\n\\n**Mathematical Reasoning and LLMs.** Recently, a plethora of work has been done on enhancing the mathematical reasoning capabilities of LLMs. Inference techniques such as Chain-of-Thought [33], its programmatic counterpart, Program of Thought [24, 34], Self-Consistency [32], and Self-Verification [23] have been shown to significantly improve the reasoning capabilities of LLMs.\\n\\nPretraining language models on math-heavy content has resulted in foundation LLMs such as Minerva [35], Galactica [36], Llemma [37], and DeepSeekMath [38] with stronger mathematical skills out-of-the-box. A more direct approach of dataset-specific training does \\\\textit{instruction fine-tuning} on problem-solution pairs derived from math reasoning datasets. Our work falls in this latter category and bears similarity with recent work such as RFT [39], ToRA [8], MAmmoTH [2], MetaMath [1] and MathCoder [7]. We differ from the previous work along one factor or a combination of the following factors: (a) reliance on GPT-3.5/4, (b) solution format, and (c) use of ground truth text solution in synthesizing code-based solutions.\\n\\n**Knowledge Distillation via Synthetic Data.** Recent work exploring the use of targeted \\\\textit{synthetic} data generated by large foundation models for pre-training/instruction tuning smaller LLMs has led to tremendous progress in skills of these smaller LLMs [5, 6, 4, 40, 41, 42].\\n\\n7 Conclusion\\n\\nWe introduce OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs which is released with a commercially permissive license. Compared to previous work, the OpenMathInstruct-1 dataset is at least four times bigger. With our proposed prompting novelty of using \\\\textit{masked text solutions} and some brute-force scaling, we achieve training set coverage of 99.9\\\\% for the GSM8K benchmark and 93\\\\% for the challenging MATH benchmark. The quality of these synthesized solutions is illustrated by finetuning experiments, which show models achieving performance comparable to or better than their \\\\textit{gpt-distilled} counterparts. To support the open-source efforts in this direction, we publicly release all our fine-tuned models, code, and the OpenMathInstruct-1 along with a further 6.6M incorrect sampled solutions.\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 11, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations and Potential Risks\\n\\nOur work aims to improve the mathematical reasoning of open-source models using open-source models. In pursuit of this goal, we create a synthetic dataset, OpenMathInstruct-1, that our experiments show aids the performance on existing math benchmarks. Below, we list the key limitations of our work:\\n\\n- Our manual analysis reveals solutions that get the right answer but via flawed reasoning (Figure 13 in Appendix). Removing these semantically noisy solutions is beyond the scope of the current work. This means a lack of guarantee about the quality of our synthetically generated solutions.\\n- Improving performance on in-domain math benchmarks may not translate to performance gain on other related tasks. The drop in performance on GSM-Hard compared to GSM indicates that our models may not be robust to input perturbations, though, they are at par with previous work.\\n\\nWhile we don\u2019t foresee any material risk due to our work, using our imperfect dataset and models to perform tasks, such as evaluating student assignments or building a math tutor, carries risk.\\n\\nReferences\\n\\n[1] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. In ICLR, 2024.\\n\\n[2] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhui Chen. MAmmoTH: Building math generalist models through hybrid instruction tuning. In ICLR, 2024. URL https://openreview.net/forum?id=yLC1Gs770I\\n\\n[3] OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim\u00f3n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, \u0141ukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, \u0141ukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David M\u00e9ly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O\u2019Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov,\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 12, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katrina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine Thompson, Phil Tillett, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cer\u00f3n Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. GPT-4 Technical Report, 2023.\\n\\n[4] Ronen Eldan and Yuanzhi Li. TinyStories: How Small Can Language Models Be and Still Speak Coherent English? *arXiv*, 2023.\\n\\n[5] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheiri, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00e9bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks Are All You Need. *arXiv*, 2023.\\n\\n[6] Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks Are All You Need II: phi-1.5 technical report. *arXiv*, 2023.\\n\\n[7] Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning. In *ICLR*, 2024.\\n\\n[8] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving. In *ICLR*, 2024. URL [https://openreview.net/forum?id=Ep0TtjVoap](https://openreview.net/forum?id=Ep0TtjVoap).\\n\\n[9] Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, and Kai Fan. MARIO: MAth Reasoning with code Interpreter Output \u2013 A Reproducible Pipeline, 2024.\\n\\n[10] Lingjiao Chen, Matei Zaharia, and James Zou. How is ChatGPT\u2019s behavior changing over time?, 2023.\\n\\n[11] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mixtral of Experts, 2024.\\n\\n[12] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training Verifiers to Solve Math Word Problems, 2021.\\n\\n[13] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset. *NeurIPS*, 2021.\\n\\n[14] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7B. *arXiv*, 2023.\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 13, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[15] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoping Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, 2023.\\n\\n[16] Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattaflori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code Llama: Open Foundation Models for Code. arXiv, 2023.\\n\\n[17] Fei Yu, Anningzhe Gao, and Benyou Wang. Outcome-supervised Verifiers for Planning in Mathematical Reasoning. arXiv, 2023.\\n\\n[18] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s Verify Step by Step. arXiv, 2023.\\n\\n[19] Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. Lila: A Unified Benchmark for Mathematical Reasoning. In EMNLP, 2022.\\n\\n[20] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct. arXiv preprint arXiv:2308.09583, 2023.\\n\\n[21] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STaR: Bootstrapping Reasoning With Reasoning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, NeurIPS, 2022.\\n\\n[22] Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large Language Models Can Self-Improve. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, EMNLP, 2023.\\n\\n[23] Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, and Hongsheng Li. Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification. In ICLR, 2024.\\n\\n[24] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: Program-aided Language Models. In ICML, pages 10764\u201310799. PMLR, 2023.\\n\\n[25] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. arXiv, 2019.\\n\\n[26] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.\\n\\n[27] O. Kuchaiev, J. Li, H. Nguyen, O. Hrinchuk, R. Leary, B. Ginsburg, S. Kriman, S. Beliaev, V. Lavrukhin, J. Cook, et al. NeMo: a toolkit for building AI applications using neural modules. In Systems for ML Workshop, NeurIPS, 2019.\\n\\n[28] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP Models really able to Solve Simple Math Word Problems? In NAACL-HLT, June 2021.\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 14, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[29] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning. In *ICLR*, 2023.\\n\\n[30] Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, *ACL*, 2020.\\n\\n[31] Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A Math Word Problem Repository. In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, *NAACL-HLT*, 2016.\\n\\n[32] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In *ICLR*, 2023.\\n\\n[33] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. *NeurIPS*, 2022.\\n\\n[34] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. *TMLR*, 2023.\\n\\n[35] Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving Quantitative Reasoning Problems with Language Models. In *NeurIPS*, 2022.\\n\\n[36] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A Large Language Model for Science, 2022.\\n\\n[37] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, 2024.\\n\\n[38] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling Relationship on Learning Mathematical Reasoning with Large Language Models, 2023.\\n\\n[39] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive Learning from Complex Explanation Traces of GPT-4, 2023.\\n\\n[40] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. WizardLM: Empowering Large Language Models to Follow Complex Instructions. *arXiv*, 2023.\\n\\n[41] Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi Zhang. TinyGSM: achieving > 80% on GSM8k with small language models. *arXiv preprint arXiv:2312.09241*, 2023.\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 15, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"NeurIPS Paper Checklist\\n\\n1. **Claims**\\n   \\n   Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope?\\n   \\n   Answer: [Yes]\\n   \\n   Justification: Our claims match our experimental results.\\n   \\n   Guidelines:\\n   \\n   - The answer NA means that the abstract and introduction do not include the claims made in the paper.\\n   - The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.\\n   - The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.\\n   - It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.\\n\\n2. **Limitations**\\n   \\n   Question: Does the paper discuss the limitations of the work performed by the authors?\\n   \\n   Answer: [Yes]\\n   \\n   Guidelines:\\n   \\n   - The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\\n   - The authors are encouraged to create a separate \\\"Limitations\\\" section in their paper.\\n   - The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\\n   - The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\\n   - The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\\n   - If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.\\n   - While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.\\n\\n3. **Theory Assumptions and Proofs**\\n   \\n   Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\\n   \\n   Answer: [NA]\\n   \\n   Justification: Empirical paper\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 16, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Guidelines:\\n\\n- The answer NA means that the paper does not include theoretical results.\\n- All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.\\n- All assumptions should be clearly stated or referenced in the statement of any theorems.\\n- The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.\\n- Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.\\n- Theorems and Lemmas that the proof relies upon should be properly referenced.\\n\\n4. **Experimental Result Reproducibility**\\n\\n   Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\\n\\n   Answer: [Yes]\\n\\n   Justification: We provide all the details to the best of our capacity. Some of the details have been shared in the supplementary material.\\n\\nGuidelines:\\n\\n- The answer NA means that the paper does not include experiments.\\n- If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.\\n- If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.\\n- Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.\\n- While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example\\n  (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.\\n  (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.\\n  (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).\\n  (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.\\n\\n5. **Open access to data and code**\\n\\n   Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 17, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Answer: [No]\\n\\nJustification: We are not sharing the data and code as of now. But we will be releasing everything with open access after the review cycle.\\n\\nGuidelines:\\n\\n- The answer NA means that paper does not include experiments requiring code.\\n- Please see the NeurIPS code and data submission guidelines [https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\\n- While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).\\n- The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines [https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\\n- The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.\\n- The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.\\n- At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).\\n- Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.\\n\\n6. **Experimental Setting/Details**\\n\\n   Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?\\n\\n   Answer: [Yes]\\n\\n   Justification: All the details have been shared in the paper.\\n\\n   Guidelines:\\n\\n   - The answer NA means that the paper does not include experiments.\\n   - The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.\\n   - The full details can be provided either with the code, in appendix, or as supplemental material.\\n\\n7. **Experiment Statistical Significance**\\n\\n   Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?\\n\\n   Answer: [No]\\n\\n   Justification: All our fine-tuning runs are quite costly, which limits our capability to do multiple runs to capture this variability.\\n\\n   Guidelines:\\n\\n   - The answer NA means that the paper does not include experiments.\\n   - The authors should answer \\\"Yes\\\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.\\n   - The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).\\n   - The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)\\n   - The assumptions made should be given (e.g., Normally distributed errors).\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 18, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.\\n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.\\n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).\\n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.\\n\\n8. **Experiments Compute Resources**\\n\\nQuestion: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?\\n\\nAnswer: [Yes]\\n\\nJustification: Details in the Appendix.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the paper does not include experiments.\\n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.\\n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.\\n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper).\\n\\n9. **Code Of Ethics**\\n\\nQuestion: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines](https://neurips.cc/public/EthicsGuidelines)?\\n\\nAnswer: [Yes]\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\\n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.\\n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).\\n\\n10. **Broader Impacts**\\n\\nQuestion: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?\\n\\nAnswer: [Yes]\\n\\nJustification: We briefly mention it in the Limitations section on Page 10.\\n\\nGuidelines:\\n\\n\u2022 The answer NA means that there is no societal impact of the work performed.\\n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.\\n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.\\n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 19, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.\\n\\n- The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.\\n\\n- If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).\\n\\n11. **Safeguards**\\n\\n   Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?\\n\\n   Answer: [No]\\n\\n   Justification: Our dataset is related to mathematical reasoning.\\n\\n   Guidelines:\\n\\n   - The answer NA means that the paper poses no such risks.\\n   - Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.\\n   - Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.\\n   - We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.\\n\\n12. **Licenses for existing assets**\\n\\n   Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?\\n\\n   Answer: [Yes]\\n\\n   Justification: We mention the use of Mixtral models and that these models are released under permissive license.\\n\\n   Guidelines:\\n\\n   - The answer NA means that the paper does not use existing assets.\\n   - The authors should cite the original paper that produced the code package or dataset.\\n   - The authors should state which version of the asset is used and, if possible, include a URL.\\n   - The name of the license (e.g., CC-BY 4.0) should be included for each asset.\\n   - For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.\\n   - If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, [paperswithcode.com/datasets](http://paperswithcode.com/datasets) has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.\\n   - For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.\\n   - If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators.\\n\\n13. **New Assets**\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 20, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?\\n\\nAnswer: [Yes]\\n\\nJustification: We have tried to share details of the dataset generation process and the dataset ultimately created to the best of our capabilities. We have also clearly communicated that all the resources will be released under a commercially permissive license.\\n\\nGuidelines:\\n\\n- The answer NA means that the paper does not release new assets.\\n- Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.\\n- The paper should discuss whether and how consent was obtained from people whose asset is used.\\n- At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.\\n\\n14. **Crowdsourcing and Research with Human Subjects**\\n\\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?\\n\\nAnswer: [NA]\\n\\nJustification: Not a paper which deals with human subjects.\\n\\nGuidelines:\\n\\n- The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\\n- Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.\\n- According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.\\n\\n15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**\\n\\nQuestion: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?\\n\\nAnswer: [NA]\\n\\nJustification: Not required for our paper.\\n\\nGuidelines:\\n\\n- The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\\n- Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.\\n- We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.\\n- For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.\\n\\nA  **Data Statistics**\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 21, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A department store displays a 20% discount on all fixtures. What will be the new price of a 25 cm high bedside lamp that was worth $120?\\n\\nLet\u2019s solve this problem using Python code.\\n\\n```python\\ndiscount_percent = 20\\nprice_before_discount = 120\\ndiscount = discount_percent / 100\\ndiscount_amount = price_before_discount * discount\\nprice = price_before_discount - discount_amount\\nprice\\n```\\n\\nSo the new price of the lamp is $96 dollars.\\n\\nFigure 5: Code-Interpreter style solution for a training set problem from GSM8K.\\n\\nFigure 6: Distribution of solutions with different code-block counts in OpenMathInstruct-1.\\n\\nA.1 Code-Block Count Frequencies\\n\\nWe\u2019re using the code-interpreter solution format, which allows for flexible reasoning in text along with precision in code-based reasoning (see Figure 10). The code-interpreter allows solving a problem by breaking it into multiple code blocks. Most of the solutions in OpenMathInstruct-1 have 0 or 1 code blocks, 16.4% and 81.7% of the solutions, respectively (see Figure 6). The remaining 2% of the solutions have two or more code blocks. Figure 7 shows an example of a solution using two code blocks.\\n\\nA.2 Frequency of solutions per problem in OpenMathInstruct-1\\n\\nFigure 8 shows the histogram of the frequency of the number of solutions in OpenMathInstruct-1 for problems in GSM8K and MATH. For GSM8K, 57.4% problems (4292 out of 7473) have more than half the sampled solutions, i.e., > 128 solutions out of 256 sampled solutions, that are valid. For MATH, the solution frequency histogram mimics a power-law curve, with most problems having very few solutions and very few problems with many solutions. Concretely, 19% problems (1324 out of\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 22, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Let $d$ and $e$ denote the solutions of $2x^2 + 3x - 5 = 0$. What is the value of $(d - 1)(e - 1)$?\\n\\n**Generated Solution**\\n\\nLet\u2019s solve the quadratic equation using Sympy:\\n\\n```python\\nfrom sympy import Symbol, solve, Eq, simplify\\n\\n# Define the variable x\\nx = Symbol('x')\\n\\n# Define the equation\\neq = 2*x**2 + 3*x - 5\\n\\n# Solve the equation\\nroots = solve(eq, x)\\n\\n# Print the solutions\\nprint(\\\"The solutions are:\\\")\\nfor root in roots:\\n    print(\\\"x = \\\", simplify(root))\\n```\\n\\n```\\nThe solutions are:\\nx = -5/2\\nx = 1\\n```\\n\\nSo $d = -5/2$ and $e = 1$.\\n\\nLet\u2019s calculate the answer:\\n\\n```python\\nd = -5/2\\ne = 1\\nd_minus_1 = d - 1\\ne_minus_1 = e - 1\\nresult = d_minus_1 * e_minus_1\\nprint(\\\"The value of (d-1)(e-1) is: \\\", result)\\n```\\n\\n```\\nThe value of (d-1)(e-1) is: -0.0\\n```\\n\\nSo the answer is $-0.0$.\\n\\nFigure 7: Sample solution with multiple code blocks. The first code block computes the roots of the given quadratic equation and the second block computes the expression involving them.\\n\\n6978) with any solutions in OpenMathInstruct-1 have $\\\\leq 10$ valid solutions out of the 896 sampled solutions per problem. To further highlight the hardness of the MATH benchmark in comparison to GSM8K, just 3.1% problems (233 out of 7500) have more than half the sampled solutions ($> 448$ out of 896 sampled solutions), that are valid.\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 23, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Let \\\\( t(x) = \\\\sqrt{3x + 1} \\\\) and \\\\( f(x) = 5 - t(x) \\\\). What is \\\\( t(f(5)) \\\\)?\\n\\n**Solution 1**\\n\\nTo find \\\\( t(f(5)) \\\\) we can substitute \\\\( x = 5 \\\\) into the functions \\\\( t(x) \\\\) and \\\\( f(x) \\\\) and then find the value of \\\\( t(f(5)) \\\\). To do the calculations we\u2019ll use sympy library.\\n\\n```python\\nfrom sympy import symbols, sqrt, simplify\\n\\n# Define the symbols\\nx, f = symbols('x f')\\n\\n# Define the functions\\nt = sqrt(3*x + 1)\\nf = 5 - t\\n\\n# Evaluate t at f(5)\\nt.subs(x, f.subs(x, 5))\\n```\\n\\n```\\n2\\n```\\n\\nSo the value of \\\\( t(f(5)) \\\\) is \\\\( 2 \\\\).\\n\\n**Solution 2**\\n\\nFirst let\u2019s calculate \\\\( t(x) \\\\) for \\\\( x = 5 \\\\):\\n\\n\\\\[\\nt(5) = \\\\sqrt{3 \\\\times 5 + 1} = \\\\sqrt{16} = 4.\\n\\\\]\\n\\nThen let\u2019s calculate \\\\( f(x) \\\\) for \\\\( x = 5 \\\\):\\n\\n\\\\[\\nf(5) = 5 - t(5) = 5 - 4 = 1.\\n\\\\]\\n\\nFinally let\u2019s calculate \\\\( t(f(5)) \\\\):\\n\\n\\\\[\\nt(f(5)) = t(1) = \\\\sqrt{3 \\\\times 1 + 1} = \\\\sqrt{4} = 2.\\n\\\\]\\n\\nSo the answer is \\\\( 2 \\\\).\\n\\n**Solution 3**\\n\\n```python\\ndef t(x):\\n    return (3 * x + 1) ** 0.5\\n\\ndef f(x):\\n    return 5 - t(x)\\n\\nt(f(5))\\n```\\n\\n```\\n2.0\\n```\\n\\nThus the answer is \\\\( 2 \\\\).\\n\\nFigure 9: Three representative solutions for a sample problem from MATH training set in OpenMathInstruct-1. The first and third solutions use code, with the first one using sympy library. The second solution is text-based.\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 24, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.3 MATH Training Data Split by Subjects\\n\\nTable 11: MATH training set decomposition by subjects.\\n\\n| Subject            | # of Training Instances |\\n|--------------------|-------------------------|\\n| Algebra            | 1744                    |\\n| Geometry           | 870                     |\\n| Intermediate Algebra | 1295                  |\\n| Number Theory      | 869                     |\\n| Prealgebra         | 1205                    |\\n| Precalculus        | 746                     |\\n| Probability        | 771                     |\\n| **Total**          | **7500**                |\\n\\nB Miscellaneous\\n\\nB.1 Training Hyperparameters\\n\\nTable 12: Details of training hyperparameters for finetuning the different base models. LR=Learning rate, TP=Tensor Parallel, PP=Pipeline Parallel.\\n\\n| Model              | Epochs | LR    | # of GPUs | TP | PP |\\n|--------------------|--------|-------|-----------|----|----|\\n| Mistral-7B         | 4      | 1e-6  | 64        | 4  | 1  |\\n| CodeLlama-7B       | 4      | 2e-5  | 64        | 4  | 1  |\\n| CodeLlama-13B      | 4      | 2e-5  | 64        | 4  | 1  |\\n| CodeLlama-34B      | 4      | 1e-5  | 128       | 8  | 1  |\\n| Llama 2-70B        | 2      | 1e-5  | 256       | 8  | 2  |\\n| CodeLlama-70B      | 3      | 1e-5  | 256       | 8  | 2  |\\n\\nTable 12 details the hyperparameters used for finetuning the different base models. Due to compute limitations, for 70B models, we limit the training to 2 epochs for Llama2 and 3 epochs for CodeLlama. For CodeLlama models, we use the learning rate of 2e-5 used by ToRA models [8]. For the Mistral-7B model, we use a lower learning rate of 1e-6 based on our hyperparameter search.\\n\\nB.2 Sample Solutions\\n\\nIn this section, we illustrate sample solutions representative of different phenomena encountered during the creation of OpenMathInstruct-1.\\n\\n- Figure 9 presents a sample problem from the MATH training set with its representative solutions from the OpenMathInstruct-1 dataset.\\n- Figure 10 shows a sample solution that utilizes the strength of the code-interpreter solution format with reasoning in both code and natural language.\\n- Figure 11 demonstrates a sample solution generated when the reference solution is used in the few-shot prompt. The model copies the children\u2019s ages from the reference solution and initializes the child_age variable. Such solutions are the reason why we propose the use of masked text solutions in the prompt.\\n- Figure 12 illustrates a sample solution where the solution goes beyond answering the question, with the model generating coherent but unrelated text for the input problem.\\n- Figure 13 shows a sample solution where the generated solution gets the right answer but through flawed reasoning. These semantically noisy solutions are much harder to detect with simple syntactic filters. One solution might be to use models like GPT-4 to grade the generated solutions as done in recent work [5]. We leave the work of developing such semantic filters for future work.\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 25, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Let \\\\( p(x) = \\\\sqrt{-x} \\\\), and \\\\( q(x) = 8x^2 + 10x - 3 \\\\). The domain of \\\\( p(q(x)) \\\\) can be written in the form \\\\( a \\\\leq x \\\\leq b \\\\). Find \\\\( b - a \\\\).\\n\\n**Generated Solution**\\n\\nLet\u2019s use sympy to solve for the domain of \\\\( p(q(x)) \\\\).\\n\\n```python\\nimport sympy as sp\\n\\n# define the symbols\\nx = sp.symbols('x')\\n\\n# define the functions\\np = sp.sqrt(-x)\\nq = 8*x**2 + 10*x - 3\\n\\n# solve for the domain of p(q(x))\\ndomain = sp.solve(q >= 0, x)\\n\\n# print the domain\\nprint(domain)\\n```\\n\\n```python-output\\n((1/4 <= x) & (x < oo)) | ((-oo < x) & (x <= -3/2))\\n```\\n\\nSo the domain is \\\\( x \\\\in [1/4, \\\\infty) \\\\cup (-\\\\infty, -3/2) \\\\).\\n\\nThe difference between the upper and lower bounds is \\\\( b - a = \\\\frac{1}{4} - \\\\left(-\\\\frac{3}{2}\\\\right) \\\\).\\n\\nFigure 10: Sample solution where the model performs the initial reasoning in code and performs the last reasoning steps to answer the question in natural language. This shows the strength of the code-interpreter solution format for mathematical reasoning.\\n\\n### B.3 Error Analysis of Solutions Generated by Fine-tuned Model\\n\\nIn this section, we illustrate instances of the different kinds of errors made by the ablation model analyzed in Section 5.\\n\\n- Figure 14 demonstrates a sample where the model performs correct reasoning while generating the code. However, the model falters at copying the code output and generates a new answer.\\n\\n- Figure 15 presents an example where the model generates correct but inefficient code that doesn\u2019t finish in our stipulated execution time and results in the *timeout* error.\\n\\n- Figure 16 illustrates a sample where the model performs correct reasoning but falters in arithmetic calculation (multiplication). Failure at arithmetic computation has been a known issue with LLMs and justifies our preference for code-based solutions.\\n\\n- Figure 17 shows an example where the model generates a wrong code block to solve the problem and repeatedly generates the same code block till it runs into our maximum execution constraint. This example shows a limitation of our current dataset where we lack examples in which the model recovers from its error, unlike GPT-4 [23].\\n\\n- Figure 18 shows a sample solution where the code generated in the solution runs into an execution error. Nevertheless, the model still generates an *incorrect* answer to the question.\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 26, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jolene and Phil have four children, each with the same birthday. They gave birth to their first child exactly 15 years ago. They gave birth to their second child exactly one year after the birth of their first child. They gave birth to their third child on the fourth birthday of their second child. Two years after the birth of their third child, they gave birth to their fourth child. How old, in years, is their fourth child?\\n\\nTheir 1st child was born 15 years ago, and therefore is 15 years old. Their 2nd child was born 1 year after their 15-year-old child, and therefore is 15-1=14 years old. Their 3rd child was born 4 years after their 14-year-old child, and therefore is 14-4=10 years old. Their 4th child was born 2 years after their 10-year-old child, and therefore is 10-2=8 years old.\\n\\nLet\u2019s write down a python script to answer this problem.\\n\\n```python\\nchildren = ['first', 'second', 'third', 'fourth']\\nchild_age = [15, 14, 10, 8]\\nnumber_of_children = len(children)\\nchildren = children[:-1] # let's get rid of the youngest one since we already know that one\\ndictionary = dict(zip(children, child_age))\\ndictionary\\n</llm-code>\\n<llm-code-output>\\n{'first': 15, 'second': 14, 'third': 10}\\n</llm-code-output>\\n```\\n\\nThe answer is 8 years old.\\n\\nFigure 11: Sample shortcut solution generated from prompting the LLM with the reference solution in context. The code initializes the variable `child_age` with the children\u2019s ages copied from the reference solution. The generated code then performs ad-hoc computations before answering the question in the text.\\n\\n### B.4 Instructions for Few-shot Data Generation\\n\\n| Task | Instruction |\\n|------|-------------|\\n| Few-shot prompt (I) | Here are some examples of questions and solutions followed by a new question that you need to solve. Make sure to put the answer (and only answer) inside \\\\boxed{}.\\n| Few-shot prompt text masking (I\\\\_mask) | Here are some examples of questions, solutions, and their masked solutions followed by a new question and solution that you need to mask. The goal is to ensure that the masked solution doesn\u2019t have any of the numerical values not mentioned in the question. So intermediate values calculated in the solution are to be masked by single letter capital variables, such as M, N.\\n| Zero-shot prompt for fine-tuned models | System: You\u2019re an expert Python programmer and mathematician. Help the user to solve this problem using code when necessary. Make sure to put the answer (and only answer) inside \\\\boxed{}.\\n\\nTable 13 details the instructions used for the different generation tasks.\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 27, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Caroline can make eleven lassis out of two mangoes. How many lassis can she make out of twelve mangoes?\\n\\nLet $f(x)$ be the number of lassis she can make out of $x$ mangoes. From the question, we can see that $f(2) = 11$. Using basic algebra, we can see that $f(12) = 66$.\\n\\nLet\u2019s verify this with sympy.\\n\\n```python\\nimport sympy as sp\\n\\n# define the unknown function\\nx, y = sp.symbols('x y')\\n\\n# let\u2019s define the parabola\\nparabola = sp.Eq(y, x**2 + b*x + c)\\n\\n# substitute points into parabola equation and solve for b, c\\npoint_1 = parabola.subs({x: -1, y: -11})\\npoint_2 = parabola.subs({x: 3, y: 17})\\nsolutions = sp.solve((point_1,point_2), (b, c))\\nsolutions[b]\\n```\\n\\nFigure 12: Sample solution where the model generates unrelated text after answering the question. During post-processing, we trim the solution by removing text beyond the solution line with the answer.\\n\\n### B.5 Masked Text Solution Generation\\n\\nWe generate masked text solutions using a pipeline very similar to the solution generation pipeline. We use the following procedure:\\n\\n- Generate eight candidate masked solutions.\\n- Filter out solutions that have very different lengths.\\n- Filter out solutions that have the final answer. This ensures that our masked solutions are at least masking the final answer.\\n- Rank solutions where higher-ranked solutions have fewer numbers. This is essentially a heuristic to ensure we pick solutions where the intermediate computation is masked out the most.\\n\\n### B.6 Few-shot prompts used for GSM8K and MATH\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 28, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The areas of two squares are in the ratio 25 : 36. What is the ratio of their perimeters? Express your answer in the form $a : b$.\\n\\nIn general, if the ratio of the perimeters of two similar figures is $a/b$, then the ratio of their areas is $(a/b)^2$. So in this case, $(a/b)^2 = 25/36$, or $a/b = 5/6$.\\n\\nSo the answer is $5 : 6$.\\n\\nLet\u2019s use sympy to print out the difference between the ratio of their perimeters and each of the options.\\n\\n```python\\nfrom sympy import Rational, Abs\\n\\n# areas are in the ratio 25:36\\narea_ratio = Rational(25, 36)\\n\\n# list of options\\noptions = [Rational(5, 6), Rational(5, 4), Rational(5, 3), Rational(5, 2), Rational(5, 1)]\\n\\n# let\u2019s print out the differences\\n[Abs(area_ratio - frac_option) for frac_option in options]\\n```\\n\\nLet\u2019s now check which difference is the smallest.\\n\\n```python\\nimport numpy as np\\n\\n# Calculate the idx of the closest option\\nmin_idx = np.argmin([5/36, 5/9, 35/36, 65/36, 155/36])\\n\\n# Print the closest option\\nprint(options[min_idx])\\n```\\n\\nSo the answer is $5 : 6$.\\n\\nFigure 13: Sample solution where the model uses completely flawed reasoning to arrive at the correct answer. Such flawed reasoning is hard to detect, but fortunately, these solutions are rare.\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 29, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question\\n\\nWhat is the 100th digit to the right of the decimal point in the decimal representation of $\\\\frac{13}{90}$?\\n\\nGenerated Solution\\n\\nWe can use sympy to calculate the decimal representation of $\\\\frac{13}{90}$ and then extract the 100th digit.\\n\\n```python\\nfrom sympy import Rational, N\\n\\ndecimal_rep = N(Rational(13, 90), 100)\\n\\ndigit = int(str(decimal_rep)[-1])\\n\\nprint(digit)\\n```\\n\\n4\\n\\nSo the 100th digit to the right of the decimal point in the decimal representation of $\\\\frac{13}{90}$ is 4.\\n\\nFigure 14: The model performs correct reasoning till code generation. But rather than copying the code output (4) it concludes with a different answer (7).\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 30, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Let $a_1, a_2, \\\\ldots$ be a sequence for which $a_1 = 2$, $a_2 = 3$, and $a_n = \\\\frac{a_{n-1}}{a_{n-2}}$ for each positive integer $n \\\\geq 3$. What is $a_{2006}$?\\n\\n**Generated Solution**\\n\\nLet\u2019s write a function that calculates $a_n$ for a given $n$.\\n\\n```python\\ndef a_n(n):\\n    if n == 1:\\n        return 2\\n    elif n == 2:\\n        return 3\\n    else:\\n        return a_n(n-1) / a_n(n-2)\\n\\nprint(a_n(2006))\\n```\\n\\nNone\\n\\nSo $a_{2006} = \\\\frac{1}{5}$.\\n\\nFigure 15: The model generates recursion-based code to solve the problem. While the code is technically correct, the lack of memoization means the code ends up being extremely inefficient and runs into timeout.\\n\\n---\\n\\n**Calculation Error**\\n\\nOur basketball team has 12 members, each of whom can play any position. In how many ways can we choose a starting lineup consisting of a center, a power forward, a shooting forward, a point guard, and a shooting guard?\\n\\n**Generated Solution**\\n\\nWe can choose a starting lineup in $12 \\\\times 11 \\\\times 10 \\\\times 9 \\\\times 8 = 11880$ ways.\\n\\nFigure 16: Sample solution where the model performs the right text reasoning but makes calculation mistake.\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 31, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In $\\\\triangle ABC$, $AB = 425$, $BC = 450$, and $AC = 510$. An interior point $P$ is then drawn, and segments are drawn through $P$ parallel to the sides of the triangle. If these three segments are of an equal length $d$, find $d$.\\n\\nLet\u2019s use sympy to solve this problem.\\n\\n```python\\nfrom sympy import symbols, Eq, solve\\n\\n# Define the variables\\nAB, BC, AC, d = symbols('AB BC AC d')\\n\\n# Define the equations\\neq1 = Eq(AB, 425)\\neq2 = Eq(BC, 450)\\neq3 = Eq(AC, 510)\\neq4 = Eq(AB + BC, 2 * d)\\neq5 = Eq(BC + AC, 2 * d)\\neq6 = Eq(AC + AB, 2 * d)\\n\\n# Solve the equations\\nsolutions = solve((eq1, eq2, eq3, eq4, eq5, eq6), (AB, BC, AC, d))\\nsolutions\\n```\\n\\nThe solutions are empty, so we can\u2019t solve this problem using sympy.\\n\\nLet\u2019s try to solve it manually.\\n\\nWe can see that the sum of any two sides of a triangle is greater than the third side. So we can write the following inequalities:\\n\\n$AB + BC > AC$\\n$BC + AC > AB$\\n$AC + AB > BC$\\n\\nLet\u2019s rewrite them using the given values:\\n\\n$425 + 450 > 510$\\n$450 + 510 > 425$\\n$510 + 425 > 450$\\n\\nWe can solve these inequalities using sympy:\\n\\n...\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 32, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The area of a triangle is 600 square feet. Find the altitude, in feet, of the triangle if the length of the corresponding base is 30 feet.\\n\\nLet\u2019s use sympy to solve this problem.\\n\\n```python\\nfrom sympy import symbols, Eq, solve\\n\\n# define the variables\\nbase, altitude = symbols('base altitude')\\n\\n# area of the triangle\\narea_eq = Eq(base * altitude / 2, 600)\\n\\n# length of the corresponding base\\nbase_eq = Eq(base, 30)\\n\\n# solve the equations\\nsolutions = solve((area_eq, base_eq), (base, altitude))\\n\\n# print the altitude\\nprint(solutions[altitude])\\n```\\n\\n```\\n[0;31m--------------------------[0m\\n[0;31mTypeError[0m Traceback (most recent call last)\\nFile [0;32m<ipython-input-1-f95732badac7>:16[0m\\n[1;32m 13[0m solutions [38;5;241m=[39m solve((area_eq, base_eq), (base, altitude))\\n[1;32m 15[0m # print the altitude[39;00m\\n[0;32m---> 16[0m [38;5;66;03mprint[39m([43msolutions[49m[43m[[49m[43maltitude[49m[43m[49m[43m]\\n\\n[0;31mTypeError[0m: list indices must be integers or slices, not Symbol\\n```\\n\\nSo the altitude is 20.\\n\\nFigure 18: Sample solution where the model generated code runs into execution error. Nevertheless, the model still generates an *incorrect* answer to the question.\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 33, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Listing 1: Few-shot prompt for GSM8K.\\n\\nHere are some examples of questions and solutions followed by a new question that you need to solve.\\nMake sure to put the answer (and only answer) inside \\\\boxed{}.\\n\\nQuestion:\\nMissy had a giant piggy bank in her bedroom. Every day she would search the house for change to put in her bank. After 4 years, the bank was opened and it contained $450 in change. If the second, third, and fourth-year she doubled the amount of money she put in the bank from the amount she had put in the previous year, how much money, in dollars, did she put in the bank the first year?\\n\\nMy solution:\\nMissy started with some amount of money in the first year. She was then doubling the amount of money every year for 4 years and ended up with $450.\\nLet\u2019s write down an equation for this problem and solve it using sympy.\\n\\n```python\\nfrom sympy import solve, symbols\\nfirst_year_money = symbols('first_year_money')\\nsecond_year_money = 2 * first_year_money\\nthird_year_money = 2 * second_year_money\\nfourth_year_money = 2 * third_year_money\\n# Solve for first year money given that the combined saving is 450\\nresult = solve(first_year_money + second_year_money + third_year_money + fourth_year_money - 450, first_year_money)[0]\\nresult\\n```\\n\\n```\\n30\\n```\\n\\nThus Missy put \\\\boxed{30} dollars in the bank the first year.\\n\\nQuestion:\\nPete has to take a 10-minute walk down to the train station and then board a 1hr 20-minute train to LA. When should he leave if he cannot get to LA later than 0900 hours? (24-hr time)\\n\\nMy solution:\\nSince Pete needs to take a 10 minutes walk and then a 1 hour 20 minutes train ride, he will spend a total of 1 hour and 30 minutes. This means that he needs to leave 1 hour and 30 minutes earlier than 09:00 hours.\\nSubtracting 1 hour and 30 minutes from 09:00 hours we get \\\\boxed{07:30} hours.\\n\\nQuestion:\\nMark deposited $88 in a bank. Bryan deposited $40 less than five times as much as Mark. How much did Bryan deposit in the bank?\\n\\nMy solution:\\nLet\u2019s solve this problem using Python code.\\n\\n```python\\nmark_deposit = 88\\nfive_times_mark_deposit = 5 * mark_deposit\\n```\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 34, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"bryan_deposit = five_times_mark_deposit - 40\\nbryan_deposit\\n</llm-code>\\n400\\n</llm-code-output>\\nSo Bryan deposited \\\\boxed{400} dollars in the bank.\\n\\nQuestion:\\nA department store displays a 20% discount on all fixtures. What will be the new price of a 25 cm high bedside lamp that was worth $120?\\n\\nMy solution:\\nLet\u2019s solve this problem using Python code.\\n<llm-code>\\ndiscount_percent = 20\\nprice_before_discount = 120\\ndiscount_portion = discount_percent / 100\\ndiscount_amount = price_before_discount * discount_portion\\nprice_after_discount = price_before_discount - discount_amount\\nprice_after_discount\\n</llm-code>\\n96.0\\n</llm-code-output>\\nSo the new price of the lamp is \\\\boxed{96} dollars.\\n\\nQuestion:\\nJames opens up a flower shop. He needs to pay rent of $1200 a week with an additional 20% of rent to pay for utilities and he has 2 employees per shift with the store open 16 hours a day for 5 days a week. If he pays each employee $12.50 an hour, what are his weekly expenses to run the store?\\n\\nMy solution:\\nThe cost consists of rent, utilities, and employee salaries. Let\u2019s compute each of them separately and then add them up.\\n<llm-code>\\n# rent cost\\nrent_per_week = 1200\\n# utility cost\\nutility_per_week = rent_per_week * 20 / 100\\n# employee cost\\nemployee_work_hours = 16\\nwork_days_per_week = 5\\nemployee_work_hours_per_week = work_days_per_week * employee_work_hours\\nnumber_of_employees = 2\\nemployee_cost_per_hour = 12.5\\nemployees_cost_per_week = number_of_employees * employee_work_hours_per_week * employee_cost_per_hour\\n# add the three to get total cost\\ncost_per_week = rent_per_week + utility_per_week + employees_cost_per_week\\ncost_per_week\\n</llm-code>\\n3440.0\\n</llm-code-output>\\nThus James\u2019s weekly expenses add up to \\\\boxed{3440} dollars.\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 35, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question:\\nTim has 30 less apples than Martha, and Harry has half as many apples as Tim. If Martha has 68 apples, how many apples does Harry have?\\n\\nMy solution:\\n35\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 36, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Listing 2: Few-shot prompt for MATH.\\n\\nHere are some examples of questions and solutions followed by a new question that you need to solve.\\nMake sure to put the answer (and only answer) inside \\\\boxed{}.\\n\\nQuestion:\\nA parabola with equation $y=x^2+bx+c$ passes through the points $(-1,-11)$ and $(3,17)$. What is $c$?\\n\\nMy solution:\\nLet\u2019s write down an equation for the parabola and solve for $c$ using sympy.\\n\\n```python\\nimport sympy as sp\\n\\n# define the symbols\\nx, y, b, c = sp.symbols('x y b c')\\n\\n# define the parabola equation\\nparabola_eq = sp.Eq(y, x**2 + b*x + c)\\n\\n# the parabola passes through the points (-1,-11) and (3,17)\\n# so we substitute these points into the parabola equation\\npoint_1 = parabola_eq.subs({x: -1, y: -11})\\npoint_2 = parabola_eq.subs({x: 3, y: 17})\\n\\n# we now have two equations and two unknowns (b and c)\\n# we will solve for b and c\\nsolutions = sp.solve((point_1,point_2), (b, c))\\nsolutions[c]\\n```\\n\\nSo $c$ is \\\\boxed{-7}\\n\\nQuestion:\\nLet $f(x)$ be an odd function. Is $f(f(x))$ even, odd, or neither?\\n\\nEnter \\\"odd\\\", \\\"even\\\", or \\\"neither\\\".\\n\\nMy solution:\\nThe definition of an odd function is that $f(-x) = -f(x)$.\\nApplying this to $f(f(-x))$ we get $f(f(-x)) = f(-f(x)) = -f(f(x))$.\\nThus, $f(f(x))$ is an \\\\boxed{odd} function.\\n\\nQuestion:\\nAt the 2007 Math Olympics, Team Canada won $17$ out of a possible $100$ medals. Which one of the following is closest to the fraction of medals that they won? $$\\\\frac{1}{4} \\\\quad \\\\frac{1}{5} \\\\quad \\\\frac{1}{6} \\\\quad \\\\frac{1}{7} \\\\quad \\\\frac{1}{8}$$\\n\\nMy solution:\\nLet\u2019s use sympy to print out the differences between the fraction of medals that Canada won and each of the options.\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 37, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"from sympy import Rational, Abs\\n\\n# team Canada won 17 out of 100 medals\\nmedal_frac = Rational(17, 100)\\n\\n# list of options\\noptions = [Rational(1, 4), Rational(1, 5), Rational(1, 6), Rational(1, 7), Rational(1, 8)]\\n\\n# let\u2019s print out the differences\\n[Abs(medal_frac - frac_option) for frac_option in options]\\n\\nLet\u2019s now check which difference is the smallest.\\n\\nimport numpy as np\\n\\n# Calculate the idx of the closest option\\nmin_idx = np.argmin([2/25, 3/100, 1/300, 19/700, 9/200])\\n\\n# Print the closest option\\nprint(options[min_idx])\\n\\nSo the answer is \\\\boxed{1/6}.\\n\\nQuestion:\\nA rectangular box $P$ is inscribed in a sphere of radius $r$. The surface area of $P$ is 384, and the sum of the lengths of its 12 edges is 112. What is $r$?\\n\\nMy solution:\\nLet the dimensions of $P$ be $x$, $y$, and $z$.\\nThe sum of the box\u2019s edges is $4(x + y + z)$ and the surface area is $2xy + 2yz + 2xz$.\\nThe diameter of the sphere is $2r$ and it\u2019s equal to the diagonal of the box.\\n\\nLet\u2019s now write down the equations based on the above information and solve them using sympy.\\n\\n# define the variables\\nx, y, z, r = symbols(\u2019x y z r\u2019)\\n\\n# equations based on the given information\\neq1 = Eq(2 * (x*y + x*z + y*z), 384)\\neq2 = Eq(4 * (x + y + z), 112)\\n\\ndiagonal_eq = Eq(x**2 + y**2 + z**2, (2*r)**2)\\n\\n# solve the equations\\nsolutions = solve((eq1, eq2, diagonal_eq), (x, y, z, r))\\n\\n# let\u2019s see which values of r we get\"}"]}
{"id": "Mbd3QxXjq5", "page_num": 38, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Since the radius of the sphere is positive, we get $r = \\\\boxed{10}$.\\n\\nQuestion:\\nA bee starts flying from point $P_0$. She flies $1$ inch due east to point $P_1$. For $j \\\\ge 1$, once the bee reaches point $P_j$, she turns $30^\\\\circ$ counterclockwise and then flies $j+1$ inches straight to point $P_{j+1}$. When the bee reaches $P_{2015}$, how far from $P_0$ is she, in inches?\\n\\nMy solution:\\nWe can represent the rotation action via $\\\\omega = e^{\\\\pi i/6}$.\\nLet\u2019s assume the bee starts at the origin, so $P_{2015}$ is at the point $z = 1 + 2 \\\\omega + 3 \\\\omega^2 + 4 \\\\omega^3 + \\\\ldots + 2015 \\\\omega^{2014}$.\\nThis is an arithmetic-geometric series which we can solve by simplifying the expression.\\nAlternatively, we can solve for $|z|$ using sympy using the following code.\\n\\n```python\\nfrom sympy import I, pi, exp, sqrt\\n\\n# rotation action of 30 degrees\\nomega = exp(I * pi/6)\\n\\nposition = 0\\nfor i in range(2015):\\n    delta = (i + 1) * omega**(i)\\n    position += delta\\n\\nreal, img = (position.as_real_imag())\\n# Distance from origin i.e. starting point\\ndist = sqrt(real**2 + img**2)\\nprint(dist)\\n```\\n\\nSo the bee is $\\\\boxed{1008\\\\sqrt{2} + 1008\\\\sqrt{6}}$ far from the starting point.\\n\\nQuestion:\\nIf $f(x) = x^2 - 1$, what is the value of $f(-1)$?\\n\\nMy solution:\"}"]}
