{"id": "WVQ4Clw1VD", "page_num": 1, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine\\n\\nYunfei Xie\\\\textsuperscript{1,*}, Ce Zhou\\\\textsuperscript{1,*}, Lang Gao\\\\textsuperscript{1,*}, Juncheng Wu\\\\textsuperscript{2,*}, Xianhang Li\\\\textsuperscript{3}, Hong-Yu Zhou\\\\textsuperscript{4}, Sheng Liu\\\\textsuperscript{5}, Lei Xing\\\\textsuperscript{5}, James Zou\\\\textsuperscript{5}, Cihang Xie\\\\textsuperscript{3}, Yuyin Zhou\\\\textsuperscript{3}\\n\\n\\\\textsuperscript{*}equal technical contribution\\n\\n\\\\textsuperscript{1}Huazhong University of Science and Technology, \\\\textsuperscript{2}Tongji University, \\\\textsuperscript{3}UC Santa Cruz, \\\\textsuperscript{4}Harvard University, \\\\textsuperscript{5}Stanford University\\n\\nAbstract\\n\\nThis paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities, with multigranular annotations for more than 65 diseases. These enriched annotations encompass both global textual information, such as disease/lesion type, modality, region-specific descriptions, and inter-regional relationships, as well as detailed local annotations for regions of interest (ROIs), including bounding boxes, segmentation masks. Unlike existing approach which is limited by the availability of image-text pairs, we have developed the first automated pipeline that scales up multimodal data by generating multigranular visual and textual annotations (in the form of image-ROI-description triplets) without the need for any paired text descriptions. Specifically, data from over 90 different sources have been collected, preprocessed, and grounded using domain-specific expert models to identify ROIs related to abnormal regions. We then build a comprehensive knowledge base and prompt multimodal large language models to perform retrieval-augmented generation with the identified ROIs as guidance, resulting in multigranular textual descriptions. Compared to existing datasets, MedTrinity-25M provides the most enriched annotations, supporting a comprehensive range of multimodal tasks such as captioning and report generation, as well as vision-centric tasks like classification and segmentation. This dataset can be utilized to support large-scale pre-training of multimodal medical AI models, contributing to the development of future foundation models in the medical domain. The dataset is publicly available at \\\\url{https://yunfeixie233.github.io/MedTrinity-25M/}.\\n\\n1 Introduction\\n\\nLarge-scale multimodal foundation models \\\\cite{1, 2, 3, 4, 5} have demonstrated remarkable success across various domains due to their ability to understand complex visual patterns in conjunction with natural language. This success has sparked significant interest in applying such models to medical vision-language tasks. Much progress has been made to improve the medical capacity of general domain multimodal foundation models by constructing medical datasets with image-text pairs and fine-tuning general domain models on these datasets \\\\cite{6, 7, 8, 9, 10}.\\n\\nHowever, current medical datasets have several limitations. Firstly, these datasets lack multigranular annotations that reveal the correlation between local and global information within medical images.\"}"]}
{"id": "WVQ4Clw1VD", "page_num": 2, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Medical images often contain detailed cues, such as regional abnormal textures or structures, which may indicate specific types of lesions. Therefore, multimodal models need the ability to infer global information, such as disease or lesion type, from local details. The absence of such data limits the models\u2019 capacity to comprehensively understand medical images. Moreover, current dataset construction methods heavily rely on medical images paired with reports or captions, which restricts their scalability.\\n\\nIn this paper, we address the above challenges by proposing an automated data construction pipeline using multimodal large language models (MLLMs) without relying on paired text descriptions. To address the lack of comprehensive medical knowledge in general-purpose MLLMs, we leverage domain-specific expert grounding models and retrieval-augmented generation (RAG) to extract relevant medical knowledge. We then prompt MLLMs to generate multigranular visual and textual annotations enriched with this knowledge based on identified regions of interest (ROIs). We utilize this pipeline to transform the collected data, including large-scale unpaired images, into image-ROI-description triplets. These triplets provide multigranular annotations that encompass both global textual information, such as disease/lesion type, modality, and inter-regional relationships, as well as detailed local annotations for ROIs, including bounding boxes, segmentation masks, and region-specific textual descriptions. Using the proposed pipeline, we create a large-scale multimodal multigranular medical dataset containing over 25 million triplets, named MedTrinity-25M. To our best knowledge, this is the largest multimodal dataset in medicine to date.\\n\\nInitially, we assemble a large amount of medical data from over 90 online resources such as TCIA, Kaggle, Zenodo, Synapse, etc. In addition to images with a small amount of high-quality paired manual reports, this assembled data also includes two types of coarse medical data: 1) Image data with segmentation masks, lesion bounding boxes, or only disease types but lacking detailed textual descriptions, and 2) Images paired with coarse captions that describe only global modality or disease information, but lack detailed descriptions of local regions. To generate multigranular annotations from the massive coarse medical data, we first identify ROIs that contain disease or lesion patterns by applying expert grounding models. We then build a comprehensive knowledge base from online corpora (e.g., PubMed) and retrieve image-related medical knowledge. Finally, we prompt MLLMs to integrate medical knowledge with guidance of identified ROIs to generate multigranular textual descriptions.\\n\\n2 Related Work\\n\\nMedical Multimodal Foundation Models. Due to the effectiveness of multimodal foundation models in understanding visual features, adapting these models to perform medical vision-language tasks has garnered increasing attention in recent years [11, 12, 9, 5]. Several papers attempt to adapt general domain multimodal foundation models with varying architecture to medical domain through end-to-end training on medical datasets. For example, Med-Flamingo [11] enhances the medical capacity of OpenFlamingo-9B [13] by fine-tuning it with 0.8M interleaved and 1.6M paired medical image-text data. While Med-PalM [12] adapts PaLM-E [14] to medical domain using approximately 1M medical data points, demonstrating competitive or surpassing performance compared to state-of-the-art models. Additionally, LLaVA-Med [9] employs end-to-end visual instruction tuning [1] with two stages, achieving remarkable results in medical Visual Question Answering (VQA) tasks. Similarly, Med-Gemini [15] employs a long-form question answering dataset to enhance the multimodal and long-context capabilities of baseline Gemini [16]. Although these models have achieved remarkable performance, they are still limited by the scale of training data. Prior research [17] has shown that scaling up the training data improves the performance of large multimodal foundation models. In this paper, we aim to build a large-scale medical dataset to facilitate the development of more powerful medical multimodal foundation models.\\n\\nMultimodal Datasets for medicine. The significance of constructing comprehensive medical multimodal datasets has garnered considerable attention [9, 18, 19, 7]. Several works attempt to collect images and paired clinical reports prepared by pathology specialist [19, 7, 8], which provide\"}"]}
{"id": "WVQ4Clw1VD", "page_num": 3, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) Qualitative Comparison with sample in radiology report of chest x-rays dataset MIMIC-CXR [21].\\n\\n(b) Qualitative Comparison with sample in visual QA dataset SLAKE [22].\\n\\n(c) Qualitative Comparison with sample in radiology objects caption dataset ROCO [18].\\n\\nFigure 1: Qualitative comparison with different types of dataset.\\n\\n3 MedTrinity-25M Dataset\\n\\n3.1 Data Triplet\\n\\nOur dataset comprises triplets of \\\\{image, ROI, description\\\\}. Each ROI is associated with an abnormality and is represented by a bounding box or a segmentation mask, specifying the relevant region within the image. For each image, we provide a multigranular textual description, which includes the disease/lesion type, modality, region-specific description, and inter-regional relationships as illustrated in Figure 2.\"}"]}
{"id": "WVQ4Clw1VD", "page_num": 4, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Images. We use the original medical image in the source dataset, we extensively collected medical datasets from the following sources: (1) online resources such as TCIA, Kaggle, Zenodo, Synapse, Hugging Face, Grand Challenge, GitHub, etc. (2) relevant medical dataset research, such as CheXpert [7] and DeepLesion [23]. These datasets were first categorized into two types: (1) datasets containing local annotations, such as MIMIC-CXR [8] with corresponding radiology reports, and PMC-OA [24] with corresponding captions, where the reports or captions provide analysis of specific local conditions in the images; another example is the 3D image segmentation dataset BraTS2024 [25], which marks the tumor regions in CT scans with masks. (2) datasets containing global annotations: such as image classification datasets ISIC2019 [26] and ISIC2020 [27], whose classification labels reflect the overall pathological condition of tissue sections; another example is the CheXpert [7] dataset, which provides detailed classification of disease types for each chest X-ray. We collect 25,001,668 samples spanning 10 modalities and over 65 diseases. For 3D volumetric images stored in DICOM or NIfTI formats, we converted each 2D slice to PNG format. Additional caption and annotations like masks and bounding boxes from these datasets were utilized to construct ROIs and corresponding textual descriptions as below.\\n\\nROIs. For each image, ROIs are highlighted using segmentation masks or bounding boxes. These ROIs mostly contain pathological findings such as lesions, inflammation, neoplasms, infections, or other potential abnormalities. In the few cases without abnormalities, the ROIs generally indicate the primary object or organ in the image, as shown in examples in the supplementary material.\\n\\nTextual Descriptions. The textual descriptions for each image are provided with detailed information across various aspects. Unlike the unstructured free-text descriptions found in previous medical report datasets [7, 8, 6] or simple short sentences in visual QA dataset [28, 22] and caption dataset [18, 24], our textual descriptions are multigranular and structured. General attributes related to the image are described first, including the image modality, the specific organ depicted, and the type of disease presented. Subsequently, ROI-related information is provided, including their locations and the abnormal characteristics within them that indicate underlying pathology, such as distinctive color and texture. Additionally, comparisons between the ROIs and surrounding regions are presented to highlight differences in features and the extent of disease progression.\\n\\nWe also demonstrate the multigranular textual descriptions in our dataset with those in other common forms. As illustrated in Figure 1, our textual description is multigranular with more attributes than radiology report of chest x-rays dataset MIMIC-CXR [21], visual QA dataset SLAKE [22] and radiology objects caption dataset ROCO [18].\\n\\n3.2 Data Construction Pipeline\\n\\nGiven a medical image, we aim to generate corresponding multigranular visual and textual annotations by leveraging MLLMs. Specifically, as shown in Figure 2, our pipeline can be decomposed into two stages - Data Processing and Generation of Multigranular Text Description. In the Data Processing stage (Section 3.2.1), we address the lack of domain-specific knowledge in general-purpose MLLMs by leveraging expert grounding models and retrieval-augmented generation (RAG). This stage includes three key steps: 1) Metadata Integration to produce coarse captions encapsulating fundamental image information such as modality and disease types; 2) ROI Locating to identify regions of abnormalities; and 3) Medical Knowledge Retrieval to extract relevant fine-grained medical details. Based on the processed data, we then prompt MLLMs to generate multigranular text descriptions, resulting in the creation of fine-grained captions, as detailed in Section 3.2.2.\\n\\n3.2.1 Data Processing\\n\\nCoarse Caption Generation via Metadata Integration. We aim to generate coarse captions that provide fundamental information for a given image, including modality, organ labels, disease types, and optionally, camera views and equipment information. Instead of extracting features directly from the images, we generate these captions by integrating dataset metadata. We first extract metadata from\"}"]}
{"id": "WVQ4Clw1VD", "page_num": 5, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2: **Data construction pipeline.** 1) Data processing: extracting essential information from collected data, including **metadata integration** to generate coarse caption, **ROI locating**, and **medical knowledge collection**. 2) Multigranular textual description generation: using this information to prompt MLLMs to generate fine-grained captions.\\n\\nFigure 3: **A qualitative comparison example of generated textual description with and without coarse caption.** Without a coarse caption, MLLMs fails to detect diseases. On the contrary, providing a caption mentioning \u201cCOVID-19\u201d allows MLLMs to identify and categorize the disease, facilitating further analysis.\\n\\nthe datasets and then apply a fixed rule to integrate this information into coarse captions. For example, for an image from the QaTa-COV19 dataset[^1] we derive metadata from the dataset\u2019s accompanying paper or documentation, indicating that it consists of COVID-19 chest X-ray images. Next, we construct coarse captions like \u201cA chest X-ray image with COVID-19 in the lungs\u201d highlighting the modality, organ types, and disease labels. If the image contains additional textual information like radiological findings, this is also integrated to enhance the richness of the caption. The effectiveness of adding coarse captions when generating fine-grained captions is illustrated in Figure 3. In contrast to the scenario without a coarse caption where MLLMs fails to recognize the disease, providing MLLMs with a coarse caption that includes the disease type \u201cCOVID-19\u201d enables it to identify and categorize the disease, thereby laying the foundation for further analysis.\\n\\n**ROI Locating.** We employ various strategies to locate Regions of Interest (ROIs) in images. For datasets that already include localization annotations, such as segmentation masks or bounding boxes, we derive the ROIs from these existing annotations. Specifically, bounding boxes are directly used\\n\\n[^1]: https://www.kaggle.com/aysendegerli/qatacov19-dataset.\"}"]}
{"id": "WVQ4Clw1VD", "page_num": 6, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Without ROIs:\\nThe image is a chest X-ray showing both lungs, centrally positioned in the thoracic cavity, flanked by the ribs and the diaphragm visible at the bottom. The heart is visible in the center between the lungs. There are no\u2026\u2026. The lungs show patchy opacities suggesting an infectious process, consistent with pulmonary involvement in COVID-19.\\n\\nWith ROIs:\\nThe image is a chest X-ray showing both lungs and the heart centrally positioned between them. In two specific regions of interest located at the left-center and right-center of the middle of the lungs, there are unusual findings suggestive of COVID-19. These areas, occupying 8.3% and 5.0% of the image respectively display changes in lung texture that may indicate infection, such as increased opacity. The left-center region is slightly larger and potentially indicates a more extensive involvement of the lung tissue compared to the right-center region. These areas of alteration in the lung tissue are critical in understanding the spread and impact of COVID-19, affecting surrounding lung areas.\\n\\nFigure 4: A qualitative comparison example of generated textual description with and without locating ROIs. Without ROIs, the caption offers only a brief global analysis; with ROIs, MLLMs conduct detailed local analysis and assesses the impact of lesion ROIs on adjacent normal regions.\\n\\nWithout medical knowledge:\\nThe image is a chest X-ray showing both lungs and the heart centrally positioned between them. In two specific regions of interest located at\u2026\u2026 of the image respectively, display changes in lung texture that may indicate infection, such as increased opacity. The left-center region is slightly larger and potentially indicates a more extensive involvement of the lung tissue compared to the right-center region. These areas of alteration in the lung tissue are critical in understanding the spread and impact of COVID-19, affecting surrounding lung areas.\\n\\nWith medical knowledge:\\nThe image is a chest X-ray showing the thoracic cavity, primarily focusing on the lungs. Visible organs include the lungs and the heart, centrally positioned beneath the sternum and between the lungs. The regions of interest, located\u2026\u2026. These regions exhibit ground-glass opacities and consolidation, typical indicators of COVID-19 pneumonia, which suggest the presence of inflammatory processes. These affected areas are significant as they indicate the primary sites of infection and inflammation in COVID-19, often leading to bilateral and multifocal lung involvement as the disease progresses.\\n\\nFigure 5: A qualitative comparison example of generated textual description with and without external medical knowledge. MLLMs can standardize medical terminology in its expressions and refine its diagnosis based on disease progressions detailed in medical literature.\\n\\nMedical Knowledge Retrieval. General-purpose MLLMs often produce content that lacks specialized medical terminology and professional expression. To address this issue, we build a medical knowledge database following the approach in MedRAG [32]. We collect three main corpora: PubMed[2] for biomedical knowledge, StatPearls[3] for clinical decision support, and medical textbooks [33] for domain-specific knowledge. We segment these corpora into short snippets and encode\\n\\nhttps://pubmed.ncbi.nlm.nih.gov/\\nhttps://www.statpearls.com/\"}"]}
{"id": "WVQ4Clw1VD", "page_num": 7, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Example of ROIs and their corresponding textual descriptions.\\n\\n(a) Example of locating ROI via SAT\\\\cite{29}.\\n(b) Example of locating ROI via BA-Transformer\\\\cite{30}.\\n(c) Example of locating ROI via MedRPG\\\\cite{31}.\\n\\nFigure 7: An example of the Top-8 retrieval results. By leveraging COVID-19-related medical knowledge, MLLMs can standardize medical terminology and enhance diagnoses according to the disease progressions described in medical literature.\\n\\nthem into high-dimensional vectors using the text encoder from Med-CPT\\\\cite{34}. These vectors are then indexed into a specialized vector knowledge base using Faiss\\\\cite{35}, optimized for efficient retrieval.\\n\\nFor a given image, we retrieve relevant medical knowledge by using its coarse caption, which is generated through metadata integration. Specifically, we encode the coarse captions, including disease and organ classifications, into vectors using the Med-CPT text encoder. We then perform a vector similarity search in the medical vector database, retrieving the top eight medical knowledge snippets that semantically match the query. These snippets provide the external medical knowledge paired with the image. A qualitative example demonstrating the effectiveness of incorporating external medical knowledge is shown in Figure\\\\cite{7} With access to COVID-19-related medical knowledge, MLLMs can standardize medical terminology and refine diagnoses based on the disease progressions outlined in medical literature.\\n\\n3.2.2 Generation of Multigranular Text Description\\n\\nAfter data processing, a comprehensive prompt is utilized to guide the MLLMs in generating multigranular descriptions. The prompt template consists of a three-level hierarchical framework with questions to instruct MLLMs: (1) a global description that captures all details of the image; (2) a local-focused analysis of specific ROIs that potentially are unusual; and (3) a local-global examination of the interaction between local and global attributes to understand the impact of local abnormalities on the entire organ. Detailed prompt template is presented in supplementary materials.\\n\\nTo ensure that the MLLMs are guided by relevant medical information not inherently present in their training data, we incorporate the processed data (coarse captions, ROIs, and retrieved medical knowledge) into the prompts. Specifically, for global information, coarse captions are directly integrated into the prompt. For local information, ROIs on images are converted into textual descriptions based on their coordinates and area ratio within the images. Examples of these textual descriptions are shown in Figure\\\\cite{6} using terms such as \u201cleft-center\u201d and \u201carea ratio: 1.2%.\u201d\\n\\nTo refine terminology and diagnosis within ROIs, relevant medical knowledge about specific diseases is incorporated into the prompt. Instead of merely inserting this knowledge, we instruct MLLMs to identify and align the relevant knowledge to ROIs that require analysis.\\n\\nChoice of MLLMs We first prompt GPT-4V with the provided medical coarse captions, ROIs, and medical knowledge to generate a subset of 200,000 samples, maintaining a similar modality and organ distribution to our full 25 million dataset. The goal of curating this subset is to calibrate a medical knowledge-guided MLLM to adhere to the formatting instructions specified for our text.\"}"]}
{"id": "WVQ4Clw1VD", "page_num": 8, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The MRI image presents a transverse slice of the brain, where the cerebral hemispheres are visible, with the region of interest located centrally and towards the upper-middle portion of the image, occupying a small area ratio. This region exhibits an abnormality characterized by altered signal intensity, which contrasts with the surrounding brain tissue, suggesting the presence of a brain tumor. The abnormal area's position, relative to the rest of the brain, could imply an effect on or from adjacent structures, potentially influencing nearby tissue due to mass effect or being part of a larger pathological process within the brain.\\n\\nThe image is a transverse slice of a magnetic resonance imaging (MRI) scan of the brain, showing the cerebral hemispheres with the cerebrum visible. A region of interest is located at center part of the image horizontally and upper-middle part of image vertically, occupying approximately 3.9% of the total area. The region of interest is located in the white matter of the left cerebral hemisphere and appears to have an abnormal texture and size, which is indicative of a brain tumor. This abnormal area is likely to be affecting the surrounding brain tissue due to its mass effect or infiltration, which can cause a shift in the position of adjacent structures or potentially disrupt the normal brain function in its vicinity. The MRI scan provides a detailed view of the brain's anatomy, allowing for the visualization of soft tissue differences that may not be possible with other imaging modalities.\\n\\nFigure 8: Qualitative Comparison with sample generated by GPT-4V Compared to GPT-4V, our model generate more detailed caption.\\n\\nSubsequently, we employ our model, LLaVA-Med++, which is based on LLAVA-Med [9], the state-of-the-art medical MLLM. To further improve this model, we leverage the latest LLaMA3[36] to enhance its linguistic capabilities, and incorporate multi-scale feature extraction [37] to improve its vision capabilities. LLaVA-Med++ undergoes continuous training on medical multimodal data and is fine-tuned using our multigranular annotations, resulting in a specialized medical model.\\n\\nAfter fine-tuning, we then use this specialized model to generate the multigranular text descriptions on our entire dataset, resulting in 25 million image-ROI-description triplets. The fine-tuning process leverages the advanced language organization capabilities of GPT-4V, providing an effective template for fine-grained captions, which our model uses to learn the formatting of fine-grained captions. As a result, our model generates more detailed descriptions compared to GPT-4V, as illustrated in Figure 8. We also show a detailed quantitative comparison in the supplementary material.\"}"]}
{"id": "WVQ4Clw1VD", "page_num": 9, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Dataset          | Modality | Lesion Type | BBox/Mask | Color Texture | Description | Region | Relationship |\\n|------------------|----------|-------------|-----------|---------------|-------------|--------|--------------|\\n| MedMNIST         | \u2713        | \u2713           | \u2713         | \u2713             | \u2713           | \u2713      | \u2713            |\\n| DeepLesion       | \u2713        | \u2713           | \u2713         | \u2713             | \u2713           | \u2713      | \u2713            |\\n| BraTS 2024       | \u2713        | \u2713           | \u2713         | \u2713             | \u2713           | \u2713      | \u2713            |\\n| MIMIC-CXR        | \u2713        | \u2713           | \u2713         | \u2713             | \u2713           | \u2713      | \u2713            |\\n| Quilt-1M         | \u2713        | \u2713           | \u2713         | \u2713             | \u2713           | \u2713      | \u2713            |\\n| VQA-RAD          | \u2713        | \u2713           | \u2713         | \u2713             | \u2713           | \u2713      | \u2713            |\\n| CRC100K          | \u2713        | \u2713           | \u2713         | \u2713             | \u2713           | \u2713      | \u2713            |\\n| SA-Med2D-20M     | \u2713        | \u2713           | \u2713         | \u2713             | \u2713           | \u2713      | \u2713            |\\n| MedTrinity-25M(Ours) | \u2713        | \u2713           | \u2713         | \u2713             | \u2713           | \u2713      | \u2713            |\\n\\nTable 1: Comparison of dataset types based on provided attributes of annotations.\\n\\n4 Dataset Analysis\\n\\n**Diversity** Our dataset encompasses a wide range of 10 imaging modalities, with more than 65 diseases across various anatomical structures in human. The distribution of anatomical and biological structures in MedTrinity-25M is shown in Figure 9b. Meanwhile, the number of samples in the dataset for each modality are shown in Figure 9a, spanning from common ones with over 1 million samples each (CT, MRI, X-ray) to rare modalities (ultrasound, dermoscopy) with at least more than 100,000 samples, demonstrating a much more balanced distribution compared to other large-scale dataset like SA-Med2D-20M[38], which only contain thousands of ultrasound and dermoscopy samples.\\n\\n**Scale** Figure 9c shows the amount of our dataset, which is significantly larger than previous datasets. To the best of our knowledge, this is the largest open-source, multi-modal multigranular medical dataset to date.\\n\\n**Diseases** The datasets involved in constructing MedTrinity-25M primarily focus on disease diagnosis and medical discovery. In MedTrinity-25M, diseases are given in the free-form text. The same disease may be referred to using different terms, allowing for elaborate identification and analysis. Figure 9d illustrates the frequently used words related to diseases in our dataset.\\n\\n**Richness** We provide both quantitative analysis and qualitative examples to show the richness of our generated multigranular compare to other medical dataset. Qualitative examples are shown in Figure 1, our textual description is multigranular with more attributes than radiology report of chest x-rays dataset MIMIC-CXR [21], visual QA dataset SLAKE[22] and radiology objects caption dataset ROCO[18]. To demonstrate the multi-granularity of our data, we compared the average word count of text descriptions in our dataset, MedTrinity-25M, with those in other medical datasets, as illustrated in Figure 10. The word count in our dataset is significantly higher, indicating greater richness.\\n\\n**Alignment with human** We leverage GPT-4 to quantify the alignment of generated text descriptions compared to clinical reports from pathologist, which is set as the ground-truth. Specifically, we utilize GPT-4 to score the helpfulness, relevance, accuracy, and level of details of the our generated text descriptions based on clinical reports, and give an overall score on a scale of 1 to 10, where a higher score indicates better overall performance. Additionally, GPT-4 is required to provide a comprehensive explanation for the evaluation score. Detailed experiment results are presented in supplementary materials.\\n\\n5 Conclusion\\n\\nThis paper introduces MedTrinity-25M, a large-scale multimodal medical dataset comprising over 25 million image-ROI-description triplets sourced from more than 90 online resources, spanning 10 modalities and covering over 65 diseases. Unlike existing dataset construction methods that rely on image-text pairs, we have developed the first automated pipeline to scale up multimodal data by generating multigranular visual and textual annotations from unpaired image inputs, leveraging expert grounding models, retrieval-augmented generation techniques, and advanced MLLMs. MedTrinity-25M\u2019s enriched annotations have the potential to support a wide range of multimodal tasks, such as captioning, report generation, classification, and segmentation, as well as facilitate the large-scale pre-training of multimodal medical AI models.\"}"]}
{"id": "WVQ4Clw1VD", "page_num": 10, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. *Advances in neural information processing systems*, 36, 2024.\\n\\n[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*, 2023.\\n\\n[3] Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai. *NEJM AI*, 1(3):A1oa2300138, 2024.\\n\\n[4] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. *arXiv preprint arXiv:2312.11805*, 2023.\\n\\n[5] Hong-Yu Zhou, Subathra Adithan, Juli\u00e1n Nicol\u00e1s Acosta, Eric J Topol, and Pranav Rajpurkar. A generalist learner for multifaceted medical image interpretation. *arXiv preprint arXiv:2405.07988*, 2024.\\n\\n[6] Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, and Maria De La Iglesia-Vaya. Padchest: A large chest x-ray image dataset with multi-label annotated reports. *Medical image analysis*, 66:101797, 2020.\\n\\n[7] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In *Proceedings of the AAAI conference on artificial intelligence*, volume 33, pages 590\u2013597, 2019.\\n\\n[8] Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng. Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs. *arXiv preprint arXiv:1901.07042*, 2019.\\n\\n[9] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Nau- mann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. *Advances in Neural Information Processing Systems*, 36, 2024.\\n\\n[10] Wisdom Ikezogwo, Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Geva, Fatwir Sheikh Mohammed, Pa- van Kumar Anand, Ranjay Krishna, and Linda Shapiro. Quilt-1m: One million image-text pairs for histopathology. *Advances in Neural Information Processing Systems*, 36, 2024.\\n\\n[11] Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: a multimodal medical few-shot learner. In *Machine Learning for Health (ML4H)*, pages 353\u2013367. PMLR, 2023.\\n\\n[12] Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai. *NEJM AI*, 1(3):A1oa2300138, 2024.\\n\\n[13] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. *arXiv preprint arXiv:2308.01390*, 2023.\\n\\n[14] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. In *International Conference on Machine Learning*, pages 8469\u20138488. PMLR, 2023.\\n\\n[15] Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al. Capabilities of gemini models in medicine. *arXiv preprint arXiv:2404.18416*, 2024.\\n\\n[16] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. *arXiv preprint arXiv:2312.11805*, 2023.\\n\\n[17] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*, 2020.\"}"]}
{"id": "WVQ4Clw1VD", "page_num": 11, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[18] Obioma Pelka, Sven Koitka, Johannes R\u00fcckert, Felix Nensa, and Christoph M Friedrich. Radiology objects in context (roco): a multimodal image dataset. In *Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis: 7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop, LABELS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Proceedings 3*, pages 180\u2013189. Springer, 2018.\\n\\n[19] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Jiayu Lei, Ya Zhang, Yanfeng Wang, and Weidi Xie. Radgenome-chest ct: A grounded vision-language dataset for chest ct analysis. *arXiv preprint arXiv:2404.16754*, 2024.\\n\\n[20] Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-clip: Contrastive language-image pre-training using biomedical documents. In *International Conference on Medical Image Computing and Computer-Assisted Intervention*, pages 525\u2013536. Springer, 2023.\\n\\n[21] AlistairEW Johnson, TomJ Pollard, SethJ Berkowitz, NathanielR Greenbaum, MatthewP Lungren, Chih-ying Deng, RogerG Mark, and Steven Horng. Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports. *Scientific data*, 6(1):317, 2019.\\n\\n[22] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering. In *2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)*, pages 1650\u20131654. IEEE, 2021.\\n\\n[23] Ke Yan, Xiaosong Wang, Le Lu, and Ronald M Summers. Deeplesion: Automated deep mining, categorization and detection of significant radiology image findings using large-scale clinical lesion annotations. *arXiv preprint arXiv:1710.01766*, 2017.\\n\\n[24] axiong/pmc_oa datasets at hugging face. [https://huggingface.co/datasets/axiong/pmc_oa](https://huggingface.co/datasets/axiong/pmc_oa).\\n\\n[25] Alexandros Karargyris, Renato Umeton, Micah J Sheller, Alejandro Aristizabal, Johnu George, Anna Wuest, Sarthak Pati, Hasan Kassem, Maximilian Zenk, Ujjwal Baid, et al. Federated benchmarking of medical artificial intelligence with medperf. *Nature Machine Intelligence*, 5(7):799\u2013810, 2023.\\n\\n[26] Noel CF Codella, David Gutman, M Emre Celebi, Brian Helba, Michael A Marchetti, Stephen W Dusza, Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, et al. Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic). In *2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018)*, pages 168\u2013172. IEEE, 2018.\\n\\n[27] Veronica Rotemberg, Nicholas Kurtansky, Brigid Betz-Stablein, Liam Caffery, Emmanouil Chousakos, Noel Codella, Marc Combalia, Stephen Dusza, Pascale Guitera, David Gutman, et al. A patient-centric dataset of images and metadata for identifying melanomas using clinical context. *Scientific data*, 8(1):34, 2021.\\n\\n[28] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. *arXiv preprint arXiv:2305.10415*, 2023.\\n\\n[29] Ziheng Zhao, Yao Zhang, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. One model to rule them all: Towards universal segmentation for medical images with text prompts. *arXiv preprint arXiv:2312.17183*, 2023.\\n\\n[30] Jiacheng Wang, Lan Wei, Liansheng Wang, Qichao Zhou, Lei Zhu, and Jing Qin. Boundary-aware transformers for skin lesion segmentation. In *Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2021: 24th International Conference, Strasbourg, France, September 27\u2013October 1, 2021, Proceedings, Part I 24*, pages 206\u2013216. Springer, 2021.\\n\\n[31] Zhihao Chen, Yang Zhou, Anh Tran, Junting Zhao, Liang Wan, Gideon Su Kai Ooi, Lionel Tim-Ee Cheng, Choon Hua Thng, Xinxing Xu, Yong Liu, et al. Medical phrase grounding with region-phrase context contrastive alignment. In *International Conference on Medical Image Computing and Computer-Assisted Intervention*, pages 371\u2013381. Springer, 2023.\\n\\n[32] Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. Benchmarking retrieval-augmented generation for medicine. *arXiv preprint arXiv:2402.13178*, 2024.\\n\\n[33] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. *Applied Sciences*, 11(14):6421, 2021.\"}"]}
{"id": "WVQ4Clw1VD", "page_num": 12, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[34] Qiao Jin, Won Kim, Qingyu Chen, Donald C Comeau, Lana Yeganova, W John Wilbur, and Zhiyong Lu. Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical information retrieval. *Bioinformatics*, 39(11):btad651, 2023.\\n\\n[35] Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with gpus. *IEEE Transactions on Big Data*, 7(3):535\u2013547, 2019.\\n\\n[36] Meta LLaMA Team. Introducing meta llama 3: The most capable openly available llm to date. [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/) 2024.\\n\\n[37] Baifeng Shi, Ziyang Wu, Maolin Mao, Xin Wang, and Trevor Darrell. When do we not need larger vision models? *arXiv preprint arXiv:2403.13043*, 2024.\\n\\n[38] Jin Ye, Junlong Cheng, Jianpin Chen, Zhongying Deng, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyang Huang, Jilong Chen, Lei Jiang, et al. Sa-med2d-20m dataset: Segment anything in 2d medical imaging with 20 million masks. *arXiv preprint arXiv:2311.11969*, 2023.\\n\\n[39] Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classification. *Scientific Data*, 10(1):41, 2023.\\n\\n[40] Ke Yan, Xiaosong Wang, Le Lu, and Ronald M Summers. Deeplesion: Automated deep mining, categorization and detection of significant radiology image findings using large-scale clinical lesion annotations. *arXiv preprint arXiv:1710.01766*, 2017.\\n\\n[41] Maria Correia de Verdier, Rachit Saluja, Louis Gagnon, Dominic LaBella, Ujjwall Baid, Nourel Hoda Tahon, Martha Foltyn-Dumitru, Jikai Zhang, Maram Alafif, Saif Baig, et al. The 2024 brain tumor segmentation (brats) challenge: Glioma segmentation on post-treatment mri. *arXiv preprint arXiv:2405.18368*, 2024.\\n\\n[42] Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. A dataset of clinically generated visual questions and answers about radiology images. *Scientific data*, 5(1):1\u201310, 2018.\\n\\n[43] Jakob Nikolas Kather, Niels Halama, and Alexander Marx. 100,000 histological images of human colorectal cancer and healthy tissue. [https://doi.org/10.5281/zenodo.1214456](https://doi.org/10.5281/zenodo.1214456).\\n\\n[44] Jin Ye, Junlong Cheng, Jianpin Chen, Zhongying Deng, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyang Huang, Jilong Chen, Lei Jiang, et al. Sa-med2d-20m dataset: Segment anything in 2d medical imaging with 20 million masks. *arXiv preprint arXiv:2311.11969*, 2023.\"}"]}
{"id": "WVQ4Clw1VD", "page_num": 13, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Checklist\\n\\nThe checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [TODO] to [Yes], [No], or [N/A]. You are strongly encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description. For example:\\n\\n- Did you include the license to the code and datasets? [Yes] See Section xxx.\\n- Did you include the license to the code and datasets? [No] The code and the data are proprietary.\\n- Did you include the license to the code and datasets? [N/A]\\n\\nPlease do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.\\n\\n1. For all authors...\\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? [Yes]\\n   (b) Did you describe the limitations of your work? [Yes] See Supplementary materials.\\n   (c) Did you discuss any potential negative societal impacts of your work? [N/A] This research is foundational works, do not include potential negative impacts.\\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\\n\\n2. If you are including theoretical results...\\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A] This paper do not include theoretical results.\\n   (b) Did you include complete proofs of all theoretical results? [N/A] This paper do not include theoretical results.\\n\\n3. If you ran experiments (e.g. for benchmarks)...\\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Refer to project page in abstract.\\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes]\\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] This paper does not report error bars\\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Supplementary materials.\\n\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n   (a) If your work uses existing assets, did you cite the creators? [Yes] We cite all utilized assets in reference.\\n   (b) Did you mention the license of the assets? [Yes]\\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We propose a new dataset, which can be access in our project page.\\n   (d) Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? [Yes] We follow corresponding licences.\\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] We collect only medical data.\\n\\n5. If you used crowdsourcing or conducted research with human subjects...\"}"]}
{"id": "WVQ4Clw1VD", "page_num": 14, "content": ["{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] This paper did not use crowdsourcing.\\n\\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\\n\\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\"}"]}
