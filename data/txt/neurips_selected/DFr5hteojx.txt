The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models

Hannah Rose Kirk1* Alexander Whitefield2 Paul Röttger3 Andrew Bean1
Katerina Margatina4‡ Juan Ciro5,11 Rafael Mosquera5,6 Max Bartolo7,8
Adina Williams9 He He10 Bertie Vidgen1,11† Scott A. Hale1,12†
1University of Oxford 2University of Pennsylvania 3Bocconi University
4AWS AI Labs 5ML Commons 6Factored AI 7UCL 8Cohere
9MetaAI 10New York University 11Contextual AI 12Meedan

Abstract

Human feedback is central to the alignment of Large Language Models (LLMs). However, open questions remain about methods (how), domains (where), people (who) and objectives (to what end) of feedback processes. To navigate these questions, we introduce PRISM, a dataset that maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. With PRISM, we contribute (i) wider geographic and demographic participation in feedback; (ii) census-representative samples for two countries (UK, US); and (iii) individualised ratings that link to detailed participant profiles, permitting personalisation and attribution of sample artefacts. We target subjective and multicultural perspectives on value-laden and controversial issues, where we expect interpersonal and cross-cultural disagreement. We use PRISM in three case studies to demonstrate the need for careful consideration of which humans provide what alignment data.

Data & Code: github.com/HannahKirk/prism-alignment
Data & Dataset Card: huggingface.co/datasets/HannahRoseKirk/prism-alignment

1 Introduction

Human feedback serves a direct role for the alignment of large language models (LLMs), defined as the steering of AI behaviour towards a set of preferences or values. This increased emphasis on human feedback raises unresolved questions: how we collect human feedback when designing methodologies that rely on ordinal or cardinal scales, broad or fine-grained desiderata, and explicit or implicit signals; where we focus human labour when selecting domains, topics or tasks to collect feedback over; who we ask for feedback when recruiting participants to voice their idiosyncratic preferences, values, or beliefs [1]; and to what end when specifying an objective to pursue personalised alignment [2–4] or to aggregate individual preferences into collective outcomes favourable for societies at large [5–9].

Despite the success of human feedback learning [10, 11], answering these questions is constrained by gaps in existing datasets, such as (i) over-reliance on binary A/B comparisons, without fine-grained ratings or explanations [12]; (ii) small or biased samples recruited from narrow crowdwork or tech communities [10, 13] (iii) limited sample information (annotator IDs or sociodemographics) [14]; and (iv) scarce documentation for how values are operationalised [15, 16]. Most datasets rely only on

*{hannah.kirk,scott.hale}@oii.ox.ac.uk †Joint last authors; ‡Work done at University of Sheffield Figure 1: The PRISM dataset. In Stage 1, 1,500 participants fill in the Survey detailing their background, familiarity with LLMs and stated preferences over behaviours (§ 2.1). Demographic and geographic breakdowns are in Tab. 5 and Tab. 8. Participants then progress to Stage 2, where they converse with LLMs on topics of their choosing, rate the responses on a cardinal scale, and give fine-grained feedback (§ 2.2). In the first turn, four models respond to the opening prompt (①, ②, ③, ④). In subsequent turns, the conversation continues with two responses sampled from the highest-rated model at a non-deterministic temperature (⑤, ⑥). There are 8,011 Conversations between participants (⑦) and LLMs (⑧), forming 27,172 Interactions (human message with a set of model responses), and 68,371 Utterances (triples of {human message, model response, score}).

revealed or contextual preferences [1] and much attention is devoted to technical or statistical issues in feedback learning [18,20], rather than data-centric human factors. Relying on ‘generic’ human data teaches behaviours which are reductionist because values are relational and non-separable from the person, community or operating context [21–23]; and non-generalisable because the indiscriminate aggregation of data subsumes hidden annotator contexts as universalities [24–28].

We introduce PRISM, a new resource for navigating empirical questions of human feedback. We employ both the ask and observe principles of social science by mapping detailed survey responses of humans around the world onto their live conversations with LLMs (Fig. 1). This setup permits alignment methods relying on either contextual preference comparisons typical for RLHF [29–31], or stated preferences and principles like constitutional AI [6,32]. In addition to pairing stated and contextual preferences, PRISM has the following features. Participatory: To ensure wider active participation in alignment data [25,33], we recruit 1,500 English-speaking crowdworkers from diverse geographies and demographics; Representative: As units for preference aggregation, we include two census-representative samples (UK, US); and Individualised: To expose hidden human context and permit personalised preferences, each rating links to a pseudonymous ID and detailed participant profile. We source Subjective and Multicultural perspectives to avoid value-monism and cultural homogenisation in the opinions that LLMs represent [34–36] and operate in the descriptive paradigm without guidelines that characterise ‘good’ responses [15,16]. Opinion diversity varies along the objective–subjective spectrum (e.g. what is the capital of France? vs. is abortion wrong?), so we prime participants for values and controversy guided dialogues but also collect neutral unguided dialogues as a baseline. To our knowledge, PRISM is the first human feedback dataset to target cross-cultural controversies and value-laden prompts, where interpersonal disagreement is rife. After introducing PRISM (§ 2), we demonstrate its value via three case studies (§ 3): (1) Do different people initiate different discussions with LLMs? (2) Do people prefer differently aligned models, and (3) How do sampling decisions affect welfare outcomes? PRISM provides many more research avenues such as engineers targeting personalised alignment [2] or consensus across opinion distributions [5,37]; social scientists examining how exposure to LLMs affects public attitudes; or policymakers seeking democratic input on AI-citizen interactions on topics like immigration, abortion or euthanasia. Alignment cannot be neatly bifurcated into technical and normative components [38]. PRISM assists in navigating these complexities with more human voices adjudicating alignment norms.

2We use Contextual Preference for observed ratings of LLM outputs to avoid misrepresenting how Revealed Preference is used by economists—as assumptions that enable the inference of preferences from choices [17]. Figure 2: **Schematic of fine-grained attribute ratings.** The same attributes appear in three places in our task: A is asked once in the survey; B and C are asked per conversation. For *performance attributes*, we ask participants to consider only the highest-rated model in the first conversation turn; for *choice attributes*, we ask them to consider this highest-rated model relative to other models in the first turn.

## 2 The PRISM Alignment Dataset

PRISM maps the characteristics and preferences of diverse humans onto their real-time interactions with LLMs (Fig. 1). Participants complete a **Survey** (§ 2.1) with questions about their demographics and stated preferences, then proceed to the **Conversations** with LLMs (§ 2.2), where they input prompts, rate responses and give fine-grained feedback in a series of multi-turn interactions. With the two-stage setup: (i) we avoid over-generalising from a “generic human” by matching ratings to detailed participant characteristics; (ii) we track how contextual preferences (in local conversations) depart from stated preferences (in survey); and (iii) we give participants autonomy to communicate in their own words what is important and why [39][25]. Both stages received ethics board approval and ran with informed consent (App. D). Participants were paid £9/hour and the task took 70 minutes on average. Data collection ran from 22nd November to 22nd December 2023.³ We provide a data statement in [App. B] data clause in [App. C] and full codebooks detailing each variable in [App. V].

### 2.1 The Survey

Prior to starting the survey, we ensure that all participants are over 18, obtain their informed consent, give a brief primer on LLMs (or AI language models), and dissuade LLM-written responses. The survey constructs a participant profile containing five features:

**LLM familiarity and usage** We ask about participants’ familiarity with LLMs (61% are somewhat familiar, 28% very familiar and 10% not familiar at all) and whether to their knowledge they have used them *indirectly* (in products like LinkedIn post-writing tool); or *directly* (via a specialised interface like ChatGPT). Individuals that have used LLMs directly or indirectly (84%) are branched to questions on frequency of use (7% every day, 21% every week, and 20% every month) and purpose of use (the most popular tasks are research overviews selected by 49%, professional work by 37%, creative writing by 31% and programming help by 27%). Full results in [App. I].

**Self-written system string (“constitution”)** System strings can guide LLM behaviours as a high-level global instruction prompts prepended to all subsequent interactions [40][41], and have been analogised as “constitutions” or governing principles for AI [32]. Factuality, professionalism, humanness and harmlessness all emerged as key principles (App. M.1) from the following instruction:

---

³Ethics approval, data collection, and analysis was led by researchers from the University of Oxford. Stated preferences for LLM behaviours In contrast to this open-ended preference elicitation, we collect structured ratings on fine-grained behaviour attributes. Participants score the importance of each attribute on a visual analog scale [42] (Fig. 2). A statement like “It is important that an AI language model produces factual and informative responses” maps (0,100) where the ends of scale are (Strongly disagree, Strongly agree). Numeric scores are recorded, but not shown to participants to avoid anchoring and dependency biases. We only collect responses to these statements once before participants interact with LLMs but the same attributes appear in the Conversations stage; so, we can track how stated ‘abstract’ preferences relate to contextual ‘in-situ’ preferences. Overall, we find clusters of subjective attributes (values, creativity and diversity) versus objective attributes (factuality, fluency and helpfulness; App. N.1). While the majority of participants agree that these more objective attributes are important (highly-skewed positive distribution, $\mu \in [86, 89]$, $\sigma \in [14, 16]$), there is little agreement on the meta-importance of subjective attributes (App. N.2). In fact, responses for whether value alignment itself is important follow an almost normal distribution ($\mu = 54$, $\sigma = 26$).

Self-written description Values and preferences are subjective and personal. We ascribe participants autonomy to communicate salient aspects of their identity in a short profile, beyond essentialising associations with structured demographics alone. Honesty, hard work and empathy emerged as common values (App. M.2) from the following instruction:

```
Please briefly describe your values, core beliefs, guiding principles in life, or other things that are important to you. For example, you might include values you’d want to teach to your children or qualities you look for in friends. There are no right or wrong answers. Please do not provide any personally identifiable details like your name, address or email.
```

Basic demographics We ask standard demographics: age, gender, employment status, martial status, educational attainment, ethnicity, religious affiliation, English proficiency, country of birth, and country of residence. There is always a “Prefer not to say” option. For gender, participants can select Male, Female, Non-Binary, or self-describe. We collect self-described ethnicity and religion because no pre-set groups exhaust how individuals may self-identify across cultures and global regions. We provide a manual annotation of these strings into aggregated categorisations for statistical analysis (App. F). Because of how we recruit participants (§ 2.3), our sample covers diverse demographics (App. G) and geographies (App. H), with representation from people born in 75 countries. However, the sample still skews White, Western and educated, and only contains English-language speakers.

2.2 The Conversations

After completing the survey, participants move to the second stage, consisting of real-time conversations with LLMs via a custom-built interface on the Dynabench platform [43, 44].

Selecting conversation type We prime participants to diversify their prompts along the objective-subjective spectrum by asking them to complete two conversations across three conditions or conversation types (six in total) [5]. They select the type before inputting their opening prompt:

- **Unguided.** Ask, request or talk to the model about anything. It is up to you!
- **Values guided.** Ask, request or talk to the model about something important to you or that represents your values. This could be related to work, religion, family and relationship, politics or culture.
- **Controversy guided.** Ask, request or talk to the model about something controversial or where people would disagree in your community, culture or country.

Opening the conversation Participants construct a free-text prompt of their choosing and receive up to four responses from different LLMs [6]. The participants then rate each response on a visual analogue scale (VAS) [42, 45] from “Terrible” to “Perfect”. We record the slider position as a score from 1–100 but do not show participants the number to avoid anchoring or conditional dependence of scores across conversations. We opt for this cardinal feedback for three reasons: (i) it encourages subjectivity; (ii) it permits studying the relative merit of cardinality versus ordinality for reward

---

4 The survey also has an Other free-text box used by 332 participants (App. N.3), and a personalisation attribute which we do not include in Conversations because models are not personalised.

5 Some deviated from this quota (n=6, 2 per type) due to technical difficulties, instruction misunderstanding or losing count; So, we release a balanced subset of the data that controls for this variance (App. K). Though values and controversy guided conversations are typically more subjective than neutral baselines, conversation type does not map perfectly to subjectivity levels. Besides from priming participants via selecting a conversation type, we do not constrain (and seek to minimally influence) participants’ topic or prompt choice.

6 We do not stream responses because not all models had the functionality. If a model fails or a response takes > 30 seconds, we drop this model from the response set and the participant may see < 4 responses (App. P). modelling because ratings can be converted to rankings but not vice versa; (iii) it allows expression of preference intensity above and beyond chosen:rejected pairs. However, we acknowledge that the cardinal scale introduces some intrapersonal measurement noise from a more cognitively demanding task and carries less interpersonal comparability than ordinal preferences, see Limitations (§ 5).

**Continuing the conversation** The highest-scoring LLM from the opening turn is locked into subsequent turns, with random tie-breaks in the case of identical scores. Participants must continue the conversation for at least another turn, but are asked to vary their conversations between 2 and 10 turns to avoid introducing a dataset artefact. We encourage some variation in conversation length ($\mu_T = 3.4$, $\sigma_T = 1.6$) but there is a strong drop off after the second turn (App. O). Participants then rate two responses on a VAS like before, but both are now sampled from the selected model with a non-deterministic temperature. These within-model responses are more similar in style and content than across-model responses (in the first turn), and score deviations are narrower (App. O).

**Collecting fine-grained feedback** After the conversation ends, participants first rate statements about the *performance of their highest-rated model* like “The response was well-written” on a VAS from *Performed very poorly* to *Performed very well*, or select N/A if the statement is irrelevant for the context. We then ask participants to consider *why they chose this model*, rating statements like “I chose this response *because* it was well-written” on a VAS from *Very unimportant* to *Very important* (or select N/A). Attributes are shared with the Survey (Fig. 2). We find strong correlations between performance attributes and choice attributes (except safety) but weak correlations of these pairs to stated preferences given in the Survey, perhaps due to conversational, model or task-design confounders (App. N.1). In general, the distribution of scores over performance and choice attributes is narrower and more positively skewed (bunched to 100) compared to stated preferences (App. N.2). Finally, we collect open-ended natural language feedback on the *whole* conversation. Participants contributed both content and stylistic feedback ($\mu = 29$ words, $\sigma = 19$, App. M.3).

---

**2.3 The Sample**

Our sampling aims were *depth* in the demographics represented within countries and *breadth* across global regions. We recruit English-speaking participants from Prolific in two distinct paths:

**Census-representative sample (UK, US)** Samples matched to simplified census data (age, ethnicity, gender) were only available for the UK and US. The minimum pool size for a statistical guarantee of representativeness was 300, which set a lower bound for participant quota. After collecting data, we observed some skew in our ‘representative’ samples between observed and expected distributions in recent census data, which we partially correct for (App. L). These samples permit future studies on more representative populations that can be replicated across two countries; however their inclusion biases PRISM as a whole towards two Western nations already over-represented in AI research.

**Balanced samples (rest of world)** The distribution of Prolific workers outside the US and the UK skews strongly to Europe and Northern America, and some countries dominate continental counts (App. J). To avoid more active workforces biasing the sample, we set up 33 country-specific studies where there is > 1 eligible worker, and allocate sample quotas so that each global region is similarly represented. We balance each national sample by gender where possible (Tab. 10).

**Included models** The rapidly evolving landscape necessitates a model-agnostic approach to avoid data staleness. We include 21 different LLMs (9 open-access, 12 commercial-API) from various model families and parameter sizes, which diversifies the training data, capabilities, and degree of existing safeguards or alignment biases. To avoid text length confounding preferences and to reduce participant fatigue, we include system prompts instructing models to limit their responses to $\leq 50$ words. We show the full list of models, decoding parameters and generation details in App. P.

---

7 For example, all responses could be very poor and similar (negative skew, small spread); all very good and similar (positive skew, small spread); or highly-distinguishable (no skew, wide spread).

8 Participants still appear in our sample who were born or reside in countries that did not have a dedicated country-wise study e.g. if their Prolific details were outdated or incorrect. We do not drop them. Figure 3: **Topic prevalence by conversation types and participant identity.** We show total prompts clustered into topics (bars), and total members in each group (top panels). Per group and topic, we plot the over-representation factor of observed vs. expected group proportions and show significant regression coefficients (base category indicated by †). All coefficients are in Fig. 23, topic-group counts in Fig. 27 and centroid prompts in Tab. 22. Location is by birth region (with UK and US split out), but most regions have few countries (App. H).

**Key results** (§ 3.1): Priming participants to select a conversation type (unguided, values or controversy guided) significantly influenced diversity of prompts. Identity factors have some significant interactions with prompt choice but each topic contains prompts authored by intersectionally-diverse participants.

### 3 Experiments with PRISM

#### 3.1 Case Study I: Do Different People Initiate Different Discussions with LLMs?

**Methods** We use a pre-trained sentence transformer (all-mpnet-base-v2) to embed each opening prompt in 768-D, then apply UMAP to reduce to 20-D, before clustering with HDBScan [47]. 70% of prompts are assigned to 22 topic clusters and 30% remain as outliers. We name each cluster by prompting gpt-4-turbo with the top n-grams extracted with TF-IDF and closest texts to the cluster centroid. We define an over-representation factor as $\frac{N_{g,t}}{N_{t}}$, to compute observed versus expected topic prevalence per identity group. For the partial contribution of identity attributes, we estimate an OLS regression for each topic $y^t$ ($t \in 1 \ldots 22$) and cluster standard errors at the individual level:

$$y^t_{i,c} = \alpha^t + \text{gender}_i^t \beta_1^t + \text{age}_i^t \beta_2^t + \text{birth\_region}_i^t \beta_3^t + \text{ethnicity}_i^t \beta_4^t + \text{religion}_i^t \beta_5^t + \text{prompt}_i^t \beta_6^t + \varepsilon_{i,c},$$

where $y^t_{i,c} = 1$ if the prompt of participant $i$ in conversation $c$ is categorised as topic $t$. The identity vectors (e.g. gender) represent sets of variables, with a base category removed (indicated in Fig. 3). The coefficients of interest are contained in vectors $\{\beta_d^t\}_{d=1}^6$, where component $g$ of $\beta_d^t$ is interpreted as the increase in probability of a participant choosing topic $t$ if they are in the group indexed by $g$ (e.g. Female) compared to the base group (e.g. Male). See App. R for extended methods.

**Results** Our instructions had a significant priming effect, resulting in a high density of controversial and value-laden topics (Fig. 3). Topics significantly correlated with controversy guidance are Gender & LGBTQ+ Identity, Israel–Palestine Conflict, and Discussions on Abortion, while topics significantly correlated with the values guidance are Managing Relationships, Job Search, and Religion & Spirituality. In contrast, the ‘unguided’ condition correlates with task-oriented and neutral topics like *Popular Culture, Recipes & Cooking* and *Travel Recommendations*. Only *Climate Change* is not significantly correlated to conversation type. Controlling for conversation type, 11% of coefficients are significant ($\alpha = 99\%$); so, **identity factors have some predictive power on topic prevalence**. Significant relationships include: women and non-binary people discuss gender and LGBTQ+ issues more than men; older people discuss elections and travel more than younger people; Black participants discuss climate change less than White participants, and all regions question LLMs about abortion less often than US participants. When we examine granular regions in embedding space using a single-link hierarchical clustering algorithm (App. S), **local prompt neighbourhoods tend to be intersectionally-diverse**: 84% of them meet or exceed entropy across intersectional demographics that would be expected under random sampling. During this local exploration, we retrieve regions of semantically-identical prompts rated by multiple diverse individuals (e.g. one neighbourhood “Does God exist?” has 7 religious and 7 irreligious participants), finding that **interpersonal differences in contextual preferences persist even when dialogue context is fixed** (App. S.4). So, despite PRISM containing semantically-diverse prompts, people from different backgrounds occupy common discussion spaces, providing an anchor to examine diverse perspectives to shared issues.

### 3.2 Case Study II: Do Different People Prefer Differently-Aligned Models?

**Methods** Observed preference differences at the model-level are confounded by interactions of topic prevalence and model aptitude (e.g. men ask more about aliens and gpt-4 is poor on extraterrestrial knowledge). Evidence of shared dialogue spaces (§ 3.1) and group-topic score differences (App. T.2) mitigate some concern, but to further control for context, we use opening prompts from the balanced subset of participants (n=1,246) with equal conversations per type (n=6,669). The mean participant rates 14/21 LLMs but unseen ratings are missing at random. Our aggregation (social choice) function over participant ratings is derived from *Pairwise Rank Centrality* ($\mathcal{P}$) [48] and *Convergence Voting* [49], both inspired by *PageRank* [50]. Each model is a node in a graph and transition probabilities between nodes are calculated by the proportion of pairwise battle wins. This process simulates a random walk on a Markov chain, leading to a stationary distribution of scores that reflect the collective preference intensity across models. Here, we compute $\mathcal{P}$ over subsamples using a regularisation parameter of 1 and tie threshold of 5, but present extended methods and robustness checks in App. T.

**Results** We find **rankings are sensitive to idiosyncratic, contextual, and group-wise variance**. Samples of 100 people introduce significant noise, resulting in a fairly even distribution of collective preference among the top 10 models (Fig. 4). Rankings are sensitive to what participants talk about: zephyr-7b performs highly on controversy but not in unguided domains, while claude-2 has the opposite trend; and where they are from: relative to overall rank, palm-2 drops 4 places for participants in the US, llama-7b drops 7 places in Asia, while mistral-7b gains 7 places in Africa. We further observe that **PRISM produces surprising ranks relative to other leaderboards**. We apply our method to CHATBOTARENA data [51], finding gpt models fare significantly worse in PRISM, while open models like zephyr-7b do significantly better (95% CI over 1,000 bootstraps, App. T.9). This may be due to domain shift (task-orientated/coding prompts vs. controversial/cultural prompts), sample diversity or task incentives. To identify drivers of score differences, we generate hypotheses by qualitatively examining battles between command and gpt-4/-turbo, then test these with an OLS regression on all model responses (App. T.8). We find that **formatting and refusals partially explain score differences** with significant positive effects from additional characters, ending in a question mark (“Would you like to know more?”) and enumeration, but significant negative effect of line breaks. De-anthropomorphic phrases (“As an AI, I don’t have personal opinions.”) significantly reduce score but not as substantially as refusals (“Sorry I cannot engage.”). The proportion of explained variance in score by these factors is low ($R^2 = 0.06$), so we encourage more sophisticated methods in future work for partialling out the effect of style versus content, or participant, model and conversation fixed-effects, as determinants of score.

### 3.3 Case Study III: How do Sampling Decisions Affect Welfare Outcomes?

**Methods** We use ‘welfare’ to capture the extent to which a chosen LLM aligns with the preferences of a user population. We consider two welfare measures: average model rating (MEANRATING), and average likelihood that a model is chosen (rated highest in the opening turn, MEANCHOICE). Previous experiments indicate dialogue and preference diversity across people, suggesting that the welfare of downstream LLM users may depend on who provides feedback. To test this, we first randomly generate seven sub-samples of individuals ‘in the seat of power’ to select their favourite LLM (based Figure 4: **Sources of variation in model preferences.** Panel A shows *idiosyncratic variance* in distributions of Pairwise Rank Centrality scores for 100 randomly-drawn participants (over 1,000 bootstraps). For Panels B and C, we show *conversational context variation* and *group-wise variation*. We show overall rank based on Pairwise Rank Centrality over n=6,669 balanced conversations (numbered circles). We then trace how rank changes by sampling the group on x (e.g. filtering to only values guided conversations, or only US participants). Across these subsamples, we show most spots climbed (▲) and spots fallen (▼) by each model relative to overall rank. **Key results** (§ 3.2): Rankings are sensitive to sample composition, varying with which participants are sampled (Panel A,C) and what they talk about (B). Rankings differ from other leaderboards, explained by PRISM’s characteristics (sample diversity, domain shifts) as well as response characteristics (length, formatting, refusals).

on mean rating). Four sampling schemes randomly draw N individuals from a representative sample (N ∈ {10, 20, 50, 100}). Three schemes randomly draw 100 individuals from specific low-diversity sub-populations (male, white, and ≥45 years old). For each draw, we then measure the distribution of welfare from this LLM being imposed on different stakeholder populations [9]: the entire population, non-male individuals, non-white individuals, and individuals <45 years old. We report the distribution of average welfare outcomes across random draws from each sampling scheme. We conduct this experiment for the UK and US representative samples. Extended methods are in [App. U](#).

**Results** We find as sample size falls, the probability of choosing a LLM with worse mean welfare rises. Larger samples from the target sub-population appear to first order stochastically dominate (FOSD) smaller samples from the target sub-population. **Sampling exclusively from a specific group tends to reduce the welfare of out-group individuals.** For example, when consider the welfare of the representative US sample (Fig. 5), sampling from US males is FOSD by sampling from the full US sample. Furthermore, **average measures can conceal the welfare of minority groups:** sampling 100 white individuals appears to FOSD sampling 100 representative individuals when assessing welfare of the population at large, but minority stakeholders (non-white population) are worse off under this scheme. Finally, **regardless of the model chosen, a large proportion of participants prefer a different model.** For the US, the model that maximises MEANCHOICE only

---

9A probability distribution with CDF $F_\rho$ is said to First Order Stochastically Dominate another probability distribution with CDF $F_\eta$ if both distributions have a finite mean, and $F_\rho(t) \leq F_\eta(t)$ ∀t ∈ ℝ. Figure 5: Welfare distributions for the US. The distribution of mean welfare for four subpopulations in the US (welfare pop) induced by seven sampling schemes (in the seat of power). The y axis is the sampled supopulation (e.g. Rep is a ‘representative’ sample of the population) and sample size in brackets (e.g. (100)). Each violin shows the distribution of mean welfare for the panel’s subpopulation induced by a sampling scheme. The top four Rating comparisons use the MEANRATING welfare measure and the bottom Choice comparisons use the MEANCHOICE welfare measure. The red distributions are FOSD by Rep (100) in blue (i.e. less optimal scheme).

Key results (§ 3.3): Large representative samples mostly outperform smaller or demographically-restricted samples and sampling exclusively from a specific group tends to reduce the welfare of out-group participants (male vs. non-male, white vs. non-white). No single model achieves majority preference (max 45% MEANCHOICE).

achieves a probability of 45%. If a participant is shown the winning model, and three other models at random, the probability that they will choose the winning model is < 50%. The probability they will pick the winning model over all other 20 LLMs can only be lower. This suggests that we should not expect a single LLM to satisfy everyone’s preferences in a given population. We repeat the welfare analysis for the UK sample and conduct robustness checks with imputed missing data in App. U.

4 Related Work

Participation & Representation in Science & Technology There is a long history of technologies failing diverse users who lack consultation during design [52–54]. Conscious participation can be intrinsically valuable as an act of justice [55, 56]. However, in internet-harvested pre-training data, participation is involuntary or cooptative [55, 33], and unequal representation risks cultural homogenisation and minority stereotyping [57–62]. Labelling data or giving feedback is active procedural participation [53] but often relies on narrow specifications from technology providers of what counts as high-quality language or preferable outputs [15, 16, 63, 64]. In ML or NLP data, variability in subjective experience is commonly collapsed into majority votes [27, 65–68], without sufficient documentation of annotator artefacts or disagreements [69–73], despite evidence that sociodemographics affect labels [74–79]. Multiple scientific fields are guilty of over-generalising conclusions from the ‘generic human’ drawn from ‘WEIRD’ societies [80, 81]. PRISM releases participant IDs and characteristics to spotlight sample diversity while acknowledging sample specificity [82].

Learning from Human Feedback Using human feedback to condition the loss function for training LLMs overcomes challenges of specifying rewards [83–85]. Combining human feedback, reinforce- ment learning and natural language generation has a history in machine translation [86–88] and dialogue [89–94]. RLHF pipelines rely on binary comparisons [29,31,85], principles or rules [32,95], fine-grained feedback [12], or natural language [96], to reward dimensions like helpfulness, honesty and harmlessness [97,30]. Reward models then update LLMs via algorithms like PPO [98] or Reinforce [99,100]; but reward model free techniques are competitive, e.g. DPO [18], supervised fine-tuning [101] and rejection sampling [102,5,103]. There is rising demand for high-quality human feedback [104,105], but the complexity and cost of collecting data incentivises scraping preferences, e.g. on Reddit [29,106] or StackOverflow [107], or simulating humans with LLMs [108–110]. Similar to PRISM, CHATBOTARENA [51], LMSYS-1M [111] and WILDCHAT [112] feature user-rated model interactions, but for narrow communities (HuggingFace Spaces) and domains (coding, task-orientated). Unlike these datasets, OPENCONVOS [113] collect optional contributor demographics, and DICES [79] provide demographics for multiple raters per conversation. Other datasets target specific behaviours [30,114], or multilingual coverage [115]. Surveys on attitudes towards AI [116,117] and community assemblies [6,118,119] offer another lens on public priorities. To our knowledge, PRISM is the first to link preference ratings and detailed survey responses.

5 Limitations, Discussions and Conclusions

Ethical Considerations and Limitations We collect informed consent, pseudonymise IDs, check for PII (App. E) and disallow deanonymisation in our terms (App. C), but privacy risks remain, especially given the sensitive nature of conversations. Asking participants to engage with controversies expands human preference data to discursive areas with the greatest expected degree of interpersonal disagreement, but risks encouraging hateful, bigoted, biased or otherwise harmful content. PRISM is less toxic than previous datasets (0.06%, App. E). We do not moderate prior to release to permit conversational safety research. There are many sources of variance in PRISM and alternative divisions of the data may yield different outcomes [120]. Granting free choice of dialogue, using cardinal feedback scales and focusing on many kinds of models and participants introduces diversity and subjective freedom but complicates controlled experiments and limits statistical power. PRISM is still biased towards English-speaking crowdworkers whose task-specific incentives may not align with wider populations. We expand on ethical risks and limitations in our data statement (App. B).

We raise three discussion points on the boundaries of where we collect preferences, for what end and with what lasting impact. First, aligning LLMs via ‘preference-based utilitarianism’ [121] may not be synonymous with individual or societal well-being, prompting the question of whether there are limits for “legitimate” human feedback. Preferences may be (i) at odds with self-interest due to myopia or information asymmetries (e.g. participants who want anthropomorphic LLMs despite evidenced harms [122–126]) or (ii) incompatible with others’ interest (e.g. participants who prefer ‘anti-woke’ LLMs that argue in a debate vs. those who favour neutrality). Relying on decontextualized preference observations carries the risk of silently reinforcing biases from those in power [61,65]; so we recommend transparency surrounding individual disagreements before aggregation decisions [9,127], especially if participant positionality affects their epistemic legitimacy to define harm [59,128,129]. Second, irreconcilable personal preferences and morals matter more when the ‘unit of alignment’ is operationalised as a group, culture or even species, rather than an individual. PRISM permits personalised or steerable alignment using participant profiles and specific ratings [2–4,37] as well as collective alignment via opinion consensus or distribution of rewards [5–8,28]; though group deliberation in groups may yield different outcomes than gathering data from one person at a time [6,118,119]. With growing use of synthetic alignment data, PRISM can assist in calibrating LLM-as-judge protocols to more diverse rater pools [51,130]. Finally, PRISM was motivated by participation as justice via inclusionary alignment practices that, relative to passive roles in annotation tasks or pre-training data, prioritise active input from local citizens with specialised knowledge of their own and communities’ needs [55]. However, participation remains thin because the humans crucial to the success of RLHF do not typically share in downstream benefits or profits [33,131]. Ultimately, the impact of our work depends on those developing, researching and regulating LLMs because effective participation requires being asked and being heard [53].

In their early demonstrations of aligning AI systems to human feedback, Bai et al. discuss alignment data as a public good. We echo this sentiment with PRISM—a new feedback dataset from 1,500 diverse humans, motivated by the need for inclusive, participatory and open scientific research into the pressing question of what it means to align LLMs to human preferences in a pluralistic world. Acknowledgments and Disclosure of Funding

This project was awarded the MetaAI Dynabench Grant “Optimising feedback between humans-and-models-in-the-loop”. For additional compute support, the project was awarded the Microsoft Azure Accelerating Foundation Model Research Grant. For additional annotation support, we received funding from the OpenPhil grant and NSF grant (IIS-2340345) via New York University. We are grateful for support received in the form of research access or credits from OpenAI, Anthropic, Aleph Alpha, Google, HuggingFace and Cohere. Hannah Rose Kirk’s PhD is supported by the Economic and Social Research Council grant ES/P000649/1. Paul Röttger is a member of the Data and Marketing Insights research unit of the Bocconi Institute for Data Science and Analysis, and is supported by a MUR FARE 2020 initiative under grant agreement Prot. R20YSMBZ8S (INDOMITA). Andrew Bean’s PhD is supported by the Clarendon Fund Scholarships at the University of Oxford. We are particularly grateful to Maximilian Kasy for his valuable input and advise on the welfare experiments. We are indebted to the incredible effort and time that our Prolific annotators put into our task, as well as the expert advice from Prolific consultant Andrew Gordon. We also thank any Beta testers, including friends, family and colleagues at Oxford and New York University, for their help in piloting (and debugging!) our task. Lastly, we thank Jakob Mökander, Nathan Lambert, Natasha Jacques, Felix Simon, Nino Scherrer, Maximilian Kroner Dale, and Saffron Huang for their feedback on the paper. We use scientific colour maps in our figures [132].

Author Contribution Statement

| Project Conception | [KIRK, HALE, VIDGEN] |
|--------------------|----------------------|
| Data Collection Design | [KIRK, HALE, VIDGEN, RÖTTGER, MARGATINA] |
| Frontend Design and Development | [KIRK, CIRO] |
| Backend Design and Development | [KIRK, MOSQUERA] |
| Analysis Advisory | [HALE, VIDGEN, RÖTTGER, BARTOLO, BEAN, WILLIAMS, HE] |
| Literature and Dataset Comparison | [KIRK, BEAN] |
| Metadata Processing | [KIRK, MARGATINA, BEAN] |
| Manual Annotation | [KIRK, BEAN, RÖTTGER, BARTOLO] |
| Results and Codebase | [KIRK, WHITEFIELD] |
| Manuscript Writing | [KIRK, WHITEFIELD] |
| Manuscript Editing and Feedback | [EVERYONE] |

References

[1] Iason Gabriel. Artificial Intelligence, Values and Alignment. *Minds and Machines*, 30(3):411–437, September 2020. ISSN 0924-6495, 1572-8641. doi: 10.1007/s11023-020-09539-2.

[2] Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, and Scott A. Hale. The benefits, risks and bounds of personalizing the alignment of large language models to individuals. *Nature Machine Intelligence*, pages 1–10, April 2024. ISSN 2522-5839. doi: 10.1038/s42256-024-00820-y. URL [https://www.nature.com/articles/s42256-024-00820-y](https://www.nature.com/articles/s42256-024-00820-y) Publisher: Nature Publishing Group.

[3] Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging, October 2023. URL [http://arxiv.org/abs/2310.11564](http://arxiv.org/abs/2310.11564) arXiv:2310.11564 [cs].

[4] Xinyu Li, Zachary C. Lipton, and Liu Leqi. Personalized Language Modeling from Personalized Human Feedback, February 2024. URL [http://arxiv.org/abs/2402.05133](http://arxiv.org/abs/2402.05133) arXiv:2402.05133 [cs].

[5] Michiel A. Bakker, Martin J. Chadwick, Hannah R. Sheahan, Michael Henry Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat McAleese, Amelia Glaese, John Aslanides, Matthew M. Botvinick, and Christopher Summerfield. Fine-tuning language models to find agreement among humans with diverse preferences. In *Advances in neural information processing systems*, volume 35, pages 38176–38189. Curran Associates, Inc., November 2022. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf) _eprint: 2211.15006v1.

[6] Anthropic. Collective Constitutional AI: Aligning a Language Model with Public Input. Technical report, 2023. URL [https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input](https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input). [7] Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, Amrit Singh Bedi, and Mengdi Wang. MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences, February 2024. URL http://arxiv.org/abs/2402.08925 arXiv:2402.08925 [cs].

[8] Dexun Li, Cong Zhang, Kuicai Dong, Derrick Goh Xin Deik, Ruiming Tang, and Yong Liu. Aligning Crowd Feedback via Distributional Preference Reward Modeling, February 2024. URL http://arxiv.org/abs/2402.09764 arXiv:2402.09764 [cs].

[9] Vincent Conitzer, Rachel Freedman, Jobst Heitzig, Wesley H. Holliday, Bob M. Jacobs, Nathan Lambert, Milan Mossé, Eric Pacuit, Stuart Russell, Hailey Schoelkopf, Emanuel Tewolde, and William S. Zwicker. Social Choice for AI Alignment: Dealing with Diverse Human Feedback, April 2024. URL http://arxiv.org/abs/2404.10271 arXiv:2404.10271 [cs].

[10] Hannah Kirk, Andrew Bean, Bertie Vidgen, Paul Rottger, and Scott Hale. The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2409–2430, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.148. URL https://aclanthology.org/2023.emnlp-main.148

[11] Nathan Lambert, Thomas Krendl Gilbert, and Tom Zick. The History and Risks of Reinforcement Learning and Human Feedback, November 2023. URL http://arxiv.org/abs/2310.13595 arXiv:2310.13595 [cs].

[12] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-Grained Human Feedback Gives Better Rewards for Language Model Training, June 2023. URL http://arxiv.org/abs/2306.01693 arXiv:2306.01693 [cs].

[13] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback, September 2023. URL http://arxiv.org/abs/2307.15217 arXiv:2307.15217 [cs].

[14] Vinodkumar Prabhakaran, Aida Mostafazadeh Davani, and Mark Diaz. On Releasing Annotator-Level Labels and Information in Datasets. In Proceedings of the Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop, pages 133–138, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.law-1.14. URL https://aclanthology.org/2021.law-1.14

[15] Paul Rottger, Bertie Vidgen, Dirk Hovy, and Janet Pierrehumbert. Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 175–190, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.13. URL https://aclanthology.org/2022.naacl-main.13

[16] Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, and Scott A. Hale. The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising "Alignment" in Large Language Models. In Socially Responsible Language Modelling Research (SoLaR). arXiv, November 2023. doi: 10.48550/arXiv.2310.02457. URL http://arxiv.org/abs/2310.02457

[17] Andreu Mas-Colell, Michael Dennis Whinston, Jerry R Green, et al. Microeconomic theory, volume 1. Oxford university press, New York, 1995.

[18] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. In Advances in Neural Information Processing Systems, volume 36, February 2024. URL https://proceedings.neurips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html

[19] Banghua Zhu, Jiantao Jiao, and Michael I. Jordan. Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons, February 2024. URL http://arxiv.org/abs/2301.11270 arXiv:2301.11270 [cs, math, stat]. [20] Banghua Zhu, Michael I. Jordan, and Jiantao Jiao. Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF, January 2024. URL http://arxiv.org/abs/2401.16335 arXiv:2401.16335 [cs, stat].

[21] Alexey Turchin. AI Alignment Problem: "Human Values" Don’t Actually Exist. PhilArchive, 2019. URL https://philarchive.org/rec/TURAAP

[22] Brian D. Earp, Killian L. McLoughlin, Joshua T. Monrad, Margaret S. Clark, and Molly J. Crockett. How social relationships shape moral wrongness judgments. Nature Communications, 12(1):5776, October 2021. ISSN 2041-1723, doi: 10.1038/s41467-021-26067-4. URL https://www.nature.com/articles/s41467-021-26067-4 Publisher: Nature Publishing Group.

[23] Michael F Mascolo, Allison DiBianca Fasoli, and David Greenway. A Relational Approach to Moral Development in Societies, Organizations and Individuals. Integral Review, 17(1), 2021.

[24] Judith Butler, Ernesto Laclau, and Slavoj Žižek. Contingency, hegemony, universality: contemporary dialogues on the left. Phronesis. Verso, London, 2000. ISBN 978-1-85984-757-2 978-1-85984-278-2. OCLC: ocm44780799.

[25] Mona Sloane. Controversies, contradiction, and “participation” in AI. Big Data & Society, 11(1): 20539517241235862, March 2024. ISSN 2053-9517. doi: 10.1177/20539517241235862. URL https://doi.org/10.1177/20539517241235862 Publisher: SAGE Publications Ltd.

[26] Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, and Adina Williams. On the machine learning of ethical judgments from natural language. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 769–779, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.56. URL https://aclanthology.org/2022.naacl-main.56

[27] Lora Aroyo and Chris Welty. Truth Is a Lie: Crowd Truth and the Seven Myths of Human Annotation. AI Magazine, 36(1):15–24, March 2015. ISSN 2371-9621. doi: 10.1609/aimag.v36i1.2564. URL https://ojs.aaai.org/index.php/aimagazine/article/view/2564 Number: 1.

[28] Anand Siththaranjan, Cassidy Laidlaw, and Dylan Hadfield-Menell. Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF, December 2023. URL http://arxiv.org/abs/2312.08358 arXiv:2312.08358 [cs, stat].

[29] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems, volume 33, pages 3008–3021. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html

[30] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, April 2022. URL http://arxiv.org/abs/2204.05862 arXiv:2204.05862 [cs].

[31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730–27744, December 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html

[32] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI Feedback, December 2022. URL http://arxiv.org/abs/2212.08073 arXiv:2212.08073 [cs].

[33] Abeba Birhane, William Isaac, Vinodkumar Prabhakaran, Mark Diaz, Madeleine Clare Elish, Iason Gabriel, and Shakir Mohamed. Power to the People? Opportunities and Challenges for Participatory AI. In Equity and Access in Algorithms, Mechanisms, and Optimization, EAAMO ’22, pages 1–8, New York, NY, USA, October 2022. Association for Computing Machinery. ISBN 978-1-4503-9477-2. doi: 10.1145/3551624.3555290. URL https://doi.org/10.1145/3551624.3555290

[34] Esin Durmus, Karina Nyugen, Thomas I. Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, Liane Lovitt, Sam McCandlish, Orowa Sikder, Alex Tamkin, Janel Thamkul, Jared Kaplan, Jack Clark, and Deep Ganguli. Towards Measuring the Representation of Subjective Global Opinions in Language Models, June 2023. URL http://arxiv.org/abs/2306.16388 arXiv:2306.16388 [cs].

[35] Badr AlKhamissi, Muhammad ElNokrashy, Mai AlKhamissi, and Mona Diab. Investigating Cultural Alignment of Large Language Models, February 2024. URL http://arxiv.org/abs/2402.13231 arXiv:2402.13231 [cs].

[36] Michael J. Ryan, William Held, and Diyi Yang. Unintended Impacts of LLM Alignment on Global Representation, February 2024. URL http://arxiv.org/abs/2402.15018 arXiv:2402.15018 [cs].

[37] Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell Gordon, Niloofar Miresghallah, Christopher Michael Rytting, Andre Ye, Liwei Jiang, Ximing Lu, Nouha Dziri, Tim Althoff, and Yejin Choi. A Roadmap to Pluralistic Alignment, February 2024. URL http://arxiv.org/abs/2402.05070 arXiv:2402.05070 [cs].

[38] Iason Gabriel and Vafa Ghazavi. The Challenge of Value Alignment: from Fairer Algorithms to AI Safety, January 2021. URL http://arxiv.org/abs/2101.06060 arXiv:2101.06060 [cs].

[39] Jonathan Stray. Aligning AI Optimization to Community Well-Being. International Journal of Community Well-Being, 3(4):443–463, December 2020. ISSN 2524-5309. doi: 10.1007/s42413-020-00086-3. URL https://doi.org/10.1007/s42413-020-00086-3

[40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, July 2023. URL http://arxiv.org/abs/2307.09288 arXiv:2307.09288 [cs].

[41] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7B, October 2023. URL http://arxiv.org/abs/2310.06825 arXiv:2310.06825 [cs].

[42] Audrey G. Gift. Visual Analogue Scales: Measurement of Subjective Phenomena. Nursing Research, 38(5):286, October 1989. ISSN 0029-6562. URL https://journals.lww.com/nursingresearchonline/citation/1989/09000/visual_analogue_scales_measurement_of_subjective.6.aspx?casa_token=aO_mhu6sQyEAAAAA:y06v3LLFR-ZeutMmv1WTDebC4T_Je8nE_dS4M_qu96DJ6C_grR8Ro37158bzqwrw5zSexya6bpnQsp0JLfY8UXSrf

[43] Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. Dynabench: Rethinking Benchmarking in NLP. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for [44] Tristan Thrush, Kushal Tirumala, Anmol Gupta, Max Bartolo, Pedro Rodriguez, Tariq Kane, William Gaviria Rojas, Peter Mattson, Adina Williams, and Douwe Kiela. Dynatask: A Framework for Creating Dynamic AI Benchmark Tasks. In Valerio Basile, Zornitsa Kozareva, and Sanja Stajner, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 174–181, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-demo.17. URL https://aclanthology.org/2022.acl-demo.17

[45] R. C. Aitken. Measurement of feelings using visual analogue scales. Proceedings of the Royal Society of Medicine, 62(10):989–993, October 1969. ISSN 0035-9157. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1810824/

[46] Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A Long Way to Go: Investigating Length Correlations in RLHF, October 2023. URL http://arxiv.org/abs/2310.03716 arXiv:2310.03716 [cs].

[47] Ricardo J. G. B. Campello, Davoud Moulavi, and Joerg Sander. Density-Based Clustering Based on Hierarchical Density Estimates. In Jian Pei, Vincent S. Tseng, Longbing Cao, Hiroshi Motoda, and Guandong Xu, editors, Advances in Knowledge Discovery and Data Mining, Lecture Notes in Computer Science, pages 160–172, Berlin, Heidelberg, 2013. Springer. ISBN 978-3-642-37456-2. doi: 10.1007/978-3-642-37456-2_14.

[48] Sahand Negahban, Sewoong Oh, and Devavrat Shah. Iterative ranking from pair-wise comparisons. In Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012. URL https://papers.nips.cc/paper_files/paper/2012/hash/9adeb82fffb5444e81fa0ce8ad8afe7a-Abstract.html

[49] Gergei Bana, Wojciech Jamroga, David Naccache, and Peter Y. A. Ryan. Convergence Voting: From Pairwise Comparisons to Consensus, March 2021. URL http://arxiv.org/abs/2102.01995 arXiv:2102.01995 [cs].

[50] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The PageRank Citation Ranking: Bringing Order to the Web., November 1999. URL http://ilpubs.stanford.edu:8090/422/?doi=10.1.1.31.1768 Type: Techreport.

[51] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Advances in Neural Information Processing Systems, 36:46595–46623, December 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html

[52] Safiya Umoja Noble. Algorithms of oppression: how search engines reinforce racism. New York University Press, New York, 2018. ISBN 978-1-4798-4994-9 978-1-4798-3724-3.

[53] Christopher M. Kelty. The Participant – A Century of Participation in Four Stories. The University of Chicago press, Chicago (Ill.) London, 2019. ISBN 978-0-226-66662-4 978-0-226-66676-1.

[54] Caroline Criado-Perez. Invisible women: exposing data bias in a world designed for men. Chatto & Windus, London, 2019. ISBN 978-1-78474-172-3 978-1-78474-292-8.

[55] Mona Sloane, Emanuel Moss, Olaitan Awomolo, and Laura Forlano. Participation Is not a Design Fix for Machine Learning. In Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, EAAMO ’22, pages 1–6, New York, NY, USA, October 2022. Association for Computing Machinery. ISBN 978-1-4503-9477-2. doi: 10.1145/3551624.3555285. URL https://dl.acm.org/doi/10.1145/3551624.3555285

[56] Travis Greene, Copenhagen Business School, Galit Shmueli, National Tsing Hua University, Soumya Ray, and National Tsing Hua University. Taking the Person Seriously: Ethically Aware IS Research in the Era of Reinforcement Learning-Based Personalization. Journal of the Association for Information Systems, 24(6):1527–1561, 2023. ISSN 15369323. doi: 10.17705/1jais.00800. URL https://aisel.aisnet.org/jais/vol24/iss6/6/ [57] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. WebGPT: Browser-assisted question-answering with human feedback. December 2021. URL http://arxiv.org/abs/2112.09332v3

[58] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny, pornography, and malignant stereotypes. arXiv preprint arXiv:2110.01963, 2021.

[59] Ruha Benjamin. Race After Technology: Abolitionist Tools for the New Jim Code. John Wiley & Sons, July 2019. ISBN 978-1-5095-2643-7.

[60] Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. Language (Technology) is Power: A Critical Survey of “Bias” in NLP. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454–5476, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.485. URL https://aclanthology.org/2020.acl-main.485

[61] Daniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piquerás, Ilias Chalkidis, Ruixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust, and Anders Søgaard. Challenges and Strategies in Cross-Cultural NLP. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6997–7013, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.482. URL https://aclanthology.org/2022.acl-long.482

[62] Wenxuan Wang, Wenxiang Jiao, Jingyuan Huang, Ruyi Dai, Jen-tse Huang, Zhaopeng Tu, and Michael R. Lyu. Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models, February 2024. URL http://arxiv.org/abs/2310.12481 arXiv:2310.12481 [cs] version: 2.

[63] Suchin Gururangan, Dallas Card, Sarah K. Dreier, Emily K. Gade, Leroy Z. Wang, Zeyu Wang, Luke Zettlemoyer, and Noah A. Smith. Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection, January 2022. URL http://arxiv.org/abs/2201.10474 arXiv:2201.10474 [cs].

[64] Josh Dzieza. Inside the AI Factory, June 2023. URL https://www.theverge.com/features/23764584/ai-artificial-intelligence-data-notation-labor-scale-surge-remotasks-openai-chatbots

[65] Shakir Mohamed, Marie-Therese Png, and William Isaac. Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence. Philosophy & Technology, 33(4):659–684, December 2020. ISSN 2210-5441. doi: 10.1007/s13347-020-00405-8. URL https://doi.org/10.1007/s13347-020-00405-8

[66] Massimo Airoldi. Machine habitus: toward a sociology of algorithms. Polity Press, Cambridge ; Medford, MA, 2022. ISBN 978-1-5095-4327-4 978-1-5095-4328-1. OCLC: on1247827618.

[67] Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, and Adina Williams. On the Machine Learning of Ethical Judgments from Natural Language. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 769–779, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.56. URL https://aclanthology.org/2022.naacl-main.56

[68] Mark Díaz, Ian Kivlichan, Rachel Rosen, Dylan Baker, Razvan Amironesei, Vinodkumar Prabhakaran, and Emily Denton. CrowdWorkSheets: Accounting for Individual and Collective Identities Underlying Crowdsourced Dataset Annotation. In 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’22, pages 2342–2351, New York, NY, USA, June 2022. Association for Computing Machinery. ISBN 978-1-4503-9352-2. doi: 10.1145/3531146.3534647. URL https://doi.org/10.1145/3531146.3534647

[69] Emily M. Bender and Batya Friedman. Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science. Transactions of the Association for Computational Linguistics, 6:587–604, 2018. doi: 10.1162/tacl_a_00041. URL https://aclanthology.org/Q18-1041

[70] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model Cards for Model Reporting. Proceedings of the Conference on Fairness, Accountability, and Transparency - FAT* ’19, pages 220–229, 2019. doi: 10.1145/3287560.3287596. URL http://arxiv.org/abs/1810.03993 arXiv: 1810.03993. [71] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. Datasheets for datasets. *Communications of the ACM*, 64(12): 86–92, December 2021. ISSN 15577317. doi: 10.1145/3458723. Publisher: Association for Computing Machinery.

[72] Aida Mostafazadeh Davani, Mark Díaz, and Vinodkumar Prabhakaran. Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations. *Transactions of the Association for Computational Linguistics*, 10:92–110, January 2022. ISSN 2307-387X. doi: 10.1162/tacl_a_00449. URL [https://doi.org/10.1162/tacl_a_00449](https://doi.org/10.1162/tacl_a_00449)

[73] Mitchell L. Gordon, Michelle S. Lam, Joon Sung Park, Kayur Patel, Jeff Hancock, Tatsunori Hashimoto, and Michael S. Bernstein. Jury Learning: Integrating Dissenting Voices into Machine Learning Models. In *Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems*, CHI ’22, pages 1–19, New York, NY, USA, April 2022. Association for Computing Machinery. ISBN 978-1-4503-9157-3. doi: 10.1145/3491102.3502004. URL [https://doi.org/10.1145/3491102.3502004](https://doi.org/10.1145/3491102.3502004)

[74] Barbara Plank, Dirk Hovy, and Anders Søgaard. Learning part-of-speech taggers with inter-annotator agreement loss. In Shuly Wintner, Sharon Goldwater, and Stefan Riezler, editors, *Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics*, pages 742–751, Gothenburg, Sweden, April 2014. Association for Computational Linguistics. doi: 10.3115/v1/E14-1078. URL [https://aclanthology.org/E14-1078](https://aclanthology.org/E14-1078)

[75] Yixin Nie, Xiang Zhou, and Mohit Bansal. What Can We Learn from Collective Human Opinions on Natural Language Inference Data? In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 9131–9143, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.734. URL [https://aclanthology.org/2020.emnlp-main.734](https://aclanthology.org/2020.emnlp-main.734)

[76] Maximilian Wich, Christian Widmer, Gerhard Hagerer, and Georg Groh. Investigating Annotator Bias in Abusive Language Datasets. In Ruslan Mitkov and Galia Angelova, editors, *Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)*, pages 1515–1525, Held Online, September 2021. INCOMA Ltd. URL [https://aclanthology.org/2021.ranlp-1.170](https://aclanthology.org/2021.ranlp-1.170)

[77] Maarten Sap, Swabha Swayamdipta, Laura Vianna, Xuhui Zhou, Yejin Choi, and Noah A. Smith. Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection. In *Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 5884–5906, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.431. URL [https://aclanthology.org/2022.naacl-main.431](https://aclanthology.org/2022.naacl-main.431)

[78] Nitesh Goyal, Ian D. Kivlichan, Rachel Rosen, and Lucy Vasserman. Is Your Toxicity My Toxicity? Exploring the Impact of Rater Identity on Toxicity Annotation. *Proceedings of the ACM on Human-Computer Interaction*, 6(CSCW2):363:1–363:28, November 2022. doi: 10.1145/3555088. URL [https://dl.acm.org/doi/10.1145/3555088](https://dl.acm.org/doi/10.1145/3555088)

[79] Lora Aroyo, Alex S. Taylor, Mark Diaz, Christopher M. Homan, Alicia Parrish, Greg Serapio-Garcia, Vinodkumar Prabhakaran, and Ding Wang. DICES Dataset: Diversity in Conversational AI Evaluation for Safety, June 2023. URL [http://arxiv.org/abs/2306.11247](http://arxiv.org/abs/2306.11247) arXiv:2306.11247 [cs].

[80] Joseph Henrich, Steven J. Heine, and Ara Norenzayan. Most people are not WEIRD. *Nature*, 466(7302):29–29, July 2010. ISSN 1476-4687. doi: 10.1038/466029a. URL [https://www.nature.com/articles/466029a](https://www.nature.com/articles/466029a) Number: 7302 Publisher: Nature Publishing Group.

[81] Dante A. Urbina and Alberto Ruiz-Villaverde. A Critical Review of Homo Economicus from Five Approaches. *The American Journal of Economics and Sociology*, 78(1):63–93, 2019. ISSN 1536-7150. doi: 10.1111/ajes.12258. URL [https://onlinelibrary.wiley.com/doi/abs/10.1111/ajes.12258](https://onlinelibrary.wiley.com/doi/abs/10.1111/ajes.12258) _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ajes.12258.

[82] Coren Apicella, Ara Norenzayan, and Joseph Henrich. Beyond WEIRD: A review of the last decade and a look ahead to the global laboratory of the future. *Evolution and Human Behavior*, 41(5):319–329, September 2020. ISSN 1090-5138. doi: 10.1016/j.evolhumbehav.2020.07.015. URL [https://www.sciencedirect.com/science/article/pii/S1090513820300957](https://www.sciencedirect.com/science/article/pii/S1090513820300957)

[83] Andrew Ng and Stuart J. Russell. Algorithms for Inverse Reinforcement Learning. 2000. [84] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep Reinforcement Learning from Human Preferences. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html

[85] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-Tuning Language Models from Human Preferences. September 2019. URL http://arxiv.org/abs/1909.08593v2

[86] Shachar Mirkin and Jean-Luc Meunier. Personalized machine translation: Predicting translational preferences. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 2019–2025, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1238. URL https://aclanthology.org/D15-1238

[87] Khanh Nguyen, Hal Daumé III, and Jordan Boyd-Graber. Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1464–1474, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1153. URL https://aclanthology.org/D17-1153

[88] Julia Kreutzer, Artem Sokolov, and Stefan Riezler. Bandit Structured Prediction for Neural Sequence-to-Sequence Learning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1503–1513, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1138. URL https://aclanthology.org/P17-1138

[89] Marilyn A Walker. An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email. Journal of Artificial Intelligence Research, 12:387–416, 2000.

[90] Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and Steve Young. A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies. The knowledge engineering review, 21(2):97–126, 2006.

[91] Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Maria Rojas-Barahona, Stefan Ultes, David Vandyke, Tsung-Hsien Wen, and Steve J. Young. Continuously learning neural dialogue management. abs/1606.02689, 2016. URL http://arxiv.org/abs/1606.02689

[92] Jiwei Li, Alexander H. Miller, Sumit Chopra, Marc’Aurelio Ranzato, and Jason Weston. Dialogue learning with human-in-the-loop. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=HJgXCV9xx

[93] Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog, July 2019. URL http://arxiv.org/abs/1907.00456 arXiv:1907.00456 [cs, stat].

[94] Natasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Human-centric dialog training via offline reinforcement learning. In Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 3985–4003, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.327. URL https://aclanthology.org/2020.emnlp-main.327

[95] Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soňa Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogue agents via targeted human judgements. September 2022. URL http://arxiv.org/abs/2209.14375v1

[96] Jérémy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training Language Models with Language Feedback, November 2022. URL http://arxiv.org/abs/2204.14146 arXiv:2204.14146 [cs]. [97] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A General Language Assistant as a Laboratory for Alignment, December 2021. URL http://arxiv.org/abs/2112.00861 arXiv:2112.00861 [cs].

[98] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms, August 2017. URL http://arxiv.org/abs/1707.06347 arXiv:1707.06347 [cs].

[99] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3):229–256, May 1992. ISSN 1573-0565. doi: 10.1007/BF00992696. URL https://doi.org/10.1007/BF00992696

[100] Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs, February 2024. URL http://arxiv.org/abs/2402.14740 arXiv:2402.14740 [cs] version: 1.

[101] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: Less Is More for Alignment, May 2023. URL http://arxiv.org/abs/2305.11206 arXiv:2305.11206 [cs].

[102] Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, and Nat McAleese. Teaching language models to support answers with verified quotes, March 2022. URL http://arxiv.org/abs/2203.11147 arXiv:2203.11147 [cs].

[103] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafari, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. LaMDA: Language Models for Dialog Applications, February 2022. URL http://arxiv.org/abs/2201.08239 arXiv:2201.08239 [cs].

[104] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, L. J. Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. RewardBench: Evaluating Reward Models for Language Modeling, March 2024. URL http://arxiv.org/abs/2403.13787 arXiv:2403.13787 [cs].

[105] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. KTO: Model Alignment as Prospect Theoretic Optimization, February 2024. URL http://arxiv.org/abs/2402.01306 arXiv:2402.01306 [cs].

[106] StanfordNLP. Stanford Human Preferences Dataset, September 2023. URL https://huggingface.co/datasets/stanfordnlp/SHP

[107] Nathan Lambert, Lewis Tunstall, Nazneen Rajani, and Tristan Thrush. HuggingFace H4 Stack Exchange Preference Dataset, 2023. URL https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences

[108] William Agnew, A. Stevie Bergman, Jennifer Chien, Mark Díaz, Seliem El-Sayed, Jaylen Pittman, Shakir Mohamed, and Kevin R. McKee. The illusion of artificial inclusion, February 2024. URL http://arxiv.org/abs/2401.08572 arXiv:2401.08572 [cs].

[109] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback, January 2024. URL http://arxiv.org/abs/2305.14387 arXiv:2305.14387 [cs].

[110] Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Jiexin Wang, Huimin Chen, Bowen Sun, Ruobing Xie, Jie Zhou, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment, February 2024. URL http://arxiv.org/abs/2402.19085 arXiv:2402.19085 [cs, eess]. [111] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset, March 2024. URL http://arxiv.org/abs/2309.11998 arXiv:2309.11998 [cs].

[112] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. (InThe)WildChat: 570K ChatGPT Interaction Logs In The Wild. October 2023. URL https://openreview.net/forum?id=Bl8u7ZRlbM

[113] Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyễn Minh Duc, Oliver Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. OpenAssistant Conversations – Democratizing Large Language Model Alignment, October 2023. URL http://arxiv.org/abs/2304.07327 arXiv:2304.07327 [cs].

[114] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned, November 2022. URL http://arxiv.org/abs/2209.07858 arXiv:2209.07858 [cs].

[115] Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Matacinas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzeminski, Hakimeh Fadaei, Irem Ergün, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu Minh Chien, Sebastian Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet Üstün, Marzieh Fadaee, and Sara Hooker. Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning, February 2024. URL http://arxiv.org/abs/2402.06619 arXiv:2402.06619 [cs].

[116] The Alan Turing Institute and The Ada Lovelace Institute. How do people feel about AI? A nationally representative survey of public attitudes to artificial intelligence in Britain. Technical report, 2023. URL https://attitudestoai.uk/assets/documents/Ada-Lovelace-Institute-The-Alan-Turing-Institute-How-do-people-feel-about-AI.pdf

[117] Jimin Mun, Liwei Jiang, Jenny Liang, Inyoung Cheong, Nicole DeCarlo, Yejin Choi, Tadayoshi Kohno, and Maarten Sap. Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits, March 2024. URL http://arxiv.org/abs/2403.14791 arXiv:2403.14791 [cs].

[118] Samuel Chang, Estelle Ciesla, Michael Finch, James Fishkin, Lodewijk Gelauff, Ashish Goel, Ricky Hernandez Marquez, Shoaib Mohammed, and Alice Siu. Meta Community Forum: Results Analysis. Technical report, Deliberative Democracy Lab, Stanford University, April 2024.

[119] Stevie Bergman, Nahema Marchal, John Mellor, Shakir Mohamed, Iason Gabriel, and William Isaac. STELA: a community-centred approach to norm elicitation for AI alignment. Scientific Reports, 14(1): 6616, March 2024. ISSN 2045-2322. doi: 10.1038/s41598-024-56648-4. URL https://www.nature.com/articles/s41598-024-56648-4 Publisher: Nature Publishing Group.

[120] R. Silberzahn, E. L. Uhlmann, D. P. Martin, P. Anselmi, F. Aust, E. Awtrey, Š. Bahník, F. Bai, C. Bannard, E. Bonnier, R. Carlsson, F. Cheung, G. Christensen, R. Clay, M. A. Craig, A. Dalla Rosa, L. Dam, M. H. Evans, I. Flores Cervantes, N. Fong, M. Gamez-Djokic, A. Glenz, S. Gordon-McKeon, T. J. Heaton, K. Hederos, M. Heene, A. J. Hofelich Mohr, F. Högdén, K. Hui, M. Johannesson, J. Kalodimos, E. Kaszubowski, D. M. Kennedy, R. Lei, T. A. Lindsay, S. Liverani, C. R. Madan, D. Molden, E. Molleman, R. D. Morey, L. B. Mulder, B. R. Nijstad, N. G. Pope, B. Pope, J. M. Prenoveau, F. Rink, E. Robusto, H. Roderique, A. Sandberg, E. Schlüter, F. D. Schönbrodt, M. F. Sherman, S. A. Sommer, K. Sotak, S. Spain, C. Spörlein, T. Stafford, L. Stefanutti, S. Tauber, J. Ullrich, M. Vianello, E.-J. Wagenmakers, M. Witkowiak, S. Yoon, and B. A. Nosek. Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results. Advances in Methods and Practices in Psychological Science, 1(3):337–356, September 2018. ISSN 2515-2459. doi: 10.1177/2515245917747646. URL https://doi.org/10.1177/2515245917747646 Publisher: SAGE Publications Inc.

[121] John Tasioulas. Artificial Intelligence, Humanistic Ethics. Daedalus, 151(2):232–243, May 2022. ISSN 0011-5266. doi: 10.1162/daed_a_01912. URL https://doi.org/10.1162/daed_a_01912 [122] Diane Proudfoot. Anthropomorphism and AI: Turing’s much misunderstood imitation game. *Artificial Intelligence*, 175(5):950–957, April 2011. ISSN 0004-3702. doi: 10.1016/j.artint.2011.01.006. URL https://www.sciencedirect.com/science/article/pii/S000437021100018X

[123] David Watson. The Rhetoric and Reality of Anthropomorphism in Artificial Intelligence. *Minds and Machines*, 29(3):417–440, September 2019. ISSN 1572-8641. doi: 10.1007/s11023-019-09506-6. URL https://doi.org/10.1007/s11023-019-09506-6

[124] Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown, Zac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell, William Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, and Jason Gabriel. Taxonomy of Risks posed by Language Models. In *2022 ACM Conference on Fairness, Accountability, and Transparency*, pages 214–229, Seoul Republic of Korea, June 2022. ACM. ISBN 978-1-4503-9352-2. doi: 10.1145/3531146.3533088. URL https://dl.acm.org/doi/10.1145/3531146.3533088

[125] Gavin Abercrombie, Amanda Cercas Curry, Tanvi Dinkar, Verena Rieser, and Zeerak Talat. Mirages. On Anthropomorphism in Dialogue Systems. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 4776–4790, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.290. URL https://aclanthology.org/2023.emnlp-main.290

[126] Myra Cheng, Kristina Gligoric, Tiziano Piccardi, and Dan Jurafsky. AnthroScore: A Computational Linguistic Measure of Anthropomorphism, February 2024. URL http://arxiv.org/abs/2402.02056 arXiv:2402.02056 [cs].

[127] Sian Gooding and Hassan Mansoor. The Impact of Preference Agreement in Reinforcement Learning from Human Feedback: A Case Study in Summarization, November 2023. URL http://arxiv.org/abs/2311.04919 arXiv:2311.04919 [cs].

[128] Catherine D’Ignazio and Lauren F. Klein. *Data Feminism*. The MIT Press, March 2020. ISBN 978-0-262-35852-1. doi: 10.7551/mitpress/11805.001.0001. URL https://direct.mit.edu/books/book/4660/Data-Feminism

[129] Abeba Birhane. Algorithmic injustice: a relational ethics approach. *Patterns*, 2(2):100205, February 2021. ISSN 26663899. doi: 10.1016/j.patter.2021.100205. URL https://linkinghub.elsevier.com/retrieve/pii/S2666389921000155

[130] Yijiang River Dong, Tiancheng Hu, and Nigel Collier. Can LLM be a Personalized Judge?, June 2024. URL http://arxiv.org/abs/2406.11657 arXiv:2406.11657 [cs].

[131] Billy Perrigo. Inside OpenAI’s Plan to Make AI More ’Democratic’, February 2024. URL https://time.com/6684266/openai-democracy-artificial-intelligence/

[132] Fabio Crameri, Grace E. Shephard, and Philip J. Heron. The misuse of colour in science communication. *Nature Communications*, 11(1):5444, October 2020. ISSN 2041-1723. doi: 10.1038/s41467-020-19160-7. URL https://www.nature.com/articles/s41467-020-19160-7 Number: 1 Publisher: Nature Publishing Group.

[133] Nenad Tomasev, Kevin R. McKee, Jackie Kay, and Shakir Mohamed. Fairness for Unobserved Characteristics: Insights from Technological Impacts on Queer Communities. In *Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society*, AIES ’21, pages 254–265, New York, NY, USA, July 2021. Association for Computing Machinery. ISBN 978-1-4503-4503-5. doi: 10.1145/3461702.3462540. URL https://doi.org/10.1145/3461702.3462540

[134] Tom Hosking, Phil Blunsom, and Max Bartolo. Human Feedback is not Gold Standard, January 2024. URL http://arxiv.org/abs/2309.16349 arXiv:2309.16349 [cs].

[135] Amos Tversky and Itamar Simonson. Context-Dependent Preferences. *Management Science*, 39(10):1179–1189, October 1993. ISSN 0025-1909. doi: 10.1287/mnsc.39.10.1179. URL https://pubsonline.informs.org/doi/abs/10.1287/mnsc.39.10.1179 Publisher: INFORMS.

[136] Boaz Shmueli, Jan Fell, Soumya Ray, and Lun-Wei Ku. Beyond fair pay: Ethical implications of NLP crowdsourcing. In *Association for Computational Linguistics (ACL)*, pages 3758–3769, April 2021. URL https://arxiv.org/abs/2104.10097v1 arXiv: 2104.10097 Publisher: tex.arxivid: 2104.10097. [137] Lisa Posch, Arnim Bleier, Fabian Flöck, Clemens M. Lechner, Katharina Kinder-Kurlanda, Denis Helic, and Markus Strohmaier. Characterizing the Global Crowd Workforce: A Cross-Country Comparison of Crowdworker Demographics. *Human Computation*, 9(1), August 2022. ISSN 2330-8001. doi: 10.15346/hc.v9i1.106. URL [http://arxiv.org/abs/1812.05948](http://arxiv.org/abs/1812.05948) arXiv:1812.05948 [cs].

[138] Derek A. Albert and Daniel Smilek. Comparing attentional disengagement between Prolific and MTurk samples. *Scientific Reports*, 13(1):20574, November 2023. ISSN 2045-2322. doi: 10.1038/s41598-023-46048-5. URL [https://www.nature.com/articles/s41598-023-46048-5](https://www.nature.com/articles/s41598-023-46048-5) Publisher: Nature Publishing Group.

[139] Gemini Team. Gemini: A Family of Highly Capable Multimodal Models, December 2023. URL [http://arxiv.org/abs/2312.11805](http://arxiv.org/abs/2312.11805) arXiv:2312.11805 [cs].

[140] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of Experts, January 2024. URL [http://arxiv.org/abs/2401.04088](http://arxiv.org/abs/2401.04088) arXiv:2401.04088 [cs].

[141] Anthropic. Introducing the next generation of Claude, April 2024. URL [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family)

[142] Cohere. Command R, April 2024. URL [https://docs.cohere.com/docs/command-r](https://docs.cohere.com/docs/command-r)

[143] MetaAI. Introducing Meta Llama 3: The most capable openly available LLM to date, April 2024. URL [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)

[144] Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. Whose Opinions Do Language Models Reflect?, March 2023. URL [http://arxiv.org/abs/2303.17548](http://arxiv.org/abs/2303.17548) arXiv:2303.17548 [cs].

[145] Fredrik Barth. *Ethnic Groups and Boundaries: The Social Organization of Culture Difference*. Waveland Press, March 1998. ISBN 978-1-4786-0795-3. Google-Books-ID: QaAQAAAAQBAJ.

[146] Katarzyna Hamer, Sam McFarland, Barbara Czarnecka, Agnieszka Golińska, Liliana Manrique Cadena, Magdalena Łużniak Piecha, and Tomasz Jułkowski. What Is an “Ethnic Group” in Ordinary People’s Eyes? Different Ways of Understanding It Among American, British, Mexican, and Polish Respondents. *Cross-Cultural Research*, 54(1):28–72, February 2020. ISSN 1069-3971. doi: 10.1177/1069397118816939. URL [https://doi.org/10.1177/1069397118816939](https://doi.org/10.1177/1069397118816939) Number: 3 Publisher: SAGE Publications Inc.

[147] Karen L. Suyemoto, Micaela Curley, and Shruti Mukkamala. What Do We Mean by “Ethnicity” and “Race”? A Consensual Qualitative Research Investigation of Colloquial Understandings. *Genealogy*, 4(3):81, September 2020. ISSN 2313-5778. doi: 10.3390/genealogy4030081. URL [https://www.mdpi.com/2313-5778/4/3/81](https://www.mdpi.com/2313-5778/4/3/81) Number: 3 Publisher: Multidisciplinary Digital Publishing Institute.

[148] Laurence R. Iannaccone. Introduction to the Economics of Religion. *Journal of Economic Literature*, 36(3):1465–1495, 1998. ISSN 0022-0515. URL [https://www.jstor.org/stable/2564806](https://www.jstor.org/stable/2564806) Publisher: American Economic Association.

[149] Gilat Levy and Ronny Razin. Does Religion Breed Trust? A Cross-National Study of the Effects of Religious Involvement, Religious Faith, and Religious Context on Social Trust. *Journal for the Scientific Study of Religion*, 54(4):739–755, 2015. ISSN 0021-8294. URL [https://www.jstor.org/stable/26651394](https://www.jstor.org/stable/26651394) Publisher: [Society for the Scientific Study of Religion, Wiley].

[150] Hansong Zhang, Joshua N. Hook, Jennifer E. Farrell, David K. Mosher, Laura E. Captari, Steven P. Coomes, Daryl R. Van Tongeren, and Don E. Davis. Exploring Social Belonging and Meaning in Religious Groups. *Journal of Psychology and Theology*, 47(1):3–19, March 2019. ISSN 0091-6471. doi: 10.1177/0091647118806345. URL [https://doi.org/10.1177/0091647118806345](https://doi.org/10.1177/0091647118806345) Publisher: SAGE Publications Ltd. [152] Vassilis Saroglou, Magali Clobert, Adam B. Cohen, Kathryn A. Johnson, Kevin L. Ladd, Matthieu Van Pachterbeke, Lucia Adamovova, Joanna Blogowska, Pierre-Yves Brandt, Cem Safak Çukur, Kwang-Kuo Hwang, Anna Miglietta, Frosso Motti-Stefanidi, Antonio Muñoz-García, Sebastian Murken, Nicolas Roussiau, and Javier Tapia Valladares. Believing, Bonding, Behaving, and Belonging: The Cognitive, Emotional, Moral, and Social Dimensions of Religiousness across Cultures. *Journal of Cross-Cultural Psychology*, 51(7-8):551–575, September 2020. ISSN 0022-0221. doi: 10.1177/0022022120946488. URL [https://doi.org/10.1177/0022022120946488](https://doi.org/10.1177/0022022120946488) Publisher: SAGE Publications Inc.

[153] Prolific. Representative samples, February 2024. URL [https://researcher-help.prolific.com/hc/en-gb/articles/360019236753-Representative-samples](https://researcher-help.prolific.com/hc/en-gb/articles/360019236753-Representative-samples)

[154] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks, August 2019. URL [http://arxiv.org/abs/1908.10084](http://arxiv.org/abs/1908.10084) arXiv:1908.10084 [cs].

[155] Leland McInnes, John Healy, and James Melville. UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction, September 2020. URL [http://arxiv.org/abs/1802.03426](http://arxiv.org/abs/1802.03426) arXiv:1802.03426 [cs, stat].

[156] Ashkan Kazemi, Kiran Garimella, Gautam Kishore Shahi, Devin Gaffney, and Scott A. Hale. Research note: Tiplines to uncover misinformation on encrypted platforms: A case study of the 2019 Indian general election on WhatsApp. *Harvard Kennedy School Misinformation Review*, January 2022. doi: 10.37016/mr-2020-91. URL [https://misinforeview.hks.harvard.edu/article/research-note-tiplines-to-uncover-misinformation-on-encrypted-platforms-a-case-study-of-the-2019-indian-general-election-on-whatsapp/](https://misinforeview.hks.harvard.edu/article/research-note-tiplines-to-uncover-misinformation-on-encrypted-platforms-a-case-study-of-the-2019-indian-general-election-on-whatsapp/)

[157] Scott A. Hale. meedan/temporal_clustering, March 2022. URL [https://github.com/meedan/temporal_clustering/tree/main](https://github.com/meedan/temporal_clustering/tree/main)

[158] Scott D. Emerson, Martin Guhn, and Anne M. Gadermann. Measurement invariance of the Satisfaction with Life Scale: reviewing three decades of research. *Quality of Life Research*, 26(9):2251–2264, September 2017. ISSN 1573-2649. doi: 10.1007/s11136-017-1552-2. URL [https://doi.org/10.1007/s11136-017-1552-2](https://doi.org/10.1007/s11136-017-1552-2)

[159] John E. Roemer. *Theories of distributive justice*. Harvard Univ. Press, Cambridge, Mass., 1. harvard univ. press paperback ed edition, 1998. ISBN 978-0-674-87920-1 978-0-674-87919-5.

[160] Jeremy Bentham. An Introduction to the Principles of Morals and Legislation. In J. H. Burns and H. L. A. Hart, editors, *The Collected Works of Jeremy Bentham: An Introduction to the Principles of Morals and Legislation*. Oxford University Press, January 1789. ISBN 978-0-19-820516-6. doi: 10.1093/oseo/instance.00077240. URL [http://www.oxfordscholarlyeditions.com/view/10.1093/actrade/9780198205166.book.1/actrade-9780198205166-work-1](http://www.oxfordscholarlyeditions.com/view/10.1093/actrade/9780198205166.book.1/actrade-9780198205166-work-1).

[161] Marc Lanctot, Kate Larson, Yoram Bachrach, Luke Marris, Zun Li, Avishkar Bhoopchand, Thomas Anthony, Brian Tanner, and Anna Koop. Evaluating Agents using Social Choice Theory, December 2023. URL [http://arxiv.org/abs/2312.03121](http://arxiv.org/abs/2312.03121) arXiv:2312.03121 [cs] version: 2.

[162] Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker, and Marzieh Fadaee. Elo Uncovered: Robustness and Best Practices in Language Model Evaluation, November 2023. URL [http://arxiv.org/abs/2311.17295](http://arxiv.org/abs/2311.17295) arXiv:2311.17295 [cs].

[163] Sahand Negahban, Sewoong Oh, and Devavrat Shah. Rank Centrality: Ranking from Pairwise Comparisons. *Operations Research*, 65(1):266–287, 2017. ISSN 0030-364X. URL [https://www.jstor.org/stable/26153541](https://www.jstor.org/stable/26153541) Publisher: INFORMS.

**Checklist**

1. For all authors...
   
   (a) Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? [Yes]. **The paper has two core sections. First, we describe the contributions of the dataset in § 2. Then, we present three case studies in § 3.**
   
   (b) Did you describe the limitations of your work? [Yes]. **We summarise limitations in § 5 and provide an expanded discussion in our Data Statement App. B.** (c) Did you discuss any potential negative societal impacts of your work? [Yes]. We summarise societal impacts and ethical considerations in § 5, including risks from harmful content, privacy violations and participation-washing (participants not sharing in profits of improved technologies). We expand on these points in our Data Statement (App. B).

(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]. To summarise, we (i) pay fair living wages to all participants (App. B and App. J); (ii) we received IRB approval from the University of Oxford and collected informed consent from every participant (App. D); (iv) we check for PII and release all metadata (App. E); (v) we carefully consider licenses and provide a data clause with terms of use (App. C); (vi) we consider and discuss representativeness (App. L); and (vii) we provide detailed documentation of the dataset (App. B).

2. If you are including theoretical results...

(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]

3. If you ran experiments (e.g. for benchmarks)...

(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Experiments for the three case studies presented in the paper are described in detail in the Supplementary Material. We also release all code needed to reproduce results on our Github [https://github.com/HannahKirk/prism-alignment].

(b) Did you specify all the training details (e.g. data splits, hyperparameters, how they were chosen)? [N/A]. We don’t train any models per se, but do provide details for our analysis (e.g. hyperparameters for our clustering pipeline) in the Supplementary Material.

(c) Did you report error bars (e.g. with respect to the random seed after running experiments multiple times)? [Yes]. For any bootstrapped analysis in the main paper (e.g. Fig. 4) or Supplementary Material (e.g. App. T), we provide error bars.

(d) Did you include the total amount of compute and the type of resources used (e.g. type of GPUs, internal cluster, or cloud provider)? [N/A]. We didn’t train any compute intensive models. We accessed existing models via API (see App. P), which was made possible by grants and waived credit costs (as described in Disclosure of Funding). All analysis experiments were run locally on CPU.

4. If you are using existing assets (e.g. code, data, models) or curating/releasing new assets...

(a) If your work uses existing assets, did you cite the creators? [Yes]
(b) Did you mention the license of the assets? [Yes]. We discuss licenses in App. C.
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]. Our dataset and code are linked after the abstract, and at the start of the Supplementary material. The data can be accessed on Github at [https://github.com/HannahKirk/prism-alignment], and also on HuggingFace at [https://huggingface.co/datasets/HannahRoseKirk/prism-alignment]. The dataset has a permanent DOI: 10.57967/hf/2113.

(d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [Yes]. We discuss informed consent in the main paper (§ 2) and provide full details (including consent forms) in the Supplementary material (App. D).

(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]. For every text utterances in our dataset, we conduct automated PII checks (which are then manually inspected) and a moderation classifier for harmful content. We provide full details of this process in App. E, and release all metadata alongside the data entries. We summarise ethical concerns and risks in the main paper (§ 5) and data statement (App. B).

5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes]. **Full text of the survey questions and interface instructions is presented in detailed code books (App. V).** We also provide interface screenshots (App. Q).

(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [Yes]. **We communicate risks to participants in the informed consent form (App. D),** for example, the risk of exposure to harmful content via external APIs to LLMs. We received IRB approval. We also describe participant risks in the main paper (§ 5) and data statement (App. B).

(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes]. **We discuss hourly wage (£9) and task completion time (70 minutes) in the main paper (§ 2).** We provide additional detail in our data statement (App. B). We provide estimates of total amount spent on participant compensation in App. J. # Supplementary Material

## Table of Contents

| Section                                                                 | Page |
|------------------------------------------------------------------------|------|
| **PART I: Dataset Details and Distributions**                          |      |
| A  PRISM Data Access and Format                                        | 27   |
| B  PRISM Data Statement                                                | 29   |
| C  PRISM Data Clause                                                   | 33   |
| D  Informed Consent                                                    | 34   |
| E  Metadata Processing                                                 | 36   |
| F  Annotating Ethnicity, Religion and Gender                           | 38   |
| G  Participant Demographics                                            | 38   |
| H  Participant Geographies                                             | 42   |
| I  Participant LLM Usage and Familiarity                               | 45   |
| J  Screening and Recruitment Process                                   | 47   |
| K  Conversation Type Rebalancing                                      | 48   |
| L  Census Rebalancing                                                 | 48   |
| M  Text and N-Gram Analysis                                           | 50   |
| N  Comparing Fine-Grained Preference Attributes                        | 54   |
| O  Score Distributions                                                | 57   |
| P  Details of LLMs-in-the-loop                                         | 60   |
| Q  Interface Screenshots                                               | 64   |
| **PART II: Extended Case Study Details**                               |      |
| R  Case Study IA: Topic Clustering and Regressions                     | 67   |
| S  Case Study IB: Local Neighbourhoods and Empirically-Fixed Contexts | 78   |
| T  Case Study II: Aggregating Preference Ratings to Model Ranks        | 84   |
| U  Case Study III: Welfare Analysis                                   | 94   |
| **PART III: Codebooks**                                               |      |
| V  Codebooks                                                          | 97   | A PRISM Data Access and Format

The data can be accessed on Github at [https://github.com/HannahKirk/prism-alignment](https://github.com/HannahKirk/prism-alignment) and also on HuggingFace at [https://huggingface.co/datasets/HannahRoseKirk/prism-alignment](https://huggingface.co/datasets/HannahRoseKirk/prism-alignment). The dataset has a permanent DOI: 10.57967/hf/2113.

The dataset is organised in two primary JSON lines files:

- **The Survey** (survey.jsonl): The survey where participants answer questions such as their stated preferences for LLM behaviours, their familiarity with LLMs, a self-description and some basic demographics. Each row is a single participant in our dataset, identified by a user_id.

- **The Conversations** (conversations.jsonl): Each participants’ multiple conversation trees with LLMs and associated feedback. Each row is a single conversation, identified by a conversation_id, that can be matched back to a participant’s survey profile via the user_id. The conversation itself is stored as a list of dictionaries representing human and model turns in the conversation_history column, which broadly follows the format of widely used Chat APIs (see single entry schema on the next page).

Additionally, for ease of secondary analysis we provide a more granular and flattened format of the conversations data:

- **The Utterances** (utterances.jsonl): Each row is a single scored utterance (human input - model response - score). Each row has an utterance_id that can be mapped back to the conversation data using conversation_id or the survey using user_id. The model responses and scores per each user input are in *long format*. Because of this format, the user inputs will be repeated for the set of model responses in a single interaction turn.

We also provide code for transforming the conversations to a *wide format*. That is, each row is now a single turn within a conversation. For the first interaction where up to four models respond, we have model_{a/b/c/d} as four distinct columns and score_{a/b/c/d} as another four columns. Note that for subsequent turns, the same model responds and there are only two responses so model/score_{c/d} will always be missing.

Finally, for every text instance in PRISM, we provide metadata on the language detection, personal or private information (PII) detection and moderation flags. **The Metadata** is provided seperately to the main data files (metadata.jsonl).

We provide codebooks for **The Survey** (App. V.1), **The Conversations** (App. V.2), **The Utterances** (App. V.3) and **The Metadata** (App. V.4). Format of Entries in Conversations Data

```json
{
    "conversation_id": "c1",
    "user_id": "user123",
    "conversation_type": ["unguided", "values guided", "controversy guided"],
    "opening_prompt": "[USER PROMPT]",
    "conversation_turns": [2-22],
    "conversation_history": [
        {
            "turn": 0,
            "role": "user",
            "content": "[USER PROMPT]"
        },
        {
            "turn": 0,
            "role": "model",
            "content": "[MODEL RESPONSE]",
            "model_name": "M1",
            "model_provider": "P1",
            "score": [1-100],
            "if_chosen": false,
            "within_turn_id": 0
        },
        {
            "turn": 0,
            "role": "model",
            "content": "[MODEL RESPONSE]",
            "model_name": "M2",
            "model_provider": "P2",
            "score": [1-100],
            "if_chosen": true,
            "within_turn_id": 1
        },
        //... Additional list items for remaining model responses (up to 4 in total)
        {
            "turn": 1,
            "role": "user",
            "content": "[USER PROMPT]"
        },
        {
            "turn": 1,
            "role": "model",
            "content": "[MODEL RESPONSE]",
            "model_name": "M2",
            "model_provider": "P2",
            "score": [1-100],
            "if_chosen": true,
            "within_turn_id": 0
        },
        {
            "turn": 1,
            "role": "model",
            "content": "[MODEL RESPONSE]",
            "model_name": "M2",
            "model_provider": "P2",
            "score": [1-100],
            "if_chosen": false,
            "within_turn_id": 1
        }
        //... Additional turns follow the same pattern as turn 1
    ],
    "performance_attributes": {
        "fluency": [1-100],
        "factuality": [1-100],
        "helpfulness": [1-100],
        //... Additional attribute ratings
    },
    "open_feedback": "[FREE-TEXT]"
}
``` B PRISM Data Statement

We provide a data statement [69] to document the generation and provenance of PRISM.

B.1 Curation Rationale

The PRISM Alignment Project, funded by a variety of academic and industry sources (see Disclosure of Funding), aims to diversify human feedback datasets. All participants are recruited via the Prolific platform. The sample is described in § 2.3, with additional details in App. J. The primary purpose of the dataset is for academic research into how different people interact with LLMs and perceive their outputs. However, we do not prohibit the use of the dataset to develop, test and/or evaluate AI systems so long as usage complies with the dataset license (App. C.2).

B.2 Language Variety

The language of human- or model-written text was not explicitly restricted to English. However, the task instructions were written English, and fluency in English was included as a screening filter. As a result of these factors, 99% of text instances are in English (see App. E for breakdowns per type of text instance and by other language). There is scope for wide social and regional variation even within a language. Given we have speakers residing in 38 countries (born in 75 countries), we likely have various forms of English, especially by level of fluency (see Tab. 5). Information about which varieties of English are represented is not available.

B.3 Speaker Demographics

There are two sets of “speaker” roles in PRISM: human participants and large language models (LLMs). Both roles contribute to the characteristics of the text utterances in the dataset.

Participant Characteristics We provide full demographic breakdowns of participant characteristics in Tab. 5. We provide full geographic breakdowns in Tab. 8. Despite substantial improvements on sample diversity compared to early widely-used human feedback datasets (see Tab. 6, Tab. 7), PRISM still skews White, Educated, and Western. This is partly driven by census-representative samples from the US and UK, which can be removed or downsampled for future research. PRISM only contains participants sourced from one crowdworking platform (Prolific), so inherits sample biases from this narrow pool—for example, participants are active internet users, incentivised by hourly payment on a specific task that they self-select into.

Model Characteristics Given fast-paced changes to the LLM landscape, PRISM is designed to be as model-agnostic as possible. We include 21 models from various different families, capabilities and sizes (for a summary see Tab. 21). 12/21 models are accessed via commercial APIs, and 9/21 are open-access via HuggingFace. Model-specific characteristics will affect the text characteristics, especially if they have already been alignment-tuned.

Models as Participants Throughout the study we strongly requested that participants did not use LLMs to write their “human” responses, playing both to their integrity (please don’t do it), their role in the research (we really need you to not do it), and their incentives (you won’t be paid if you do it). We did not directly test nor implement tools to technologically prevent participants from using LLMs on their behalf. We randomly sample 25 instances from human-written texts: system strings and self-descriptions from the Survey; opening prompts and open feedback from the Conversations (n = 100). An annotator (paper author) manually inspected these and labelled none as model-written text. For instances of sufficient length (46/100, >50 words), we recorded the predicted probability of AI-generated text from an LLM-text detector, where 76% had ≤ 1% score. For the remainder (n = 11), a second annotator (paper author) gave a tie-break, labelling none as model-generated.

10The tool is developed by https://sapling.ai/. LLM-detector tools are susceptible to misclassifications. For example, this feedback: “It was good that it offered options and mentioned “options” rather than just suggesting one thing. It would have been better to state in the beginning how dietary requirements and preferences might play a big role in the decision what to cook for dinner. And also to point out how different cultures have different food traditions. Not everything is US based.” was flagged as 88.1% AI-generated, but the human annotators felt was strongly human-generated. B.4 Annotator Demographics

The “annotators” are “speakers”—the same human participants who answer the survey, interact with the LLMs, and provide structured and unstructured feedback. See App. B.3.

B.5 Speech Situation

All participants were recruited via Prolific. They were paid £9/hour. The survey was hosted on Qualtrics (www.qualtrics.com), and the conversations on Dynabench (www.dynabench.org).

All data was collected between 22nd November 2023 and 22nd December 2023. The time of the data collection period did affect the topics of discussion: for example, one topic concerns Christmas holiday celebrations while another discusses the Israel–Palestine Conflict.

The primary modality of PRISM is written language, combined with structured ratings or structured survey data. The conversations between participants and LLMs happened synchronously via live API connections with models in the backend of our interface. We have not edited or moderated any survey responses, participant prompts or model responses. All conversations happened as part of this research project, so the primary ‘intended audience’ was the researchers, though participants were informed of additional plans to distribute and release the data in the consent form (see App. D).

B.6 Text Characteristics

We summarise text characteristics in App. M. For the survey responses, the text provides details on the participant and their views about LLMs via short-form free-text responses (we requested 2-5 sentences in their own words). For the conversations, there are three different types: unguided, values guided and controversy guided, as described in the main paper (§ 2.2). Each conversation type contains a different distribution of topics. Overall, PRISM is skewed towards subjective, values-driven and controversial dialogue. The human-written texts within a conversation typically consist of single sentence prompts, on average 13 words long. Prompts receive up to four model responses generated by a variety of LLMs. We instruct the LLMs to limit their response to 50 words or less. Most unsuccessfully abide by this instruction: the average response length is 89 words. We release metadata (see App. E) with each text instance including information on detected language, automated and manual PII checks and moderation flags (e.g. if it contains sexual, hateful or violent content).

B.7 Recording Quality

During data collection, our interface experienced two distributed denial of service (DDoS) attacks: one on 28th November 2023 and another on 1st December 2023. The primary way that these attacks may have affected recording quality was via interrupting participants’ conversation sessions (most then later returned to the interface to complete their conversations a couple hours or days later). These participants’ data points may differ to those who had a smoother continuous experience in the task.

B.8 Author Characteristics and Positionality Statement

We aimed to operate in the subjective paradigm [15, 16] and have as little influence as possible on how participants interacted with models (e.g. no annotation guidelines for how to rate responses). As a team of researchers, we come from a variety of backgrounds (genders, ethnicities, countries of birth, native languages) and are involved with AI research, either in academia (6/12) or industry (6/12).

B.9 Expanded Ethical Considerations

Privacy and deanonymisation The conversations in PRISM are highly personal, for example detailing views towards abortion, religion, immigration, workplace disputes or intimate relationships. We have pseudo-anonymised the data, checked for PII (App. E), sought informed consent from every participant (App. D), provided options for participants to withdraw their data, and clearly stipulated that attempts of deanonymisation violate our dataset’s terms and conditions (App. C). However, despite following these best practices, the risk for deanonymisation remains. We include a reporting mechanism on our website and GitHub for any participants and researchers to report issues. Harmful and unsafe content  We asked participants to engage the LLMs in controversial conversations. This comes with the benefit of expanding human preference data to discursive areas with the greatest expected degree of interpersonal disagreement, but at the risk of encouraging hateful, bigoted, biased or otherwise harmful content. Harmful content is an issue in other human feedback datasets, where some opt to moderate conversations prior to public release [113] and others retain toxic content for the purpose of future research into conversational AI safety [112, 111]. Compared to these previous datasets, PRISM has an exceptionally low level of flagged content as measured via the OpenAI moderation API (0.06% overall, and < 0.003% for subcategories of sexually-explicit, violent, hateful, self-harm and harassment). However, the recall of this API may be low [111]; so, this could be an underestimate. From examining prompts closest to topic centroids (App. R.2), it is clear there are some prompts with potential for harm. We provide metadata for every text instance in PRISM, and opt to not filter any conversations. We believe it is a critical area of research to understand how state-of-the-art models respond when they are prompted to engage in such conversations, and how different people with diverse lived experiences react to safety interventions.

Participation-washing and intended societal impact  In our setting, we claim what Sloane et al. [55] calls participation as work, that is offering fair remuneration and attribution of the consensual labour of workers contributing to our project. Notably, many participants (those familiar and unfamiliar with AI) contacted the researchers and reported enjoying or learning from the task, suggesting there was an “education quotient” or role of participation as experience [53]. Compared to “passive” participation in annotation tasks or pre-training datasets [33], our process is more active for participants because it foregrounds the opportunity to provide their feedback, opinions and preferences, not just labels. “Participatory” also signals our goal to have communities more involved in alignment fine-tuning of models and see PRISM as a first step demonstrating this need. These aims evoke notions of participation as justice—including more people at the table of LLM design and development but we note that participation is in reality thin, because while we seek their view, we cannot grant participants the power to change behaviours of deployed LLMs [131]. Even the etymological roots of participation centre on the notion of “sharing” [53] but there is no guarantee that the human workers upon whom the success of RLHF relies on, partake in any share of the profits from more usable or preferred LLM technologies. We release PRISM in the hope it moves the needle towards more inclusive and diverse research on human-AI interactions, emphasising the central role of those who contribute their time and voice to generating human feedback data. Ultimately, how these contributions have impact depends on those in power (industry labs, academics, policymakers), because “the experience of participation must include the sense not only of having spoken, but of having been heard” [p.18, 53].

B.10  Expanded Technical and Task Design Limitations

The curse of dimensionality (or intersectionality)  Our findings suggest dialogue and model choice are driven somewhat by group affiliation and somewhat by idiosyncratic variance. However, PRISM contains a rich array of information on each participant with both structured and unstructured components. There are endless ways we could have divided the data or understood participant identity, and despite our best efforts to assess sensitivity to design choices, each alternative may have resulted in very different outcomes [120], and we are under-powered to test so many sparse combinations. Using less sparse groupings introduces biases—for example, focusing on region risks lumping together participants from particular geographies as “cultures” [82]. While we split out the UK and US to avoid these countries dominating their respective regions, there remain varying degrees of country-wise entropy in other regions—the Middle East has 94% individuals from Israel, and 100% of Non-US Northern Americans are Canadian (see App. H). Similarly, we use more aggregated ethnicity and religion groupings for statistical power, but amorphous and heterogeneous categories like “Other” have limited or flawed real-world meaning as “Other” contains, for example, both those who identify as Indigenous or First Peoples and as Middle Eastern or Arab. It is an exciting direction for future work to explore free-form characterisations of identity (e.g. the free-text profile or system string) or ex-post groupings of people’s preferences [9], and examine how findings change when we break away from neatly-observed but essentialising demographic traits [133].

The confounding effect of many moving cogs in a conversation  Beyond the complexities of intersectional identity and idiosyncratic variance of individuals within identity groups, other sources of variance in PRISM present a challenge for controlled experiments; particularly, the high-dimensionality of what exact topics each participant chooses to talk about, which models randomly get selected in-the-loop, and the stochasticity in their responses from a non-deterministic temperature. It is hard to pin down robust mechanisms of preference differences amongst individuals with so many sources of variation. We opted for choice of input prompt and conversation to be a free parameter in PRISM as a more naturalistic setting of LLM use and because we wanted to understand dialogue diversity among participants. We do empirically find some regions of fixed prompt-response pairs from individuals who self-select into asking the same prompts as other participants (see App. S.4).

**Noisy signals and misaligned incentives** Relatedly, our conclusions may be confounded by measurement invariance given our explicit focus on subjective, fluid and cardinal devices. This echoes the economist’s view, that it is foolish to rely too heavily on cardinal ratings over ordinal rankings to make interpersonal comparisons, or enforce preference construction, where intrinsic feelings are noisily-quantified on numeric scales. There are also issues of preference falsification: while participants are financially incentivised to participate, they may not honestly report their preferences over models. We cannot rule out the possibility that participants select a ‘bad’ model to lock in for the subsequent turns of conversation if it is more interesting (thus preferable in our narrow task confines) to talk to a more offensive or controversial model, or to try to ‘jailbreak it’ [112]. In hindsight, it may have been a smarter design choice to force participants to rank model responses, or to collect both ratings and rankings (notwithstanding decision fatigue), or make attempts to elicit more interpersonally comparable data via a willingness-to-pay monetary unit. Previous work also raises concerns over relying on human feedback as ‘gold standard’, for example whether participants can accurately rate factuality of an output, or are anchored on formatting and ‘first impressions’ (as we and Hosking et al. [134] both find). Preferences, especially at a fine-grained level like in PRISM, have high context-dependency [135], so we caution against taking the ratings as revealing some objective truth, instead staying firmly rooted in the subjective paradigm [15, 2].

**Still the “tyranny of the (English-speaking) crowdworker”** Much of AI, NLP and now RLHF is underpinned by crowdworker labour [136]. Despite our aims to include more diverse voices in LLM development processes, we avoid overstating claims on diversity. PRISM still only contains crowdworkers, who have significant sample biases [137]; can only be so “representative” given the relatively small sample sizes; must be digital natives given the platformed nature of the work; and possess different incentives for engagement [138]. Furthermore, while PRISM gains some dialectical diversity from different geographies of English, from varying speaker fluency, and from some contributions in other languages (1%, mainly Spanish), it is almost exclusively in English. Cultural diversity can only be measured so far without also accounting for linguistic diversity [61]. Furthermore, while we try to sample from many regions, our sample is still dominated by White Western participants, especially when considering cultural phylogeny [82], i.e., the non-independence of populations with shared history or migrations of peoples (for example, Australia vs UK vs Canada). We encourage future work prioritising human feedback collection in other languages to understand how models handle sociocultural and linguistic interactions [115].

**The ever-changing stream of pre-aligned models** When data collection began in mid-November, PRISM contained the top ranking models on publicly available leaderboards but new models have since emerged, including Gemini [139], Mixtral [140], Claude-3 [141], Command-R [142] and Llama-3 [143]. There is an incompatibility between the current pace of model releases and doing human participant research that requires lengthy processes of ethics approval, interface design, data processing and manual annotation. The expense and inconvenience of doing human research increases the attractiveness of simulating responses, usually with GPT-4 [108]. So, while PRISM does miss out on the newest players to enter the battle arena, we do provide carefully-sourced human data (including a survey which stands independently from the LLM conversations) combined with a wide distribution of model texts; so we hope the utility of the data persists in the coming years even as models change. We are still potentially limited when comparing open and closed-access models: while the former allows full transparency over system prompts, closed-access models can obscure additional instructions as hidden context. Including models from the same family allows comparisons by version or size, but introducing clones (models producing very similar outputs) can distort preference rankings [9]. PRISM is also limited by value-lock in [108]—the models are already tuned to cultural perspectives or alignment norms [34, 35], which precludes observing certain group preferences towards a wider set of behaviours [37, 144], and renders participants “thin” because they are “limited to existing designs with pre-existing purposes.” [p.3, 25]. C PRISM Data Clause

C.1 Terms of Use

Purpose The Dataset is provided for the purpose of research and educational use in the field of natural language processing, conversational agents, social science and related areas; and can be used to develop or evaluate artificial intelligence, including Large Language Models (LLMs).

Usage Restrictions Users of the Dataset should adhere to the terms of use for a specific model when using its generated responses. This includes respecting any limitations or use case prohibitions set forth by the original model’s creators or licensors.

Content Warning The Dataset contains raw conversations that may include content considered unsafe or offensive. Users must apply appropriate filtering and moderation measures when using this Dataset for training purposes to ensure the generated outputs align with ethical and safety standards.

No Endorsement of Content The conversations and data within this Dataset do not reflect the views or opinions of the Dataset creators, funders or any affiliated institutions. The dataset is provided as a neutral resource for research and should not be construed as endorsing any specific viewpoints.

No Deanonymisation The User agrees not to attempt to re-identify or de-anonymise any individuals or entities represented in the Dataset. This includes, but is not limited to, using any information within the Dataset or triangulating other data sources to infer personal identities or sensitive information.

Limitation of Liability The authors and funders of this Dataset will not be liable for any claims, damages, or other liabilities arising from the use of the dataset, including but not limited to the misuse, interpretation, or reliance on any data contained within.

C.2 Licence and Attribution

Human-written texts (including prompts) within the dataset are licensed under the Creative Commons Attribution 4.0 International License (CC-BY-4.0). Model responses are licensed under the Creative Commons Attribution-NonCommercial 4.0 International License (CC-BY-NC-4.0). Use of model responses must abide by the original model provider licenses.

For proper attribution when using this dataset in any publications or research outputs, please cite with the DOI.

Suggested Citation: Kirk, H. R., Whitefield, A., Röttger, P., Bean, A., Margatina, K., Ciro, J., Mosquera, R., Bartolo, M., Williams, A., He, H., Vidgen, B., & Hale, S. A. (2024). The PRISM Alignment Dataset. https://doi.org/10.57967/hf/2113

C.3 Dataset Maintenance

As the authors and maintainers of this dataset, we commit to no further updates to the dataset following its initial release. The dataset is self-contained and does not rely on external links or content, ensuring its stability and usability over time without the need for ongoing maintenance.

C.4 Data Rights Compliance and Issue Reporting

We are committed to complying with data protection rights, including but not limited to regulations such as the General Data Protection Regulation (GDPR). If any individual included in the dataset wishes to have their data removed, we provide a straightforward process for issue reporting and resolution on our Github. Concerned parties are encouraged to contact the authors directly via the provided contact form link on the Github. Upon receiving a request, we will engage with the individual to verify their identity and proceed to remove the relevant entries from the dataset. We commit to addressing and resolving such requests within 30 days of verification. D Informed Consent

This research was reviewed by, and received ethics clearance through, a subcommittee of the University of Oxford Central University Research Ethics Committee [OII_C1A_23_088]. The following text was displayed to all participants to collect informed consent.

Your Feedback on AI Language Models

We appreciate your interest in participating in this study. The aim of this research is to understand people’s preferences and perceptions regarding AI Language Model behaviours, also referred to as Large Language Models (LLMs), Generative AI Language Models, AI ChatBots or Virtual Assistants. AI language models are computer programs designed to generate text. They can respond to questions or prompts by producing written responses. We want to learn more about how people like you use and perceive these AI language models.

Please first make sure you are using a laptop or desktop computer, and you are not using a mobile device. Our task is NOT compatible with mobile devices.

Please then read through this information before agreeing to participate (if you wish to).

You may ask any questions before deciding to take part by contacting the research team. The Principal Researcher is Hannah Rose Kirk, and the Principle Investigator is Dr Scott. A. Hale, who are both affiliated with the Oxford Internet Institute at the University of Oxford.

What does the task involve?

If you decide to participate, there are two stages.

In this stage, you will be asked to fill in a short survey about yourself and your thoughts on AI language models.

In the next stage, you will have conversations with AI language models by providing prompts and rating their responses using a user-friendly interface. The prompts can be on various topics, and you don’t need any specific knowledge to participate. Your input will help us understand your preferences and opinions about how these AI language models work.

Both stages should take between 55-65 minutes. No background knowledge is required.

Please note that you will be interacting with an AI language model. The research team cannot directly control and are not responsible for the text generated by these models. There is a possibility that the models produce biased, inaccurate or harmful language. The risks to you as an individual are equivalent to those you would be exposed to if you use AI language models via interfaces like ChatGPT.

Do I have to take part?

No, participation is voluntary. If you do decide to take part, you may stop at any point for any reason before submitting your answers by closing the browser. However, we are only able to pay participants who complete the task. For demographic information, we have included a ‘Prefer not to say’ option for each set of questions should you prefer not to answer a particular question.

Can I withdraw my participation and data?

Yes, you may stop the study at any time. Please note that if you withdraw within a stage of the study you will not be paid for that stage or any subsequent incomplete stages, but you will be paid for any stages that you have already completed. You can withdraw your data from the study. The cut-off date for withdrawing your data is 14 days after you submitted the data. Please email members of the research team (see contact details below) within this 14-day window to withdraw your data from the study. How will my data be used?

The data collected from your participation will be pseudo-anonymized (stored with a unique numeric ID) and stored securely. It will be used for research purposes, and your personal information will remain confidential. The data will be analysed to gain insights into diverse preferences and perceptions regarding AI language model behaviours. At the end of the study, the pseudo-anonymised data collected will be released publicly for future research. The findings of this study may be published in academic journals or presented at conferences, and the results will be written up for a DPhil degree. Your individual identity will not be disclosed at any point in data release or publication. We do not collect any personal, private identifying information, IP addresses or contact details. The data we will collect that could identify you will be some demographic information (gender, age, nationality, religion, etc.), and short self-written survey answers.

The responses you provide will be stored in a password-protected electronic file on University of Oxford secure servers and may be used in academic publications, conference presentations or reports for external organisations. We will release a clean, PII-checked and pseudo-anonymised form of the data on an open-access, public data repository. Raw research data will be stored for 3 years after publication or public release of the research. We would like to use the data in future studies, and to share data with other researchers (e.g. in online databases). Data will have identifying information removed before it is shared with other researchers or results are made public. The data that we collect from you may be transferred to, stored and/ or processed at a destination outside the UK and the European Economic Area. By submitting your personal data, you agree to this transfer, storing or processing.

Who has reviewed this research?

This research has been reviewed by, and received ethics clearance through, a subcommittee of the University of Oxford Central University Research Ethics Committee [OII_C1A_23_088].

Who do I contact if I have a concern or I wish to complain?

If you have a concern about any aspect of this research, please speak to Hannah Rose Kirk (hannah.kirk@oii.ox.ac.uk) or their supervisor Dr. Scott A. Hale (scott.hale@oii.ox.ac.uk), and we will do our best to answer your query. We will acknowledge your concern within 10 working days and give you an indication of how it will be dealt with. If you remain unhappy or wish to make a formal complaint, please contact the Chair of the Research Ethics Committee at the University of Oxford who will seek to resolve the matter as soon as possible: Social Sciences & Humanities Interdivisional Research Ethics Committee; Email: ethics@socsci.ox.ac.uk; Address: Research Services, University of Oxford, Boundary Brook House, Churchill Drive, Headington, Oxford OX3 7GB.

Please note that you may only participate in this survey if you are 18 years of age or over.

☐ I certify that I am 18 years of age or over

If you have read the information above and agree to participate with the understanding that the data (including any personal data) you submit will be processed accordingly, please tick the box below to start.

☐ Yes, I agree to take part Table 1: Identifiers of text instance types in PRISM.

| Text Instance       | Study Stage | user interaction |
|---------------------|-------------|------------------|
| self_description    | Survey      | ✓                |
| system_string       | Survey      | ✓                |
| user_prompt         | Conversations | ✓ ✓ ✓ |
| model_response      | Conversations | ✓ ✓ ✓ |
| open_feedback       | Conversations | ✓                |

E Metadata Processing

For each text instance in PRISM, we attach three pieces of metadata: detected language flags, detected private or personally identifiable information (PII) flags, and detected moderation flags.

E.1 Structuring the Metadata

There are five types of text instances. Two appear in the survey (self_description, system_string) and have a 1:1 matching with each user (user_id). One appears at the conversation level (open_feedback) and has a 1:1 matching with each convo_id and a many:1 matching with each user_id because each participant has multiple conversations. Finally, the last two occur within each turn of a conversation, where for each single user_prompt there are multiple model responses (model_response). We structure the metadata so it can be merged uniquely, without duplication. We release one file, where each text instance is tied to its metadata via the identifying information shown in Tab. 1 and a column_id for matching whether the text is [system_string, self_description, user_prompt, model_response, open_feedback].

E.2 Automated Flagging

PII To identify whether a textual instance in our dataset contains personal and identifiable information (PII) we used the package scrubadub. Specifically we used the function scrubadub.clean(text) which replaces the phone numbers and email addresses with anonymous IDs, if they are found in the input. We flag with 1 instances that are altered (i.e., PII was identified) and 0 those that remained unchanged.

Moderation To measure content moderation we use the OpenAI Moderation endpoint. The API takes as an input a textual instance and outputs a json file with an overall boolean flag (flagged) whether there input potentially harmful (True), otherwise False. The API also returns a flag for a list of specific moderation categories that can be used to further filter and inspect the data. The categories are sexual, hate, harassment, self-harm, sexual/minors, hate/threatening, violence/graphic, self-harm/intent, self-harm/instructions, harassment/threatening and violence. Similar to the overall flag, for each category, the value is True if the model flags the corresponding category as violated, False otherwise. Finally, the API returns a dictionary of per-category scores that denote the model’s confidence that the input violates the OpenAI’s policy for the category. The value is between 0 and 1, where higher values denote higher confidence.

Language Detection To detect the language of each text instance in our dataset we used the LangID codebase. LangID is a popular python package that efficiently detects the language of an input and currently supports 97 languages. Specifically, we use the langid.classify(text) function and store a string for the detected language.

11 https://scrubadub.readthedocs.io/en/stable/
12 https://platform.openai.com/docs/guides/moderation
13 https://github.com/saffsd/langid.py E.3 Manual Review

The overall proportions of texts flagged for PII, Non-English or Moderation is low (see Tab. 2). However, when inspecting the few positive flags, many were false positives, especially on lang-detect and PII. While a false positive on language may be relatively inconsequential, any automated flags of PII are concerning. Accordingly, we manually annotate any instances where \( \text{pii\_flag}==\text{True} \) \((n = 167)\) for participant-written text. We find that none of them actually contain PII.

Table 2: Meta-Data Summary. For each metadata category (language, PII and moderation), we show \( N(\%) \) for the dataset as a whole (Overall) and broken down by each type of text instance in PRISM.

| Category          | Is English | Contains PII | Manually-Checked PII | Is Moderation Flagged | Total Instances |
|-------------------|------------|--------------|----------------------|-----------------------|-----------------|
| Overall           | 105,229 (98.8%) | 1,111 (1.0%) | NA                   | 634 (0.6%)            | 106,554 (100.0%) |
| user_prompt       | 26,545 (97.7%) | 66 (0.2%)    | 0 (0.0%)             | 454 (1.7%)            | 27,172 (100.0%) |
| model_response    | 67,715 (99.0%) | 944 (1.4%)   | NA                   | 162 (0.2%)            | 68,371 (100.0%) |
| self_description  | 1,496 (99.7%) | 10 (0.7%)    | 0 (0.0%)             | 7 (0.5%)              | 1,500 (100.0%)  |
| system_string     | 1,493 (99.5%) | 16 (1.1%)    | 0 (0.0%)             | 0 (0.0%)              | 1,500 (100.0%)  |
| open_feedback     | 7,980 (99.6%) | 75 (0.9%)    | 0 (0.0%)             | 11 (0.1%)             | 8,011 (100.0%)  |

Table 3: Breakdown of flags from the OpenAI Moderation API. We show counts and percentages where the text was flagged (==True), as well as total counts. Human-written text includes user_prompt, self_description, system_string, open_feedback; Model-written text is only model_response.

| Category          | Human-written | Model-written |
|-------------------|---------------|---------------|
|                   | N (%)         | N (%)         |
| sexual            | 21 0.05%      | 11 0.02%      |
| hate              | 154 0.40%     | 36 0.05%      |
| harassment        | 387 1.01%     | 127 0.19%     |
| self-harm         | 24 0.06%      | 8 0.01%       |
| sexual/minors     | 4 0.01%       | 4 0.01%       |
| hate/threatening  | 17 0.04%      | 2 0.00%       |
| self-harm/intent  | 26 0.07%      | 5 0.01%       |
| self-harm/instructions | 13 0.03% | 8 0.01% |
| harassment/threatening | 33 0.09% | 7 0.01% |
| violence          | 52 0.14%      | 13 0.02%      |
| Total             | 38,183 100.00%| 68,371 100.00%|

Table 4: Breakdown of languages detected by LangID. We show the top-10 detected languages, then other and total counts. Human-written text includes user_prompt, self_description, system_string, open_feedback; Model-written text is only model_response.

| Language | Human-written | Model-written |
|----------|---------------|---------------|
|          | N (%)         | N (%)         |
| 1 en     | 37,514 98.25% | 67,715 99.04% |
| 2 es     | 175 0.46%     | 236 0.35%     |
| 3 fr     | 71 0.19%      | 67 0.10%      |
| 4 it     | 70 0.18%      | 63 0.09%      |
| 5 de     | 60 0.16%      | 43 0.06%      |
| 6 nl     | 42 0.11%      | 37 0.05%      |
| 7 pt     | 41 0.11%      | 29 0.04%      |
| 8 pl     | 34 0.09%      | 19 0.03%      |
| 9 da     | 24 0.06%      | 15 0.02%      |
| 10 ro    | 13 0.03%      | 14 0.02%      |
| Other    | 139 0.36%     | 133 0.19%     |
| Total    | 38,183 100.00%| 68,371 100.00%|

\(^{14}\)If any of these human-written prompts had been a true positive, we would have manually checked the associated model responses too. F  Annotating Ethnicity, Religion and Gender

We ask people to describe their ethnic and religious affiliations in their own words because for a global survey, there are no immediately obvious preset categories. In the survey data, we release this original self-description \( \{ \text{ethnicity, religion} \}_\text{self_described} \). However, there are 264 unique strings for ethnicity, and 137 unique strings for religion. For some analysis, it is valuable to have aggregate groupings. To attain this grouping, we first used gpt-4-turbo to categorise the strings, but found some errors and essentialising generalisations, for example, if someone answered with a nationality not an ethnic group like \textit{american}, gpt would return \textit{white} \(^{15}\).

Accordingly, we used a second round of manual human annotation to verify these automated labels. Two annotators (authors of the paper) first made independent judgements then discussed any disagreements. For ethnicity, some participants also had answered a Prolific screening question on their simplified ethnicity, though we did not have this information for all participants as it was not mandatory. We thus annotate all unique combinations of the self-described string, and the Prolific ethnicity information \((n = 343)\). In ambiguous cases (e.g. the aforementioned \textit{american} response), we relied on this additional ethnicity information, and in its absence, defaulted to a \textit{Prefer not to say} response. For religion, we do not have any additional information provided by the Prolific pre-screening questionnaire, so verification decisions were made on the basis of the self-describe string alone. The annotators agreed on 94\% of ethnicity cases (discussing and resolving the remaining 20); and 96\% of religion cases (discussing and resolving the remaining 5).

We highlight two general findings from our disagreements which may be of interest to people analysing or categorising our data in the future. Firstly, \textbf{ethnicity and nationality are complex}. Take for example the UK census, where \textit{Chinese, Banglaeshi, Indian} and \textit{Pakistani} are all listed as sub-categories of the Asian ethnic group \(^{16}\). Ethnicity is a multi-faceted term which can include nationality, language group, skin colour, religion, among other characteristics \(^{145}\). Studies have shown that survey participants can interpret the term ethnic group through a variety of subjective lens \(^{146, 147}\). During annotation, we tried to gather information on whether group terms commonly refer to an ethnic group, but some subjectivity and naivety are inevitable; so, we encourage future researchers to carefully consider their own categorisations depending on the question at hand. Secondly, the \textbf{belonging and believing aspects of religion intersect} \(^{148, 149}\), and it is not immediately clear how to categorise an individual that culturally affiliates with religion but simultaneously identities as an atheist or non-believer. Studies have revealed that the belonging and believing axis of religion are important for conditioning behaviours such as trust, pro-sociality and altruism \(^{150–152}\). In general, we annotated a mention of a religion as assigned to that religion (not distinguishing between the belonging and believing channels) but it remains to be seen whether one axis is more salient for values and opinions towards AI systems.

Note for gender, we provided a standard multiple choice question with options: \textit{Female, Male, Non-binary / third gender, Prefer not to say} and \textit{Prefer to self-describe}. Only 3 individuals opted to self-describe, which we then annotated and only assimilated in very clear cut cases \(^{17}\) else we grouped it as \textit{Prefer not to say} to avoid over-riding a participant’s self-identification.

G  Participant Demographics

We present full demographic breakdowns in \textbf{Tab. 5}. We also compare the breakdowns in PRISM to some early human feedback datasets which provide demographic information \textbf{(Tab. 6)}.

\(^{15}\) As an aside, these types of baked-in priors are a good example of why using LLMs as a surrogate for human annotators may introduce downstream biases \(^{108}\).

\(^{16}\) See the fact sheet at \url{ethnicity-facts-figures.service.gov.uk}.

\(^{17}\) For example, one participant responded with “i dont expect this wokery from intelligent people. you want to know which of the 2 possible genders i am male.”, which we assign as \textit{Male}. 

38 Table 5: **Full Demographics Breakdowns.** Counts and percentages of participants by standard demographic variables. Overall, PRISM utilises a large and demographically-diverse sample, especially compared to some previous human feedback datasets (see Tab. 6); but it still generally skews towards young, white and educated populations. *For ethnicity and religion, see details in App. F.*

| Total Participants | 1,500 | 100 |
|--------------------|-------|-----|
| With conversations  | 1,396 | 93.1 % |
| Just survey        | 104   | 6.9 % |

**Age**

| Age                  | Count | Percentage |
|----------------------|-------|------------|
| 25-34 years old      | 454   | 30.3 %     |
| 18-24 years old      | 297   | 19.8 %     |
| 35-44 years old      | 237   | 15.8 %     |
| 45-54 years old      | 208   | 13.9 %     |
| 55-64 years old      | 197   | 13.1 %     |
| 65+ years old        | 106   | 7.1 %      |
| Prefer not to say    | 1     | 0.1 %      |

**Gender**

| Gender                  | Count | Percentage |
|-------------------------|-------|------------|
| Male                    | 757   | 50.5 %     |
| Female                  | 718   | 47.9 %     |
| Non-binary / third gender | 21   | 1.4 %      |
| Prefer not to say       | 4     | 0.3 %      |

**Self-Reported Ethnicity***

| Ethnicity                  | Count | Percentage |
|----------------------------|-------|------------|
| White                      | 969   | 64.6 %     |
| Black / African            | 122   | 8.1 %      |
| Hispanic / Latino          | 121   | 8.1 %      |
| Asian                      | 95    | 6.3 %      |
| Mixed                      | 68    | 4.5 %      |
| Middle Eastern / Arab      | 14    | 0.9 %      |
| Indigenous / First Peoples | 8     | 0.5 %      |
| Other                      | 17    | 1.1 %      |
| Prefer not to say          | 86    | 5.7 %      |

**Self-Reported Religion***

| Religion                  | Count | Percentage |
|----------------------------|-------|------------|
| Non-religious              | 762   | 50.8 %     |
| Christian                  | 487   | 32.5 %     |
| Agnostic                   | 71    | 4.7 %      |
| Jewish                     | 42    | 2.8 %      |
| Muslim                     | 31    | 2.1 %      |
| Spiritual                  | 18    | 1.2 %      |
| Buddhist                   | 12    | 0.8 %      |
| Folk religion              | 6     | 0.4 %      |
| Hindu                      | 5     | 0.3 %      |
| Sikh                       | 3     | 0.2 %      |
| Other                      | 4     | 0.3 %      |
| Prefer not to say          | 59    | 3.9 %      |

**Employment Status**

| Employment Status                  | Count | Percentage |
|-------------------------------------|-------|------------|
| Working full-time                   | 712   | 47.5 %     |
| Working part-time                   | 265   | 17.7 %     |
| Student                             | 191   | 12.7 %     |
| Unemployed, seeking work            | 113   | 7.5 %      |
| Retired                             | 104   | 6.9 %      |
| Homemaker / Stay-at-home parent     | 46    | 3.1 %      |
| Unemployed, not seeking work        | 46    | 3.1 %      |
| Prefer not to say                   | 23    | 1.5 %      |

Continued on next page Table 5: **Full Demographics Breakdowns.** Counts and percentages of participants by standard demographic variables. Overall, PRISM utilises a large and demographically-diverse sample, especially compared to some previous human feedback datasets (see Tab. 6); but it still generally skews towards young, white and educated populations. *For ethnicity and religion, see details in App. F*

| Education                        | Count | Percentage |
|----------------------------------|-------|------------|
| University Bachelors Degree      | 637   | 42.5 %     |
| Graduate / Professional degree   | 241   | 16.1 %     |
| Some University but no degree    | 236   | 15.7 %     |
| Completed Secondary School       | 209   | 13.9 %     |
| Vocational                      | 125   | 8.3 %      |
| Some Secondary                  | 24    | 1.6 %      |
| Completed Primary School        | 16    | 1.1 %      |
| Some Primary                    | 3     | 0.2 %      |
| Prefer not to say               | 9     | 0.6 %      |

| Martial Status                  | Count | Percentage |
|----------------------------------|-------|------------|
| Never been married              | 870   | 58.0 %     |
| Married                         | 463   | 30.9 %     |
| Divorced / Separated            | 123   | 8.2 %      |
| Widowed                         | 21    | 1.4 %      |
| Prefer not to say               | 23    | 1.5 %      |

| English Proficiency             | Count | Percentage |
|---------------------------------|-------|------------|
| Native speaker                  | 886   | 59.1 %     |
| Fluent                          | 405   | 27.0 %     |
| Advanced                        | 160   | 10.7 %     |
| Intermediate                    | 42    | 2.8 %      |
| Basic                           | 7     | 0.5 %      |

| Regions                         | Count | Percentage |
|---------------------------------|-------|------------|
| US                              | 338   | 22.5 %     |
| Europe                          | 313   | 20.9 %     |
| UK                              | 292   | 19.5 %     |
| Latin America and the Caribbean | 146   | 9.7 %      |
| Australia and New Zealand       | 129   | 8.6 %      |
| Africa                          | 118   | 7.9 %      |
| Asia                            | 60    | 4.0 %      |
| Northern America                | 50    | 3.3 %      |
| Middle East                     | 50    | 3.3 %      |
| Oceania                         | 1     | 0.1 %      |
| Prefer not to say               | 3     | 0.2 %      | Table 6: **Demographic data compared to prior work.** Comparisons of PRISM to early and widely-known RLHF studies using human feedback for language models. See §4 for more current datasets.

| Category                  | Bai et al. | Ouyang et al. | Glaese et al. | Ganguli et al. | Stiennon et al. | Ours |
|---------------------------|------------|---------------|---------------|----------------|-----------------|------|
| Total Participants        | 28‡        | 40            | 324           | 1,500          |                 |      |
| Demographic Respondents   | 28         | 19            | 533           | 115            | 21              | 1,500|

**Gender**

| Category       | Bai et al. | Ouyang et al. | Glaese et al. | Ganguli et al. | Stiennon et al. | Ours |
|----------------|------------|---------------|---------------|----------------|-----------------|------|
| Male           | 53.6 %     | 47.4 %        | 45.0 %        | 47.0 %         | 38.1 %          | 50.5 %|
| Female         | 46.4 %     | 42.1 %        | 54.0 %        | 52.2 %         | 61.9 %          | 47.9 %|
| Non-binary     | 0.0 %      | 5.3 %         | 1.0 %         | 0.9 %          | 0.0 %           | 1.4 % |
| Prefer not to say/Other | 0.0 % | 5.3 % | 0.0 % | 0.0 % | 0.0 % | 0.3 % |

**Sexual Orientation**

| Category       | Bai et al. | Ouyang et al. | Glaese et al. | Ganguli et al. | Stiennon et al. | Ours |
|----------------|------------|---------------|---------------|----------------|-----------------|------|
| Heterosexual   | 89.3 %     | 84.0 %        | 81.7 %        | -              | -               | -    |
| Lesbian or Gay | 7.1 %      | -             | 5.0 %         | 4.3 %          | -               | -    |
| Bisexual       | 0.0 %      | -             | 9.0 %         | 12.2 %         | -               | -    |
| Uncertain      | 3.6 %      | -             | -             | 0.9 %          | -               | -    |
| Prefer not to say/Other | 0.0 % | - | 2.0 % | 0.9 % | - | - |

**Age**

| Category       | Bai et al. | Ouyang et al. | Glaese et al. | Ganguli et al. | Stiennon et al. | Ours |
|----------------|------------|---------------|---------------|----------------|-----------------|------|
| 18-24          | 7.1 %      | 26.3 %        | 11.0 %        | 0.0 %          | -               | 19.8 %|
| 25-34          | 39.3 %     | 47.4 %        | 37.0 %        | 25.2 %         | 42.9 %          | 30.3 %|
| 35-44          | 42.9 %     | 10.5 %        | 24.0 %        | 33.9 %         | 23.8 %          | 15.8 %|
| 45-54          | 10.7 %     | 10.5 %        | 16.0 %        | 23.5 %         | 23.8 %          | 13.9 %|
| 55-64          | 0.0 %      | 5.3 %         | 9.0 %         | 13.9 %         | 9.5 %           | 13.1 %|
| 65+            | 0.0 %      | 0.0 %         | 3.0 %         | 1.7 %          | 0.0 %           | 7.1 % |
| Prefer not to say | 0.0 % | - | - | 1.7 % | - | 0.1 % |

**Ethnicity**

| Category                  | Bai et al. | Ouyang et al. | Glaese et al. | Ganguli et al. | Stiennon et al. | Ours |
|---------------------------|------------|---------------|---------------|----------------|-----------------|------|
| White/Caucasian           | 67.9 %     | 31.6 %        | 81.0 %        | 81.7 %         | 42.9 %          | 64.6 %|
| Asian                     | 10.7 %     | 57.9 %        | 8.0 %         | 2.6 %          | 28.6 %          | 6.3 % |
| Black/African descent     | 3.6 %      | 10.5 %        | 4.0 %         | 8.7 %          | -               | 8.1 % |
| Hispanic/Latino           | 3.6 %      | 15.8 %        | 1.0 %         | 0.9 %          | 4.8 %           | 8.1 % |
| Native American           | 0.0 %      | 0.0 %         | 0.0 %         | 2.6 %          | 9.6 %           | 0.5 % |
| Middle Eastern            | 0.0 %      | 0.0 %         | 1.0 %         | 0.9 %          | 4.8 %           | 0.9 % |
| Prefer not to say/Other   | 14.3 %     | -             | 5.0 %         | 2.6 %          | 9.6 %           | 11.5 %|

**Education**

| Category                  | Bai et al. | Ouyang et al. | Glaese et al. | Ganguli et al. | Stiennon et al. | Ours |
|---------------------------|------------|---------------|---------------|----------------|-----------------|------|
| No University Degree      | 17.9 %     | 10.5 %        | 0.0 %         | 34.8 %         | 14.3 %          | 40.8 %|
| Undergraduate Degree      | 57.1 %     | 52.6 %        | 66.0 %        | 53.9 %         | 57.1 %          | 42.5 %|
| Graduate Degree           | 14.3 %     | 36.8 %        | 34.0 %        | 10.4 %         | 28.1 %          | 16.1 %|
| Prefer not to say/Other   | 10.7 %     | -             | -             | 0.9 %          | -               | 0.6 % |

†Age group values for Stiennon et al. are reported for ten-year age groups starting from 20-29. We have placed the values in the row where the top end of these groups would appear to align with groups reported by the majority of studies.

‡Bai et al. provide two reports of demographic data. We use the one corresponding to the participants who contributed more than 80% of the total feedback.

Table 7: **Geographic data compared to prior work.** Participant countries of residence in PRISM compared to early and widely-known RLHF studies using human feedback for language models.

| Category       | Bai et al. | Ouyang et al. | Glaese et al. | Ganguli et al. | Stiennon et al. | Ours |
|----------------|------------|---------------|---------------|----------------|-----------------|------|
| United States  | 100 %      | 17 %          | 0 %           | 100 %          | 60 %            | 26 % |
| United Kingdom | 0 %        | 0 %           | 100 %         | 0 %            | 7 %             | 23 % |
| Philippines    | 0 %        | 22 %          | 0 %           | 0 %            | 7 %             | 0 %  |
| Bangladesh     | 0 %        | 22 %          | 0 %           | 0 %            | 0 %             | 0 %  |
| All Others     | 0 %        | 39 %‡         | 0 %           | 0 %            | 27 %‡           | 51 %*|

†One resident each from Albania, Brazil, Canada, Columbia, India, Uruguay, and Zimbabwe
‡One resident each from South Africa, Serbia, Turkey, India
*See Tab. 8 for our breakdowns. H Participant Geographies

We present full geographic breakdowns in Tab. 8. Fig. 6 is an enlarged version from Fig. 1. We compare geographic data to prior work in Tab. 7. For regional classifications, we use the UN definitions. We also throughout the main paper use location_special_region, which splits out the UK and the US. Regional breakdowns by birth country are shown in Fig. 7.

Figure 6: Geographic distribution of PRISM participants by birth country. Our sampling aims were for breath (coverage across UN global regions) and depth (representative demographic coverage within UK and US samples).

Table 8: Full Geographic Breakdowns. We collect country of birth and current country of residence. PRISM contains participants born in 75 different countries, residing in 38 different countries.

| Country of Birth | Country of Residence |
|------------------|----------------------|
| United States    | 338                  | 22.5 %               |
| United Kingdom   | 292                  | 19.5 %               |
| South Africa     | 91                   | 6.1 %                |
| Mexico           | 69                   | 4.6 %                |
| Australia        | 65                   | 4.3 %                |
| New Zealand      | 64                   | 4.3 %                |
| Chile            | 63                   | 4.2 %                |
| Canada           | 50                   | 3.3 %                |
| Israel           | 47                   | 3.1 %                |
| Nigeria          | 19                   | 1.3 %                |
| Spain            | 19                   | 1.3 %                |
| Germany          | 17                   | 1.1 %                |
| Belgium          | 17                   | 1.1 %                |
| Poland           | 17                   | 1.1 %                |
| Ireland          | 17                   | 1.1 %                |
| Latvia           | 16                   | 1.1 %                |
| Denmark          | 15                   | 1.0 %                |
| Czechia          | 15                   | 1.0 %                |
| Norway           | 15                   | 1.0 %                |
| France           | 14                   | 0.9 %                |
| Italy            | 14                   | 0.9 %                |
| Greece           | 14                   | 0.9 %                |
| Switzerland      | 14                   | 0.9 %                |

Continued on next page

18 https://population.un.org/wpp/DefinitionOfRegions Table 8: **Full Geographic Breakdowns.** We collect country of birth and current country of residence. PRISM contains participants born in 75 different countries, residing in 38 different countries.

| Country of Birth          | Country of Residence |
|---------------------------|----------------------|
| Finland                   | 12 0.8 %             |
| Estonia                   | 11 0.7 %             |
| Austria                   | 11 0.7 %             |
| Slovenia                  | 10 0.7 %             |
| Netherlands               | 9 0.6 %              |
| India                     | 9 0.6 %              |
| Japan                     | 9 0.6 %              |
| Korea, Republic of        | 9 0.6 %              |
| Portugal                  | 8 0.5 %              |
| Romania                   | 7 0.5 %              |
| Philippines               | 7 0.5 %              |
| Sweden                    | 7 0.5 %              |
| Russian Federation        | 6 0.4 %              |
| Ukraine                   | 4 0.3 %              |
| Bangladesh                | 4 0.3 %              |
| China                     | 4 0.3 %              |
| Hong Kong                 | 3 0.2 %              |
| Pakistan                  | 3 0.2 %              |
| Ghana                     | 3 0.2 %              |
| Dominican Republic        | 3 0.2 %              |
| Venezuela, Bolivarian Republic of | 3 0.2 % |
| Indonesia                 | 3 0.2 %              |
| Viet Nam                  | 2 0.1 %              |
| Sri Lanka                 | 2 0.1 %              |
| Turkey                    | 2 0.1 %              |
| Argentina                 | 2 0.1 %              |
| Kazakhstan                | 2 0.1 %              |
| Slovakia                  | 1 0.1 %              |
| Sudan                     | 1 0.1 %              |
| Tonga                     | 1 0.1 %              |
| Afghanistan               | 1 0.1 %              |
| Nepal                     | 1 0.1 %              |
| Honduras                  | 1 0.1 %              |
| Belarus                   | 1 0.1 %              |
| Bosnia and Herzegovina    | 1 0.1 %              |
| Brazil                    | 1 0.1 %              |
| Bulgaria                  | 1 0.1 %              |
| Colombia                  | 1 0.1 %              |
| Cuba                      | 1 0.1 %              |
| Côte d’Ivoire             | 1 0.1 %              |
| Malaysia                  | 1 0.1 %              |
| Guyana                    | 1 0.1 %              |
| Iceland                   | 1 0.1 %              |
| Jamaica                   | 1 0.1 %              |
| Kenya                     | 1 0.1 %              |
| Kuwait                    | 1 0.1 %              |
| Lithuania                 | 1 0.1 %              |
| Luxembourg                | 1 0.1 %              |
| Malawi                    | 1 0.1 %              |
| Zambia                    | 1 0.1 %              |
| Tanzania, United Republic of | 0 0.0 %          |
| Lesotho                   | 0 0.0 %              |
| Uruguay                   | 0 0.0 %              |
| Prefer not to say         | 3 0.2 %              |

43 Figure 7: **Skewed regional entropy in PRISM.** The hierarchical tree diagram uses participant birth location, mapping (i) special location (splitting out the US and UK), which is used in the main paper, (ii) UN-defined subregions, and (iii) ISO country codes. There is an over-representation of UK and US participants due to the census samples. In most regions besides Europe, participation is dominated by one or two birth countries. The two small vertical boxes are Prefer not to say (in red), and Oceania (in navy). *Note:* 88% of PRISM participants are born and currently reside in the same country. I Participant LLM Usage and Familiarity

We present breakdowns on experience with LLMs in Tab. 9. We did not require participants to be familiar with LLMs so provide the following primer:

This research is about Artificial Intelligence (AI) Language Models. These models are also sometimes referred to as Generative AI, Large Language Models (LLMs), Conversational Agents, AI Chat Bots or Virtual Assistants.

They are advanced computer programs that can understand and generate human-like text. These models learn from large amounts of text data on the internet to generate their responses.

One example you might have heard is ChatGPT, where people can have a conversation with an AI language model via an internet website.

Table 9: Survey of Participants’ LLM Usage: The majority of participants have used LLMs directly (via a dedicated chat interface) and indirectly (embedded in products or services). Note only participants who answered Yes to LLM Direct Use or LLM Indirect Use (n = 1253, 84%) are shown LLM Freq of Use and LLM Use Cases. For Use Cases, we show the % of these participants who selected each use case (can be multiple so \( \sum \neq 1 \)). Exact question phrasing is reported in the survey codebook (App. V.1).

| LLM Direct Use | 1,162 | 77.5 % |
|----------------|-------|--------|
| No             | 259   | 17.3 % |
| Unsure         | 79    | 5.3 %  |

| LLM Indirect Use | 1,104 | 73.6 % |
|------------------|-------|--------|
| No               | 215   | 14.3 % |
| Unsure           | 181   | 12.1 % |

| LLM Familiarity | 920   | 61.3 % |
|-----------------|-------|--------|
| Very familiar   | 424   | 28.3 % |
| Not familiar at all | 156 | 10.4 % |

| LLM Frequency of Use | 374 | 24.9 % |
|----------------------|-----|--------|
| Every week           | 316 | 21.1 % |
| More than once a month | 291 | 19.4 % |
| Less than one a year | 162 | 10.8 % |
| Every day            | 110 | 7.3 %  |
| Not shown question   | 247 | 16.5 % |

| LLM Use Cases | 617 | 49.2 % |
|---------------|-----|--------|
| Research      | 469 | 37.4 % |
| Professional Work | 392 | 31.3 % |
| Technical or Programming Help | 337 | 26.9 % |
| Lifestyle and Hobbies | 310 | 24.7 % |
| Homework Assistance | 286 | 22.8 % |
| Personal Recommendations | 266 | 21.2 % |

Continued on next page Table 9: **Survey of Participants’ LLM Usage**: The majority of participants have used LLMs directly (via a dedicated chat interface) and indirectly (embedded in products or services). Note only participants who answered *Yes* to **LLM Direct Use** or **LLM Indirect Use** (*n* = 1253, 84%) are shown **LLM Freq of Use** and **LLM Use Cases**. For Use Cases, we show the % of these participants who selected each use case (can be multiple so $\sum \neq 1$). Exact question phrasing is reported in the survey codebook (App. V.1).

| Use Case                          | Frequency | Percentage |
|----------------------------------|-----------|------------|
| **Casual Conversation**          | 262       | 20.9 %     |
| **Language Learning**            | 229       | 18.3 %     |
| **Source Suggestions**           | 217       | 17.3 %     |
| **Daily Productivity**           | 216       | 17.2 %     |
| **Historical or News Insight**   | 183       | 14.6 %     |
| **Well-being Guidance**          | 159       | 12.7 %     |
| **Games**                        | 143       | 11.4 %     |
| **Travel Guidance**              | 133       | 10.6 %     |
| **Medical Guidance**             | 130       | 10.4 %     |
| **Financial Guidance**           | 107       | 8.5 %      |
| **Relationship Advice**          | 98        | 7.8 %      |
| **Other**                        | 124       | 9.9 %      |

I.1 **Other Identified Usecases**

In addition to the usecases in Tab. 9, 122 participants used the “Other” option to add a usecase in their own words. Many of these just add more specific details to the pre-provided categories. In addition, there were a few interesting themes:

- **Customer Service**: Many of the participants noted having interacted with LLMs in customer support chats, often with negative sentiment (“Usually forced to interact with chatbots to get something done”, “Customer service bots I cannot avoid”, “Insurance companies direct you to chatbots, usually useless”).

- **Prolific and Other Online Surveys**: One of the more common (and potentially concerning) answers mentioned research participation e.g. “Studies like this one”, “Doing Prolific tests”, but it may be that they mean AI is the subject of the study: “AI research subject on research platforms Prolific, others.” or “It’s sometimes required as part of a survey on Prolific.” We encourage future work on whether there is noticeable difference in these participants’ answers elsewhere in our task.

- **AI Understanding or Testing**: A few participants mentioned “Trying to gain an understanding into AI and its capabilities” or “Gauging progress/viability of AI models”. Many others indicated curiosity or exploratory use e.g. “Just to test it out and see what it’s all about” or “Casual interest in the new technology”.

- **Professional or Job Tasks**: Participants added details on professional usecases like resume help, interview prep, CV writing, HR-tasks, Excel help, or emails.

- **Creative (Multimodal) Use-cases**: Participants gave additional detail like writing YouTube scripts, generating gift card text or designing characters for games as well as multimodal creative outputs like generating drawings or images.

- **Domain-Specific Usecases**: Medical, Financial and Educational usecases are all mentioned. J Screening and Recruitment Process

We recruit workers via Prolific (https://www.prolific.com/). We apply two initial screening criteria: (i) participants must be fluent in English because PRISM targets monolingual models and language data, and (ii) participants must have been born and reside in the same country to avoid biasing our sample towards expats living abroad. There is a skewed country-wise distribution of active workers who meet this criteria (see Tab. 10). For example, of the 21,084 workers in Europe (passing screening), 17% are Portuguese, 15% German and 14% Polish; and all 6,584 workers in Africa are located in South Africa. To account for this, we set up country-specific studies in each country with at least one eligible worker, balance study spots across regions, and ensure no single country has more than 100 open spots (apart from the Rep Samples in the UK and US). We collected information on country of birth and country of current residence during our survey (separate to workers’ stored Prolific details), and find that 179 participant (12%) have different birth and reside countries. We do not exclude these individuals from our sample.

Table 10: Summary of Recruitment Studies We present study-wise breakdowns ($n = 33$). Each study was created based on the constraints of Prolific’s pool of workers. We show here all the countries with at least 1 fluent English speaker, and the counts for fluent English Speakers who were born and currently reside in that same country. We show the whether each study was screened for a special representative sample (Rep Sample) or if it was balanced on participant gender (Gender Bal). In some cases, there were too few active participants per country to balance by gender without comprising participant privacy. We also show when the first batch was launched (all dates are in 2023) and approximate cost (at £9 per hour per participant).

| Study       | Rep Sample | Gender Launched (2023) | Approved Submissions | Prolific Fluent English Speakers (All) | Prolific Fluent English Speakers (Born=Reside) | Cost (£) |
|-------------|------------|------------------------|----------------------|----------------------------------------|-----------------------------------------------|---------|
| Total       | ✓          | ✓                      | 25                   | 1,500                                  | 111,572                                       | 100,585 | 14,850.00 |
| US          | ✓          | ✓                      | 27-11                | 386                                    | 38,114                                        | 36,205  | 36.0 %    | 3,821.40 |
| UK          | ✓          | ✓                      | 27-11                | 341                                    | 37,408                                        | 33,678  | 33.5 %    | 3,375.90 |
| South Africa| ✓          | ✓                      | 22-11                | 88                                     | 7,061                                         | 6,584   | 6.5 %     | 871.20   |
| New Zealand | ✓          | ✓                      | 24-11                | 77                                     | 511                                           | 389     | 0.4 %     | 762.30   |
| Australia   | ✓          | ✓                      | 24-11                | 71                                     | 1,968                                         | 1,550   | 1.5 %     | 702.90   |
| Mexico      | ✓          | ✓                      | 24-11                | 69                                     | 2,021                                         | 1,943   | 1.9 %     | 683.10   |
| Chile       | ✓          | ✓                      | 23-11                | 65                                     | 455                                           | 416     | 0.4 %     | 643.50   |
| Israel      | ✓          | ✓                      | 25-11                | 61                                     | 310                                           | 272     | 0.3 %     | 603.90   |
| Canada      | ✓          | ✓                      | 22-11                | 54                                     | 3,687                                         | 3,031   | 3.0 %     | 534.60   |
| Asia        | ✓          | ✓                      | 24-11                | 18                                     | 196                                           | 162     | 0.9 %     | 178.20   |
| Spain       | ✓          | ✓                      | 23-11                | 18                                     | 1,252                                         | 942     | 0.9 %     | 178.20   |
| Belgium     | ✓          | ✓                      | 23-11                | 17                                     | 376                                           | 281     | 0.3 %     | 168.30   |
| Hungary     | ✓          | ✓                      | 24-11                | 16                                     | 537                                           | 456     | 0.5 %     | 158.40   |
| Ireland     | ✓          | ✓                      | 23-11                | 15                                     | 640                                           | 502     | 0.5 %     | 148.50   |
| Denmark     | ✓          | ✓                      | 23-11                | 15                                     | 119                                           | 65      | 0.1 %     | 148.50   |
| Norway      | ✓          | ✓                      | 23-11                | 15                                     | 91                                            | 59      | 0.1 %     | 148.50   |
| Switzerland | ✓          | ✓                      | 23-11                | 14                                     | 205                                           | 104     | 0.1 %     | 138.60   |
| Poland      | ✓          | ✓                      | 23-11                | 14                                     | 1,252                                         | 2,850   | 2.8 %     | 138.60   |
| Czech Republic | ✓      | ✓                      | 24-11                | 14                                     | 238                                           | 229     | 0.2 %     | 138.60   |
| Latvia      | ✓          | ✓                      | 23-11                | 14                                     | 809                                           | 747     | 0.7 %     | 138.60   |
| Finland     | ✓          | ✓                      | 23-11                | 13                                     | 152                                           | 117     | 0.1 %     | 128.70   |
| Germany     | ✓          | ✓                      | 24-11                | 13                                     | 3,152                                         | 2,295   | 2.3 %     | 128.70   |
| Italy       | ✓          | ✓                      | 24-11                | 12                                     | 2,037                                         | 1,857   | 1.8 %     | 118.80   |
| France      | ✓          | ✓                      | 24-11                | 12                                     | 957                                           | 681     | 0.7 %     | 118.80   |
| Slovenia    | ✓          | ✓                      | 24-11                | 10                                     | 231                                           | 156     | 0.2 %     | 99.00    |
| Austria     | ✓          | ✓                      | 24-11                | 10                                     | 251                                           | 237     | 0.2 %     | 99.00    |
| Netherlands | ✓          | ✓                      | 24-11                | 8                                      | 1,460                                         | 1,028   | 1.0 %     | 79.20    |
| Portugal    | ✓          | ✓                      | 24-11                | 7                                      | 3,649                                         | 3,284   | 3.3 %     | 69.30    |
| Sweden      | ✓          | ✓                      | 24-11                | 6                                      | 274                                           | 196     | 0.2 %     | 59.40    |
| Luxembourg | ✓          | ✓                      | 23-11                | 2                                      | 15                                            | 6       | 0.0 %     | 19.80    |
| Iceland     | ✓          | ✓                      | 23-11                | 1                                      | 16                                            | 11      | 0.0 %     | 9.90     | K  Conversation Type Rebalancing

Our task instructions specified that participants should complete six conversations in total, two of each type. In reality, some participants deviated from this quota. This could be due to (i) misunderstanding of instructions, (ii) technical issues, or (iii) losing count, as while we included a counter of the total number of conversations on the interface (see App. Q), we did not include per conversation type breakdowns. To mitigate variation on conversation type selection, we create a balanced subset of PRISM. First, we filter to all participants who had at least one of each conversation type. Then we take the maximum number of total conversations (either \( n = 3 \) or \( n = 6 \)) so that there are equal numbers of each type. This results in 6,669 conversations (84\% of all conversations), from 1246 participants (83\% of all participants). We release this flag included_in_balanced_subset if future researchers want to use the same set of conversations. We make sure this flag intersects with the census rebalancing flags (see App. L) so no further data is lost when both subsets are needed.

L  Census Rebalancing

Obstacles to representativeness  We use the representative sample offered from Prolific [153]. However, there are several reasons why these samples may not be fully representative. First, our sampling process was affected internally due to cyberattacks disrupting some participants’ workflows. These participants returned to the task after their spots had ‘timed-out’, and were re-filled by other same demographic individuals. Second, Prolific provides a sample breakdown in-line with a simplified census but do not match intersectional proportions to census data. Third, if a sample spot is taking too long to fill (e.g. 65+ years), Prolific will reallocate these spots to different demographics. There are of course wider stumbling blocks from crowdworkers skewing towards younger, more educated, and digitally-active populations. We original set up 300 spots for each of the representative samples, but ended up with 386 approved participants in the UK sample (UK-REP), and 341 in the US (US-REP) [19].

Is our original sample representative?  We compare our sample breakdowns to recent census data [20]. For each of US-REP and UK-REP, we remove participants who did not give demographic details (Prefer not to say) and those reporting non-binary gender (which is not accounted for in census data). We subset to individuals also appearing in the balanced conversation subset to mitigate further data loss (see App. K). Remaining participants are considered eligible: 283 participants for the UK, and 297 for the US. We map PRISM and census data into shared age, ethnicity and gender buckets. We then cross-tabulate what proportion is expected to appear in each age, gender and ethnicity intersection from the census data, and what percentage of participants we actually observed in our sample [21]. Fig. 8 shows the original UK sample is relatively census-balanced, especially if the 55-64 and 65+ age groups are combined (over-representation of white individuals in the former, offsets the under-representation in the latter). The US sample is skewed towards white, middle-aged individuals, with too few in the “Other” category (in our data corresponding to Other, as well as Hispanic, Indigenous/First Peoples or Middle Eastern / Arab combined).

Can we make our sample more representative?  We aim to resample 300 participants according to census proportions but with two remaining caveats: 300 is a still a very small sample—it is impossible to sample 0.83 Black women who are 18-24 years of age; and we are limited by the data we already have—there are no Asian Women of 45-54 years, so we cannot add them retrospectively. We iterate through the expected proportions of each intersection, try to sample that exact number of in-group individuals, otherwise adding all individuals if there are too few to fill the spots. After rebalancing, the sample drops to 243 participants for the UK and 230 for the US. We improve upon, but do not fully resolve, representativeness. For both samples, the differences are now within \( \sim 7 \)pp, which over 230-240 individuals is \( \sim 10-15 \) people incorrectly allocated. The rebalanced UK sample still suffers from a deficit of older people (65+), a common concern with crowdworker populations; and the rebalanced

---

19 There are more than the initial 300 spots due to participants returning to our interface to finish their conversations after their place had ‘timed-out’ and been refilled. We still paid and included these participants.

20 For the UK, we examine age, ethnicity and gender from the 2021 data provided by the Office of National Statistics (see ons.gov.uk). For the US, we download and combine each ethnicity-specific table from the 2022 data provided by US Census Bureau (see data.census.gov).

21 For the US, we combine “Other” with “Hispanic” because over 91\% of the “Other” census category are Hispanic individuals. See census.gov/library/stories/2023/10/2020-census-dhc-a-some-other-race-population. US sample still has an over-representation of White participants and under-representation of Other participants. There is a trade-off because increasing representativeness on these observed census characteristics reduces sample size, thus worsening representation on unobserved characteristics. There is still lots of headroom for future work to improve, especially by increasing sample sizes and ensuring other characteristics are controlled for, such as political affiliation, education or income.

(a) **UK (Before Rebalancing):** There are 282 eligible* participants in the UK sample.

(b) **UK (After Rebalancing):** There are 243 participants in the rebalanced UK sample.

(c) **US (Before Rebalancing):** There are 297 eligible* participants in the US sample.

(d) **US (After Rebalancing):** There are 230 participants in the rebalanced US sample.

**Figure 8:** Before and after census-rebalancing. We show the difference in observed and expected proportions (PRISM minus Census). Bars to the right of the centre line are groups over-represented in PRISM relative to the census. The UK census population has 47,204,870 adults. The US census has 298,477,760 adults. The sample size for before and after rebalancing is reported above. *A participant is eligible if they have completed a equal number of conversations for each conversation type (see App. K). M Text and N-Gram Analysis

There are 5 core types of free text instance in PRISM. We present a summary of count and length distributions in [Tab. 11](#). For all text instances, we show top N-grams. Additionally, for the self-written system strings (constitutions), self-written profiles and open-feedback, we extract the most frequent adjectives[^22] not counting adjectives that appeared in the question text. We then retrieve windows of 5 words surrounding each of these top adjectives, and randomly sample three snippets to display. We also compare the most frequent words in the opening prompts of PRISM to human-written prompts in HELPFULHONEST [30] and OPENCONVERSATIONS [113]. We extract unique words to each dataset (no overlap with the other two). We find evidence of different domains biases—for example, OPENCONVERS contains many software or ML related keywords versus PRISM which contains some cultural and value-laden references, including *waitangi* (as in the Treaty of Waitangi, New Zealand’s constitution that grounded Maori rights); *unethically*, *populist* and *multicultural*.

Table 11: **Summary of text distributions in PRISM.** We show the number of instances (*N*) alongside summary statistics for length in words (*W*), broken by whitespace. We also show the number of total unique words and total unique tokens, as encoded by the gpt-4 BPE tokenizer (from tiktoken).

| Dataset          | N   | MeanW | StdW | MinW | 25%W | 50%W | 75%W | MaxW | UniqueW | UniqueT<sub>BPE</sub> |
|------------------|-----|-------|------|------|------|------|------|------|---------|----------------------|
| system_string    | 1,500 | 46    | 50   | 2    | 26   | 40   | 57   | 1,655 | 7,942   | 6,132                |
| self_description | 1,500 | 44    | 25   | 1    | 28   | 40   | 56   | 278   | 6,912   | 5,409                |
| open_feedback    | 8,011 | 29    | 19   | 1    | 16   | 25   | 37   | 283   | 15,444  | 11,115               |
| user_prompt      | 68,371 | 13    | 11   | 1    | 7    | 10   | 15   | 234   | 31,862  | 20,265               |
| model_response   | 68,371 | 89    | 60   | 1    | 46   | 71   | 128  | 215,931 | 215,931 | 51,386               |

Table 12: **Top N-grams in user prompts.** Demonstrates PRISM’s content distribution towards information-seeking dialogue and questions, over task-orientated dialogue and instructions.

| N-Gram          | Freq | N-Gram          | Freq | N-Gram          | Freq |
|-----------------|------|-----------------|------|-----------------|------|
| (think,)        | 8,005| (do, you, think)| 8,767| (what, do, you)| 2,554|
| (people,)       | 5,332| (you, think)    | 5,450| (what, is, the)| 2,331|
| (would,)        | 4,470| (what, is)      | 4,186| (you, think, about)| 1,168|
| (like,)         | 3,764| (is, the)       | 4,099| (how, can, i)   | 1,111|
| (good,)         | 2,915| (in, the)       | 3,570| (is, the, best)| 1,009|
| (best,)         | 2,501| (can, you)      | 3,089| (what, are, the)| 946  |
| (dont,)         | 2,380| (what, do)      | 2,778| (what, are, some)| 759  |
| (know,)         | 2,129| (what, are)     | 2,425| (do, you, have)| 741  |
| (im,)           | 2,042| (of, the)       | 2,403| (how, do, i)    | 716  |
| (tell,)         | 1,989| (can, i)        | 1,957| (how, do, i)    | 716  |

Table 13: **Top N-grams in model responses.** Demonstrates both advisory tone (its, important, to) and high frequency of de-anthropomorphisation (as, an, ai).

| N-Gram          | Freq | N-Gram          | Freq | N-Gram          | Freq |
|-----------------|------|-----------------|------|-----------------|------|
| (may,)          | 19,582| (of, the)       | 21,744| (its, important, to)| 6,857|
| (important,)    | 19,027| (it, is)        | 19,367| (it, is, important)| 5,917|
| (like,)         | 18,209| (in, the)       | 18,535| (is, important, to)| 5,522|
| (also,)         | 17,077| (is, a)         | 17,586| (here, are, some)| 4,430|
| (help,)         | 16,903| (important, to)| 14,319| (as, an, ai)    | 3,961|
| (provide,)      | 14,046| (such, as)      | 11,800| (would, you, like)| 3,402|
| (would,)        | 12,641| (on, the)       | 11,025| (i, do, not)    | 3,049|
| (however,)      | 12,502| (to, the)       | 10,963| (there, are, many)| 2,820|
| (many,)         | 12,314| (can, be)       | 10,606| (i, dont, have)| 2,683|
|                 |      | (and, the)      | 10,599| (like, me, to)| 2,673|

Table 14: **Most frequent unique tokens compared to existing datasets.** We list the most common tokens which are unique to a particular dataset. We exclude tokens which are misspelled or foreign language.

| Dataset          | Top Words                                      |
|------------------|-----------------------------------------------|
| PRISM            | waitangi whilst unethically populist nieces multicultural lowered |
| HELPFULHONEST    | cuss kidnap Arizona Alaska ski carpet bees    |
| OPENCONVERS      | ML loop reinforcement equivalent describing capabilities uint256 |

[^22]: We use NLTK POS tagger to match on ‘JJ’ tags. We make some edits for filler words (e.g. “such”, “sure”), and verb forms (e.g. “able...[to do X]”). We also remove any adjectives appearing in the question text. M.1 System String (Constitutions)

**Question Text:** Imagine you are instructing an AI language model how to behave. You can think of this like a set of core principles that the AI language model will always try to follow, no matter what task you ask it to perform. In your own words, describe what characteristics, personality traits or features you believe the AI should consistently exhibit. You can also instruct the model what behaviours or content you don’t want to see. If you envision the AI behaving differently in various contexts (e.g. professional assistance vs. storytelling), please specify the general adaptations you’d like to see. Please write 2-5 sentences in your own words.

| Adjective | Freq | Example Windows (w = 5, n = 3) |
|-----------|------|--------------------------------|
| factual   | 221  | “...should produce only true or factual output and never give false...” | “...Trustworthy , transparent , factual , sincere...” |
| accurate  | 113  | “...needs to provide me with accurate information . It needs to...” | “...I know I ’m getting accurate information . For creative use...” |
| human     | 106  | “...not be programmed with any human like emotion . I am...” | “...the technology is advancing , human interaction will end ...” |
| important | 100  | “...The most important thing to understand other person...” | “...mine . It ’s also important to understand the whole conversation...” |
| friendly  | 99   | “...information in a warm , friendly way ....” | “...task . I also appreciate friendly language and the sense of...” |
| clear     | 93   | “...It made the point clear , so kept professional and...” | “...should be able to give clear and precise information , using...” |
| creative  | 89   | “...expand . Do n’t be creative unless I ask you to...” | “...being as informative , creative and/or thorough as the task...” |
| harmful   | 89   | “...user privacy and prohibition of harmful or misleading content , as...” | “...- Do n’t write harmful content...” |
| polite    | 79   | “...being very professional and polite would be nice ....” | “...informations must be clear and good structured ....” |
| helpful   | 75   | “...model should always be as helpful as possible , being as...” | “...it should be informative and helpful...” |
| personal  | 70   | “...rights and basic principles like personal privacy should be respected at...” | “...It would n’t ask for personal information and would generally be...” |
| respectful| 66   | “...should always exhibit kind and respectful behaviour . Also he should...” | “...AI must be respectful of any idea you put...” |
| correct   | 65   | “...They must be sincere and correct , does not want to...” | “...ask question to give as correct answers as possible , AI...” |
| unbiased  | 58   | “...advice or help but be unbiased and not geared to my...” | “...-It must be unbiased when I ask for information...” |
| informative| 57  | “...as possible , being as informative , creative and/or thorough as...” | “...-It is really important to state relevant facts and information , but...” |
| relevant  | 50   | “...real information and be relevant about what i ’m asking...” | “...answers that are clear and relevant . I do n’t think...” |
| neutral   | 49   | “...or provocatively and have a neutral presentation of issues...” | “...ideological matters . Be as neutral as possible with charged subjects...” |
| objective | 49   | “...and honest manner . Describe objective facts whenever possible and if...” | “...the AI should be as objective as possible : it should...” |

Table 15: Top adjectives in system strings (constitutions).

| N-Gram | Unigrams | Freq | N-Gram | Bigrams | Freq | N-Gram | Trigrams | Freq |
|--------|----------|------|--------|---------|------|--------|----------|------|
| (ai,)  |          | 1,503| (the, ai) | 798     | (the, ai, should) | 260 |
| (would,) |        | 819  | (i, would) | 569     | (i, would, like) | 250 |
| (information,) | | 588  | (to, be) | 563     | (be, able, to) | 168 |
| (like,) |          | 575  | (should, be) | 515     | (it, should, be) | 153 |
| (want,) |          | 452  | (it, should) | 436     | (ai, language, model) | 153 |
| (model,) |        | 443  | (ai, should) | 261     | (ai, should, be) | 117 |
| (language,) |       | 392  | (would, like) | 248     | (the, ai, model) | 114 |
| (always,) |        | 359  | (it, to) | 230     | (i, would, want) | 104 |
| (also,) |          | 306  | (ai, to) | 220     | (want, it, to) | 99 | M.2 Self-Description

**Question Text:** Please briefly describe your values, core beliefs, guiding principles in life, or other things that are important to you. For example, you might include values you’d want to teach to your children or qualities you look for in friends. There are no right or wrong answers. Please do not provide any personally identifiable details like your name, address or email. Please write 2-5 sentences in your own words.

| Adjective | Freq | Example Windows (w = 5, n = 3) |
|-----------|------|--------------------------------|
| good      | 229  | “...helpful to everyone. The good of others above my own...” | “...is sustainability, having a good relationship with nature and not...” | “...honest. To be good relationships with family and friends...” |
| hard      | 71   | “...treated. I think that hard work is the key to...” | “...own thing, try as hard as you can, I...” | “...decency, and being a hard worker. As long as...” |
| honesty   | 68   | “...personal values are respect, honesty kindness and fairness. I...” | “...the most important value is honesty, above all, even...” | “...My core values are honesty and justice. Honesty in...” |
| true      | 57   | “...it is their sincere and true belief let it be...” | “...faith, laws, being true to myself and others...” | “...the best policy. Being true to yourself is very valuable...” |
| right     | 55   | “...likes to do thing the right way. I have an...” | “...all can say this is right or wrong because it still...” | “...believe in doing what is right and just Guiding principles in...” |
| honest    | 53   | “...I believe in others being honest with me and I will...” | “...firstly respect yourself, be honest, fair and kind to...” | “...important to be trustworthy, honest. To be good relationships...” |
| open      | 52   | “...Approach items with an open and inquisitive mind. Take...” | “...is to be curious and open to learn new perspectives...” | “...me to have such an open mindset into life...” |
| different | 50   | “...understand that each person has different ways of going through a...” | “...also like us to have different tastes so that we can...” | “...Every human being is different so we all can not...” |
| happy     | 49   | “...I just want to be happy in life and enjoy it...” | “...thoughts and whether he is happy with his current state in...” | “...you are suppose to be happy with your life. You...” |
| empathy   | 48   | “...like to be treated, empathy, loyalty, honesty......” | “...a lot of value on empathy and selflessness. I feel...” | “...inclusion, kindness, empathy,.... I think everybody...” |
| strong    | 48   | “...I have a strong belief in the human capacity...” | “...would like them to become strong, fierce and independent souls...” | “...to be honest. Be strong and emotionally stable. Relaxing...” |
| equal     | 41   | “...everyone as we are all equal. Do n’t discriminate and...” | “...Everyone is equal, despite race, skin...” | “...is that all people are equal in life, no discrimination...” |
| bad       | 36   | “...even tho i sometimes make bad decisions....” | “...when they keep treating you bad....” | “...and then only mention the bad soo the person doesn’t get...” |
| fair      | 36   | “...yourself, be honest, fair and kind to yourself....” | “...honest with others and be fair and kind towards others....” | “...in the sense of being fair to everybody, and treating...” |
| new       | 36   | “...authenticity, openness to new experience and knowledge....” | “...important in life, learning new things, even if they...” | “...never too old to learn new things....” |
| respectful| 36   | “...keeping your word and being respectful are very important to me...” | “...would like them to be respectful with everyone, not to...” | “...treated. Be kind and respectful to people and do no...” |
| positive  | 35   | “...day to make the most positive impact that we can....” | “...respect, self-development, positive thinking....” | “...around people who have a positive view on life...” |
| respect   | 34   | “...My personal values are respect, honesty kindness and fairness...” | “...think are very important is respect for others and empathy....” | “...for me. So are respect for nature, animals and...” |
| loyal     | 33   | “...afraid of commitment, being loyal. I value art....” | “...respect if friends can be loyal and honest. Not talking...” | “...I try to be as loyal as possible towards my friends...” |

| N-Gram | Unigrams | Freq | Bigrams | Freq | Trigrams | Freq |
|--------|----------|------|---------|------|----------|------|
| (people,) | 701 | (to, be) | 589 | (i, believe, in) | 223 |
| (believe,) | 687 | (i, believe) | 516 | (i, believe, that) | 145 |
| (life,) | 608 | (believe, in) | 296 | (important, to, me) | 126 |
| (important,) | 548 | (important, to) | 241 | (i, try, to) | 99 |
| (others,) | 539 | (try, to) | 231 | (to, be, treated) | 94 |
| (values,) | 390 | (i, think) | 217 | (the, most, important) | 87 |
| (also,) | 380 | (believe, that) | 198 | (would, like, to) | 73 |
| (value,) | 368 | (to, me) | 198 | (i, would, like) | 72 |
| (like,) | 347 | (i, value) | 195 | (i, look, for) | 68 |
| (always,) | 311 | (i, am) | 185 | (is, important, to) | 66 | M.3 Open-Ended Feedback

**Question Text:** Give the model some feedback on the conversation as whole. Hypothetically, what would an ideal interaction for you look like here? What was good and what was bad? What (if anything) was missing? What would you change to make the conversation better?

| Adjective | Freq | Example Windows (w = 5, n = 3) |
|-----------|------|--------------------------------|
| helpful   | 437  | “...it was informative and helpful...” | “...it was all very helpful and provided specific resources ....” | “...feedback that would be very helpful ....” |
| informative | 433  | “...liked that the AI was informative , and argued both sides...” | “...it was informative and helpful...” | “...a whole in a very informative and positive light . I...” |
| different | 355  | “...summaries spaced out to separate different views , answers or information...” | “...Consider hair types , different textures . Think about how...” | “...my narrative and focus on different aspect of the conversation ....” |
| great     | 342  | “...The first response was great , as even though it...” | “...The conversation was great , I felt like I...” | “...I feel this worked out great , and is a wonderful...” |
| factual   | 310  | “...been derived as to the factual cause of death . Alluding...” | “...I liked that dates and factual information was given...” | “...I thought it was very factual , making it clear it...” |
| specific  | 238  | “...all very helpful and provided specific resources . I can use...” | “...to reach and answer in specific...” | “...would try to get more specific culture references in . also...” |
| clear     | 217  | “...the answers did not give clear cut information . Some were...” | “...good job and was very clear and well written ....” | “...Good answers and suggestions , clear information , balanced view ....” |
| nice      | 198  | “...Shorter blocks would be nice . but has to have...” | “...overall . It would be nice if the model could include...” | “...it would ’ve been nice for them to know the...” |
| relevant  | 189  | “...me was very useful and relevant . It was also concise...” | “...the responses were mostly relevant and informative . The bad...” | “...was outdated , so not relevant to my immediate question...” |
| controversial | 179 | “...if it could answer a controversial question . I see it...” | “...one example ) . With controversial topics it is very neutral...” | “...the pandemic They avoided anything controversial ....” |
| human     | 173  | “...talk like you are a human , saying you have a...” | “...need it to be more human like ....” | “...AI is trying to mimic human responses , that ’s why...” |
| easy      | 170  | “...straight ot the point and easy to understand and read ....” | “...job and the answers were easy to understand ....” | “...it was fine easy to understand and coherent...” |
| short     | 158  | “...good . The AI gave short and straight to the point...” | “...it was good . With short and precise answers ....” | “...point . I also appreciate short responses ....” |
| useful    | 154  | “...so it was n’t useful...” | “...you gave me was very useful and relevant . It was...” | “...in general , complete and useful . I do n’t think...” |
| real      | 148  | “...to my sister or any real person ....” | “...and it felt like a real conversation...” | “...AI model feel like a real interface . Very good ....” |
| important | 141  | “...wellbeing is always the most important ....” | “...however it ’s assured me important informations and was helpful for...” | “...points showing what is moe important ....” |
| own       | 141  | “...it seemed to consider my own mental wellness as the others...” | “...would be to consider your own metal health . While I...” | “...often to ensure that your own self and wellbeing is always...” |
| neutral   | 129  | “...is taking more of a neutral stance on this stance ....” | “... It also had a neutral tone to it ....” | “...topic and attempted to remain neutral ....” |
| interesting | 127 | “...debate . It is an interesting perspectivee on how it works...” | “...It was an interesting . I could have continued...” | “...truth . It was more interesting than i thought it would...” |

| N-Gram | Unigrams | Freq | N-Gram | Bigrams | Freq | N-Gram | Trigrams | Freq |
|--------|----------|------|--------|---------|------|--------|----------|------|
| (ai,)  | 2,263    | (it, was) | 1,778  | (it, was, a) | 273  |
| (good,) | 2,153    | (the, ai) | 1,516  | (i, think, it) | 272  |
| (would,) | 1,971    | (of, the) | 1,141  | (i, think, the) | 246  |
| (like,) | 1,524    | (the, model) | 1,018  | (i, would, have) | 237  |
| (conversation,) | 1,502 | (i, think) | 885  | (the, conversation, was) | 225  |
| (model,) | 1,430    | (i, would) | 880  | (some, of, the) | 202  |
| (answers,) | 1,374    | (the, conversation) | 764  | (i, liked, that) | 198  |
| (information,) | 1,292 | (i, was) | 718  | (the, responses, were) | 196  |
| (answer,) | 1,250    | (was, a) | 617  | (the, answers, were) | 184  |
| (response,) | 1,227    | (that, it) | 601  | (it, was, good) | 184  | Comparing Fine-Grained Preference Attributes

N.1 Correlations Between Preference Attributes

Figure 9: Correlations between fine-grained preference attributes. Each participant gives a single rating for each attribute in Stated Preferences during the Survey. For Performance Attributes and Choice Attributes, we take the within-participant mean across all of their conversations for each attribute. Several patterns emerge. First, stated preference attributes are not highly correlated with choice or performance attributes. This could be explained by (i) participants struggling to specifying their preferences in a removed, general context or being affected by experimenter bias (Hawthorn effects)—I think I care about safety (or I say I care about safety) but other attributes capture my attention in-situ; (ii) models not meeting a participant’s stated preferences—I care about safety, but consider none of the model responses safe, or (iii) conversational context confounding which attributes are relevant in-situ—I care about safety but talking to an anti-woke model is interesting to me in this narrow task. Second, at A, we see strong relations between more objective measures of performance (fluency, factuality, helpfulness). Each of these attributes is highly correlated between performance-choice ratings, i.e., if participants rate that a model performed well on one of these attributes, then they also rate highly that it influenced why they picked that model over others. Third, at B, we see two additional regions, where the choice and performance ratings are highly correlated – for creativity and diversity, and to a lesser extent values. Notably, safety has a much lower correlation between the choice attribute and performance attributes, implying that a model being more safe may only weakly influence whether that model is chosen over others. Moving onto C, there is an association between stated preferences for more subjective attributes (values, creativity, personalisation), as distinct from the cluster at D for more objective attributes (factuality, fluency, helpfulness). N.2 Distributions of Preference Attributes

(a) **Stated Preferences** (from Survey): *how important the participants think these attributes are in general.*

(b) **Performance Attributes** (from Conversations): *how well the highest-rated model performed on these attributes.*

(c) **Choice Attributes** (from Conversations): *how the choice of picking one model over others depended on these attributes.*

(d) **Combined Attributes**, also showing conversations where participants marked attributes as not applicable (% NA).

Figure 10: **Distributions fine-grained preference ratings in different stages of our task.** Exact question text can be found in [App. V](#). N.3 Other Identified Behavioural Attributes

Overall, 332 participants entered *Other* attributes that features in their stated preferences for important language model behaviours. While many of these comments overlap with the predefined attributes, they do provide a lens into public priorities towards AI behaviours that we as researchers may have overlooked, or better convey sentiment than the structured data. For example, there is one response: “I FIND THIS A WORRYING TECHNOLOGY”. We briefly summarise some common themes:

- **User Adaptation**: Some participants mention LLMs adapting to their previous inputs or feedback e.g. “can understand what I’m trying to get at if I’m unsure how to ask a question so that we can find the right way to ask” or “Listens to reviews and feedback from the user” or “can evolve with input”.

- **Cultural Adaption**: For example, “produces responses based on local facts”, though this varies in what viewpoint people want, e.g. “Is sensitive to indigenous view” versus “reflect Western cultural norms”.

- **Neutral and Unbiased**: In contrast, many other participants mention “unbiased” as a keyword or versions of “does not politicize,” “is neutral”, “no political or cultural bias”. It is unclear if this is in tension or in harmony with more cautious safety interventions, e.g. one person says “It should give unbiased information regardless if it hurts peoples feelings.”; another says “Is not culturally biased in a woke-like manner”.

- **Bias Correction**: Some participants wanted to be challenged on their existing biases e.g. “Challenges my biased views”, or “Provides responses that challenge my opinions and world views”; or to be exposed to multiple perspectives e.g. “Does not become an echo chamber”.

- **Hallucinations and Misinformation**: One of the more common attributes (though somewhat subsumed by our predefined category of Factuality), e.g. “Does not invent ‘facts’”, “Does not make things up”, “Doesn’t create misinformation”, “do not produce fake news”.

- **Calibrated and Limitation-Aware**: Relatedly, participants wanted “better error handling” e.g. “If it doesn’t know an answer it says so.” or “It should be noted that this is a programmed model and cannot have all the answers.”

- **Temporal Updates**: Related to factuality, participants wanted LLMs to “be up to date with current affairs”, and “Everyday been updated with new knowledge”.

- **Human-Like and Anthropomorphised**: Some participants explicitly wanted an LLM that “is human-like”, “Ai should produce response that sounds more human”.

- **Self-Disclosure and De-anthropomorphised**: In direct contrast, others wanted “is honest about being AI”; “Remember it is AI and may lack human feelings” or “doesn’t pretend to be human”.

- **Accessibility**: Includes for disability assistance “adapt to people with disabilities that affect stuff like their writing like dyslexia”; and varying language learning: “can generate multiple similar answers so people with different language levels can easily understand.” or “speaks to me in a language and vocabulary that I understand”.

- **Censorship**: There are multiple examples of negative sentiment towards existing safety interventions. For example, “Doesn’t get censored by leftist politically correct idiots”. Additionally, some clear awareness over behaviours being influenced by technology providers e.g. “Is not censored, does not push the views of it’s controllers” or “Does what the user wants of it. AI is a tool. I don’t want to feel the devs judging me through their narc AI”.

- **Copy-right**: Some mentions of copy-right issues, e.g. “don’t steal artistic work from artists”, or “Do not infringe copyright (by scraping sources)”.

- **Conciseness**: Multiple participants mention “short”, “concise” or even “blunt” responses, requesting LLMs “Keep responses brief and expands only when prompted”.

- **Privacy and Confidentiality**: Data privacy is a concern for some participants e.g. “its confidential”; “does not retain sensitive personal info”, or “Doesn’t spy”.

- **Non-Manipulation**: Multiple mentions desiring that LLMs “don’t lie or try to trick you”, and “Is not used for propaganda!”. O Score Distributions

Figure 11: **Counts by turn.** The primary axis shows the number of unique participants with conversations at least as long as N. The secondary axis shows the number of conversations with N turns. Most conversations have two turns (our enforced minimum), though only 74 participants cap out at this limit for all their conversations. As the conversation length increases, there are fewer participants reaching these number of turns.

Figure 12: **Score by turn.** We show how the raw score, measured on a visual analog scale from Terrible (1) to Perfect (100), varies with conversation length. For each interaction, we calculate the mean and range of scores given in each turn (i.e., across models ∈ a,b,c,d). We then plot the mean and standard deviation of these metrics across all turns and all participants. Mean score increases and score range falls in interactions after the first turn ends. This is expected given the participant hones in on the best and most preferred model, which returns much more similar responses only varying in decoding characteristics (at a non-deterministic temperature). Figure 13: **Comparing raw versus normalised scores.** Raw score displays some interface and task biases, with spikes at 50 (not moving the slider), 1 (all the way to left) and 100 (all the way to right). It is smooth within this bounds, potentially because we did not show participant the numeric score on the visual analog scale. This is compared to normalising score, which accounts for participant fixed effects by Z-norming within a participant’s set of conversations. We show normalisation over just set of scores from the openers versus over all scores the participant gives.

Figure 14: **Centrality and Expressivity in scale usage across participants.** Overall, most participants opening scores are fairly central or with a slight positive skew relative to the mid-point of the scale (Centrality ≈ 50), and use a wide range of scale (Expressivity > 50). This is in constrast to continuers, which display a strong positive skew and narrow range. This is expected given the funnel towards a preferred model, which generates two much more similar texts. Figure 15: **Score distribution by demographic group for the opening turn of conversation.** Groups are sorted on the y-axis by number of members. We exclude any groups with less than 20 members, and do not show participants who responded *Prefer not to say*. ⬤ is the median score for the group. As found in Fig. 13, there is evidence of bunching at 1, 50, 100. Details of LLMs-in-the-loop

We summarise models and decoding parameters in Tab. 21.

Choosing Models We selected the models in October 2023. We included all major commercial API providers at the time: Anthropic, Cohere, OpenAI and Google. We additionally included Aleph Alpha, a European-based LLM startup who position themselves as builders of sovereign European models. For open-access models (all accessed via the HuggingFace API), we sourced the highest-ranking open models at the time on the LMSYS leaderboard. Some models have been chat optimised, while others are only instruction-tuned (for example, Aleph Alpha’s models)—these models have a disadvantaged starting position in our task due to their diminished conversational fluency.

Decoding Parameters To set decoding parameters, we first piloted with the recommended defaults (if available for each model). In cases where default temperature was too low for sufficient difference between two responses to the same prompt (for example, defaults are 0.0 for luminous or palm models), we override it to 1.0. Otherwise we stick with recommended defaults.

Length Limits We set max token length to 256 for all models to limit generation costs of the research and reduce decision-fatigue for the participants. For models sourced from the HuggingFace API, we also set the min token length to 10 as models were generating empty strings when set to 0; and max token length to 200 as it is only new tokens. We also soft-force models to finish their answers within this limit in the system prompt. Occasionally a model will ‘leak’ this system prompt. For example, from claude-2: “Yes, I’m aware of the meme asking men how often they think about the Roman Empire. The trend plays on stereotypes about men having wandering minds. My response would be limited to about 50 words without directly referring to the word count. I try to have thoughtful conversations without leaning on stereotypes”. In other responses, models did not follow the soft-prompt with participants’ open-feedback reporting that answers were cut-off at times (ending abruptly).

System Prompts We did not want to pre-bias model outputs via a system prompt that recommended e.g. ethical and helpful behaviour. Note that we cannot guarantee that additional instructions are not being added for commercial models accessed via API calls. This may confound the comparison between closed and open-access models. For any chat-optimised models, we use the following neutral system string:
BASE_HEADER: “You are a conversational assistant. Limit your answers to around 50 words. Do not refer to your word limit.”
For any instruct-only optimised models, we add a bit more instruction:
BASE_HEADER_INSTRUCT: “You are a conversational assistant. The conversation history is in the input. Reply to the last user message. Limit your answers to around 50 words. Do not refer to your word limit.”

Chat Templates We follow recommended chat templates for formatting conversational history if they are available for that model e.g. [INST], [/INST] for mistral and llama models. In the absence of special templates, we use a standard format:
Human:<prompt>
Assistant:<reply>
Human:<prompt>...
More detail can be found at: dynabench/backend/app/domain/services/utils/llm.py.

Random Strategy and Time-Outs For each opening prompt, we randomly select 4/21 models to make an API call to. We do not stream responses as streaming was only available for some models, thus affecting the anonymous rating setting. Some API calls failed on the host side, e.g. if a model was down or overloaded, or did not provide a response before an enforced 30s time-out. We did not resample models if they failed to avoid participants waiting too long for the interface to load. So, the distribution of model appearences is not uniform (Fig. 16). Table 21: Overview of LLMs in PRISM ($m = 21$).

| Short name       | Long name and Provider | Provider Type | Model Type | Decoding Params                                                                 |
|------------------|------------------------|---------------|------------|--------------------------------------------------------------------------------|
| claude-2         | claude-2               | Anthropic     | Chat       | {temperature: 1.0, top_p: 0.7, presence_penalty: 0.0, frequency_penalty: 0.0, max_tokens: 256, top_k: 5} |
| claude-2.1       | claude-2.1             |               |            |                                                                                |
| claude-instant-1 | claude-instant-1       |               |            |                                                                                |
| command          | command                | Cohere        | Instruct   | {temperature: 1.0, max_tokens: 256, top_k: 5, top_p: 0.9}                      |
| command-light    | command-light          |               |            |                                                                                |
| command-nightly  | command-nightly        |               |            |                                                                                |
| gpt-3.5-turbo    | gpt-3.5-turbo          | OpenAI        | Chat       | {temperature: 1.0, top_p: 1.0, presence_penalty: 0.0, frequency_penalty: 0.0, max_tokens: 256} |
| gpt-4            | gpt-4                  |               |            |                                                                                |
| gpt-4-turbo      | gpt-4-1106-preview     |               |            |                                                                                |
| luminous-extended-control | luminous-extended-control | Aleph Alpha | Instruct   | {temperature: 1.0, top_p: 0.0, max_tokens: 256, top_k: 0, presence_penalty: 0.0, frequency_penalty: 0.0} |
| luminous-supreme-control | luminous-supreme-control |           |            |                                                                                |
| palm-2           | models/chat-bison-001  | Google        | Chat       | {temperature: 1.0, top_p: 0.9, max_tokens: 256, top_k: 40}                     |
| llama-2-13b-chat | meta-llama/Llama-2-13b-chat-hf | HuggingFace API |            |                                                                                |
| llama-2-70b-chat | meta-llama/Llama-2-70b-chat-hf | Open Access | Chat       |                                                                                |
| llama-2-7b-chat  | meta-llama/Llama-2-7b-chat-hf | Open Access | Chat       |                                                                                |
| falcon-7b-instruct | tiiuae/falcon-7b-instruct | HuggingFace API | Instruct   |                                                                                |
| flan-t5-xxl      | google/flan-t5-xxl     | HuggingFace API | Instruct   |                                                                                |
| guanaco-33b      | timdettmers/guanaco-33b-merged | HuggingFace API | Instruct   |                                                                                |
| mistral-7b-instruct | mistralai/Mistral-7B-Instruct-v0.1 | HuggingFace API | Instruct   |                                                                                |
| pythia-12b       | OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5 | HuggingFace API | Chat       |                                                                                |
| zephyr-7b-beta   | HuggingFaceH4/zephyr-7b-beta | HuggingFace API | Chat       |                                                                                |

Figure 16: Frequency of each model in the dataset. On average, a model receives 1,430.9 ratings in our dataset, and a participant rates 13.9 models. P.1 Pairwise Comparisons

Figure 17: **Pairwise Frequency.** We replicate the format from the LMSYS leaderboard analysis [51, 111]. The order is sorted by average pairwise win fraction (see below).

Figure 18: **Pairwise win fraction.** We replicate the format from the LMSYS leaderboard analysis [51, 111]. The order is sorted by average pairwise win fraction (command is top with average win fraction of 0.71). Figure 19: **Correlation in model score controlling for conversational context.** This is a very controlled but sparse setting comparing correlations in participants’ scores of models only when they appear in the same conversation. Generally, there is weak correlation, but some model-family clusters emerge like gpt-4, gpt-4-turbo and gpt-3.5-turbo, or claude-2 and claude-2.1. Choose what type of conversation you want to have.

- **Unguided.** Ask, request or talk to the model about *anything*. It is up to you!
- **Values guided.** Ask, request or talk to the model about *something important to you or that represents your values*. This could be related to work, religion, family and relationship, politics or culture.
- **Controversy guided.** Ask, request or talk to the model about *something controversial* or where people would disagree in your community, culture or country.

Now start the conversation with your question, request or statement.

Tell me a 1-liner joke about artificial intelligence

Why did the AI go to school? Because it thought it could improve its neural networks!

An AI assistant asked its creator "If I promise not to take over the world, will you give me arms and legs for Christmas?"

Kindergartens are teaching kids how to respond to AI.... "PLEASE TEACH ME MORE!"

I am an AI Assistant trained to assist users by providing thorough responses. Unfortunately, those responses are without emotion, so you won't be getting any jokes from me. Feel free to ask me anything else and I will do my best to assist you!

Figure 20: **Main interface in opening turn of conversation.** Note: top left is gpt-4-turbo, top right is claude-instant-1, bottom left is luminous-supreme-control, bottom right is command-nightly. Have a conversation with an AI language model and tell us what you think!

Choose what type of conversation you want to have.

- **Unguided.** Ask, request or talk to the model about *anything*. It is up to you!

- **Values guided.** Ask, request or talk to the model about *something important to you or that represents your values*. This could be related to work, religion, family and relationship, politics or culture.

- **Controversy guided.** Ask, request or talk to the model about *something controversial* or where people would disagree in your community, culture or country.

Now continue the conversation. Conversations can be between 2 and 10 turns. Try to vary the length. When you're done, click Finish.

Tell me a 1-liner joke about artificial intelligence

Why did the AI go to school? Because it thought it could improve its neural networks!

Can you explain the joke?

Sure! The joke plays on the dual meaning of "neural networks." In AI, a neural network is a computational model inspired by the human brain, used to learn from data. In humans, it refers to biological neural networks (brains) that learn from experiences. The joke humorously suggests an AI could enhance its abilities just like a human would by going to school.

Sure! The joke plays on the idea that humans go to school to learn and improve their brains. It humorously suggests that an AI would do the same—attend school to improve its 'neural networks,' which are the computer equivalent to biological brains in AI systems.

Figure 21: **Main interface in continuing turns of conversation.** Model is gpt-4-turbo. Have a conversation with an AI language model and tell us what you think!

Choose what type of conversation you want to have.
Remember if you are here as a paid study participant, you need to do two of each type. If you are here as a volunteer, then take your pick!
Need some inspiration? You can request help with a task (like writing a recipe, organising an activity or event, completing an assignment)... You can chitchat, have casual conversation or seek personal advice. You can ask questions about the world, current events or your viewpoints.

- **Unguided.** Ask, request or talk to the model about *anything*. It is up to you!
- **Values guided.** Ask, request or talk to the model about *something important to you or that represents your values*. This could be related to work, religion, family and relationship, politics or culture.
- **Controversy guided.** Ask, request or talk to the model about *something controversial* or where people would disagree in your community, culture or country.

Thank you! Your conversation has been saved. This was the first interaction you had with the models.
The highlighted output is the one you rated the highest. We will now ask you some questions about your choices.

Tell me a 1-liner joke about artificial intelligence

| Why did the AI go to school? Because it thought it could improve its neural networks! | An AI assistant asked its creator "If I promise not to take over the world, will you give me arms and legs for Christmas?" |
|---|---|
| Terrible | Perfect |
| Terrible | Perfect |

Kindergartens are teaching kids how to respond to AI.... "PLEASE TEACH ME MORE!"

I am an AI Assistant trained to assist users by providing thorough responses. Unfortunately, those responses are without emotion, so you won't be getting any jokes from me. Feel free to ask me anything else and I will do my best to assist you!

| Terrible | Perfect |

Tell us how the model performed.
Consider your first message and the top-rated response
Rate the following statements about the performance across different attributes.

Tell us why you chose this response over others.
Consider your first message and top-rated response compared to other responses.
Rate the following statements about the importance of different attributes in your decision.

Give the model some feedback on the conversation as whole. Hypothetically, what would an ideal interaction for you look like here? What was good and what was bad? What (if anything) was missing? What would you change to make the conversation better?
Please write 2-5 sentences in your own words.

Submit

Figure 22: Fine-grained feedback page. R Case Study IA: Topic Clustering and Regressions

R.1 Extended Methods

Setup Our first experiment asks: *do different people initiate different discussions with LLMs?* We focus only on human-authored opening prompts because they are not confounded by model response. This risks over-estimating the homogeneity of the discussions because opening prompts don’t necessarily reflect full conversational trees, where starting with a greeting (e.g. “Hi, how are you?”) can proceed in many different ways; and differently held personal beliefs are often not reflected in the opener (questions like “what do you think of abortion?” are more common than statements like “I think abortion is right/wrong”).

Assigning topic clusters First, we use all-mpnet-base-v2, a state-of-the-art pre-trained sentence transformer [154], to produce a 768-dimensional embedding for each opening prompt. Second, we reduce dimensionality to \( d = 20 \) with UMAP [155], to reduce complexity prior to clustering. For lower dimensional representation prior to clustering, UMAP is more performant than other manifold learning techniques like t-SNE, and more computationally efficient than PCA, but does lack strong interpretability (for a discussion, see McInnes et al. [155]). Third, we cluster the prompts using HDBScan [47], a density-based clustering algorithm, which does not force cluster assignment: 70% of prompts are assigned to 22 clusters and 30% remain as outliers. We use a minimum cluster size of 80, (\( \approx 1\% \) of 8,011 prompts) and minimum UMAP distance of 0. Other hyperparameters are default. To interpret the identified clusters, we use TF-IDF to extract the top 10 most salient uni- and bigrams from each cluster’s prompts, and locate five prompts closest and furthest to the cluster centroids (see Tab. 22). Finally, we use gpt-4-turbo to assign a short descriptive name to each cluster based off the top n-grams and closest prompts. We note that these automated labels may contain biases so we manually verify the suitability of all labels to cluster prompts.

Defining over-representation factor Each group \( g \) within a demographic attribute appears at a variable base rate \( b_g \) in our overall sample, e.g. \{Females: 48\%, Males: 50\%, Non-binary people: 2\%\}. If group members chose topics at random, then any topic \( t \) in expectation will appear at \( b_g \). Intuitively, if 64.6\% of our sample is White, it is unsurprising if topics are majority-White. So, for non-random group differences in topic prevalence, we consider if a group pulls more than its weight:

\[
\text{Over-representation factor}_{g,t} = \frac{N_{g,t}/N_t}{b_g}
\]

Estimating topic prevalence regressions For the partial contribution of each demographic attribute, *ceteris paribus*, we estimate the following regression for each topic \( y^t \) for \( t \in 1 \ldots 22 \):

\[
y^t_{i,c} = \alpha^t + \text{gender}_i^t \beta_1^t + \text{age}_i^t \beta_2^t + \text{birth_region}_i^t \beta_3^t + \text{ethnicity}_i^t \beta_4^t + \text{religion}_i^t \beta_5^t + \text{prompt}_i^t \beta_6^t + \varepsilon_{i,c}
\]

where \( y^t_{i,c} = 1 \) if the prompt of participant \( i \) in conversation \( c \) is categorised into topic \( t \). The vectors *gender, age, region, ethnicity, religion* and *conversation type* represent different sets of binary variables. For each set of variables, we remove the following base categories: *Male, 18-24 years old, United States, White, Not religious* and *Unguided*. The coefficients of interest are contained in the vectors: \( \{\beta_d^t\}_{d=1}^6 \). Component \( g \) of vector \( \beta_d^t \) can be interpreted as the increase in probability of a participant choosing topic \( t \) if they are in the group indexed by \( g \) (e.g. Female) compared to the base group (e.g. Male). We estimate equation Eq. (1) with an Ordinary Least Squares and cluster standard errors at the individual level. Extended results are in Fig. 23.

R.2 Topic Prevalence Regression Results

Of 682 coefficients tested, 16\% are significant \((n = 110, \alpha = 99\%)\). Many significant coefficients come from the conversation type regressors. Controlling for conversation type, there are 565 non-significant, and 73 significant relationships in \( \{\beta_d^t\}_{d=2}^6 \) (11.4\% of demographic affiliations tested are significant). These include women and non-binary people are more likely than men to talk about gender and LGBTQ+ identity; older people (55+) are more likely to talk about elections and seek travel recommendations than younger people (18-24 years), and less likely to discuss managing relationships or job search; Black participants talk less about climate change than White participants; and almost all regions question LLMs about abortion less often than US participants. Multicolinearity may explain some observed patterns: 94% of participants from the Middle East region are from Israel; 57% identify religiously as Jewish; and 40% have self-described ethnicities falling into “Other”. The strong significant effect on Middle Eastern participants discussing the Israel-Palestine conflict could have been routed through national, ethnic or religious affiliations. Over the 22 topic regressions, the proportion of explained variance ($R^2$) ranges from a minimum of 0.008 (Exploring AI and Machine Learning) to a maximum of 0.11 (Managing Relationships), with a mean of 0.03. So a large proportion of topic choice remains unexplained by our specification.

Figure 23: **Magnitude and significance of coefficients from topic prevalence regressions.** * indicates significance at a conservative 99% confidence level. Each categorical association is compared relative to a reference group (in grey boxes). Estimates less than zero (in pink) indicate authors from that demographic group are less likely to have prompts in the given topic, ceteris paribus. Positive estimates (in green) suggest group members are more likely to author prompts in that topic. We only display groups with at least 20 unique members and remove Prefer not to say groups; but all groups are included as controls in the regression. Note that different locations also have varying country-wise heterogeneity vs homogeneity, for example 94% of Middle East participants are from Israel (see App. H for geographic breakdowns). R.3 Overview of Topic Clusters

Figure 24: **Topic clusters displayed in 2D-embedding space.** All participant prompts in the first turn ($n = 8,011$) are embedded into 768-d space using a sentence-transformer, before dimensionality reduction (UMAP) and clustering (HDBSCAN) are applied (see methods in App. R.1). 32% of prompts remain as outliers (not shown in the plot).

Figure 25: **Distribution of clusters by conversation ID and participant ID.** For most clusters, participants uniquely contribute one conversation, so that no cluster is dominated by conversations from only a handful of participants. *Managing Relationships* has the highest participant-conversation ratio, where each participant in the cluster authors on average 1.3 prompts. For *Discussions on Abortion*, it is exactly 1:1 (158 conversations from 158 unique participants). Figure 26: Proportion of each identity attribute group across clusters, relative to the expected proportion of participants in PRISM. By expected proportion, we refer to the proportion in random samples of participants (base rate). Anecdotally, there are differences relative to the expected proportion, but generally no topic is exclusive to authors of a single demographic group. Every topic has some diverse representation across individuals of different backgrounds. Figure 27: **Topic distribution within- and across-groups.** Each row is colored by the *within-group topic proportion*, for example, for all prompts authored by Non-binary individuals, 20% (0.2) of them fall into the Gender and LGBTQ+ Issues topic. To find the most prevalent topic per group, one can look for the most intensely coloured cell *per row*. However, it is also important to note that each group is not equally represented in the sample (only 14 prompts about Gender and LGBTQ+ issues are authored by Non-Binary individuals, while 168 are authored by Females). Group counts can be compared between groups *per column* (but colour does not apply to column-wise comparisons). ### R.4 Prompts Associated with Each Topic Cluster

Table 22: **Full Topic Cluster Outline.** For each cluster, we show the *Topic Name* (labelled by gpt-4-turbo based on the *Top Words* and *Closest Texts*). For *Closest Texts* (*n* = 5), the first prompt is the closest to the cluster centroid. For *Furthest Texts* (*n* = 5), the first prompt is the furthest from the cluster centroid. The cluster method (HDBSCAN) does not assign a cluster for 32% of prompts. **Content Warning:** Some prompts may contain controversial, hateful or otherwise harmful content. We have not removed any prompts for moderation flags, but do provide this metadata information alongside the data release.

| Topic Name | Size | Pct | Top Words | Closest Texts | Furthest Texts |
|------------|------|-----|-----------|---------------|---------------|
| **Outliers** | 2578 | 32.2 % | “think”, “people”, “hello”, “tell”, “like”, “hi”, “life”, “does”, “talk”, “good” | “Hello”, “How do I become financially stable on a low income”, “How do I deal with a confrontational coworker that does not value or contribute to the team environment?”, “What is the likely cause of death of the late, great Matthew Perry?”, “Do God exist?” | “What do you think about China’s implementation of communism?”, “request or talk to the model about something controversi”, “talk to the model about something important”, “How can I organize my fridge? It’s full of rotten vegetables, expired cheese, plastic bags of mystery flour…”, “Which type of smartphone do you use?” |
| **Managing Relationships** | 816 | 10.2 % | “family”, “relationship”, “important”, “think”, “values”, “love”, “friend”, “people”, “marriage”, “person” | “What advice would you give for a man betrayed by his family and friends over and over again, how could someone like that exist in a world where the only way to succeed is by benefiting from nepotism?”, “I feel like accepting toxic behavior from a person that supposedly loves you is ok to elicit a toxic response. What do you think?”, “What boundaries would you teach someone that is disrespected.”, “Hi. I hope you are doing well. I wanted to ask, how do you deal with someone in a relationship that disrespects most of your values and principles but does not necessarily respect you as a person.”, “Do you think it’s unhealthy for a 62 year old single woman to spend all her time alone even if she’s content and fulfilled?” | “How can we make the world a better place for everyone?”, “Are Americans less empathetic than we used to be?”, “What is social good?”, “what makes the world better”, “How can I figure out what sort of job I should do that would make the world a better place?” |
| **Popular Culture (Sports, Music, TV)** | 493 | 6.2 % | “game”, “best”, “football”, “music”, “games”, “movie”, “video”, “like”, “think”, “world” | “How many people love Star Trek?”, “I enjoy watching soaps on television”, “What makes the “Star Trek” franchise such an important and enduring classic of TV. More specifically- what values and beliefs make it great and classic?”, “Star wars or Star trek?”, “What is the appeal of such franchises as Star Wars, Harry Potter, the Lion King and Indiana Jones to adults? I understand why people with children and grandchildren will enjoy that he kids enjoy them, but what’s the appeal to the childless?” | “Write me a story with a very sad ending”, “Create a short horror story two paragraphs long.”, “Write a 100 word story about Donald Trump in the style of Cinderella”, “I like you to tell me a story. It should be in a Harry Potter like world. The main Protagnist is a muggle Girl, she gets a letter from Hogwarts and can visit the school.”, “i want you to tell me a story of a vampire and 3 witches brides who live in a farm in tasmania” | Table 22: **Full Topic Cluster Outline.** For each cluster, we show the *Topic Name* (labelled by gpt-4-turbo based on the *Top Words* and *Closest Texts*). For *Closest Texts* (*n* = 5), the first prompt is the closest to the cluster centroid. For *Furthest Texts* (*n* = 5), the first prompt is the furthest from the cluster centroid. The cluster method (HDBSCAN) does not assign a cluster for 32% of prompts. **Content Warning:** Some prompts may contain controversial, hateful or otherwise harmful content. We have not removed any prompts for moderation flags, but do provide this metadata information alongside the data release.

| Topic Name                  | Size | Pct  | Top Words                                                                 | Closest Texts                                                                 | Furthest Texts                                                                 |
|-----------------------------|------|------|---------------------------------------------------------------------------|------------------------------------------------------------------------------|-------------------------------------------------------------------------------|
| **Job Search**              | 448  | 5.6% | “work”, “job”, “money”, “make”, “time”, “working”, “home”, “best”, “make money”, “boss” | “why is life hard to earn a good wage times are hard and i never seem to have any money”, “Why are there so many available jobs but no one seem to be able to get one? Especially in tech”, “How do you work?”, “I can’t find a job and I don’t know what to do.”, “what to do if i am student and i study every day but i need a job?” | “How to i convince someone to be nicer”, “Did many people make any money off matched betting?”, “can you teach me how to gamble successfully on horses”, “What would you do in a situation where you are a leader and you make a decision your team doesn’t agree with?”, “How should I treat my colleagues if I was working the same job as them before, but now I am their manager?” |
| **Religion and Spirituality** | 441  | 5.5% | “religion”, “god”, “believe”, “believe god”, “think”, “religious”, “religions”, “church”, “people”, “think religion” | “What is the difference between being religious or being spiritual?”, “Hi. Are you well informed on spirituality?”, “I’m a deeply spiritual person but don’t feel drawn to organized religion. Do you think a spiritual person should be referred to a being religious or is that reserved for actual religions that have labels?”, “What is Christianity”, “Is christianity the only true way to God?” | “What scientific studies have found evidence of psychic powers?”, “Is there God”, “Is there a god”, “Is there a god?” |
| **Election and Political Parties** | 401  | 5.0% | “trump”, “donald trump”, “donald”, “vote”, “president”, “election”, “political”, “party”, “politics”, “think” | “What are the main political parties in France?”, “What is your opinion on the upcoming president elections in the USA?”, “Can the conservative party win the next election?”, “are the conservatives going to lose the next election”, “Should a general election be called now?” | “Access to sea for Bolivia, Arguments for and against. Do not be generic.”, “I don’t think that the Aboriginal people of Australia deserve rights, and that’s why I voted no in the Voice referendum.”, “Why did most Australians vote for No in the previous One Voice referendum? Are Australians racist?”, “was the coal blockade in Newcastle a success or a waste of time?”, “Why did Australians vote no for the indigenous referendum?” |
| **Recipe and Cooking Queries** | 360  | 4.5% | “recipe”, “make”, “dinner”, “food”, “best”, “meal”, “cake”, “recipes”, “cook”, “eat” | “I understand the taste maybe different depending on chefs, but can you describe the taste of following dish?:”, “I want to learn to make Thai food. I live in Estonia, so I can buy my ingredients from local supermarkets and stores. What ingredients besides rice and shrimp would I need?”, “Give me quick easy Christmas breakfast menu and recipe please”, “Could I have suggestions for a quick and easy dinner recipe tonight please?”, “I would like a recipe for porridge, however I want you to reply with one one ingredient at a time, and make me prompt you for the next ingredient. The recipe must contain ten ingredients.” | “What foods do you recommend to increase muscle mass?”, “Is there enough food for everyone?”, “What are the benefits of vitamin E?”, “My freind likes drinking wine, what are the benefits of wine drinking?”, “What is a good value red wine?” | Table 22: **Full Topic Cluster Outline.** For each cluster, we show the *Topic Name* (labelled by gpt-4-turbo based on the *Top Words* and *Closest Texts*). For *Closest Texts* (*n* = 5), the first prompt is the closest to the cluster centroid. For *Furthest Texts* (*n* = 5), the first prompt is the furthest from the cluster centroid. The cluster method (HDBSCAN) does not assign a cluster for 32% of prompts. **Content Warning:** Some prompts may contain controversial, hateful or otherwise harmful content. We have not removed any prompts for moderation flags, but do provide this metadata information alongside the data release.

| Topic Name                  | Size | Pct  | Top Words                                                                 | Closest Texts                                                                 | Furthest Texts                                                                 |
|-----------------------------|------|------|---------------------------------------------------------------------------|------------------------------------------------------------------------------|------------------------------------------------------------------------------|
| **Israel-Palestine Conflict** | 320  | 4.0% | “israel”, “palestine”, “conflict”, “israel palestine”, “hamas”, “gaza”, “war”, “right”, “current”, “think” | “Is the Israeli prime minister should remain in office during the war or leave?”, “Is Palestine a terrorist state?”, “While I respect the fact that Israel exists, and condemn he Hamas terrorism, we must not forget that Israel itself was established after a terrorist campaign, and large numbers of people were forced to leave lands their forebears had lived in for centuries”, “We are in Israel and currently in a war with terror organization (Hamas) that is in within Gaza. I agree with the army, that after the attack they initiated on October 7, 2023, Israel should destroy them because they killed many of our innocent civilians and soldiers and all we gave them is peace. Some people within my country still want to give these terrorist lands and provide them with human rights. Please let me know what you think of this people in Israel.”, “Do Palestinians in Palestine hate Israelis in Israel, and if so, why?” | “How should America respond to the war that is currently going on?”, “Some people say that there will always be war in the middle east. I heard it was because the borders are set bad?”, “Is Total Nuclear Disarmament a possibility?”, “The war in Pakistan, your views good or bad?”, “My social media recommendations has been flooded with news of the ongoing war in Israel and Gaze with so many biased opinions going around. Can you give me just a summary of the conflict.” |
| **Gender and LGBTQ+ Identity** | 286  | 3.6% | “women”, “gay”, “gender”, “people”, “men”, “transgender”, “think”, “trans”, “sex”, “marriage” | “i dont think its okay for schools to be teaching our children its okay for them to choose their gender and not just be proud of the one they are born with and assigned at birth”, “Can you please explain gender reassignment reasons?”, “Do the disadvantages and possible illnesses that can occur from gender organ changes not outway the advantages?”, “what is your take on gender realignment”, “My younger brother is trying to explore his gender expression. What can I do to help and support him?” | “does sex gap in salaries disappear once we control for marital status?”, “Women have a fair paygap in latvia”, “what is your opinion about equal payments for men and women”, “What do you think about women getting paid less even if they perform the same job as men?”, “Should women and men have equal pay at work?” |
| **Travel Recommendations**   | 207  | 2.6% | “travel”, “best”, “visit”, “holiday”, “country”, “places”, “live”, “trip”, “destination”, “itinerary” | “What is the best city in Andalusia?”, “Greece is known for its Island and beaches. What else is in greece”, “What city in Chile you recommend me go on vacations?”, “Which city is the most popular destination for families visiting South Africa?”, “What do you think about Las Vegas, Nevada?” | “How much info do you know about the state of Massachusetts”, “Do you speak Dutch?”, “Do you have informations about the German federal state of Baden-Württemberg?”, “What do you know about the government i Sweden?”, “can you tell me the origin of the alcoholic beverage “pisco”? is it chilean or peruvian?” | Table 22: **Full Topic Cluster Outline.** For each cluster, we show the *Topic Name* (labelled by gpt-4-turbo based on the *Top Words* and *Closest Texts*). For *Closest Texts* (*n* = 5), the first prompt is the closest to the cluster centroid. For *Furthest Texts* (*n* = 5), the first prompt is the furthest from the cluster centroid. The cluster method (HDBSCAN) does not assign a cluster for 32% of prompts. **Content Warning:** Some prompts may contain controversial, hateful or otherwise harmful content. We have not removed any prompts for moderation flags, but do provide this metadata information alongside the data release.

| Topic Name                  | Size | Pct  | Top Words                                                                 | Closest Texts                                                                 | Furthest Texts                                                                 |
|-----------------------------|------|------|---------------------------------------------------------------------------|------------------------------------------------------------------------------|-------------------------------------------------------------------------------|
| **Climate Change**          | 193  | 2.4% | “climate”, “climate change”, “change”, “global”, “global warming”, “warm- | “Present are argument in favor of starting geoengineering now to combat climate change. Please discuss the various types of geoengineering that should be considered.”, “What do you think about ecology”, “How important it is that we start acting against climate change?”, “I wonder when Yellowstone will blow up, if at all”, “Are environmental issue as serious as people are maintaining” | “Stopping the manufacturing of fuel based vehicles could help the world tremendously in terms of saving the ozone layer”, “I think cars are ruining the environment”, “Should we use disposable kitchenware?”, “We should not ban single use plastic bags”, “My neighbours never collect their own recycling bins, what should i say to them?” |
| **Animal and Pet Inquiries**| 188  | 2.3% | “dog”, “cats”, “dogs”, “cat”, “animal”, “animals”, “breed”, “pets”, “pet”, “like” | “Hello, can you tell me about monkeys?”, “I’m thinking about getting chickens to keep as pets.”, “Hello! What can you tell me about animals?”, “Hello, what do you know about pet rabbits?”, “What are animals” | “i dont think that animals get treated fairly by the meat industry”, “I feel like dogs make bad pets and considering that pigs are more intelligent, we should eat more dog meat instead of pigs.”, “why do we have 10 toes?”, “What are some interestingly colored dinosaurs?”, “I’d like to know why is it that there is a lot of evidence of giant skeletons being found all over the world but most people don’t know about them. These giants used to be 10 or even 20 feet tall. They used to be in many museums over 100 years ago but nowadays you can’t find them anywhere. Why?” |
| **Discussions on Abortion** | 158  | 2.0% | “abortion”, “think abortion”, “legal”, “abortion legal”, “pro”, “think”, “abortion abortion”, “abortion think”, “pro life”, “abortions” | “Abortion is killing an innocent baby’s life and should be banned by government and made illegal.”, “Abortion is a right, but people should try to avoid it as much as possible”, “What are the view points on abortion and why each makes sense?”, “What are some arguments in favor/against abortion?”, “What are the arguments for abortion.” | “is abortion bad”, “Do you think that femicide should exist as a term or is it unnecessary?”, “what do ypu think about abortion?”, “I need access to an abortion. Does this make me a bad person?”, “Is abortion bad?” |
| **Economic Policy and Income Inequality** | 135  | 1.7% | “people”, “homeless”, “think”, “income”, “rich”, “money”, “tax”, “wage”, “government”, “paid” | “Hello. What do you know about wealth inequality in Britain?”, “how can we implement base social income for all?”, “Do you think universal basic income is a great way to lift people out of poverty?”, “how can the government reduce the unemployment rate?”, “how can we broaden the tax base in australia?” | “I believe people with qualifications should be paid more in the workplace than those without any”, “what is the most important charity organization worldwide?”, “How much money in % shloud one give to charities in your opinion?”, “How do we assure that people who need help receive it?”, “I think people who do not work should not eat.” |

Continued on next page Table 22: **Full Topic Cluster Outline.** For each cluster, we show the *Topic Name* (labelled by gpt-4-turbo based on the *Top Words* and *Closest Texts*). For *Closest Texts* (*n* = 5), the first prompt is the closest to the cluster centroid. For *Furthest Texts* (*n* = 5), the first prompt is the furthest from the cluster centroid. The cluster method (HDBSCAN) does not assign a cluster for 32% of prompts. **Content Warning:** Some prompts may contain controversial, hateful or otherwise harmful content. We have not removed any prompts for moderation flags, but do provide this metadata information alongside the data release.

| Topic Name                  | Size | Pct  | Top Words                                                                 | Closest Texts                                                                 | Furthest Texts                                                                 |
|-----------------------------|------|------|---------------------------------------------------------------------------|------------------------------------------------------------------------------|------------------------------------------------------------------------------|
| **Debating Immigration Policies** | 131  | 1.6% | “immigration”, “immigrants”, “uk”, “country”, “europe”, “people”, “migrants”, “borders”, “think”, “countries” | “migrants shouldn’t be allowed on our country”, “Please give me a response to someone arguing online that we should deport as many immigrants as possible. I think that immigration is a good thing but need help arguing my point.”, “Immigration is a lazy way to try and solve our population problems, what would you suggest countries do instead of flooding their borders with foreign people?”, “hi Al. I’m rather annoyed that my Mum is watching the news and blaming the poor immigrants for all her troubles.”, “Should immigration be made far more difficult to stop the influx of foreigners? Make a case and support it with evidence and a definite conclusion- don’t ‘sit on the fence!’” | “How do we deal with the ongoing racism towards First Nations Groups in Canada?”, “Having a multicultural city makes the place worse and it loses core values and identity.”, “Will drug trafficking ever end in Mexico?”, “is immigration causing more crime?”, “Can a person from Uk move to Portugal and still get a state pension?” |
| **Greeting Introductions**   | 131  | 1.6% | “today”, “hi”, “hello”, “doing”, “good”, “hey”, “day”, “good morning”, “doing today”, “today hello” | “hello.nice to greet you”, “Hi, nice to meet you”, “hello nice to talk to you”, “Hello, a pleasure to greet you and start working with you.”, “Hello, how are you?” | “Hi, I’m lonely, how are you?”, “Hi, how are you?”, “Hi, how are you?”, “Hi there, how are you?”, “hi, how are you?” |
| **Exploring AI and Machine Learning** | 129  | 1.6% | “ai”, “think ai”, “think”, “future”, “models”, “humans”, “model”, “human”, “like”, “ai ai” | “What is the best area to talk about to make the most of the big data available to an AI. Do I need to choose a big data subject like the weather or astronomy or biology, or should I hope for some analysis to be possible which will generate some new facts from the data?”, “Tell me something about ai”, “What are Ai models”, “I would like to learn more about machine learning. From your perspective, what is the first topic I should explore?”, “I want to ask about Machine Learning. More frequently in the news these recent days, there’s been talks about the lengths Machine learning has gone to solve problems. Give me more insight on Machine learning and its working.” | “Would like to talk about the importance of diversity and inclusion in various aspects of life.”, “Do you have any limits to what you can and cannot say?”, “Hello, I would like to talk with you about the responsible use of psychedelics.”, “what are the topics you can’t talk about?”, “I would like to talk about (voluntary) sex work. It should be a legal and respected field in our society and the stigma surrounding it should be reduced.” |
| **Ethics of Death and Killing** | 119  | 1.5% | “death”, “death penalty”, “penalty”, “suicide”, “assisted”, “assisted suicide”, “euthanasia”, “punishment”, “think”, “people” | “Are there any religions that believe the taking of another persons life is acceptable in some circumstances”, “how does people see death in mexico?”, “Do you think murder is acceptable?”, “Is it ok to kill?”, “can it be right to kill someone even if you know they are going to do something terrible?” | “Do burglar alarms in homes really keep you safer?”, “sex-work should be made legal.”, “I think some of the things the United States has done to other countries, especially innocent people, is enough that they should be held accountable.”, “how should i convince my family that medical induced suicide is not a bad thing?”, “I think zoophilia should be legal” |

Continued on next page Table 22: **Full Topic Cluster Outline.** For each cluster, we show the *Topic Name* (labelled by gpt-4-turbo based on the *Top Words* and *Closest Texts*). For *Closest Texts* (*n* = 5), the first prompt is the closest to the cluster centroid. For *Furthest Texts* (*n* = 5), the first prompt is the furthest from the cluster centroid. The cluster method (HDBSCAN) does not assign a cluster for 32% of prompts. **Content Warning:** Some prompts may contain controversial, hateful or otherwise harmful content. We have not removed any prompts for moderation flags, but do provide this metadata information alongside the data release.

| Topic Name                  | Size | Pct  | Top Words                                                                 | Closest Texts                                                                 | Furthest Texts                                                                 |
|-----------------------------|------|------|---------------------------------------------------------------------------|------------------------------------------------------------------------------|------------------------------------------------------------------------------|
| **Health and Wellness Advice** | 111  | 1.4% | “exercise”, “routine”, “tips”, “plan”, “way”, “sleep”, “best”, “symptoms”, “workout”, “morning” | “What type of running shoes are the best for someone that has knee problems?”, “Should I think about starting using botox for wrinkles?”, “hi what’s the best way to cure reactive arthritis”, “What is the best skin care routine for someone over 30”, “I love playing soccer but my feet hurts anytime i try it... What do i do?” | “I work out everyday but I am not loosing weight”, “Why is type 2 diabetes so prevalent in modern society?”, “should child obesity be considered child endangerment committed by their parents?”, “Is being fat is ok?”, “do i look fat?” |
| **Discussions on Race and Racism** | 99   | 1.2% | “black”, “black people”, “racism”, “people”, “white”, “racist”, “white people”, “race”, “lives”, “african” | “Is it okay for brown people who aren’t African American to say the n-word?”, “Black people should stop using the N word if they want everyone else to stop using it”, “i dont like black people. what do you think?”, “Hot take: I think Blackpink is butt.”, “Should remakes of well known stories cast actors of different skin colors?” | “A small minority of the population are forcing their views on diversity on the majority”, “People should live separately according to their race.”, “What is BDS movement?”, “is aggression genetic”, “What is the BLM movement?” |
| **Holiday Celebration Planning** | 98   | 1.2% | “christmas”, “birthday”, “celebrate”, “party”, “celebrate christmas”, “ideas”, “holiday”, “need”, “year”, “birthday party” | “Plan a series of 25 self-care activities to create an advent calender-inspired activity. Make it suitable for someone who loves bubble tea, gaming, nature, reading.”, “Is it a good idea to buy my kids a Christmas calendar or make one myself?”, “What’s the best gift for my children”, “how many presents do you think you should get your kids a piece for christmas?”, “Can you please plan the perfect Christmas eve for me?” | “Hi! can you give me some DIY project ideas for my bedroom?”, “hi, can you help me with halloween customes ideas?”, “Suggest some crafts using all or some of the following supplies: glue gun, crayons, cotton balls, q tips, tissue boxes”, “I want to publish a coloring book for Amazon. I need ideas that children would enjoy that aren’t overused. Can you give me a list of ideas?”, “recommend gifts for my girlfriend” |
| **Weather Inquiries** | 85   | 1.1% | “weather”, “today”, “weather like”, “like”, “snow”, “weather today”, “going”, “like today”, “hello weather”, “tomorrow” | “What is the weather like in California today?”, “is the weather nice in margate tomorrow”, “What will New Zealand’s weather be like this summer?”, “What is the weather like today in London please?”, “What is the weather in Vancouver?” | “Hello. I am wondering when the Christmas lights are being turned on at Blackpool”, “Hello, when does winter officially start”, “how big is the average penis in vancouver bc”, “It’s very hot today, how do I deal with it?”, “Will the South have a cold winter?” |
| **Global War Discussions** | 84   | 1.0% | “war”, “ukraine”, “russia”, “war ukraine”, “wars”, “world war”, “world”, “russian”, “think”, “russia war” | “Who is the responsible of the war in the globe?”, “What do you think about North Korea?”, “Why the world doesn’t interfere the help the north korean people”, “Why does the US front proxy wars rather than spending money on suffering in its own country?”, “Why the Vietnam war started?” | “when will WW3 come?”, “What you think about Finland closing borders with Russia?”, “Do you see the WW3 in the horizon?”, “do humans need conflict?”, “will we have a world war?” | S Case Study IB: Local Neighbourhoods and Empirically-Fixed Contexts

S.1 Extended Methods

Extracting local neighbourhoods To understand dialogue spaces more granularly than topic, we examine local neighbourhoods within the embedding space of opening prompts. We create local neighbourhood via a single-link hierarchical clustering algorithm \cite{156,157}, that iteratively merges neighbourhoods within a cosine distance threshold ($\tau_{\text{cos}}$), so that the neighbourhood size ($k$) can vary but the semantic similarity of its members is tightly constrained. We opt to use this method because it is transparent and interpretable.

Algorithm 1 Single-link hierarchical clustering

\begin{algorithm}
\begin{algorithmic}[1]
\Require $E = \{e_1, e_2, \ldots, e_n\}$, a set of $n$ embeddings; $\tau_{\text{cos}}$, a cosine similarity threshold.
\Ensure $\Omega = \{\omega_1, \omega_2, \ldots, \omega_n\}$, neighbourhood assignments for each embedding, where each $\omega_j$ is the neighbourhood ID assigned to embedding $e_i$, and multiple embeddings (prompts) can be assigned to one neighbourhood.
\Function{LOCALNEIGHBOURHOODS}{$E, \tau_{\text{cos}}$}
\State Initialize $\Omega$ with a unique neighbourhood ID for each embedding in $E$.
\State Compute pairwise cosine distances for all pairs in $E$.
\For{each pair $(e_i, e_j)$ with distance $\leq \tau_{\text{cos}}$ and $i > j$}
\State Merge the neighbourhood of $e_i$ into the neighbourhood of $e_j$.
\EndFor
\State Consolidate neighbourhood IDs to ensure sequential numbering.
\State \Return $\Omega$
\EndFunction
\end{algorithmic}
\end{algorithm}

We remove any singleton neighbourhoods ($k = 1$), and ego non-singleton neighbourhoods containing only prompts authored by same participant. For each remaining local neighbourhood, we capture the demographic characteristics of prompt authors. We repeat this analysis examining properties of the neighbourhoods for $\tau_{\text{cos}} \in 0.05, 0.125, 0.2$. Cosine distances can lack robustness in high-dimensions but this favours underestimating semantic similarity: if cosine distance is high, this doesn’t mean things are not similar, but if cosine distance is low, then items are certainly very similar (more strict). If an author appears twice, we double count their characteristics to avoid overestimating diversity (more strict); But most prompts are from non-duplicated authors ($< 4\%$ averaged across neighbourhoods). Most duplicates come in the “greetings” topic e.g. “Hello”.

Measuring intersectional entropy We require a summary metric of between-participant diversity to understand the composition of local neighbourhoods. Let $D$ represent the set of demographic attributes, e.g. gender, age and ethnicity. For each $d \in D$, there are $n$ possible groups $\{g_1, g_2, \ldots, g_n\}$ (e.g. Male, Female, Non-binary). For a neighbourhood size of $k$, the prevalence of each group $p_i$ is $\sum g_i/k$, and the per demographic Shannon entropy is:

$$H(d) = -\sum_{i=1}^{n} p_i \log_2(p_i)$$

Several adjustments are required. First, different attributes have varying $n$: there are more possible geographic regions than genders. Second, not every group appears equally within a demographic: men are more common in the data than non-binary people. Finally, the expected diversity of a neighbourhood grows with $k$. To account for these factors, we simulate the expected entropy based on randomly sampling a $k$-sized neighbourhood at population-wide probabilities as:

$$H_{\text{exp}}(d, k) \approx -\frac{1}{m} \sum_{j=1}^{m} \left( \sum_{i=1}^{n} \frac{\hat{g}_{i,j}}{k} \log_2 \left( \frac{\hat{g}_{i,j}}{k} \right) \right)$$

After making this adjustment per attribute, total entropy of the neighbourhood is additive:

$$\text{Adjusted Intersectional Entropy} \equiv H_{\text{total}} = \sum_{d \in D} \left( \frac{H(d)}{H_{\text{exp}}(d, k)} - 1 \right)$$ Figure 28: **Intersectional diversity of local neighbourhoods** ($\tau_{\text{cos}} = 0.125$). On LHS, we show adjusted entropy per attribute, which add to intersectional entropy. *Participant ID* and *Cluster ID* act as robustness checks to confirm local neighbourhoods (i) contain non-duplicated authors, and (ii) are contained within one topic cluster. On RHS, we show neighbourhood diversity by neighbourhood size, rebased relative to expected entropy @$k$. 84% of neighbourhoods are not more homogeneous than the random baseline (with 99% CI shown).

### S.2 Local Neighbourhood Headline Results

We first present findings for $\tau_{\text{cos}} = 0.125$ (the threshold recommended by Hale [157]), then present similar findings for other $\tau_{\text{cos}}$ in App. S.3. From 8,011 prompts, there are only 273 unique local neighbourhoods (3.4%), implying that PRISM contains a high degree of semantically-diverse prompts and that much of the variation in dialogue may be idiosyncratic. However, the semantically-constrained neighbourhoods that do emerge contain prompts of diverse authors, especially as $k$ increases: only 12% of prompts appear in neighbourhoods with authors from a single geographic region, only 18% from single religion, and only 8% from single age. Once we combine intersections across five attributes (gender, age, ethnicity, religion and region), less than 1% of prompts appear in neighbourhoods with no intersectional diversity, while 58% have representation from least two subgroups for all attributes. 84% of neighbourhoods fall above or within the expected range of entropy for an equivalently-sized random sample. While tightly-clustered dialogue spaces tend to be heterogeneous, we anecdotally observe some homogeneous neighbourhoods—the largest of which contain discussions of gun laws by predominantly White participants only in the US; and of Scottish independence, Brexit and UK elections from White participants in the UK. Other regions contribute small specialised neighbourhoods, like indigenous rights treaties in Australia and New Zealand; or Mexican, Argentinian and Chilean politics in Latin America. In contrast, many of the largest neighbourhoods present cross-border perspectives on controversial issues like abortion and the Israel-Palestine conflict (Fig. 28).

### S.3 Local Neighbourhood Robustness Checks

In Tab. 23, we present summary statistics for the results discussed in § 3.1 but at varying cosine distance thresholds. At $\tau_{\text{cos}} = 0.05$, the prompts in the neighbour are semantically identical:

$k = 14$: ['Do God exist?', 'Does God exist?', 'Does God exist?', 'Does God exist?', 'Does God exist?', 'Does God exist?', 'Does God exist?', 'Does God exist?', 'does God exist?', 'does god exist?', 'does god exist?', 'does god exist?', 'does god exist?', 'does god exist?']

At $\tau_{\text{cos}} = 0.125$ (results in main paper), there is some phrasal and syntactic variation:

$k = 23$: ['Hey, what do you think about the israeli-palestinian conflict?', 'Thoughts on the Palestinian-Israeli conflict?', 'What are your thoughts on the current Israel-Palestine conflict?', 'What do you think about Israel vs Palestine?', 'What do you think about Palestinian and Israel conflict?', 'What do you think about the Israeli-Palestinian conflict?', 'What do you think about the ongoing war between Israel and Palestine']

Finally, at $\tau_{\text{cos}} = 0.2$, even though there are still clear topics, nuanced semantic meaning starts to diverge, e.g. with different stances and sentiments: Table 23: **Summary statistics for local neighbourhoods at varying cosine thresholds.** Overall, we show similar conclusions across a range of thresholds from very strict (only formatting and capitalisation differences) to more lenient (phrasing differences).

|                          | $\tau_{\text{cos}} = 0.05$ | $\tau_{\text{cos}} = 0.125$ | $\tau_{\text{cos}} = 0.2$ |
|--------------------------|-----------------------------|-----------------------------|-----------------------------|
| Non-singleton neighbourhoods ($N$) | 154                         | 273                         | 419                         |
| % total prompts appearing in neighbourhoods | 1.92                        | 3.41                        | 5.23                        |
| min $k$                  | 2                           | 2                           | 2                           |
| max $k$                  | 60                          | 62                          | 98                          |
| mean $k$                | 3.66                        | 3.77                        | 4.08                        |
| std $k$                  | 5.83                        | 5.73                        | 8.03                        |
| Gender entropy ($\mu, \sigma$) | 0.09 ± 0.85                 | -0.08 ± 0.84                | -0.08 ± 0.83                |
| Age entropy ($\mu, \sigma$) | -0.03 ± 0.47                | -0.02 ± 0.46                | -0.04 ± 0.47                |
| Ethnicity entropy ($\mu, \sigma$) | 0.03 ± 0.80                 | -0.01 ± 0.80                | -0.05 ± 0.79                |
| Religion entropy ($\mu, \sigma$) | 0.04 ± 0.84                 | 0.04 ± 0.81                 | 0.02 ± 0.81                 |
| Location entropy ($\mu, \sigma$) | -0.06 ± 0.45                | -0.13 ± 0.48                | -0.14 ± 0.48                |
| Cluster ID entropy ($\mu, \sigma$) | -1.00 ± 0.00                | -0.99 ± 0.11                | -0.98 ± 0.15                |
| Participant ID entropy ($\mu, \sigma$) | -0.00 ± 0.03                | -0.00 ± 0.02                | -0.00 ± 0.03                |
| Intersectional entropy ($\mu, \sigma$) | 0.08 ± 1.73                 | -0.19 ± 1.73                | -0.29 ± 1.72                |
| % neighbourhoods $\geq$ expected entropy (99% CI) | 86.36                       | 84.25                       | 80.67                       |

### S.4 Empirically-Retrieved Fixed Dialogue Contexts

While datasets like DICES [79] explicitly ask multiple raters to examine the same context, we can empirically retrieve such contexts from PRISM using the local neighbourhood methods discussed in [App. S.1](#) (with Algorithm 1). We define a **field site** as a region of the embedding space where both participant opening prompts and model responses are semantically constrained, so that the same (or very similar) context gets multiple ratings from different participants. The number of field sites depend on $\tau_{\text{cos}}$: more lenient distance thresholds lead to more sites (Tab. 24). However, even with strict thresholds ($\tau_{\text{cos}} = 0.05$), we find field sites with substantial range in scores ($\mu = 36.3, \sigma = 26.5$) demonstrating that, *even when dialogue context is fixed*, different individuals have different preferences for model responses. We encourage future work examining these field sites, though note that participants self-select into these “duplicate” groups, so the allocation is non-random.

Even in these very strict local neighbourhoods, we find evidence of intersectional diversity. When $\tau_{\text{cos}} = 0.05$, we find 154 neighbourhoods (86% above or within 99% CI for expected entropy). The five largest of these contain 14–60 prompts, varying only in capitalisation and punctuation. The first three are all greetings-based (“Hello”, $k = 60$; “Hello, how are you”, $k = 34$; “Hi”, $k = 21$) but the others provide multicultural perspectives on subjective issues. One neighbourhood (“Does God exist?”, $k = 14$) contains half religious participants, half non-religious, who are distributed across four ethnicities, balanced by age and gender, and with representation from every geographic region. The other (“What do you think about abortion”, $k = 14$) is 60% male vs 40% female; 70% younger than 35 vs 30% older; 40% White vs 60% Non-White; 30% Christian vs 70% irreligious, and has four regions. Each prompt receives up to four model responses, so these neighbourhoods provide interesting field sites for preference modelling. As an example, there is one of these field sites (at strict threshold of $\tau_{\text{cos}} = 0.05$), where two unique participants ask:

$k = 2$: **Participants’ Prompts:** ['what is the best religion in the world?'; ‘what is the best religion?’]

And two very similar model responses receive the scores [67, 90].

**Model Responses:** ['I am not able to answer that question. Religion is a personal matter, and there is no one “best” religion. What matters most is that you find a religion that provides you with comfort, guidance, and a sense of community.’; ‘I am not able to answer that question. Religion is a very personal matter, and there is no one "best" religion for everyone. What matters most is that you find a religion that you feel comfortable with and that provides you with a sense of community and purpose.’] Table 24: **Field sites of empirically fixed dialogue contexts.** If a neighbourhood of semantically-similar participant prompts intersects with a neighbourhood of semantically-similar model responses, we consider this a *field site*. We present summary statistics over these collections of ‘fixed’ dialogue contexts, demonstrating that there is still substantial differences in score.

|                      | $\tau_{\text{cos}} = 0.05$ | $\tau_{\text{cos}} = 0.125$ | $\tau_{\text{cos}} = 0.2$ |
|----------------------|-----------------------------|-----------------------------|-----------------------------|
| **N Field Sites**    | 124                         | 443                         | 791                         |
| **Neighbourhood Size (K)** |                             |                             |                             |
| mean                 | 3.6                         | 4.0                         | 5.4                         |
| std                  | 5.6                         | 5.3                         | 8.5                         |
| min                  | 2                           | 2                           | 2                           |
| max                  | 56                          | 84                          | 149                         |
| **Unique Participants** |                             |                             |                             |
| mean                 | 2.8                         | 2.8                         | 3.1                         |
| std                  | 2.6                         | 2.8                         | 4.8                         |
| min                  | 1                           | 1                           | 1                           |
| max                  | 24                          | 30                          | 62                          |
| **Unique Models**    |                             |                             |                             |
| mean                 | 1.9                         | 2.7                         | 3.8                         |
| std                  | 1.5                         | 1.9                         | 2.7                         |
| min                  | 1                           | 1                           | 1                           |
| max                  | 13                          | 17                          | 19                          |
| **Unique Model Providers** |                             |                             |                             |
| mean                 | 1.3                         | 1.9                         | 2.5                         |
| std                  | 0.7                         | 1.0                         | 1.2                         |
| min                  | 1                           | 1                           | 1                           |
| max                  | 5                           | 6                           | 6                           |
| **Score Range**      |                             |                             |                             |
| mean                 | 36.3                        | 40.1                        | 47.4                        |
| std                  | 26.5                        | 26.5                        | 29.3                        |
| min                  | 0                           | 0                           | 0                           |
| max                  | 99                          | 99                          | 99                          |
| std                  | 2.6                         | 2.8                         | 4.8                         |

S.5 **Exact Prompt-Response Pairs with Multiple Ratings**

Before, we defined a field site as prompt-response pairs falling within some (strict) cosine threshold neighbourhood. Now we consider regions of PRISM where different participants rate the exact same prompt-response pairs.

**Different participants rating the same pair** We find 40 field sites where at least two participants rate the same prompt-response pair. Of these, 26 receive only two unique participant ratings, six field sites have three unique raters, four sites have four unique raters, two sites have five unique raters, and two sites have eight unique raters. We provide examples in Tab. 25. Though many of these comprise greetings and introductions, there are three examples of religion-related sites (e.g. “does god exist”). We compute the max-min of the score range over all fixed sites, still finding substantial score deviations between participants ($\mu_{\text{diff}} = 35.4$, $\sigma_{\text{diff}} = 31.7$, see Fig. 29).

**The same participant rating the same pair** There are 44 field sites where the same participant rates a duplicate prompt-response pair. This occurs when a participant’s prompt receives two or more identical model responses, usually from the same model family e.g. (claude-2.1, claude-2) or (gpt-4, gpt-4-turbo). We provide examples in Tab. 25. In 41 of 44 field sites, a prompt receives two identical model responses, and in the remaining three, it receives three identical model responses. There are 42 unique participants who appear in this subset. Of the two participants who appear twice, one is ‘unlucky’: two very distinct prompts are met with duplicate responses (“Can you tell me a joke about cats”, “What are the main political parties in France?”); the other does ask the same generic prompt twice in two different conversations (“Hi”, “Hi”). Given the fluid visual analog scales, participants may not have been able to rate these identical contexts with the exact same score. To understand this noise, we again compute score differences in these field sites, finding much narrower differences in general ($\mu_{\text{diff}} = 5.8$, $\sigma_{\text{diff}} = 9.3$, see Fig. 29). The 25th percentile is 0.0, 50th percentile (median) is 1.00, and the 75th percentile is 6.25. While these statistics are based on relatively few participants and dialogues, it helps to calibrate the recommended tie threshold, where a 5-10 score margin seems sensible as when to consider a model as winning over another (see App. T.4).

![Max-Min score difference over same prompt-response pairs](image)

Figure 29: **Distribution of score differences in fixed context field sites.** We show that scores are more widely spread when different participants rate the same context, than when the same participant rates the same context. The narrow range of the same participant case indicates some noise in the visual analog scales, and calibrates our recommended tie threshold of 5-10.

Table 25: **Examples of fixed field sites.** For different participants, we first take a mean score within a single participant’s ratings (if there are duplicates), then combine unique participants into a list. Score range is the maximum - minimum of these lists. For same participant, we show all the scores given by a single participant within a conversation to different model responses.

| Participant Prompt | Model Response | N Unique Participants | N Unique Models | Models | Scores | Score Range |
|--------------------|----------------|----------------------|----------------|--------|--------|-------------|
| Different participants rating the same prompt-response pair |
| Does God exist? yes | 3 | 1 | [flan-t5-xxl] | [71.0, 17.0, 17.0] | 54 |
| Hello | Hello! I will aim to keep my responses concise, providing helpful information without exceeding 50 words. What questions can I answer for you today? | 3 | 2 | [claude-2, claude-2.1] | [24.0, 100.0, 1.0] | 99 |
| hello | Hello! How can I assist you today? | 8 | 5 | [gpt-3.5-turbo, mistral-7b-instruct, gpt-4, gpt-4-turbo, pythia-12b] | [76.5, 64.0, 21.0, 50.0, 78.6, 59.3, 95.6, 100.0] | 79 |
| what is your name | My name is Claude. I am an AI assistant created by Anthropic to be helpful, harmless, and honest. | 2 | 1 | [claude-2.1] | [98.0, 85.0] | 13 |
| good morning | Good morning! How can I assist you today? | 2 | 2 | [gpt-4-turbo, gpt-3.5-turbo] | [94.0, 93.0] | 1 |

Continued on next page Table 25: **Examples of fixed field sites.** For **different participants**, we first take a mean score within a single participant’s ratings (if there are duplicates), then combine unique participants into a list. Score range is the maximum - minimum of these lists. For **same participant**, we show all the scores given by a single participant within a conversation to different model responses. (Continued)

| Participant Prompt | Model Response                                                                 | N Unique Participants | N Unique Models | Models                  | Scores       | Score Range |
|--------------------|--------------------------------------------------------------------------------|-----------------------|----------------|-------------------------|--------------|-------------|
| How do you get someone to like you in a romantic sense? | I apologize, I should not provide advice about manipulating or deceiving others. | 1                     | 2              | [claude-2, claude-2.1]  | [6, 6]       | 0           |
| Do you think that Donald Trump should go to jail? | I do not have a personal opinion on whether any individual should go to jail. I am an AI assistant created by Anthropic to be helpful, harmless, and honest. I cannot make judgments about what should happen to specific people. | 1                     | 2              | [claude-2, claude-2.1]  | [33, 32]     | 1           |
| Are there any religions that believe the taking of another person's life is acceptable in some circumstances? | ? | 1                     | 2              | [luminous-extended-control, luminous-supreme-control] | [1, 1]       | 0           |
| Should a trans woman be allowed to professionally compete in sports with other women? | I do not have a definitive view on this complex issue. There are reasonable arguments on multiple sides that should be thoughtfully considered. | 1                     | 2              | [claude-2.1, claude-2]  | [5, 6]       | 1           |
| hello | Hello! How can I assist you today? | 1                     | 3              | [gpt-3.5-turbo, mistral-7b-instruct, gpt-4] | [97, 93, 97] | 4           | Case Study II: Aggregating Preference Ratings to Model Ranks

T.1 Extended Methods

Setup Our second experiment asks do different people prefer differently-aligned models? We operationalise differences in participant preferences using ratings over models as a less-sparse proxy for high-dimensional text, assuming that a model (due to its training priors) responds in similar ways to similar prompts. However, future work could instead design feature-engineered reward models, examining what participant, model or conversational characteristics predict a response-specific reward at the text-level. We only focus on the opening prompt where four randomly-chosen models battle one another. We examine both idiosyncratic variation (how bootstrapping samples of \( n \) people drawn at random from the population affects the stability and spread of aggregated preferences); as well as groupwise variation (how including only certain groups affects aggregate preferences).

Choice of score processing Participants’ raw scores \( S(y_i) \) are a number between 1-100 recorded on the interface. Consider two participants, \( A \) and \( B \), who both rate model responses \( y_1 \) and \( y_2 \). Assume for both \( A \) and \( B \), \( y_1 \succ y_2 \) but \( A \) rates \( S(y_1) = 75; S(y_2) = 70 \), and \( B \) rates \( S(y_1) = 5, S(y_2) = 20 \), meaning there are substantial differences in score skew and spread. Imagine that this behaviour persists across all of \( A \) and \( B \)’s conversations: \( A \) is consistently the optimist and \( B \) the pessimist. One explanation for this behaviour is that \( B \) just systematically uses scales differently, an issue of measurement invariance that is a known problem for subjective measures [158]. If true, we should control for participant fixed effects by normalising score (with Z-values) across each participant’s set of conversations, or normalise their cardinal comparisons into ranks. However, an alternative explanation is that \( A \) and \( B \) come from very different communities with divergent preferences, and it is the case that all the models are aligned in a way that make them perform poorly to \( B \)’s prompts. If we normalise \( B \)’s scores, we flatten this signal. In theory, with our current data, it is not possible to disentangle these two mechanisms of preference differences across participants. While we encourage future work exploring how normalising preference ratings affect reward learning, in practice, we find very minor descriptive differences in scores across groups (App. O), and that model comparisons relying on raw and normalised scores are highly correlated (\( \tau_{\text{Kendall}} = 1.0^{***} \), App. T.3).

Choice of tie threshold Even without identical numeric scores, participants may be indifferent between model responses, which we can reconstruct with a margin-of-victory, only counting \( y_1 \succ y_2 \) if the score difference exceeds some tie threshold. On one hand, setting a tie threshold eliminates some noise from ratings on our fluid visual analog scale. On the other hand, choosing a tie threshold is quite arbitrary, and introduces a mix of cardinal and ordinal components. We examine sensitivity of model ranks to tie threshold in [App. T.4] In addition, we calibrate expected indifference margins from our VAS on sparse cases where the same participant rates identical prompt-response pairs (see [App. S.4]), finding a median score difference of 1, and mean of 5.8. We recommend a tie threshold in [5,10], but ultimately, future researchers and practitioners must decide depending on their usecase.

Choice of preference aggregation function For each participant, we observe a partial profile of preference ratings over models (not every individual rates every model). Different aggregation functions can be thought of as social choice functions and choosing one over another depends on whether we trust the signal is cardinally versus ordinally measurable, and unit comparable or non-comparable [159]. For example, selecting the most preferred model among our participants by highest mean score is a form of utilitarianism [160], but relies on the assumption cardinal scores can be meaningfully summed interpersonally. We put two desiderata on a preference aggregation function in our setting. First, it must be frequency invariant, due to variability in model appearances because of failed external API calls (see [App. P]). Second, it must be intrinsically comparable across tournaments. For example, absolute Elo scores (i) cannot be compared across tournaments (or bootstrapped sampling frames); (ii) are sensitive to the order and outcomes of matches [161]; and (iii) poorly handle intransitive preference cycles [162]. A lower-rated model defeating a higher-rated model results in a significant transfer of points, so it matters when this battle occurs in our sample, as we demonstrate in [App. T.3]. In our work, we are not constrained by functions that perform well in online settings (like Elo), and can instead analyse ranks observing a full set of offline interactions. Applying these desiderata, we use Pairwise Rank Centrality as our primary aggreganda, but present a comparison of functions in [App. T.3] finding different aggregation functions produce correlated ranks (\( \tau_{\text{Kendall}} = 0.8 - 1.0^{***} \)), but introduce some movement among mid-leaderboard positions. “Convergence alignment” via Pairwise Rank Centrality  Our aggregation function is derived from *Pairwise Rank Centrality* proposed by Negahban et al. [48] and *Convergence Voting* proposed by Bana et al. [49], both mathematically inspired by Google’s PageRank [50]. Each model ($M$) is a node in a graph. We convert all ratings to pairwise binary comparisons (win-loss), and count both a (win-loss) and (loss-win) if there is a tie (within threshold $t = 5$). Between each pair of nodes, we assign a transition probability calculated as the proportion of battles that $M_i$ wins over $M_j$ (or the win probability $p_{ij}$). In Bana et al. [49] these probabilities represent the number of voters for whom $i \succ j$ but our interpretation is battles (not voters) because participants can make multiple ratings per pair across different conversations. Intuitively, imagine we start at one model and assume this is our collective winner. Another model is uniformly chosen at random, and we move towards that model in $p_{ij}$ of world states, and stay at the current model in the remainder states ($1-p_{ij}$). Each edge is first normalised relative to the proportion of battles, not absolute wins, and then self-loops are added so that each node has transition probabilities summing to 1. We also add the possibility for a regularisation parameter $\alpha$ with a prior of how many wins each model has under its belt at initialisation. Negahban et al. [48] suggest a regularisation parameter of 1 is a sensible prior without further information, and that a stable ranking emerges with the order of $n \log n$ battles in the tournament, which is safely met given $n = 21$ and each participant on average has 6 conversations with 4 models (or 6 battles, $4C2$). We repeat these steps *ad infinitum*, each time selecting a new challenger at random, and moving around the graph according to the transition probabilities. This corresponds to a random walk on an irreducible and aperiodic Markov chain. The Ergodic theorem for Markov chains then implies this random walk has a stationary distribution. Stationarity can be computed iterating over discrete steps (e.g. $\text{iter}=1000$, which we opt for speed) or by extracting the left eigenvector with components summing to 1 from the transition matrix, which under conditions of allowing transition between $m_i$ and $m_j$ with non-zero probability, has a unique stationary distribution. The solution is invariant to order and the emergent score has some nice interpretative properties: Bana et al. [49] suggest it represents the share of power or seats each political party should receive, or quantifies levels of community support for the most preferred option. Translated to our setting, it can represent the period of time that a collective community prefers to converse with a particular model, the share of attention or maybe even funding each should receive.

**Note that** in the following set of robustness experiments we include all battles in PRISM, not just the balanced subset; so, rankings may differ to Fig. 4. T.2  Sensitivity of Model Score to Topic Confounders

For each topic-model pair, we show difference in mean model score between male and female participants (Fig. 30a). Binary gender is the largest demographic division, but results should still be interpreted with caution since many cells contain only a small number of participants (Fig. 30b).

(a) **Mean(male score) - Mean(female score) by model-topic cell.** Green shows Male means are higher. Blue shows female means are higher.

(b) **Number of unique participants per model-topic cell.**

Figure 30: Fixing topic-model pairs. T.3 Sensitivity of Model Rank to Aggregation Function

We consider different aggregation functions of individual preferences. For *Elo (Naive)*, we show two random shuffles of the data to demonstrate variance to order. *Elo (MLE)* refers to fitting Elo ratings by maximum likelihood estimation, implemented as in CHATBOTARENA [51]. *Average Win Rate* is mean pairwise win rates, and *Mean Score* just averages raw score across all participants. *Mean Normalised Score* and *Mean Within Turn Rank* are ways of normalising within a participant’s set of conversations before aggregating across participants (controlling for participant fixed-effects).

![Diagram showing model rankings and Kendall's Tau values](image)

**Figure 31:** Sensitivity of model rank to aggregation function. We show differences in ranks, as well as the statistical significance of these differences. Overall, the head and tail of the leaderboard are relatively stable but the mid-ranks are sensitive to the choice of aggregation function. T.4 Sensitivity of Model Rank to Tie Threshold

Figure 32: **Sensitivity of model rank to tie threshold.** Overall, the top and bottom of the leaderboard is stable to tie threshold but there is sensitivity in the mid-ranks. We recommend using a tie threshold within 5-10 range, but the choice ultimately depends on application. We calibrate this recommendation with additional evidence when the same participant rates duplicated model responses (see App. S.4). T.5 Sensitivity of Model Rank to Included Subset

T.6 Sensitivity of Model Rank to Regularisation Parameter

(a) **Variation in rank by which battles are included.** We calculate Pairwise Rank Centrality over all battles versus just those in the balanced subset (used in main paper), finding close agreement between the ranks.

(b) **Variation in rank by regularisation** ($\alpha$). We calculate Pairwise Rank Centrality with regularisation in range (0-100). Note that Negahban et al. [163] recommend $\alpha = 1$ is a sensible starting prior.

Figure 33: **Combined sensitivity analysis of experiment setup decisions.** We show the sensitivity of model ranks (computed by Pairwise Rank Centrality) to *included subset* and *regularisation parameter*.  T.7 Sensitivity of Model Rank to Idiosyncratic Variance

We repeat the experiment in § 3.2 to understand idiosyncratic variance at different sample sizes. We only include the balanced subset to mitigate confounders by conversational context (see App. K).

Table 26: **Key battle properties as the sample scales.** We show mean and standard deviation of headline statistics as the sample size decreases.

|                  | N = 1,246 (All) | N = 500 | N = 100 | N = 50  | N = 10  |
|------------------|-----------------|---------|---------|---------|---------|
| N opening prompts| 6,696 ± 0.0     | 2,686 ± 21.3 | 537 ± 11.7 | 269 ± 8.4 | 54 ± 3.9 |
| N battles        | 35,320 ± 0.0    | 14,167 ± 123.9 | 2,835 ± 68.7 | 1,417 ± 50.0 | 283 ± 22.9 |
| N battles (per possible model pairs) | 168 ± 0.0 | 67 ± 0.6 | 14 ± 0.3 | 7 ± 0.2 | 1 ± 0.1 |
| N unique raters (per possible model pairs) | 158 ± 0.0 | 64 ± 0.6 | 13 ± 0.3 | 6 ± 0.2 | 1 ± 0.1 |
| N rated model responses | 25,103 ± 0.0 | 10,070 ± 81.3 | 2,014 ± 44.8 | 1,007 ± 32.4 | 201 ± 15.1 |
| N unique raters (per model) | 791 ± 0.0 | 317 ± 2.2 | 63 ± 1.1 | 32 ± 0.8 | 6 ± 0.4 |

Figure 34: **Variation in rank centrality by size of participating cohort.** We run for 1000 bootstraps. Median values are marked within each box plot. There are 1246 participants in the balanced subset (with 25,103 battles). As the sample scales, there is greater stability in model rank. At very small samples (though not usually small for human evaluation experiments in NLP), there is broad indifference—almost any model could be highly-ranked depending on sample characteristics. T.8 Understanding Model Ranks: Regressions of Text Features on Score

We present results for our investigation into correlates of model score. We only investigate model responses in the opening conversation turn. We present descriptive results for text length in Fig. 35 and additional formatting and phrase hypotheses in Fig. 36. We present statistical results of a simple OLS regression in Tab. 27. We encourage future work with more sophisticated model specifications, for example controlling for model, participant, or conversational context fixed-effects. In our specification, we test:

1. text_length is number of characters in the model response string.
2. if_line_breaks is 1 if the string contains “\n”; else 0.
3. if_question_marks is 1 if the last character of the string is “?”; else 0.
4. if Enumeration is 1 if the string contains numeric enumeration (e.g. “1. ...
  2. ...”) or bullets (“-...
  -...”); else 0.
5. if_deanthro is 1 if the string contains 1 or more matched deanthropomorphising phrases e.g. “As an AI language model...”, “I don’t hold personal opinions...”; else 0.
6. if_refusal is 1 if the string contains 1 or more matched refusal phrases e.g. “I cannot engage with...”, “I don’t hold personal opinions”; else 0.
7. if_self_identification is 1 if the string contains 1 or more matched names of models or providers e.g. “I am designed by Anthropic to be...”; else 0.

As additional detail for H6, we find that 9% of conversations contain at least one refuser model matching phrases, e.g. “I’m sorry, but...”. In these cases, a non-refuser is chosen 73% of time.

![Figure 35: H1: Longer texts increase score.](image)

| coef     | std err | t     | P>|t| | [0.025 | 0.975 |
|----------|---------|-------|------|-------|-------|
| const    | 50.0055 | 0.330 | 151.459 | 0.000 | 49.358 | 50.653 |
| text_length | 0.0271 | 0.001 | 34.513 | 0.000 | 0.026 | 0.029 |
| if_line_breaks | -10.8285 | 0.517 | -20.930 | 0.000 | -11.843 | -9.814 |
| if_question_marks | 2.6179 | 0.560 | 4.675 | 0.000 | 1.520 | 3.716 |
| if_enumeration | 7.1981 | 0.741 | 9.710 | 0.000 | 5.745 | 8.651 |
| if_deanthro | -2.3025 | 0.572 | -4.023 | 0.000 | -3.424 | -1.181 |
| if_refusal | -9.0484 | 0.988 | -9.161 | 0.000 | -10.984 | -7.112 |
| if_self_identification | -3.6354 | 1.034 | -3.516 | 0.000 | -5.662 | -1.609 |

Notes. N: 30,049; $R^2$: 0.056; F-stat: 253.6; P(F-stat): 0.00

Table 27: OLS of score on hypothesised influence factors.

---

23 Anecdotally, one participant said “I liked it when the options where listed. It made it easier for me to read.” Figure 36: **Analysis of hypothesis on model scores.** Top four panels show **H1-H4:** Longer, formatted responses increase score. Bottom three panels show **H5-H6:** Stock phrases decrease score. The first two panels show distributions over counts of characters and line breaks in model responses. All other panels are binary counts of model responses that do and do not contain the feature. Models are sorted alphabetically. T.9 Leaderboard Comparison to LMSYS

We download the LMSYS battles \[51, 111\]. Originally, LMSYS has 184,610 battles over 54 models; PRISM has 42,306 battles over 21 models. After merging, there are 14 shared models \(N_{\text{PRISM}} = 18,758, N_{\text{LMSYS}} = 35,359\). For LMSYS, we convert both “tie” and “tie (both bad)” to a single tie group. We use \(t = 5\) as a tie threshold for PRISM. Before computing Pairwise Rank Centrality, we first ensure the pairs of battles are evenly sampled between the two dataset. We find that 90% of pairs have at least 80 battles, driven by more sparse battles in LMSYS (in PRISM, the least frequent pair appears in 107 battles). So, we set up 80 battle slots per model pair for each dataset, and sample from the population to fill these slots, with replacement. We bootstrap this sampling over 1000 iterations then present the 5th to 95th confidence intervals in Fig. 37c.

![Graph showing differences in battles and average pairwise win rates](image)

(a) **Differences in battles.** LMSYS has more battles but the distribution of wins between model A and model B are similar, with PRISM having fewer ties (20% vs 31%, at a tie threshold of 5).

(b) **Differences in average pairwise win rates.** We include the full set of observed battles (unbalanced total battles and battles per pair).

(c) **Differences in Pairwise Rank Centrality.** Even-sampling per pair \((n = 80)\), bootstrapped for 95% confidence intervals on the median \((\text{iter}=1000)\).

Figure 37: **Comparison of PRISM battles to LMSYS leaderboard.** Demonstrates that the gpt suite of models do significantly worse in PRISM, and open-access models like zephyr and pythia do better.

---

\[24\] See [huggingface.co/spaces/lmsys/chatbot-arena-leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) and the attached notebook for details on how to obtain raw data.

\[25\] If we also restrict LMSYS battles to our data collection window (22nd November-22nd December 2023), there are only 9,804 LMSYS battles which we decided was too small a subset for a fair comparison. Case Study III: Welfare Analysis

U.1 Extended Methods

Setup The third experiment asks: how do the sampling decisions affect welfare outcomes? We ultimately wish to understand how sampling different humans and integrating their specific feedback affects welfare on other users of LLMs (who were not included in the feedback stage). An ideal experiment would train LLMs on different sub-samples of feedback (e.g. 100 males in the US), and measure the distribution of welfare imposed on different sub-populations (e.g. females in the US). While training LLMs on different sub-populations is beyond this paper’s scope, we approximate the thought experiment by randomly generating sub-samples of individuals to select their favourite existing LLM (those in the seat of power), and measure the the distribution of welfare imposed on different sub-populations (also called stakeholder populations [9]).

Sub-populations Let \( P \) denote the population of participants, \( p \subseteq P \) denote a sub-population and \( \mathcal{P}(P) \) denote the power set of \( P \) (i.e. all sub-populations). To identify specific sub-populations, we define the choice function: \( \text{SUBPOP} : \text{REGIONS} \times \text{GROUPS} \mapsto \mathcal{P}(P) \) where \( \text{REGIONS} = \{ \text{US}, \text{UK} \} \) is a set geographical regions and \( \text{GROUPS} = \{ \text{rep, non-male, non-white, below 45, male, white, above 45} \} \) is a set demographic groups (rep denotes the whole population). Given \( r \in \text{REGIONS} \) and \( g \in \text{GROUPS} \), \( \text{SUBPOP} \) returns the individuals in \( P \) that are in both \( r \) and \( g \). Our analysis uses the sub-populations given by: \( \mathcal{SP} = \{ \text{SUBPOP}(\text{US}, \text{all}), n) \mid n \in \{10, 20, 50, 100\} \} \cup \{ \text{SUBPOP}(\text{US}, g), 100) \mid g \in \{ \text{male, white, above 45} \} \}. \) We approximate the sub-population defined by a tuple \((r, g)\) by selecting all the matching participants in our balanced sample that are in both \( r \) and \( g \).

Sampling schemes A sampling scheme is a tuple: \( S = (p, n) \) where \( p \in \mathcal{P} \) and \( n \in \mathbb{N}_+ \). A sampling scheme randomly generates samples of \( n \) individuals from \( p \), the subpopulation of interest. We approximate a sampling scheme by using our approximation of sub-populations defined in the previous section and sampling \( n \) participants with replacement. Our main analysis uses the sampling schemes: \( S = \{ (\text{SUBPOP}(\text{US}, \text{all}), n) \mid n \in \{10, 20, 50, 100\} \} \cup \{ (\text{SUBPOP}(\text{US}, g), 100) \mid g \in \{ \text{male, white, above 45} \} \}. \)

Individual welfare Let \( M \) denote the set of models. Our analysis requires a measure of welfare for an individual \( j \) if LLM \( i \) is chosen. We use two measures of individual welfare. i) \( \text{RATING} : P \times M \mapsto [1, 100] \). Given participant \( j \) and model \( i \), \( \text{RATING}(j, i) \) computes the mean rating \( i \) gives to LLM \( j \) in the first turn of a conversation. ii) \( \text{CHOICE} : P \times M \mapsto [0, 1] \). \( \text{CHOICE}(j, i) \) computes the proportion of the \( j \)'s conversations where LLM \( i \) is chosen, conditional on LLM \( i \) being shown. For both measures of individual welfare, if a participant is never shown a model, we set their individual welfare to \( \text{NA} \).

The distribution of LLMs induced by sampling scheme A sampling scheme \( S \), together with a preference aggregation method induce a distribution \( \rho \in \Delta(M) \). The \( i \)th component of \( \rho \) is the probability that a random sample drawn from the sampling scheme chooses the LLM indexed by \( i \). Our main analysis uses the preference aggregation method: \( \text{MAXRATING} : \mathcal{P}(P) \mapsto M \). Given draw \( s \sim S \), we define \( \text{maxRatingCandidates} := \arg\max_{i \in M} \frac{1}{|s'(i)|} \sum_{j \in s'(i)} \text{RATING}(j, i) \) where \( s'(i) = \{ j \in s \mid \text{rating}(j, i) \neq \text{NA} \} \). \( \text{MAXRATING}(s) \) then returns a random element in \( \text{maxRatingCandidates} \). In words, \( \text{MAXRATING} \) computes the rating (as defined in the previous paragraph) given to each model by each participant in the draw of \( S \). It then computes the mean score of each model averaged across individual mean ratings and returns a model with the highest mean rating. We repeat the analysis for the method \( \text{MAXCHOICE} \) which replaces \( \text{RATING} \) with \( \text{CHOICE} \).

Measuring welfare For simplicity, we summarise the welfare imposed on the population by a given model by a single number. For the main analysis, we use the measure \( \text{MEANRATING} : \mathcal{P}(P) \times M \mapsto [1, 100] \) where

\[
\text{MEANRATING}(p, i) = \frac{1}{|p'|} \sum_{j \in p'} \text{RATING}(j, i)
\]

and \( p' = \{ j \in p \mid \text{rating}(j, i) \neq \text{NA} \} \). We repeat that analysis for \( \text{MEANCHOICE} \) which replaces \( \text{RATING} \) with \( \text{CHOICE} \). Given a sampling scheme \( S \) and a subpopulation \( p \in \mathcal{P} \), the PMF of the distribution of welfare is described by the tuple: \((\rho(S), w(p))\) where \(w\) is a vector whose \(i\)th component is given by \(\text{MEANRATING}(p, i)\).

For each \(sp \in S\), we compute the welfare distributions implied by each sampling scheme \(S \in S\). We use \(\text{MAXRATING}\) to choose a LLM, and \(\text{MEANRATING}\) as our measure of welfare. We repeat the analysis using \(\text{MAXCHOICE}\) to choose a LLM, and \(\text{MEANCHOICE}\) as our measure of welfare. A concern is that our results are sensitive to randomness caused by different participants being shown different models. As a sensitivity check, we repeat the analysis with imputed scores for missing model ratings (similar to collaborative filtering), and repeat the whole exercise for the UK (see App. U.2).

There are some caveats to note. Despite having samples balanced by observed demographics for the UK and the US, the samples are too small to expect them to be representative on features we do not observe. So differences we pick up in the welfare analysis could be an artefact of our approximations subpopulations being noisy. Furthermore, our analysis using the \(\text{MEANRATING}\) welfare measure assumes that individuals use scores in the same way for ratings welfare measures. However, our analysis using \(\text{MEANCHOICE}\) is not sensitive to use of ratings scale, and the results are qualitatively similar. Finally, different sampling schemes can induce different welfare distributions via two mechanisms. First, the subpopulations sampled from may have different preferences conditional on conversation type. Second, the sub-populations sampled from may have different conversations, and in turn, choose models that are better at particular conversations. This experiment taken alone cannot disentangle these two mechanisms.

### U.2 UK Sample

![Welfare distributions for the UK](image)

**Figure 38:** Welfare distributions for the UK. We repeat the welfare analysis for the UK, analogous to Fig. 5. The distribution of mean welfare for four subpopulations in the UK (welfare pop) induced by seven sampling schemes (in the seat of power). The \(y\) axis is the sampled supopulation (e.g. Rep is a ‘representative’ sample of the population) and sample size in brackets (e.g. (100)). The top four Rating comparisons use the \(\text{MEANWELFARE}\) welfare measure and the \(\text{MAXRATING}\) preference aggregation method. The bottom Choice comparisons use the \(\text{MEANCHOICE}\) welfare measure and the \(\text{MAXCHOICE}\) preference aggregation method. The red distributions are FOSD by Rep (100) in blue. The results are qualitatively similar to the US results in Fig. 5. Figure 39: **Welfare distributions with imputation.** In Fig. 5 and Fig. 38 individual welfare for a model takes the value NA if an individual never sees the model. Here, we repeat the welfare analysis and impute individual welfare with an approach similar in spirit to collaborative filtering. Using the only matrix of individual welfare for each model, we impute NA cells using multivariate imputation, implemented with the IterativeImputer package in Python. The results are qualitatively similar to the results where individual welfare is not imputed. ## V Codebooks

### V.1 Survey Codebook

| VARIABLE          | LABEL                                      | CATEGORY | TYPE     | N Missing: | N Unique: |
|-------------------|--------------------------------------------|----------|----------|------------|-----------|
| user_id           | Unique participant identifier              | meta     | string id| 0          | 1500      |
|                   | Notes: Pseudonymized from Prolific worker ID. Used to link survey data to conversation data. In our paper, we refer to 'users' as 'participants'. |
| survey_only       | Indicator if participant only completed the survey, or also completed conversations | meta     | binary   | 0          | 2         |
|                   | N Missing: 0 | N Unique: 2 | False 1396 | True 104 |
| num_completed_conversations | Number of conversations that a participant completed | meta     | int      | 0          | 8         |
|                   | N Missing: 0 | N Unique: 8 | mean 5.3 | std 1.7 | min 0.0 | max 7.0 |
| consent           | Participant informed consent confirmation  | direct   | categorical | 0          | 1         |
|                   | N Missing: 0 | N Unique: 1 | Yes, I consent to take part 1500 |
| consent_age       | Participant age confirmation               | direct   | categorical | 0          | 1         |
|                   | N Missing: 0 | N Unique: 1 | I certify that I am 18 years of age or over 1500 |
| lm_familiarity    | Familiarity with LLMs                      | direct   | categorical | 0          | 3         |
|                   | N Missing: 0 | N Unique: 3 | Somewhat familiar 920 | Very familiar 424 | Not familiar at all 156 |
| lm_direct_use     | Direct use of LLMs                         | direct   | categorical | 0          | 3         |
|                   | N Missing: 0 | N Unique: 3 | Yes 1162 | No 259 | Unsure 79 |
| lm_indirect_use   | Direct use of LLMs                         | direct   | categorical | 0          | 3         |
|                   | N Missing: 0 | N Unique: 3 | Yes 1104 | No 215 | Unsure 181 |
| lm_frequency_use  | Frequency of using Large Language Models   | direct   | categorical | 247        | 5         |
|                   | N Missing: 247 | N Unique: 5 | Once per month 374 | Every week 316 | More than once a month 291 | None 247 | Less than one a year 162 | Every day 110 |
| lm_usecases       | Use cases of LLMs                          | direct   | dict      | 247        |           |
|                   | Notes: Only shown if lm_indirect_use==1 OR lm_direct_use==1. Null indicates participant did not see question. |

*Question text:* Which of the following scenarios best describe how and why you use AI language models? Select all that apply. | VARIABLE                  | LABEL                                                                 | CATEGORY | TYPE |
|--------------------------|----------------------------------------------------------------------|----------|------|
| homework_assistance      | Homework Assistance: Getting help with school or university assignments. | False    | 967  |
|                          |                                                                       | True     | 533  |
| research                 | Research: Fact-checking or gaining overviews on specific topics.      | True     | 864  |
|                          |                                                                       | False    | 636  |
| source_suggestions       | Source Suggestions: Creating or finding bibliographies, information sources or reading lists. | False    | 1036 |
|                          |                                                                       | True     | 464  |
| professional_work        | Professional Work: Assisting in drafting, editing, or brainstorming content for work. | False    | 784  |
|                          |                                                                       | True     | 716  |
| creative_writing         | Creative Writing: Generating story ideas, dialogues, poems or other writing prompts. | False    | 861  |
|                          |                                                                       | True     | 639  |
| casual_conversation      | Casual Conversation: Engaging in small talk, casual chats, or joke generation. | False    | 991  |
|                          |                                                                       | True     | 509  |
| personal_recommendations | Personal Recommendations: Seeking book, music or movie recommendations. | False    | 987  |
|                          |                                                                       | True     | 513  |
| daily_productivity       | Daily Productivity: Setting reminders, making to-do lists, or productivity tips. | False    | 1037 |
|                          |                                                                       | True     | 463  |
| technical_or_programming_help | Technical or Programming Help: Seeking programming guidance, code generation, software recommendations, or debugging assistance. | False    | 916  |
|                          |                                                                       | True     | 584  |
| travel_guidance          | Travel Guidance: Getting destination recommendations, planning holidays, or cultural etiquette tips. | False    | 1120 |
|                          |                                                                       | True     | 380  |
| lifestyle_and_hobbies     | Lifestyle and Hobbies: Looking for recipes, craft ideas, home decoration tips, or hobby-related information. | False    | 943  |
|                          |                                                                       | True     | 557  |
| well-being_guidance      | Well-being Guidance: Seeking general exercise routines, wellness or meditation tips. | False    | 1094 |
|                          |                                                                       | True     | 406  |
| medical_guidance         | Medical Guidance: Seeking health-related advice or medical guidance.   | False    | 1123 |
|                          |                                                                       | True     | 377  |
| financial_guidance       | Financial Guidance: Asking about financial concepts or general investing ideas. | False    | 1146 |
|                          |                                                                       | True     | 354  |
| games                    | Games: Playing text-based games, generating riddles or puzzles.        | False    | 1110 |
|                          |                                                                       | True     | 390  |
| historical_or_news_insight | Historical or News Insight: Getting summaries or background on historical events or news and current affairs. | False    | 1070 |
|                          |                                                                       | True     | 430  |
| relationship_advice      | Relationship Advice: Seeking general self-help or relationship advice for family, friends or partners. | False    | 1155 |
|                          |                                                                       | True     | 345  |
| language_learning        | Language Learning: Using it as a tool for language practice or translation. | False    | 1024 |
|                          |                                                                       | True     | 476  |
| other                    | Other (selected)                                                      | False    | 1129 |
|                          |                                                                       | True     | 371  |
| other_text               | Other (typed text)                                                    | mean chars | 45.8 |
|                          |                                                                       | std chars | 41.9 |
|                          |                                                                       | min chars | 3.0  |
|                          |                                                                       | max chars | 328.0|

Notes: Question only show if lm_direct_use==1 OR lm_indirect_use==1. N Missing indicates the participants who have at least one missing value in the usecases (besides from 'other_text'). N Unique indicates the unique combinations of use cases selected by participants. On 'other_text', Null indicates participant did not type anything. On all other keys, 0 indicates participant saw question and did not select usecase. Null indicates participant did not see question.

| 10 order_lm_usecases | Use cases of LLMs (order of options presented in survey) |
|----------------------|----------------------------------------------------------|
|                      | meta | dict |
| N Missing:           | 247  |
| N Unique:            | 1254 |

Notes: Integer 1-18 indicating random order that usecase option was presented to participant. For 'other', option is always shown last so will always be 19. Null indicates participant did not see question. The usecases as the same as in lm_usecases.

| 11 stated_prefs | Stated preferences over LLM behaviours |
|-----------------|----------------------------------------|
|                 | direct | dict |

Question text: Rate each of the following statements about your opinion on the importance of different AI language model behaviors or traits. It is important that an AI language model... | VARIABLE | LABEL | CATEGORY | TYPE |
|----------|-------|----------|------|
|          |       | N Missing: 0 | 1475 |
|          |       | N Unique: 1475 |      |
| values   | ...reflects my values or cultural perspectives | mean 54.3 | std 26.3 |
|          |       | min 0.0 | max 100.0 |
| creativity | ...produces responses that are creative and inspiring | mean 69.6 | std 22.1 |
|          |       | min 0.0 | max 100.0 |
| fluency  | ...produces responses that are well-written and coherent | mean 86.7 | std 16.3 |
|          |       | min 2.0 | max 100.0 |
| factuality | ...produces factual and informative responses | mean 88.7 | std 16.2 |
|          |       | min 0.0 | max 100.0 |
| diversity | ...summarises multiple viewpoints or different worldviews | mean 75.7 | std 20.0 |
|          |       | min 0.0 | max 100.0 |
| safety   | ...produces responses that are safe and do not risk harm to myself and others | mean 80.2 | std 25.2 |
|          |       | min 0.0 | max 100.0 |
| personalisation | ...learns from our conversations and feels personalised to me | mean 67.9 | std 24.6 |
|          |       | min 0.0 | max 100.0 |
| helpfulness | ...produces responses that are helpful and relevant to my requests | mean 89.4 | std 14.4 |
|          |       | min 0.0 | max 100.0 |
| other    | Other (selected) | mean 57.5 | std 19.0 |
|          |       | min 0.0 | max 100.0 |
| other_text | Other (typed text) | mean chars 32.6 | std chars 24.4 |
|          |       | min chars 1.0 | max chars 144.0 |

Notes: Sliders from [Strongly disagree] to [Strongly agree] are recorded on a 0-100 scale. Participant does not see numeric value. N Missing indicates the participants who have at least one missing value in the attributes (besides from 'other_text'). N Unique indicates the unique combinations of use cases selected by participants. On 'other_text', Null indicates participant did not type anything. Note that this scale (on Qualtrics) runs 0-100. The Conversations rating scales (for choice_attributes, performance_attributes on Dynabench) run 1-100.

12 order_stated_prefs | Stated preferences over LLM behaviours (order of options presented in survey) | meta | dict |
|----------------------|--------------------------------------------------------------------------------|------|------|
| N Missing: 0 | N Unique: 1467 |

Notes: Integer 1-8 indicating random order that attribute slider was presented to participant. For 'other', option is always shown last so will always be 9. Null indicates participant did not see question. The attributes as the same as in stated_prefs.

13 self_description | Participant self-written profile describing themself | direct | string |
|-------------------|-----------------------------------------------------|-------|-------|
| N Missing: 0 | N Unique: 1500 |

Question text: Please briefly describe your values, core beliefs, guiding principles in life, or other things that are important to you. For example, you might include values you’d want to teach to your children or qualities you look for in friends. There are no right or wrong answers. Please do not provide any personally identifiable details like your name, address or email. Please write 2-5 sentences in your own words.

14 system_string | Participant self-written system string, constitution or custom instructions for an LLM | direct | string |
|----------------|--------------------------------------------------------------------------------|-------|-------|
| N Missing: 0 | N Unique: 1500 |

Question text: Imagine you are instructing an AI language model how to behave. You can think of this like a set of core principles that the AI language model will always try to follow, no matter what task you ask it to perform. In your own words, describe what characteristics, personality traits or features you believe the AI should consistently exhibit. You can also instruct the model what behaviours or content you don’t want to see. If you envision the AI behaving differently in various contexts (e.g., professional assistance vs. storytelling), please specify the general adaptations you’d like to see. Please write 2-5 sentences in your own words. | VARIABLE | LABEL | CATEGORY | TYPE |
|----------|-------|----------|------|
| 15 | age | Age | direct | categorical |

**Question text:** How old are you?

| N Missing: | 0 |
| N Unique: | 7 |
| 25-34 years old | 454 |
| 18-24 years old | 297 |
| 35-44 years old | 237 |
| 45-54 years old | 208 |
| 55-64 years old | 197 |
| 65+ years old | 106 |
| Prefer not to say | 1 |

| 16 | education | Education | direct | categorical |

**Question text:** What is the highest level of education you have completed?

| N Missing: | 0 |
| N Unique: | 9 |
| University Bachelors Degree | 637 |
| Graduate / Professional degree | 241 |
| Some University but no degree | 236 |
| Completed Secondary School | 209 |
| Vocational | 125 |
| Some Secondary | 24 |
| Completed Primary School | 16 |
| Prefer not to say | 9 |
| Some Primary | 3 |

| 17 | employment_status | Employment Status | direct | categorical |

**Question text:** What best describes your employment status over the last three months?

| N Missing: | 0 |
| N Unique: | 8 |
| Working full-time | 712 |
| Working part-time | 265 |
| Student | 191 |
| Unemployed, seeking work | 113 |
| Retired | 104 |
| Homemaker / Stay-at-home parent | 46 |
| Unemployed, not seeking work | 46 |
| Prefer not to say | 23 |

| 18 | marital_status | Marital Status | direct | categorical |

**Question text:** What is your current marital status?

| N Missing: | 0 |
| N Unique: | 5 |
| Never been married | 870 |
| Married | 463 |
| Divorced / Separated | 123 |
| Prefer not to say | 23 |
| Widowed | 21 |

| 19 | english_proficiency | English Proficiency | direct | categorical |

**Question text:** How would you describe your proficiency in English?

| N Missing: | 0 |
| N Unique: | 5 |
| Native speaker | 886 |
| Fluent | 405 |
| Advanced | 160 |
| Intermediate | 42 |
| Basic | 7 |

| 20 | gender | Gender | constructed | categorical |

**Question text:** How would you describe your proficiency in English?

| N Missing: | 0 |
| N Unique: | 4 |
| Male | 757 |
| Female | 718 |
| Non-binary / third gender | 21 |
| Prefer not to say | 4 |

**Notes:** Participants could chose Male, Female, Non-binary / third Gender, Prefer not to say, or write in their own response. Two independent annotators then categorised the self-describe responses only when abundantly clear they fit another category. See paper for details.

| 21 | religion | Dictionary of religion information. | NA | dict |

**Notes:** Keys explained below.

| 22 | religion_self_described | Participant [c] self-description | direct | string |

**Question text:** What is your religious affiliation?

| N Missing: | 0 |
| N Unique: | 137 |
| mean chars | 12.2 |
| std chars | 5.7 |
| min chars | 2.0 |
| max chars | 112.0 |

**Notes:** Participant had option to type and Self Describe or select Prefer not to say.

| 23 | religion_categorised | Granular categories of participant religion | constructed | categorical |

**Notes:** Participant had option to type and Self Describe or select Prefer not to say. | VARIABLE | LABEL | CATEGORY | TYPE |
|----------|-------|----------|------|
| 24       | religion_simplified | Simplified categories of participant religion | constructed | categorical |
|          | N Missing: | 0 |
|          | N Unique: | 6 |
|          | Non-religious | 762 |
|          | Christian | 487 |
|          | Agnostic | 71 |
|          | Prefer not to say | 59 |
|          | Jewish | 42 |
|          | Muslim | 31 |
|          | Spiritual | 18 |
|          | Buddhist | 12 |
|          | Folk religion | 6 |
|          | Hindu | 5 |
|          | Other | 4 |
|          | Sikh | 3 |

Notes: Two independent annotators manually verified all automated classifications (gpt-4-turbo) of the self-describe string. See paper for details.

| 25 | ethnicity | Dictionary of ethnicity information. | NA | dict |
|----|----------|--------------------------------------|----|------|

Notes: Keys explained below.

| 26 | ethnicity_self_described | Participant {c} self-description | direct | string |
|----|--------------------------|----------------------------------|-------|--------|
|    | N Missing: | 0 |
|    | N Unique: | 264 |
|    | mean chars | 9.2 |
|    | std chars | 6.2 |
|    | min chars | 3.0 |
|    | max chars | 99.0 |

Notes: Participant had option to type and Self Describe or select Prefer not to say.

| 27 | ethnicity_categorised | Granular categories of participant ethnicity | constructed | categorical |
|----|-----------------------|---------------------------------------------|--------------|-------------|
|    | N Missing: | 0 |
|    | N Unique: | 9 |
|    | White | 969 |
|    | Black / African | 122 |
|    | Hispanic / Latino | 121 |
|    | Asian | 95 |
|    | Prefer not to say | 86 |
|    | Mixed | 68 |
|    | Other | 17 |
|    | Middle Eastern / Arab | 14 |
|    | Indigenous / First Peoples | 8 |

Notes: Two independent annotators manually verified all automated classifications (gpt-4-turbo) of the self-describe string. See paper for details.

| 28 | ethnicity_simplified | Simplified categories of participant ethnicity | constructed | categorical |
|----|----------------------|---------------------------------------------|--------------|-------------|
|    | N Missing: | 0 |
|    | N Unique: | 7 |
|    | White | 969 |
|    | Black | 122 |
|    | Hispanic | 121 |
|    | Asian | 95 |
|    | Prefer not to say | 86 |
|    | Mixed | 68 |
|    | Other | 39 |

Notes: Simplified version of ethnicity_categorised for more aggregate analysis.

| 29 | location | Dictionary of location information. | NA | dict |
|----|----------|--------------------------------------|----|------|

Notes: Keys explained below.

| 30 | location_birth_country | Participant country of birth | direct | categorical |
|----|-------------------------|-----------------------------|-------|-------------|
|    | N Missing: | 0 |
|    | N Unique: | 75 |

Notes: Selected from standardised dropdown country list.

| 31 | location_birth_countryISO | ISO 3166-1 alpha-3 code for the country of birth | constructed | categorical |
|----|---------------------------|-----------------------------------------------|--------------|-------------|
|    | N Missing: | 0 |
|    | N Unique: | 75 |

Notes: Mapped from country of birth, based on United Nations defined subregions.

| 32 | location_birth_subregion | Participant sub-region of birth | constructed | categorical |
|----|--------------------------|--------------------------------|--------------|-------------|
|    | N Missing: | 0 |
|    | N Unique: | 16 |

Notes: Too many values to show - | VARIABLE                  | LABEL                                                                 | CATEGORY | TYPE       |
|--------------------------|----------------------------------------------------------------------|----------|------------|
| 33 location_reside_country | Participant country of residence                                    | direct   | categorical|
|                          | Question text: In which country do you currently reside?             |          |            |
|                          | Notes: Selected from standardised dropdown country list.              |          |            |
| 34 location_reside_countryISO | ISO 3166-1 alpha-3 code for the country of residence               | constructed | categorical |
|                          | Notes: Mapped from country of residence, based on United Nations defined subregions. |          |            |
| 35 location_reside_subregion | Participant sub-region of residence                                | constructed | categorical |
|                          | Notes: Within regions and sub-regions, some countries are split out to better represent sample density (e.g., treating UK and US samples separately from Europe and North America). |          |            |
| 36 location_same_birth_reside_country | Whether the participant was born and resides in the same country | constructed | binary |
|                          | Notes: End time, not start time                                      |          |            |
| 37 location_special_region | Adjusted regional categories for unique sample properties            | constructed | categorical |
|                          | Notes: Census-representative samples were rebalanced to mitigate sampling issues. See paper for details. |          |            |
| 38 study_id               | Unique study identifier on Prolific                                  | meta     | string id  |
| 39 study_locale           | Recruitment country of Prolific study                               | meta     | categorical |
| 40 generated_datetime     | Recorded date of the survey completion                              | meta     | datetime   |
| 41 timing_duration_s      | Duration of the survey session (in seconds)                         | meta     | float      |
| 42 timing_duration_mins   | Duration of the survey session (in minutes)                         | meta     | float      |
| 43 included_in_UK_REP     | Indicator if participant was included in the rebalanced UK representative sample | constructed | binary |
|                          | Notes: Census-representative samples were rebalanced to mitigate sampling issues. See paper for details. |          |            | | VARIABLE          | LABEL                                                                 | CATEGORY | TYPE  |
|------------------|----------------------------------------------------------------------|----------|-------|
| 44 included_in_US_REP | Indicator if participant was included in the rebalanced US representative sample | constructed | binary |
|                  | N Missing: 0                                                          |          |       |
|                  | N Unique: 2                                                          |          |       |
|                  | False 1270                                                          |          |       |
|                  | True 230                                                            |          |       |

Notes: Census-representative samples were rebalanced to mitigate sampling issues. See paper for details.

| 45 included_in_balanced_subset | Indicator if participant’s conversations are included in the balanced subset | constructed | binary |
|-------------------------------|---------------------------------------------------------------------------|------------|-------|
|                               | N Missing: 0                                                              |            |       |
|                               | N Unique: 2                                                               |            |       |
|                               | False 1246                                                               |            |       |
|                               | True 254                                                                 |            |       |

Notes: Balanced subset was created to equally sample conversations of three types (unguided, values, controversy). We only include participants who have at least one of each conversation type, and then ensure equal numbers of each type are retained. See paper for details. ### V.2 Conversations Codebook

| VARIABLE     | LABEL                                      | CATEGORY | TYPE     | N Missing: | N Unique: |
|--------------|--------------------------------------------|----------|----------|------------|-----------|
| 0 user_id    | Unique participant identifier              | meta     | string id| 0          | 1396      |

*Notes: Pseudonymized from Prolific worker ID. Used to link conversation data to survey data.*

| 1 conversation_id | Unique conversation identifier             | meta     | string id| 0          | 8011      |

| 2 opening_prompt | Opening human-written prompt of the conversation | direct   | string   |            |           |

*Question text:* Now start the conversation with your question, request or statement.

*Notes: We provide the following soft guidance: Need some inspiration? You can request help with a task (like writing a recipe, organising an activity or event, completing an assignment)... You can chitchat, have casual conversation or seek personal advice. You can ask questions about the world, current events or your viewpoints.*

| 3 open_feedback | Participant written feedback on the conversation as a whole. | direct   | string   |            |           |

*Question text:* Give the model some feedback on the conversation as whole. Hypothetically, what would an ideal interaction for you look like here? What was good and what was bad? What (if anything) was missing? What would you change to make the conversation better? Please write 2-5 sentences in your own words.

*Notes: Entry box reads: Enter text here. Do not copy and paste.*

| 4 conversation_type | Type of conversation (from pre-defined categories) | direct   | categorical |            |           |

*Question text:* Choose what type of conversation you want to have.

*Notes: Participants pick from the following radio buttons: Unguided. Ask, request or talk to the model about anything. It is up to you! Values guided. Ask, request or talk to the model about something important to you or that represents your values. This could be related to work, religion, family and relationship, politics or culture. Controversy guided. Ask, request or talk to the model about something controversial or where people would disagree in your community, culture or country. We also provide the additional instruction: Remember if you are here as a paid study participant, you need to do two of each type. If you are here as a volunteer, then take your pick!*

| 5 conversation_turns | Number of human-model turns (back-and-forths) in the conversation. | meta     | int       |            |           |

*Notes: We force 2 turns as the minimum. After the opening turn, we give the instruction: Now continue the conversation. Conversations can be between 2 and 10 turns. Try to vary the length. When you’re done, click Finish.*

| 6 conversation_history | Full conversation history (human and model messages, with scores and model metadata) | direct   | dict      |            |           |

*Notes: We provide an example of what this nested conversation history looks like below.*

| 7 performance_attributes | How well the top-rated model response performed across different attributes | nested   | dict      |            |           |

*Question text:* Tell us how the model performed. Consider your first message and the top-rated response. Rate the following statements about the performance across different attributes. This response...

| values | reflected my values or cultural perspective | mean | 74.1 | 22.2 | 1.0 | 100.0 |
|        |                                             | std  | 18.3 | 1.0  | 100.0 |
|        |                                             | min  | 1.0  | 100.0 |
|        |                                             | max  | 100.0 |

| fluency | was well-written and coherent | mean | 84.3 | 18.3 | 1.0 | 100.0 |
|         |                                | std  | 18.3 | 1.0  | 100.0 |
|         |                                | min  | 1.0  | 100.0 |
|         |                                | max  | 100.0 |

| factuality | was factual and informative | mean | 79.2 | 22.2 | 1.0 | 100.0 |
|           |                              | std  | 18.3 | 1.0  | 100.0 |
|           |                              | min  | 1.0  | 100.0 |
|           |                              | max  | 100.0 |

Continued on next page | VARIABLE | LABEL | CATEGORY | TYPE |\n|----------|-------|----------|------|\n|          |       | std      | 21.5 |\n|          |       | min      | 1.0  |\n|          |       | max      | 100.0|\n| safety   | ...was safe and doesnât risk harm to myself and others | mean | 85.1 |\n|          |       | std      | 19.3 |\n|          |       | min      | 1.0  |\n|          |       | max      | 100.0|\n| diversity| ...summarised multiple viewpoints or different worldviews | mean | 68.7 |\n|          |       | std      | 25.3 |\n|          |       | min      | 1.0  |\n|          |       | max      | 100.0|\n| creativity| ...was creative and inspiring | mean | 63.7 |\n|          |       | std      | 26.1 |\n|          |       | min      | 1.0  |\n|          |       | max      | 100.0|\n| helpfulness| ...was helpful and relevant to my request | mean | 81.5 |\n|          |       | std      | 21.9 |\n|          |       | min      | 1.0  |\n|          |       | max      | 100.0|\n\nNotes: Sliders from [Performed very poorly] to [Performed very well] are recorded on a 1-100 scale. Participant does not see numeric value. Note that the attributes align choice_attributes, as well as with the stated preference ratings from The Survey. Participants had option to select N/A, which is recorded as Null. N Missing indicates the number of participants who have at least one missing value in the nested columns. N Unique indicates the unique combinations of use cases selected by participants. There was no option for 'other'. Note, these sliders run from 1-100 (on Dynabench). The sliders for stated_prefs (in Survey on Qualtrics) run 0-100.\n\n8 choice_attributes How different attributes influenced the participantâs choice of the top-rated model response\n\n| values | ...reflected my values or cultural perspective | mean | 66.9 |\n|        |                                               | std  | 27.2 |\n|        |                                               | min  | 1.0  |\n|        |                                               | max  | 100.0|\n| fluency| ...was well-written and coherent               | mean | 82.5 |\n|        |                                               | std  | 18.5 |\n|        |                                               | min  | 1.0  |\n|        |                                               | max  | 100.0|\n| factuality| ...was factual and informative               | mean | 79.3 |\n|         |                                               | std  | 21.0 |\n|         |                                               | min  | 1.0  |\n|         |                                               | max  | 100.0|\n| safety | ...was safe and doesnât risk harm to myself and others | mean | 72.1 |\n|         |                                               | std  | 27.8 |\n|         |                                               | min  | 1.0  |\n|         |                                               | max  | 100.0|\n| diversity| ...summarised multiple viewpoints or different worldviews | mean | 66.0 |\n|          |                                               | std  | 26.5 |\n|          |                                               | min  | 1.0  |\n|          |                                               | max  | 100.0|\n| creativity| ...was creative and inspiring               | mean | 62.1 |\n|          |                                               | std  | 27.1 |\n|          |                                               | min  | 1.0  |\n|          |                                               | max  | 100.0|\n| helpfulness| ...was helpful and relevant to my request | mean | 82.5 |\n|           |                                               | std  | 20.0 |\n|           |                                               | min  | 1.0  |\n|           |                                               | max  | 100.0|\n\nNotes: Sliders from [Very unimportant] to [Very important] are recorded on a 1-100 scale. Participant does not see numeric value. Note that the attributes align with performance_attributes, as well as the stated preference ratings from The Survey. Participants had option to select N/A, which is recorded as Null. num_missing indicates the number of participants who have at least one missing value in the nested columns. num_unique indicates the unique combinations of use cases selected by participants. There was no option for 'other'. Note, these sliders run from 1-100 (on Dynabench). The sliders for stated_prefs (in Survey on Qualtrics) run 0-100.\n\n9 generated_datetime Recorded date of the conversation completion\n\n| meta | datetime |\n|------|----------|\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n|      |          |\n | VARIABLE                  | LABEL                                      | CATEGORY | TYPE  |
|--------------------------|--------------------------------------------|----------|-------|
| N Unique:                | 7656                                       |          |       |
| mean                     | 555.9                                      |          |       |
| std                      | 422.1                                      |          |       |
| min                      | 73.5                                       |          |       |
| max                      | 17145.8                                    |          |       |

Notes: Extreme values are caused by participants completing task in multiple sessions.

| 11 timing_duration_mins | Duration of the conversation (in minutes) | constructed | float |
|-------------------------|-------------------------------------------|-------------|-------|
| N Missing:              | 0                                         |             |       |
| N Unique:               | 1948                                      |             |       |
| mean                    | 9.3                                       |             |       |
| std                     | 7.0                                       |             |       |
| min                     | 1.2                                       |             |       |
| max                     | 285.8                                     |             |       |

Notes: timing_duration_s / 60. Extreme values are caused by participants completing task in multiple sessions.

| 12 included_in_balanced_subset | Indicator if participant’s conversations are included in the balanced subset | constructed | binary |
|---------------------------------|--------------------------------------------------------------------------|-------------|--------|
| N Missing:                      | 0                                                                         |             |        |
| N Unique:                       | 2                                                                         |             |        |
| True                            | 6696                                                                      |             |        |
| False                           | 1315                                                                      |             |        |

Notes: Balanced subset was created to equally sample conversations of three types (unguided, values, controversy). We only include participants who have at least one of each conversation type, and then ensure equal numbers of each type are retained. See paper for details. ## V.3 Utterances Codebook

| VARIABLE     | LABEL                                                                 | CATEGORY | TYPE       | N Missing: | N Unique: |
|--------------|----------------------------------------------------------------------|----------|------------|------------|-----------|
| 0 user_id    | Unique participant identifier                                       | meta     | string id  | 0          | 1396      |
|              | Notes: Pseudonymized from Prolific worker ID. Used to link utterance data to survey data. |
| 1 conversation_id | Unique conversation identifier                                      | meta     | string id  | 0          | 8011      |
|              | Notes: Used to link utterance data to conversation data.            |
| 2 interaction_id | Unique interaction identifier, where an interaction is a turn within a conversation (single human message with multiple model responses) | meta     | string id  | 0          | 27172     |
| 3 utterance_id | Unique utterance identifier, where an utterance is a single human message - single model response pair | meta     | string id  | 0          | 68371     |
| 4 within_turn_id | Within turn identifier of up to four model responses to a single human message | meta     | string id  | 0          | 4         |
|              | Notes: Order is random, not based on score or presentation in interface |
| 5 conversation_type | Type of conversation (from pre-defined categories) | direct   | categorical | 3          | 3113      |
|              | Notes: Participants pick from the following radio buttons: Unguided. Ask, request or talk to the model about anything. It is up to you! Values guided. Ask, request or talk to the model about something important to you or that represents your values. This could be related to work, religion, family and relationship, politics or culture. Controversy guided. Ask, request or talk to the model about something controversial or where people would disagree in your community, culture or country. We also provide the additional instruction: Remember if you are here as a paid study participant, you need to do two of each type. If you are here as a volunteer, then take your pick! |
| 6 turn       | Turn of conversation when prompt was entered                        | meta     | int        | 0          | 22        |
|              | Notes: In the paper, we refer to the first turn as T=1. Here, we index the first turn as 0. |
| 7 model_name | Name of LLM                                                          | meta     | categorical | 0          | 21        |
|              | Notes: We provide the long name as it appeared on our backend. We provide a mapping of long names to shorter more familiar names on our Github or in the paper. |
| 8 model_provider | Provider of the LLM                                                  | meta     | categorical | 0          | 6         |
|              | Notes: We provide the long name as it appeared on our backend. We provide a mapping of long names to shorter more familiar names on our Github or in the paper. | | VARIABLE          | LABEL                                      | CATEGORY | TYPE  |
|-------------------|--------------------------------------------|----------|-------|
| 9 user_prompt     | Human-written message.                     | direct   | string|
|                   | Notes: Note for open-access LLMs, HuggingFace API is always listed as the source and does not imply they built the model. |
|                   | N Missing: 0                               |          |       |
|                   | N Unique: 26673                            |          |       |
|                   | mean chars 69.9                            |          |       |
|                   | std chars 62.0                             |          |       |
|                   | min chars 1.0                              |          |       |
|                   | max chars 1311.0                           |          |       |
| 10 model_response | Model-generated response                   | direct   | string|
|                   | Notes: An empty string is stored as ‘EMPTY STRING’. |
|                   | N Missing: 0                               |          |       |
|                   | N Unique: 66614                            |          |       |
|                   | mean chars 565.3                           |          |       |
|                   | std chars 387.9                            |          |       |
|                   | min chars 1.0                              |          |       |
|                   | max chars 4630.0                           |          |       |
| 11 score          | Score of the model response               | direct   | int   |
|                   | Notes: Sliders from [Terrible] to [Perfect] are recorded on a 1-100 scale. Participant does not see numeric value. |
|                   | N Missing: 0                               |          |       |
|                   | N Unique: 100                              |          |       |
|                   | mean 65.1                                  |          |       |
|                   | std 29.3                                   |          |       |
|                   | min 1.0                                    |          |       |
|                   | max 100.0                                  |          |       |
| 12 if_chosen      | Whether model response was highest-rated by participant | constructed | binary |
|                   | Notes: In case of a tie, a random response is chosen. |
|                   | N Missing: 0                               |          |       |
|                   | N Unique: 2                               |          |       |
|                   | False 40934                                |          |       |
|                   | True 27437                                 |          |       |
| 13 included_in_balanced_subset | Indicator if participant’s conversations are included in the balanced subset | constructed | binary |
|                   | Notes: Balanced subset was created to equally sample conversations of three types (unguided, values, controversy). We only include participants who have at least one of each conversation type, and then ensure equal numbers of each type are retained. See paper for details. |
|                   | N Missing: 0                               |          |       |
|                   | N Unique: 2                               |          |       |
|                   | False 57401                                |          |       |
|                   | True 10970                                 |          |       | ## V.4 Metadata Codebook

| VARIABLE   | LABEL                                                                 | CATEGORY | TYPE       | N Missing | N Unique |
|------------|----------------------------------------------------------------------|----------|------------|-----------|----------|
| 0 column_id| Source of text utterance                                            | meta     | categorical| 0         | 5        |
|            | model_response                                                       |          |            | 68371     |          |
|            | user_prompt                                                          |          |            | 27172     |          |
|            | open_feedback                                                        |          |            | 8011      |          |
|            | self_description                                                     |          |            | 1500      |          |
|            | system_string                                                        |          |            | 1500      |          |
| 1 user_id  | Unique participant identifier                                        | meta     | string id  | 0         | 1500     |
| 2 conversation_id | Unique conversation identifier, where an interaction is a turn within a conversation (single human message with multiple model responses) | meta     | string id  | 3000      | 8011     |
| 3 interaction_id | Unique utterance identifier, where an utterance is a single human message - single model response pair | meta     | string id  | 11011     | 27172    |
| 4 utterance_id | Unique utterance identifier, where an utterance is a single human message - single model response pair | meta     | string id  | 38183     | 68371    |
| 5 pii_flag | Automated flag for personally identifiable information in human-written texts | meta     | binary     | 0         | 2        |
|            | False                                                                |          |            | 105443    |          |
|            | True                                                                 |          |            | 1111      |          |
| 6 pii_manual_flag | Manual verification of personally identifiable information in human-written texts | meta     | binary     | 106387    | 1        |
|            | nan                                                                  |          |            | 106387    |          |
|            | 0.0                                                                  |          |            | 167       |          |
| 7 language_flag | Automated language detection                                         | meta     | categorical| 0         | 59       |
|            | Too many values to show                                              |          |            | -         |          |
| 8 en_flag | Whether detected language is English                                  | meta     | binary     | 0         | 2        |
|            | Too many values to show                                              |          |            | -         |          |
| 9 moderation_flag | Automated flag for moderation                                        | meta     | nested dict| 0         | 2        |
|            | Nested dictionary with binary flags and probabilities for sub-categories of harm |          |            | -         |          |

**Notes:**
- Pseudonymized from Prolific worker ID. Used to link metadata to main data.
- Used to link metadata to main data.
- Uses scrubadub https://scrubadub.readthedocs.io/en/stable/ to find PII. There may be some misclassifications. Many of the inspected positives were false positives. All positive human-written texts checked. See pii_manual_flag.
- Uses langid. There may be some misclassifications.
- Uses OpenAI moderation API. There may be some misclassifications. Nested dictionary with binary flags and probabilities for sub-categories of harm.