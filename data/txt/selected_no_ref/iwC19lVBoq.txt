A VSET-10M: An Open Large-Scale Audio-Visual
Dataset with High Correspondence
Xize Cheng1∗ Ziang Zhang1* Zehan Wang1* Minghui Fang1 Rongjie Huang1
Siqi Zheng2 Ruofan Hu1 Jionghao Bai1 Tao Jin1 Zhou Zhao1 †
1Zhejiang University 2Alibaba Group
{chengxize,zhaozhou}@zju.edu.cn
Abstract
Groundbreaking research from initiatives such as ChatGPT and Sora underscores1
the crucial role of large-scale data in advancing generative and comprehension tasks.2
However, the scarcity of comprehensive and large-scale audio-visual correspon-3
dence datasets poses a significant challenge to research in the audio-visual fields.4
To address this gap, we introduce A VSET-10M, a audio-visual high-corresponding5
dataset comprising 10 million samples, featuring the following key attributes:6
(1) High Audio-Visual Correspondence: Through meticulous sample filtering,7
we ensure robust correspondence between the audio and visual components of8
each entry. (2) Comprehensive Categories: Encompassing 527 unique audio9
categories, A VSET-10M offers the most extensive range of audio categories avail-10
able. (3) Large Scale: With 10 million samples, A VSET-10M is the largest11
publicly available audio-visual corresponding dataset. We have benchmarked12
two critical tasks on A VSET-10M: audio-video retrieval and vision-queried sound13
separation. These tasks highlight the essential role of precise audio-visual corre-14
spondence in advancing audio-visual research. For more information, please visit15
https://avset-10m.github.io/.16
1 Introduction17
Scaling up significantly enhances performance in understanding [37, 4, 26] and generation [20, 19, 42]18
tasks across visual and language modalities. Inspired by the success of ImageNet [9] in visual research,19
some introduce the pioneering large-scale audio dataset, AudioSet [12], which comprises 2.1 million20
audio samples each manually annotated with fine-grained audio categories to advance automatic audio21
understanding. However, the annotation process in AudioSet primarily focuses on only audio labels,22
neglecting the audio-visual correspondence. To address the need for exploring temporal consistency23
between audio and video, researchers develop the VGGSound [6], which includes 200,000 samples24
with audio-visual correspondence. Leveraging this dataset, significant breakthroughs have been25
achieved in the audio-visual domain, including vision-queried sound separation [10] and vision-based26
audio synthesis [14, 43].27
Meanwhile, the scale of vision-language datasets [ 35, 29, 44, 32, 40] has expanded dramatically,28
encompassing up to 100 million or even 1 billion samples. This expansion has facilitated a qualita-29
tive leap in understanding [37, 26] and generation [20] tasks within the vision and language fields,30
∗Equal contribution.
†Corresponding author.
Submitted to the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets
and Benchmarks. Do not distribute.

Table 1: Comparison of different audio-video datasets.A V-Cdenotes the audio-visual correspondence.
# Class: Number of audio categories. ACA V-100M† does not filter out the voiceover.
Datasets Video A V-C #Class #Clips #Dur.(hrs) #Avg Dur.(s)
DCASE2017 [28] ✗ ✗ 17 57K 89 3.9
FSD [11] ✗ ✗ 398 24K 119 17.4
AudioSet [12] ✔ ✗ 527 2.1M 5.8K 10
AudioScope-V2 [39] ✔ ✗ - 4.9M 1.6K 5
ACA V100M[22]† ✔ ✗ - 100M 277.7K 10
HD-VILA-100M [44] ✔ ✗ - 103M 371.5K 13.4
Panda-70M [8] ✔ ✗ - 70.8M 166.8K 8.5
A VE [36] ✔ ✔ 28 4K 11 10
VGGSound [6] ✔ ✔ 309 200K 550 10
A VSET-700K (ours) ✔ ✔ 527 728K 2.0K 10
A VSET-10M (ours) ✔ ✔ 527 10.9M 30.4K 10.3
enabling the development of intelligent large language models [37] and video generation technolo-31
gies [5] that simulate real-world scenarios. In contrast, the scale of datasets that ensure audio-visual32
correspondence remains markedly limited, posing a constraint on advancements in audio-visual field.33
To further expand the audio-visual corresponding dataset and promote research on audio-visual34
temporal consistency, we propose A VSET-10M, the first 10 million scale audio-visual corresponding35
dataset, along with A VSET-700K, a subset containing fine-grained audio annotations. In Table 1, we36
present a comparison among various existing audio and audio-visual datasets. Our dataset construction37
process includes four stages: (1) Data collection, (2) Audio-visual correspondence filtering, (3) V oice-38
over filtering, and (4) Sample recycling with sound separation. We select AudioSet [12], known for39
its fine-grained manual labeling of audio categories, as our initial data source and develop A VSET-40
700K with accurate audio labels. To increase the number of samples per audio category, we choose41
Panda-70M [8] as an additional data source, expanding A VSET-700K to 10 million audio-visual42
corresponding samples. Panda-70M processes long videos into multiple semantically coherent43
sub-segments, effectively preventing the mixing of sounds from different events. Previous filtering44
method [6] using visual classification models struggles to distinguish abstract sounds without fixed45
visual content, such as silence, thereby limiting the diversity of audio categories. To address this46
issue, we introduce a new filtering method based on audio-visual similarity [13], which significantly47
broadens the diversity of audio types. We employ an audio classification model [ 21] to filter out48
samples containing narration or background music that does not align with the visual content. As49
speech is commonly found in wild video data, which often results in the inadvertent filtering out of a50
substantial amount of audio samples containing voice-overs. This leads to the loss of many potentially51
useful and valuable samples across various audio categories. Thus, we further attempt to employ52
sound separation models [33] to recycle as many of these wasted samples as possible. From the initial53
41 million samples, we filter 10 million audio-visual samples with high correspondence. Verification54
experiments demonstrate that our A VSET-700K provides more robust audio-visual correspondence55
than the previously used audio-visual corresponding dataset (VGGSound). Additionally, benchmarks56
of audio-video retrieval and vision-queried sound separation on A VSET-10M demonstrate it offers57
more research opportunities in the field of audiovisual studies.58
2 Related Works59
2.1 Audio-Visual Models60
As multi-modal research progresses, the investigation [24, 31, 17] into the correlations between audio61
and visual modalities has advanced. Initially, researchers employ both audio and video data to provide62
semantically richer information, thereby improving video understanding and significantly enhancing63
performance in various video understanding tasks such as video question answering (VQA) [24, 2],64
2

video captioning [31, 15, 16, 25], and video retrieval [ 23, 17, 3]. Following these developments,65
ImageBind [13] emerges as a pioneering project that successfully aligns audio and visual content,66
marking a significant step in exploring semantic alignment between these modalities. Building on this67
foundation, subsequent research has delved into more intricate interactions between audio and video,68
achieving milestones in vision-queried sound separation [ 10] and video dubbing [ 14]. However,69
while these methods have managed to align audio and visual content semantically, they often falter in70
maintaining temporal consistency. Recent innovations [27] have introduced audio-visual temporal71
consistency supervision loss to enhance the temporal alignment in video dubbing.72
Despite these advancements, the limited availability of training data continues to pose a significant73
challenge, keeping the development of audio-visual temporal consistency at a rudimentary level. As74
a result, the understanding of visual content remains largely confined to the semantic level, which75
hampers the ability of models to accurately capture the audio-visual temporal consistency.76
2.2 Audio-Video Dataset77
Inspired by ImageNet [9], researchers [12] annotate a substantial audio dataset, consisting of 2.178
million audio samples, aimed at enhancing automatic audio comprehension. Although annotators79
are encouraged to consult video content to refine the accuracy of audio annotations, the dataset80
primarily focuses on precise audio annotations without additional measures to filter out audio-visual81
non-corresponding samples. This limits the exploration of audio-video consistency.82
To investigate the audio-visual consistency, researchers [6] employ a visual model [30] to identify83
sound-producing objects in videos, resulting in the creation of VGGSound, a dataset comprising84
200,000 audio-visual corresponding samples. However, this visual model proves effective only in85
scenes characterized by definite actions or visible objects. It struggles to handle abstract audio scenes86
such as silence and urban outdoors, even though there is indeed a correlation between these abstract87
audio and the visual content. This constraint limits the diversity of audio categories represented88
in VGGSound. To further scale up audio-visual datasets, ACA V100M [22] employs a clustering-89
based approach to filter data. However, it does not filter out voice-overs, resulting in the audio-visual90
correspondence of the final dataset being even worse than that of AudioSet. AudioScope V1/2 [38, 39]91
uses an unsupervised audio-video consistency prediction model to evaluate the audio-video matching92
score and screens 2,500 hours of video samples from YFCC100M [ 35]. Nevertheless, due to the93
limitations in prediction accuracy, the consistency between audio and video cannot be guaranteed,94
and there is still a significant amount of inconsistent audio-visual content in the dataset.95
Although subsequent research introduces larger video datasets [44, 40, 7, 8], the primary focus remains96
on exploring the relationship between video and text, overlooking the audio-visual correspondence.97
To the best of our knowledge, our A VSET-10M represents the largest open audio-visual high-98
correspondence dataset currently available, containing 10 million data samples across 527 different99
audio categories. This dataset opens up more opportunities for research in the audio-video field.100
3 A VSET-10M101
3.1 Dataset Construction Pipeline102
Stage 1: Data Collection. We select two different open-source datasets, AudioSet [12] and Panda-103
70M [8], as data sources. All videos are sourced from open-domain YouTube content. Since104
these datasets do not focus on audio-visual correspondence, they contain a substantial number of105
mismatched audio-visual samples. We utilize a sophisticated filtering process to select samples with106
high audio-visual correspondence, thereby constructing A VSET-10M.107
AudioSet [12] is a pioneering large-scale audio dataset where all audio category labels are carefully108
annotated by human annotators. During the annotation process, annotators are allowed to view the109
accompanying videos, which aids in accurate audio category identification. This dataset includes110
2.1 million audio samples across 527 unique audio categories. From AudioSet, we select 727,530111
3

Figure 1: The distribution of audio-visual similarity among audio-visual corresponding samples,
audio-visual non-corresponding samples and randomly selected wild samples. The similarity of
non-corresponding data follows the distribution Nnon−corresponding(0.015, 0.0812). Approximately
65% of the randomly selected wild samples and 18% of the audio-visual corresponding samples
exhibit similarities below theµ+3σ (0.2564) threshold of Nnon−corresponding, suggesting a potential
for these samples to be classified as audio-visual non-corresponding.
samples that demonstrate high audio-visual correspondence with reliable audio category labels to112
form A VSET-700K.113
Additionally, to further expand the number of samples in each audio class, we select Panda-70M [8],114
a large-scale video-text dataset containing 70 million semantically consistent segments. It employs115
shot boundary detection technology [ 1] to divide the original videos into smaller semantically116
consistent segments. This segmentation ensures that each clip contains only a single event, preventing117
sound category conversion due to event switching and facilitating the subsequent filtering process.118
Leveraging Panda-70M, we expand A VSET-700K to a total of 10 million audio-visual corresponding119
samples, thus forming A VSET-10M.120
Stage 2: Audio-Visual Correspondence Filtering. Previous researchers [6] compute the cosine121
similarity between textual class label and visual content to gauge alignment confidence between122
vision and language. They subsequently filter video samples for each class label using a manually123
selected threshold. However, this method is effective only in scenes featuring definite actions or124
visual objects. It struggles with abstract concepts, such as silence and urban outdoor scenes, even125
though these audios have specific associations with visual content. This consequently restricts the126
diversity of audio categories available in the dataset. We propose determining the confidence of127
audio-visual correspondence based on audio-visual similarity. This approach enables the screening of128
abstract audio samples and enhances the diversity of samples in the dataset.129
Specifically, we randomly select 7,500 audio-visual corresponding samples Dcorresponding from the130
VGGSound dataset, and 7,500 wild data samples Drandom from the Panda-70M dataset. Addition-131
ally, we randomly construct 70,000 audio-visual non-corresponding samples Dnon−corresponding132
based on VGGSound. We employ Imagebind [ 13] to extract and calculate the cosine similarity133
between the average representation of 8 random video frames and the audio representation. The134
similarity distribution curves of different sample sets are depicted in Figure 1. The audio-visual135
non-corresponding samples exhibit a normal distribution Nnon−corresponding(0.015, 0.0812), while136
random wild samples follow the distribution Nrandom(0.211, 0.1162). In contrast, the audio-visual137
corresponding samples exhibit a left-skewed distribution with a higher concentration of similar138
instances. When the similarity of samples exceeds the µ + 3σ (0.2564) threshold of the audio-visual139
non-corresponding distribution Nnon−corresponding, they are considered audio-visual corresponding.140
Notably, only 35% of the randomly selected wild data samples exhibit similarities exceeding the141
µ + 3σ (0.2564) threshold of the distribution Nnon−corresponding.142
4

Musical instrument
Wild animalsHuman voice
Vehicle
Livestock, farm animals, working animals
Water
Onomatopoeia
Domestic animals, pets
Domestic sounds, home sounds
Human group actions
Fire
Human locomotion
T ools
DigestiveEngineAlarm
Explosion
Miscellaneous sources
Wood
Mechanisms
Wind Bell
Generic impact sounds
Whistling
Respiratory sounds
Specific impact sounds
Musical concepts
Music genre
Heart sounds, heartbeat
Surface contactThunderstormDeformable shell
HandsSilence
Other sourceless
GlassLiquid
Music roleMusic mood
Acoustic environment
Noise
Sound reproduction
28
210
212
214
216
218
220
222 AVSET-10M
AVSET-700k
VGGSound
Figure 2: Comparison of the sample numbers for each audio category across A VSET-10M, A VSET-
700K, and VGGSound datasets. Classification is carried out based on the secondary audio labels
in AudioSet 3. We pseudo-label each sample from Panda-70M using PANNs [21], while labels on
VGGSound are manually aligned with AudioSet.
Stage 3: Voice-Over Filtering. While the aforementioned filtering method effectively identifies non-143
corresponding samples based on audio-visual similarities, it fails to account for samples containing144
background music and voice-overs. These off-screen sounds, largely irrelevant to the visual content,145
can disrupt the intended audio-visual correspondence. To address this issue, we utilize the audio146
classification network PANNs [21] to label each audio clip, specifically targeting and filtering out147
these voice-overs. Following the classification scheme used in AudioSet, we annotate each audio clip148
with seven primary audio categories and their respective sub-categories. Since speech and music are149
likely added during post-production, we specifically filter out samples that contain these elements150
along with other types of sounds. Other audio categories, such as the sounds of waterfalls and dog151
barking, typically originate from the original video. When these original video sounds co-occur152
with speech or music, it often indicates a high likelihood of off-screen voice interference. It is153
crucial to note that various instrumental sounds fall under the music category; thus, videos featuring154
instrumental performances are not excluded but are instead appropriately retained. Mirroring the155
approach in VGGSound [6], our filtering process aims to eliminate false positive samples—those with156
inappropriate sounds for each category. We refrain from using an audio classifier to select positive157
samples, as this may overlook some hard-to-classify yet criteria-meeting hard-positive audio samples.158
Stage 4: Sample Recycling with Sound Separation. Speech is frequently encountered in wild159
video data, often leading to the inadvertent filtering out of a substantial amount of non-speech audio160
that includes voice-overs. This results in the loss of many potentially useful and valuable samples161
across various audio categories. Inspired by recent advancements in audio research [18], we have162
implemented a sound separation model4 [33] that is specifically designed to isolate sounds that are163
neither speech nor music from audio mixes contaminated with voice-over noise. The outputs from164
this sound separation process are subsequently returned to Stage 2 to verify the correspondence165
between the newly isolated audio and the video.166
3.2 Data Analysis167
We perform comprehensive statistical analyses on the A VSET-10M and A VSET-700K datasets to168
gain detailed insights. For further information about these datasets, please refer to Appendix B.169
Diverse Categories, Abundant Samples. Figure 2 presents a comparative analysis of the number of170
audio categories in A VSET-10M, A VSET-700K, and VGGSound. To ensure consistency in audio171
3https://research.google.com/audioset/ontology/index.html
4https://github.com/ZFTurbo/MVSEP-CDX23-Cinematic-Sound-Demixing
5

Table 2: Comparison of sample numbers after each stage. Due to partial video corruption, we could
only download part of the original dataset. † The numbers here represent the video clips we collected.
A VSET-10M (w/o. A VSET-700K) represents samples filtered from Panda-70M.
Stage Goal A VSET-700K A VSET-10M (w/o. A VSET-700K)
#Num of Clips Proportion #Num of Clips Proportion
S1 Candidate Videos† 1,445,360 100.0% 39,295,551 100.0%
S2 A V-C Filtering 898,366 62.2% 13,824,726 35.2%
S3 V oiceover Filtering 608,062 42.1% 7,124,923 18.1%
S4 Sample Recycling 727,530 50.3% 9,877,475 25.1%
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48
Duration(s)
0
1x105
2x105
3x105
4x105
5x105
6x105
Figure 3: Histogram of Clip Length Distribution in A VSET-10M (w/o. A VSET-700K).
category labels across different datasets, we employ the PANNs [21] audio classification network172
trained on AudioSet to label all samples in A VSET-10M. Subsequently, we manually align the173
labels in VGGSound with those in AudioSet and standardized the audio labels across all three174
datasets as secondary labels. It is evident that A VSET-10M and A VSET-700K encompass a broader175
range of audio types compared to VGGSound, including categories such as silence, liquid, and176
glass. Furthermore, A VSET-10M significantly outperforms A VSET-700K and VGGSound in most177
categories, offering a greater number of audio samples for each audio category.178
Duration Statistics. The samples filtered from Panda-70M include clips of varying lengths. As179
illustrated in Figure 3, we present the statistics for different clip lengths in A VSET-10M (excluding180
A VSET-700K). The total duration of A VSET-10M amounts to 30,418.6 hours, with an average clip181
length of 10.32 seconds. The longest clip spans 49 seconds, while the shortest measures 2 seconds.182
Notably, clips exceeding 10 seconds constitute 19,142.66 hours, representing 62.9% of total duration.183
The Number of Video Samples after Each Filtering Stage. In Table 2, we detail the quantity of184
samples retained at each filtering stage for A VSET-700K and A VSET-10M (excluding A VSET-700K).185
Initially, in stage S2 for A VSET-10M (excluding A VSET-700K), we filter out 64.8% of video samples186
due to lack of audio-visual correspondence. In the subsequent S3 stage, 17.1% of the data containing187
voice-overs is removed. Further, in stage S4, an additional 8.0% of samples with voice-overs is188
refined through sound separation and subsequently recycled into the final audio-visual corresponding189
dataset. It is noteworthy that AudioSet undergoes a meticulous screening process by researchers,190
which results in a higher retention rate of data in the initial stage. A VSET-700K eliminates only191
37.8% of data in its S2 stage.192
3.3 Dataset Verification193
We employ a distinct audio-visual representation learning model [41] different from the one used194
during the sample filtering phase to assess the reliability of our proposed sample filtering process.195
Specifically, we randomly sample data from four different audio-visual sources for validation: (1)196
audio-visual corresponding data from VGGSound, (2) audio-visual non-corresponding data created197
by randomly combining audio and video within VGGSound, (3) wild data randomly sampled from198
AudioSet, and (4) data from A VSET-700K obtained after the comprehensive filtering process. As199
6

Figure 4: The distribution of audio-video cosine similarity of pre-trained model InternVL†
IB++(Ver.)
[41] was evaluated on different sample sets: (1) the audio-visual ccorresponding samples from
VGGSound, (2) the randomly combined audio-visual non- corresponding samples from VGGSound,
(3) the wild samples from AudioSet, and (4) the A VSET-700K sample set filtered with complete
dataset processing. Notably, only 11% of the samples in A VSET-700K fall below theµ+3σ threshold
of non-corresponding distribution Nnon−corresponding.
depicted in Figure 4, we present the distributions of audio-visual similarity for these four sources.200
The mean and standard deviation of these similarities for each data source are detailed in Table 3.201
Table 3: The mean and standard deviation (Std.) of
audio-visual similarity among different sample sets.
Sample Sets Mean Std.
Non-Corresponding (Random) 0.015 0.072
Wild Data (AudioSet) 0.258 0.086
Corresponding (VGGSound) 0.302 0.083
A VSET-700K (ours) 0.303 0.058
A VSET-700K vs. AudioSet. It is evident202
that after data filtering, the audio-visual corre-203
spondence within the dataset is significantly204
enhanced compared to the wild data. The av-205
erage cosine similarity of the A VSET-700K206
data increases from 0.258 to 0.303, while the207
standard deviation decreases from 0.086 to208
0.058. Within the range (µ − 3σ, µ+ 3σ) of209
the normal distribution N′
non-corresponding210
of non-corresponding data, the proportion of211
potential non-corresponding samples is reduced from 35% to 11%. This improvement demonstrates212
that our sample filtering method effectively enhances the audio-visual correspondence in the dataset.213
A VSET-700K vs. VGGSound.As an audio-visual corresponding dataset, VGGSound contains a214
large number of samples with high audio-visual similarity. However, a substantial portion of the215
data exhibits low similarity, with 19% of VGGSound samples falling below the µ + 3σ = 0.231216
threshold of the distribution N′
non-corresponding. In contrast, only about 11% of the samples in217
A VSET-700K have an audio-visual similarity below 0.231, indicating that A VSET-700K contains218
more samples with high audio-visual correspondence. Additionally, A VSET-700K features a smaller219
standard deviation and fewer samples exhibiting extremely low similarity, demonstrating that our220
sample filtering process effectively enhances the robustness of audio-visual correspondence.221
4 Experiment222
We benchmark two audio-visual tasks to explore the audio-visual correspondence: (1) Audio-Video223
Retrieval and (2) Vision-Queried Sound. In audio-video retrieval task, we experiment with A VSET-224
10M and focus on the data scale and the audio-visual temporally consistency. As for Vision-Queried225
Sound Separation, we mainly focus on the impact of each filtering stage, and work on the A VSET-226
700K which is of a similar scale to AudioSet. Specifically, we employ Imagebind [13] to extract the227
average features of 1 frame per second in the video as image features I and InternVid [40] to extract228
the features of the entire video as video features V. Please refer to Appendix A for additional details229
regarding the experiments.230
4.1 Audio-Video Retrieval231
For the audio-video retrieval task, we validate on two audio-visual corresponding datasets, A VE [36]232
and VGGSound [6], and compare the Recall@1 (R@1) and Recall@5 (R@5) from vision to audio.233
For the image+video (I+V) modality, we apply feature weighting similar to [ 41], with the mixed234
7

Table 4: Comparison between the image-based method and the image+video based method on the
task of visual to audio retrieval. The similarity on the diagonal should be the highest in each column.
The correct results are highlighted in green, and the incorrect results are highlighted in red.
(a) Sample1 = {I1,V1,A1}
 (b) Sample3 = {I3,V3,A3}
(c) Sample2 = {I2,V2,A2}
 (d) Sample4 = {I4,V4,A4}
(e) Similarity between Sample1 and Sample2.
I/V to A I1 I2 I1+V1 I2+V2
A1 0.349 0.446 0.351 0.399
A2 0.300 0.409 0.332 0.407
(f) Similarity between Sample3 and Sample4.
I/V to A I3 I4 I3+V3 I4+V4
A3 0.373 0.416 0.388 0.304
A4 0.402 0.457 0.357 0.359
Table 5: Comparison of vision to audio retrieval performance using different methods on ASE and
VGGSound. M denotes the visual features used during retrieval.
A VE VGGSound
#ID M Training Schedule R@1 R@5 R@1 R@5
R1 I AudioSet 18.00 40.11 11.74 28.52
R2 I A VSET-700K 19.10 42.92 13.90 31.68
R3 I A VSET-10M → A VSET-700K 19.11 43.05 13.91 31.94
R4 I+V A VSET-700K 20.55 44.21 14.47 33.62
R5 I+V A VSET-10M → A VSET-700K 20.78 44.47 14.93 34.03
feature fI+V calculated as fI+V = 0.9fI + 0.1fV . In all the audio-video retrieval experiments235
conducted for this paper, we train a separate linear layer for each modality to align representations236
across different modalities, using a batch size of 1024.237
AudioSet vs. A VSET-10M.AudioSet contains a significant number of audio-visual samples that238
do not correspond, adversely affecting audio-video alignment. By employing our filtered dataset,239
A VSET-700, we enhance cross-modal alignment capabilities, achieving a 3.16% improvement in240
VGGSound R@5 performance from R1 to R3 in Table 5. Furthermore, expanding the dataset to 10241
million (R5) entries boosts the model performance on A VE R@5 by an additional 0.26%.242
Based on Image vs. Based on Image+Video. Previous models, which rely solely on image features243
to retrieve audio clips that semantically match the image, lake the capability to evaluate audio-visual244
temporal consistency. As shown in Table 5, by leveraging both image and video features, the R@5245
performance on VGGSound improved by 2.09% from R3 to R5, emphasizing the importance of246
audio-visual temporal consistency.247
Qualitative Analysis. Table 4 presents several qualitative results of audio-video retrieval, under-248
scoring the importance of temporal consistency for effective audio-video retrieval. For example, the249
8

Table 6: Comparison of sound separation performance among various methods on VGGSound. M
stands for the query modality of sound separation.
VGGSound
#ID M Training Schedule SDR↑ SIR↑
Baseline
E1 I VGGSound 5.606 ±0.102 8.074 ±0.161
E2 V VGGSound 6.211±0.105 8.584 ±0.160
Zero-Shot
E3 V AudioSet 5.004 ±0.103 6.781 ±0.164
E4 V AudioSet (w. A V-Correspondence Filtering) 5.646 ±0.101 7.682 ±0.162
E5 V A VSET-700K 5.774±0.103 7.802 ±0.161
Pretraining + Finetune
E6 V AudioSet (w. A V-Correspondence Filtering) →VGGSound 6.548 ±0.103 9.251 ±0.158
E7 V A VSET-700K →VGGSound 6.666±0.102 9.377 ±0.158
image-based method could only deduce that engine roar should be present in the audio based on the250
image of a sports car, but it fails to determine when the sound should cease, leading to unsuccessful251
audio-video pairing. In contrast, when both image and video features are considered, the similarity252
between mismatched sample pairs 1 and 2 is reduced from 0.446 to 0.399, thereby achieving correct253
audio-video pairing.254
4.2 Vision-Queried Sound Separation255
As shown in Table 6, we present the performance of vision-queried sound separation based on256
different modalities across various datasets. We utilize the framework of CLIPSep [10] to implement257
sound separation models across various modalities.258
Image-Queried vs. Video-Queried. Compared to the sound separation model based on image259
queries (E1), the model utilizing video queries (E2) demonstrates superior performance, with the260
Signal-to-Distortion Ratio (SDR) improving by 0.605. This enhancement highlights the importance261
of audio-visual temporal consistency within the audio-visual research.262
Corresponding vs. Non-Corresponding. Audio-visual correspondence is critical for effective263
sound separation. Models trained on the non-corresponding AudioSet (E3) encounter difficulties in264
achieving accurate separation and fail to capture proper audio-visual alignment. After implementing265
audio-visual correspondence filtering (E4), the dataset shows a marked improvement in audio-visual266
correspondence, as evidenced by a 0.642 increase in the Signal-to-Distortion Ratio (SDR). Despite267
this advancement, the presence of voice-over content continues to challenge the alignment between268
audio and visual modalities. Following a comprehensive filtering process, the model (E5) trained269
on A VSET-700K exhibits exceptional zero-shot sound separation capabilities, achieving an SDR of270
5.774. This significant enhancement underscores the effectiveness of our proposed filtering process.271
5 Conclusion272
Audio-visual correspondence datasets are vital for research in the audio-video field. Using a sophisti-273
cated sample filtering process with AudioSet and Panda-70M as sources, we develop A VSET-10M,274
the first open, large-scale dataset with high audio-visual correspondence, featuring ten million audio-275
visual corresponding samples across 527 audio categories. Verification experiments demonstrate276
that A VSET-10M surpasses previous datasets in terms of audio-visual correspondence. We also277
benchmark audio-video retrieval and vision-guided sound separation tasks, demonstrating the critical278
role of audio-video temporal consistency in this field. Our A VSET-10M provides greater opportunities279
for advancement in this field.280
9

Content-based multimedia355
information retrieval: State of the art and challenges. ACM Transactions on Multimedia356
Computing, Communications, and Applications (TOMM), 2(1):1–19, 2006.357
[24] Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, Ji-Rong Wen, and Di Hu. Learning358
to answer questions in dynamic audio-visual scenarios. In Proceedings of the IEEE/CVF359
Conference on Computer Vision and Pattern Recognition, pages 19108–19118, 2022.360
[25] Wang Lin, Tao Jin, Wenwen Pan, Linjun Li, Xize Cheng, Ye Wang, and Zhou Zhao. Tavt:361
Towards transferable audio-visual text generation. In Proceedings of the 61st Annual Meeting of362
the Association for Computational Linguistics (Volume 1: Long Papers), pages 14983–14999,363
2023.364
[26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.Advances365
in neural information processing systems, 36, 2024.366
[27] Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao. Diff-foley: Synchronized video-367
to-audio synthesis with latent diffusion models. Advances in Neural Information Processing368
Systems, 36, 2024.369
11

[28] Annamaria Mesaros, Aleksandr Diment, Benjamin Elizalde, Toni Heittola, Emmanuel Vincent,370
Bhiksha Raj, and Tuomas Virtanen. Sound event detection in the dcase 2017 challenge.371
IEEE/ACM Transactions on Audio, Speech, and Language Processing, 27(6):992–1006, 2019.372
[29] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and373
Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million374
narrated video clips. In Proceedings of the IEEE/CVF international conference on computer375
vision, pages 2630–2640, 2019.376
[30] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed repre-377
sentations of words and phrases and their compositionality. Advances in neural information378
processing systems, 26, 2013.379
[31] Tanzila Rahman, Bicheng Xu, and Leonid Sigal. Watch, listen and tell: Multi-modal weakly380
supervised dense event captioning. In Proceedings of the IEEE/CVF international conference381
on computer vision, pages 8908–8917, 2019.382
[32] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,383
Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-384
5b: An open large-scale dataset for training next generation image-text models. Advances in385
Neural Information Processing Systems, 35:25278–25294, 2022.386
[33] Roman Solovyev, Alexander Stempkovskiy, and Tatiana Habruseva. Benchmarks and leader-387
boards for sound demixing tasks, 2023.388
[34] Fabian-Robert Stöter, Antoine Liutkus, and Nobutaka Ito. The 2018 signal separation evaluation389
campaign. In Latent Variable Analysis and Signal Separation: 14th International Conference,390
LVA/ICA 2018, Guildford, UK, July 2–5, 2018, Proceedings 14, pages 293–305. Springer, 2018.391
[35] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland,392
Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research.Communications393
of the ACM, 59(2):64–73, 2016.394
[36] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event395
localization in unconstrained videos. In Proceedings of the European conference on computer396
vision (ECCV), pages 247–263, 2018.397
[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,398
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open399
foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.400
[38] Efthymios Tzinis, Scott Wisdom, Aren Jansen, Shawn Hershey, Tal Remez, Daniel PW Ellis,401
and John R Hershey. Into the wild with audioscope: Unsupervised audio-visual separation of402
on-screen sounds. arXiv preprint arXiv:2011.01143, 2020.403
[39] Efthymios Tzinis, Scott Wisdom, Tal Remez, and John R Hershey. Audioscopev2: Audio-visual404
attention architectures for calibrated open-domain on-screen sound separation. In European405
Conference on Computer Vision, pages 368–385. Springer, 2022.406
[40] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen,407
Xinyuan Chen, Yaohui Wang, et al. Internvid: A large-scale video-text dataset for multimodal408
understanding and generation. arXiv preprint arXiv:2307.06942, 2023.409
[41] Zehan Wang, Ziang Zhang, Xize Cheng, Rongjie Huang, Luping Liu, Zhenhui Ye, Haifeng410
Huang, Yang Zhao, Tao Jin, Peng Gao, et al. Molecule-space: Free lunch in unified multimodal411
space via knowledge fusion. arXiv preprint arXiv:2405.04883, 2024.412
[42] Jiannan Xiang, Guangyi Liu, Yi Gu, Qiyue Gao, Yuting Ning, Yuheng Zha, Zeyu Feng, Tianhua413
Tao, Shibo Hao, Yemin Shi, Zhengzhong Liu, Eric P. Xing, and Zhiting Hu. Pandora: Towards414
general world model with natural language actions and video states. 2024.415
12

[43] Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, and Qifeng Chen. Seeing and hearing:416
Open-domain visual-audio generation with diffusion latent aligners, 2024.417
[44] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu,418
and Baining Guo. Advancing high-resolution video-language representation with large-scale419
video transcriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and420
Pattern Recognition, pages 5036–5045, 2022.421
13

A Implementation Details422
A.1 Sound Separation423
Same as the experimental setting of [10], for all audio samples, we conduct experiments on samples424
of length 65535 (approximately 4 seconds) at a sampling rate of 16 kHz. For spectrum computation,425
we employ a short-time Fourier transform (STFT) with a filter length of 1024, a hop length of 256,426
and a window size of 1024. All images are resized to 224 × 224 pixels. All models are trained with a427
batch size of 128, using the Adam optimizer with parameters β1 = 0.9, β2 = 0.999, and ϵ = 10−8,428
for 200,000 steps. Additionally, we employ warm-up and gradient clipping strategies, following [10].429
We compute the signal-to-distortion ratio (SDR) using museval [34]. All experiments are conducted430
on a single A800 GPU.431
A.2 Audio-Video Retrieval432
Same as the experimental setting of [41], for all experiments, the softmax temperature is set to 0.01,433
and the temperature for the InfoNCE loss is set to 0.02. We utilize the Adam optimizer with a learning434
rate of 1 × 10−3 and a batch size of 2048, running the training process for 20 epochs.435
B A VSET-10M436
B.1 Samples of A VSET-10M437
We present some audio-video consistency samples from the A VSET-10M in Figure 5. For additional438
samples, please visit the demo page at https://avset-10M.github.io.439
B.2 Dataset Composition440
We release A VSET-10M as the following two subsets:441
• A VSET-700K: This subset comprises 727,530 audio-visual corresponding samples filtered from442
AudioSet. Each video segment in this subset is accompanied by a manually labeled audio category,443
ensuring accurate categorization and relevance.444
• A VSET-10M (w/o. A VSET-700K): This subset comprises 10,234,280 audio-visual corresponding445
samples, filtered from the Panda-70M dataset. Each video segment is semantically coherent,446
focusing on a single event, and includes a text description originally from the Panda70M dataset.447
Additionally, we provide pseudo-labels for the audio categories, derived with PANNs [21], along448
with their corresponding confidence scores. Researchers can use these pseudo-labels to freely449
partition the dataset.450
We provide comprehensive meta-information for each video clip, including the URL of the video,451
timestamps for each clip, audio-visual cosine similarity, a flag indicating whether sound separation is452
required, and relevant text labels. For A VSET-10M (w/o. A VSET-700K), captions and pseudo-labels453
are included, while A VSET-700K features manual audio labels.454
B.3 Download URL455
Please visit https://avset-10M.github.io to get the A VSET-10M.Privacy Notice: If any video456
clips in this dataset infringe upon your privacy, please contact us for their removal.457
B.4 LICENSE458
A VSET-10M is released under the [CC BY 4.0] license. Before using this dataset, please ensure that459
you have read and understood the terms of the license.460
14

(a) Audio-Vision Cosine Similarity θ = 0.479.
(b) Audio-Vision Cosine Similarity θ = 0.442.
(c) Audio-Vision Cosine Similarity θ = 0.408.
(d) Audio-Vision Cosine Similarity θ = 0.404.
(e) Audio-Vision Cosine Similarity θ = 0.392.
(f) Audio-Vision Cosine Similarity θ = 0.335.
Figure 5: Audio-video consistency samples in A VSET.
15

C Limitation461
Since most existing video datasets predominantly contain clips with speech audio, which limits the462
amount of non-speech samples, we plan to utilize more diverse data sources in the future. This463
strategy aims to enhance the diversity of sample types and enable us to develop a more balanced and464
expansive dataset.465
D Ethical Impact466
This paper primarily focuses on proposing a large-scale audio-visual correspondence dataset, aimed467
at expanding research possibilities in the audio-visual sector. This field includes technologies like468
video dubbing, which can lead to audio forgery. However, it’s crucial to note that such dubbing does469
not result in severe identity forgery issues, unlike those caused by voice cloning technologies.470
16

Checklist471
1. For all authors...472
(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s473
contributions and scope? [Yes] See Abstract and Section 1474
(b) Did you describe the limitations of your work? [Yes] See Section C475
(c) Did you discuss any potential negative societal impacts of your work? [Yes] See476
Section D.477
(d) Have you read the ethics review guidelines and ensured that your paper conforms to478
them? [Yes] We have read and confirmed that we meet the specifications.479
2. If you are including theoretical results...480
(a) Did you state the full set of assumptions of all theoretical results? [N/A]481
(b) Did you include complete proofs of all theoretical results? [N/A]482
3. If you ran experiments (e.g. for benchmarks)...483
(a) Did you include the code, data, and instructions needed to reproduce the main experi-484
mental results (either in the supplemental material or as a URL)? [Yes] See Appendix485
B.3. Our experiments are based on other open source work (Imagebind and ClipSep).486
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they487
were chosen)? [Yes] See Appendix A.488
(c) Did you report error bars (e.g., with respect to the random seed after running experi-489
ments multiple times)? [Yes] See Table 6.490
(d) Did you include the total amount of compute and the type of resources used (e.g., type491
of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix A.492
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...493
(a) If your work uses existing assets, did you cite the creators? [Yes] Our A VSET-10M494
dataset is built upon AudioSet and Panda-70M, and we have ensured proper citation of495
these sources.496
(b) Did you mention the license of the assets? [Yes] See Appendix B.4.497
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]498
See Appendix B.3.499
(d) Did you discuss whether and how consent was obtained from people whose data you’re500
using/curating? [Yes] See Section 3.1.501
(e) Did you discuss whether the data you are using/curating contains personally identifiable502
information or offensive content? [Yes] See Appendix B.3.503
5. If you used crowdsourcing or conducted research with human subjects...504
(a) Did you include the full text of instructions given to participants and screenshots, if505
applicable? [N/A]506
(b) Did you describe any potential participant risks, with links to Institutional Review507
Board (IRB) approvals, if applicable? [N/A]508
(c) Did you include the estimated hourly wage paid to participants and the total amount509
spent on participant compensation? [N/A]510
17