MedTrinity-25M: A Large-scale Multimodal Dataset
with Multigranular Annotations for Medicine
Yunfei Xie1,∗, Ce Zhou1,∗, Lang Gao1,∗, Juncheng Wu2,∗, Xianhang Li3,
Hong-Yu Zhou4, Sheng Liu5, Lei Xing5, James Zou5, Cihang Xie3, Yuyin Zhou3
⋆equal technical contribution
1Huazhong University of Science and Technology, 2Tongji University,
3UC Santa Cruz, 4Harvard University, 5Stanford University
Abstract
This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal1
dataset for medicine, covering over 25 million images across 10 modalities, with2
multigranular annotations for more than 65 diseases. These enriched annotations3
encompass both global textual information, such as disease/lesion type, modality,4
region-specific descriptions, and inter-regional relationships, as well as detailed5
local annotations for regions of interest (ROIs), including bounding boxes, seg-6
mentation masks. Unlike existing approach which is limited by the availability7
of image-text pairs, we have developed the first automated pipeline that scales8
up multimodal data by generating multigranular visual and texual annotations (in9
the form of image-ROI-description triplets) without the need for any paired text10
descriptions. Specifically, data from over 90 different sources have been collected,11
preprocessed, and grounded using domain-specific expert models to identify ROIs12
related to abnormal regions. We then build a comprehensive knowledge base13
and prompt multimodal large language models to perform retrieval-augmented14
generation with the identified ROIs as guidance, resulting in multigranular tex-15
ual descriptions. Compared to existing datasets, MedTrinity-25M provides the16
most enriched annotations, supporting a comprehensive range of multimodal tasks17
such as captioning and report generation, as well as vision-centric tasks like clas-18
sification and segmentation. This dataset can be utilized to support large-scale19
pre-training of multimodal medical AI models, contributing to the development of20
future foundation models in the medical domain. The dataset is publicly available21
at https://yunfeixie233.github.io/MedTrinity-25M/.22
1 Introduction23
Large-scale multimodal foundation models [ 1, 2, 3, 4, 5] have demonstrated remarkable success24
across various domains due to their ability to understand complex visual patterns in conjunction with25
natural language. This success has sparked significant interest in applying such models to medical26
vision-language tasks. Much progress has been made to improve the medical capacity of general27
domain multimodal foundation models by constructing medical datasets with image-text pairs and28
fine-tuning general domain models on these datasets [6, 7, 8, 9, 10].29
However, current medical datasets have several limitations. Firstly, these datasets lackmultigranular30
annotations that reveal the correlation between local and global information within medical images.31
Submitted to the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets
and Benchmarks. Do not distribute.

Medical images often contain detailed cues, such as regional abnormal textures or structures, which32
may indicate specific types of lesions. Therefore, multimodal models need the ability to infer global33
information, such as disease or lesion type, from local details. The absence of such data limits34
the models’ capacity to comprehensively understand medical images. Moreover, current dataset35
construction methods heavily rely on medical images paired with reports or captions, which restricts36
their scalability.37
In this paper, we address the above challenges by proposing an automated data construction pipeline38
using multimodal large language models (MLLMss) without relying on paired text descriptions. To39
address the lack of comprehensive medical knowledge in general-purpose MLLMs, we leverage40
domain-specific expert grounding models and retrieval-augmented generation (RAG) to extract41
relevant medical knowledge. We then prompt MLLMs to generate multigranular visual and textual42
annotations enriched with this knowledge based on identified regions of interest (ROIs). We utilize43
this pipeline to transform the collected data, including large-scale unpaired images, into image-44
ROI-description triplets. These triplets provide multigranular annotations that encompass both45
global textual information, such as disease/lesion type, modality, and inter-regional relationships,46
as well as detailed local annotations for ROIs, including bounding boxes, segmentation masks, and47
region-specific textual descriptions. Using the proposed pipeline, we create a large-scale multimodal48
multigranular medical dataset containing over 25 million triplets, named MedTrinity-25M. To our49
best knowledge, this is the largest multimodal dataset in medicine to date.50
Initially, we assemble a large amount of medical data from over 90 online resources such as TCIA,51
Kaggle, Zenodo, Synapse, etc. In addition to images with a small amount of high-quality paired52
manual reports, this assembled data also includes two types of coarse medical data: 1) Image53
data with segmentation masks, lesion bounding boxes, or only disease types but lacking detailed54
textual descriptions, and 2) Images paired with coarse captions that describe only global modality55
or disease information, but lack detailed descriptions of local regions. To generate multigranular56
annotations from the massive coarse medical data, we first identify ROIs that contain disease or lesion57
patterns by applying expert grounding models. We then build a comprehensive knowledge base from58
online corpora (e.g., PubMed) and retrieve image-related medical knowledge. Finally, we prompt59
MLLMs to integrate medical knowledge with guidance of identified ROIs to generate multigranular60
textual descriptions.61
2 Related Work62
Medical Multimodal Foundation Models. Due to the effectiveness of multimodal foundation63
models in understanding visual features, adapting these models to perform medical vision-language64
tasks has garnered increasing attention in recent years [ 11, 12, 9, 5]. Several papers attempt to65
adapt general domain multimodal foundation models with varying architecture to medical domain66
through end-to-end training on medical datasets. For example, Med-Flamingo [ 11] enhances the67
medical capacity of OpenFlamingo-9B [ 13] by fine-tuning it with 0.8M interleaved and 1.6M68
paired medical image-text data. While Med-PalM [ 12] adapts PaLM-E [ 14] to medical domain69
using approximately 1M medical data points, demonstrating competitive or surpassing performance70
compared to state-of-the-art models. Additionally, LLaV A-Med [ 9] employs end-to-end visual71
instruction tuning [ 1] with two stages, achieving remarkable results in medical Visual Question72
Answering (VQA) tasks. Similarly, Med-Gemini [ 15] employs a long-form question answering73
dataset to enhance the multimodal and long-context capabilities of baseline Gemini [16]. Although74
these models have achieved remarkable performance, they are still limited by the scale of training75
data. Prior research [ 17] has shown that scaling up the training data improves the performance of76
large multimodal foundation models. In this paper, we aim to build a large-scale medical dataset to77
facilitate the development of more powerful medical multimodal foundation models.78
Multimodal Datasets for medicine. The significance of construting comprehensive medical79
multimodal datasets has garnered considerable attention [ 9, 18, 19, 7]. Several works attempt to80
collect images and paired clinical reports prepared by pathology specialist [19, 7, 8], which provide81
2

MIMIC-CXR
EXAMINATION:  CHEST (PA AND LAT)
INDICATION:  ... year old man with pleural effusion  // eval
TECHNIQUE:  Chest PA and lateral...
COMPARISON:  Chest radiograph from ...
FINDINGS: There is been reaccumulation of a moderate left pleural 
effusion common new since the most recent previous study. There is 
likely also concomitant left  basilar atelectasis. The right lung is clear. 
There is no pneumothorax. Calcified granuloma is are noted in the right 
lower lobe. The aorta is tortuous but unchanged in configuration.
IMPRESSION: 
Reaccumulation of moderate left pleural effusion.
Med-Trinity-25M
(Ours)
This chest X-Ray image, taken from the PA position,shows the 
thoracic cavity, including the lungs, heart, and surrounding 
structures. The lungs occupy the majority of the image, with the 
heart situated centrally. The region of interest is located horizontally 
to the right and vertically in the lower center, covering approximately 
15% of the image. Within this region, there is an unusual opacity, 
suggesting a pleural effusion. The content in this area indicates fluid 
accumulation that is likely affecting the adjacent lung and potentially 
influencing the position and function of surrounding structures.
ROI 
Analysis
Lesion
TextureModality Local-global
Relation
Structure
Detection
ROI 
AnalysisModality
(a) Qualitative Comparison with sample in radiology report of chest x-rays dataset MIMIC-CXR [21].
Med-Trinity-25M
(Ours)
The image is a chest CT scan prominently displaying the lungs with 
the heart not visible. The left-center horizontally and middle 
vertically situated region of interest, covering 1.0% of the area, 
shows a potential abnormality in lung tissue. This area contains a 
texture or density that differs from the surrounding lung tissue, 
possibly indicating lung cancer. The affected area might be 
influencing adjacent tissues, suggesting a local progression of the 
disease without direct implication on distant parts of the lung.
ROI 
Analysis
Lesion
TextureModality Local-global
Relation
Structure
Detection
SLAKE
Q: "What modality is used to take this image?", A: "CT" 
Q: "Which part of the body does this image belong to?", A: "Chest"
Q: "What is the main organ in the image?", A: "Lung"
Q: "Does the picture contain lung?", A: "Yes"
Q: "Does the picture contain heart?", A: "No"
Q: "What diseases are included in the picture?", A: "Lung Cancer"
Q: "Where is/are the abnormality located?", A: "Right Lung, Left"
ROI 
AnalysisModality Structure
Detection
(b) Qualitative Comparison with sample in visual QA dataset SLAKE [22].
ROCO
A 49-year-old man presenting 
a pancreatic neoplasia with 
peritoneal carcinomatosis. 
Axial T2W TSE fat-suppressed 
M R I  s h o w s  t w o  h e p a t i c 
p e r i c a p s u l a r  i m p l a n t s  o f 
peritoneal carcinomatosis 
(arrowheads), biconvex, in 
high signal iontensity. Med-Trinity-25M
(Ours)
The image is an axial T2W TSE fat-suppressed MRI focusing on the liver and surrounding areas, 
highlighting two hepatic pericapsular implants indicative of peritoneal carcinomatosis, marked by 
their high signal intensity and biconvex shape. These abnormalities, located on the right side of the 
liver, are positioned horizontally to the left and vertically at the bottom of the image, occupying 
about 1.5% of the area. The region of interest reveals these unusual features, contrasting with the 
normal liver texture and appearance. These hepatic implants are significant as they suggest a 
spread from the primary pancreatic neoplasia, indicating a direct relationship where the primary 
disease has metastasized to adjacent organs, further complicating the patient's condition.
ROI 
Analysis
Lesion
TextureModality Local-global
Relation
Structure
DetectionModality Structure
Detection
Lesion
Texture
(c) Qualitative Comparison with sample in radiology objects caption dataset ROCO [18].
Figure 1: Qualitative comparison with different types of dataset.
comprehensive descriptions of images, including disease types and corresponding reasoning. For82
example, MIMIC-CXR[8] comprises 227,835 images for 65,379 patients, containing pathological83
findings and impressions in reports paired with each images. However, manually constructing such84
reports is both time-consuming and expensive, thereby limiting the scale of these datasets. PMC-85
OA [20] aims to expand the dataset scale by extracting a large number of image-caption pairs from86
medical papers, increasing the number of data samples to 1.65 million. However, the extracted87
captions are less detailed compared to manual clinical reports, resulting in a lack of multigranular88
annotations. RadGenome-Chest CT [19] includes more detailed annotations, such as segmentation89
masks and medical reports generated by MLLMs. Nonetheless, its construction method still relies90
on paired image-text data, which limits its scalability. Unlike these existing methods, we devise the91
first automated data construction pipeline to generate multigranular annotations for unpaired images,92
achieving a comprehensive multigranular dataset with 25 million data samples.93
3 MedTrinity-25M Dataset94
3.1 Data Triplet95
Our dataset comprises triplets of {image, ROI, description}. Each ROI is associated with an96
abnormality and is represented by a bounding box or a segmentation mask, specifying the relevant97
region within the image. For each image, we provide a multigranular textual description, which98
includes the disease/lesion type, modality, region-specific description, and inter-regional relationships99
as illustrated in Figure 2.100
3

Images. We use the original medical image in the source dataset, we extensively collected medical101
datasets from the following sources: (1) online resources such as TCIA, Kaggle, Zenodo, Synapse,102
Hugging Face,Grand Challenge , GitHub, etc. (2) relevant medical dataset research, such as CheX-103
pert [7] and DeepLesion [ 23]. These datasets were first categorized into two types: (1) datasets104
containing local annotations, such as MIMIC-CXR [8] with corresponding radiology reports, and105
PMC-OA [24] with corresponding captions, where the reports or captions provide analysis of specific106
local conditions in the images; another example is the 3D image segmentation dataset BraTS2024 [25],107
which marks the tumor regions in CT scans with masks. (2) datasets containing global annotations:108
such as image classification datasets ISIC2019 [26] and ISIC2020 [27], whose classification labels109
reflect the overall pathological condition of tissue sections; another example is the CheXpert [ 7]110
dataset, which provides detailed classification of disease types for each chest X-ray. We collect111
25,001,668 samples spanning 10 modalities and over 65 diseases. For 3D volumetric images stored112
in DICOM or NIfTI formats, we converted each 2D slice to PNG format. Additional caption and113
annotations like masks and bounding boxes from these datasets were utilized to construct ROIs and114
corresponding textual descriptions as below.115
ROIs. For each image, ROIs are highlighted using segmentation masks or bounding boxes. These116
ROIs mostly contain pathological findings such as lesions, inflammation, neoplasms, infections, or117
other potential abnormalities. In the few cases without abnormalities, the ROIs generally indicate the118
primary object or organ in the image, as shown in examples in the supplementary material.119
Textual Descriptions. The textual descriptions for each image are provided with detailed infor-120
mation across various aspects. Unlike the unstructured free-text descriptions found in previous121
medical report datasets[7, 8, 6] or simple short sentences in visual QA dataset[28, 22] and caption122
dataset[18, 24], our textual descriptions are multigranular and structured. General attributes related to123
the image are described first, including the image modality, the specific organ depicted, and the type124
of disease presented. Subsequently, ROI-related information is provided, including their locations125
and the abnormal characteristics within them that indicate underlying pathology, such as distinctive126
color and texture. Additionally, comparisons between the ROIs and surrounding regions are presented127
to highlight differences in features and the extent of disease progression.128
We also demonstrate the multigranular textual descriptions in our dataset with those in other common129
forms. As illustrated in Figure 1, our textual description is multigranular with more attributes130
than radiology report of chest x-rays dataset MIMIC-CXR [21], visual QA dataset SLAKE[22] and131
radiology objects caption dataset ROCO[18].132
3.2 Data Construction Pipeline133
Given a medical image, we aim to generate corresponding multigranular visual and texual annotations134
by leveraging MLLMs. Specifically, as shown in Figure 2, our pipeline can be decomposed into two135
stages - Data Processing and Generation of Multigranular Text Description. In the Data Pro-136
cessing stage (Section 3.2.1), we address the lack of domain-specific knowledge in general-purpose137
MLLMs by leveraging expert grounding models and retrieval-augmented generation (RAG). This138
stage includes three key steps: 1) Metadata Integration to produce coarse captions encapsulating139
fundamental image information such as modality and disease types; 2) ROI Locating to identify140
regions of abnormalities; and 3) Medical Knowledge Retrieval to extract relevant fine-grained141
medical details. Based on the processed data, we then prompt MLLMs to generate multigranular text142
descriptions, resulting in the creation of fine-grained captions, as detailed in Section 3.2.2.143
3.2.1 Data Processing144
Coarse Caption Generation via Metadata Integration. We aim to generate coarse captions that145
provide fundamental information for a given image, including modality, organ labels, disease types,146
and optionally, camera views and equipment information. Instead of extracting features directly from147
the images, we generate these captions by integrating dataset metadata. We first extract metadata from148
4

CoarseCaption
ROI
…
KnowledgeBase
MedicalKnowledge
(1) Data Processing
Report or QA
Classification
Mask or B-box
 KnowledgeRetrieval
 ROILocating
 MetadataIntegration
Give me detailed description of the image, based oncoarsecaption, lesion region, medical knowledge... 
Prompt
MLLM
(2) MultigranularTextualDescription Generation
ModalityOrgan & tissue DetectionROI Analysis Lesion TextureLocal-global relationship
Multigranular Textual Description
Data Triplet
A chest X-ray.Showing lungs centrally located within the thoracic cavity.ROI is positioned horizontally at the left-center and vertically at …,Region exhibits increased opacity and irregular texture, indicating …Showing a pattern of bilateral lung involvement typically seen in COVID-19 cases.
ROI
Image
Multigranular Description
Figure 2: Data construction pipeline. 1) Data processing: extracting essential information from
collected data, including metadata integration to generate coarse caption, ROI locating, and medi-
cal knowledge collection. 2) Multigranular textual description generation: using this information to
prompt MLLMs to generate fine-grained captions.
Coarse
Caption
Without coarse caption:
The image is a chest X-ray showing detailed 
views of the lungs and heart. The lungs occupy 
the majority of the thoracic cavity, which is the 
region encased by the rib cage, extending from 
the collarbone to the diaphragm. The heart is 
located centrally just beneath the ribs, slightly 
tilted to the left. No medical devices are visible in 
the image.
With coarse caption:
The image is a chest X-ray showing both lungs, 
centrally positioned in the thoracic cavity, flanked 
by the ribs and the diaphragm visible at the bottom. 
The heart is visible in the center between the 
lungs. There are no......The lungs show patchy 
opacities suggesting an infectious process, 
consistent with pulmonary involvement in 
COVID-19.
“A chest X-Ray 
image with COVID-
19 in the lungs”
Specify
Disease
Figure 3: A qualitative comparison example of generated textual description with and without
coarse caption. Without a coarse caption, MLLMs fails to detect diseases. On the contrary, providing
a caption mentioning “COVID-19” allows MLLMs to identify and categorize the disease, facilitating
further analysis.
the datasets and then apply a fixed rule to integrate this information into coarse captions. For example,149
for an image from the QaTa-COV19 dataset1, we derive metadata from the dataset’s accompanying150
paper or documentation, indicating that it consists of COVID-19 chest X-ray images. Next, we151
construct coarse captions like “A chest X-ray image with COVID-19 in the lungs” highlighting the152
modality, organ types, and disease labels. If the image contains additional textual information like153
radiological findings, this is also integrated to enhance the richness of the caption. The effectiveness154
of adding coarse captions when generating fine-grained captions is illustrated in Figure 3. In contrast155
to the scenario without a coarse caption where MLLMs fails to recognize the disease, providing156
MLLMs with a coarse caption that includes the disease type “COVID-19” enables it to identify and157
categorize the disease, thereby laying the foundation for further analysis.158
ROI Locating. We employ various strategies to locate Regions of Interest (ROIs) in images. For159
datasets that already include localization annotations, such as segmentation masks or bounding boxes,160
we derive the ROIs from these existing annotations. Specifically, bounding boxes are directly used161
1https://www.kaggle.com/aysendegerli/qatacov19-dataset.
5

Without ROIs:
The image is a chest X-ray showing 
both lungs, centrally positioned in 
the thoracic cavity, flanked by the 
ribs and the diaphragm visible at the 
bottom. The heart is visible in the 
center between the lungs. There are 
no...... The lungs show patchy 
opacities suggesting an infectious 
process, consistent with pulmonary 
involvement in COVID-19.
With ROIs:
The image is a chest X-ray showing both lungs and the heart 
centrally positioned between them. In two specific regions of 
interest located at the left-center and right-center of the middle 
of the lungs, there are unusual findings suggestive of COVID-19. 
These areas, occupying 8.3% and 5.0% of the image respectively, 
display changes in lung texture that may indicate infection, such 
as increased opacity. The left-center region is slightly larger and 
potentially indicates a more extensive involvement of the lung tissue 
compared to the right-center region. These areas of alteration in the 
lung tissue are critical in understanding the spread and impact of 
COVID-19, affecting surrounding lung areas.
ROIs
ROI 
analysis
Region
Relationship
Figure 4: A qualitative comparison example of generated textual description with and without
locating ROIs. Without ROIs, the caption offers only a brief global analysis; with ROIs, MLLMs con-
ducts detailed local analysis and assesses the impact of lesion ROIs on adjacent normal regions.
Without medical knowledge：
The image is a chest X-ray showing both lungs and 
the heart centrally positioned between them. In two 
specific regions of interest located at ...... of the 
image respectively, display changes in lung texture 
that may indicate infection, such as increased 
opacity. The left-center region is slightly larger and 
potentially indicates a more extensive involvement of 
the lung tissue compared to the right-center region. 
These areas of alteration in the lung tissue are critical 
in understanding the spread and impact of COVID-19, 
affecting surrounding lung areas.
external medical 
knowledge
“glass opacities”, 
“consolidation” With medical knowledge：
The image is a chest X-ray showing the thoracic 
cavity, primarily focusing on the lungs. Visible organs 
include the lungs and the heart, centrally positioned 
beneath the sternum and between the lungs. The 
regions of interest, located...... These regions 
exhibit ground-glass opacities and consolidation, 
typical indicators of COVID-19 pneumonia, which 
suggest the presence of inflammatory processes. 
These affected areas are significant as they indicate 
the primary sites of infection and inflammation in 
COVID-19, often leading to bilateral and multifocal 
lung involvement as the disease progresses.
Standardize  
Terminology
Revise 
Diagnosis
Figure 5: A qualitative comparison example of generated textual description with and without
external medical knowledge. MLLMs can standardize medical terminology in its expressions and
refine its diagnosis based on disease progressions detailed in medical literature.
as the ROIs, while segmentation masks are converted to ROIs by creating the smallest bounding162
box that covers the mask. When such localization annotations are not available, we apply different163
pretrained expert models listed in the Appendix to generate ROIs. For text-prompt driven grounding164
model[29], we use disease and organ information in coarse captions as text prompts to guide the165
model in segmenting specific parts. Examples of generated ROIs from various modalities with166
different models are demonstrated in Figure 6.167
Without ROIs, the original description is limited to a brief global analysis of the image. However,168
with ROIs, MLLMs can perform a more detailed local analysis of the ROIs and assess the impact of169
lesion ROIs on the surrounding normal regions, as demonstrated in Figure 4.170
Medical Knowledge Retrieval. General-purpose MLLMs often produce content that lacks spe-171
cialized medical terminology and professional expression. To address this issue, we build a medical172
knowledge database following the approach in MedRAG [ 32]. We collect three main corpora:173
PubMed2 for biomedical knowledge, StatPearls 3 for clinical decision support, and medical text-174
books [33] for domain-specific knowledge. We segment these corpora into short snippets and encode175
2https://pubmed.ncbi.nlm.nih.gov/
3https://www.statpearls.com/
6

Textual description of ROI
horizontally: right-center 
vertically: lower-middle 
area ratio:1.2%
ROI
(a) Example of locating ROI via
SAT[29].
Textual description of ROI
horizontally: center 
vertically: middle
area ratio:21.2%
ROI
(b) Example of locating ROI via
BA-Transformerr [30].
Textual description of ROI
horizontally: right
vertically: lower-middle
area ratio:8.5%
ROI
(c) Example of locating ROI via
MedRPG [31].
Figure 6: Example of ROIs and their corresponding textual descriptions.
Knowledge 1: 
Title: Mobile chest X-ray manifestations of 54 deceased patients with coronavirus disease 2019: Retrospective study.
Content: ...... We found that 50 (93%) patients with lesions occurred in the bilateral lung, 4 (7%) patients occurred in the right lung, 54 (100%) 
patients were multifocal involvement. The number of lung fields involved was 42 (78%) patients in 6 fields, 3 (6%) patients in 5 lung fields, 4 
(7%) patients in 4 lung fields, and 5 (9%) patients in 3 lung fields. Fifty-three (98%) patients had patchy opacities, 3 (6%) patients had round or 
oval solid nodules, 9 (17%) patients had fibrous stripes, 13 (24%) patients had pleural effusion, 8 (15%) patients had pleural thickening, 6 
(11%) patients had pneumothorax, 3 (6%) patients had subcutaneous emphysema. Among the 24 patients who had serial mobile chest X-rays, 
16 (67%) patients had the progression of the lesions, 8 (33%) patients had no significant change of the lesions, and there was no case of 
reduction of the lesions.The mobile chest X-ray manifestations of deceased patients with COVID-19 were mostly bilateral lung, multifocal 
involvement, and extensive lung field, and pleural effusion, pleural thickening, and pneumothorax probably could be observed. The 
serial mobile chest X-ray showed that the chest lesions were progressive with a high probability.
.......
Figure 7: An example of the Top-8 retrieval results. By leveraging COVID-19-related medical
knowledge, MLLMs can standardize medical terminology and enhance diagnoses according to the
disease progressions described in medical literature.
them into high-dimensional vectors using the text encoder from Med-CPT [ 34]. These vectors176
are then indexed into a specialized vector knowledge base using Faiss[35], optimized for efficient177
retrieval.178
For a given image, we retrieve relevant medical knowledge by using its coarse caption, which is179
generated through metadata integration. Specifically, we encode the coarse captions, including disease180
and organ classifications, into vectors using the Med-CPT text encoder. We then perform a vector181
similarity search in the medical vector database, retrieving the top eight medical knowledge snippets182
that semantically match the query. These snippets provide the external medical knowledge paired183
with the image. A qualitative example demonstrating the effectiveness of incorporating external184
medical knowledge is shown in Figure 7. With access to COVID-19-related medical knowledge,185
MLLMs can standardize medical terminology and refine diagnoses based on the disease progressions186
outlined in medical literature.187
3.2.2 Generation of Multigranular Text Description188
After data processing, a comprehensive prompt is utilized to guide the MLLMs in generating multi-189
granular descriptions. The prompt template consists of a three-level hierarchical framework with190
questions to instruct MLLMs: (1) a global description that captures all details of the image; (2) a191
local-focused analysis of specific ROIs that potentially are unusual; and (3) a local-global examination192
of the interaction between local and global attributes to understand the impact of local abnormalities193
on the entire organ. Detailed prompt template is presented in supplementary materials.194
To ensure that the MLLMs are guided by relevant medical information not inherently present195
in their training data, we incorporate the processed data (coarse captions, ROIs, and retrieved196
medical knowledge) into the prompts. Specifically, for global information, coarse captions are197
directly integrated into the prompt. For local information, ROIs on images are converted into textual198
descriptions based on their coordinates and area ratio within the images. Examples of these textual199
descriptions are shown in Figure 6, using terms such as “left-center" and “area ratio: 1.2%."200
To refine terminology and diagnosis within ROIs, relevant medical knowledge about specific diseases201
is incorporated into the prompt. Instead of merely inserting this knowledge, we instruct MLLMs to202
identify and align the relevant knowledge to ROIs that require analysis.203
Choice of MLLMs We first prompt GPT-4V with the provided medical coarse captions, ROIs,204
and medical knowledge to generate a subset of 200,000 samples, maintaining a similar modality205
and organ distribution to our full 25 million dataset. The goal of curating this subset is to calibrate206
a medical knowledge-guided MLLM to adhere to the formatting instructions specified for our text.207
7

The MRI image presents a transverse slice of the brain, where 
the cerebral hemispheres are visible, with the region of 
interest located centrally and towards the upper-middle 
portion of the image, occupying a small area ratio. This region 
exhibits an abnormality characterized by altered signal 
intensity, which contrasts with the surrounding brain tissue, 
suggesting the presence of a brain tumor. The abnormal 
area's position, relative to the rest of the brain, could imply an 
effect on or from adjacent structures, potentially influencing 
nearby tissue due to mass effect or being part of a larger 
pathological process within the brain.
The image is a transverse slice of a magnetic resonance imaging (MRI) scan of the 
brain, showing the cerebral hemispheres with the cerebrum visible. A region of interest 
is located at center part of the image horizontally and upper-middle part of image 
vertically, occupying approximately 3.9% of the total area. The region of interest is 
located in the white matter of the left cerebral hemisphere and appears to have an 
abnormal texture and size, which is indicative of a brain tumor. This abnormal area is 
likely to be affecting the surrounding brain tissue due to its mass effect or infiltration, 
which can cause a shift in the position of adjacent structures or potentially disrupt the 
normal brain function in its vicinity. The MRI scan provides a detailed view of the brain's 
anatomy, allowing for the visualization of soft tissue differences that may not be possible 
with other imaging modalities. 
GPT-4V LLaVA-Med++
More detailed
ROI analysis
More detailed
area ratio
Contain image
analysis
More detailed
lesion impact
analysis
Figure 8: Qualitative Comparison with sample generated by GPT-4V Compared to GPT-4V , our
model generate more detailed caption.
PET(61K)
Ultrasound(61K)
Endoscopy(123K)
Fundus(139K)
Dermoscopy(197K)
X-Ray
970KMicroscopy
1.7M
Histopathology
3.9M
MR
8.2M
CT
10M
(a) Modality distribution in MedTrinity-25M.
Thorax
Others
Abdomen
Head&Neck
lung
kidney
liver
breast
bone
vascular
cell
tissue
brain
Pelvis
Thorax
Others
Abdomen
Head&Neck
lung
kidney
liver
breast
bone
vascular
cell
tissue
brain
Pelvis
trachea
mediastinal
pleuraesophagusmediastinum
aorta
(b) Anatomical and biological structures in MedTrinity-
25M.
25M
15M
1.6M
337K
208K
MedTrinit
y-25M
PMC-
15M
PMC-O A
MIMIC-
CXR-JPG
O penPath
N umber of Samples
Dataset
MedTrinity-25M
PMC-15M
PMC-OA
MIMIC-CXR-JPG
OpenPath
(c) Data size comparison.
 (d) Wordcloud of disease statistic.
Figure 9: Statistical overview of MedTrinity-25M.
Subsequently, we employ our model, LLaV A-Med++, which is based on LLA V A-Med [9], the208
state-of-the-art medical MLLM. To further improve this model, we leverage the latest LLaMA3[36]209
to enhance its linguistic capabilities, and incorporate multi-scale feature extraction [37] to improve210
its vision capabilities. LLaV A-Med++ undergoes continuous training on medical multimodal data211
and is fine-tuned using our multigranular annotations, resulting in a specialized medical model.212
After fine-tuning, we then use this specialized model to generate the multigranular text descriptions213
on our entire dataset, resulting in 25 million image-ROI-description triplets. The fine-tuning process214
leverages the advanced language organization capabilities of GPT-4V , providing an effective template215
for fine-grained captions, which our model uses to learn the formatting of fine-grained captions. As a216
result, our model generates more detailed descriptions compared to GPT-4V , as illustrated in Figure 8.217
We also show a detailed quantitative comparison in the supplementary material.218
8

Dataset Modality Lesion
Type
Lesion
BBox/Mask
Color Texture
Description
Region
Relationship
MedMNIST [39] ✗ ✓ ✗ ✗ ✗
DeepLesion [40] ✓ ✗ ✓ ✗ ✗
BraTS 2024 [41] ✓ ✗ ✓ ✗ ✗
MIMIC-CXR [21] ✓ ✓ ✓ ✓ ✗
Quilt-1M [10] ✓ ✓ ✗ ✓ ✓
VQA-RAD [42] ✓ ✓ ✗ ✓ ✗
CRC100K [43] ✓ ✓ ✗ ✗ ✗
SA-Med2D-20M [44] ✓ ✓ ✓ ✗ ✗
MedTrinity-25M(Ours) ✓ ✓ ✓ ✓ ✓
Table 1: Comparison of dataset types based
on provided attributes of annotations.
Figure 10: Comparison of the av-
erage word count of text descrip-
tions.
4 Dataset Analysis219
Diversity Our dataset encompasses a wide range of 10 imaging modalties, with more than 65220
diseases across various anatomical structures in human. The distribution of Anatomical and biological221
structures in MedTrinity-25M is shown in Figure 9b. Meanwhile, the number of samples in the dataset222
for each modality are shown in Figure 9a, spanning from common ones with over 1 million samples223
each (CT, MRI, X-ray) to rare modalities(ultrasound, dermoscopy) with at least more than 100,000224
samples, demonstrating a much more balanced distribution compared to other large-scale dataset like225
SA-Med2D-20M[38], which only contain thousands of ultrasound and dermoscopy samples.226
Scale Figure 9c shows the amount of our dataset, which is significantly larger than previous datasets.227
To the best of our knowledge, this is the largest open-source, multi-modal multigranular medical228
dataset to date.229
Diseases The datasets involved in constructing MedTrinity-25M primarily focus on disease diagno-230
sis and medical discovery. In MedTrinity-25M, diseases are given in the free-form text. The same231
disease may be referred to using different terms, allowing for elaborate identification and analysis.232
Figure 9d illustrates the frequently used words related to diseases in our dataset.233
Richness We provide both quantitative analysis and qualitative examples to show the richness234
of our generated multigranular compare to other medical dataset. Qualitative examples are shown235
in Figure 1, our textual description is multigranular with more attributes than radiology report of236
chest x-rays dataset MIMIC-CXR [21], visual QA dataset SLAKE[22] and radiology objects caption237
dataset ROCO[18]. To demonstrate the multi-granularity of our data, we compared the average word238
count of text descriptions in our dataset, MedTrinity-25M, with those in other medical datasets, as239
illustrated in Figure 10. The word count in our dataset is significantly higher, indicating greater240
richness.241
Alignment with human We leverage GPT-4 to quantify the alignment of generated text descriptions242
compared to clinical reports from pathologist, which is set as the ground-truth. Specifically, we243
utilize GPT-4 to score the helpfulness, relevance, accuracy, and level of details of the our generated244
text descriptions based on clinical reports, and give an overall score on a scale of 1 to 10, where245
a higher score indicates better overall performance. Additionally, GPT-4 is required to provide a246
comprehensive explanation for the evaluation score. Detailed experiment results are presented in247
supplementary materials.248
5 Conclusion249
This paper introduces MedTrinity-25M, a large-scale multimodal medical dataset comprising over250
25 million image-ROI-description triplets sourced from more than 90 online resources, spanning251
10 modalities and covering over 65 diseases. Unlike existing dataset construction methods that rely252
on image-text pairs, we have developed the first automated pipeline to scale up multimodal data by253
generating multigranular visual and textual annotations from unpaired image inputs, leveraging expert254
grounding models, retrieval-augmented generation techniques, and advanced MLLMs. MedTrinity-255
25M’s enriched annotations have the potential to support a wide range of multimodal tasks, such as256
captioning, report generation, classification, and segmentation, as well as facilitate the large-scale257
pre-training of multimodal medical AI models.258
9

Supplementary materials.405
(c) Did you discuss any potential negative societal impacts of your work? [N/A] This406
research is foundational works, do not include potential negative impacts.407
(d) Have you read the ethics review guidelines and ensured that your paper conforms to408
them? [Yes]409
2. If you are including theoretical results...410
(a) Did you state the full set of assumptions of all theoretical results? [N/A] This paper do411
not include theoretical results.412
(b) Did you include complete proofs of all theoretical results? [N/A] This paper do not413
include theoretical results.414
3. If you ran experiments (e.g. for benchmarks)...415
(a) Did you include the code, data, and instructions needed to reproduce the main experi-416
mental results (either in the supplemental material or as a URL)? [Yes] Refer to project417
page in abstract.418
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they419
were chosen)? [Yes]420
(c) Did you report error bars (e.g., with respect to the random seed after running experi-421
ments multiple times)? [No] This paper does not report error bars422
(d) Did you include the total amount of compute and the type of resources used (e.g., type423
of GPUs, internal cluster, or cloud provider)? [Yes] See Supplementary materials.424
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...425
(a) If your work uses existing assets, did you cite the creators? [Yes] We cite all utilized426
assets in reference.427
(b) Did you mention the license of the assets? [Yes]428
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]429
We propose a new dataset, which can be acssess in our project page.430
(d) Did you discuss whether and how consent was obtained from people whose data you’re431
using/curating? [Yes] We follow corresponding licences.432
(e) Did you discuss whether the data you are using/curating contains personally identifiable433
information or offensive content? [N/A] We collect only medical data.434
5. If you used crowdsourcing or conducted research with human subjects...435
13

(a) Did you include the full text of instructions given to participants and screenshots, if436
applicable? [N/A] This paper did not use crowdsourcing.437
(b) Did you describe any potential participant risks, with links to Institutional Review438
Board (IRB) approvals, if applicable? [N/A]439
(c) Did you include the estimated hourly wage paid to participants and the total amount440
spent on participant compensation? [N/A]441
14