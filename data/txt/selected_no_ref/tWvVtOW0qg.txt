Data Measurements for Decentralized Data Markets
Anonymous Author(s)
Affiliation
Address
email
Abstract
Decentralized data markets can provide more equitable forms of data acquisition for1
machine learning. However, to realize practical marketplaces, efficient techniques2
for seller selection need to be developed. We propose and benchmark federated data3
measurements to allow a data buyer to find sellers with relevant and diverse datasets.4
Diversity and relevance measures enable a buyer to make relative comparisons5
between sellers without requiring intermediate brokers and training task-dependent6
models.7
1 Introduction8
Massive training datasets have proved foundational to AI breakthroughs, from earlier deep learning9
breakthroughs in computer vision to large language models (LLM) [65, 35]. However, AI companies10
face increasing scrutiny and backlash for their data collection practices, resulting in lawsuits from data11
owners such as artists, software developers, and journalists [24, 61, 60]. As AI applications continue12
to be developed and deployed, more equitable and transparent means of data acquisition must be13
designed and implemented [53, 16]. Recently, data markets have been proposed to incentivize greater14
data sharing and access for data-restricted domains [9, 2]. As the ethical challenges and legal risks of15
acquiring data increase, data market platforms will be crucial to address the ethical and economic16
challenges in training AI models.17
To facilitate practical data market platforms, we investigate the challenge ofseller selection for a data18
buyer using a framework based on federated data measurements. We benchmark several proposed19
heuristic measures of diversity and relevance, which can be used by the buyer to compare the relative20
value of different sellers. The advantage of this federated data measurement framework is that it does21
not require direct access to the seller’s data, is training-free, and is task-agnostic. These attributes are22
desirable for a decentralized marketplace to enable scalable seller selection for many different buyers.23
The three main steps of the data measurement framework are depicted in Figure 1. We evaluate24
several definitions of diversity and relevance on multiple computer vision datasets by benchmarkiing25
each data measurement for its ability to rank sellers, correlation with classification performance, and26
robustness to duplicate and noisy data. In summary, we show that federated data measurements allow27
private and lightweight seller discovery that can lower search costs for a data buyer in a decentralized28
data marketplace.29
2 Decentralized Data Markets30
Current data brokers are highly centralized and aggregate vast amounts of data, often without a user’s31
knowledge, consent, or compensation [57, 13]. This massive centralization of data has led to increased32
Submitted to the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets
and Benchmarks. Do not distribute.

Figure 1: Steps of data measurements framework. A buyer embeds their data through some
embedding model and sends a private query of matrix projections to each seller. Each seller responds
with data measurements that allow the buyer to compare and transact with sellers that have the most
relevant data.
data breaches, the erosion of privacy, and harmful data misuse. For example, the 2017 Equifax data33
breach exposed the private records of more than 150 million people [74]. In contrast, decentralized34
data markets may present a more equitable and efficient approach to data acquisition [53, 55, 36].35
On a decentralized marketplace platform, buyers can transact directly with sellers, bypassing inter-36
mediate data brokers by utilizing decentralized and privacy-enhancing technologies such as smart37
contracts and trusted execution environments [28, 6]. Bypassing data brokers may result in lower38
transaction costs and greater market efficiency by allowing data owners to capture more of the39
revenue generated from their data. In addition, whereas data brokers indiscriminately acquire data40
and sell bundled datapoints wholesale, data marketplaces could take a more targeted approach to41
data acquisition. by only paying for the most valuable datapoints, lowering the overall privacy42
incursion [51]. Lastly, compensating data owners may incentivize greater data access from a more43
diverse range of individual data producers, which may decrease bias in data acquisition by increasing44
participation from smaller, more heterogeneous data sources.45
However, to fully realize this paradigm shift to decentralized data marketplaces, scalable methods46
are needed to match buyers with relevant data sellers. A survey of data market participants found47
that finding relevant sellers was a major source of friction and recommended lowering search costs48
for the data buyer [36]. In a centralized one-sided marketplace, this process can be facilitated by a49
data broker. However, in the absence of brokers in a decentralized marketplace, we need federated50
techniques to signal the value of data sellers to different buyers, each of whom may have different51
preferences and goals for data acquisition. This problem of seller selection is related to client selection52
in federated learning [22]. Without new federated methods to lower search costs, market platforms53
will struggle to attract enough participants to attain the scale and network effects for a sustainable54
marketplace.55
Most current work in data valuation, such as Data Shapley [23], assumes a centralized setting where56
all data is fully accessible to train models to estimate data value. In a decentralized setting, a seller57
would not permit data access before payment since data is easily copied. However, a buyer would be58
reluctant to pay a fair price for data if they cannot be assured of its value. Therefore, a fundamental59
asymmetry arises between the buyer and seller, related to Arrow’s Information Paradox [5], resulting60
in increased search costs and fewer transactions taking place. New methods must be developed for61
the decentralized data market setting taking into account only limited, “white-box” data access [10].62
To allow a buyer to search for the most promising sellers in a decentralized marketplace, we evaluate63
federated data measurements, which have the advantage of being computationally cheap to compute,64
task-agnostic, and only require indirect data access. Many different data measurements have been65
developed to quantify intrinsic, task-agnostic characteristics [48, 40, 43]. Data measurements can66
be general-purpose, such as central tendency (e.g., mean, median) and “distance” (e.g., Euclidean67
distance, KL divergence) or modality-specific, such as Fréchet Inception Distance [ 3] and lexical68
diversity [31]. Recent work proposed to use conditional diversity and relevance measurements to69
value data without requiring model training or validation data evaluation [4]. We incorporate their70
work by evaluating several other definitions of diversity and relevance in the context of private and71
federated data valuation on medical imaging datasets.72
2

3 Federated Data Measurements73
Instead of directly attempting to measure the contribution of each datapoint in the seller’s dataset, we74
measure inherent properties of the seller’s aggregate dataset through data measurements. These data75
measurements, µ, can be used by the buyer to compare between data sellers. For instance, a seller j76
with measurement µj ≫ µi would be deemed to have more valuable data than seller i.77
Many data measurements have been developed to quantify intrinsic, task-agnostic characteristics [48,78
40, 43]. Data measurements can be general-purpose, such as central tendency and distance metrics, or79
modality-specific, such as Fréchet Inception Distance [3] and lexical diversity [31]. Many data quality80
measures have been developed for structured relational data, such as completeness, consistency, and81
accuracy; however, data quality becomes more complicated for unstructured data [8].82
Before measuring the seller’s data, a buyer sends a personalized query, Q, to each seller. We assume83
that a buyer has a small sample of reference data, Xbuyer
i ∼ Dbuyer, from the desired distribution84
to create the query. The buyer communicates this query to the seller, and the seller uses this query85
to transform their data, calculate the data measurements, and return the measurements to the buyer.86
The query can be any matrix projection to measure the seller’s data. For instance, this basis can be87
chosen to maximize variance (PCA), independence (ICA), or class separability (LDA) [46, 29, 7].88
Empirically, we found PCA with 10 principal directions appropriate for most datasets as most of the89
variance is captured in the first few components (see Figure 11).90
Another common preprocessing step is to embed data into a low-dimensional representation using91
a deep learning model [ 47, 42, 69]. The choice of embedding, f : X →Rd, can incorporate92
domain-specific knowledge and has become popular for retrieval augmented generation (RAG) and93
vector databases [44, 52]. For our benchmark, we use a pretrained CLIP (ViT-16) model — due to its94
good performance for zero-shot capabilities across a wide range of image domains — to precompute95
512-dimensional embedding vectors for each dataset [54]. We envision that more application-specific96
platforms could use multiple choices of embeddings, such as medical foundation models [49].97
First, buyer i sends seller j their query, Q = πk
 
f
 
Xbuyer
, where πk : Rn×d → Rk×d computes98
the first k principal directions using the buyer’s reference data. Then, the seller uses this query99
to transform their data and returns certain information to the buyer to calculate a specified data100
measurement. The measurement function, g : Rk×d × Rk×d → R, takes in the projected data from101
the seller and buyer to produce a scalar data measurement µij ∈ R, µij = g
 
QCseller, QCbuyer
,102
where C ≜ f(X)⊤f(X) is the covariance matrix of the embedded data.103
In prior work, g has been defined as measuring heuristic notions of relevance and diversity [4, 70, 21].104
For our benchmark, we evaluate the four different definitions of relevance and four definitions of105
diversity for our decentralized data market setting. Intuitively, relevance should capture the similarity106
between the buyer and seller. For example, if the buyer has chest X-ray (CXR) images with COVID-107
19, then a seller with similar COVID-19 CXR images would be more relevant than CXR from normal108
patients. Likewise, CXR data should be more relevant than MRI data or photography images. We109
evaluate four definitions of relevance for seller selection.110
1. Negative Euclidean (L2) distance between the mean vectors of the buyer and seller:111
−


¯Xbuyer − ¯Xseller

2, where ¯X ≜ 1
k
Pk
i=1 QiCi.112
2. Cosine similarity between mean vectors: (¯Xbuyer · ¯Xseller)/∥¯Xbuyer∥2 ∥¯Xseller∥2.113
3. Correlation between mean vectors: Cov(¯Xbuyer, ¯Xseller)/
q
Var(¯Xbuyer) · Var(¯Xseller).114
4. Overlap between principal components [ 4]:
k
qQk
i=1 min(λbuyer
i ,λseller
i )/max(λbuyer
i ,λseller
i ),115
where λi ≜ ∥QiCi∥2 is the magnitude of the projected vector.116
For many machine learning applications, using only relevance measures is insufficient to guarantee117
useful training data. For example, a seller’s data may be highly relevant but have duplicate data118
or imbalanced classes that result in brittle, low-performing models. Intuitively, a seller with X-ray119
images from 1,000 unique patients contains more non-redundant information than 1,000 X-rays from120
3

a single patient. Then, training on the more diverse seller should lead to better model generalization121
on unseen test data as more of the input space has been learned [70, 20]. We evaluate four definitions122
of diversity.123
1. Volume of the projected covariance [70]: log
 
det
 
QCseller
124
2. Vendi score [21], defined as the exponential of negative entropy of eigenvalues of the125
covariance: exp
 
−trace
 
QCseller log QCseller
.126
3. Dispersion of the features, measured as the geometric mean of standard deviations [ 40]:127
k
rQk
i=1 diag (QCsellerQ⊤)i

128
4. Difference in the normalized magnitude between principal components [ 4]:129
k
qQk
i=1 |λbuyer
i −λseller
i |/max(λbuyer
i ,λseller
i ), where λi ≜ ∥QiCi∥2.130
These data measurements of diversity and relevance are computationally efficient to compute, even131
for large datasets (>100,000 datapoints), and only require indirect data access from each seller.132
Additionally, leveraging deep embeddings allows high-dimensional, multi-modal data such as images133
and text to be measured in a task-agnostic and training-free manner.134
4 Experiments135
Ranking Sellers with Measurements We first evaluate each data measurement in correctly ranking136
the seller with data IID with the buyer’s distribution. For example, when the buyer has reference137
data from ImageNet, the seller with ImageNet data should have the largest data measurement (see138
Figure 8). A common metric to evaluate ranking quality in information retrieval is discounted139
cumulative gain (DCG) [30]. For simplicity, we assume that the IID seller has a maximum gain of 1140
and non-IID sellers have a gain of 0. In Table 1, we report the mean rank of the IID seller and DCG141
over 10 random trials using 20 computer vision datasets (listed in Appendix A). For all experiments,142
we use 100 datapoints for the buyer query and 10,000 datapoints for each seller unless otherwise143
specified.144
Overall, we find that relevance measurements, such as L2 distance and the “overlap” measure, are145
better than diversity measurements at ranking the IID seller. This reflects the intuition that relevance146
directly compares distributional information between buyer and seller. On the other hand, most147
diversity measures only consider information from the buyer through the query projection step.148
Among all data measurements, the “difference” measure had the lowest DCG, often ranking the IID149
seller very low (see Figure 8 for an example).150
Table 1: Performance of data measurements for seller ranking
DATA MEASUREMENT AVG. RANKING ↓ AVG. DCG ↑
RELEVANCE
L2 1.25 ± 0.85 0 .94 ± 0.15
COSINE 1.28 ± 0.99 0 .94 ± 0.16
CORRELATION 1.34 ± 1.16 0 .93 ± 0.17
OVERLAP [4] 1.18 ± 0.53 0 .95 ± 0.14
DIVERSITY
VOLUME [70] 3.64 ± 5.28 0 .79 ± 0.30
VENDI [21] 3.38 ± 2.87 0 .69 ± 0.31
DISPERSION [40] 2.73 ± 2.87 0 .80 ± 0.29
DIFFERENCE [4] 19.47 ± 1.04 0 .23 ± 0.0
Correlation with Downstream Classifier Performance Next, we evaluate how useful each data151
measurement is as a proxy for training data quality. In this experiment, we assume that the buyer152
wants to use the seller’s data to train a model to predict a held-out test set, which is IID with the153
buyer’s query data. We train a model for each seller using their data as a training set and correlate the154
4

Table 2: Correlation test performance across three tasks on four MedMNIST datasets
PREDICTION TASK VALUATION METHOD CORRELATION WITH TEST ACCURACY ↑
BLOOD ORGAN PATH TISSUE AVG.
BINARY
CLASSIFICATION
L2 -0.02 0.04 0.03 0.10 0.04
COSINE 0.16 0.09 0.13 0.20 0.15
CORRELATION 0.13 0.07 0.13 0.21 0.14
OVERLAP 0.04 -0.02 0.01 0.06 0.02
VOLUME 0.28 0.29 0.31 0.28 0.29
VENDI 0.19 0.19 0.22 0.18 0.20
DISPERSION 0.17 0.18 0.18 0.14 0.17
DIFFERENCE -0.03 0.02 0.03 -0.09 -0.02
KNN S HAPLEY 0.10 0.07 0.05 0.08 0.08
LAVA -0.02 -0.02 0.02 0.01 0.00
MULTICLASS
CLASSIFICATION
L2 0.22 0.15 0.19 0.22 0.20
COSINE 0.23 0.14 0.12 0.18 0.17
CORRELATION 0.24 0.15 0.12 0.19 0.18
OVERLAP 0.27 0.19 0.19 0.24 0.22
VOLUME 0.42 0.35 0.32 0.36 0.36
VENDI 0.30 0.23 0.19 0.22 0.24
DISPERSION 0.22 0.20 0.12 0.18 0.18
DIFFERENCE -0.23 -0.14 -0.14 -0.18 -0.17
KNN S HAPLEY 0.09 0.12 0.07 0.12 0.10
LAVA -0.01 0.00 -0.02 0.00 -0.01
K-M EANS
CLUSTERING
L2 0.22 0.23 0.20 0.19 0.21
COSINE 0.29 0.28 0.31 0.26 0.29
CORRELATION 0.29 0.29 0.31 0.26 0.29
OVERLAP 0.31 0.35 0.36 0.32 0.34
VOLUME 0.55 0.54 0.52 0.55 0.54
VENDI 0.45 0.45 0.49 0.48 0.47
DISPERSION 0.35 0.38 0.32 0.36 0.25
DIFFERENCE -0.22 -0.27 -0.29 -0.25 -0.26
KNN S HAPLEY 0.01 0.05 0.02 -0.01 0.02
LAVA 0.01 0.00 -0.03 0.02 0.00
resulting model’s test performance with the data measurements for that seller. In this way, a seller155
with a high data measurement value should ideally have test performance for a particular buyer than156
a seller with a lower data measurement value.157
We use four medical imaging datasets (BloodMNIST, OrganMNIST, PathMNIST, and TissueMNIST)158
from the MedMNIST benchmark (see Figure 6 for example images) [71]. To introduce heterogeneity159
between sellers, we sample classes from a Dirichlet distribution as standard practice in federated160
learning to simulate non-IID clients [73, 45]. For each dataset, we evaluate three different prediction161
task scenarios: binary classification with logistic regression, multiclass classification with a random162
forest classifier, and K-means clustering. For each data buyer, we randomly sample a subset of163
classes for multiclass classification and evaluate the accuracy score as the performance metric. For164
binary classification, we consider the selected subset of classes as “positive” and the other classes165
as “negative” and evaluate accuracy. For clustering, we set the number of clusters equal to the total166
number of classes for each dataset and evaluate homogeneity score, a common clustering metric, as167
the performance metric [58].168
For another baseline, we also evaluate two centralized data valuation, KNN Shapley [ 32] and169
LA V A [34], using the OpenDataVal framework [33]. We selected these two valuation methods for170
5

their efficient runtime. We split the seller’s data into 20% for training and used the other 80% as a171
validation set. To aggregate a value for each seller, we take the average data value of the validation172
datapoints. In Table 2, we report these correlations between data measurement and test accuracy for173
500 sellers, each with 5,000 datapoints, and average correlations over 10 buyers for each dataset.174
Intuitively, we expect that sellers with more similar data as the buyer will learn higher-performing175
classifiers and be associated with larger data measurement values. For several of the diversity measures176
(volume, Vendi score), we find a moderate-strong correlation to test performance across datasets and177
prediction tasks. See Figure 9 for an example of strong correlations between volume measurements178
and test prediction accuracy. Compared to diversity measures, relevance measures and the centralized179
data valuation methods (KNN Shapley, LA V A) had a weak correlation with downstream classification180
performance. These results support that a seller with higher diversity measurements is more likely to181
have training data that is more useful for a particular, even without specifying the exact prediction task182
or model architecture. Similar observations between generalization performance and data diversity183
are reported in determinantal point processes [70, 38].184
Detecting Seller Misreporting with Multiple Queries One practical challenge that arises with185
a decentralized marketplace is ensuring that the seller is not able to “cheat” by artificially inflating186
the value of their data measurements. In the case of relevance measures, a malicious seller would187
aim to report mean vectors similar to those of the buyer, but a buyer could avoid sending their own188
mean vectors to prevent this. However, this strategy would not work for diversity measures, which189
are independent of the buyer’s data given the query.190
To counteract this, a buyer could send multiple queries containing “false” directions that may be191
computed using non-relevant data or even random directions in addition to their actual data (see192
Figure 10. Then, the buyer could discount sellers with large data measurements in these false193
directions while only considering sellers with high value using the real query. We evaluate each data194
measurement’s ability to discriminate between data measurements using the real query and false195
queries with the following ratio196
ratio(%) = µreal
quantile({µ(i)
false}m
i , %)
, (1)
which is simply the ratio of the data measurement using the real query µreal over the %-quantile of197
measurement using false queries. In our experiment, we compute false queries using 20 non-IID198
datasets and consider three quantile threshold ratios:50%, 75%, and 90%. The 50% ratio corresponds199
to the real IID measurement divided by the median measurement when using buyer queries from the200
19 other non-IID datasets.
Table 3: Ratio of measurement using real query over measurements of false queries
DATA MEASUREMENT RATIOS ↑
50% 75% 90%
RELEVANCE
L2 1.02× 0.93× 0.89×
COSINE 2.97× 1.57× 1.25×
CORRELATION 2.83× 1.53× 1.18×
OVERLAP 2.88× 2.02× 1.64×
DIVERSITY
VOLUME 1.39× 1.31× 1.24×
VENDI SCORE 2.20× 1.92× 1.64×
DISPERSION 1.91× 1.73× 1.58×
DIFFERENCE 0.38× 0.30× 0.27×
201
In Table 3, we report measurement ratios and find that most data measurements of relevance and202
diversity have high ratios, implying that sending multiple queries can be an effective strategy to deal203
with adversarial sellers that misreport their measurements. This will incentivize the sellers to honestly204
report their true data measurements as they do not know which queries are real or fake. Sending205
6

additional queries increases communication overhead, but this may be tolerable since each query is206
cheap — being only a k × d matrix, where k ≪ n. For instance, each of our queries is 10 × 512 in207
our experiments.208
Robustness to Duplicate Data Because there is no cost to copying data, an adversarial seller may209
duplicate portions of their data to try to obtain higher measurement values. In Figure 2, we vary210
the amount of duplicate data to observe the effect on each data measurement when both the seller211
and buyer have IID data. We note that the implementation of the considered volume method [ 70]212
explicitly quantizes the data into a d-dimensional hypercube to achieve robustness to duplicate213
data. Therefore, increasing the amount of duplicated data has a negative effect on volume. For all214
other data measurements, the value is relatively consistent until falling off for extreme numbers215
of duplicates, e.g., each datapoint is duplicated 200 times, leaving only 10,000/200 = 50 unique216
datapoints. Exploring duplicate-robust versions of data measurements would be interesting for future217
work.218
0 5 10 25 50 100 200
Number of duplicate 
0.5
0.6
0.7
0.8
0.9
1.0
1.1
Correlation
0 5 10 25 50 100 200
Number of duplicate 
0.60
0.65
0.70
0.75
0.80
0.85
0.90
Overlap
0 5 10 25 50 100 200
Number of duplicate 
−0.012
−0.010
−0.008
−0.006
−0.004
−0.002
0.000
L2
0 5 10 25 50 100 200
Number of duplicate 
0.5
0.6
0.7
0.8
0.9
1.0
1.1
Co ine
0 5 10 25 50 100 200
Number of duplicate 
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Difference
0 5 10 25 50 100 200
Number of duplicate 
20
40
60
80
100
120
V olume
0 5 10 25 50 100 200
Number of duplicate 
1
2
3
4
5
6
V endi
0 5 10 25 50 100 200
Number of duplicate 
0.2
0.4
0.6
0.8
1.0
1.2
Di per ion
Figure 2: Effect of duplicate data on data measurements. Each seller has 10,000 total datapoints,
and a subset of datapoints are duplicated, keeping the total number of datapoints the same. Each
colored dotted line represents an individual dataset, and the solid black line represents the average of
all datasets. Errors bars represent one standard deviation.
0 1 2 3 4
Noise severi y
0.75
0.80
0.85
0.90
0.95
1.00
Correla ion
0 1 2 3 4
Noise severi y
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Overlap
0 1 2 3 4
Noise severi y
−0.012
−0.010
−0.008
−0.006
−0.004
L2
0 1 2 3 4
Noise severi y
0.75
0.80
0.85
0.90
0.95
1.00
Cosine
0 1 2 3 4
Noise severi y
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Difference
0 1 2 3 4
Noise severi y
92.5
95.0
97.5
100.0
102.5
105.0
107.5
110.0
V olume
0 1 2 3 4
Noise severi y
1.0
1.5
2.0
2.5
3.0
3.5
4.0
V endi
0 1 2 3 4
Noise severi y
0.5
0.6
0.7
0.8
0.9
1.0
1.1
1.2
Dispersion
Figure 3: Effect of different types of noise corruptions on each data measurement. See Figure 7 for
example images on the ImageNet-C dataset.
Effect of Noisy and Corrupted Data In this experiment, we utilize the ImageNet-C benchmark219
dataset [26] to study the effect of 19 different types of noise corruptions (blurring, intensity changes,220
7

10
1
10
2
10
3
10
4
Amount  of seller da a
0.2
0.4
0.6
0.8
1.0
1.2
Correla ion
10
1
10
2
10
3
10
4
Amoun  of seller da a
0.5
0.6
0.7
0.8
0.9
1.0
Overlap
10
1
10
2
10
3
10
4
Amoun  of seller da a
−0.025
−0.020
−0.015
−0.010
−0.005
0.000
0.005
L2
10
1
10
2
10
3
10
4
Amoun  of seller da a
0.2
0.4
0.6
0.8
1.0
1.2
Cosine
10
1
10
2
10
3
10
4
Amoun  of seller da a
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Difference
10
1
10
2
10
3
10
4
Amoun  of seller da a
−25
0
25
50
75
100
125
150
V olume
10
1
10
2
10
3
10
4
Amoun  of seller da a
1
2
3
4
5
6
V endi
10
1
10
2
10
3
10
4
Amoun  of seller da a
0.2
0.4
0.6
0.8
1.0
1.2
Dispersion
Figure 4: Varying the amount of data each IID seller has while fixing the buyer query to 100
datapoints.
10
1
10
2
10
3
10
4
Amount of buyer data
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1.1
Correlation
10
1
10
2
10
3
10
4
Amount of buyer data
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Overlap
10
1
10
2
10
3
10
4
Amount of buyer data
−0.04
−0.03
−0.02
−0.01
0.00
0.01
L2
10
1
10
2
10
3
10
4
Amount of buyer data
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1.1
Co ine
10
1
10
2
10
3
10
4
Amount of buyer data
−0.2
0.0
0.2
0.4
0.6
0.8
1.0
Difference
10
1
10
2
10
3
10
4
Amount of buyer data
65
70
75
80
85
90
95
100
105
V olume
10
1
10
2
10
3
10
4
Amount of buyer data
1
2
3
4
5
6
V endi
10
1
10
2
10
3
10
4
Amount of buyer data
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Di per ion
Figure 5: Varying the amount of data in the buyer query has while fixing each seller to 5,000
datapoints.
compression, style effects, etc.) applied to the original ImageNet dataset [59]. Each corruption and221
noise type has 5 levels of increasing severity. See Figure 7 for an example images. The buyer has222
100 datapoints from the original ImageNet dataset, while each seller has 10,000 datapoints from one223
ImageNet-C corruption type.224
As shown in Figure 3, as the severity of the noise/corruption increases, the values of all data225
measurements decrease (with the exception of the “difference” measurement, which increases). This226
degradation in diversity and relevance also depends on the type of noise corruption. More subtle227
changes, such as brightness shifts and saturation, which do not change the spatial information in228
the image and result in more gradual decreases in measured values. In contrast, heavy corruptions,229
such as Gaussian noise and glass blur, which affect the image’s semantic structure, have much larger230
effects on measured diversity and relevance.231
Varying the Amount of Seller and Buyer Data For these experiments, we use the 20 datasets in232
Appendix A. In Figure 4, we vary the amount of data each seller has from 10 datapoints to 50,000233
datapoints while keeping the buyer’s query fixed at 100 datapoints. We find all data measurements,234
except volume, stabilized after around 1,000 seller datapoints. The volume value continued to235
increase with the number of seller datapoints. We also vary the amount of in the buyer’s query236
from 10 datapoints to 10,000 datapoints while fixing the number of seller datapoints to 5,000 in237
8

Figure 5. We find that data measurements were relatively stable for most datasets after around 100238
query datapoints.239
5 Discussion240
As observed in the experiments, both diversity and relevance measures capture important aspects of241
data value for a buyer. Relevance measures allow a buyer to filter out irrelevant data and identify242
sellers with in-domain data distributions. On the other hand, diversity measures, such as volume,243
reveal which sellers have the most informative and useful data (correlated with test performance,244
non-duplicated data). As shown with the corruption experiments using ImageNet-C, both diversity245
and relevance are associated with data quality as noisier and more corrupted data have lower data246
measurements.247
In contrast with prior work [4], we find their “difference” definition of diversity to underperform in248
most experiments compared to other definitions of diversity. Subjectively, we observe that “difference”249
measurements tend to be the inverse of “overlap” measurements and thus redundant in terms of250
information. On the other hand, volume has additional nice properties, such as being robust to data251
duplication and increasing with the number of seller datapoints. Based on our benchmark experiments,252
we conclude that cosine similarity and “overlap” are appropriate relevance measures and that the253
volume-based definition of diversity is well-suited for seller selection.254
Advantages of Federated Data Measurements Unlike centralized and training-based approaches255
to data valuation, using federated data measurements is a lightweight and private way to match a256
buyer with relevant sellers in a decentralized marketplace with millions of participants. Measuring a257
seller’s data is agnostic to the modeling task and model architecture. This approach allows a buyer to258
compare the value of multiple sellers relatively without requiring direct access to the seller’s data,259
which would not be allowed before payment. Different choices of embedding functions could be260
precomputed to serve different types of modalities and domains. In summary, this decentralized data261
valuation scheme allows private and scalable seller discovery to lower search costs for a data buyer,262
enabling more efficient markets and lower transaction costs.263
Limitations While our work presents an initial benchmark of different data measurements, it is264
limited in several ways. Firstly, while our data measurements framework can accommodate other265
types of data modalities such as text and tabular data, we only consider common computer vision266
datasets for our benchmark. Future work would extend the experiments and embeddings for other267
domains such as natural language and graphical data. Another limitation is the lack of formal268
privacy guarantees. While the federated nature of the query and measurement step should prevent269
reconstruction attacks, techniques such as differential privacy [18] and homomorphic encryption [1]270
could be employed to provide explicit guarantees. Additionally, further work could incorporate271
incentive mechanisms to study adversarial seller behavior.272
6 Conclusion273
Reimagining a new decentralized model of data acquisition where individual data producers are fairly274
compensated for sharing data could help redistribute the economic benefits from AI technology to275
those whose data enables AI research and development [64]. Decentralized data markets may address276
issues with current centralized settings by providing a more equitable and efficient exchange of data277
resources, as well as enabling more collective data governance [53, 17].278
In this paper, we presented federated data measurements for decentralized data marketplaces. These279
measurements allow a buyer to perform seller selection without direct access to the seller’s data and280
are more scalable than current data valuation approaches. We benchmark several properties of data281
measurements on computer vision datasets and find that a combination of relevance and diversity282
performs well for several practical data marketplace considerations.283
9

Appendix D.456
(d) Have you read the ethics review guidelines and ensured that your paper conforms to457
them? [Yes]458
2. If you are including theoretical results...459
(a) Did you state the full set of assumptions of all theoretical results? [N/A] We do not460
present theoretical results.461
(b) Did you include complete proofs of all theoretical results? [N/A]462
3. If you ran experiments (e.g. for benchmarks)...463
(a) Did you include the code, data, and instructions needed to reproduce the main exper-464
imental results (either in the supplemental material or as a URL)? [Yes] We include465
code for our experiments in the supplemental materials.466
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they467
were chosen)? [Yes] We specify additional experimental details in the Appendix.468
13

(c) Did you report error bars (e.g., with respect to the random seed after running exper-469
iments multiple times)? [Yes] We report error bars of 1 standard deviation over 10470
random trials for all results.471
(d) Did you include the total amount of compute and the type of resources used (e.g., type472
of GPUs, internal cluster, or cloud provider)? [Yes] We include hardware details in the473
Appendix B.474
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...475
(a) If your work uses existing assets, did you cite the creators? [Yes] We cite all the476
datasets used in the Appendix A.477
(b) Did you mention the license of the assets? [Yes] The license will be included in the478
code repository upon release.479
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]480
We include a sample of code as supplemental materials.481
(d) Did you discuss whether and how consent was obtained from people whose data you’re482
using/curating? [N/A]483
(e) Did you discuss whether the data you are using/curating contains personally identifiable484
information or offensive content? [N/A]485
5. If you used crowdsourcing or conducted research with human subjects...486
(a) Did you include the full text of instructions given to participants and screenshots, if487
applicable? [N/A]488
(b) Did you describe any potential participant risks, with links to Institutional Review489
Board (IRB) approvals, if applicable? [N/A]490
(c) Did you include the estimated hourly wage paid to participants and the total amount491
spent on participant compensation? [N/A]492
14

A Datasets493
We use the following computer vision datasets in our experiments:494
• MNIST Handwritten Digits [41]495
• Fashion-MNIST [68]496
• EMNIST [12]497
• SVHN [50]498
• CIFAR10 [37]499
• STL-10 [11]500
• ImageNet (validation set) [59]501
• ImageNet-Sketch [66]502
• ImageNet-Rendition [25]503
• ImageNet-Adversarial [27]504
• ImageNet-V2 [56]505
• ImageNet-Corruption [26]506
• BloodMNIST (224 by 224 pixel version) from MedMNIST-V2 Benchmark [72]507
• BreastMNIST (224 by 224 pixel version) from MedMNIST-V2 Benchmark [72]508
• ChestMNIST (224 by 224 pixel version) from MedMNIST-V2 Benchmark [72]509
• DermaMNIST (224 by 224 pixel version) from MedMNIST-V2 Benchmark [72]510
• OrganAMNIST (224 by 224 pixel version) from MedMNIST-V2 Benchmark [72]511
• PathMNIST (224 by 224 pixel version) from MedMNIST-V2 Benchmark [72]512
• PneumoniaMNIST (224 by 224 pixel version) from MedMNIST-V2 Benchmark [72]513
• RetinaMNIST (224 by 224 pixel version) from MedMNIST-V2 Benchmark [72]514
• TissueMNIST (224 by 224 pixel version) from MedMNIST-V2 Benchmark [72]515
B Experimental Setup516
Each experiment is averaged over 10 trials of randomly splitting buyer and seller data. For the binary517
classification task, a random subset of classes was selected for each buyer to be the positive class,518
while the rest of the classes were labeled negative. For the multiclass classification, a random subset519
of classes was selected for each buyer, while for the clustering task, all classes were used. Logistic520
regression was used for the binary task, a random forest model for the multiclass classification, and a521
K-means model was used for clustering with the number of clusters being initialized to the number522
of total classes. 100 datapoints were used for the buyer query, and 500 datapoints were used for a test523
set. For each seller, 5,000 datapoints were randomly sampled from a Dirichlet class distribution and524
used to train a model to predict the held-out test set. The centralized data valuation baselines (KNN525
Shapley and LA V A) used 1.000 samples from the seller for training and the rest of the 4000 samples526
for validation, and the average data value was reported for the seller. The test performance metric527
was prediction accuracy for binary and multiclass classification, while the homogeneity score was528
used for the clustering task. In general, the diversity measure is the most correlated with prediction529
performance across datasets and tasks.530
For hardware details, we use an Intel Xeon E5-2620 CPU with 32 cores equipped with Nvidia GTX531
1080 Ti GPUs. For baseline implementation of centralized KNN Shapley and LA V A data valuation532
methods, we use the OpenDataVal package [ 33] version 1.2.1 with the default hyperparameter533
settings.534
15

Figure 6: Example images from datasets in the MedMNIST benchmark. See medmnist.com for
more information.
C Additional Figures535
16

Figure 7: Example noise and image corruptions at the highest severity from the ImageNet-C dataset.
17

0.0 0.2 0.4 0.6 0.8 1.0
retina
breast
path
svhn
tissue
fashion
organ
mnist
imagenet-s
emnist
pneumonia
blood
stl10
derma
imagenet-r
imagenet-a
cifar
chest
imagenet-v2
imagenet-val
Correlation
0.0 0.2 0.4 0.6 0.8 1.0
chest
mnist
emnist
retina
pneumonia
blood
derma
breast
svhn
fashion
tissue
path
organ
cifar
imagenet-s
imagenet-a
stl10
imagenet-r
imagenet-val
imagenet-v2
Overlap
−0.025 −0.020 −0.015 −0.010 −0.005 0.000
retina
chest
breast
pneumonia
mnist
emnist
blood
svhn
fashion
derma
path
tissue
organ
imagenet -s
imagenet-r
stl10
imagenet-a
cifar
imagenet-v2
imagenet-val
L2
0.0 0.2 0.4 0.6 0.8 1.0
retina
breast
path
svhn
organ
tissue
fashion
mnist
imagenet-s
emnist
pneumonia
stl10
blood
derma
imagenet-r
imagenet-a
chest
cifar
imagenet-v2
imagenet-val
Cosine
0.0 0.2 0.4 0.6 0.8 1.0
stl10
imagenet-v2
imagenet-r
imagenet-val
imagenet-a
imagenet-s
organ
path
cifar
tissue
svhn
breast
fashion
derma
blood
mnist
pneumonia
emnist
retina
chest
Difference
0 20 40 60 80 100
chest
mnist
retina
emnist
breast
pneumonia
blood
tissue
svhn
path
fashion
derma
organ
cifar
stl10
imagenet-s
imagenet-r
imagenet-a
imagenet-v2
imagenet-val
V olume
0 1 2 3 4 5
mnist
emnist
retina
chest
pneumonia
blood
svhn
tissue
fashion
derma
breast
path
organ
cifar
imagenet-r
imagenet-s
imagenet-a
stl10
imagenet-v2
imagenet-val
V endi
0.0 0.2 0.4 0.6 0.8 1.0 1.2
chest
mnist
retina
emnist
pneumonia
blood
breast
svhn
tissue
derma
fashion
path
organ
cifar
imagenet-s
imagenet-r
stl10
imagenet-a
imagenet-v2
imagenet-val
Dispersion
Figure 8: Ranked data measurements of each seller when the buyer query consists of 100 samples
from ImageNet. The orange bar denotes the seller with IID data distribution (ImageNet) that should
be ranked first.
18

Figure 9: Correlation between volume data measurements and test prediction accuracy on MedMNIST
datasets.
0.0 0.2 0.4 0.6 0.8 1.0
Relevance
30
40
50
60
70
80
Diversity
Real directions
0.0 0.2 0.4 0.6 0.8 1.0
Relevance
F alse directions
0.0 0.2 0.4 0.6 0.8 1.0
Relevance
Random directions
IID seller seller 1 seller 2 seller 3 seller 4 seller 5
Figure 10: Comparing diversity and relevance measurements when the buyer sends a real query
computed on their actual data (left), a false query computed on a random dataset (middle), and a false
query computed using random data (right).
19

0 5 10 15 20 25 30
Number of principal components
0.0
0.2
0.4
0.6
0.8
1.0
Relevance
0 5 10 15 20 25 30
Number of principal components
0
50
100
150
200
250
Diversity
MNIST
QMNIST
EMNIST
KMNIST
F ashion
MNIST -M
SVHN
CIF AR10
Noise
Figure 11: varying the number of principal components used to calculate diversity and relevance.
10,00 samples from the buyer and 10,000 samples from the seller were randomly sampled.
20

D Broader Impact536
We believe that AI developers must reconcile important ethical questions regarding data acquisition in537
current AI development. Class-action lawsuits have been filed against several AI companies for their538
data collection practices, raising questions about data compensation and consent from data owners.539
Current data acquisition norms may actively discourage further data sharing, which can hamper the540
progress and impact of AI, especially in data-limited domains such as healthcare.541
Current centralized data brokers acquire data and operate in nontransparent and obfuscatory ways542
— data is resold between interlinked brokers that make data provenance and traceability of the543
source difficult [ 67, 14]. Individuals are often left without recourse or due process over what544
data is collected or how that data is used [ 15]. Outdated, incorrect, or out-of-context data may545
cause harm to the individual. For instance, millions of mugshots of arrested — but not necessarily546
convicted — individuals are routinely sold on commercial websites and impact those individuals’547
future employment opportunities and access to housing [39]. Data brokers may also pose risks to civil548
liberties, such as when individuals’ data on race, ethnicity, gender, sexual orientation, immigration549
status, and other demographic characteristics is utilized in discriminatory practices, policing, and550
surveillance by corporations and government agencies [62].551
In contrast, decentralized data marketplaces may be more robust and transparent. However, to fully552
realize the promises of a paradigm shift to decentralized data markets, several social, ethical, and553
technical challenges need to be addressed, such as privacy protections, fair data pricing mechanism,554
and secure platform infrastructure [ 63, 19]. Enabling data market platforms also raises ethical555
concerns and security risks associated with the commodification of personal data, such as the loss of556
privacy and lack of consent in the collection and use of this data [75]. Marginalized and vulnerable557
groups are more at risk of data commodification and privacy erosion, and special protections should558
be enforced for these groups. Safeguards need to be developed to ensure the participation, consent,559
and compensation of the data owners and producers in establishing the provenance and use of data.560
21

1. Submission introducing new datasets must include the following in the supplementary561
materials:562
(a) Dataset documentation and intended uses. Recommended documentation frameworks563
include datasheets for datasets, dataset nutrition labels, data statements for NLP, and564
accountability frameworks.565
(b) URL to website/platform where the dataset/benchmark can be viewed and downloaded566
by the reviewers.567
(c) URL to Croissant metadata record documenting the dataset/benchmark available for568
viewing and downloading by the reviewers. You can create your Croissant metadata569
using e.g. the Python library available here: https://github.com/mlcommons/croissant570
(d) Author statement that they bear all responsibility in case of violation of rights, etc., and571
confirmation of the data license.572
(e) Hosting, licensing, and maintenance plan. The choice of hosting platform is yours, as573
long as you ensure access to the data (possibly through a curated interface) and will574
provide the necessary maintenance.575
2. To ensure accessibility, the supplementary materials for datasets must include the following:576
(a) Links to access the dataset and its metadata. This can be hidden upon submission if the577
dataset is not yet publicly available but must be added in the camera-ready version. In578
select cases, e.g when the data can only be released at a later date, this can be added579
afterward. Simulation environments should link to (open source) code repositories.580
(b) The dataset itself should ideally use an open and widely used data format. Provide a581
detailed explanation on how the dataset can be read. For simulation environments, use582
existing frameworks or explain how they can be used.583
(c) Long-term preservation: It must be clear that the dataset will be available for a long time,584
either by uploading to a data repository or by explaining how the authors themselves585
will ensure this.586
(d) Explicit license: Authors must choose a license, ideally a CC license for datasets, or an587
open source license for code (e.g. RL environments).588
(e) Add structured metadata to a dataset’s meta-data page using Web standards (like589
schema.org and DCAT): This allows it to be discovered and organized by anyone. If590
you use an existing data repository, this is often done automatically.591
(f) Highly recommended: a persistent dereferenceable identifier (e.g. a DOI minted by592
a data repository or a prefix on identifiers.org) for datasets, or a code repository (e.g.593
GitHub, GitLab,...) for code. If this is not possible or useful, please explain why.594
3. For benchmarks, the supplementary materials must ensure that all results are easily repro-595
ducible. Where possible, use a reproducibility framework such as the ML reproducibility596
checklist, or otherwise guarantee that all results can be easily reproduced, i.e. all necessary597
datasets, code, and evaluation procedures must be accessible and documented.598
4. For papers introducing best practices in creating or curating datasets and benchmarks, the599
above supplementary materials are not required.600
22