TSTTC: A Large-Scale Dataset for Time-to-Contact
Estimation in Driving Scenarios
Yuheng Shi
Tianjin University
yuheng@tju.edu.cn
Zehao Huang
TuSimple
zehaohuang18@gmail.com
Yan Yan
TuSimple
yanyan.paper@outlook.com
Naiyan Wang
TuSimple
winsty@gmail.com
Xiaojie Guo
Tianjin University
xj.max.guo@gmail.com
Abstract
Time-to-Contact (TTC) estimation is a critical task for assessing collision risk and is1
widely used in various driver assistance and autonomous driving systems. The past2
few decades have witnessed development of related theories and algorithms. The3
prevalent learning-based methods call for a large-scale TTC dataset in real-world4
scenarios. In this work, we present a large-scale object oriented TTC dataset in the5
driving scene for promoting the TTC estimation by a monocular camera. To collect6
valuable samples and make data with different TTC values relatively balanced, we7
go through thousands of hours of driving data and select over 200K sequences with8
a preset data distribution. To augment the quantity of small TTC cases, we also9
generate clips using the latest Neural rendering methods. Additionally, we provide10
several simple yet effective TTC estimation baselines and evaluate them extensively11
on the proposed dataset to demonstrate their effectiveness. The proposed dataset12
and code are publicly available at dataset link and code link.13
1 Introduction14
In recent years, there has been a growing trend towards equipping vehicles with Advanced Driver15
Assistance System (ADAS), which consists of several subsystems such as Adaptive Cruise Control16
(ACC), Automated Emergency Braking (AEB) and Forward Collision Warning (FCW). ADAS17
aims to detect potential hazards as quickly as possible and alert the driver, or take corrective action18
to improve driving safety. AEB and FCW are critical features of ADAS that protect drivers and19
passengers and prevent traffic accidents. They both rely on the estimation of Time-to-Contact (TTC)20
which is defined as the time for an object to collide with the observer‚Äôs plane. Although TTC can be21
predicted using data from various sensors, such as LiDAR, radar or camera. Vision-based methods are22
particularly attractive due to their low-cost and have been a popular choice among ADAS designers23
and manufacturers. Even in high-level (L3+) autonomous driving system, direct TTC estimation24
could also serve as an redundant observation when other distance measuring sensors fail.25
Prior to the widespread adoption of deep learning, numerous vision-based theories and algorithms [25,26
29, 10, 6, 4] for estimating TTC had been proposed. These algorithms are not data-driven, and usually27
rely on hand-crafted cues. Recently, several deep learning based TTC estimation algorithms have28
emerged [1, 45] and demonstrate promising results in driving scenarios. The emergence of deep29
Submitted to the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets
and Benchmarks. Do not distribute.

NeRF Scenes
‚Ä¶
ùúèùëñ‚àí5 = 2.3
‚Ä¶
ùúèùëñ = 1.9
Real Scenes
ùúèùëñ‚àí3 = 2.1
‚Ä¶
ùúèùëñ‚àí5 = 3.1 ùúèùëñ‚àí7 = 3.1 ùúèùëñ = 2.4ùúèùëñ‚àí3 = 2.7
‚Ä¶ ‚Ä¶‚Ä¶
ùúèùëñ‚àí3 = 2.9
Figure 1: Example sequences and annotations from our dataset. The œÑ denotes the TTC ground-truth
while the subscript denotes the frame index. We could observe that, the scale of the object increases
as the TTC decreases.
learning has brought more powerful tools to computer vision and also brought higher demands for30
large-scale datasets. However, due to the lack of large-scale TTC datasets that capture real-world31
driving scenarios, these methods have to pre-train their model on synthetic flow datasets [27, 12, 22].32
In this paper, we primarily address the challenge of TTC estimation in highway scenarios. Com-33
pared to urban scenarios, high speed driving in highway exhibits longer braking distances, thereby34
necessitating a broader range of perception capabilities. In order to facilitate the development of35
vision-based TTC estimation algorithms, we propose a large-scale monocular TTC dataset in this36
paper, using a class 8 heavy truck as the data collection platform. From raw data collected in urban37
and highway scenarios, we identify over 200K sequences covering a depth range of 400 meters.38
Each sequence contains six consecutive frames captured at a rate of 10Hz, with 2D, 3D bounding39
box and TTC ground-truth provided for a single object in each frame. Additionally, to address the40
limited availability of samples in rare scenarios, such as sudden braking (e.g. small TTC cases), we41
utilize Neural Radiance Fields (NeRF) [30] to generate additional data. These artificially generated42
data can be seamlessly integrated into our dataset, thereby increasing the quantity and diversity of43
data available for training. Fig.1 illustrates two typical examples from our dataset: vehicles are44
gradually approaching in real scenes and NeRF scenes, respectively. Besides the proposed dataset,45
we focus on object level TTC estimation rather than pixel level TTC estimation in [1]. Specifically,46
we provide a sequence of images for a certain object and ask for estimating the TTC value of it in the47
last frame. 2D Bounding boxes are available as optional inputs. Then we provide simple yet effective48
baselines based on the relationship between the scale ratio of objects in adjacent frames and TTC. We49
reformulate the problem as choosing the scale with the highest similarity in adjacent frames. Inspired50
by recent studies [1, 2], we further transform scale estimation from a regression problem into a set of51
binary classification tasks. A series of quantitative experiments are conducted to demonstrate the52
effectiveness and feasibility of our proposed techniques. Our main contribution can be summarized53
as follows:54
‚Ä¢ We propose a large-scale monocular TTC dataset for driving scenarios and will make it55
publicly available along with relevant toolkits to facilitate the development of TTC estimation56
algorithms for driving scenes.57
‚Ä¢ We propose two simple yet effective TTC estimation algorithms and extensively test them58
on the dataset to validate their effectiveness, which could serve as baseline methods for59
future study.60
2 Related work61
Task and Methods. In the scheme of monocular TTC estimation, TTC describes the time that an62
object will cross the camera plane under concurrent relative velocity. Denote the depth of an object63
in the camera coordinate as y, the time for the object under the current velocity to cross the camera64
plane could be calculated by:65
œÑ = ‚àíy/ dy
dt = ‚àíy/ Àôy, (1)
2

where Àôy is the relative velocity between the object and the camera. Though estimating either velocity66
or depth is an ill-posed problem, TTC can be estimated from images directly because it only depends67
on the ratio of them. Researchers have proposed various approaches to accomplish TTC estimation.68
A viable approach is to utilize hand-crafted features such as closed contours, optical flow, brightness,69
or intensity from images [10, 8, 19, 29, 37, 43, 20]. Mobileye [10] adopted geometric information of70
the vehicles in image to estimate TTC by establishing the relationship between TTC and the width of71
vehicle. In addition to geometric-based methods, several studies have been proposed to address the72
task of TTC estimation using photometric-based features, without relying on geometric features or73
high-level processing. For instance, [ 19] adopted accumulated sums of suitable products of image74
brightness derivatives from time varying images to determine the TTC value. Furthermore, [ 43]75
elucidates the relationship between TTC and the changes in intensity in images. However, these hand-76
crafted features need carefully tuned parameters or strong priors, such as constant brightness [19] or77
static scene [20], which restricts their practical applicability.78
Besides to hand-crafted methods, deep-learning approaches can also be employed for TTC estimation.79
One alternative approach is to use scene flow estimation methods [ 28, 40, 35, 21, 45] that predict80
both depth and velocity simultaneously, enabling the generation of pixel-level TTC estimation maps.81
However, these methods depend on accurate optical flow information, which can result in significant82
computational overhead. Recently, [1] proposed Binary TTC that bypasses optical flow computation83
and directly computes TTC via estimating scale ratio between consecutive images. While these84
learning-based methods may produce more promising results, they require a larger amount of data85
with annotated scene flow ground-truth. Due to the expensive labeling cost, scene flow datasets are86
mostly obtained through synthesis, leading to a domain gap for real-world applications.87
In addition to the aforementioned literature, single object tracking (SOT) [ 3, 23, 41, 9, 46] can also88
infer the scale ratio between the template and the tracked object, serving as an alternative approach to89
estimating TTC. However, these methods often rely on large downsampling rates, which may lead to90
the bounding box estimation not accurate enough to meet the requirements of TTC estimation. The91
baseline method which utilized SOT models in our experiment validates the similar effect.92
Table 1: Comparison with several autonomous vehicle
(A V) datasets. "2D-to-3D" indicates the presence of
tightly bounded 2D box annotations with corresponding
3D bounding boxes, which is crucial for TTC estima-
tion.U and H in scenes indicate Urban and Highway
respectively. And we report the average velocity of the
recording platform in the training set for comparison.
KITTI NuScenes Waymo Ours
Scenes U U U U+H
Frequency (hz) 10 2 10 10
Range (m) [0,125] [-80,80] [-75,75] [-160,400]
2D-to-3D ‚úî ‚úñ ‚úñ ‚úî
Avg Speed (m/s) - 5.1 6.9 19.1
Ann. Frames 15K 40K 200K 1M
Boxes 200K 1.4M 12M 1M
Datasets. Contrary to previous TTC esti-93
mation studies, our proposed dataset facil-94
itates the expansion of hand-craft features95
from solely relying on RGB images to uti-96
lizing the features extracted by neural net-97
works. Moreover, we propose a method to98
address the problem of estimating the TTC99
by classifying the scale ratio. And we ex-100
tend the implementation on RGB images to101
feature maps extracted using deep learning102
models, thereby significantly enhancing the103
accuracy of TTC estimation.104
For TTC estimation, several datasets col-105
lected in real scenes are available. For ex-106
ample, [13] proposed a multi-person track-107
ing dataset with stereo depth details, and108
[26] presented a large-scale dataset for TTC estimation in indoor scenes. However, these datasets109
mainly focus on low speed scenarios and are not suitable for direct application to TTC estimation in110
driving scenarios. In addition to datasets specifically designed for TTC estimation, some datasets have111
been synthesized for scene flow tasks and can provide detailed depth information, which are suitable112
for TTC estimation. For example, KITTI scene flow [ 28] proposed an outdoor scene flow dataset113
containing 400 dynamic scenes collected from KITTI [ 17]. These scenes are annotated using 3D114
CAD models for vehicles in motion and manually mask non-rigid moving objects. The Driving [27]115
dataset proposed a synthetic stereo video dataset rendering in realistic style. However, these datasets116
3

are constrained by synthetic and limited scenes, which result in domain gaps with real scenes. Except117
to the scene flow datasets, some RGBD datasets [33, 34, 39], equipped with comprehensive depth118
annotations, can be utilized to train depth estimation models, which in turn can be used for TTC119
estimation. However, the majority depth annotations in these datasets are typically confined to a120
relatively small range (less than 50 meters) and the number of scenes available is limited. In addition121
to the aforementioned datasets, several large-scale datasets proposed for autonomous driving, such122
as [5, 7, 26, 38], offering comprehensive annotations like 3D LiDAR bounding boxes that can be123
used to generate TTC ground-truth. Nevertheless, these datasets were not specifically tailored for124
TTC estimation and presented issues like unbalanced TTC distribution. Furthermore, these datasets125
were mainly collected from urban areas, lacking data for highway scenarios where the estimation of126
TTC is particularly important. Please refer to Table 1 for a comparison of various datasets. Compared127
to the aforementioned datasets, our proposed dataset holds the distinct advantage of being large-scale128
and recorded in real scenarios, encompassing both urban and highway scenes.129
3 Discussion130
A common question related to TTC is that is TTC only applicable for low level assistant driving131
system? Why do we need TTC when we have distance and velocity measurement in advanced132
assistant or autonomous driving system? We argue that there are two main reasons for the essence133
of TTC: First, among all the three physical properties of one object (distance, velocity and time to134
contact), TTC is the only direct observation from monocular image. Monocular depth estimations135
are mostly from fully data-driven aspect, which highly relies on the recognition of the objects and136
scenes, which suffers from out of domain issue seriously [11]. For velocity, the situation is similar137
to that of depth estimation. However, TTC estimation does not require the recognition of semantic138
of objects (can even be achieved by pixel photometric loss), thus has better generalization in corner139
cases. Second, for a high level autonomous driving system, redundancy design is indispensable. Even140
though we have distance and velocity observation, TTC can also be fused into these observations in141
subsequent perception fusion modules, which serves as another independent safety guarantee.142
4 TTC Dataset143
4.1 Data Collection144
The dataset consists of two parts, including a majority of data from real scenes and a minority of data145
from NeRF virtual rendering. After obtaining raw sensor data, we consider a sequence that contains a146
number of consecutive frames of a vehicle as a sample. To gather valuable data, we discard truncated147
objects within the camera plane and filter out 2D boxes smaller than 15 √ó 15 pixels.148
Real Scenes. For real-world scenarios, raw data was collected by our commercial trucks. We capture149
image data using three frontal and two backwards cameras with same 1024 √ó 576 resolution. We150
adopt 2D object detection and tracking algorithms on the images to obtain 2D bounding boxes and151
corresponding track ID for other vehicles. And the LiDAR and Radar data are adopted to generate152
accurate depth and velocity of them. The sampling rate is 10Hz. Detailed sensor specification can be153
found in the appendix. After applying these rules to filter the raw data, we observe that the resulting154
data distribution is highly imbalanced across different TTC ranges. For example, vehicles with155
small TTC are extremely rare in driving scenes, especially for vehicles lie in the same lanes as ego156
vehicle. However these are the cases which FCW and AEB should focus on. In such conditions,157
data collection without rebalancing may result in lack of these valuable scenarios. To overcome this158
challenge, we pre-define a data distribution and sample the data accordingly. The sampling weights159
for the TTC intervals, specifically (0,3], (3,6], (6,10], (10,15], and (15,20], is set to 14%. For the160
TTC range of [-20,0), the sampling weight is designated at 30%.161
NeRF Scenes. Despite thorough data collection efforts, we discover samples with small TTC values162
within the same lane are still extremely scarce, which is a crucial scenario in automated driving and163
4

(ùíôùíã‚àóùíä ,ùíöùíã‚àóùíä ,ùíõùíã‚àóùíä )
ùë°ùëñ
ùë°ùëñ‚àí1
ùë°ùëñ‚àí2
ùë°ùëñ
ùë°ùëñ‚àí1
ùë°ùëñ‚àí2
ùëå
X‚äôZ ‚äô: upwards
ùíÖùíéùíäùíè
Figure 2: Relative position between ego ve-
hicle and an object in bird-eye-view. Only
three frames are plotted for brevity.
Figure 3: Histogram of TTC GT and relative depth
of the training set.
ADAS. To supplement the absence of data in particular conditions, we adopt an internal undisclosed164
project which is developed based on Instant-NGP [31] to render novel scenes. Briefly speaking, the165
background models and object models are firstly extracted and trained separately. And then we can166
form a new scene and render them together. Given a specific object, we pre-define some scripts in167
which the TTC of the object is distributed between 0 and 6. The preset script can be found in our168
appendix. After obtaining NeRF rendered images, we organize them with the same format as real169
scenes data, which serves as an optional component within the training set.170
4.2 Annotation171
In each sequence, we provide the TTC ground-truth for every frame as the annotation. In the172
following, we will describe how we generate the TTC annotation. Given a frame, we first run 2D173
detection on the image and 3D detection on LiDAR. The long range LiDAR detection algorithm174
which reliably covers [-160, 400] meter range. Then, we could obtain its corresponding 2D detection175
box by projecting the 3D box to the image plane then picking the 2D detection box which has highest176
IoU between the projected box. In the vehicle coordinate system, one corner of the 3D bounding177
box could be denoted as xj, yj, zj where j ‚àà {1, 2, ...,8} is the corner index. Here, we take the178
y-coordinate of the corner point which is the closest to ego as the depth of this object:179
j‚àó = arg min
j‚àà{1,2,...,8}
(
q
(xi
j ‚àí 0)2 + (yi
j ‚àí 0)2 + (zi
j ‚àí 0)2), where yi = yi
j‚àó , (2)
where yi is the depth of the vehicle in i-th frame. Here, we assume the relative velocity between the180
vehicle and ego is constant in a short time interval (e.g. q frames) to acquire a stable estimation of the181
velocity. Given the depth of the vehicle in the past q frames, we fit the depth to obtain the relative182
velocity vi by RANSAC [15] algorithm. We set the value of q to 10 by default. It is worth noting183
that the velocity is acquired prior to sequence splitting, thereby q may be greater than the sequence184
length. After obtaining the depth and relative velocity of current frame, the TTC ground-truth could185
be obtained by œÑi = yi/vi.186
Since the camera planes are almost parallel to the XZ plane in the vehicle coordinate and the origins187
of these coordinate systems are highly aligned, we regard the TTC value calculated in the vehicle188
coordinate system as the TTC ground-truth for camera planes. The annotation process is illustrated in189
Fig 2. One may challenge that using past 10 frames to fit the velocity may result in latency in velocity190
estimation especially for sudden break. To remedy this issue, we first generate TTC ground-truth191
with different q (e.g. 3, 5 and 10) and consider the vehicle with varied TTC values as an accelerating192
or decelerating object. For object with varied TTC values, we select the one closest to the TTC193
computed by the velocity of radar sensor as the ground truth. Note that we do not directly utilize194
radar sensors for all objects since they could not cover too distant objects. The data annotations are195
then manually checked by our annotation team to ensure the quality196
5

4.3 Dataset Statistics197
We construct the dataset following the pre-defined rules and annotation pipeline, resulting in 206K198
sequences comprising over 1M frames from real scenes, as well as 1K sequences from 6.0K NeRF199
rendered images. We split the sequences from the real scenes based on their recorded date to training,200
validation and test sets, yielding 149.1K, 28.8K, and 28.6K sequences respectively. For better201
understanding the data distribution, we plot the histogram of the TTC ground-truth and depth in the202
training set in Fig. 3. The distribution in validation and test set is similar. Note that the far away203
samples are rare because we set a minimum 2D bounding box size. More detailed information about204
the dataset statistics is provided in our appendix.205
4.4 Task Definition206
As the tracklet of a vehicle is collected in the format of fixed length sequences, we formulate the TTC207
estimation task in sequence level. For a sequence of a specific vehicle, we provide six consecutive208
frames and their corresponding 2D bounding boxes as input. The last frame in the sequence is209
considered as the target frame while the rest of the frames serve as references. With all frames and210
boxes available in the sequence, the objective is to predict the TTC value of the object in the target211
frame or equivalently relative scale ratio between the target box and the referenced one.212
5 Metrics & Method213
In this section, we first review the relationship between TTC estimation and scale ratio briefly. Then,214
we explicate the evaluation metrics for our TTC estimation task. Subsequently, we introduce our215
approach, which comprises two variants: the pixel MSE approach and its deep learning counterpart.216
5.1 Estimate TTC via Scale Ratio217
As shown in Sec. 4, TTC could be obtained by depth and its rate of change. However, estimating the218
depth and relative velocity of an object directly with only a monocular camera is very challenging. To219
address this issue, researchers proposed to estimate the TTC of frontal-parallel, planar non-deformable220
objects according to the change of object scales. As described in [19], we can obtain the image size221
of a frontal-parallel, non-deformable object of size S at distance y as:222
s = fS/y, (3)
where f is the focal length of the camera. For an object without rotation, TTC can be formulated as a223
function of object size in image space by combining Eq. (1) and (3):224
œÑ = t1 ‚àí t0
1 ‚àí s(t0)
s(t1)
= t1 ‚àí t0
1 ‚àí Œ± , (4)
where s(t0) and s(t1) are the sizes of image of an object in frame t0 and t1 correspondingly. Thus,225
the estimation of TTC can be simplified as a scale ratio estimation problem which can be done with226
only observations in image space. With the development of deep learning, modern object detectors or227
tackers could produce relatively accurate 2D bounding boxes for vehicles. Given the detection or228
tracking bounding box in consecutive frames of a vehicle, one intuitive idea is that we can use the229
ratio of the box or mask area to accomplish the scale ratio estimation. However, are these bounding230
boxes accurate enough for the TTC estimation task and does there exist more accurate scale ratio231
estimation algorithms under such conditions? We try to answer these questions via experimental232
validation on the proposed dataset.233
6

t1
ùë°0
Crop
Resize
Pixel MSE
Similarity
Measurement
{ùí¢(ùë≠0,ùíÉ0
ùíä )}
ùí¢(ùë≠1,ùíÉ1)
Backbone
ùíÉ1
ùìëConv Pred
Deep Scale
GT
ÔÉª ÔÉª‚úì
FC
{ùí¢(ùë≠0,ùíÉ0
ùíä )}
ùí¢(ùë≠1,ùíÉ1)
Similarity
Measurement
ÔÉª
ÔÉª
‚úì Average
Similarity Measurement
‚äó
Figure 4: Framework of our proposed methods. After aligning the size between contents in b1 and B,
an operator N is applied to measure their similarity. For simplicity, we only illustrate three scale
rates of B. Some operations such as center shift are omitted for brevity. The green dashed box and
the orange dashed box represent Pixel MSE and Deep Scale, respectively.
5.2 Evaluation Metrics234
Before introducing the detailed design, we need to design the metrics for evaluation. Here, we adopt235
Motion-in-Depth (MiD) error and Relative TTC Error (RTE) as performance indicators, which could236
be denoted as:237
MiD = ||log(Œ±) ‚àí log(ÀÜŒ±)||1 √ó 104,
RTE = ||œÑ ‚àí ÀÜœÑ
œÑ ||1 √ó 100%,
(5)
where Œ± and ÀÜŒ± mean the ground-truth and predicted scale ratios while the ÀÜœÑ denotes predicted TTC238
value. The scale ratio ground-truth Œ± is obtained by Eq. (4). MiD is utilized in previous works [45, 1]239
to describe the TTC error indirectly from the perspective of scale ratio. Due to the instability of240
TTC at larger values, we prioritize the MiD as the primary metric. With the purpose of revealing241
more information from the evaluation metrics, we partition several TTC intervals, namely crucial(c) /242
small(s) / large(l) / negative(n)1, which correspond to TTC values of 0‚àº3, 3‚àº6, 6‚àº20, and -20‚àº0,243
respectively. We mark them on the indices of the RTE and MiD. The threshold for crucial cases is244
determined by rounding the typical TTC threshold of 2.7 seconds used in FCW systems [ 36, 48].245
This division allows a more detailed analysis of the performance for the TTC estimation algorithms246
in different TTC intervals, providing a better understanding of their limitations and strengths. During247
the prediction, TTC values that exceed the predefined range will be truncated to the boundary value.248
5.3 Our Design249
Formulation. We denote the 2D bounding box as b = [x, y, w, h] where x, yrepresent the center250
coordinate and w, hdenote the width and height. Given a reference frame F0 and a target frame251
F1, b0 = [x0, y0, w0, h0] and b1 = [x1, y1, w1, h1] are bounding boxes of a specific object in these252
two frames. The core idea of our methods is to estimate the relative scale ratio of this vehicle253
between the reference frame and the target frame. A straightforward approach is simply adopting254
the scale ratio between b0 and b1 as the result. However, this strategy is largely influenced by the255
precision of detection algorithms. In our methods, we estimate the scale ratio change in these two256
frames in pixel space or feature space, yielding two kinds of implementation: Pixel MSE and Deep257
Scale. For the reference frame, we enumerate n different scale ratios to obtain a series of scaled258
boxes B = [bŒ±1
0 , bŒ±2
0 , ...,bŒ±i
0 , ...,bŒ±n
0 ] where bŒ±i
0 = [x0, y0, Œ±iw1, Œ±ih1]. Then, we crop F0 via B259
and resize them to a target size of W, H, which could be denoted as G(F0, bŒ±i
0 ) for the i-th scale260
ratio, where G(F, b) denotes the crop b on F and resize it. Similarly, we could get G(F1, b1) for261
the target frame. Finally, we use an operator to measure the similarity between G(F0, bŒ±i
0 ) and262
1Negative TTC indicates away from the observer.
7

G(F1, b1), yielding n similarity scores. With five frames free to reference, taking different frames263
as the reference will produce different scale ratios. To address this issue, we convert scale ratio to264
corresponding TTC value via Eq. (4) and convert it to the scale ratio under the setting of 10Hz when265
computing MiD. We list the relationship between different scale ratios of same œÑ in the appendix.266
Pixel MSE. We can measure the similarity between G(F0, bŒ±i
0 ) and G(F1, b1) in image space with267
Mean Squared Error (MSE). The weighted sum of the top k similar scale ratios is adopted as the final268
estimation and the weight is normalized by the reciprocal of MSE. The top of Fig. 4 illustrates the269
pipeline of Pixel MSE.270
Deep Scale. For the deep version, we first input two images into a backbone network for feature271
extraction. Afterwards, the grid sampling operation is used to align the features of different box sizes272
into one fixed size. Then, we calculate the similarity of each position in the two feature maps via273
cosine similarity, yielding a similarity mapSi. Afterwards, the similarity score of scale Œ±i is obtained274
by adopting a Global Average Pooling (GAP) operation to Si. Then we concatenate the similarity275
scores of different scale ratios and use a Fully-Connected (FC) layer to obtain the final prediction.276
During training, binary cross-entropy (BCE) loss is used and we adopt the strategy proposed in277
[24, 47] to convert the scale ratio ground-truth to a size n vector as soft label. Similar to Pixel278
MSE, we apply a top k weighted sum operation to get the final results and the weight is defined279
as the sigmoid of the FC output. For fast inference, we adopt a convolutional layer followed by a280
stage of modified CSPNet [ 42] used in [16] as our backbone. After obtaining backbone features,281
we fed them into one transposed convolutional layer and two convolutional layers before similarity282
measurement. To capture subtle details, all the stride in the network is set to 1 except the first and the283
transposed convoultional layer which are set to 2 yielding a feature map with the same size of input.284
The framework is illustrated at the bottom of Fig. 4.285
Center Shift. The boxes predicted by the object detection model may be inaccurate, which will286
bring misalignment between the centers of reference and target boxes. To address this problem, we287
introduce a center shift operation. Specifically, we enumerate an offset of [‚àíc, c] along height and288
width direction, respectively, which yields a total of (2c + 1)√ó (2c + 1)candidates. After obtaining289
(2c + 1)√ó (2c + 1)similarity scores for a single scale, we adopt the highest score as the final score290
for both Pixel MSE and Deep Scale. Our experiments show that this operation will bring significant291
improvement in terms of MiD and RTE with little time cost.292
6 Experimental Validation293
6.1 Implementation Details294
Pixel MSE. For Pixel MSE, we validate its performance on validation, and test set. The target size295
W, Hafter interpolation is set to the size of b1. The scale ratio is set to the range of [0.65, 1.5] to296
cover samples with different scale ratios in the training set. The number of scale bins n is 125, the297
top k for the weighted sum and the c for center shift are 3. Besides, the detection boxes are manually298
expanded with a maximum ratio of 1.1 (if the expanded boxes do not exceed the image boundary) to299
reduce the influence of inaccurate detection results.300
Deep Scale. In terms of Deep Scale, we train it for 36 epochs on the train/train+val set dataset for301
evaluation on val/test respectively with a batch size of 16 using SGD [ 18]. The image is resized302
to 1024 √ó 576 for both training and test phases. We adopt random color on HSV space as data303
augmentation during training. The weight decay and SGD momentum parameters are set to 0.0005304
and 0.9, respectively. We start from a learning rate of10‚àí4√ó BatchSize and adopt cosine learning305
rate schedule. The target size is set to 50 √ó 50 for grid sample as larger size does not bring more306
benefits. The input channel for the backbone is set to 12, while the channel for the three followed307
convolutional layers is set to 24. The kernel size of the convolutional layers is 7 while the transposed308
one is 3. For the scale range and box expansion, we keep them the same as Pixel MSE. Besides, the309
8

number of scale bins n, the top k for the weighted sum and the c for center shift are set to 20, 4 and 1310
respectively. We test the latency of all models with FP16 and batch size of 1 on a 3090 GPU.311
Table 2: Main results of different methods on the
validation set. The ‚Ä† means the result is obtained
under padding NeRF data. The % after RTE is
omitted for brevity.
Methods MiD MiD c MiDs MiDl MiDn RTE
Detection 213.9 675.4 305.1 112.4 115.3 58.1
SOT [46] 200.8 641.1 261.1 77.4 158.8 57.1
Pixel MSE 41.0 57.4 36.5 32.5 48.4 29.9
Depth 62.3 111.9 74.6 36.1 68.4 47.3
LiDAR 6.4 12.5 6.0 5.3 3.3 -
Deep Scale 14.4 27.1 16.4 10.9 13.5 12.1
Deep Scale‚Ä† 14.3 26.5 15.2 10.8 13.5 12.0
Besides the aforementioned methods, we further312
propose two baselines termed as Detection and313
SOT in Table 4. For Detection, the scale ratio is314
obtained by simply computing the ratio between315
the area of the target box and the reference box.316
As for SOT, given a target box, we adopt a state-317
of-the-art (SOTA) SOT tracker [ 46] to obtain318
a reference box and then estimate the scale ra-319
tio as the same as in Detection. Additionally,320
we include results from an internal monocular321
depth algorithm and a LiDAR detection + track-322
ing algorithm for a comprehensive evaluation.323
Details of these algorithms are provided in our324
appendix. Although there are other methods available, such as [45, 1], the models released by the325
authors achieve poor results in our dataset due to the domain gap, and no training codes are provided.326
6.2 Main Results327
The detailed comparison between various methods is presented in Tab. 4. Due to page constraints,328
we report only their performance on the MiD metric and overall RTE. As observed, the RTE329
predicted by box detection or tracking methods is approximately 50%, which is inadequate for330
practical applications. In contrast, our Pixel MSE method demonstrates significantly lower MiD331
errors, indicating more accurate scale ratio estimations and consequently, lower RTEs. Among332
learning-based methods, our Deep Scale significantly outperforms the depth estimation method,333
although it remains inferior to the LiDAR detection + tracking algorithm. Nevertheless, it achieves334
the best performance among methods that utilize images. More detailed comparison, ablations and335
visualizations could be found in the appendix.336
7 Limitations337
Our work is a large-scale benchmark for Time-To-Contact estimation, but there are still some338
limitations that need to be addressed in future works. In regard to our dataset, the collected data339
primarily focuses on trucks and cars in highway and urban scenarios, lacking more diverse categories340
that are commonly found in autonomous driving datasets, such as cyclists and pedestrians. Besides,341
we have to acknowledge that due to the inherent differences in distribution between our dataset and342
real-world scenarios, there may be potential challenges when directly applying the model trained on343
our dataset to real-world contexts.344
Furthermore, the baseline methods proposed in this study operate under the assumption that objects345
exhibit frontal-parallel characteristics and are non-deformable. However, it is essential to acknowledge346
that real-world conditions are considerably more intricate, and these methods may yield suboptimal347
performance when the underlying assumption is not met. Additionally, as we mentioned earlier, our348
methods are sensitive to the alignment between the box center of the target and reference frame,349
which poses another limitation.350
8 Conclusion351
In this work, we built a large-scale TTC dataset and provided a simple yet effective TTC estimation352
algorithm as baselines for the community. Our dataset is characterized by its focus on objects in353
driving scenes, which contains both urban and highway scenarios and covers a wider range of depth.354
We hope that our proposed dataset could facilitate the development of TTC estimation algorithms.355
9

Appendix465
Sensor Specification466
GNSS
cam1cam3 cam4 cam8
cam9
radar2
radar3 radar1
radar4radar5
Lidar2 Lidar1
ùëå
X‚äôZ ‚äô: upwards
Lidar4
Figure 5: Sensor layout and coordinate system. The coordinate system of radars and Lidars are
omitted for brevity. And GNSS means the Global Navigation Satellite System.
In our research, we conducted a comprehensive data collection process utilizing five cameras with467
varying focal lengths, five radar sensors, and three Lidar sensors. The spatial arrangement of these468
sensors is visually depicted in Fig. 5. Furthermore, we provide detailed specifications of the cameras469
in Tab. 3.470
More details about data statistic471
Figure 6: Distribution of object sizes and time-of-day.
Real scenes comprise 10.8 % ur-472
ban/suburban and 89.2 % highway473
data. However, rare cases with small474
TTC on the same lane ([0,6]) make up475
only 0.02 % of real scenes. To address476
this, we have supplemented these rare477
cases using data synthesized by NeRF.478
The synthesized data, constituting 5K479
sequences, represents highway scenes and480
is exclusively used for the training set. The481
data distribution of training set for object482
sizes and time-of-day are shown in Fig 6.483
As data in training, validation, and test sets484
are randomly split, their distributions are485
Table 3: Camera specification. all images captured by the cameras are subjected to downsampling
and cropping, resulting in a size of 1024x576 pixels. The camera‚Äôs horizontal field of view (HFOV)
refers to the angular range covered by the camera along the y-axis in the x-y plane of the camera
sensor frame.
Camera 1 3 4 8 9
HFOV ¬±63.2‚ó¶ ¬±40.4‚ó¶ ¬±18.4‚ó¶ ¬±63.2‚ó¶ ¬±63.2‚ó¶
13

consistent, so we‚Äôve omitted the latter two. Besides, the geographical distribution of the dataset spans486
two cities: Shanghai and Hebei in China.487
More explanation on MiD computing488
In the Sec 5.3 of our main paper, we first convert the predicted scale ratio under different FPS settings489
to corresponding TTC value and then convert the TTC value to the scale ratio under the setting of490
10Hz. Actually, the scale ratios computed under different FPS settings could convert to each other.491
Since the TTC ground-truth for the target frame is specific, we could get the relationship between the492
scale ratios (e.g. Œ±m, Œ±n) under different FPS settings (e.g. F P Sm, FP Sn) by Eq.(5) in our paper:493
Œ±m = 1
FPS n
FPS m
(1/Œ±n ‚àí 1) + 1. (6)
In practice, the scale ratios obtained under different FPS settings are converted to the 10Hz setting494
via Eq (6) directly.495
Detailed Results496
Table 4: Detailed results of different methods. The ‚Ä† means the result is obtained under padding
NeRF data. The % after RTE is omitted.
Methods Validation Set
MiD MiD c MiDs MiDl MiDn RTE RTE c RTEs RTEl RTEn
Detection 213.9 675.4 305.1 112.4 115.3 54.2 58.1 56.3 55.0 50.2
SOT [46] 200.8 641.1 261.1 77.4 158.8 52.8 57.1 50.9 48.1 58.4
Pixel MSE 41.0 57.4 36.5 32.5 48.4 29.9 11.3 13.0 31.0 44.3
Depth 62.3 111.9 74.6 36.1 68.4 47.3 - - - -
LiDAR 6.4 12.5 6.0 5.3 3.3 - - - - -
Deep Scale 14.4 27.1 16.4 10.9 13.5 12.1 6.3 8.7 13.0 14.8
Deep Scale‚Ä† 14.3 26.5 15.2 10.8 13.5 12.0 6.2 8.4 12.9 14.6
Target Box Reference Box GT & Predictions
Figure 7: Case study for our Pixel MSE and
Deep Scale, best viewed in color. For the last
column, box with green, red and blue color
denote the scaled box obtained by GT, Pixel
MSE and Deep Scale.
We report detailed results of different methods on497
validation set in Tab. 4 of both MiD and RTE. For498
the Depth method, we first estimate the depth us-499
ing a mono depth estimation model and then utilize500
RANSAC to fit the TTC value. Our experiments on501
the validation set demonstrate that the relative error502
of the depth estimation is only 10.7%. However, the503
relative TTC error and MiD error are significantly504
worse than our proposed method, reaching 47.3 %505
and 62.3 respectively. The primary reason for such506
large errors is the inherent noise present in the depth507
estimation. For the LiDAR model, it is a internal508
sparse detector like SECOND [44] and FSDv2 [14].509
The 3D bboxes in the training set were generated by510
an internal algorithm. The tracker we used is Sim-511
ple Track [32]. The average BEV IoU between the512
3D bboxes generated by internal algorithm and the513
manual annotations on the validation and test sets is514
83.1%.515
In addition to quantitative results, we also illustrate516
several cases in Fig. 7 for intuitive understanding. In517
the last column, we draw the scaled target box in the518
reference frame which is obtained by utilizing the center of the reference box and the scale ratio519
14

Table 5: Influence of scale bins n in Pixel MSE
n 50 75 100 125 150
MiD 42.5 41.8 41.3 41.0 41.0
RTE(%) 31.9 31.9 30.8 29.9 30.0
Time(ms) 10.0 13.6 15.5 18.5 21.2
Table 6: Influence of scale bins n in Deep Scale.
n 10 15 20 25 30
MiD 16.8 14.9 14.4 14.4 14.4
RTE(%) 13.5 12.7 12.1 12.4 12.2
Time(ms) 11.7 11.9 12.0 12.2 12.4
Table 7: Ablation on the center shift operation
for Pixel MSE and Deep Scale
Pixel MSE Deep Scale
w/ S w/o S w/ S w/o S
MiD 41.0 73.3 14.4 17.2
RTE(%) 29.9 37.6 12.1 14.6
Time(ms) 18.5 17.4 12.0 10.8
Table 8: Ablation on padding different amounts of
NeRF rendered sequences.
Seqs MiD MiDc MiDs MiDl MiDn
0 K 14.4 27.1 15.5 10.9 13.5
3 K 14.5 27.1 15.4 10.8 13.6
5 K 14.3 26.5 15.2 10.8 13.7
7 K 14.6 26.7 15.4 11.2 13.8
obtained from various methods and ground truth. From the first and second cases, we can observe that520
the Deep Scale is more robust to the illumination changes and inaccurate detection boxes compared521
to Pixel MSE. In the last row, we showcase a failure case caused by a severely inaccurate detection522
box. As described before, our methods are sensitive to the alignment between the box center of523
the target and reference frame, which is also a limitation of our methods. These cases also reveal524
the key difference between TTC estimation and tracking task: TTC estimation requires far more525
accurate estimation than tracking, while tracking usually focuses on complicated appearance change.526
Furthermore, we have included visualizations in GIF format in the supplementary material located527
at ./Vis/monoDepth_visualization to compare our proposed method and TTC estimation via528
depth estimation. These visualizations provide a more intuitive understanding of the noise in the529
depth estimation.530
Ablation Study531
To validate the contribution of different components and hyper-parameters, we perform extensive532
experiments and report the results on the validation set unless otherwise specified. Default hyper-533
parameters are denoted in bold.534
Number of Scale Bin. To probe the suitable scale bins for our Pixel MSE, we conduct ablation535
experiments, as shown in Table 5. The performance continues to boost until scale bin number reaches536
125 and then becomes stable. Thus, we take the 125 bins as the default setting. To determine the537
optimal scale bin number n for Deep Scale, we vary the value of n from 10 to 30. As shown in538
Table 6, the MiD and RTE continuously decrease until n = 20, after which they tend to be stable.539
However, an increase in scale bins results in a larger computation cost. As a consequence, we set n to540
20 by default.541
Center Shift. In this experiment, we present a comparison between the results obtained with and542
without center shift operation. We list the results for both pixel MSE and Deep scale, as shown in543
Table 7. The center shift operation is effective in reducing the estimation error.544
NeRF Augmentation. To investigate the impact of supplementing NeRF data on the training process545
for the Deep Scale, we conducted ablation experiments by adding different numbers of NeRF546
sequences. The added NeRF sequences were randomly selected from the NeRF data we rendered.547
To avoid the potential impact of data randomness on the experimental results, we report the average548
results of five runs with different random seeds in Table 8. The results show that padding NeRF549
sequences can effectively reduce the MiD error in crucial scenes. However, too many NeRF sequences550
lead to a slight decrease in overall performance. Therefore, we use 5K rendered NeRF sequences in551
our experiments.552
15

On Target Size. In this experiment, we ablate the target size W, Hfor grid sample in Deep Scale553
and we keep W = H when conducting ablation for convenience. Table 9 lists the result for different554
settings and we can observe that the performance continuously to boost until 50 and then becomes555
stable. As a consequence, we set the default target size to 50.556
Table 9: Influence of the target size for grid sam-
pling.
Size 10 25 50 75 100
MiD 20.0 14.9 14.4 14.8 14.9
RTE(%) 16.3 12.4 12.1 12.3 12.7
Table 10: Influence of the kernel size.
K 1 3 5 7 9
MiD 18.5 15.1 14.8 14.4 14.0
RTE(%) 15.2 12.3 12.3 12.1 11.8
On Kernel Size. Without large downsampling rate in our Deep Scale, the receptive field is mainly557
decided by the kernel size of the convolutional layers. To find the optimal kernel size, we train our558
model with the kernel size ranging from 1 to 9 and report the results in Table 10. Although increasing559
the kernel size can improve the performance, it comes at the cost of longer inference latency. As a560
trade off, we set the default kernel size to 7.561
On Plate Blur. To verify whether the plate blur will influence the scale ratio estimation, we conduct562
ablation experiments. We test the MiD and RTE on the validation, and test set in w/ and w/o blur563
settings for the Pixel MSE. For the Deep Scale, we train the model on the blurred train/train+val564
set and report the result of val/test set in w/ and w/o blur settings. As shown in Table 11, for both565
methods, the plate blur operation brings negligible differences. As a conclusion, we take the dataset566
with blurred license plates as the release version.567
Number of Frame Gap. In this experiment, we validate the influence of frame gap for both Pixel568
MSE and Deep Scale. The minimum and maximum scale ratio for different frame gaps are adjusted569
according to Eq. (6). For the Deep Scale, we train our model in different frame gaps. As for testing,570
we maintain the frame gap consistent with the training settings to ensure optimal results. We list571
detailed results in Table 12. Larger frame gap brings more obvious scale changes and thus benefits572
the classification process. As a result, we set the default frame gap to 5.573
More Visualization574
In Fig. 8, we present more samples from our dataset, covering different ranges of TTC, meteorological575
fluctuations, and models rendered using NeRF. To get more intuitive understanding for different576
methods, we present more cases for visualization in Fig. 9. In the first column of Fig. 9, we plot577
the detection boxes of the target frames. In the second column, we show the boxes generated from578
detection and tracking model. For the last column, we show the scaled boxes obtained by GT,579
Pixel MSE and Deep Scale. As we can observe, the Pixel MSE produces unsatisfactory outcomes580
when object images encounter significant illumination changes or low quality, as exemplified in the581
first, second, and fourth cases. In contrast, the Deep Scale metric continues to perform robustly.582
Nonetheless, severe occlusion remains a challenge that adversely affects the performance of both583
Pixel MSE and Deep Scale, as demonstrated in the third case.584
Additionally, we have included enhanced visualizations in GIF format within our supplementary585
material. These visualizations, which can be found in the ./Vis/prediction_visualization586
Table 11: Ablation on plate blur for our methods
Pixel MSE Deep Scale
w/ blur w/o blur w/ blur w/o blur
Val MiD 41.0 41.0 14.4 14.4
RTE(%) 29.9 30.0 12.1 12.1
Test MiD 40.3 40.3 14.8 14.8
RTE(%) 28.7 28.7 12.3 12.3
16

Table 12: Ablation on the frame gap for Pixel MSE and Deep Scale.
Gap 1 2 3 4 5
PixelMSE MiD 102.1 61.2 46.4 41.2 41.0
RTE(%) 95.7 59.0 42.8 35.1 29.9
DeepScale MiD 31.5 22.7 18.1 15.8 14.4
RTE(%) 29.5 20.1 15.9 13.5 12.1
NeRF Scenes
‚Ä¶
ùúèùëñ‚àí5 = ‚àí2.8
‚Ä¶
ùúèùëñ = ‚àí3.2
Real Scenes
ùúèùëñ‚àí3 = ‚àí3.0
‚Ä¶
‚Ä¶
ùúèùëñ‚àí5 = 3.2
‚Ä¶
ùúèùëñ = 2.7ùúèùëñ‚àí3 = 2.9
‚Ä¶
‚Ä¶
ùúèùëñ‚àí5 = 3.1 ùúèùëñ = 2.5ùúèùëñ‚àí3 = 2.7
‚Ä¶‚Ä¶
ùúèùëñ‚àí3 = 2.9
‚Ä¶
ùúèùëñ‚àí5 = 2.7 ùúèùëñ = 2.3
‚Ä¶‚Ä¶
ùúèùëñ‚àí3 = 2.5
Figure 8: More visualization for samples in real and NeRF scenes.
and ./Vis/scene_visualization directories, separately showcase qualitative results and various587
scene representations.588
NeRF Script589
For the scripts used for NeRF rendering, we list them in Table 13. vego and vvehicle denote the initial590
speed of ego and the target vehicle respectively. The y denotes the relative distance in depth direction591
and we only consider the straight lane when setting the scripts. We permute and combine the speed of592
ego and vehicle to get more scenarios. For the rendered images, we also adopt the detection model to593
generate 2D bounding boxes and the truncated objects will be discarded to ensure the completeness.594
17

Target Box GT & Prediction 
Boxes
Det & Tracked
Boxes
Target Box GT & Prediction 
Boxes
Det & Tracked
Boxes
(1) (3)
(2) (4)
Figure 9: More visual comparison of the detection, tracking, and proposed methods, best viewed in
color. In the second column, we use the blue and red color to distinguish the box from detection and
tracking. For the last column, box with green, red and blue color denote the scaled box obtained by
GT, Pixel MSE and Deep Scale.
18

Table 13: Script for NeRF rendering.
No. Initial Status Scriptvego(km/h) vvehicle(km/h) y(m)
1
40
60
80
vego 50 1. Ego drives at vego while the target vehicle de-
celerates at a speed of -3m/s2.
2. After 3 seconds, ego decelerates with an accel-
eration of -4m/s2 until its velocity matches that of
the target vehicle.
2
40
60
80
vego‚àí20 65 1. Ego drives at vego.
2. At a distance range of (10, 50, 5)m to the target
vehicle, ego performs a lane change at a constant
lateral relative speed of 2m/s, until it is completely
in a different lane from the target vehicle.
3 60
80
20
40 65 1. Ego gradually accelerates towards the tar-
get vehicle with an acceleration of range(0.5, 3,
0.5)m/s2.
2. When the distance between ego and the target
vehicle is within the range of (10, 50, 5) m, the
target vehicle changes lanes with a constant lateral
relative speed of 2m/s from a adjacent lane, until
ego and the target vehicle are completely in same
lane. At the same time, ego decelerates at -4m/s2
within the same distance range of (10, 50, 5)m.
4 60
80 60 65 1. Ego drives at vego while the target vehicle de-
celerates at a speed of -3m/s2.
2. After 3 seconds, ego decelerates with an accel-
eration of -4m/s2 until its velocity matches that of
the target vehicle.
5 40
60
20
30 65 1. Ego drives at vego while the target vehicle grad-
ually accelerates with an acceleration range of (0.5,
3, 0.5)m/s2.
2. At a distance range of (20, 60, 4)m to the target
vehicle, ego smoothly changes lanes with a lateral
velocity range of (0.5, 1.5, 0.2)m/s, until ego and
the target vehicle are completely in different lanes
or the distance between them is less than 5m.
6
40
60
80
vego‚àí30 65 1. Ego drives at vego while the target vehicle
gradually accelerates with an acceleration range of
(0.5,3,0.5)m/s2.
2. When the distance to the target vehicle is in the
range of (10, 50, 4) m, ego gradually decelerates
with an acceleration of -(1, 4, 0.5)m/s2 until ego‚Äôs
velocity matches the target vehicle‚Äôs velocity or the
distance between them is less than 5m.
19

Checklist595
1. For all authors...596
(a) Do the main claims made in the abstract and introduction accurately reflect the paper‚Äôs597
contributions and scope? [Yes]598
(b) Did you describe the limitations of your work? [Yes] See Sec. 7.599
(c) Did you discuss any potential negative societal impacts of your work? [Yes] See600
Supplementary materials.601
(d) Have you read the ethics review guidelines and ensured that your paper conforms to602
them? [Yes]603
2. If you are including theoretical results...604
(a) Did you state the full set of assumptions of all theoretical results? [Yes] See Sec. 5.605
(b) Did you include complete proofs of all theoretical results? [Yes]606
3. If you ran experiments (e.g. for benchmarks)...607
(a) Did you include the code, data, and instructions needed to reproduce the main experi-608
mental results (either in the supplemental material or as a URL)? [Yes] See abstract.609
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they610
were chosen)? [Yes]611
(c) Did you report error bars (e.g., with respect to the random seed after running ex-612
periments multiple times)? [No] The paper does not report error bars or statistical613
significance information because the results of multiple experiments are consistent and614
show minimal variation.615
(d) Did you include the total amount of compute and the type of resources used (e.g., type616
of GPUs, internal cluster, or cloud provider)? [Yes]617
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...618
(a) If your work uses existing assets, did you cite the creators? [Yes]619
(b) Did you mention the license of the assets? [Yes]620
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]621
See abstract.622
(d) Did you discuss whether and how consent was obtained from people whose data you‚Äôre623
using/curating? [Yes]624
(e) Did you discuss whether the data you are using/curating contains personally identifiable625
information or offensive content? [Yes]626
5. If you used crowdsourcing or conducted research with human subjects...627
(a) Did you include the full text of instructions given to participants and screenshots, if628
applicable? [N/A]629
(b) Did you describe any potential participant risks, with links to Institutional Review630
Board (IRB) approvals, if applicable? [N/A]631
(c) Did you include the estimated hourly wage paid to participants and the total amount632
spent on participant compensation? [N/A]633
20