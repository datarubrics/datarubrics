Few-Class Arena: A Benchmark for Efficient Selection
of Vision Models and Dataset Difficulty Measurement
Anonymous Author(s)
Affiliation
Address
email
Abstract
A wide variety of benchmark datasets with many classes (80-1000) have been1
created to assist Computer Vision architectural evolution. An increasing number of2
vision models are evaluated with these many-class datasets. However, real-world3
applications often involve substantially fewer classes of interest (2-10). This gap4
between many and few classes makes it difficult to predict performance of the5
few-class applications using models trained on the available many-class datasets.6
To date, little has been offered to evaluate models in thisFew-Class Regime. We7
propose Few-Class Arena (FCA), as a unified benchmark with focus on testing8
efficient image classification models for few classes. We conduct a systematic9
evaluation of the ResNet family trained on ImageNet subsets from 2 to 1000 classes,10
and test a wide spectrum of Convolutional Neural Networks and Transformer11
architectures over ten datasets by using our newly proposedFCA tool. Furthermore,12
to aid an up-front assessment of dataset difficulty and a more efficient selection13
of models, we incorporate a difficulty measure as a function of class similarity.14
FCA offers a new tool for efficient machine learning in the Few-Class Regime,15
with goals ranging from a new efficient class similarity proposal, to lightweight16
model architecture design, to a new scaling law. FCA is user-friendly and can be17
easily extended to new models and datasets, facilitating future research work. Our18
benchmark is available at https://github.com/fewclassarena/fca.19
1 Introduction20
The de-facto benchmarks for evaluating efficient vision models are large scale with many classes21
(e.g. 1000 in ImageNet [ 1], 80 in COCO [ 2], etc.). Such benchmarks have expedited the advance of22
vision neural networks toward efficiency [3, 4, 5, 6, 7, 8, 9, 10] with the hope of reducing the financial23
and environmental cost of vision models [11, 12]. More efficient computation is facilitated by using24
quantization [ 13, 14, 15], pruning [ 16, 17, 18, 19], and data saliency [ 20]. Despite efficiency25
improvements such as these, many-class datasets are still the standard of model evaluation.26
Real-world applications, however, typically comprise only a few number of classes (e.g, less than27
10) [21, 22, 23] which we termed Few-Class Regime. To deploy a vision model pre-trained on large28
datasets in a specific environment, it requires the re-evaluation of published models or even retraining29
to find an optimal model in an expensive architectural search space [24].30
One major finding is that, apart from scaling down model and architectural design for efficiency,31
dataset difficulty also plays a vital role in model selection [25] (described in Section 4.3).32
Submitted to the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets
and Benchmarks. Do not distribute.

0 200 400 600 800 1000
NCL
30
40
50
60
70
80
90
100T op-1 Acc. (%)
18 NCL
18 1K
34 NCL
34 1K
50 NCL
50 1K
101 NCL
101 1K
152 NCL
152 1K
(a) Accuracies for sub-models (blue) and full models
(red).
0 200 400 600 800 1000
NCL
30
40
50
60
70
80
90
100T op-1 Acc. (%)
5 10
75
100
101 NCL
101 1K
101 NCL
101 1K
(b) Zoomed window shows accuracy values and
range for full and sub-models in the few-class range.
0 200 400 600 800 1000
NCL
30
40
50
60
70
80
90
100T op-1 Acc. (%)
5 10
60
70
80
18 1K 34 1K 50 1K 101 1K 152 1K
(c) Zoomed window shows (c.1) drop of accuracy
as NCL decreases, (c.2) accuracy scales with model
size for full models in the few-class range.
0 200 400 600 800 1000
NCL
30
40
50
60
70
80
90
100T op-1 Acc. (%)
5 10
90
95
100
18 NCL 34 NCL 50 NCL 101 NCL 152 NCL
(d) Zoomed window shows (d.1) rising accuracy as
NCL decreases, (d.2) accuracy does not scale with
model size for sub-models in the few-class range.
Figure 1: Top-1 accuracies of various scales of ResNet, whose model sizes are shown in the legend, and whose
plots vary from dark to light by decreasing size. Plots range along number of classesNCl from the full ImageNet
size (1000) down to the Few-Class Regime. Each model is tested on 5 subsets whose NCl classes are randomly
sampled from the original 1000 classes. (a) Plots for sub-models trained on subsets of classes (blue) and full
models trained on all 1000 classes (red). (b) Zoomed window shows the standard deviation of subset’s accuracies
is much smaller than for the full model. (c.1) Full model accuracies drop when NCL decreases. (c.2) Full model
accuracies increase as model scales up in the Few-Class Regime. (d.1) Sub-model accuracies grow as NCL
decreases. (d.2) Sub-model accuracies do not increase when model scales up in the Few-Class Regime.
Figure 1 summarizes several key findings under the Few-Class Regime. On the left graph in red33
are accuracy results for a range of number of classes NCL for what we call the “full model”, that34
is ResNet models pre-trained on the full 1000 classes of ImageNet (generally available from many35
websites). On the right are accuracy results for what we call “sub-models”, each of which is trained36
and tested on the same NCL, where this number of classes is sampled from the full dataset down to37
the Few-Class Regime. Findings include the following. (a) Sub-models attain higher upper-bound38
accuracy than full models. (b) The range of accuracy widens for full models at few-classes, which39
increases the uncertainty of a practitioner selecting a model for few classes. In contrast, sub-models40
narrow the range. (c) Full models follow the scaling law [26] in the dimension of model size - larger41
models (darker red) have higher accuracy from many to few classes. (4) Surprisingly, the scaling law42
is violated for sub-models in the Few-Class Regime (see the zoomed-in subplot) where larger models43
(darker blue) do not necessarily perform better than smaller ones (lighter blue). From these plots,44
our key insight is that, instead of using full models, researchers and practitioners in the Few-Class45
Regime should use sub-models for selection of more efficient models.46
However, obtaining sub-models involves computationally expensive training and testing cycles since47
they need to be converged on each of the few-class subsets. By carefully studying and comparing the48
experiment and evaluation setup of these works in the literature, we observe that, how models scale49
down to Few-Class Regimeis rarely studied. The lack of comprehensive benchmarks for few-class50
research impedes both researchers and practitioners from quickly finding models that are the most51
2

efficient for their dataset size. To fill this need, we propose a new benchmark,Few-Class Arena(FCA),52
with the goal of benchmarking vision models under few-class scenarios. To our best knowledge, FCA53
is the first benchmark for such a purpose.54
We formally define Few-Class Regimeas a scenario where the dataset has a limited number of classes.55
Real-world applications often comprise only a few number of classes (e.g. NCL < 10 or 10% classes56
of a dataset). Consequently, Few-Class Arena refers to a benchmark to conduct research experiments57
to compare models in the Few-Class Regime. This paper focuses on the image classification task,58
although Few-Class Regimecan generalize to object detection and other visual tasks.59
Statement of Contributions. Four contributions are listed below:60
• To be best of our knowledge, we are the first to explore the problems in the Few-Class61
Regime and develop a benchmark toolFew-Class Arena(FCA) to facilitate scientific research,62
analysis, and discovery for this range of classes.63
• We introduce a scalable few-class data loading approach to automatically load images and64
labels in the Few-Class Regime from the full dataset, avoiding the need to duplicate data65
points for every additional few-class subset.66
• We incorporate dataset similarity as an inverse difficulty measurement in Few-Class Arena67
and propose a novel Silhouette-based similarity score namedSimSS. By leveraging the visual68
feature extraction power of CLIP and DINOv2, we show that SimSS is highly correlated69
with ResNet performance in the Few-Class Regimewith Pearson coefficient scores ≥ 0.88.70
• We conduct extensive experiments that comprise ten models on ten datasets and 2-100071
numbers of classes on ImageNet, totalling 1591 training and testing runs. In-depth analyses72
on this large body of testing reveal new insights in the Few-Class Regime.73
2 Related Work74
Visual Datasets and Benchmarks. To advance deep neural network research, a wealth of large-scale75
many-class datasets has been developed for benchmarking visual neural networks over a variety of76
tasks. Typical examples 1 include 1000 classes in ImageNet [1] for image classification, and 80 object77
categories in COCO [2] for object detection. Previous benchmarks also extend vision to multimodal78
research such as image-text [27, 28, 29, 30]. While prior works often scale up the number of object79
categories for general purpose comparison, studies [31, 32] raise a concern on whether models trained80
on datasets with such a large number of classes (e.g. ImageNet) can be reliably transferred to real81
world applications often with far fewer classes. A close work to ours is vision backbone comparison82
[33] whose focus is on model architectures. Our perspective differs in a focus on cases with fewer83
number of classes, which often better aligns with real-world scenarios.84
Dataset Difficulty Measurement. Research has shown the existence of inherent dataset difficulty85
[32] for classification and other analytic tasks. Efficient measurement methods are proposed to86
characterize dataset difficulty using Silhouette Score [ 34], K-means Fréchet inception distance87
[35, 36, 37], and Probe nets [25]. Prior studies have proposed image quality metrics using statistical88
heuristics, including peak signal-to-noise ratio (PSNR) [ 38], structural similarity (SSIM) Index89
[39], and visual information fidelity VIF [40]. A neuroscience-based image difficulty metric [ 32]90
is defined as the minimum viewing time related to object solution time (OST) [ 41]. Another type91
of difficulty measure method consists of additional procedures such as c-score [ 42] , prediction92
depth [43] , and adversarial robustness [44] . Our work aligns with the line of research [45, 46, 47]93
involving similarity-based difficulty measurements: similar images are harder to distinguish from94
each other while dissimilar images are easier. Previous studies are mainly in the image retrieval95
context [48, 49, 50]. Similarity score is used in [51] with the limitation that a model serving similarity96
measurement has to be trained for one dataset. We push beyond this limit by leveraging large vision97
models that learn general visual features using CLIP [52] and DINOv2 [53]. The study [32] shows98
that CLIP generalizes well to both easy and hard images, making it a good candidate for measuring99
1A detailed list of many-class datasets used in this paper can be found in the Appendix.
3

image difficulty. Supported by the evidence that better classifiers can act as better perceptual feature100
extractors [54] , in later sections we show how CLIP and DINOv2 will be used as our similarity base101
function.102
Despite the innovation of difficulty measure algorithms on many-class datasets, little attention has103
been paid to leveraging these methods in the Few-Class Regime. We show that, as the number of104
classes decreases, sub-dataset difficulty in the Few-Class Regimeplays a more critical role in efficient105
model selection. To summarize, unlike previous work on many-class benchmarks and difficulty106
measurements, our work takes few-class and similarity-based dataset difficulty into consideration,107
and in doing so we believe the work pioneers the development of visual benchmark dedicated to108
research in the Few-Class Regime.109
3 Few-Class Arena (FCA)110
We introduce the Few-Class Arena (FCA) benchmark in this section. In practice, we have integrated111
FCA into the MMPreTrain framework [55], implemented in Python3 and Pytorch2.112
3.1 Goals113
1. Generality. All vision models and existing datasets for classification should be compatible in this114
framework. In addition, users can extend to custom models and datasets for their needs.115
2. Efficiency. The benchmark should be time- and space-efficient for users. The experimental setup116
for the few-class benchmark should be easily specified by a few hyper-parameters (e.g. number of117
classes). Since the few-class regime usually includes sub-datasets extracted from the full dataset, the118
benchmark should be able to locate those sub-datasets without generating redundant duplicates for119
reasons of storage efficiency. For time-efficiency, it should conduct training and testing automatically120
through use of user-specified configuration files, without users’ manual execution.121
3. Large-Scale Benchmark. The tool should allow for large-scale benchmarking, including training122
and testing of different vision models on various datasets when the number of classes varies.123
3.2 Few-Class Dataset Preparation124
Few-Class Arenaprovides an easy way to prepare datasets in the Few-Class Regime. By leveraging125
the MMPreTrain framework, users only need to specify the parameters of few-class subsets in the126
configuration files, which includes the list of models, datasets, number of classes ( NCL), and the127
number of seeds ( NS). Few-Class Arena generates the specific model and dataset configuration128
files for each subset, where subset classes are randomly extracted from the full set of classes, as129
specified by the seed number. Note that only one copy of the full, original dataset is maintained during130
the whole benchmarking life cycle because few-class subsets are created through the lightweight131
configurations, thus maximizing storage efficiency. We refer readers to the Appendix and the publicly132
released link for detailed implementations and use instructions.133
3.3 Many-Class Full Dataset Trained Benchmark134
We conducted large-scale experiments spanning ten popular vision models (including CNN and135
ViT architectures) and ten common datasets 3. Except for ImageNet1K, where pre-trained model136
weights are available, we train models in other datasets from scratch. While different models’137
2Code is available at https://github.com/fewclassarena/fca, including detailed documentation and
long-term plans of maintenance.
3Models include: ResNet50 (RN50), VGG16, ConvNeXt V2 (CNv2), Inception V3 (INCv3), EfficientNet
V2 (EFv2), ShuffleNet V2 (SNv2), MobileNet V3 (MNv2), Vision Transformer base (ViTb), Swin Transformer
V2 base (SWv2b) and MobileViT small (MViTs). Datasets include CalTech101 (CT101), CalTech256 (CT256),
CIFAR100 (CF100), CUB200 (CB200), Food101 (FD101), GTSRB43, (GT43), ImageNet1K (IN1K), Indoor67
(ID67), Quickdraw345 (QD345) and Textures47 (TT47).
4

training procedures may incur various levels of complexity (particularly in our case for MobileNet138
V3 and Swin Transformer V2 base), we have endeavored to minimize changes in the existing training139
pipelines from MMPreTrain. The rationale is that if a model exhibits challenges in adapting it to a140
dataset, then it is often not a helpful choice for a practitioner to select for deployment.141
Results are summarized in Table 1. We make several key observations: (1) models in different datasets142
(in rows) yield highly variable levels of performance by Top-1 accuracy; (2) no single best model143
(bold, in columns) exists across all datasets; and (3) model rankings vary across various datasets.144
The first two observations are consistent with the findings in [25, 31]. For (1), it suggests there exists145
underlying dataset-specific difficulty. To capture this characteristic, we adopt the reference dataset146
classification difficulty number (DCN) [25] to refer to the empirically highest accuracy achieved in147
a dataset from a finite number of models shown in Table 1 and Figure 2 (a). For observation (3),148
we can examine the rankings among the ten models of ResNet50 and EfficientNet V2 in Figure 2149
(b). ResNet50’s ranking varies dramatically for the different datasets, for instance ranking 7th on150
ImageNet1K and 1st on Quickdraw345. This ranking variability is also observed in other models151
(see all models in the Appendix). However, a common practice is to benchmark models – even for152
efficiency – on large datasets, especially ImageNet1K. The varied dataset rankings in our experiments153
expose the limitations of such a practice, further supporting our new benchmark paradigm, especially154
in the Few-Class Regime. In later sections, we leverage DCN and image similarity for further analysis.155
Dataset RN50 VGG16 CNv2 INCv3 EFv2 SNv2 MNv3 ViTb SWv2b MViTs DCN
[56] [57] [58] [59] [4] [9] [7] [60] [61] [10] [25]
GT43 [62] 99.85 96.60 99.83 99.78 99.86 99.87 5.98 99.31 99.78 99.69 99.87
CF100 [63] 74.56 71.12 85.89 75.97 77.05 77.89 1.00 32.65 78.49 76.51 85.89
IN1K [1] 76.55 71.62 84.87 77.57 85.01 69.55 67.66 82.37 84.6 78.25 85.01
FD101 [64] 83.76 75.82 63.80 83.96 80.82 79.36 0.99 52.21 84.30 82.23 84.30
CT101 [65] 77.70 74.99 77.52 77.52 77.82 84.13 76.58 59.59 78.82 80.06 84.13
CT256 [66] 65.07 59.08 73.57 66.09 62.80 68.13 22.63 44.23 67.28 65.80 73.57
QD345 [67] 69.14 19.86 62.86 68.25 68.81 67.32 0.72 19.67 66.54 68.76 69.14
CB200 [68] 45.86 21.26 27.61 45.58 44.48 53.95 47.22 23.73 54.52 58.46 58.46
ID67 [69] 53.75 26.01 33.21 45.95 43.85 54.72 49.10 30.51 48.58 54.05 54.72
TT47 [70] 30.43 12.55 6.49 14.20 21.17 43.83 2.18 31.38 33.94 24.41 43.83
Table 1: Top-1 accuracy across ten models in ten datasets. Models are trained and tested on full
datasets with their original number of classes (e.g. 1K from ImageNet1K); this is denoted in the last
few digits of the abbreviation of the dataset name. The best score is highlighted in bold while the
second best is underlined for each dataset.
GT43CF100IN1KFD101CT101CT256QD345CB200ID67 TT47
Dataset
0
20
40
60
80
100T op-1 Acc. (%)
DCN
(a) Top-1 accuracy and DCN in ten full datasets.
IN1KQD345CT256CB200FD101CT101CF100ID67 TT47GT43
1st
2nd
3rd
4th
5th
6th
7th
Rank by T op-1 Acc.
RN50
EFv2 (b) Ranking of ResNet50 (RN50) and Efficient-
Net V2 (EFv2) across 10 datasets by Top-1 acc.
Figure 2: Many-Class Full Dataset Benchmark.
In the next subsections, we introduce three new types of benchmarks: (1) Few-Class, Full Dataset156
Trained Benchmark (FC-Full), which benchmarks vision models trained on the full dataset with the157
original number of classes; (2) Few-Class, Subset Trained Benchmark (FC-Sub), which benchmarks158
vision models trained on subsets of a fewer number of classes than the full dataset, and (3) Few-Class159
Similarity Benchmark (FC-Sim), which benchmarks image similarity methods and their correlation160
with model performance.161
5

3.4 Few-Class Full Dataset Trained Benchmark (FC-Full)162
Traditionally, a large number of models are trained and compared on many-class datasets. However,163
results for such benchmarks are not directly useful to the Few-Class Regime and many real-world164
scenarios. Therefore, we introduce the Few-Class Full Dataset Trained Benchmark (FC-Full), with the165
objective of effortlessly conducting large-scale experiments and analyses in the Few-Class Regime.166
The procedure of FC-Full consists of two main stages. In the first stage, users select the models167
and datasets upon which they would like to conduct experiments. They can choose to download168
pre-trained model weights, which are usually available on popular model hubs (PyTorch Hub [71],169
TensorFlow Hub [72], Hugging Face [73], MMPreTrain [55] etc.). In case of no pre-trained weights170
available from public websites, users can resort to the option of training from scratch. To that end,171
our tool is designed and implemented to generate bash scripts for easily configurable and modifiable172
training through the use of configuration files.173
In the second stage, users conduct benchmarking in the Few-Class Regime. By specifying the list of174
classes, Few-Class Arena automatically loads pre-trained weights of the chosen models and evaluates175
performance of the models on the selected datasets. Note that this process is accomplished through176
configuration files created by the user’s specifications, thus enabling hundreds of experiments to be177
launched by a single command. This dramatically reduces human effort that would otherwise be178
expended to run these experiments without Few-Class Arena.179
3.5 Few-Class Subset Trained Benchmark (FC-Sub)180
Our study in Figure 1 (red lines) reveals the limits of existing pre-trained models in the Few-Class181
Regime. To facilitate further research and analyze the upper bound performance in the Few-Class182
Regime, we introduce the Few-Class Subset Trained Benchmark (FC-Sub).183
FC-Sub follows a similar procedure to FC-Full, except that, when evaluating a model in a subset with184
a specific number of classes, that model should have been trained on that same subset. Specifically, in185
Stage One (described for FC-Full), users specify models, datasets and the list of number of classes in186
configuration files. Then Few-Class Arenagenerates bash scripts for model training on each subset.187
In Stage two, Few-Class Arenatests each model in the same subset that it was trained on.188
3.6 Few-Class Similarity Benchmark (FC-Sim)189
One objective of our tool is to provide the Similarity Benchmark as a platform for researchers to190
design custom similarity scores for efficient comparison of models and datasets.191
The intrinsic image difficulty of a dataset affects a model’s classification performance (and human)192
[74, 75, 32]. We show – as is intuitive – that the more similar two images are, the more difficult it is193
for a vision classifier to make a correct prediction. This suggests that the level of similarity of images194
in a dataset can be used as a proxy for a dataset difficulty measure. In this section, we first adopt and195
provide the basic formulation of similarity, the baseline of a similarity metric. Then we propose a196
Similarity-Based Silhouette Score to capture the characteristic of image similarity in a dataset.197
We first adopt the basic similarity formulation from [51]. Intra-Class Similarity S(C)
α is defined as a198
scalar describing the similarity of images within a class by taking the average of all the distinct class199
pairs in C, while Inter-Class Similarity denotes a scalar describing the similarity among images in200
two different classes C1 and C2. For a dataset D, these are defined as the mean of their similarity201
scores over all classes, respectively:202
S(D)
α = 1
|L|
X
l∈L
S(Cl)
α = 1
|L| × |P(Cl)|
X
l∈L
X
i,j∈Cl; i̸=j
cos(Zi, Zj), (1)
203
S(D)
β = 1
|P(D)|
X
a,b∈L;a̸=b
S(Ca,Cb)
β = 1
|P(D)| × |P(C1,C2)|
X
a,b∈L; a̸=b
X
i∈C1,j∈C2
cos(Zi, Zj). (2)
6

where |L| is the number of classes in a dataset, Zi is the visual feature of an image i, |P(C)| is the204
total number of distinct image pairs in class C, |P(D)| is the total number of distinct class pairs, and205
|P(C1,C2)| is the total number of distinct image pairs excluding same-class pairs.206
Averaging these similarities provides a single scalar score at the class or dataset level. However,207
this simplicity neglects other cluster-related information that can better reveal the underlying dataset208
difficulty property of a dataset. In particular, the (1) tightness of a class cluster and (2) distance to209
other classes of class clusters, are features that characterize the inherent class difficulty, but are not210
captured by Sα or Sβ alone.211
To compensate the aforementioned drawback, we adopt the Silhouette Score (SS) [34, 76]: SS(i) =212
b(i)−a(i)
max(a(i),b(i)) , where SS(i) is the Silhouette Score of the data pointi, a(i) is the average dissimilarity213
between i and other instances in the same class, and b(i) is the average dissimilarity between i and214
other data points in the closest different class.215
Observe that the above Intra-Class Similarity S(C)
α already represents the tightness of the class (C),216
therefore a(i) can be replaced with the inverse of Intra-Class Similarity a(i) = −Sα(i). For the217
second term b(i), we adopt the previously defined Inter-Class SimilarityS(C1,C2)
β and introduce a new218
similarity score as Nearest Inter-Class Similarity S′
β
(C), which is a scalar describing the similarity219
among instances between class C and the closest class of each instance in C. The dataset-level220
Nearest Inter-Class Similarity S′(D)
β is expressed as:221
S′(D)
β = 1
|L|
X
l∈L
S′(Cl, ˆCl)
β = 1
|L| × |P(Cl, ˆCl)|
X
l∈L
X
i∈Cl,j∈ ˆCl
cos(Zi, Zj). (3)
where ˆC is the set of the nearest class to C ( ˆC ̸= C). To summarize, we introduce our novel222
Similarity-Based Silhouette Score SimSS 4:223
SimSS (D) = 1
|L| × |Cl|
X
i∈Cl
Sα(i) − S′β(i)
max(Sα(i), S′β(i)). (4)
4 Experimental Results224
4.1 Results on FC-Full225
In this section, we present the results of FC-Full. A model trained on the dataset with its original226
number of classes (e.g. 1000 in ImageNet1K) is referred to as a full-class model. These experiments227
are designed to understand how full-class model performance changes when the number of classes228
NCl decreases from many to few classes. We analyze the results of DCN-Full, shown in Figure 3229
(details of all models are presented in the Appendix), and we make two key observations when NCl230
reduces to the Few-Class Regime(from right to left). (1) The best performing models do not always231
increase its accuracy for fewer classes, as shown by the solid red lines that represent the average of232
DCN for each NCl. (2) The variance, depicted by the light red areas, of the best models broaden233
dramatically for low NCl, especially for NCl < 10.234
Both observations support evidence of the limitations of using the common many-class benchmark235
for application model selection in the Few-Class Regime, since it is not consistent between datasets236
that a model can be made smaller with higher accuracy. Furthermore, the large variance in accuracy237
means that prediction of performance for few classes is unreliable for this approach.238
4.2 Results on FC-Sub239
In this section, we show how using Few-Class Arenacan help reveal more insights in the Few-Class240
Regime to mitigate the issues of Section 4.1.241
4The extended derivation is detailed in the Appendix.
7

0 50
65
70
75
80
85
90
DCN-Full (%)
CalT ech101
0 200
60
70
80
CalT ech256
0 50
80
85
90
CIFAR100
0 100
50
55
60
65
70
CUB200
0 50
82.5
85.0
87.5
90.0
92.5
95.0
Food101
10 20 30
98.0
98.5
99.0
99.5
100.0DCN-Full (%)
GTSRB43
0 500
75
80
85
90
ImageNet1K
0 50
NCL
45
50
55
60
65
Indoor67
0 200
60
65
70
75
80
85
Quickdraw345
10 20 30
35
40
45
50
T extures47
Figure 3: DCN-Full by Top-1 Accuracy (%). NCl ranges from many to 2.
FC-Sub results are displayed in Figure 4. Recall that a sub-class model is a model trained on a subset242
of the dataset where NCl is smaller than the original number of classes in the full dataset. Observe243
that in the Few-Class Regime (when NCl decreases from 4 to 2) that: (1) DCN increases as shown by244
the solid blue lines, and (2) variance reduces as displayed by the light blue areas.245
The preceding observation for FC-Full 4.1 seems to contradict the common belief that, the fewer the246
classes, the higher is the accuracy that a model can achieve. Conversely, the FC-Sub results do align247
with this belief. We argue that a full-class model needs to accommodate many parameters to learn248
features that will enable high performance across all classes in a many-class, full dataset. With the249
same parameters, however, a sub-class model can adapt to finer and more discriminative features that250
improve its performance when the number of target classes are much smaller.251
2 3 4
70
80
90
100
DCN (%)
CalT ech101
2 3 4
60
70
80
90
100
CalT ech256
2 3 4
80
85
90
95
100
CIFAR100
2 3 4
50
60
70
80
90
100
CUB200
2 3 4
87.5
90.0
92.5
95.0
97.5
Food101
2 3 4
99.0
99.2
99.4
99.6
99.8
100.0DCN (%)
GTSRB43
2 3 4
75
80
85
90
95
100
ImageNet1K
2 3 4
NCL
50
60
70
80
90
Indoor67
2 3 4
60
70
80
90
100
Quickdraw345
2 3 4
40
60
80
T extures47
Figure 4: DCN-Sub (red) and DCN-Full (blue) by Top-1 Accuracy (%). NCL ranges from 2 to 4.
4.3 Results on FC-Sim252
In this section, we analyze the use of SimSS (Equation 4) as proxy for few-class dataset difficulty.253
Experiments are conducted on ImageNet1K using the ResNet family for the lowerNCL ≤ 10% range254
of the original 1000 classes, NCL ∈ {2, 3, 4, 5, 10, 100}, and the results are shown in Figure 5. Each255
datapoint of DCN-Full (diamond in red) or DCN-Sub (square in blue) represents an experiment in a256
8

subset of a specific NCL, where classes are sampled from the full dataset. For reproducible results,257
we use seed numbers from 0 to 4 to generate 5 subsets for one NCL by default. A similarity base258
function (sim()) is defined as the atomic function that takes a pair of images as input and outputs a259
scalar that represents their image similarity.260
In our experiments, we leverage the general visual feature extraction ability of CLIP (image + text)261
[52] and DINOv2 (image) [53] by self-supervised learning. Specifically, a pair of images are fed into262
its latent space from which the the cosine score is calculated and normalized to 0 to 1. Note that we263
only use the Image Encoder in CLIP.264
Comparing Accuracy and Similarity To evaluate SimSS, we compute the Pearson correlation265
coefficient (PCC) (r) between model accuracy and SimSS. Results in Figure 5 (a) (b) show that266
SimSS is poorly correlated with DCN-Full (r = 0.18 and r = 0.26 for CLIP and DINOv2) due to the267
large variance shown in Section 4.1. In contrast, SimSS is highly correlated with DCN-Sub (shown268
in blue squares), with r = 0.90 and r = 0.88 using CLIP (dashed) and DINOv2 (solid), respectively.269
The high PCC [77, 78] demonstrates that SimSS is a reliable metric to estimate few-class dataset270
difficulty, and this can help predict the empirical upper-bound accuracy of a model in the Few-Class271
Regime. Comparison between SimSS and all models can be found in the Appendix. Such a high272
correlation suggests this offers a reliable scaling relationship to estimate model accuracy by similarity273
for other values of NCL without an exhaustive search. Due to the dataset specificity of the dataset274
difficulty property, this score is computed once and used for all times the same dataset is used. We275
have made available difficulty scores for many datasets at the Few-Class Arenasite.276
0.05 0.10 0.15
SimSS[CLIP]
50
60
70
80DCN-Full (%)
r=0.18
p=0.34
(a) DCN-Full
0.1 0.2 0.3
SimSS[DINOv2]
50
60
70
80DCN-Full (%)
r=0.26
p=0.16 (b) DCN-Full
0.05 0.10 0.15
SimSS[CLIP]
90
95
100DCN-Sub (%)
 r=0.90
p=8.8e-12 (c) DCN-Sub
0.1 0.2 0.3
SimSS[DINOv2]
90
95
100DCN-Sub (%)
 r=0.88
p=2.4e-10 (d) DCN-Sub
Figure 5: Pearson correlation coefficient (r) between DCN and SimSS when NCl ∈ {2, 3, 4, 5, 10, 100}. DCN-
Sub (blue squares) is more highly correlated than DCN-Full (red diamonds) with SimSS using both similarity
base functions of CLIP (dashed line) and DINOv2 (solid line) with r ≥ 0.88.
5 Conclusion277
We have proposed Few-Class Arena and a dataset difficulty measurement, which together form278
a benchmark tool to compare and select efficient models in the Few-Class Regime. Extensive279
experiments and analyses over 1500 experiments with 10 models on 10 datasets have helped identify280
new behavior that is specific to the Few-Class Regimeas compared to for many-classes. One finding281
reveals a newnCl-scaling law whereby dataset difficulty must be taken into consideration for accuracy282
prediction. Such a benchmark will be valuable to the community by providing both researchers and283
practitioners with a unified framework for future research and real applications.284
Limitations and Future Work. We note that the convergence of sub-models is contingent on various285
factors in a training scheduler, such as learning rate. A careful tuning of training procedure may286
increase a model’s performance, but it shouldn’t change the classification difficulty number drastically287
since this represents a dataset’s intrinsic difficulty property. The current difficulty benchmark supports288
image similarity while in the future it can be expanded to other difficulty measurements [ 25]. As289
CLIP and DINOv2 are trained toward general visual features, it is unclear if they will be appropriate290
for other types of images such as sketches without textures in Quickdraw [67] . For this reason, a291
universal similarity foundation model would be appealing that applies to any image type. In summary,292
Few-Class Arena identifies a promising new path to achieve efficiencies that are focused on the293
important and practical Few-Class Regime, establishing this as a baseline for future work.294
9

References295
[1] Deng, J., W. Dong, R. Socher, et al. Imagenet: A large-scale hierarchical image database. In296
2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.297
[2] Lin, T.-Y ., M. Maire, S. Belongie, et al. Microsoft coco: Common objects in context. In298
European conference on computer vision, pages 740–755. Springer, 2014.299
[3] Tan, M., Q. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In300
International conference on machine learning, pages 6105–6114. PMLR, 2019.301
[4] Tan, M., Q. L. Efficientnetv2: Smaller models and faster training. In International conference302
on machine learning, pages 10096–10106. PMLR, 2021.303
[5] Sinha, D., M. El-Sharkawy. Thin mobilenet: An enhanced mobilenet architecture. In 2019304
IEEE 10th annual ubiquitous computing, electronics & mobile communication conference305
(UEMCON), pages 0280–0285. IEEE, 2019.306
[6] Sandler, M., A. Howard, M. Zhu, et al. Mobilenetv2: Inverted residuals and linear bottlenecks.307
In Proceedings of the IEEE conference on computer vision and pattern recognition , pages308
4510–4520. 2018.309
[7] Howard, A., M. Sandler, G. Chu, et al. Searching for mobilenetv3. In Proceedings of the310
IEEE/CVF international conference on computer vision, pages 1314–1324. 2019.311
[8] Iandola, F. N., S. Han, M. W. Moskewicz, et al. Squeezenet: Alexnet-level accuracy with 50x312
fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.313
[9] Ma, N., X. Zhang, H.-T. Zheng, et al. Shufflenet v2: Practical guidelines for efficient cnn314
architecture design. In Proceedings of the European conference on computer vision (ECCV),315
pages 116–131. 2018.316
[10] Mehta, S., M. Rastegari. Mobilevit: Light-weight, general-purpose, and mobile-friendly vision317
transformer. arxiv 2021. arXiv preprint arXiv:2110.02178.318
[11] Patterson, D., J. Gonzalez, Q. Le, et al. Carbon emissions and large neural network training.319
arXiv preprint arXiv:2104.10350, 2021.320
[12] Rae, J. W., S. Borgeaud, T. Cai, et al. Scaling language models: Methods, analysis & insights321
from training gopher. arXiv preprint arXiv:2112.11446, 2021.322
[13] Gysel, P., J. Pimentel, M. Motamedi, et al. Ristretto: A framework for empirical study of323
resource-efficient inference in convolutional neural networks. IEEE transactions on neural324
networks and learning systems, 29(11):5784–5789, 2018.325
[14] Han, S., H. Mao, W. J. Dally. Deep compression: Compressing deep neural networks with326
pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.327
[15] Leng, C., Z. Dou, H. Li, et al. Extremely low bit neural network: Squeeze the last bit out with328
admm. In Proceedings of the AAAI conference on artificial intelligence, vol. 32. 2018.329
[16] Cheng, Y ., D. Wang, P. Zhou, et al. A survey of model compression and acceleration for deep330
neural networks. arXiv preprint arXiv:1710.09282, 2017.331
[17] Blalock, D., J. J. Gonzalez Ortiz, J. Frankle, et al. What is the state of neural network pruning?332
Proceedings of machine learning and systems, 2:129–146, 2020.333
[18] Li, H., A. Kadav, I. Durdanovic, et al. Pruning filters for efficient convnets. arXiv preprint334
arXiv:1608.08710, 2016.335
[19] Shen, M., H. Yin, P. Molchanov, et al. Structural pruning via latency-saliency knapsack. arXiv336
preprint arXiv:2210.06659, 2022.337
[20] Yeung, S., O. Russakovsky, G. Mori, et al. End-to-end learning of action detection from frame338
glimpses in videos. In Proceedings of the IEEE conference on computer vision and pattern339
recognition, pages 2678–2687. 2016.340
10

[21] Shao, Z., L. Wang, Z. Wang, et al. Saliency-aware convolution neural network for ship detection341
in surveillance video. IEEE Transactions on Circuits and Systems for Video Technology ,342
30(3):781–794, 2020.343
[22] A. Delplanque, P. L. J. L. J. T., S. Foucher. Multispecies detection and identification of african344
mammals in aerial imagery using convolutional neural networks. Remote Sensing in Ecology345
and Conservation, 8(April):166–179, 2022.346
[23] Cai, Y ., T. Luan, H. Gao, et al. Yolov4-5d: An effective and efficient object detector for347
autonomous driving. IEEE Transactions on Instrumentation and Measurement, 70:1–13, 2021.348
[24] Scheidegger, F., L. Benini, C. Bekas, et al. Constrained deep neural network architecture search349
for iot devices accounting for hardware calibration. Advances in Neural Information Processing350
Systems, 32, 2019.351
[25] Scheidegger, F., R. Istrate, G. Mariani, et al. Efficient image dataset classification difficulty352
estimation for predicting deep-learning accuracy. The Visual Computer, 37(6):1593–1610, 2021.353
[26] Kaplan, J., S. McCandlish, T. Henighan, et al. Scaling laws for neural language models. arXiv354
preprint arXiv:2001.08361, 2020.355
[27] Lee, T., M. Yasunaga, C. Meng, et al. Holistic evaluation of text-to-image models. Advances in356
Neural Information Processing Systems, 36, 2024.357
[28] Le, T., V . Lal, P. Howard. Coco-counterfactuals: Automatically constructed counterfactual358
examples for image-text pairs. Advances in Neural Information Processing Systems, 36, 2024.359
[29] Laurençon, H., L. Saulnier, L. Tronchon, et al. Obelics: An open web-scale filtered dataset of360
interleaved image-text documents. Advances in Neural Information Processing Systems, 36,361
2024.362
[30] Bitton, Y ., N. Bitton Guetta, R. Yosef, et al. Winogavil: Gamified association benchmark to363
challenge vision-and-language models. Advances in Neural Information Processing Systems,364
35:26549–26564, 2022.365
[31] Fang, A., S. Kornblith, L. Schmidt. Does progress on imagenet transfer to real-world datasets?366
Advances in Neural Information Processing Systems, 36, 2024.367
[32] Mayo, D., J. Cummings, X. Lin, et al. How hard are computer vision datasets? calibrating dataset368
difficulty to viewing time. Advances in Neural Information Processing Systems , 36:11008–369
11036, 2023.370
[33] Goldblum, M., H. Souri, R. Ni, et al. Battle of the backbones: A large-scale comparison of371
pretrained models across computer vision tasks. Advances in Neural Information Processing372
Systems, 36, 2024.373
[34] Rousseeuw, P. J. Silhouettes: a graphical aid to the interpretation and validation of cluster374
analysis. Journal of computational and applied mathematics, 20:53–65, 1987.375
[35] Dowson, D., B. Landau. The fréchet distance between multivariate normal distributions.Journal376
of multivariate analysis, 12(3):450–455, 1982.377
[36] Heusel, M., H. Ramsauer, T. Unterthiner, et al. Gans trained by a two time-scale update rule378
converge to a local nash equilibrium. Advances in neural information processing systems, 30,379
2017.380
[37] Lucic, M., K. Kurach, M. Michalski, et al. Are gans created equal? a large-scale study.Advances381
in neural information processing systems, 31, 2018.382
[38] Hore, A., D. Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference383
on pattern recognition, pages 2366–2369. IEEE, 2010.384
[39] Wang, Z., A. C. Bovik, H. R. Sheikh, et al. Image quality assessment: from error visibility to385
structural similarity. IEEE transactions on image processing, 13(4):600–612, 2004.386
[40] Sheikh, H. R., A. C. Bovik. Image information and visual quality. IEEE Transactions on image387
processing, 15(2):430–444, 2006.388
11

[41] Kar, K., J. Kubilius, K. Schmidt, et al. Evidence that recurrent circuits are critical to the ventral389
stream’s execution of core object recognition behavior. Nature neuroscience, 22(6):974–983,390
2019.391
[42] Jiang, Z., C. Zhang, K. Talwar, et al. Characterizing structural regularities of labeled data in392
overparameterized models. arXiv preprint arXiv:2002.03206, 2020.393
[43] Baldock, R., H. Maennel, B. Neyshabur. Deep learning through the lens of example difficulty.394
Advances in Neural Information Processing Systems, 34:10876–10889, 2021.395
[44] Goodfellow, I. J., J. Shlens, C. Szegedy. Explaining and harnessing adversarial examples. arXiv396
preprint arXiv:1412.6572, 2014.397
[45] Arun, S. Turning visual search time on its head. Vision Research, 74:86–92, 2012.398
[46] Trick, L. M., J. T. Enns. Lifespan changes in attention: The visual search task. Cognitive399
Development, 13(3):369–386, 1998.400
[47] Wolfe, J. M., E. M. Palmer, T. S. Horowitz. Reaction time distributions constrain models of401
visual search. Vision research, 50(14):1304–1311, 2010.402
[48] Zhang, D., G. Lu. Evaluation of similarity measurement for image retrieval. In International403
conference on neural networks and signal processing, 2003. proceedings of the 2003, vol. 2,404
pages 928–931. IEEE, 2003.405
[49] Wang, J., Y . Song, T. Leung, et al. Learning fine-grained image similarity with deep ranking.406
In Proceedings of the IEEE conference on computer vision and pattern recognition , pages407
1386–1393. 2014.408
[50] Tudor Ionescu, R., B. Alexe, M. Leordeanu, et al. How hard can it be? estimating the difficulty409
of visual search in an image. In Proceedings of the IEEE Conference on Computer Vision and410
Pattern Recognition, pages 2157–2166. 2016.411
[51] Cao, B. B., L. O’Gorman, M. Coss, et al. Data-side efficiencies for lightweight convolutional412
neural networks. arXiv preprint arXiv:2308.13057, 2023.413
[52] Radford, A., J. W. Kim, C. Hallacy, et al. Learning transferable visual models from natural414
language supervision. In International conference on machine learning , pages 8748–8763.415
PMLR, 2021.416
[53] Oquab, M., T. Darcet, T. Moutakanni, et al. Dinov2: Learning robust visual features without417
supervision. arXiv preprint arXiv:2304.07193, 2023.418
[54] Kumar, M., N. Houlsby, N. Kalchbrenner, et al. Do better imagenet classifiers assess perceptual419
similarity better? arXiv preprint arXiv:2203.04946, 2022.420
[55] Contributors, M. Openmmlab’s pre-training toolbox and benchmark. https://github.com/421
open-mmlab/mmpretrain, 2023.422
[56] He, K., X. Zhang, S. Ren, et al. Deep residual learning for image recognition. In Proceedings423
of the IEEE conference on computer vision and pattern recognition, pages 770–778. 2016.424
[57] Simonyan, K., A. Zisserman. Very deep convolutional networks for large-scale image recogni-425
tion. arXiv preprint arXiv:1409.1556, 2014.426
[58] Woo, S., S. Debnath, R. Hu, et al. Convnext v2: Co-designing and scaling convnets with masked427
autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern428
Recognition, pages 16133–16142. 2023.429
[59] Szegedy, C., V . Vanhoucke, S. Ioffe, et al. Rethinking the inception architecture for computer430
vision. In Proceedings of the IEEE conference on computer vision and pattern recognition ,431
pages 2818–2826. 2016.432
[60] Dosovitskiy, A., L. Beyer, A. Kolesnikov, et al. An image is worth 16x16 words: Transformers433
for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.434
12

[61] Liu, Z., H. Hu, Y . Lin, et al. Swin transformer v2: Scaling up capacity and resolution. In435
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages436
12009–12019. 2022.437
[62] Stallkamp, J., M. Schlipsing, J. Salmen, et al. Man vs. computer: Benchmarking machine438
learning algorithms for traffic sign recognition. Neural Networks, (0):–, 2012.439
[63] Krizhevsky, A., G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.440
[64] Bossard, L., M. Guillaumin, L. Van Gool. Food-101 – mining discriminative components with441
random forests. In European Conference on Computer Vision. 2014.442
[65] Li, F.-F., M. Andreeto, M. Ranzato, et al. Caltech 101, 2022.443
[66] Griffin, G., A. Holub, P. Perona. Caltech 256, 2022.444
[67] Ha, D., D. Eck. A neural representation of sketch drawings. arXiv preprint arXiv:1704.03477,445
2017.446
[68] Wah, C., S. Branson, P. Welinder, et al. The Caltech-UCSD Birds-200-2011 Dataset. 2011.447
[69] Quattoni, A., A. Torralba. Recognizing indoor scenes. In 2009 IEEE conference on computer448
vision and pattern recognition, pages 413–420. IEEE, 2009.449
[70] Cimpoi, M., S. Maji, I. Kokkinos, et al. Describing textures in the wild. In Proceedings of the450
IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2014.451
[71] Foundation, T. L. Pytorch hub. https://pytorch.org/hub, 2024. Accessed on 2024-06-04.452
[72] Inc., G. Tensorflow hub. https://www.tensorflow.org/hub, 2024. Accessed on 2024-06-453
04.454
[73] Face, H. Hugging face models. https://huggingface.co/models, 2024. Accessed on455
2024-06-04.456
[74] Geirhos, R., D. H. Janssen, H. H. Sch ¨"utt, et al. Comparing deep neural networks against457
humans: object recognition when the signal gets weaker. arXiv preprint arXiv:1706.06969,458
2017.459
[75] Rajalingham, R., E. B. Issa, P. Bashivan, et al. Large-scale, high-resolution comparison of the460
core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial461
neural networks. Journal of Neuroscience, 38(33):7255–7269, 2018.462
[76] Shahapure, K. R., C. Nicholas. Cluster quality analysis using silhouette score. In 2020 IEEE463
7th international conference on data science and advanced analytics (DSAA), pages 747–748.464
IEEE, 2020.465
[77] Wicklin, R. Weak or strong? how to interpret a spearman or kendall correlation. https:466
//blogs.sas.com/content/iml/2023/04/05/interpret-spearman-kendall-corr.467
html, 2024. Accessed on 2024-06-04.468
[78] Schober, P., C. Boer, L. A. Schwarte. Correlation coefficients: appropriate use and interpretation.469
Anesthesia & analgesia, 126(5):1763–1768, 2018.470
13