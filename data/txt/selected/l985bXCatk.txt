LRVS-Fashion: Extending Visual Search with
Referring Instructions
Simon Lepage1, 2 Jérémie Mary1 David Picard2
1 CRITEO AI Lab, Paris, France
2 LIGM, École des Ponts, Marne-la-Vallée, France
{s.lepage, j.mary}@criteo.com david.picard@enpc.fr
Abstract
This paper introduces a new challenge for image similarity search in the context of1
fashion, addressing the inherent ambiguity in this domain stemming from complex2
images. We present Referred Visual Search (RVS), a task allowing users to deﬁne3
more precisely the desired similarity, following recent interest in the industry. We4
release a new large public dataset, LRVS-Fashion, consisting of 272k fashion5
products with 842k images extracted from fashion catalogs, designed explicitly6
for this task. However, unlike traditional visual search methods in the industry,7
we demonstrate that superior performance can be achieved by bypassing explicit8
object detection and adopting weakly-supervised conditional contrastive learning9
on image tuples. Our method is lightweight and demonstrates robustness, reaching10
Recall at one superior to strong detection-based baselines against 2M distractors.111
Retrieved Items
Query 
Categorical ConditioningLOWER BODY OUTWEAR
Textual Conditioning"Same handbag" "I want her t-shirt"
Figure 1: Overview of the Referred Visual Search task. Given a query image and conditioning
information, the goal is to retrieve a target instance from a large gallery.Note that a query is made of
an image and an additional text or category, precising what aspect of the image is relevant.
1 Introduction12
Image embeddings generated by deep neural networks play a crucial role in a wide range of computer13
vision tasks. Image retrieval has gained substantial prominence, leading to the development of14
dedicated vector database systems [22]. These systems facilitate efﬁcient retrieval by comparing15
embedding values and identifying the most similar images within the database.16
Image similarity search in the context of fashion presents a unique challenge due to the inherently17
ill-founded nature of the problem. The primary issue arises from the fact that two images can be18
considered similar in various ways, leading to ambiguity in deﬁning a single similarity metric. For19
1The dataset is available athttps://huggingface.co/datasets/Slep/LAION-RVS-Fashion
Submitted to the 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets
and Benchmarks. Do not distribute.

instance, two images of clothing items may be deemed similar based on their color, pattern, style, or20
even the model pictured. This multifaceted nature of similarity in fashion images complicates the21
task of developing a universally applicable similarity search algorithm, as it must account for the22
various ways in which images can be related.23
An intuitive approach is to request users furnish supplementary information delineating their interests,24
such as providing an image of an individual and denoting interest in the hat (see Fig. 1). Numerous25
industry leaders including Google, Amazon, and Pinterest have adopted this tactic, however academic26
discourse on potential alternative methodologies for this task remains scarce as the domain lacks27
dedicated datasets. For convenience, we propose terming this task Referred Visual Search (RVS),28
as it is likely to garner attention from the computer vision community due to the utility for product29
search in extensive catalogs.30
In practice, object selection in complex scenes is classically tackled using object detection and31
crops [21, 17, 12, 42]. Some recent approaches use categorical attributes [8] or text instead [6], and32
automatically crop the image based on learned attention to input attributes. It is also possible to ask33
the user to perform the crop himself, yet in all the situations the performance of the retrieval will be34
sensitive to this extraction step making it costly to build a generic retrieval tool. Recently, Jiao et al.35
[20] went a step further, incorporating prior knowledge about the taxonomy of fashion attributes and36
classes without using crops. They use a multi-granularity loss and two sub-networks to learn attribute37
and class-speciﬁc representations, resulting in improved robustness for fashion retrieval, yet without38
providing any code.39
In this work, we seek to support these efforts by providing a dataset dedicated to RVS. We extracted40
a subset of LAION 5B [41] focused on pairs of images sharing a labeled similarity in the domain of41
fashion, and propose a method to eliminate the need for explicit detection or segmentation, while still42
producing similarities in the embedding space speciﬁc to the conditioning. We think that such end-to-43
end approach has the potential to be more generalizable and robust, whereas localization-dependent44
approaches hinge on multi-stage processing heuristics speciﬁc to the dataset.45
This paper presents two contributions to the emerging ﬁeld of Referred Visual Search, aiming at46
deﬁning image similarity based on conditioning information.47
X The introduction of a new dataset, referred to as LRVS-Fashion, which is derived from the48
LAION-5B dataset and comprises 272k fashion products with nearly 842k images. This dataset49
features a test set with an addition of more than 2M distractors, enabling the evaluation of method50
robustness in relation to gallery size. The dataset’s pairs and additional metadata are designed to51
necessitate the extraction of particular features from complex images.52
X An innovative method for learning to extract referred embeddings using weakly-supervised53
training. Our approach demonstrates superior accuracy against a strong detection-based baseline54
and existing published work. Furthermore, our method exhibits robustness against a large number55
of distractors, maintaining high R @ 1 even when increasing the number of distractors to 2M.56
2 Related Work57
Retrieval Datasets. Standard datasets in metric learning literature consider that the images are58
object-centric, and focus on single salient objects [49, 25, 45]. In the fashion domain there exist59
multiple datasets dedicated to product retrieval, with paired images depicting the same product and60
additional labeled attributes. A recurrent focus of such datasets is cross-domain retrieval, where the61
goal is to retrieve images of a given product taken in different situations, for exemple consumer-to-62
shop [31, 50, 32, 12], or studio-to-shop [32, 27]. The domain gap is in itself a challenge, with issues63
stemming from irregular lighting, occlusions, viewpoints, or distracting backgrounds. However, the64
query domain (consumer images for exemple) often contains scenes with multiple objects, making65
queries ambiguous. This issue has been circumvented with the use of object detectors and landmarks66
detectors [23, 18, 32, 12]. Some are not accessible anymore [23, 32, 50].67
With more than 272k distinct training product identities captured in multi-instance scenes, our new68
dataset proposes an exact matching task similar to the private Zalando dataset [27], while being larger69
than existing fashion retrieval datasets and publicly available. We also create an opportunity for new70
multi-modal approaches, with captions referring to the product of interest in each complex image,71
and for robustness to gallery size with 2M added distractors at test time.72
2

Instance Retrieval. In the last decade, content-based image retrieval has changed because of the73
arrival of deep learning, which replaced many handcrafted heuristics (keypoint extraction, descriptors,74
geometric matching, re-ranking. . . ) [11]. In the industry this technology has been of interest to retail75
companies and search engines to develop visual search solutions, with new challenges stemming from76
the large scale of such databases. Initially using generic pretrained backbones to extract embeddings77
with minimal retraining [53], methods have evolved toward domain-speciﬁc embeddings supervised78
by semantic labels, and then multi-task domain-speciﬁc embeddings, leveraging additional product79
informations [58, 3, 46]. The latest developments in the ﬁeld incorporate multi-modal features for80
text-image matching [59, 54, 62], with speciﬁc vision-language pretext tasks.81
However, these methods often consider that the query image is unambiguous, and often rely on a82
region proposal system to crop the initial image [21, 60, 17, 42, 3, 10]. In our work, we bypass this83
step and propose an end-to-end framework, leveraging the Transformer architecture to implicitly84
perform this detection step conditionally to the referring information.85
Referring Tasks. Referring tasks are popular in vision-language processing, in particular Referring86
Expression Comprehension and Segmentation where a sentence designates an object in a scene,87
that the network has to localize. For the comprehension task (similar to open-vocabulary object88
detection) the goal is to output a bounding box [34, 56, 57, 30]. The segmentation task aims at89
producing an instance mask for images [61, 34, 19, 7, 24] and recently videos [52, 4]. In this paper,90
we propose a referring expression task, where the goal is to embed the designated object of interest91
into a representation that can be used for retrieval. We explore the use of Grounding DINO [30] and92
Segment Anything [24] to create a strong baseline on our task.93
Conditional Embeddings. Conditional similarity search has been studied through the retrieval94
process and the embedding process. On one hand, for the retrieval process, Hamilton et al.[15]95
propose to use a dynamically pruned random projection tree. On the other hand, previous work96
in conditional visual similarity learning focused on attribute-speciﬁc retrieval, deﬁning different97
similarity spaces depending on chosen discriminative attributes [47, 36]. They use either a mask98
applied on the features [47], or different projection heads [36], and require extensive data labeling.99
In Fashion, ASEN [35] uses spatial and channel attention to an attribute embedding to extract speciﬁc100
features in a global branch. Dong et al.[8] and Das et al.[6] build upon this model and add a local101
branch working on an attention-based crop. Recently, Jiao et al.[20] incorporated prior knowledge102
about fashion taxonomy in this process to create class-conditional embeddings based on known103
ﬁne-grained attributes, using multiple attribute-conditional attention modules. In a different domain,104
Asai et al.[1] tackle a conditional document retrieval task, where the user intent is made explicit by105
concatenating instructions to the query documents. In our work, we use Vision Transformers [9] to106
implicitly pool features depending on the conditioning information, without relying on explicit ROI107
cropping or labeled ﬁne-grained attributes.108
Composed Image Retrieval (CIR) [48] is another retrieval task where the embedding of an image must109
be modiﬁed following a given instruction. Recent methods use a composer network after embedding110
the image and the modifying text [28, 5, 2]. While CIR shares similarities with RVS in terms of inputs111
and outputs, it differs conceptually. Our task focuses on retrieving items based on depicted attributes112
and specifying a similarity computation method, rather than modifying the image. In Fashion, CIR113
has been extended to dialog-based interactive retrieval, where an image query is iteratively reﬁned114
following user instructions [14, 51, 55, 16].115
3 Dataset116
Metric learning methods work by extracting features that pull together images labeled as similar [11].117
In our case, we wanted to create a dataset where this embedding has to focus on a speciﬁc object118
in a scene to succeed. We found such images in fashion, thanks to a standard practice in this ﬁeld119
consisting in taking pictures of the products alone on neutral backgrounds, and worn by models in120
scenes involving other clothing items (see Fig. 3).121
We created LAION-RVS-Fashion (abbreviated LRVS-F) from LAION-5B by collecting images of122
products isolated and in context, which we respectively callsimple and complex. We grouped them123
using extracted product identiﬁers. We also gathered and created a set of metadata to be used as124
3

b
a
Select DomainsExtract IDs Generate Metadata Filter
c
Classifiers
Captioner
d
Figure 2: Overview of the data collection.a) Selection of a subset of domains belonging to known
fashion retailers. b) Extraction of product identiﬁers in the URLs using domain-speciﬁc regular
expressions. c) Generation of synthetic metadata for the products (categories, captions, ...) using both
pretrained and ﬁnetuned models.d) Deduplication of the images, and assignment to subsets.
{
 Images:
 Category: Lower Body  LAION Alt Text: Michael Kors 'Samantha' skinny trousers Caption: A women's beige trousers }
{
 Images:
 Category: Bags  LAION Alt Text: BARK - striped tote 7 Caption: A handbag with navy stripes}
Figure 3: Samples from LRVS-F. Each product is represented on at least a simple and a complex
image, and is associated with a category. The simple images are also described by captions from
LAION and BLIP2. Please refer to Appendix A.1 for more samples.
referring information, namely LAION captions, generated captions, and generated item categories.125
The process is depicted Fig. 2, presented in Section 3.1 with additional details in Appendix A.3.126
3.1 Construction127
Image Collection. The URLs in LRVS-F are a subset of LAION-5B, curated from content delivery128
networks of fashion brands and retailers. By analyzing the URL structures we identiﬁed product129
identiﬁers, which we extracted with regular expressions to recreate groups of images depicting the130
same product. URLs without distinct identiﬁers or group membership were retained as distractors.131
Annotations. We generated synthetic labels for the image complexity, the category of the product,132
and added new captions to replace the noisy LAION alt-texts. For the complexity labels, we133
employed active learning to incrementally train a classiﬁer to discern between isolated objects on134
neutral backdrops and photoshoot scenes. The product categories were formed by aggregating various135
ﬁne-grained apparel items into 10 coarse groupings. This categorization followed the same active136
learning protocol. Furthermore, the original LAION captions exhibited excessive noise, including137
partial translations or raw product identiﬁers. Therefore, we utilized BLIP-2 [29] to generate new,138
more descriptive captions.139
Dataset Split. We grouped together images associated to the same product identiﬁer and dropped140
the groups that did not have at least a simple and a complex image. We manually selected 400 of141
them for the validation set, and 2,000 for the test set. The distractors are all the images downloaded142
previously that were labeled as "simple" but not used in product groups. This mostly includes images143
for which it was impossible to extract any product identiﬁer.144
Dataset Cleaning. In order to mitigate false negatives in our results, we utilized Locality Sensitive145
Hashing and OpenCLIP ViT-B/16 embeddings to eliminate duplicates. Speciﬁcally, we removed146
duplicates between the test targets and test distractors, as well as between the validation targets and147
validation distractors. Throughout our experiments, we did not observe any false negatives in the148
results. However, there remains a small quantity of near-duplicates among the distractor images.149
4

3.2 Composition150
In total, we extracted 272,451 products for training, represented in 841,718 images. This represents151
581,526 potential simple/complex positive pairs. We additionally extracted 400 products (800 images)152
to create a validation set, and 2,000 products (4,000 images) for a test set. We added 99,541 simple153
images in the validation gallery as distractors, and 2,000,014 in the test gallery.154
We randomly sampled images and manually veriﬁed the quality of the labels. For the complexity155
labels, we measured an empirical error rate of1/1000 on the training set and3/1000 for the distractors.156
For the product categories, we measured a global empirical error rate of1%, with confusions mostly157
arising from semantically similar categories and images where object scale was ambiguous in isolated158
settings (e.g. long shirt vs. short dress, wristband vs. hairband). The BLIP2 captions we provided159
exhibit good quality, increasing the mean CLIP similarity with the image by+7.4%. However, as160
synthetic captions, they are not perfect and may contain occasional hallucinations.161
Please refer to Appendix A.4 for metadata details, A.5 for considerations regarding privacy and biases162
and C for metadata details and a datasheet [13].163
3.3 Benchmark164
We deﬁne a benchmark on LRVS-F to evaluate different methods on a held-out test set with a large165
number of distractors. The test set contains 2,000 unseen products, and up to 2M distractors. Each166
product in the set is represented by a pair of images - a simple one and a complex one. The objective167
of the retrieval task is to retrieve the simple image of each product from among a vast number of168
distractors and other simple test images, given the complex image and conditioning information.169
For this dataset, we propose to frame the benchmark as an asymmetric task : the representation of170
simple images (the gallery) should not be computed conditionally. This choice is motivated by three171
reasons. First, when using precise free-form conditioning (such as LAION texts, which contain172
hashed product identiﬁers and product names) a symmetric encoding would enable a retrieval based173
solely on this information, completely disregarding the image query. Second, for discrete (categorical)174
conditioning it allows the presence of items of unknown category in the gallery, which is a situation175
that may occur in distractors. Third, these images only depict a single object, thus making referring176
information unnecessary. A similar setting is used by Asai et al. [1].177
Additionally, we provide a list of subsets sampled with replacement to be used for boostrapped178
estimation of conﬁdence intervals on the metrics. We created 10 subsets of 1000 test products, and179
10 subsets of 10K, 100K and 1M distractors. We also propose a validation set of 400 products with180
nearly 100K other distractors to monitor the training and for hyperparameter search.181
4 Conditional Embedding182
Task Formulation. Let xq be a query image containing several objects of interest (e.g., a person183
wearing many different clothes and items), andcq the associated referring information that provides184
cues about what aspect ofxq is relevant for the query (e.g., a text describing which garment is of185
interest, or directly the class of the garment of interest). Similarly, letxt be a target image, described186
by the latent informationct. The probability ofxt to be relevant for the queryxq is given by the187
conditional probabilityP (xt,c t|xq,c q). When working with categories forcq and ct, a ﬁltering188
strategy consists in assuming independence between the images and their category,189
P (xt,c t|xq,c q)= P (xt|xq)P (ct|cq) , (1)
and further assuming that categories are uncorrelated (i.e., P (ct|cq)=  cq=ct with   the Dirac190
distribution). In this work, we remove those assumptions and instead assume thatP (xt,c t|xq,c q)191
can be directly inferred by a deep neural network model. More speciﬁcally, we propose to learn a192
ﬂexible embedding function  such that193
h (xq,c q),  (xt,c t)i/ P (xt,c t|xq,c q) . (2)
Our approach offers a signiﬁcant advantage by allowing the ﬂexibility to change the conditioning194
information (cq) at query time, resulting in a different representation that focuses on different aspects195
of the image. It is alsoweakly supervisedin the sense that the referring informationcq is not required196
5

Shared Weights
 Cosine Similarity Matrix
Figure 4: Overview of our method on LRVS-F. For each element in a batch, we embed the scene
conditionally and the isolated item unconditionally. We optimize an InfoNCE loss over the cosine
similarity matrix.  denotes concatenation to the patch sequence.
to provide localized information about the content of interest (like a bounding box) and can be as197
imprecise as a free-form text, as shown in Fig. 1.198
Method: We implement  by modifying the Vision Transformer (ViT) architecture [9]. The condi-199
tioning is an additional input token with an associated learnable positional encoding, concatenated200
to the sequence of image patches. The content of this token can either be learned directly (e.g. for201
discrete categorical conditioning), or be generated by another network (e.g. for textual conditioning).202
At the end of the network, we linearly project the[CLS] token to map the features to a metric203
space. We experimented with concatenating at different layers in the transformer, and found that204
concatenating before the ﬁrst layer is the most sensible choice (see Appendix B.1).205
We train the network with the InfoNCE loss [44, 38], following CLIP [40], which is detailed in the206
next paragraph. However, we hypothesize that even though our method relies on a contrastive loss,207
it does not explicitly require a speciﬁc formulation of it. We choose the InfoNCE loss because of208
its popularity and scalability. During training, given a batch ofN pairs of images and conditioning209
((xA
i ,c A
i ); (xB
i ,c B
i ))i=1..N , we compute their conditional embeddings(zA
i ,z B
i )i=1..N with z =210
 (x, c) 2 Rd. We compute a similarity matrixS where Sij = s(zA
i ,z B
j ), withs the cosine similarity.211
We then optimize the similarity of the correct pair with a cross-entropy loss, effectively considering212
the N   1 other products in the batch as negatives:213
l(S)=   1
N
NX
i=1
log exp(Sii⌧)PN
j=1 exp(Sij⌧)
, (3)
with ⌧ a learned temperature parameter, and the ﬁnal loss isL = l(S)/2+ l(S>)/2. Please refer to214
Fig. 4 for an overview of the method. The⌧ parameter is used to follow the initial formulation of215
CLIP [40] and is optimized by gradient during the training. At test time, we use FAISS [22] to create216
a unique index for the entire gallery and perform fast similarity search on GPUs.217
5 Experiments218
We compare our method to various baselines on LRVS-F, using both category- and caption-based219
settings. We report implementation details before analyzing the results.220
5.1 Implementation details221
All our models take as input images of size224 ⇥ 224, and output an embedding vector of 512222
dimensions. We use CLIP weights as initialization, and then train our models for 30 epochs with223
AdamW [33] and a maximum learning rate of10 5 determined by a learning rate range test [43]. To224
avoid distorting pretrained features [26], we start by only training the ﬁnal projection and new input225
6

embeddings (conditioning and positional) for a single epoch, with a linear warm-up schedule. We226
then train all parameters for the rest of the epochs with a cosine schedule.227
We pad the images to a square with white pixels, before resizing the largest side to 224 pixels. During228
training, we apply random horizontal ﬂip, and random resized crops covering at least 80% of the229
image area. We evaluate the Recall at 1 (R @ 1) of the model on the validation set at each epoch, and230
report test metrics (recall and categorical accuracy) for the best performing validation checkpoint.231
We used mixed precision and sharded loss to run our experiments on multiple GPUs. B/32 models232
were trained for 6 hours on 2 V100 GPUs, with a total batch size of 360. B/16 were trained for 9233
hours on 12 V100, with a batch size of 420. Batch sizes were chosen to maximize GPU memory use.234
5.2 Results235
Detection-based Baseline We leveraged the recent Grounding DINO [30] and Segment Anything236
[24] to create a baseline approach based on object detection and segmentation. In this setting, we237
feed the model the query image and conditioning information, which can be either the name of the238
category or a caption. Subsequently, we use the output crops or masks to train a ViT following the239
aforementioned procedure. Please refer to Tab. 1 for the results.240
Initial experiments conducted with pretrained CLIP features showed a slight preference toward241
segmenting the object. However, training the image encoder revealed that superior performances242
can be attained by training the network on crops. Our supposition is that segmentation errors lead to243
deﬁnitive loss of information, whereas the network’s capacity is sufﬁcient for it to learn to disregard244
irrelevant information and recover from a badly cropped image.245
Overall, using Grounding DINO makes for a strong baseline. However, it is worth highlighting that246
the inherent imprecision of category names frequently results in overly large bounding boxes, which247
in turn limits the performances of the models. Indeed, adding more information into the dataset such248
as bounding boxes with precise categories would help, yet this would compromise the scalability249
of the model as such data is costly to obtain. Conversely, the more precise boxes produced by the250
caption-based model reach67.8%R @ 1 against 2M distractors.251
Table 1: Comparisons of results on LRVS-F for localization-based models. For 0, 10K, 100K and
1M distractors, we report bootstrapped means and standards deviations estimated from 10 randomly
sampled sets. We observe superior performances from the caption-based models, due to the precision
of the caption which leads to better detections.
Distractors! +10K +100K +1M +2M
Condi. PreprocessingEmbedding%R@1 %Cat@1%R@1 %Cat@1%R@1 %Cat@1%R@1 %Cat@1
Category
Gr. DINO-T + SAM-BCLIP ViT-B/3216.9±1.4567.4±1.70 8.9±0.79 65.6±1.93 4.4±0.44 64.5±1.48 2.9 64.0Gr DINO-T + SAM-BViT-B/3283.0±1.0694.6±0.75 69.4±1.3692.0±0.67 53.1±1.6390.0±0.77 46.4 89.2Gr. DINO-T ViT-B/3288.7±0.7496.4±0.55 77.0±1.7994.3±0.82 62.8±1.9292.2±1.26 56.0 91.8Gr. DINO-B ViT-B/1689.9±0.8796.2±0.77 80.8±1.3594.5±0.73 68.8±2.1793.2±0.90 62.9 92.5
Caption
Gr. DINO-T + SAM-BCLIP ViT-B/3227.3±1.2972.9±1.68 16.3±0.8671.1±1.17 9.1±0.73 70.1±1.56 6.2 69.8Gr. DINO-T + SAM-BViT-B/3283.5±1.5694.6±0.39 72.2±1.5993.0±0.42 56.5±1.6190.9±0.74 50.8 90.2Gr. DINO-T ViT-B/3289.7±0.7696.7±0.74 79.0±0.8295.1±0.74 65.4±2.0393.1±1.14 59.0 92.0Gr. DINO-B ViT-B/1691.6±0.7797.6±0.31 83.6±0.9396.1±0.60 73.6±1.4994.7±0.64 67.8 94.3
Categorical Conditioning We compare our method with categorical detection-based approaches,252
and unconditional ViTs ﬁnetuned on our dataset. To account for the extra conditioning information253
used in our method, we evaluated the latter on ﬁltered indexes, with only products belonging to the254
correct category. We did not try to predict the item of interest from the input picture, and instead255
consider it as a part of the query. We also report unﬁltered metrics for reference. Results are in Tab. 2.256
Training the ViTs on our dataset greatly improves their performances, both in terms of R @ 1 and257
categorical accuracy. Filtering the gallery brings a modest mean gain of2   4%R @ 1 across all258
quantities of distractors (Fig. 4b), reaching62.4%R @ 1 for 2M distractors with a ViT-B/16 architecture.259
In practice, this approach is impractical as it necessitates computing and storing an index for each260
category to guarantee a consistent quantity of retrieved items. Moreover, a qualitative evaluation of261
the ﬁltered results reveals undesirable behaviors. When ﬁltering on a category divergent from the262
network’s intrinsic focus, we observe the results displaying colors and textures associated with the263
automatically focused object rather than the requested one.264
7

Table 2: Comparisons of results on LRVS-F for unconditional, category-based and caption-based
models. For 0, 10K, 100K and 1M distractors, we report bootstrapped means and standards deviations
from 10 randomly sampled sets. Our CondViT-B/16 outperforms other methods for both groups.
Distractors! +10K +100K +1M +2M
Model %R@1 %Cat@1 %R@1 %Cat@1 %R@1 %Cat@1 %R@1 %Cat@1
ViT-B/32 85.6±1.08 93.7±0.31 73.4±1.35 90.9±0.78 58.5±1.37 87.8±0.86 51.7 86.9
ViT-B/16 88.4±0.88 94.8±0.52 79.0±1.02 92.3±0.73 66.1±1.21 90.2±0.92 59.4 88.8
ASENg [8] 63.1±1.50 76.3±1.26 46.1±1.21 68.5±0.84 29.8±1.86 62.9±1.27 24.1 62.0
ViT-B/32 + Filt. 88.9±1.01 — 76.8±1.24 — 62.0±1.31 — 55.1 —
CondViT-B/32 - Category90.9±0.98 99.2±0.31 80.2±1.55 98.8±0.39 65.8±1.42 98.4±0.65 59.0 98.0
ViT-B/16 + Filt. 90.9±0.88 — 81.9±0.87 — 68.9±1.11 — 62.4 —
CondViT-B/16 - Category93.3±1.04 99.5±0.25 85.6±1.06 99.2±0.35 74.2±1.82 99.0±0.42 68.4 98.8
CoSMo [28] 88.3±1.30 97.6±0.45 76.1±1.85 96.0±0.32 59.1±1.42 94.7±0.40 52.1 94.8
CLIP4CIR [2] 92.9±0.64 99.0±0.33 81.9±1.63 98.1±0.68 66.9±2.05 96.5±0.67 59.1 95.5
CondViT-B/32 - Caption92.7±0.77 99.1±0.30 82.8±1.22 98.7±0.40 68.4±1.50 98.1±0.43 62.1 98.0
CondViT-B/16 - Caption94.2±0.90 99.4±0.37 86.4±1.13 98.9±0.49 74.6±1.65 98.4±0.58 69.3 98.2
103 104 105 106
Number of Distractors
0⊿5
0⊿6
0⊿7
0⊿8
0⊿9R@1
Categorical Conditioning
ViT B/32 + Filt.
ViT B/16 + Filt.
Gr. DINO-B + B/16 - Cat.
CondViT B/32 - Cat.
CondViT B/16 - Cat.
103 104 105 106
Number of Distractors
0⊿5
0⊿6
0⊿7
0⊿8
0⊿9R@1
Textual Conditioning
CoSMoCLIP4CIRGr. DINO-B + B/16 - Caption
CondViT B/32 - Caption
CondViT B/16 - Caption
Figure 5: R @ 1 with repects to number of added distractors, evaluated on the entire test set. Please
refer to Tab. 1 and 2 for bootstrapped metrics and conﬁdence intervals. Our categorical CondViT-B/16
reaches the performances of the best caption-based models, while using a sparser conditioning.
We also compare with ASEN [8] trained on our dataset using the authors’ released code. This265
conditional architecture uses a global and a local branch with conditional spatial attention modules,266
respectively based on ResNet50 and ResNet34 backbones, with explicit ROI cropping. However267
in our experiments the performances decrease with the addition of the local branch in the second268
training stage, even after tuning the hyperparameters. We report results for the global branch.269
We train our CondViT using the categories provided in our dataset, learning an embedding vector270
for each of the 10 clothing categories. For thei-th product in the batch, we randomly select in the271
associated data a simple imagexs and its categorycs, and a complex imagexc. We then compute272
their embeddingszA
i =  (xc,c s),z B
i =  (xs). We also experimented with symmetric conditioning,273
using a learned token for the gallery side (see Appendix B.1).274
Our categorical CondViT-B/16, with68.4%R @ 1 against 2M distractors signiﬁcantly outperforms275
all other category-based approaches (see Fig. 5, left) and maintains a higher categorical accuracy.276
Furthermore, it performs similarly to the detection-based method conditioned on richer captions,277
while requiring easy-to-aquire coarse categories. It does so without making any assumption on the278
semantic nature of these categories, and adding only a few embedding weights (7.7K parameters) to279
the network, against 233M parameters for Grounding DINO-B. We conﬁrm in Appendix B.2 that its280
attention is localized on different objects depending on the conditioning.281
8

Outwear Lower Body Upper Body
"Long dress" "Sandals" "Ankle boots"
"Her blouse" "Same shorts" "Puffer vest"
Figure 6: Qualitative results for our categorical (ﬁrst 2 rows) and textual (last 2 rows) CondViT-B/16.
We use free-form textual queries instead of BLIP2 captions to illustrate realistic user behavior, and
retrieve from the whole test gallery. See Fig. 13 and 14 in the Appendix for more qualitative results.
Textual Conditioning To further validate our approach, we replaced the categorical conditioning282
with referring expressions, using our generated BLIP2 captions embedded by a Sentence T5-XL283
model [37]. We chose this model because it embeds the sentences in a 768-dimensional vector,284
allowing us to simply replace the categorical token. We pre-computed the caption embeddings, and285
randomly used one of them instead of the product category at training time. At test time, we used the286
ﬁrst caption.287
In Tab. 2, we observe a gain of3.1%R @ 1 for the CondViT-B/32 architecture, and0.9%R @ 1 for288
CondViT-B/16, compared to categorical conditioning against 2M distractors, most likely due to the289
additional details in the conditioning sentences. When faced with users, this method allows for more290
natural querying, with free-form referring expressions. See Figure 6 for qualitative results.291
We compare these models with CIR methods: CoSMo [28] and CLIP4CIR [2]. Both use a compositor292
network to fuse features extracted from the image and accompanying text. CoSMo reaches perfor-293
mances similar to an unconditional ViT-B/32, while CLIP4CIR performs similarly to our textual294
CondViT-B/32. We hypothesize that for our conditional feature extraction task, early condition-295
ing is more effective than modifying embeddings through a compositor at the network’s end. Our296
CondViT-B/16 model signiﬁcantly outperforms all other models and achieves results comparable to297
our caption-based approach using Grounding DINO-B (see Fig. 5, right). As the RVS task differs298
from CIR, despite both utilizing identical inputs, this was anticipated. Importantly, CondViT-B/16299
accomplishes this without the need for explicit detection steps or dataset-speciﬁc preprocessing.300
Notably, we observe that our models achieve a categorical accuracy of98% against 2M distractors,301
surpassing the accuracy of the best corresponding detection-based model, which stands at94.3%.302
6 Conclusion & Limitations303
We studied an approach to image similarity in fashion called Referred Visual Search (RVS), which304
introduces two signiﬁcant contributions. Firstly, we introduced the LAION-RVS-Fashion dataset,305
comprising 272K fashion products and 842K images. Secondly, we proposed a simple weakly-306
supervised learning method for extracting referred embeddings. Our approach outperforms strong307
detection-based baselines. These contributions offer valuable resources and techniques for advancing308
image retrieval systems in the fashion industry and beyond.309
However, one limitation of our approach is that modifying the text description to refer to something310
not present or not easily identiﬁable in the image does not work effectively. For instance, if the311
image shows a person carrying a green handbag, a reﬁned search with "red handbag" as a condition312
would only retrieve a green handbag. The system may also ignore the conditioning if the desired313
item is small or absent in the database. Examples of such failures are illustrated in Appendix B.3.314
Additionally, extending the approach to more verticals would be relevant.315
9

References316
[1] Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh317
Hajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions.arXiv preprint arXiv:2211.09260,318
2022. 3, 5319
[2] Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Alberto Del Bimbo. Composed image retrieval using320
contrastive learning and task-oriented clip-based features.ACM Transactions on Multimedia Computing,321
Communications and Applications, 2023. 3, 8, 9322
[3] Sean Bell, Yiqun Liu, Sami Alsheikh, Yina Tang, Edward Pizzi, M. Henning, Karun Singh, Omkar Parkhi,323
and Fedor Borisyuk. GrokNet: Uniﬁed Computer Vision Model Trunk and Embeddings For Commerce. In324
Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.325
ACM, 2020. ISBN 978-1-4503-7998-4. doi: 10.1145/3394486.3403311. 3326
[4] Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin. End-to-end referring video object segmentation327
with multimodal transformers. InProceedings of the IEEE/CVF Conference on Computer Vision and328
Pattern Recognition (CVPR), 2022. 3329
[5] Yiyang Chen, Zhedong Zheng, Wei Ji, Leigang Qu, and Tat-Seng Chua. Composed image retrieval with330
text feedback via multi-grained uncertainty regularization, 2022. 3331
[6] Nilotpal Das, Aniket Joshi, Promod Yenigalla, and Gourav Agrwal. MAPS: Multimodal Attention for332
Product Similarity.Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision333
(WACV), 2022. 2, 3334
[7] Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang. Vision-Language Transformer and Query335
Generation for Referring Segmentation. In2021 IEEE/CVF International Conference on Computer Vision336
(ICCV). IEEE, 2021. ISBN 978-1-66542-812-5. doi: 10.1109/ICCV48922.2021.01601. 3337
[8] Jianfeng Dong, Zhe Ma, Xiaofeng Mao, Xun Yang, Yuan He, Richang Hong, and Shouling Ji. Fine-Grained338
Fashion Similarity Prediction by Attribute-Speciﬁc Embedding Learning.IEEE Transactions on Image339
Processing, 2021. ISSN 1057-7149, 1941-0042. doi: 10.1109/TIP.2021.3115658. 2, 3, 8340
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas341
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,342
and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In343
International Conference on Learning Representations, 2021. 3, 6344
[10] Ming Du, Arnau Ramisa, Amit Kumar K C, Sampath Chanda, Mengjiao Wang, Neelakandan Rajesh,345
Shasha Li, Yingchuan Hu, Tao Zhou, Nagashri Lakshminarayana, Son Tran, and Doug Gray. Amazon346
Shop the Look: A Visual Search System for Fashion and Home. InProceedings of the 28th ACM SIGKDD347
Conference on Knowledge Discovery and Data Mining. ACM, 2022. ISBN 978-1-4503-9385-0. doi:348
10.1145/3534678.3539071. 3349
[11] Shiv Ram Dubey. A Decade Survey of Content Based Image Retrieval using Deep Learning.IEEE350
Transactions on Circuits and Systems for Video Technology, 2022. ISSN 1051-8215, 1558-2205. doi:351
10.1109/TCSVT.2021.3080920. 3352
[12] Yuying Ge, Ruimao Zhang, Xiaogang Wang, Xiaoou Tang, and Ping Luo. DeepFashion2: A Versatile353
Benchmark for Detection, Pose Estimation, Segmentation and Re-Identiﬁcation of Clothing Images. In354
2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2019. ISBN355
978-1-72813-293-8. doi: 10.1109/CVPR.2019.00548. 2356
[13] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal357
Daumé III, and Kate Crawford. Datasheets for datasets, 2021. 5358
[14] Xiaoxiao Guo, Hui Wu, Yu Cheng, Steven Rennie, Gerald Tesauro, and Rogerio Feris. Dialog-based359
Interactive Image Retrieval. InAdvances in Neural Information Processing Systems. Curran Associates,360
Inc., 2018. 3361
[15] Mark Hamilton, Stephanie Fu, Mindren Lu, Johnny Bui, Darius Bopp, Zhenbang Chen, Felix Tran,362
Margaret Wang, Marina Rogers, Lei Zhang, Chris Hoder, and William T. Freeman. MosAIc: Finding363
Artistic Connections across Culture with Conditional Image Retrieval. InProceedings of the NeurIPS 2020364
Competition and Demonstration Track. PMLR, 2021. 3365
[16] Xiao Han, Sen He, Li Zhang, Yi-Zhe Song, and Tao Xiang. UIGR: Uniﬁed Interactive Garment Retrieval.366
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 3367
10

[17] Houdong Hu, Yan Wang, Linjun Yang, Pavel Komlev, Li Huang, Xi Chen, Jiapei Huang, Ye Wu, Meenaz368
Merchant, and Arun Sacheti. Web-scale responsive visual search at bing. InProceedings of the 24th ACM369
SIGKDD international conference on knowledge discovery & data mining, 2018. 2, 3370
[18] Junshi Huang, Rogerio Feris, Qiang Chen, and Shuicheng Yan. Cross-Domain Image Retrieval with a371
Dual Attribute-Aware Ranking Network. In2015 IEEE International Conference on Computer Vision372
(ICCV). IEEE, 2015. ISBN 978-1-4673-8391-2. doi: 10.1109/ICCV .2015.127. 2373
[19] Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, and Bo Li.374
Referring Image Segmentation via Cross-Modal Progressive Comprehension. In2020 IEEE/CVF Confer-375
ence on Computer Vision and Pattern Recognition (CVPR). IEEE, 2020. ISBN 978-1-72817-168-5. doi:376
10.1109/CVPR42600.2020.01050. 3377
[20] Yang (Andrew) Jiao, Yan Gao, Jingjing Meng, Jin Shang, and Yi Sun. Learning attribute and class-speciﬁc378
representation duet for ﬁne-grained fashion analysis. InCVPR 2023, 2023. 2, 3379
[21] Yushi Jing, David Liu, Dmitry Kislyuk, Andrew Zhai, Jiajing Xu, Jeff Donahue, and Sarah Tavel. Visual380
search at pinterest. InProceedings of the 21th ACM SIGKDD International Conference on Knowledge381
Discovery and Data Mining, 2015. 2, 3382
[22] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus.IEEE Transac-383
tions on Big Data, 2019. 1, 6384
[23] M. Hadi Kiapour, Xufeng Han, Svetlana Lazebnik, Alexander C. Berg, and Tamara L. Berg. Where to385
Buy It: Matching Street Clothing Photos in Online Shops. In2015 IEEE International Conference on386
Computer Vision (ICCV). IEEE, 2015. ISBN 978-1-4673-8391-2. doi: 10.1109/ICCV .2015.382. 2387
[24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,388
Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything.389
arXiv:2304.02643, 2023. 3, 7390
[25] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3D Object Representations for Fine-Grained391
Categorization. In 2013 IEEE International Conference on Computer Vision Workshops. IEEE, 2013.392
ISBN 978-1-4799-3022-7. doi: 10.1109/ICCVW.2013.77. 2393
[26] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can394
distort pretrained features and underperform out-of-distribution. InInternational Conference on Learning395
Representations, 2022. 6396
[27] Julia Lasserre, Katharina Rasch, and Roland V ollgraf. Studio2Shop: from studio photo shoots to fashion397
articles. In Proceedings of the 7th International Conference on Pattern Recognition Applications and398
Methods, 2018. doi: 10.5220/0006544500370048. 2399
[28] Seungmin Lee, Dongwan Kim, and Bohyung Han. Cosmo: Content-style modulation for image retrieval400
with text feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern401
Recognition (CVPR), 2021. 3, 8, 9402
[29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training403
with frozen image encoders and large language models.arXiv preprint arXiv:2301.12597, 2023. 4, 16, 20404
[30] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,405
Hang Su, Jun Zhu, et al. Grounding DINO: Marrying DINO with grounded pre-training for open-set object406
detection. arXiv preprint arXiv:2303.05499, 2023. 3, 7407
[31] Si Liu, Zheng Song, Guangcan Liu, Changsheng Xu, Hanqing Lu, and Shuicheng Yan. Street-to-shop:408
Cross-scenario clothing retrieval via parts alignment and auxiliary set. In2012 IEEE Conference on409
Computer Vision and Pattern Recognition, 2012. 2410
[32] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. DeepFashion: Powering Robust Clothes411
Recognition and Retrieval with Rich Annotations. In2016 IEEE Conference on Computer Vision and412
Pattern Recognition (CVPR). IEEE, 2016. ISBN 978-1-4673-8851-1. doi: 10.1109/CVPR.2016.124. 2413
[33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. InInternational Conference on414
Learning Representations, 2019. 6415
[34] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji. Multi-416
Task Collaborative Network for Joint Referring Expression Comprehension and Segmentation. In2020417
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2020. ISBN 978-1-418
72817-168-5. doi: 10.1109/CVPR42600.2020.01005. 3419
11

[35] Zhe Ma, Jianfeng Dong, Zhongzi Long, Yao Zhang, Yuan He, Hui Xue, and Shouling Ji. Fine-grained420
fashion similarity learning by attribute-speciﬁc embedding network. InProceedings of the AAAI Conference421
on Artiﬁcial Intelligence, 2020. 3422
[36] Emily Mu and John Guttag. Conditional Contrastive Networks. InNeurIPS 2022 First Table Representation423
Workshop, 2022. 3424
[37] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei425
Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. InFindings of the426
Association for Computational Linguistics: ACL 2022. Association for Computational Linguistics, 2022.427
doi: 10.18653/v1/2022.ﬁndings-acl.146. 9428
[38] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive429
coding. arXiv preprint arXiv:1807.03748, 2018. 6430
[39] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V . V o, Marc Szafraniec, Vasil Khalidov, Pierre431
Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu,432
Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel433
Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.434
Dinov2: Learning robust visual features without supervision, 2023. 19435
[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish436
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from437
natural language supervision. InInternational conference on machine learning. PMLR, 2021. 6438
[41] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,439
Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale440
dataset for training next generation image-text models.arXiv preprint arXiv:2210.08402, 2022. 2441
[42] Raymond Shiau, Hao-Yu Wu, Eric Kim, Yue Li Du, Anqi Guo, Zhiyuan Zhang, Eileen Li, Kunlong Gu,442
Charles Rosenberg, and Andrew Zhai. Shop The Look: Building a Large Scale Visual Shopping System at443
Pinterest. InProceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &444
Data Mining, 2020. doi: 10.1145/3394486.3403372. 2, 3445
[43] Leslie N Smith. Cyclical learning rates for training neural networks. In2017 IEEE winter conference on446
applications of computer vision (WACV). IEEE, 2017. 6447
[44] Kihyuk Sohn. Improved Deep Metric Learning with Multi-class N-pair Loss Objective. InAdvances in448
Neural Information Processing Systems. Curran Associates, Inc., 2016. 6449
[45] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep Metric Learning via Lifted450
Structured Feature Embedding. In2016 IEEE Conference on Computer Vision and Pattern Recognition451
(CVPR). IEEE, 2016. ISBN 978-1-4673-8851-1. doi: 10.1109/CVPR.2016.434. 2452
[46] Son Tran, R. Manmatha, and C. J. Taylor. Searching for fashion products from images in the wild. InKDD453
2019 Workshop on AI for Fashion, 2019. 3454
[47] Andreas Veit, Serge Belongie, and Theofanis Karaletsos. Conditional similarity networks. InProceedings455
of the IEEE conference on computer vision and pattern recognition, 2017. 3456
[48] Nam V o, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. Composing text and457
image for image retrieval-an empirical odyssey. InCVPR, 2019. 3458
[49] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The Caltech-UCSD459
Birds-200-2011 Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011. 2460
[50] Xi Wang, Zhenfeng Sun, Wenqiang Zhang, Yu Zhou, and Yu-Gang Jiang. Matching User Photos to Online461
Products with Robust Deep Features. InProceedings of the 2016 ACM on International Conference on462
Multimedia Retrieval. ACM, 2016. ISBN 978-1-4503-4359-6. doi: 10.1145/2911996.2912002. 2463
[51] Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, and Rogerio Feris.464
Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language Feedback. InCVPR, 2019. 3465
[52] Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping Luo. Language as Queries for Referring Video466
Object Segmentation. In2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).467
IEEE, 2022. ISBN 978-1-66546-946-3. doi: 10.1109/CVPR52688.2022.00492. 3468
[53] Fan Yang, Ajinkya Kale, Yury Bubnov, Leon Stein, Qiaosong Wang, Hadi Kiapour, and Robinson469
Piramuthu. Visual Search at eBay. InProceedings of the 23rd ACM SIGKDD International Conference on470
Knowledge Discovery and Data Mining, 2017. doi: 10.1145/3097983.3098162. 3471
12

[54] Licheng Yu, Jun Chen, Animesh Sinha, Mengjiao Wang, Yu Chen, Tamara L Berg, and Ning Zhang. Com-472
mercemm: Large-scale commerce multimodal representation learning with omni retrieval. InProceedings473
of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. 3474
[55] Yifei Yuan and Wai Lam. Conversational Fashion Image Retrieval via Multiturn Natural Language475
Feedback. InProceedings of the 44th International ACM SIGIR Conference on Research and Development476
in Information Retrieval (SIGIR ’21), 2021. 3477
[56] Yan Zeng, Xinsong Zhang, and Hang Li. Multi-Grained Vision Language Pre-Training: Aligning Texts478
with Visual Concepts. InProceedings of the 39th International Conference on Machine Learning. PMLR,479
2022. 3480
[57] Yan Zeng, Xinsong Zhang, Hang Li, Jiawei Wang, Jipeng Zhang, and Wangchunshu Zhou. X2-VLM:481
All-in-one pre-trained model for vision-language tasks.arXiv preprint arXiv:2211.12402, 2022. 3482
[58] Andrew Zhai, Hao-Yu Wu, Eric Tzeng, Dong Huk Park, and Charles Rosenberg. Learning a uniﬁed em-483
bedding for visual search at pinterest. InProceedings of the 25th ACM SIGKDD International Conference484
on Knowledge Discovery & Data Mining, 2019. 3485
[59] Xunlin Zhan, Yangxin Wu, Xiao Dong, Yunchao Wei, Minlong Lu, Yichi Zhang, Hang Xu, and Xiaodan486
Liang. Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-Modal487
Pretraining. In2021 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, 2021. ISBN488
978-1-66542-812-5. doi: 10.1109/ICCV48922.2021.01157. 3489
[60] Yanhao Zhang, Pan Pan, Yun Zheng, Kang Zhao, Yingya Zhang, Xiaofeng Ren, and Rong Jin. Visual490
Search at Alibaba. InProceedings of the 24th ACM SIGKDD International Conference on Knowledge491
Discovery & Data Mining, 2018. doi: 10.1145/3219819.3219820. 3492
[61] Yuting Zhang, Luyao Yuan, Yijie Guo, Zhiyuan He, I-An Huang, and Honglak Lee. Discriminative493
Bimodal Networks for Visual Localization and Detection with Natural Language Queries. In2017 IEEE494
Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017. ISBN 978-1-5386-0457-1.495
doi: 10.1109/CVPR.2017.122. 3496
[62] Xiaoyang Zheng, Zilong Wang, Ke Xu, Sen Li, Tao Zhuang, Qingwen Liu, and Xiaoyi Zeng. MAKE:497
Vision-Language Pre-training based Product Retrieval in Taobao Search. InCompanion Proceedings of the498
ACM Web Conference 2023, 2023. doi: 10.1145/3543873.3584627. 3499
Checklist500
1. For all authors...501
(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s502
contributions and scope?[Yes]503
(b) Did you describe the limitations of your work?[Yes]504
(c) Did you discuss any potential negative societal impacts of your work?[Yes] See505
Appendix A.5.506
(d) Have you read the ethics review guidelines and ensured that your paper conforms to507
them? [Yes] See Appendix A.5.508
2. If you are including theoretical results...509
(a) Did you state the full set of assumptions of all theoretical results?[N/A]510
(b) Did you include complete proofs of all theoretical results?[N/A]511
3. If you ran experiments (e.g. for benchmarks)...512
(a) Did you include the code, data, and instructions needed to reproduce the main ex-513
perimental results (either in the supplemental material or as a URL)?[Yes] See Ap-514
pendix A.2.515
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they516
were chosen)?[Yes] See Section 5.1.517
(c) Did you report error bars (e.g., with respect to the random seed after running experi-518
ments multiple times)?[Yes] See Table 1 and 2.519
(d) Did you include the total amount of compute and the type of resources used (e.g., type520
of GPUs, internal cluster, or cloud provider)?[Yes] See Section 5.1.521
13

4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...522
(a) If your work uses existing assets, did you cite the creators?[Yes]523
(b) Did you mention the license of the assets?[Yes]524
(c) Did you include any new assets either in the supplemental material or as a URL?[Yes]525
We provide the URLs to the assets in Appendix A.2.526
(d) Did you discuss whether and how consent was obtained from people whose data you’re527
using/curating? [Yes]528
(e) Did you discuss whether the data you are using/curating contains personally identiﬁable529
information or offensive content?[Yes]530
5. If you used crowdsourcing or conducted research with human subjects...531
(a) Did you include the full text of instructions given to participants and screenshots, if532
applicable? [N/A]533
(b) Did you describe any potential participant risks, with links to Institutional Review534
Board (IRB) approvals, if applicable?[N/A]535
(c) Did you include the estimated hourly wage paid to participants and the total amount536
spent on participant compensation?[N/A]537
14