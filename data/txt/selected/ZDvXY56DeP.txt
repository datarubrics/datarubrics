OPEN RL B ENCHMARK : Comprehensive Tracked
Experiments for Reinforcement Learning
Shengyi Huang1,2∗ Quentin Gallouédec1,3∗ Florian Felten4 Antonin Raffin5
Rousslan Fernand Julien Dossa6 Yanxiao Zhao7,8 Ryan Sullivan9 Viktor Makoviychuk10
Denys Makoviichuk11 Mohamad H. Danesh12 Cyril Roumégous13 Jiayi Weng
Chufan Chen14 Md Masudur Rahman15 João G. M. Araújo16 Guorui Quan17
Daniel C.H. Tan18,19 Timo Klein20,21 Rujikorn Charakorn22 Mark Towers23
Yann Berthelot24,25 Kinal Mehta26 Dipam Chakraborty27 Arjun KG
Valentin Charraut28 Chang Ye29 Zichen Liu30 Lucas N. Alegre31 Alexander Nikulin32
Xiao Hu33 Tianlin Liu34 Jongwook Choi35 Brent Yi36
Abstract
In many Reinforcement Learning (RL) papers, learning curves are useful indicators1
to measure the effectiveness of RL algorithms. However, the complete raw data2
of the learning curves are rarely available. As a result, it is usually necessary3
to reproduce the experiments from scratch, which can be time-consuming and4
error-prone. We present OPEN RL B ENCHMARK (ORLB ), a set of fully tracked5
RL experiments, including not only the usual data such as episodic return, but also6
all algorithm-specific and system metrics. ORLB is community-driven: anyone7
can download, use, and contribute to the data. At the time of writing, more than8
25,000 runs have been tracked, for a cumulative duration of more than 8 years.9
It covers a wide range of RL libraries and reference implementations. Special10
care is taken to ensure that each experiment is precisely reproducible by providing11
not only the full parameters, but also the versions of the dependencies used to12
generate it. In addition, ORLB comes with a command-line interface (CLI) for13
easy fetching and generating figures to present the results. In this document, we14
include two case studies to demonstrate the usefulness of ORLB in practice. To15
the best of our knowledge, ORLB is the first RL benchmark of its kind, and the16
authors hope that it will improve and facilitate the work of researchers in the field.17
1 Introduction18
Reinforcement Learning (RL) research is based on comparing new methods to baselines to assess19
progress (Patterson et al., 2023). This process requires the availability of the data associated with20
these baselines (Raffin et al., 2021) or, alternatively, the ability to replicate them and generate the21
data oneself (Raffin, 2020). In addition, reproducible results allow the methods to be compared with22
new benchmarks and to identify the areas in which the methods excel and those in which they are23
likely to fail, thus providing avenues for future research.24
In practice, the RL research community faces complex challenges in comparing new methods with25
reference data. The unavailability of reference data requires researchers to reproduce experiments,26
which is difficult due to insufficient source code documentation and evolving software dependencies.27
∗Equal contributions
Submitted to the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets
and Benchmarks. Do not distribute.

0M 2M 4M 6M 8M 10M
0
100
200
300
400
500
BreakoutNoFrameskip-v4
0M 2M 4M 6M 8M 10M
0
2500
5000
7500
10000
12500
BeamRiderNoFrameskip-v4
0M 2M 4M 6M 8M 10M
20
10
0
10
20
PongNoFrameskip-v4
Steps
Episodic Return
OpenAI Baselines PPO2 CleanRL PPO CleanRL DQN SB3 PPO SB3 DQN
Figure 1: Example of learning curves obtained with OPEN RL B ENCHMARK . These compare the
episodic returns obtained by different implementations of PPO and DQN on three Atari games.
Implementation details, as highlighted in past research, can significantly impact results (Henderson28
et al., 2018; Huang et al., 2022a). Moreover, limited computing resources play a crucial role, hindering29
the reproduction process and affecting researchers without substantial access.30
The lack of standardized metrics and benchmarks across studies not only impedes comparison but31
also results in a substantial waste of time and resources. To address these issues, the RL community32
must establish rigorous reproducibility standards, ensuring replicability and comparability across33
studies. Transparent sharing of data, code, and experimental details, along with the adoption of34
consistent metrics and benchmarks, would collectively enhance the evaluation and progression of RL35
research, ultimately accelerating advancements in the field.36
ORLB presents a rich collection of tracked RL experiments and aims to set a new standard by37
providing a diverse training dataset. This initiative prioritizes the use of existing data over re-running38
baselines, emphasizing reproducibility and transparency. Our contributions are:39
• Extensive dataset: Offers a large, diverse collection of tracked RL experiments.40
• Standardization: Establishes a new norm by encouraging reliance on existing data, reducing41
the need for re-running baselines.42
• Comprehensive metrics: Includes diverse tracked metrics for method-specific and system43
evaluation, in addition to episodic return.44
• Reproducibility: Emphasizes clear instructions and fixed dependencies, ensuring easy45
experiment replication.46
• Resource for research: Serves as a valuable and collaborative resource for RL research.47
• Facilitating exploration: Enables reliable exploration and assessment of new and exisiting48
RL methods.49
2 Comprehensive overview of ORLB: content, methodology, tools, and50
applications51
This section provides a detailed exploration of the contents of ORLB , including its diverse set of52
libraries and environments, and the metrics it contains. We also look at the practical aspects of using53
ORLB , highlighting its ability to ensure accurate reproducibility and facilitate the creation of data54
visualizations thanks to its CLI.55
2.1 Content56
ORLB data is stored and shared with Weights and Biases (Biewald, 2020). The data is contained57
in a common entity named openrlbenchmark. Runs are divided into several projects. A project58
can correspond to a library, but it can also correspond to a set of more specific runs, such as59
envpool-cleanrl in which we find CleanRL runs (Huang et al., 2022b) launched with the EnvPool60
2

implementation of environments (Weng et al., 2022b). A project can also correspond to a reference61
implementation, such as TD3 (project sfujim-TD3) or Phasic Policy Gradient (Cobbe et al., 2021)62
(project phasic-policy-gradient). ORLB also includes reports, which are interactive documents63
designed to enhance the visualization of selected representations. These reports provide a more64
user-friendly format for practitioners to share, discuss, and analyze experimental results, even across65
different projects. Figure 2 shows a preview of one such report.66
Figure 2: An example of a report on the Weights and Biases platform, dealing with the
contribution of QDagger (Agarwal et al., 2022), and using data from ORLB . The URL
to access the report is https://wandb.ai/openrlbenchmark/openrlbenchmark/reports/
Atari-CleanRL-s-Qdagger--Vmlldzo0NTg1ODY5 .
At the time of writing, ORLB contains nearly 25,000 runs, for a total of 72,000 hours (more than 867
years) of tracking. In the following paragraphs, we present the libraries and environments for which68
runs are available in ORLB, as well as the metrics tracked.69
Libraries ORLB contains runs for several reference RL libraries. These libraries are: abcdRL70
(Zhao, 2022), Acme (Hoffman et al., 2020), Cleanba (Huang et al., 2023), CleanRL (Huang et al.,71
2022b), jaxrl (Kostrikov, 2021), moolib (Mella et al., 2022), MORL-Baselines (Felten et al., 2023),72
OpenAI Baselines (Dhariwal et al., 2017), rlgames (Makoviichuk & Makoviychuk, 2021) Stable73
Baselines3 (Raffin et al., 2021; Raffin, 2020) Stable Baselines Jax (Raffin et al., 2021) and TorchBeast74
(Küttler et al., 2019).75
Environments The runs contained in ORLB cover a wide range of classic environments. They76
include Atari (Bellemare et al., 2013; Machado et al., 2018), Classic control (Brockman et al., 2016),77
Box2d (Brockman et al., 2016) and MuJoCo (Todorov et al., 2012) as part of either Gym (Brockman78
et al., 2016) or Gymnasium (Towers et al., 2023) or EnvPool (Weng et al., 2022b). They also79
include Bullet (Coumans & Bai, 2016), Procgen Benchmark (Cobbe et al., 2020), Fetch environments80
(Plappert et al., 2018), PandaGym (Gallouédec et al., 2021), highway-env (Leurent, 2018), Minigrid81
(Chevalier-Boisvert et al., 2023) and MO-Gymnasium (Alegre et al., 2022).82
Tracked metrics Metrics are recorded throughout the learning process, consistently linked with a83
global step indicating the number of interactions with the environment, and an absolute time, which84
allows to compute the duration of a run. We categorize these metrics into four distinct groups:85
• Training-related metrics: These are general metrics related to RL learning. This category86
contains, for example, the average returns obtained, the episode length or the number of87
collected samples per second.88
3

• Method-specific metrics: These are losses and measures of key internal values of the89
methods. For PPO, for example, this category includes the value loss, the policy loss, the90
entropy or the approximate KL divergence.91
• Evolving configuration parameters: These are configuration values that change during the92
learning process. This category includes, for example, the learning rate when there is decay,93
or the exploration rate (ϵ) in the Deep Q-Network (DQN) (Mnih et al., 2013).94
• System metrics: These are metrics related to system components. These could be GPU95
memory usage, its power consumption, its temperature, system and process memory usage,96
CPU usage or even network traffic.97
The specific metrics available may vary from one library to another. In addition, even where the98
metrics are technically similar, the terminology or key used to record them may vary from one99
library to another. Users are advised to consult the documentation specific to each library for precise100
information on these measures.101
2.2 Everything you need for perfect repeatability102
Reproducing experimental results in computational research, as discussed in Section 4.3, is often103
challenging due to evolving codebases, incomplete hyperparameter listings, version discrepancies,104
and compatibility issues. Our approach aims to enhance reproducibility by ensuring users can105
exactly replicate benchmark results. Each experiment includes a complete configuration with all106
hyperparameters, frozen versions of dependencies, and the exact command, including the necessary107
random seed, for systematic reproducibility. As a example, CleanRL (Huang et al., 2022b) introduces108
a unique utility that streamlines the process of experiment replication (see Figure 3). This tool109
produces the command lines to set up a Python environment with the necessary dependencies,110
download the run file, and the precise command required for the experiment reproduction. Such111
an approach to reproduction facilitates research and makes it possible to study in depth unusual112
phenomena, or cases of rupture2, in learning processes, which are generally ignored in the results113
presented, either because they are deliberately left out or because they are erased by the averaging114
process.115
Figure 3: CleanRL’s module reproduce allows the user to generate, from an ORLB run reference,
the exact command suite for an identical reproduction of the run.
2.3 The CLI for generating figures in one command line116
ORLB offers convenient access to raw data from RL libraries on standard environments. It includes a117
feature for easily extracting and visualizing data in a paper-friendly format, streamlining the process118
of filtering and extracting relevant runs and metrics for research papers through a single command.119
The CLI is a powerful tool for generating most metrics-related figures for RL research and notably,120
all figures in this document were generated using the CLI. The data in ORLB can also be accessed121
by custom scripts, as detailed in Appendix A.2. Specifically, the CLI integrated into ORLB provides122
users with the flexibility to:123
2Exemplified in https://github.com/DLR-RM/rl-baselines3-zoo/issues/427
4

• Specify algorithms’ implementations (from which library) along with their corresponding124
git commit or tag;125
• Choose target environments for analysis;126
• Define the metrics of interest;127
• Opt for the additional generation of metrics and plots using RLiable (Agarwal et al., 2021).128
Concrete example usage of the CLI and resulting plots are available in Appendix A.1.129
3 ORLB in action: an insight into case studies130
ORLB offers a powerful tool for researchers to evaluate and compare different RL algorithms. In this131
section, we will explore two case studies that showcase its benefits. First, we propose to investigate132
the effect of using TD(λ) for value estimation in PPO (Schulman et al., 2017) versus using Monte133
Carlo (MC). This simple study illustrates the use of ORLB through a classic research question.134
Moreover, to the best of our knowledge, this question has never been studied in the literature. We135
then show how ORLB is used to demonstrate the speedup and variance reduction of a new IMPALA136
implementation proposed by Huang et al. (2023). By using ORLB , we can save time and resources137
while ensuring consistent and reproducible comparisons. These case studies highlight the role of the138
benchmark in providing insights that can advance the field of RL research.139
3.1 Easily assess the contribution of TD( λ) for value estimation in PPO140
In the first case study, we show how ORLB can be used to easily compare the performance of141
different methods for estimating the value function in PPO (Schulman et al., 2017), one of the142
many implementation details of this algorithm (Huang et al., 2022a). Specifically, we compare the143
commonly used Temporal Difference (TD)(λ) estimate to the Monte-Carlo (MC) estimate.144
PPO typically employs Generalized Advantage Estimation (GAE) (Schulman et al., 2016) to update145
the actor. The advantage estimate is expressed as follows:146
AGAE(γ,λ)
t =
N−1X
l=0
(γλ)lδV
t+l (1)
where λ ∈ [0, 1] adjusts the bias-variance tradeoff and δV
t+l = Rt+l + γ ˆV (St+l+1) − ˆV (St+l). The147
target return for critic optimization is estimated with TD(λ) as follows:148
Gλ
t = (1− λ)
∞X
n=1
λn−1Gt:t+n (2)
where Gt:t+n = Pn−1
k=0 γkRt+k+1 + γnV (St+n) is the n-steps return. In practice, the target return149
for updating the critic is computed from the GAE value, by adding the minibatch return, a detail150
usually overlooked by practitioners (Huang et al., 2022a, point 5). While previous studies (Patterson151
et al., 2023) have shown the joint benefit of GAE and over MC estimates for actor and critic, we152
focus on the value function alone. To isolate the influence of the value function estimation, we vary153
the method used for the value function and keep GAE for advantage estimation.154
The first step is to identify the reference runs in ORLB . Since PPO is a well-known baseline, there155
are many runs available; we decided to use those from Stable Baselines3 for this example. We156
then retrieve the exact source code and command used to generate the runs – thanks to the pinned157
dependencies that come with them – and make the necessary changes to the source code. For each158
selected environment, we start three learning runs using the same command as the one we retrieved.159
The runs are saved in a dedicated project3. For fast and user-friendly rendering of the results, we160
3https://wandb.ai/modanesh/openrlbenchmark
5

create a Weights and Biases report4. Using ORLB CLI, we generate Figure 4 and 5. The command161
used to generate the figures is given in Appendix B.162
Figures 4 and 5 give an overview of the results, while detailed plots in the Appendix B provide a163
closer look at each environment. The proposed modification to the PPO value function estimation164
has an impact on the performance for Atari games (Figure 4a): not using TD( λ) results in lower165
scores. However, PPO with MC estimates has similar performance to the original PPO in Box2D166
and MuJoCo environments. This example shows how ORLB can be used to quickly investigate the167
influence of design choices in RL. It provides baseline results and tools to compare and reproduce168
results.169
0M 5M
0.0
0.5Median
0M 5M
0.0
0.5IQM
0M 5M
0.0
0.5Mean
0M 5M
0.5
1.0Optimality Gap
Steps
PPO PPO w/ MC for value estimation
(a) Results for Atari games
0K 500K 1000K
0.25
0.50
0.75Median
0K 500K 1000K
0.25
0.50
0.75IQM
0K 500K 1000K
0.50
0.75Mean
0K 500K 1000K
0.25
0.50Optimality Gap
Steps
PPO PPO w/ MC for value estimation (b) Results for Box2D and MuJoCo environments
Figure 4: Comparing the original PPO and the PPO with Monte-Carlo (MC) for value estimation.
These experiments were conducted over 15 environments, including Atari games, Box2D, and
MuJoCo. The plot shows min-max normalized scores with 95% stratified bootstrap CIs.
0.64 0.72 0.80 0.88
PPO w/ MC for value estimation
PPO
Median
0.64 0.72 0.80 0.88
Normalized Score
IQM
0.72 0.80 0.88
Mean
0.16 0.24 0.32
Optimality Gap
(a) Results for Atari games
0.80 0.85 0.90
PPO w/ MC for value estimation
PPO
Median
0.850 0.875 0.900 0.925
Normalized Score
IQM
0.78 0.81 0.84
Mean
0.15 0.18 0.21 0.24
Optimality Gap
(b) Results for Box2D and MuJoCo environments
Figure 5: Study of the contribution of GAE for estimating the value used to update the critic in PPO,
compared against its variant which uses the MC estimator instead. Figures show the aggregated
min-max normalized scores with stratified 95% stratified bootstrap CIs.
3.2 Demonstrating the utility of ORLB through the Cleanba case study170
This section describes how ORLB was instrumental in the evaluation and presentation of Cleanba171
(Huang et al., 2023), a new open-source platform for distributed RL implementing highly optimized172
distributed variants of PPO (Schulman et al., 2017) and IMPALA (Espeholt et al., 2018). Cleanba’s173
authors asserted three points: (1) Cleanba implementations compare favorably with baselines in terms174
of sample efficiency, (2) for the same system, the Cleanba implementation is more optimized and175
therefore faster, and (3) the design choices allow a reduction in the variability of results.176
To prove these assertions, the evaluation of Cleanba encountered a common problem in RL research:177
the works that initially proposed these baselines did not provide the raw results of their experiments.178
Although a reference implementation is available5, it is no longer maintained. Subsequent works179
such as Moolib (Mella et al., 2022) and TorchBeast (Küttler et al., 2019) have successfully replicated180
4https://api.wandb.ai/links/modanesh/izf4yje4
5https://github.com/google-deepmind/scalable_agent
6

the IMPALA results. However, these shared results are limited to the paper’s presented curves, which181
provide a smoothed measure of episodic return as a function of interaction steps on a specific set of182
Atari tasks. It is worth noting that these tasks are not an exact match for the widely recognized Atari183
57, and the raw data used to generate these curves is unavailable.184
Recognizing the lack of raw data for existing IMPALA implementations, the authors reproduced the185
experiments, tracked the runs and integrated them into ORLB . As a reminder, these logged data186
include not only the return curves, but also the system configurations and temporal data, which are187
crucial to support the Cleanba authors’ optimization claim. Comparable experiments have been run,188
tracked and shared on ORLB with the proposed Cleanba implementation.189
0M 10M 20M 30M 40M 50M
Steps
0
1Median
0 200 400 600 800
Time (m)
Monobeast IMPALA, 1 A100, 10 CPU
Moolib IMPALA, 1 A100, 10 CPU
Cleanba IMPALA, 1 A100, 10 CPU
Cleanba PPO (Sync), 1 A100, 10 CPU
Figure 6: Median human-normalized scores with 95% stratified bootstrap CIs of Cleanba (Huang
et al., 2023) variants compared with moolib (Mella et al., 2022) and monobeast (Küttler et al., 2019).
The experiments were conducted on 57 Atari games (Bellemare et al., 2013). The data used to
generate the figure comes from ORLB , and the figure was generated with a single command from
ORLB’s CLI. Figure from (Huang et al., 2023).
1.2 1.4 1.6 1.8
Cleanba PPO (Sync), 8 A100, 46 CPU
Cleanba PPO (Sync), 1 A100, 10 CPU
Cleanba IMPALA, 8 A100, 46 CPU
Cleanba IMPALA, 1 A100, 10 CPU
Moolib (Resnet CNN) 8 A100, 80 CPU
Moolib (Resnet CNN) 1 A100, 10 CPU
Median
1.50 1.75 2.00
Normalized Score
IQM
6 9 12 15
Mean
0.24 0.27 0.30 0.33
Optimality Gap
Figure 7: Aggregated normalized human scores with stratified 95% bootstrap CIs, showing that
unlike moolib (Mella et al., 2022), Cleanba (Huang et al., 2023) variants have more predictable
learning curves (using the same hyperparameters) across different hardware configurations. Figure
from (Huang et al., 2023).
Using ORLB CLI, the authors generated several figures. In Figure 6, taken from (Huang et al.,190
2023), the authors show that the results in terms of sample efficiency compare favorably with the191
baselines, and that for the same system configuration, convergence was temporally faster with the192
proposed implementation, thus proving claims (1) and (2). Figure 7 demonstrates that Cleanba193
variants maintain consistent learning curves across different hardware configurations. Conversely,194
moolib’s IMPALA shows marked variability in similar settings, despite identical hyperparameters,195
confirming the authors’ third claim.196
4 Current practices in RL: data reporting, sharing and reproducibility197
Many new methods have emerged in recent years, with some becoming standard baselines, but198
current practices in the field make it challenging to interpret, compare, and replicate study results. In199
this section, we highlight the inconsistent presentation of results, focusing on learning curves as an200
example. This inconsistency can hinder interpretation and lead to incorrect conclusions. We also note201
the insufficient availability of learning data, despite some positive efforts, and examine challenges202
related to method reproducibility.203
7

4.1 Analyzing learning curve practices204
Plotting learning curves is a common way to show an agent’s performance over learning. We closely205
examine the components of learning curves and the choices made by key publications. We find a lack206
of uniformity, with presentation choices rarely explained and sometimes not explicitly stated.207
Axis Typically, they axis measures either the return acquired during data collection or evaluation.208
Some older papers, like (Schulman et al., 2015; Mnih et al., 2016; Schulman et al., 2017), fail to209
specify the metric, using the vague termlearning curve. The first approach sums the rewards collected210
during agent rollout (Dabney et al., 2018; Burda et al., 2019). The second approach suspends training,211
averaging the agent’s return over episodes, deactivating exploration elements (Fujimoto et al., 2018;212
Haarnoja et al., 2018; Hessel et al., 2018; Janner et al., 2019; Badia et al., 2020b; Ecoffet et al., 2021;213
Chen et al., 2021). This method is prevalent and provides a more precise evaluation. Regarding the x214
axis, while older baselines (Schulman et al., 2015; Mnih et al., 2016) use policy updates and learning215
epochs, the norm is to use interaction counts with the environment. In Atari environments, it is often216
the number of frames, adjusting for frame skipping to match human interaction frequency.217
Shaded area Data variability is typically shown with a shaded area, but its definition varies across218
studies. Commonly, it represents the standard deviation (Chen et al., 2021; Janner et al., 2019) and219
less commonly half the standard deviation (Fujimoto et al., 2018). Haarnoja et al. (2018) uses a220
min-max representation to include outliers, covering the entire observed range. This method offers221
a comprehensive view but amplifies outliers’ impact with more runs. Ecoffet et al. (2021) adopts222
a probabilistic approach, showing a 95% bootstrap confidence interval around the mean, ensuring223
statistical confidence. Unfortunately, Schulman et al. (2015, 2017); Mnih et al. (2016); Dabney et al.224
(2018); Badia et al. (2020b) omit statistical details or even the shaded area, introducing uncertainty in225
data variability interpretation, as seen in (Hessel et al., 2018).226
Normalization and aggregation Performance aggregation assesses method results across various227
tasks and domains, indicating their generality and robustness. Outside the Atari context, aggregation228
practices are uncommon due to the lack of a universal normalization standard. Without a widely229
accepted normalization strategy, scores are typically not aggregated, or if they are, it relies on a230
min-max approach lacking absolute significance and unsuitable for comparisons. Early Atari research231
did not use normalization or aggregate results (Mnih et al., 2013). There has been a shift towards232
normalizing against human performance, though this has weaknesses and may not reflect true agent233
mastery (Toromanoff et al., 2019). Aggregation methods vary: the mean is common but influenced234
by outliers, leading some studies to prefer the more robust median, as in (Hessel et al., 2018). Many235
papers now report both mean and median results (Dabney et al., 2018; Hafner et al., 2023; Badia236
et al., 2020a). Recent approaches, like using the Interquartile Mean (IQM), provide a more accurate237
performance representation across diverse games (Lee et al., 2022), as suggested by Agarwal et al.238
(2021).239
4.2 Spectrum of data sharing practices240
While the mentioned studies often have reference implementations (see Section 4.3), the sharing of241
training data typically extends only to the curves presented in their articles. This necessitates reliance242
on libraries that replicate these methods, offering benchmarks with varying levels of completeness.243
Several widely-used libraries in the field provide high-level summaries or graphical representations244
without including raw data (e.g., Tensorforce (Kuhnle et al., 2017), Garage (garage contributors,245
2019), ACME (Hoffman et al., 2020), MushroomRL (D’Eramo et al., 2021), ChainerRL (Fujita246
et al., 2021), and TorchRL (Bou et al., 2023)). Spinning Up (Achiam, 2018) offers partial data247
accessibility, providing benchmark curves but withholding raw data. TF-Agent (Guadarrama et al.,248
2018) is slightly better, offering experiment tracking with links to TensorBoard.dev, though its future249
is uncertain due to service closure. Tianshou (Weng et al., 2022a) provides individual run reward data250
for Atari and average rewards for MuJoCo, with more detailed MuJoCo data available via a Google251
8

Drive link, but it is not widely promoted. RLLib (Liang et al., 2018) maintains an intermediate252
stance in data sharing, hosting run data in a dedicated repository. However, this data is specific to253
select experiments and often presented in non-standard, undocumented formats, complicating its254
use. Leading effective data-sharing platforms include Dopamine (Castro et al., 2018) and Sample255
Factory (Petrenko et al., 2020). Dopamine consistently provides accessible raw evaluation data for256
various seeds and visualizations, along with trained agents on Google Cloud. Sample Factory offers257
comprehensive data via Weights and Biases (Biewald, 2020) and a selection of pre-trained agents on258
the Hugging Face Hub, enhancing reproducibility and collaborative research efforts.259
4.3 Review on reproducibility260
The literature shows variations in these practices. Some older publications like (Schulman et al.,261
2015, 2017; Bellemare et al., 2013; Mnih et al., 2016; Hessel et al., 2018) and even recent ones262
like (Reed et al., 2022) lack a codebase but provide detailed descriptions for replication6. However,263
challenges arise because certain hyperparameters, important but often unreported, can significantly264
affect performance (Andrychowicz et al., 2020). In addition, implementation choices have proven to265
be critical (Henderson et al., 2018; Huang et al., 2023, 2022a; Engstrom et al., 2020), complicating266
the distinction between implementation-based improvements and methodological advances.267
Recognizing these challenges, the RL community is advocating for higher standards. NeurIPS, for268
instance, has been requesting a reproduction checklist since 2019 (Pineau et al., 2021). Recent269
efforts focus on systematic sharing of source code to promote reproducibility. However, codebases270
are often left unmaintained post-publication (with rare exceptions (Fujimoto et al., 2018)), creating271
complexity for users dealing with various dependencies and unsolved issues. To address these272
challenges, libraries have aggregated multiple baseline implementations (see Section 2.1), aiming273
to match reported paper performance. However, long-term sustainability remains a concern. While274
these libraries enhance reproducibility, in-depth repeatability is still rare.275
5 Discussion and conclusion276
Reproducing results in RL research is often difficult due to limited access to data and code, as well277
as the impact of minor implementation variations on performance. Researchers typically rely on278
imprecise comparisons with paper figures, making the reproduction process time-consuming and279
challenging. To address these issues, we introduce ORLB , a large collection of tracked experiments280
spanning various algorithms, libraries and benchmarks. ORLB records all relevant metrics and281
data points, offering detailed resources for precise reproduction. This tool facilitates access to282
comprehensive datasets, simplifies the extraction of valuable information, enables metric comparisons,283
and provides a CLI for easier data access and visualization. As a dynamic resource,ORLB is regularly284
updated by both its maintainers and the user community, gradually improving the reliability of the285
available results.286
Despite its strengths, ORLB faces challenges in user-friendliness that need to be addressed. Incon-287
sistencies between libraries in evaluation strategies and terminology can make it difficult for users.288
Scaling community engagement becomes a challenge with more members, libraries, and runs. The289
lack of Git-like version tracking for runs adds to these limitations.290
ORLB is a major step forward in addressing the needs of RL research. It offers a comprehensive,291
accessible, and collaborative experiment database, enabling precise comparisons and analysis. It292
improves data access and promotes a deeper understanding of algorithmic performance. While293
challenges remain, ORLB has the potential to raise the standard of RL research.294
6This section uses the taxonomy introduced by Lynnerup et al. (2019): repeatability means accurately
duplicating an experiment with source code and random seed availability,reproducibility involves redoing an
experiment using an existing codebase, and replicability aims to achieve similar results independently through
algorithm implementation.
9

Affiliations295
1Hugging Face296
2Drexel University297
3Univ. Lyon, Centrale Lyon, CNRS, INSA Lyon, UCBL, LIRIS, UMR 5205298
4SnT, University of Luxembourg299
5German Aerospace Center (DLR) RMC, Weßling, Germany300
6Graduate School of System Informatics, Kobe University, Hyogo, Japan301
7School of Computer Science and Technology, University of Chinese Academy of Sciences302
8Chengdu Institute of Computer Applications, Chinese Academy of Sciences303
9University of Maryland, College Park304
10NVIDIA305
11Snap Inc.306
12School of Computer Science, McGill University307
13Polytech Montpellier DO308
14Zhejiang University309
15Department of Computer Science, Purdue University310
16Work done while at Cohere311
17Chinese University of Hong Kong, Shenzhen312
18University College London313
19Agency for Science, Technology and Research314
20Faculty of Computer Science, University of Vienna, Vienna, Austria315
21UniVie Doctoral School Computer Science, University of Vienna316
22Vidyasirimedhi Institute of Science and Technology (VISTEC)317
23University of Southampton318
24Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 – CRIStAL319
25Saint-Gobain Research Paris320
26International Institute of Information Technology, Hyderabad, India321
27AIcrowd SA322
28Valeo Driving Assistance Research323
29New York University324
30Sea AI Lab325
31Institute of Informatics, Federal University of Rio Grande do Sul326
32AIRI327
33Department of Automation, Tsinghua University328
34University of Basel329
35University of Michigan330
36UC Berkley331
Acknowledgments332
This work has been supported by a highly committed RL community. We have listed all the333
contributors to date, and would like to thank all future contributors and users in advance.334
This work was granted access to the HPC resources of IDRIS under the allocation 2022-335
[AD011012172R1] made by GENCI. The MORL-Baselines experiments have been conducted on the336
HPCs of the University of Luxembourg, and of the Vrije Universiteit Brussel. This work was partly337
supported by the National Key Research and Development Program of China (2023YFB3308601),338
Science and Technology Service Network Initiative (KFJ-STS-QYZD-2021-21-001), the Talents by339
Sichuan provincial Party Committee Organization Department, and Chengdu - Chinese Academy340
of Sciences Science and Technology Cooperation Fund Project (Major Scientific and Technological341
Innovation Projects). Some experiments are conducted at Stability AI and Hugging Face’s cluster.342
10

References343
Joshua Achiam. Spinning Up in Deep Reinforcement Learning. https://github.com/openai/344
spinningup, 2018. URL https://github.com/openai/spinningup.345
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C. Courville, and Marc G. Bellemare.346
Deep Reinforcement Learning at the Edge of the Statistical Precipice. In Marc’Aurelio Ranzato,347
Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Ad-348
vances in Neural Information Processing Systems 34: Annual Conference on Neural Information349
Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 29304–29320, 2021.350
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C. Courville, and Marc G. Belle-351
mare. Reincarnating Reinforcement Learning: Reusing Prior Computation to Accelerate Progress.352
In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.),353
Advances in Neural Information Processing Systems 35: Annual Conference on Neural Infor-354
mation Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - De-355
cember 9, 2022 , 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/356
ba1c5356d9164bb64c446a4b690226b0-Abstract-Conference.html.357
Lucas N. Alegre, Florian Felten, El-Ghazali Talbi, Grégoire Danoy, Ann Nowé, Ana L. C. Bazzan, and358
Bruno C. da Silva. MO-Gym: A Library of Multi-Objective Reinforcement Learning Environments.359
In Proceedings of the 34th Benelux Conference on Artificial Intelligence BNAIC/Benelearn 2022,360
2022.361
Marcin Andrychowicz, Anton Raichuk, Piotr Stanczyk, Manu Orsini, Sertan Girgin, Raphaël Marinier,362
Léonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly, and Olivier363
Bachem. What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study.364
arXiv preprint arXiv:2006.05990, 2020.365
Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi,366
Zhaohan Daniel Guo, and Charles Blundell. Agent57: Outperforming the Atari Human Benchmark.367
In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18368
July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 507–517.369
PMLR, 2020a. URL http://proceedings.mlr.press/v119/badia20a.html.370
Adrià Puigdomènech Badia, Pablo Sprechmann, Alex Vitvitskyi, Zhaohan Daniel Guo, Bilal Piot,371
Steven Kapturowski, Olivier Tieleman, Martín Arjovsky, Alexander Pritzel, Andrew Bolt, and372
Charles Blundell. Never Give Up: Learning Directed Exploration Strategies. In 8th International373
Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.374
OpenReview.net, 2020b. URL https://openreview.net/forum?id=Sye57xStvB.375
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning376
Environment: An Evaluation Platform for General Agents. Journal of Artificial Intelligence377
Research, 47:253–279, 2013. doi: 10.1613/JAIR.3912. URL https://doi.org/10.1613/378
jair.3912.379
Lukas Biewald. Experiment Tracking with Weights and Biases, 2020. URL https://www.wandb.380
com/. Software available from wandb.com.381
Albert Bou, Matteo Bettini, Sebastian Dittert, Vikash Kumar, Shagun Sodhani, Xiaomeng Yang,382
Gianni De Fabritiis, and Vincent Moens. TorchRL: A Data-Driven Decision-Making Library for383
Pytorch. arXiv preprint arXiv:2306.00577, 2023.384
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and385
Wojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016.386
Yuri Burda, Harrison Edwards, Amos J. Storkey, and Oleg Klimov. Exploration by random network387
distillation. In 7th International Conference on Learning Representations, ICLR 2019, New388
Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/389
forum?id=H1lJJnR5Ym.390
11

Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Belle-391
mare. Dopamine: A Research Framework for Deep Reinforcement Learning. arXiv preprint392
arXiv:1812.06110, 2018.393
Xinyue Chen, Che Wang, Zijian Zhou, and Keith W. Ross. Randomized Ensembled Double Q-394
Learning: Learning Fast Without a Model. In 9th International Conference on Learning Rep-395
resentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. URL396
https://openreview.net/forum?id=AY8zfZm0tDd.397
Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem398
Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & Miniworld: Modular &399
Customizable Reinforcement Learning Environments for Goal-Oriented Tasks. arXiv preprint400
arXiv:2306.13831, 2023.401
Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging Procedural Generation402
to Benchmark Reinforcement Learning. In Proceedings of the 37th International Conference on403
Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of404
Machine Learning Research, pp. 2048–2056. PMLR, 2020. URL http://proceedings.mlr.405
press/v119/cobbe20a.html.406
Karl Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman. Phasic Policy Gradient. In Marina407
Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine408
Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine409
Learning Research, pp. 2020–2027. PMLR, 2021. URL http://proceedings.mlr.press/410
v139/cobbe21a.html.411
Erwin Coumans and Yunfei Bai. PyBullet, a Python Module for Physics Simulation for Games,412
Robotics and Machine Learning. 2016.413
Will Dabney, Georg Ostrovski, David Silver, and Rémi Munos. Implicit Quantile Networks for414
Distributional Reinforcement Learning. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings415
of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan,416
Stockholm, Sweden, July 10-15, 2018, volume 80 ofProceedings of Machine Learning Research, pp.417
1104–1113. PMLR, 2018. URL http://proceedings.mlr.press/v80/dabney18a.html.418
Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. MushroomRL:419
Simplifying Reinforcement Learning Research. Journal of Machine Learning Research, 22(131):420
1–5, 2021. URL http://jmlr.org/papers/v22/18-056.html.421
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,422
John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. OpenAI Baselines. https://423
github.com/openai/baselines, 2017. URL https://github.com/openai/baselines.424
Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. First Return,425
Then Explore. Nature, 590(7847):580–586, 2021. doi: 10.1038/S41586-020-03157-9. URL426
https://doi.org/10.1038/s41586-020-03157-9 .427
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph,428
and Aleksander Madry. Implementation Matters in Deep RL: A Case Study on PPO and429
TRPO. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,430
Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?431
id=r1etN1rtPB.432
Lasse Espeholt, Hubert Soyer, Rémi Munos, Karen Simonyan, V olodymyr Mnih, Tom Ward, Yotam433
Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA:434
Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures. In Jen-435
nifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on436
12

Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, vol-437
ume 80 of Proceedings of Machine Learning Research , pp. 1406–1415. PMLR, 2018. URL438
http://proceedings.mlr.press/v80/espeholt18a.html.439
Florian Felten, Lucas Nunes Alegre, Ann Nowe, Ana L. C. Bazzan, El Ghazali Talbi, Grégoire440
Danoy, and Bruno Castro da Silva. A Toolkit for Reliable Benchmarking and Research in Multi-441
Objective Reinforcement Learning. In Proceedings of the Neural Information Processing Systems442
Track on Datasets and Benchmarks 3, NeurIPS Datasets and Benchmarks 2023 , 2023. URL443
https://openreview.net/forum?id=jfwRLudQyj.444
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing Function Approximation Error445
in Actor-Critic Methods. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the446
35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm,447
Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1582–448
1591. PMLR, 2018. URL http://proceedings.mlr.press/v80/fujimoto18a.html.449
Yasuhiro Fujita, Prabhat Nagarajan, Toshiki Kataoka, and Takahiro Ishikawa. ChainerRL: A Deep450
Reinforcement Learning Library. Journal of Machine Learning Research, 22(77):1–14, 2021.451
URL http://jmlr.org/papers/v22/20-376.html.452
Quentin Gallouédec, Nicolas Cazin, Emmanuel Dellandréa, and Liming Chen. panda-gym: Open-453
Source Goal-Conditioned Environments for Robotic Learning. 4th Robot Learning Workshop:454
Self-Supervised and Lifelong Learning at NeurIPS, 2021.455
The garage contributors. Garage: A toolkit for reproducible reinforcement learning research. https:456
//github.com/rlworkgroup/garage, 2019.457
Sergio Guadarrama, Anoop Korattikara, Oscar Ramirez, Pablo Castro, Ethan Holly, Sam Fishman,458
Ke Wang, Ekaterina Gonina, Neal Wu, Efi Kokiopoulou, Luciano Sbaiz, Jamie Smith, Gábor459
Bartók, Jesse Berent, Chris Harris, Vincent Vanhoucke, and Eugene Brevdo. TF-Agents: A library460
for Reinforcement Learning in TensorFlow.https://github.com/tensorflow/agents, 2018.461
URL https://github.com/tensorflow/agents.462
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-Policy463
Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. In Jennifer G. Dy and464
Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,465
ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings466
of Machine Learning Research, pp. 1856–1865. PMLR, 2018. URL http://proceedings.mlr.467
press/v80/haarnoja18b.html.468
Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering Diverse Domains469
through World Models. arXiv preprint arXiv:2301.04104, 2023.470
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.471
Deep Reinforcement Learning That Matters. In Sheila A. McIlraith and Kilian Q. Weinberger472
(eds.), Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18),473
the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium474
on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA,475
February 2-7, 2018, pp. 3207–3214. AAAI Press, 2018. doi: 10.1609/AAAI.V32I1.11694. URL476
https://doi.org/10.1609/aaai.v32i1.11694.477
Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney,478
Dan Horgan, Bilal Piot, Mohammad Gheshlaghi Azar, and David Silver. Rainbow: Combining479
Improvements in Deep Reinforcement Learning. In Sheila A. McIlraith and Kilian Q. Weinberger480
(eds.), Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18),481
the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium482
on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA,483
February 2-7, 2018, pp. 3215–3222. AAAI Press, 2018. doi: 10.1609/AAAI.V32I1.11796. URL484
https://doi.org/10.1609/aaai.v32i1.11796.485
13

Matthew W. Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Nikola Momchev,486
Danila Sinopalnikov, Piotr Sta´nczyk, Sabela Ramos, Anton Raichuk, Damien Vincent, Léonard487
Hussenot, Robert Dadashi, Gabriel Dulac-Arnold, Manu Orsini, Alexis Jacq, Johan Ferret, Nino488
Vieillard, Seyed Kamyar Seyed Ghasemipour, Sertan Girgin, Olivier Pietquin, Feryal Behbahani,489
Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson,490
Abe Friesen, Ruba Haroun, Alex Novikov, Sergio Gómez Colmenarejo, Serkan Cabi, Caglar491
Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Andrew Cowie, Ziyu Wang, Bilal Piot, and Nando492
de Freitas. Acme: A Research Framework for Distributed Reinforcement Learning. arXiv preprint493
arXiv:2006.00979, 2020.494
Shengyi Huang, Rousslan Fernand Julien Dossa, Antonin Raffin, Anssi Kanervisto, and495
Weixun Wang. The 37 Implementation Details of Proximal Policy Optimization.496
In ICLR Blog Track , 2022a. URL https://iclr-blog-track.github.io/2022/497
03/25/ppo-implementation-details/. https://iclr-blog-track.github.io/2022/03/25/ppo-498
implementation-details/.499
Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal500
Mehta, and João G.M. Araújo. CleanRL: High-quality Single-file Implementations of Deep501
Reinforcement Learning Algorithms. Journal of Machine Learning Research , 23(274):1–18,502
2022b. URL http://jmlr.org/papers/v23/21-1342.html.503
Shengyi Huang, Jiayi Weng, Rujikorn Charakorn, Min Lin, Zhongwen Xu, and Santiago Ontañón.504
Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform, 2023.505
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to Trust Your Model:506
Model-Based Policy Optimization. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelz-507
imer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neu-508
ral Information Processing Systems 32: Annual Conference on Neural Information Pro-509
cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada ,510
pp. 12498–12509, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/511
5faf461eff3099671ad63c6f3f094f7f-Abstract.html.512
Ilya Kostrikov. JAXRL: Implementations of Reinforcement Learning algorithms in JAX. https:513
//github.com/ikostrikov/jaxrl, Oct 2021. URL https://github.com/ikostrikov/514
jaxrl.515
Alexander Kuhnle, Michael Schaarschmidt, and Kai Fricke. Tensorforce: a TensorFlow library516
for applied reinforcement learning. https://github.com/tensorforce/tensorforce, 2017.517
URL https://github.com/tensorforce/tensorforce.518
Heinrich Küttler, Nantas Nardelli, Thibaut Lavril, Marco Selvatici, Viswanath Sivakumar, Tim519
Rocktäschel, and Edward Grefenstette. TorchBeast: A PyTorch Platform for Distributed RL. arXiv520
preprint arXiv:1910.03552, 2019.521
Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman, Sergio Guadarrama, Ian522
Fischer, Winnie Xu, Eric Jang, Henryk Michalewski, and Igor Mordatch. Multi-Game Decision523
Transformers. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh524
(eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural525
Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 -526
December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/527
b2cac94f82928a85055987d9fd44753f-Abstract-Conference.html.528
Edouard Leurent. An Environment for Autonomous Driving Decision-Making. https://github.529
com/eleurent/highway-env, 2018. URL https://github.com/eleurent/highway-env.530
Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph531
Gonzalez, Michael I. Jordan, and Ion Stoica. RLlib: Abstractions for Distributed Reinforcement532
Learning. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International533
14

Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15,534
2018, volume 80 of Proceedings of Machine Learning Research, pp. 3059–3068. PMLR, 2018.535
URL http://proceedings.mlr.press/v80/liang18b.html.536
Nicolai A. Lynnerup, Laura Nolling, Rasmus Hasle, and John Hallam. A Survey on Repro-537
ducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots. In538
Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura (eds.), 3rd Annual Conference on539
Robot Learning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings, vol-540
ume 100 of Proceedings of Machine Learning Research , pp. 466–489. PMLR, 2019. URL541
http://proceedings.mlr.press/v100/lynnerup20a.html.542
Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew J. Hausknecht, and543
Michael Bowling. Revisiting the Arcade Learning Environment: Evaluation Protocols and Open544
Problems for General Agents. Journal of Artificial Intelligence Research, 61:523–562, 2018. doi:545
10.1613/JAIR.5699. URL https://doi.org/10.1613/jair.5699.546
Denys Makoviichuk and Viktor Makoviychuk. rl-games: A High-performance Framework for547
Reinforcement Learning. https://github.com/Denys88/rl_games, May 2021. URL https:548
//github.com/Denys88/rl_games.549
Vegard Mella, Eric Hambro, Danielle Rothermel, and Heinrich Küttler. moolib: A Platform for550
Distributed RL. GitHub repository, 2022. URL https://github.com/facebookresearch/551
moolib.552
V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan553
Wierstra, and Martin A. Riedmiller. Playing Atari with Deep Reinforcement Learning. arXiv554
preprint arXiv:1312.5602, 2013.555
V olodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim556
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous Methods for Deep Reinforcement557
Learning. In Maria-Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of the 33nd558
International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24,559
2016, volume 48 of JMLR Workshop and Conference Proceedings, pp. 1928–1937. JMLR.org,560
2016. URL http://proceedings.mlr.press/v48/mniha16.html.561
Andrew Patterson, Samuel Neumann, Martha White, and Adam White. Empirical Design in Rein-562
forcement Learning. arXiv preprint arXiv:2304.01315, 2023.563
Aleksei Petrenko, Zhehui Huang, Tushar Kumar, Gaurav S. Sukhatme, and Vladlen Koltun. Sample564
Factory: Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement565
Learning. In Proceedings of the 37th International Conference on Machine Learning, ICML566
2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research,567
pp. 7652–7662. PMLR, 2020. URL http://proceedings.mlr.press/v119/petrenko20a.568
html.569
Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer,570
Florence d’Alché-Buc, Emily B. Fox, and Hugo Larochelle. Improving Reproducibility in Machine571
Learning Research (A Report from the NeurIPS 2019 Reproducibility Program). Journal of572
Machine Learning Research, 22:164:1–164:20, 2021. URL http://jmlr.org/papers/v22/573
20-303.html.574
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell,575
Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, and Wojciech576
Zaremba. Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request577
for Research. arXiv preprint arXiv:1802.09464, 2018.578
Antonin Raffin. RL Baselines3 Zoo. https://github.com/DLR-RM/rl-baselines3-zoo , 2020.579
15

Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah580
Dormann. Stable-Baselines3: Reliable Reinforcement Learning Implementations. Journal of581
Machine Learning Research, 22(268):1–8, 2021.582
Scott E. Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov,583
Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom584
Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol585
Vinyals, Mahyar Bordbar, and Nando de Freitas. A Generalist Agent. Transactions on Machine586
Learning Research, 2022, 2022. URL https://openreview.net/forum?id=1ikK0kHjvj.587
John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust Region588
Policy Optimization. In Francis R. Bach and David M. Blei (eds.), Proceedings of the 32nd589
International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 ,590
volume 37 of JMLR Workshop and Conference Proceedings, pp. 1889–1897. JMLR.org, 2015.591
URL http://proceedings.mlr.press/v37/schulman15.html.592
John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-593
Dimensional Continuous Control Using Generalized Advantage Estimation. In Yoshua Ben-594
gio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR595
2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings , 2016. URL596
http://arxiv.org/abs/1506.02438.597
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy598
Optimization Algorithms. arXiv preprint arXiv:1707.06347, 2017.599
Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A Physics Engine for Model-Based600
Control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS601
2012, Vilamoura, Algarve, Portugal, October 7-12, 2012, pp. 5026–5033. IEEE, 2012.602
Marin Toromanoff, Émilie Wirbel, and Fabien Moutarde. Is Deep Reinforcement Learning Really603
Superhuman on Atari? arXiv preprint arXiv:1908.04683, 2019.604
Mark Towers, Jordan K. Terry, Ariel Kwiatkowski, John U. Balis, Gianluca de Cola, Tristan Deleu,605
Manuel Goulão, Andreas Kallinteris, Arjun KG, Markus Krimmel, Rodrigo Perez-Vicente, Andrea606
Pierré, Sander Schulhoff, Jun Jet Tai, Andrew Tan Jin Shen, and Omar G. Younis. Gymnasium,607
March 2023. URL https://zenodo.org/record/8127025.608
Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang Su,609
and Jun Zhu. Tianshou: A Highly Modularized Deep Reinforcement Learning Library. Journal610
of Machine Learning Research, 23(267):1–6, 2022a. URL http://jmlr.org/papers/v23/611
21-1127.html.612
Jiayi Weng, Min Lin, Shengyi Huang, Bo Liu, Denys Makoviichuk, Viktor Makoviychuk, Zichen613
Liu, Yufan Song, Ting Luo, Yukun Jiang, Zhongwen Xu, and Shuicheng Yan. EnvPool: A Highly614
Parallel Reinforcement Learning Environment Execution Engine. In Proceedings of the Neural615
Information Processing Systems Track on Datasets and Benchmarks 2, NeurIPS Datasets and616
Benchmarks 2022, 2022b. URL http://papers.nips.cc/paper_files/paper/2022/hash/617
8caaf08e49ddbad6694fae067442ee21-Abstract-Datasets_and_Benchmarks.html.618
Yanxiao Zhao. abcdRL: Modular Single-file Reinforcement Learning Algorithms Library. https:619
//github.com/sdpkjc/abcdrl, December 2022. URL https://github.com/sdpkjc/620
abcdrl.621
Checklist622
1. For all authors...623
16

(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s624
contributions and scope? [Yes]625
(b) Did you describe the limitations of your work? [Yes] see Section 5.626
(c) Did you discuss any potential negative societal impacts of your work? [No]627
(d) Have you read the ethics review guidelines and ensured that your paper conforms to628
them? [Yes]629
2. If you are including theoretical results...630
(a) Did you state the full set of assumptions of all theoretical results? [N/A]631
(b) Did you include complete proofs of all theoretical results? [N/A]632
3. If you ran experiments (e.g. for benchmarks)...633
(a) Did you include the code, data, and instructions needed to reproduce the main experi-634
mental results (either in the supplemental material or as a URL)? [Yes] The paper deals635
specifically with new ways of sharing experimental results to improve reproducibility.636
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they637
were chosen)? [Yes]638
(c) Did you report error bars (e.g., with respect to the random seed after running experi-639
ments multiple times)? [Yes]640
(d) Did you include the total amount of compute and the type of resources used (e.g., type641
of GPUs, internal cluster, or cloud provider)? [Yes]642
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...643
(a) If your work uses existing assets, did you cite the creators? [Yes]644
(b) Did you mention the license of the assets? [Yes]645
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]646
Each experiment on ORLB is carefully linked to the source code needed to produce it.647
(d) Did you discuss whether and how consent was obtained from people whose data you’re648
using/curating? [N/A] Data is collected by proactive contributors649
(e) Did you discuss whether the data you are using/curating contains personally identifiable650
information or offensive content? [N/A]651
5. If you used crowdsourcing or conducted research with human subjects...652
(a) Did you include the full text of instructions given to participants and screenshots, if653
applicable? [N/A]654
(b) Did you describe any potential participant risks, with links to Institutional Review655
Board (IRB) approvals, if applicable? [N/A]656
(c) Did you include the estimated hourly wage paid to participants and the total amount657
spent on participant compensation? [N/A]658
17

A Plotting results guidelines659
A.1 Using the CLI660
This section gives notable additional examples of usage of the provided CLI. A more comprehensive661
set of examples and manual is available in the README page of the project.662
A.1.1 Plotting episodic return from various libraries663
First, we showcase the most basic usage of the CLI, that is comparing two different implementations664
of the same algorithm based on learning curve of episodic return. For example, Figure 8 and 9665
compare CleanRL’s TD3 implementation against the original TD3, both in terms of sample efficiency666
and time. The command used to generate this plot is listed below.667
python -m openrlbenchmark.rlops \668
--filters ’?we=openrlbenchmark&wpn=sfujim-TD3&ceik=env&cen=policy&metric=charts/episodic_return’ ’TD3?669
cl=Official TD3’ \670
--filters ’?we=openrlbenchmark&wpn=cleanrl&ceik=env_id&cen=exp_name&metric=charts/episodic_return’ ’671
td3_continuous_action_jax?cl=Clean RL TD3’ \672
--env-ids HalfCheetah-v2 Walker2d-v2 Hopper-v2 \673
--pc.ncols 3 \674
--pc.ncols-legend 2 \675
--output-filename static/td3_vs_cleanrl \676
--scan-history677
In the above command,wpn denotes the project name, typically the learning library name. This allows678
to fetch results of implementations from different projects. Moreover, it is possible to specify which679
metric to compare, in this case charts/episodic_return. Also, the CLI provides the possibility680
to select a given algorithm and apply a different name in the plot, e.g. we rename TD3 to Official681
TD3 and td3_continuous_action_jax to Clean RL TD3 . Finally, we can also select a set of682
environments through the --end-ids option.683
0K 200K 400K 600K 800K 1000K
0
2000
4000
6000
8000
10000
HalfCheetah-v2
0K 200K 400K 600K 800K 1000K
0
1000
2000
3000
4000
Walker2d-v2
0K 200K 400K 600K 800K 1000K
0
1000
2000
3000
Hopper-v2
Steps
Episodic Return
Official TD3 Clean RL TD3
Figure 8: Comparing CleanRL’s TD3 against the original TD3 implementation (sample efficiency).
0 20 40 60
0
2500
5000
7500
10000
HalfCheetah-v2
0 20 40 60 80
0
1000
2000
3000
4000
Walker2d-v2
0 20 40 60
0
1000
2000
3000
4000
Hopper-v2
Time (m)
Episodic Return
Official TD3 Clean RL TD3
Figure 9: Comparing CleanRL’s TD3 against the original TD3 implementation (time).
18

A.1.2 RLiable integration684
ORLB also integrates with RLiable (Agarwal et al., 2021). To enable such plot, the option--rliable685
can be toggled, then additional parameters are available under --rc. Figures 10, 11, 12, 13 showcase686
the resulting plots of the following command:687
python -m openrlbenchmark.rlops \688
--filters ’?we=openrlbenchmark&wpn=baselines&ceik=env&cen=exp_name&metric=charts/episodic_return’ ’689
baselines-ppo2-cnn?cl=OpenAI Baselines PPO2’ \690
--filters ’?we=openrlbenchmark&wpn=envpool-atari&ceik=env_id&cen=exp_name&metric=charts/691
avg_episodic_return’ ’ppo_atari_envpool_xla_jax_truncation?cl=CleanRL PPO’ \692
--env-ids AlienNoFrameskip-v4 AmidarNoFrameskip-v4 AssaultNoFrameskip-v4 AsterixNoFrameskip-v4693
AsteroidsNoFrameskip-v4 AtlantisNoFrameskip-v4 BankHeistNoFrameskip-v4 BattleZoneNoFrameskip-v4694
BeamRiderNoFrameskip-v4 BerzerkNoFrameskip-v4 BowlingNoFrameskip-v4 BoxingNoFrameskip-v4695
BreakoutNoFrameskip-v4 CentipedeNoFrameskip-v4 ChopperCommandNoFrameskip-v4696
CrazyClimberNoFrameskip-v4 DefenderNoFrameskip-v4 DemonAttackNoFrameskip-v4 DoubleDunkNoFrameskip-697
v4 EnduroNoFrameskip-v4 FishingDerbyNoFrameskip-v4 FreewayNoFrameskip-v4 FrostbiteNoFrameskip-v4698
GopherNoFrameskip-v4 GravitarNoFrameskip-v4 HeroNoFrameskip-v4 IceHockeyNoFrameskip-v4699
PrivateEyeNoFrameskip-v4 QbertNoFrameskip-v4 RiverraidNoFrameskip-v4 RoadRunnerNoFrameskip-v4700
RobotankNoFrameskip-v4 SeaquestNoFrameskip-v4 SkiingNoFrameskip-v4 SolarisNoFrameskip-v4701
SpaceInvadersNoFrameskip-v4 StarGunnerNoFrameskip-v4 SurroundNoFrameskip-v4 TennisNoFrameskip-v4702
TimePilotNoFrameskip-v4 TutankhamNoFrameskip-v4 UpNDownNoFrameskip-v4 VentureNoFrameskip-v4703
VideoPinballNoFrameskip-v4 WizardOfWorNoFrameskip-v4 YarsRevengeNoFrameskip-v4 ZaxxonNoFrameskip-704
v4 JamesbondNoFrameskip-v4 KangarooNoFrameskip-v4 KrullNoFrameskip-v4 KungFuMasterNoFrameskip-v4705
MontezumaRevengeNoFrameskip-v4 MsPacmanNoFrameskip-v4 NameThisGameNoFrameskip-v4706
PhoenixNoFrameskip-v4 PitfallNoFrameskip-v4 PongNoFrameskip-v4 \707
--env-ids Alien-v5 Amidar-v5 Assault-v5 Asterix-v5 Asteroids-v5 Atlantis-v5 BankHeist-v5 BattleZone-v5708
BeamRider-v5 Berzerk-v5 Bowling-v5 Boxing-v5 Breakout-v5 Centipede-v5 ChopperCommand-v5709
CrazyClimber-v5 Defender-v5 DemonAttack-v5 DoubleDunk-v5 Enduro-v5 FishingDerby-v5 Freeway-v5710
Frostbite-v5 Gopher-v5 Gravitar-v5 Hero-v5 IceHockey-v5 PrivateEye-v5 Qbert-v5 Riverraid-v5711
RoadRunner-v5 Robotank-v5 Seaquest-v5 Skiing-v5 Solaris-v5 SpaceInvaders-v5 StarGunner-v5712
Surround-v5 Tennis-v5 TimePilot-v5 Tutankham-v5 UpNDown-v5 Venture-v5 VideoPinball-v5 WizardOfWor-713
v5 YarsRevenge-v5 Zaxxon-v5 Jamesbond-v5 Kangaroo-v5 Krull-v5 KungFuMaster-v5 MontezumaRevenge-v5714
MsPacman-v5 NameThisGame-v5 Phoenix-v5 Pitfall-v5 Pong-v5 \715
--no-check-empty-runs \716
--pc.ncols 5 \717
--pc.ncols-legend 2 \718
--rliable \719
--rc.score_normalization_method atari \720
--rc.normalized_score_threshold 8.0 \721
--rc.sample_efficiency_plots \722
--rc.sample_efficiency_and_walltime_efficiency_method Median \723
--rc.performance_profile_plots \724
--rc.aggregate_metrics_plots \725
--rc.sample_efficiency_num_bootstrap_reps 50000 \726
--rc.performance_profile_num_bootstrap_reps 2000 \727
--rc.interval_estimates_num_bootstrap_reps 2000 \728
--output-filename static/cleanrl_vs_baselines_atari \729
--scan-history730
0.75 0.90 1.05
CleanRL PPO
OpenAI Baselines PPO2
Median
0.88 0.96 1.04
Normalized Score
IQM
5.6 6.4 7.2 8.0
Mean
0.350 0.375 0.400
Optimality Gap
Figure 10: Clean RL PPO vs. OpenAI Baselines PPO, normalized score (RLiable).
0 1 2 3 4 5 6 7 8
Normalized Score ( )
0.00
0.25
0.50
0.75
1.00Fraction of runs with score >
0 1 2 3 4 5 6 7 8
Normalized Score ( )
0.00
0.25
0.50
0.75
1.00Fraction of tasks with score > 
OpenAI Baselines PPO2 CleanRL PPO
Figure 11: Clean RL PPO vs. OpenAI Baselines PPO, performance profile (RLiable).
19

0M 2M 4M 6M 8M
0.00
0.25
0.50
0.75
1.00Median
0M 2M 4M 6M 8M
0.00
0.25
0.50
0.75
1.00IQM
0M 2M 4M 6M 8M
0
2
4
6
8Mean
0M 2M 4M 6M 8M
0.4
0.6
0.8
1.0
1.2Optimality Gap
Steps
OpenAI Baselines PPO2 CleanRL PPO
Figure 12: Clean RL PPO vs. OpenAI Baselines PPO, sample efficiency (RLiable).
0M 2M 4M 6M 8M
Steps
0.00
0.25
0.50
0.75
1.00Median
0 20 40 60 80 100 120 140
Time (m)
OpenAI Baselines PPO2 CleanRL PPO
Figure 13: Clean RL PPO vs. OpenAI Baselines PPO, walltime efficiency (RLiable).
20

A.1.3 Multi-metrics731
Sometimes, such as in multi-objective RL (MORL), it is useful to report multiple metrics in the paper.732
Hence, the CLI includes an option to plot multiple metrics. Below is an example of CLI and resulting733
plots (Figure 14) for multiple MORL algorithms on different environments.734
python -m openrlbenchmark.rlops_multi_metrics \735
--filters ’?we=openrlbenchmark&wpn=MORL-Baselines&ceik=env_id&cen=algo&metrics=eval/hypervolume&metrics=736
eval/igd&metrics=eval/sparsity&metrics=eval/mul’ \737
’Pareto Q-Learning?cl=Pareto Q-Learning’ \738
’MultiPolicy MO Q-Learning?cl=MPMOQL’ \739
’MultiPolicy MO Q-Learning (OLS)?cl=MPMOQL (OLS)’ \740
’MultiPolicy MO Q-Learning (GPI-LS)?cl=MPMOQL (GPI-LS)’ \741
--env-ids deep-sea-treasure-v0 deep-sea-treasure-concave-v0 fruit-tree-v0 \742
--pc.ncols 3 \743
--pc.ncols-legend 4 \744
--pc.xlabel ’Training steps’ \745
--pc.ylabel ’’ \746
--pc.max_steps 400000 \747
--output-filename morl/morl_deterministic_envs \748
--scan-history749
0M 0.1M 0.2M 0.3M 0.4M
400
500
600
700
800hypervolume
deep-sea-treasure-v0
0M 0.1M 0.2M 0.3M 0.4M
1000
1500
2000
2500
3000
3500
deep-sea-treasure-concave-v0
0M 0.1M 0.2M 0.3M 0.4M
0
10000
20000
30000
40000
fruit-tree-v0
0M 0.1M 0.2M 0.3M 0.4M
0
2
4
6
8
10igd
0M 0.1M 0.2M 0.3M 0.4M
0
20
40
60
80
0M 0.1M 0.2M 0.3M 0.4M
0
2
4
6
8
0M 0.1M 0.2M 0.3M 0.4M
0
100
200
300
400sparsity
0M 0.1M 0.2M 0.3M 0.4M
2500
0
2500
5000
7500
10000
12500
0M 0.1M 0.2M 0.3M 0.4M
0
20
40
60
80
100
120
0M 0.1M 0.2M 0.3M 0.4M
0
2
4
6
8
10
12
14mul
0M 0.1M 0.2M 0.3M 0.4M
0
20
40
60
80
0M 0.1M 0.2M 0.3M 0.4M
0
2
4
6
8
Training steps
Pareto Q-Learning MPMOQL MPMOQL (OLS) MPMOQL (GPI-LS)
Figure 14: Plotting different metrics for different environments.
21

A.2 Using a custom script750
Our CLI proves highly beneficial for generating standard RL plots, as demonstrated above. Neverthe-751
less, in certain specialized cases, researchers may wish to expose the data in an alternative format.752
Fortunately, all the data hosted in ORLB is accessible through the Weights and Biases API. The753
following example illustrates how this API can be utilized. From there, researchers can employ any754
custom script for plotting this data to suit their specific needs. A simple example of such a script is755
given below, and the corresponding generated plot is shown in Figure 15.756
import matplotlib.pyplot as plt757
import wandb758
759
project_name = "sb3"760
run_id = "0a1kqgev"761
762
api = wandb.Api()763
run = api.run(f"openrlbenchmark/{project_name}/{run_id}")764
history = run.history(keys=["global_step", "eval/mean_reward"])765
plt.plot(history["global_step"], history["eval/mean_reward"])766
plt.title(run.name)767
plt.savefig("custom_plot.png")768
Figure 15: Example of a plot created with a custom script, by importing data directly from ORLB
using the WandB API.
22

B Additional details for the case study769
This appendix gives additional results related to the first case study presented in Section 3.1. Figure770
17 shows the results by environment for the Atari benchmark, and Figure 16 shows them for the771
MuJoCo and Box2d benchmarks. The command lines used to generate these figures are as follows.772
python -m openrlbenchmark.rlops \773
--filters ’?we=openrlbenchmark&wpn=sb3&ceik=env&cen=algo&metric=eval/mean_reward’ ’ppo?cl=PPO’ \774
--filters ’?we=modanesh&wpn=openrlbenchmark&ceik=env&cen=algo&metric=eval/mean_reward’ ’ppo?cl=PPO w/775
MC for value estimation’ \776
--env-ids BreakoutNoFrameskip-v4 SpaceInvadersNoFrameskip-v4 SeaquestNoFrameskip-v4 EnduroNoFrameskip-777
v4 PongNoFrameskip-v4 QbertNoFrameskip-v4 BeamRiderNoFrameskip-v4 \778
--no-check-empty-runs \779
--pc.ncols 3 \780
--pc.ncols-legend 2 \781
--rliable \782
--rc.score_normalization_method atari \783
--rc.normalized_score_threshold 8.0 \784
--rc.sample_efficiency_plots \785
--rc.sample_efficiency_and_walltime_efficiency_method Median \786
--rc.performance_profile_plots \787
--rc.aggregate_metrics_plots \788
--rc.sample_efficiency_num_bootstrap_reps 1000 \789
--rc.performance_profile_num_bootstrap_reps 1000 \790
--rc.interval_estimates_num_bootstrap_reps 1000 \791
--output-filename static/gae_for_ppo_value_atari_per_env \792
--scan-history \793
--rc.sample_efficiency_figsize 7 4794
795
python -m openrlbenchmark.rlops \796
--filters ’?we=openrlbenchmark&wpn=sb3&ceik=env&cen=algo&metric=eval/mean_reward’ ’ppo?cl=PPO’ \797
--filters ’?we=modanesh&wpn=openrlbenchmark&ceik=env&cen=algo&metric=eval/mean_reward’ ’ppo?cl=PPO w/798
MC for value estimation’ \799
--env-ids InvertedDoublePendulum-v2 InvertedPendulum-v2 Reacher-v2 HalfCheetah-v3 Hopper-v3 Swimmer-v3800
Walker2d-v3 LunarLander-v2 \801
--no-check-empty-runs \802
--pc.ncols 3 \803
--pc.ncols-legend 2 \804
--rliable \805
--rc.normalized_score_threshold 1.0 \806
--rc.sample_efficiency_plots \807
--rc.sample_efficiency_and_walltime_efficiency_method Median \808
--rc.performance_profile_plots \809
--rc.aggregate_metrics_plots \810
--rc.sample_efficiency_num_bootstrap_reps 1000 \811
--rc.performance_profile_num_bootstrap_reps 1000 \812
--rc.interval_estimates_num_bootstrap_reps 1000 \813
--output-filename static/gae_for_ppo_value_mujoco_per_env \814
--scan-history \815
--rc.sample_efficiency_figsize 7 4816
23

0M 2M 4M 6M 8M 10M
0
100
200
300
400
BreakoutNoFrameskip-v4
0M 2M 4M 6M 8M 10M
200
400
600
800
1000
SpaceInvadersNoFrameskip-v4
0M 2M 4M 6M 8M 10M
500
1000
1500
2000
SeaquestNoFrameskip-v4
0M 2M 4M 6M 8M 10M
0
200
400
600
800
EnduroNoFrameskip-v4
0M 2M 4M 6M 8M 10M
20
10
0
10
20
PongNoFrameskip-v4
0M 2M 4M 6M 8M 10M
0
2500
5000
7500
10000
12500
15000
QbertNoFrameskip-v4
0M 2M 4M 6M 8M 10M
1000
2000
3000
4000
BeamRiderNoFrameskip-v4
Steps
Episodic Return
PPO PPO w/ MC for value estimation
Figure 16: Comparison between the original PPO and the PPO with MC value estimates in various
MuJoCo and Box2D environments. Plots represent the evolution of the episodic return as a function
of the number of interactions with the environment, and shaded areas represent the standard deviation.
24

0M 0.2M 0.4M 0.6M 0.8M 1M
0
2000
4000
6000
8000
10000
InvertedDoublePendulum-v2
0M 0.2M 0.4M 0.6M 0.8M 1M
0
250
500
750
1000
1250
InvertedPendulum-v2
0M 0.2M 0.4M 0.6M 0.8M 1M
10
8
6
4
Reacher-v2
0M 0.2M 0.4M 0.6M 0.8M 1M
0
1000
2000
3000
4000
5000
6000
HalfCheetah-v3
0M 0.2M 0.4M 0.6M 0.8M 1M
200
400
600
800
1000
1200
Hopper-v3
0M 0.2M 0.4M 0.6M 0.8M 1M
0
100
200
300
400
Swimmer-v3
0M 0.2M 0.4M 0.6M 0.8M 1M
0
1000
2000
3000
4000
Walker2d-v3
0K 200K 400K 600K 800K 1000K
600
400
200
0
200
LunarLander-v2
Steps
Episodic Return
PPO PPO w/ MC for value estimation
Figure 17: Comparison between the original PPO and the PPO with MC value estimates in various
MuJoCo and Box2D environments. Plots represent the evolution of the episodic return as a function
of the number of interactions with the environment, and shaded areas represent the standard deviation.
25

C Refine the MuJoCo benchmark with Stable Baselines3817
In this appendix, we present a synthetic representation of the learning results of the Stable Baselines3818
algorithms (Raffin et al., 2021) tested on the MuJoCo benchmark (Brockman et al., 2016; Todorov819
et al., 2012), whose data is contained in ORLB . At the time of writing, data from 757 runs has820
been used, unevenly distributed between the different experiments. It is important to emphasise that821
the optimisation of hyperparameters and the training budget vary from one experiment to another.822
Consequently, the results should be interpreted with caution. All the hyperparameters and raw823
data used to generate these curves are available on ORLB . Figure 18 shows the aggregation of824
the final performances following the recommendations of Agarwal et al. (2021), and Figure 19 the825
corresponding performance profiles. Figure 20 shows the learning curves as a function of the number826
of interactions.827
0.2 0.4 0.6 0.8
ARS
TQC
TD3
SAC
PPO LSTM
PPO
A2C
DDPG
TRPO
Median
0.2 0.4 0.6 0.8
Normalized Score
IQM
0.4 0.6 0.8
Mean
0.2 0.4 0.6
Optimality Gap
Figure 18: Aggregated final normalized episodic return with 95% stratified bootstrap CIs on the
MuJoCo benchmark of the algorithms integrated into Stable Baselines3.
0.0 0.2 0.4 0.6 0.8 1.0
Normalized Score ( )
0.00
0.25
0.50
0.75
1.00Fraction of runs with score >
0.0 0.2 0.4 0.6 0.8 1.0
Normalized Score ( )
0.00
0.25
0.50
0.75
1.00Fraction of tasks with score > 
TRPO
DDPG
A2C
PPO
PPO LSTM
SAC
TD3
TQC
ARS
Figure 19: Performance profile of algorithms implemented using Stable Baselines 3 (Raffin et al.,
2021) on the MuJoCo benchmark (Todorov et al., 2012). Scores are normalized using the min-max
method.
The command used to generate Figures 18, 19 and 20 is as follows7.828
python -m openrlbenchmark.rlops \829
--filters ’?we=openrlbenchmark&wpn=sb3&ceik=env&cen=algo&metric=eval/mean_reward’ ’trpo?cl=TRPO’ \830
--filters ’?we=openrlbenchmark&wpn=sb3&ceik=env&cen=algo&metric=eval/mean_reward’ ’ddpg?cl=DDPG’ \831
--filters ’?we=openrlbenchmark&wpn=sb3&ceik=env&cen=algo&metric=eval/mean_reward’ ’a2c?cl=A2C’ \832
--filters ’?we=openrlbenchmark&wpn=sb3&ceik=env&cen=algo&metric=eval/mean_reward’ ’ppo?cl=PPO’ \833
--filters ’?we=openrlbenchmark&wpn=sb3&ceik=env&cen=algo&metric=eval/mean_reward’ ’ppo_lstm?cl=PPO LSTM834
’ \835
--filters ’?we=openrlbenchmark&wpn=sb3&ceik=env&cen=algo&metric=eval/mean_reward’ ’sac?cl=SAC’ \836
--filters ’?we=openrlbenchmark&wpn=sb3&ceik=env&cen=algo&metric=eval/mean_reward’ ’td3?cl=TD3’ \837
--filters ’?we=openrlbenchmark&wpn=sb3&ceik=env&cen=algo&metric=eval/mean_reward’ ’ars?cl=ARS’ \838
--filters ’?we=openrlbenchmark&wpn=sb3&ceik=env&cen=algo&metric=eval/mean_reward’ ’tqc?cl=TQC’ \839
--env-ids Ant-v3 BipedalWalker-v3 BipedalWalkerHardcore-v3 HalfCheetah-v3 Hopper-v3 Humanoid-v3 Swimmer840
-v3 Walker2d-v3 \841
--no-check-empty-runs \842
--pc.ncols 2 \843
--pc.ncols-legend 4 \844
--rliable \845
--rc.normalized_score_threshold 1.0 \846
--output-filename static/mujoco_sb3 \847
--scan-history848
7For Figure 20, we are omitting ARS as it was run with many more steps, and its inclusions hinder readability.
26

0M 0.2M 0.4M 0.6M 0.8M 1M
0
2000
4000
6000
Ant-v3
0M 1M 2M 3M 4M 5M
100
0
100
200
300
400
BipedalWalker-v3
0M 50M 100M 150M 200M
200
100
0
100
200
300
BipedalWalkerHardcore-v3
0M 0.2M 0.4M 0.6M 0.8M 1M
0
2000
4000
6000
8000
10000
12000
HalfCheetah-v3
0M 0.2M 0.4M 0.6M 0.8M 1M
0
1000
2000
3000
4000
Hopper-v3
0M 2M 4M 6M 8M 10M
0
2000
4000
6000
8000
Humanoid-v3
0M 0.2M 0.4M 0.6M 0.8M 1M
0
100
200
300
400
Swimmer-v3
0M 0.2M 0.4M 0.6M 0.8M 1M
0
1000
2000
3000
4000
5000
Walker2d-v3
Steps
Episodic Return
TRPO
DDPG
A2C
PPO
PPO LSTM
SAC
TD3
TQC
Figure 20: Sample efficiency curves for algorithms on the MuJoCo Benchmark (Todorov et al., 2012).
This graph presents the mean episodic return for algorithms implemented using Stable Baselines 3
(Raffin et al., 2021), averaged across a minimum of 10 runs (refer to ORLB for specific run counts).
Data points are subsampled to 10,000 and interpolated for clarity. The curves are smoothed using
a rolling average with a window size of 100. The shaded regions around each curve indicate the
standard deviation.
27